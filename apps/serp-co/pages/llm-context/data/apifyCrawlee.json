[
  {
    "owner": "apify",
    "repo": "crawlee",
    "content": "TITLE: Complete Product Data Extraction with Playwright\nDESCRIPTION: A comprehensive code snippet that combines all the previous extraction techniques to collect complete product information. It extracts and formats manufacturer, title, SKU, price, and stock information from a product page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Puppeteer Crawler\nDESCRIPTION: Implementation of a web crawler using Puppeteer to navigate and extract all links from a website. Uses enqueueLinks() to automatically add discovered links to the RequestQueue while handling JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing new URLs...');\n        // Add new URLs to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl\nconst startUrls = await downloadListOfUrls({ url: 'https://example.com' });\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Complete Website Crawler with Conditional Logic in TypeScript\nDESCRIPTION: A comprehensive PlaywrightCrawler implementation that handles different page types using request labels. The code navigates through category pages, enqueues product detail pages, and handles pagination to crawl an entire e-commerce website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Playwright Crawler with Cheerio Integration for Enhanced Scraping\nDESCRIPTION: An advanced implementation combining Playwright with Cheerio for more efficient HTML parsing. This approach allows for scraping JavaScript-rendered content while using Cheerio's simpler selectors for data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        // This is a function that will be called for each URL\n        console.log('Testing the crawler');\n\n        // Get the HTML content of the page\n        const html = await page.content();\n        // Parse the HTML with cheerio\n        const $ = cheerio.load(html);\n        \n        // Use cheerio to select the elements and extract data\n        $('.collection-block-item').each((index, element) => {\n            const categoryText = $(element).text();\n            console.log('CATEGORY:', categoryText);\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Complete Crawlee Scraping Example with Data Saving (JavaScript)\nDESCRIPTION: A full example of a Crawlee scraper that extracts data from a website and saves it using Dataset.pushData(). This code demonstrates the entire process from setting up the crawler to saving the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright.\n        const results = await page.evaluate(() => {\n            const books = [];\n            document\n                .querySelectorAll('div.book-item')\n                .forEach((book) => {\n                    const title = book.querySelector('h3').innerText;\n                    const url = book.querySelector('h3 a').href;\n                    const imgUrl = book.querySelector('img').src;\n                    const price = book.querySelector('p.price').innerText;\n                    books.push({\n                        title,\n                        url,\n                        imgUrl,\n                        price,\n                    });\n                });\n            return books;\n        });\n\n        // Save the data to the default dataset.\n        await Dataset.pushData(results);\n\n        // Enqueue all links from the page.\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://books.toscrape.com/']);\n```\n\n----------------------------------------\n\nTITLE: Performing Recursive Web Crawl with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler from Crawlee. It includes handling of request queues, parsing of HTML content, and extraction of data from crawled pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\nimport { chromium } from 'playwright';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Web Crawler with PlaywrightCrawler\nDESCRIPTION: This example demonstrates how to create a PlaywrightCrawler instance with request and error handlers. The crawler visits specified URLs, extracts page titles, and stores the data to a Dataset. It also includes error handling for failed requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/playwright-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // This function is called to extract data from a single web page\n        // 'page' is an instance of Playwright.Page with page.goto(request.url) already called\n        // 'request' is an instance of Request class with information about the page to load\n        await Dataset.pushData({\n            title: await page.title(),\n            url: request.url,\n            succeeded: true,\n        })\n    },\n    async failedRequestHandler({ request }) {\n        // This function is called when the crawling of a request failed too many times\n        await Dataset.pushData({\n            url: request.url,\n            succeeded: false,\n            errors: request.errorMessages,\n        })\n    },\n});\n\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Complete Web Crawling with Category and Product Pages\nDESCRIPTION: Comprehensive crawling implementation that handles different page types (start page, category pages, and product detail pages) using request labels to differentiate between them and pagination support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright\nDESCRIPTION: Code to determine if a product is in stock by checking for the existence of a specific element. It uses element filtering and counting to convert the presence of an 'In stock' message to a boolean value.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Crawling with PuppeteerCrawler\nDESCRIPTION: Example of recursive crawling of the Crawlee website using PuppeteerCrawler. It extracts the title of each page and saves the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler with Implicit RequestQueue\nDESCRIPTION: Creates a CheerioCrawler using the simplified approach with implicit RequestQueue. This optimized example demonstrates how to directly provide URLs to the crawler.run() method, avoiding manual queue initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Crawling of a Shopify Store with PlaywrightCrawler\nDESCRIPTION: This code snippet shows a complete crawling setup for a Shopify store using PlaywrightCrawler. It handles the start page, category pages, and product detail pages. The crawler uses different selectors and labels to manage the crawling process efficiently.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Complete Crawlee Scraper with Data Saving (JavaScript)\nDESCRIPTION: This is the final version of the Crawlee scraper code, including data extraction and saving. It uses PlaywrightCrawler to navigate web pages and Dataset.pushData() to store the extracted data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Parsing Product Price with Playwright\nDESCRIPTION: Complex price extraction that filters for elements containing price data, extracts the numeric price value, and converts it to a number. It handles formatting issues like commas in the price string.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Recursive Web Scraping with PuppeteerCrawler and RequestQueue in Crawlee\nDESCRIPTION: This example demonstrates how to use PuppeteerCrawler to recursively scrape the Hacker News website using headless Chrome/Puppeteer. It starts with a single URL, finds links to next pages, enqueues them and continues until no more desired links are available, with results stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CrawlSource}\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawling Implementation with PlaywrightCrawler\nDESCRIPTION: A comprehensive TypeScript example that handles crawling through both category and product detail pages. It uses different selectors and labels to navigate through categories, handle pagination, and queue up product detail pages for further processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: Code to extract the product SKU from an HTML page using Playwright. It targets a span element with the class 'product-meta__sku-number' and retrieves its text content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Performing Recursive Web Crawl with PuppeteerCrawler in JavaScript/TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler from Crawlee. It includes configuration for the crawler, request handling, and data extraction logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n        await Dataset.pushData({ title, url: request.url });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            globs: ['https://apify.com/**'],\n            label: 'detail',\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://apify.com/']);\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Screenshot Using Utils saveSnapshot\nDESCRIPTION: Advanced implementation using PuppeteerCrawler with the context-aware saveSnapshot utility. Provides automatic handling of screenshots with additional metadata and naming conventions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from '@crawlee/puppeteer';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, utils }) {\n        await utils.puppeteer.saveSnapshot(page, {\n            key: `screenshot-${request.urlDetails.domain}`,\n            saveHtml: false,\n        });\n    },\n});\n\nawait crawler.run(['https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Basic Playwright Crawler for Category Scraping in Crawlee\nDESCRIPTION: A simple Playwright Crawler implementation that visits an e-commerce site and extracts text content from category elements. This code serves as a sanity check to verify the scraping approach before implementing the full solution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        // This is a function that will be called for each URL\n        console.log('Testing the crawler');\n\n        // Extract data from the page using Playwright\n        const categoryElements = await page.$$('.collection-block-item');\n        \n        for (const categoryElement of categoryElements) {\n            const categoryText = await categoryElement.textContent();\n            console.log('CATEGORY:', categoryText);\n        }\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Repository Search with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to fill out and submit a search form on GitHub, extract search results, and save them to a dataset. It utilizes Puppeteer for browser automation and includes error handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\nimport { startOfYear } from 'date-fns';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, log }) {\n        log.info('Navigating to GitHub');\n        await page.goto('https://github.com/search/advanced');\n\n        log.info('Filling in search form');\n        await page.type('#adv_code_search input.js-advanced-search-input', 'apify');\n        await page.type('#search_from', 'apify');\n        await page.type('#search_date', startOfYear(new Date()).toISOString().split('T')[0]);\n        await page.select('#search_language', 'JavaScript');\n\n        log.info('Submitting form');\n        await Promise.all([\n            page.waitForNavigation(),\n            page.click('#search_form .js-advanced-search-submit'),\n        ]);\n\n        log.info('Extracting data from results');\n        const repositories = await page.$$eval('.repo-list-item', (elements) =>\n            elements.map((el) => ({\n                url: el.querySelector<HTMLAnchorElement>('a.v-align-middle')?.href,\n                name: el.querySelector('a.v-align-middle')?.textContent?.trim(),\n                description: el.querySelector('p.mb-1')?.textContent?.trim(),\n            }))\n        );\n\n        log.info(`Found ${repositories.length} repositories`);\n        await Dataset.pushData(repositories);\n    },\n    maxRequestsPerCrawl: 1,\n});\n\nawait crawler.run(['https://github.com/search/advanced']);\n\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Usage in TypeScript\nDESCRIPTION: A simple example of using CheerioCrawler in TypeScript to scrape product information. It demonstrates how to use typed interfaces for the data being collected and how to set up the crawler with proper TypeScript types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Define the structure of your data\ninterface ProductData {\n    url: string;\n    title: string;\n}\n\n// Create the crawler and add the required handlers\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the results to dataset with typing\n        await Dataset.pushData<ProductData>({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Store Crawler\nDESCRIPTION: Full implementation of a PlaywrightCrawler that handles category pages, product details, and pagination using different request labels and selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler. It includes configuration for the crawler, request queue setup, and a handler function for processing each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n        await Dataset.pushData({ title, url: request.url });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            globs: ['https://apify.com/**'],\n            exclude: ['.pdf'],\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://apify.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Strategy Implementation\nDESCRIPTION: Implementation showing how to crawl all links regardless of domain using CheerioCrawler. Uses the 'all' enqueue strategy to process any URLs found during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{AllLinksSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl a list of URLs from a file, make HTTP requests, parse HTML using Cheerio, and extract the page title and h1 tags. It includes error handling and logging of results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Crawler configuration\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        const h1Texts = $('h1')\n            .map((_, el) => $(el).text())\n            .get();\n\n        // Save results to Dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1Texts,\n        });\n\n        // Add all links from page to crawler queue\n        await enqueueLinks();\n    },\n    // Function called in case of error\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Read list of URLs from a text file\nconst urls = await readFile('urls.txt', 'utf8')\n    .then((text) => text.split('\\n').filter((line) => line.trim() !== ''));\n\n// Add URLs to crawler queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This code example demonstrates SessionPool integration with PuppeteerCrawler for browser automation. It shows how to handle sessions, configure proxies, and manage blocked requests while scraping with Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // The default sessionPoolOptions are described in BasicCrawler example\n    // and it will work the same out of the box with PuppeteerCrawler\n    async requestHandler({ page, request, session }) {\n        // Process the data on the page using Puppeteer\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // With browser crawlers we can check if the page was actually blocked\n        // and we can retire the session and throw an error to retry the request\n        const isBlocked = await page.evaluate(() => {\n            const title = document.title.toLowerCase();\n            return title.includes('access denied') || title.includes('forbidden');\n        });\n\n        if (isBlocked) {\n            session!.retire();\n            throw new Error('We got blocked, lets use another session for these requests');\n        }\n\n        // Else we can mark the session good\n        session!.markGood();\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: ['http://user:password@proxy.com:8000']\n    }),\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler\nDESCRIPTION: Example code demonstrating how to use PlaywrightCrawler and RequestQueue to recursively scrape Hacker News. The crawler starts with a single URL, finds and enqueues links to next pages, and stores results in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Add new request to queue\n        await enqueueLinks({\n            globs: ['https://news.ycombinator.com/*'],\n            label: 'detail',\n        });\n    },\n});\n\n// Start the crawler with initial URL\nawait crawler.run(['https://news.ycombinator.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Website Links with Cheerio Crawler in Crawlee\nDESCRIPTION: This code shows how to crawl all links on a website using Cheerio Crawler in Crawlee. It sets up a RequestQueue, initializes a CheerioCrawler, and uses enqueueLinks() to add new links to the queue as the crawler navigates through pages. The crawler also logs the URL and title of each page it visits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n    maxConcurrency: 10, // Maximum concurrency\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // This function is called for every page the crawler visits.\n    async requestHandler({ request, enqueueLinks, log, $ }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract desired data from the page using Cheerio.$.\n\n        // Add all links from the page to the crawler's RequestQueue.\n        // The `enqueueLinks()` method ignores URLs that were already visited.\n        // The function loads the URLs using the same options as the crawler\n        await enqueueLinks();\n    },\n});\n\n// Start the crawler and wait for it to finish.\nawait crawler.run(['https://crawlee.dev']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler in TypeScript\nDESCRIPTION: This code demonstrates using Playwright to crawl a website's sitemap. It leverages the Sitemap utility to extract URLs and then uses Playwright's automation capabilities to navigate to each page, extract content, and store the resulting metadata.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Prepare the crawler configuration\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, log, pushData }) {\n        // Wait for the content to load\n        await page.waitForSelector('title');\n\n        // Extract title and meta description\n        const title = await page.title();\n        const h1Content = await page.$eval('h1', (el) => el.textContent);\n        const metaDescription = await page.$eval('meta[name=description]', (el) => el.getAttribute('content'));\n\n        const metadata = {\n            url: request.url,\n            title,\n            h1: h1Content,\n            metaDescription,\n        };\n\n        log.info('Page scraped', metadata);\n\n        // Store the results\n        await pushData(metadata);\n    },\n});\n\n// Create Sitemap instance\nconst sitemap = new Sitemap({ urls: startUrls });\n\n// Download and process sitemap\nawait sitemap.downloadAllSitemaps();\n\n// Get URLs from the sitemap\nconst urls = sitemap.getSitemapUrls().map((item) => ({ url: item.url }));\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration Setup in Crawlee\nDESCRIPTION: Demonstrates how to create a basic ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL. This is the fundamental setup for using proxies in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Advanced Autoscaled Pool Configuration for CheerioCrawler\nDESCRIPTION: This code demonstrates how to fine-tune the autoscaling behavior of a crawler by configuring the autoscaledPoolOptions. It adjusts parameters like desired concurrency, scaling ratios, and interval settings for precise performance control.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\n// Advanced configuration of how the crawler should scale up and down\nconst crawler = new CheerioCrawler({\n    // Control how many requests should be done at a time when starting\n    minConcurrency: 5,\n    // Control the maximum parallel requests at any time\n    maxConcurrency: 100,\n    // Extreme fine-tuning of the autoscaling behavior\n    autoscaledPoolOptions: {\n        // These are the values that you can set, but they're all optional\n        desiredConcurrency: 20,\n        desiredConcurrencyRatio: 0.9,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 1,\n        loggingIntervalSecs: 80,\n        autoscaleIntervalSecs: 15,\n        maxTasksPerMinute: 60,\n    },\n    // other options...\n});\n```\n\n----------------------------------------\n\nTITLE: Scraping Dynamic Content with PlaywrightCrawler\nDESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content, automatically waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of the first actor card\n        const actorText = await page.textContent('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Setup with Implicit RequestQueue in Crawlee\nDESCRIPTION: Demonstrates a more concise way to set up a CheerioCrawler using the crawler's implicit RequestQueue. This approach allows for faster request addition and simpler code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: A simple example of using the enqueueLinks() function without parameters to find and follow all links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler for Hacker News Scraping\nDESCRIPTION: Demonstrates recursive web scraping of Hacker News using PuppeteerCrawler and RequestQueue. The crawler processes URLs from Hacker News, extracts data including titles and next page links, and stores results in a dataset. Includes error handling and request queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, RequestQueue } from '@crawlee/core';\nimport { PuppeteerCrawler } from '@crawlee/puppeteer';\n\n// Create a RequestQueue\nconst requestQueue = await RequestQueue.open();\n// Add the initial request\nawait requestQueue.addRequest({ url: 'https://news.ycombinator.com' });\n\n// Create a PuppeteerCrawler\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n\n        // Extract data from the page\n        const results = await page.$$eval('.athing', ($posts) => {\n            return $posts.map((el) => {\n                const titleEl = el.querySelector('.title a');\n                return {\n                    title: titleEl?.textContent ?? '',\n                    rank: el.querySelector('.rank')?.textContent ?? '',\n                    href: titleEl?.getAttribute('href') ?? '',\n                };\n            });\n        });\n\n        // Save the data to dataset\n        await Dataset.pushData(results);\n\n        // Enqueue the 'More' button if it exists\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request }) {\n        console.log(`Request ${request.url} failed too many times`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-Rendered Content with PlaywrightCrawler\nDESCRIPTION: This snippet uses PlaywrightCrawler to successfully scrape content from Apify Store that is rendered client-side with JavaScript. It demonstrates automatic waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of an actor card\n        const actorText = await page.textContent('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with CheerioCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with CheerioCrawler to handle IP rotation and session management. Shows how to integrate with proxy configuration, handle requests, and process responses while maintaining session health.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    // This works directly with CheerioCrawler\n    useSessionPool: true,\n    // By default this is 3, which means a session is retired\n    // when it gets 3 error (400+) responses.\n    maxSessionUsageCount: 5,\n    // This is the default value and will be used to add\n    // Headers and options to fetch when using a session\n    sessionPoolOptions: {\n        sessionOptions: {\n            // This is used to identify the user agent of the session.\n            // It is used in the default createSessionFunction.\n            // The userAgent is also added automatically to the request\n            // headers when using sessions.\n            maxErrorScore: 1,\n            // This is the default value. It means that sessions with\n            // at least 3 successful responses will be preserved\n            // and reused when the crawler restarts or other sessions\n            // are not available.\n            maxUsageCount: 50,\n        },\n    },\n    async requestHandler({ request, $, response, session, log }) {\n        const { statusCode } = response;\n  \n        // Based on the status code, mark the session as working, blocked, or failed.\n        if (statusCode === 200) {\n            session.markGood();\n            log.info(`Request ${request.url} succeeded and loaded ${$('title').text()}`);\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Advanced AutoscaledPool Configuration in Crawlee\nDESCRIPTION: Example of configuring advanced autoscaling behavior for a CheerioCrawler through the autoscaledPoolOptions object. This demonstrates how to fine-tune scaling parameters like concurrency ratios and check intervals.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n\tautoscaledPoolOptions: {\n\t\tdesiredConcurrency: 50,\n\t\tdesiredConcurrencyRatio: 0.9,\n\t\tscaleUpStepRatio: 0.05,\n\t\tscaleDownStepRatio: 0.05,\n\t\tmaybeRunIntervalSecs: 0.5,\n\t\tloggingIntervalSecs: 60,\n\t\tautoscaleIntervalSecs: 10,\n\t\tmaxTasksPerMinute: 60, // 1 request per second\n\t},\n\t// ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl all links on a website using CheerioScraper. It uses enqueueLinks() to add new links to the RequestQueue as the crawler navigates through pages and logs the URL and title of each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // On each request, call the following function\n    // which will extract data from the site and add new links to the queue.\n    async requestHandler({ request, enqueueLinks, log, $ }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract all links from the page.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl all links on a website. It sets up the crawler, initializes a request queue, and processes each page to extract the title and enqueue new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Multiple Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It creates a crawler that visits multiple URLs, takes a screenshot of each page, and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshot = await page.screenshot();\n        const key = `screenshot-${Math.random()}.png`;\n        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl all links on a website using Cheerio Crawler. It creates a request queue, initializes the crawler with a starting URL, and uses enqueueLinks() to add new links to the queue as it navigates through pages. Links are filtered to the same subdomain by default.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls, Dataset } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerCrawl: 100,\n    maxConcurrency: 50,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee configuration and handlers\n    // that will be used for each of the requests.\n    async requestHandler({ request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n\n        // Save the HTML content of the page to dataset\n        await Dataset.pushData({\n            url: request.url,\n            // html: $ is the Cheerio object, $.html() is the HTML content of the page\n            html: $.html(),\n        });\n    },\n});\n\n// Add first URL to a RequestQueue\nconst url = 'https://apify.com';\nawait crawler.run([url]);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Cheerio Crawler in TypeScript\nDESCRIPTION: This code sets up a CheerioScraper to crawl all links on a starting URL. It uses the enqueueLinks() method to add new discovered links to the request queue and limits the crawl to a maximum of 100 requests. The crawler extracts and logs the title from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioScraper } from 'crawlee';\n\n// Create an instance of the CheerioScraper class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioScraper({\n    // Let's limit our crawls to make the example quick to run.\n    maxRequestsPerCrawl: 100,\n});\n\n// Crawlers come with various utilities, such as request queue support.\nawait crawler.run([\n    // Tell the crawler to start with this URL\n    'https://crawlee.dev',\n], {\n    // This function will be called for each URL\n    async requestHandler({ request, enqueueLinks, log, $ }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Extract data from the page\n        const title = $('title').text();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Add all links from the page to the crawling queue\n        await enqueueLinks();\n    },\n});\n\nlog.info('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This example shows how to capture screenshots of multiple web pages when using PuppeteerCrawler. It demonstrates creating a crawler that processes multiple URLs, taking screenshots of each page using the page.screenshot() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page }) {\n        await page.setViewport({ width: 1920, height: 1080 });\n\n        // Capture the screenshot using page.screenshot()\n        const screenshot = await page.screenshot({ fullPage: true });\n\n        // Create a key from the URL\n        const { host, pathname } = new URL(request.url);\n        const key = `screenshot-${host}${pathname}`\n            .replace(/\\.[^.]+$/, '')\n            .replace(/[^a-zA-Z0-9-_]/g, '-');\n\n        // Save the screenshot to the default key-value store.\n        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });\n        console.log(`Screenshot of ${request.url} saved with key: ${key}`);\n    },\n});\n\n// Start the crawler with a list of URLs\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: Demonstrates the usage of SessionPool with CheerioCrawler for managing sessions and proxy rotations. It includes configuration for proxy, session pool, and shows how to handle requests and block detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // The `useSessionPool` is enabled by default\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ $, request, session }) {\n        const title = $('title').text();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // Check if you got blocked here and invalidate the session\n        if (await isBlocked($)) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler to scrape Hacker News. It starts from the main page, extracts article details, and follows pagination links. The crawler uses a RequestQueue to manage URLs and stores results in the default dataset. It includes error handling and custom page function logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestQueue to add more URLs to crawl.\n    requestQueue: await RequestQueue.open(),\n\n    // Here you can set options that are passed to the Playwright browser.\n    launchContext: {\n        launchOptions: {\n            headless: true,\n        },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 50,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Playwright scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    async requestHandler({ request, page, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Playwright within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link from each post on Hacker News.\n            $posts.forEach(($post) => {\n                scrapedData.push({\n                    title: $post.querySelector('.title a').innerText,\n                    rank: $post.querySelector('.rank').innerText,\n                    href: $post.querySelector('.title a').href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        const infos = await page.$('.morelink');\n        if (infos) {\n            const href = await infos.getAttribute('href');\n            await crawler.requestQueue.addRequest({ url: `https://news.ycombinator.com/${href}` });\n        }\n    },\n\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.warning(`Request ${request.url} failed too many times.`);\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(['https://news.ycombinator.com/']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Playwright Crawler in TypeScript\nDESCRIPTION: This code illustrates crawling a website with PlaywrightCrawler. It navigates through pages starting from the initial URL, extracts the page title, and uses enqueueLinks() to add all discovered links to the crawling queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome browser.\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // On each request, call the following function\n    // which will extract data from the site and add new links to the queue.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract all links from the page.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Playwright Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL using the crawler. The crawler extracts the page title and URL, storing them as results. It includes a tip for running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await crawler.pushData({ title, url: request.url });\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests for demonstration purposes\n});\n\nconst { requestQueue } = await crawler.getRequestQueue();\n\n// Add sitemap.xml URLs to the request queue\nconst baseUrl = 'https://example.com';\nconst urls = await downloadListOfUrls({\n    url: `${baseUrl}/sitemap.xml`,\n});\nfor (const url of urls) {\n    await requestQueue.addRequest({ url });\n}\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Cheerio Crawler\nDESCRIPTION: Implementation of a Cheerio-based crawler that follows and processes all links within a website's subdomain. Uses the enqueueLinks() method to automatically add new discovered links to the RequestQueue for processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('enqueueing new URLs');\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Session Management with PlaywrightCrawler\nDESCRIPTION: Shows how to implement proxy session management in PlaywrightCrawler using SessionPool to maintain consistent browser fingerprints and avoid detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    sessionPoolOptions: {\n        persistStateKeyValueStoreId: 'my-session-pool',\n        persistStateKey: 'playwright-crawler-sessions',\n    },\n    async requestHandler({ request, page, session, log }) {\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to scrape Hacker News. It initializes a RequestQueue, sets up the crawler with custom handlers for page processing and link discovery, and runs the crawler. The script extracts titles and links from Hacker News pages and stores them in a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://news.ycombinator.com/' });\n\n// Create the crawler\nconst crawler = new PlaywrightCrawler({\n    requestQueue,\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API\n        const data = await page.$$eval('.athing', ($posts) => {\n            return $posts.map($post => {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const url = $post.querySelector('.title a').href;\n                return {\n                    title,\n                    rank: Number(rank.replace('.', '')),\n                    url\n                };\n            });\n        });\n\n        // Save the data to dataset\n        await Dataset.pushData(data);\n\n        // Enqueue all links from the page\n        await enqueueLinks({\n            globs: ['https://news.ycombinator.com/news?p=*'],\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Start the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler with RequestQueue\nDESCRIPTION: Creates a CheerioCrawler that processes a RequestQueue and extracts the title from each page. This example shows how to initialize a crawler with a pre-configured request queue and define a handler function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Product Pages and Handling Pagination with Playwright and Crawlee\nDESCRIPTION: This snippet shows how to crawl both category and product detail pages using PlaywrightCrawler. It demonstrates handling different page types using request labels, enqueueing product links, and managing pagination.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler with Crawlee's BasicCrawler in TypeScript\nDESCRIPTION: A simple implementation of BasicCrawler that downloads web pages via HTTP requests using the sendRequest utility function and stores their raw HTML and URLs in the default dataset. This demonstrates the core functionality of Crawlee without the extra features of specialized crawlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create a BasicCrawler instance.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    // Here you can write the crawler's logic.\n    async requestHandler({ request, sendRequest, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n\n        // Fetch the page content as a string\n        const { statusCode, body } = await sendRequest();\n\n        // Store the HTML content and URL to the default dataset.\n        await Dataset.pushData({\n            url,\n            html: body,\n            statusCode,\n        });\n\n        log.info(`${url} completed.`);\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl a sitemap).\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/docs/quick-start' },\n    { url: 'https://crawlee.dev/docs/introduction' },\n]);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler Implementation for Crawlee\nDESCRIPTION: Full JavaScript implementation of an AWS Lambda handler function for a Crawlee crawler, including browser configuration and properly formatted response object for the Lambda execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Demonstrates how to use the context-aware sendRequest function with BasicCrawler to make HTTP requests. The sendRequest function internally uses got-scraping which helps mimic browser requests to avoid being blocked.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Playwright Crawler for Website Exploration\nDESCRIPTION: This code snippet sets up a PlaywrightCrawler to scrape category information from a Shopify store. It demonstrates how to initialize the crawler, navigate to the start URL, and extract text content from category elements using CSS selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer for the websites\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract information from the page using Playwright API\n        const categoryElements = await page.$$('.collection-block-item');\n\n        for (const categoryElement of categoryElements) {\n            const categoryText = await categoryElement.textContent();\n            log.info(`Category: ${categoryText}`);\n        }\n\n        // We don't need to do anything else here. Just continue with the next request\n    }\n});\n\n// add first URL to the queue and start the crawl\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Implementing HttpCrawler to Process URLs and Save HTML\nDESCRIPTION: This code demonstrates how to use HttpCrawler from the Crawlee library to process a list of URLs from an external file. It shows the setup of a crawler that loads each URL using plain HTTP requests, processes the HTML content, and saves the results to the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    // Let's limit the crawls to only 10 requests\n    // This is optional, but good for testing and not overloading the target websites\n    maxRequestsPerCrawl: 10,\n    // Called for each URL that is processed\n    async requestHandler({ request, body, statusCode, headers, contentType, crawler, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            statusCode,\n            contentType,\n            html: body.toString(),\n            headers,\n        });\n    },\n});\n\n// Read the list of URLs from an external file and add them to the crawler\n// The file contains a list of URLs, one per line\nconst urls = await readFile('./urls.txt', 'utf8');\n\nawait crawler.addRequests(\n    urls\n        .trim()\n        .split('\\n')\n        .map((url) => ({ url })),\n);\n\n// Run the crawler and wait for it to finish\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Disabled Storage Persistence for GCP\nDESCRIPTION: Updates the CheerioCrawler configuration to disable storage persistence, which is necessary for running in serverless environments like GCP Cloud Functions where persistent file storage is not available.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Performing Recursive Web Crawl with PuppeteerCrawler in JavaScript/TypeScript\nDESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to recursively crawl a website. It sets up the crawler, defines the request handler, and extracts data from the crawled pages. The example includes error handling and logging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        await Dataset.pushData({\n            title,\n            url: request.loadedUrl,\n            html: await page.content(),\n        });\n\n        await enqueueLinks({\n            globs: ['https://apify.com/**'],\n            label: 'detail',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://apify.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Recursively Scraping Hacker News with PlaywrightCrawler\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and stores results in a dataset. The crawler continues until no more desired links are available.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, log } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API.\n        const resultsSelector = '.athing';\n        const links = await page.$$eval(resultsSelector, $posts => {\n            const scrapedData = [];\n\n            // We're extracting title and rank of each post.\n            $posts.forEach($post => {\n                scrapedData.push({\n                    title: $post.querySelector('.title a').innerText,\n                    rank: $post.querySelector('.rank').innerText,\n                    href: $post.querySelector('.title a').href,\n                })\n            });\n\n            return scrapedData;\n        });\n\n        // Save the data to dataset.\n        await Dataset.pushData(links);\n\n        // Enqueue all links from the page.\n        await enqueueLinks();\n    },\n\n    // Uncomment this option to see the browser window.\n    // headless: false,\n\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 100,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawler Routes with PlaywrightRouter in JavaScript\nDESCRIPTION: Defines routing logic for a web crawler using Crawlee's PlaywrightRouter. It includes handlers for detail pages, category pages, and a default handler for the start page. Each handler extracts specific data or enqueues new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler to extract data from webpages with Cheerio in TypeScript\nDESCRIPTION: This code demonstrates how to create a CheerioCrawler that processes URLs from an external file, makes HTTP requests to each URL, and uses Cheerio to extract the page title and all h1 tags from the HTML content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Prepare a list of URLs to crawl\nconst sourceUrls = await downloadListOfUrls({ url: 'https://apify.com/datasets/SNK6PHQ2ZDWB785NZ/items?format=json&clean=1' });\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    minConcurrency: 10,\n    maxConcurrency: 50,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // Limit to 10 requests per one crawler run\n    maxRequestsPerCrawl: 10,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Cheerio code to extract data from the HTML page,\n    // store the results to the default dataset, etc.\n    async requestHandler({ request, $, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract all h1 texts.\n        const h1Texts = [];\n        $('h1').each((index, el) => {\n            h1Texts.push($(el).text());\n        });\n        log.info(`H1 texts: ${h1Texts.join(', ')}`);\n    },\n});\n\n// Run the crawler with initial list of URLs\nawait crawler.run(sourceUrls);\n\n```\n\n----------------------------------------\n\nTITLE: Streaming File Downloads with Node.js and FileDownload Crawler\nDESCRIPTION: Implementation showing how to download large files using Node.js streams via Crawlee's FileDownload crawler. The code demonstrates progress logging and storing downloaded files in a key-value store, with local storage defaulting to './storage/key_value_stores/default'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/file_download_stream.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload } from 'crawlee';\n\nawait FileDownload.download({\n    requests: [\n        { url: 'https://commondatastorage.googleapis.com/books/n-ggpht1.pdf' },\n        { url: 'https://commondatastorage.googleapis.com/books/n-ggpht2.pdf' },\n    ],\n    onFileDownload: (downloadOptions) => {\n        console.log(`Downloading file: ${downloadOptions.url}`);\n        return {\n            key: `file-${Date.now()}`,\n            progress: true,\n        };\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Global Configuration Class\nDESCRIPTION: Example demonstrating how to use the Configuration class to adjust Crawlee's global configuration. This code sets persistStateIntervalMillis to 10000, which causes the crawler to persist its state after 10 seconds.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Downloading Files using Node.js Streams with Crawlee's FileDownload Crawler\nDESCRIPTION: This example shows how to use the FileDownload crawler class to efficiently download large files using Node.js streams. It demonstrates logging download progress and storing the downloaded files in the key-value store, which are saved locally in the './storage/key_value_stores/default' directory when running in local mode.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/file_download_stream.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload, KeyValueStore } from 'crawlee';\n\nconst files = [\n    {\n        url: 'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4',\n        name: 'big-buck-bunny.mp4',\n    },\n];\n\nconst crawler = new FileDownload({\n    // Instead of downloading all data to memory, we will use streams to download files.\n    // Streams are better for large files, because they don't consume that much memory.\n    useStreams: true,\n    maxDownloadBytes: 1000 * 1000 * 1000, // Define a maximum download size (1GB)\n    downloadListeners: {\n        // This listener will be called whenever a piece of data is downloaded.\n        // You can see the download progress here.\n        onChunk: ({ scrollSpeed, loadedBytes, totalBytes, percent }) => {\n            console.log(`Progress: ${loadedBytes}/${totalBytes} (${percent.toFixed(2)}%) - ${scrollSpeed}/s`);\n        },\n    },\n\n    // This is called for every download request made by the crawler.\n    // You can override other options too. They will be passed to got\n    // See this for all the options: https://github.com/sindresorhus/got/blob/main/documentation/2-options.md\n    downloadOptions: {\n        timeout: { response: 30000 },\n    },\n});\n\nawait crawler.run(files);\n\nconst store = await KeyValueStore.open();\n// We can access the downloaded files from the store\nconst { contentType, buffer } = await store.getRecord('big-buck-bunny.mp4');\nconsole.log(`Downloaded: ${contentType} with size ${buffer?.byteLength}`);\n```\n\n----------------------------------------\n\nTITLE: Crawling Selected Links Using CheerioCrawler and Glob Patterns\nDESCRIPTION: This snippet demonstrates how to use CheerioCrawler to selectively crawl webpages by matching URL patterns with globs. It configures a request queue, sets up the crawler with handlers, and uses the enqueueLinks method with a glob pattern to only process links that match specific criteria.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy, log } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the HTML pages and parses them using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler will only process requests from the same hostname as the start URL\n    requestHandler: async ({ request, enqueueLinks, $ }) => {\n        const title = $('title').text();\n        log.info(`The title of \"${request.url}\" is: ${title}`);\n\n        // Add all links from the page to the queue, but only if they match the glob pattern provided\n        await enqueueLinks({\n            // globs: ['https://crawlee.dev/*/*'], // <= Add more page depth\n            globs: ['https://crawlee.dev/*'],\n            strategy: EnqueueStrategy.All,\n        });\n    },\n    // Comment this option to scrape the full website.\n    maxRequestsPerCrawl: 10,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL using the crawler. The crawler extracts the page title and URL, storing them as results. It includes a tip for running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await crawler.pushData({ title, url: request.url });\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests for demonstration purposes\n});\n\nconst { requestQueue } = await crawler.getRequestQueue();\n\n// Add sitemap.xml URLs to the request queue\nconst baseUrl = 'https://example.com';\nconst urls = await downloadListOfUrls({\n    url: `${baseUrl}/sitemap.xml`,\n});\nfor (const url of urls) {\n    await requestQueue.addRequest({ url });\n}\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using PlaywrightCrawler with Element Waiting\nDESCRIPTION: This code demonstrates how to scrape JavaScript-rendered content using PlaywrightCrawler, which includes automatic waiting for elements to appear in the DOM. It properly extracts text from actor cards on Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// PlaywrightCrawler will properly extract JavaScript-rendered content\n// Playwright automatically waits for elements to appear\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // This will automatically wait for the element to appear\n        const actorText = await page.textContent('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Cheerio Crawler in TypeScript\nDESCRIPTION: A code snippet demonstrating how to create a Cheerio-based crawler that extracts and follows all links on a website. It uses enqueueLinks() method to add new URLs to the RequestQueue and allows filtering links by domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Creating and Adding Requests to RequestQueue in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it. The RequestQueue is used to store requests that the crawler will process.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue and Adding URLs in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it. This is the foundation for building crawlers as it establishes where the crawler will navigate.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Playwright Crawler\nDESCRIPTION: Implementation using PlaywrightCrawler for modern browser automation-based link crawling. Provides advanced browser control while discovering and processing links. Requires apify/actor-node-playwright-chrome Docker image for platform execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tiered Proxies with CheerioCrawler in JavaScript\nDESCRIPTION: This code snippet demonstrates how to set up tiered proxies using the ProxyConfiguration class and use it with a CheerioCrawler instance. It shows the configuration of multiple proxy tiers and how to add requests to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/06-24-proxy-management-in-crawlee/index.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    tieredProxyUrls: [\n        ['http://tier1-proxy1.example.com', 'http://tier1-proxy2.example.com'],\n        ['http://tier2-proxy1.example.com', 'http://tier2-proxy2.example.com'],\n        ['http://tier2-proxy1.example.com', 'http://tier3-proxy2.example.com'],\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    requestHandler: async ({ request, response }) => {\n        // Handle the request\n    },\n});\n\nawait crawler.addRequests([\n    { url: 'https://example.com/critical' },\n    { url: 'https://example.com/important' },\n    { url: 'https://example.com/regular' },\n]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler\nDESCRIPTION: Implements a complete CheerioCrawler that fetches a webpage and extracts its title. Shows request queue initialization, crawler configuration, and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawler with Category and Product Pages\nDESCRIPTION: A comprehensive crawler implementation that handles three types of pages: the main page, category pages with pagination, and product detail pages. It uses request labels to differentiate between page types and applies different crawling strategies for each.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Child Processes in Crawlee\nDESCRIPTION: Main implementation of the parallel scraper that forks itself into multiple worker processes. Each worker process reuses the shared request queue and processes URLs concurrently. The parent process coordinates workers and collects data.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { fork } from 'node:child_process';\nimport { fileURLToPath } from 'node:url';\nimport { router } from './routes.mjs';\nimport { getOrInitQueue } from './requestQueue.mjs';\n\n// If this is a worker thread, we want to start scraping from the queue\nif (process.env.IS_WORKER_THREAD) {\n    // Disable the automatic purge on start\n    // This is needed when running locally, as otherwise multiple processes will try to clear the default storage (and that will cause clashes)\n    Configuration.set('purgeOnStart', false);\n\n    // Get the request queue from the parent process\n    const requestQueue = await getOrInitQueue(false);\n\n    // Configure crawlee to store the worker-specific data in a separate directory (needs to be done AFTER the queue is initialized when running locally)\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    // Create a new crawler just for this worker\n    // Note that we're now just using the queue, but we'll do post-processing on the main thread\n    // The router remains the same for both the initial and the worker crawlers\n    const crawler = new PlaywrightCrawler({\n        requestQueue,\n        requestHandler: router,\n        maxRequestRetries: 3,\n        maxConcurrency: 10,\n        preNavigationHooks: [\n            async ({ request, page, crawler }) => {\n                // Acquire a lock for the current URL\n                try {\n                    // Acquire a lock for the request\n                    // The lockRequestTimeout value is randomly generated between 1 and 2 seconds\n                    // to prevent multiple workers from trying to acquire the same lock at the same time\n                    const url = request.url;\n                    const timeoutMs = 1000 + Math.random() * 1000;\n                    const lock = await crawler.getRequestQueue().lockRequest(url, {\n                        lockRequestTimeoutMillis: timeoutMs,\n                    });\n\n                    // To prevent race conditions, when doing e.g. re-enqueuing based on status codes\n                    // we add a custom handler to the request object\n                    let lockReleased = false;\n                    request.releaseLock = () => {\n                        if (lockReleased) return Promise.resolve();\n                        lockReleased = true;\n                        return crawler.getRequestQueue().releaseLock(lock);\n                    };\n\n                    // And create a post-request hook to release the lock if it hasn't been released by the handler\n                    crawler.hooks.afterRequestHandler.add(async ({ request }) => {\n                        // @ts-expect-error Custom property\n                        return request.releaseLock?.();\n                    });\n                    \n                    // Also create a post-request hook to release the lock if the request handler throws an error\n                    // This is just a safeguard, as the lock auto-expires after 30 minutes (set in requestQueue.mjs)\n                    crawler.hooks.failedRequestHandler.add(async ({ request }) => {\n                        // @ts-expect-error Custom property\n                        return request.releaseLock?.();\n                    });\n                } catch (e) {\n                    // If the the lock can't be acquired, invalidate the request so it's not processed\n                    await crawler.getRequestQueue().markRequestHandled(request);\n                    return;\n                }\n            },\n        ],\n    });\n\n    await crawler.run();\n} else {\n    // This is the parent process, so we want to start the workers\n    // We recommend starting more workers than CPUs available to maximize throughput\n    // Also, for simplicity we're hard-coding the number of workers to two\n    const workerCount = 2;\n    const workers = [];\n\n    // Prepare any storage\n    const finished = [];\n    const scrapedData = [];\n\n    // Create the workers\n    for (let i = 0; i < workerCount; i++) {\n        console.log(`Starting worker ${i}...`);\n\n        const worker = fork(fileURLToPath(import.meta.url), [], {\n            env: {\n                ...process.env,\n                IS_WORKER_THREAD: 'true',\n                WORKER_INDEX: i.toString(),\n            },\n        });\n\n        // Listen for messages from the worker\n        worker.on('message', (data) => {\n            console.log(`Received data from worker ${i}`);\n            scrapedData.push(data);\n        });\n\n        // Create a promise for each worker that resolves when the worker exits\n        // Note that in a production environment, you would want to handle worker failures and restart them\n        const workerPromise = new Promise((resolve) => {\n            worker.on('exit', (code) => {\n                console.log(`Worker ${i} exited with code ${code}`);\n                resolve();\n            });\n        });\n\n        workers.push(worker);\n        finished.push(workerPromise);\n    }\n\n    // Wait for all workers to finish\n    await Promise.all(finished);\n\n    console.log(`All workers finished!`);\n    console.log(`Scraped ${scrapedData.length} items`);\n\n    // Write the results back to the storage\n    const { KeyValueStore } = await import('crawlee');\n    await KeyValueStore.setValue('OUTPUT', scrapedData);\n}\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator Using JSDOMCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script navigates to the calculator, clicks a sequence of buttons (1+1=), and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler } from 'crawlee';\n\nconst crawler = new JSDOMCrawler();\n\nawait crawler.run([\n    // The example calculator was moved to a new URL\n    'https://ahfarmer.github.io/calculator/'\n]);\n\ncrawler.router.addDefaultHandler(async ({ window, log }) => {\n    const { document } = window;\n    log.info('Waiting for calculator to load...');\n    \n    // Example how to inject a script into the browser\n    window.someGlobalValue = 'foo';\n    \n    // Calculate 1 + 1\n    document.querySelector('.component-button:nth-child(1) .orange')\n        ?.dispatchEvent(new window.MouseEvent('click', { bubbles: true }));\n    document.querySelector('.component-button:nth-child(4) .orange')\n        ?.dispatchEvent(new window.MouseEvent('click', { bubbles: true }));\n    document.querySelector('.component-button:nth-child(1) .orange')\n        ?.dispatchEvent(new window.MouseEvent('click', { bubbles: true }));\n    document.querySelector('.component-button:nth-child(5) .orange')\n        ?.dispatchEvent(new window.MouseEvent('click', { bubbles: true }));\n    \n    const display = document.querySelector('.component-display');\n    if (display) {\n        const result = display.textContent;\n        log.info(`The result is: ${result}`);\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Specific Links with CheerioCrawler in TypeScript\nDESCRIPTION: This code snippet shows how to use CheerioCrawler to crawl specific links on a website. It uses the globs property in the enqueueLinks() method to filter and queue only URLs matching a certain pattern. The crawler starts from a specified URL and processes each page, logging the title and URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/**'],\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue and Adding a Request in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it. The RequestQueue stores the URLs that the crawler will visit, and the addRequest method converts the URL string into a Request object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler\nDESCRIPTION: A basic setup to test the crawler by visiting the start URL and printing the text content of all category cards. It demonstrates how to use PlaywrightCrawler to navigate and extract data from a web page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, log }) {\n        const categoryElements = await page.$$('.collection-block-item');\n\n        for (const categoryElement of categoryElements) {\n            const categoryText = await categoryElement.textContent();\n            log.info(`Category: ${categoryText}`);\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawlee Router with Multiple Handlers in JavaScript\nDESCRIPTION: Defines a router for a Crawlee scraper with multiple handlers for different page types (detail, category, default). Each handler processes specific page elements and enqueues further links as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This code snippet shows how to completely disable browser fingerprinting in PuppeteerCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Here we disable the automatic generation of fingerprints\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // Other crawler options...\n});\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Store Crawler with Page Type Handling\nDESCRIPTION: A complete implementation that handles different page types (start page, category pages, detail pages) using request labels. It demonstrates pagination handling and targeted link selection for both category and product detail pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: This example demonstrates how to create a custom configuration with a shorter state persistence interval (10 seconds), pass it to a CheerioCrawler, and implement a simple crawling scenario with two requests and controlled delays.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'SameDomain' enqueue strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Crawling!!');\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // We can also use the following:\n            // strategy: 'same-domain',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific browser and operating system parameters for fingerprint generation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'firefox', minVersion: 88 },\n                    { name: 'chrome', minVersion: 93 },\n                ],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management in Crawlee\nDESCRIPTION: Example of using SessionPool independently without a crawler for manual session management. Shows explicit creation of a session pool, managing session lifecycles, and handling request operations with direct session control.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { ProxyConfiguration, SessionPool } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Create a session pool\nconst sessionPool = new SessionPool({\n    // Maximum number of sessions in the pool\n    maxPoolSize: 25,\n    sessionOptions: {\n        // Number of requests for which a session can be used\n        maxUsageCount: 50,\n        // Maximum number of days for which the session can live\n        maxAgeSecs: 86400, // 1 day\n        // Maximum acceptable error rate for the session\n        maxErrorScore: 1,\n    },\n    // Create new session function, to customize the session creation\n    createSessionFunction: async (sessionPool, options) => {\n        const session = new Session({\n            ...options,\n            // Generate a random user agent for the session\n            userData: {\n                ...options.userData,\n                userAgent: `Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36 ${Math.floor(Math.random() * 100000000)}`,\n            },\n        });\n\n        // Assign to session a proxy url\n        await proxyConfiguration.initialize();\n        session.proxyUrl = await proxyConfiguration.newUrl(session.id);\n\n        return session;\n    },\n});\n\n// Get a random session from the pool\nconst session = await sessionPool.getSession();\n\n// Perform a request with the session\ntry {\n    const { statusCode } = await fetch('https://example.com', {\n        headers: {\n            'User-Agent': session.userData.userAgent,\n        },\n        // Use proxy URL from the session\n        agent: session.proxyUrl ? new HttpsProxyAgent(session.proxyUrl) : undefined,\n    });\n    \n    if (statusCode === 200) {\n        // Mark session as working\n        session.markGood();\n    } else if (statusCode === 403) {\n        // Mark session as blocked and retire it\n        session.retire();\n    } else {\n        // Mark session as maybe blocked or having temporary issues\n        session.markBad();\n    }\n} catch (error) {\n    // Mark session as bad and retry with a different session\n    session.markBad();\n}\n\n// When finished with all requests, tear down the session pool\nawait sessionPool.teardown();\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler in Crawlee\nDESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It utilizes the Sitemap class to download and process sitemap URLs, then uses CheerioScraper to scrape the content of each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioScraper, Dataset } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new CheerioScraper({\n    maxRequestsPerCrawl: 20,\n});\n\nconst sitemap = new Sitemap({\n    sitemapUrls: ['https://crawlee.dev/sitemap.xml'],\n});\n\nscraper.router.addDefaultHandler(async ({ $, request, enqueueLinks }) => {\n    const title = $('title').text();\n    await Dataset.pushData({ url: request.url, title });\n    await enqueueLinks();\n});\n\nawait scraper.run(sitemap.urls());\n\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PuppeteerCrawler in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to scrape the Hacker News website. It sets up a crawler that starts from the main page, extracts post details, and follows pagination links. The extracted data is stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue: await RequestQueue.open(),\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Logic to extract data from Hacker News posts\n        const title = await page.locator('.title a').first().textContent();\n        const rank = await page.locator('.rank').first().textContent();\n        const href = await page.locator('.title a').first().getAttribute('href');\n\n        await Dataset.pushData({\n            title,\n            rank,\n            href: href?.startsWith('item?id=') ? `https://news.ycombinator.com/${href}` : href,\n            url: request.loadedUrl,\n        });\n\n        // Enqueue pagination links\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\n// Run the crawler with the initial request\nawait crawler.run(['https://news.ycombinator.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring development execution script in package.json\nDESCRIPTION: Package.json script configuration for running TypeScript code directly in development using ts-node-esm with transpilation mode for faster execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Cheerio Crawler in Crawlee\nDESCRIPTION: This code demonstrates how to use Cheerio Crawler to extract and crawl all links from a website. It creates a RequestQueue, initializes a CheerioCrawler, and uses enqueueLinks() to add new links to the queue as it traverses the site. The crawler handles the request and extracts the title from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Using Global Configuration in Crawlee\nDESCRIPTION: Example demonstrating how to use the global Configuration instance to adjust Crawlee settings in code. This example sets the state persistence interval to 10 seconds using the Configuration class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Scraping with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code shows how to set up a PuppeteerCrawler to recursively scrape the Hacker News website. It extracts titles, urls, and scores from each article, then enqueues discovered links to continue crawling. Results are stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, log } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract data from the page\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n            // We're getting the title, rank and link of each post on Hacker News.\n            for (const $post of $posts) {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const itemId = $post.id;\n                const postUrl = $post.querySelector('.title a').href;\n                const nextPost = $post.nextElementSibling;\n                const points = nextPost.querySelector('.score')?.innerText || 'unknown';\n\n                scrapedData.push({\n                    title,\n                    rank: Number(rank.replace('.', '')),\n                    href: postUrl,\n                    points: points === 'unknown' ? 'unknown' : Number(points.replace(' points', '')),\n                    id: Number(itemId),\n                });\n            }\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n});\n\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in TypeScript\nDESCRIPTION: This code shows how to use Puppeteer Crawler to crawl multiple specified URLs. It extracts the title from each page and stores the results using the Dataset API. Requires the apify/actor-node-puppeteer-chrome image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data using JSDOMCrawler in TypeScript\nDESCRIPTION: This example demonstrates crawling a list of URLs from an external file using JSDOMCrawler. For each page, it parses the HTML with jsdom and extracts the page title and all h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\nconst crawler = new JSDOMCrawler({\n    // Let's limit our crawls to make things fast.\n    maxRequestsPerCrawl: 10,\n});\n\n// This function will be called for each URL \ncrawler.router.addDefaultHandler(async ({ window, enqueueLinks, log }) => {\n    const { document } = window;\n\n    log.info(`Processing ${document.title}`);\n\n    // Extract data from the page using a combination of pure DOM\n    // methods and jQuery-like methods provided by jsdom.\n    const title = document.querySelector('title').textContent;\n    const h1Texts = Array.from(document.querySelectorAll('h1'))\n        .map((el) => el.textContent);\n\n    // Store the results to the default dataset.\n    await Dataset.pushData({\n        title,\n        h1Texts,\n        url: window.location.href,\n    });\n\n    // Extract links from the current page\n    // and add them to the crawling queue.\n    await enqueueLinks();\n});\n\n// Read the list of start URLs from a text file.\nconst startUrls = (await readFile('urls.txt', 'utf8'))\n    .split('\\n')\n    .filter(Boolean);\n\nconst sources = process.env.SOURCES ?? 'urls';\nconst useStartUrls = sources === 'urls';\nif (!useStartUrls) {\n    // You can also use a RequestList\n    await crawler.run(['https://crawlee.dev']);\n} else {\n    await crawler.run(startUrls);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawlee Router for Web Scraping in JavaScript\nDESCRIPTION: Defines a router with handlers for different page types (detail, category, default). Each handler extracts specific data or enqueues new links based on the page content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Puppeteer Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Puppeteer Crawler in Crawlee. It leverages the Sitemap utility to download and process URLs from a sitemap, then uses Puppeteer's browser automation to crawl those URLs and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from '@crawlee/puppeteer';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs and renders their JavaScript using a headless Chrome browser.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Choose a new link from the page but only from the same hostname\n        await enqueueLinks({\n            transformRequestFunction: (req) => {\n                // Ignore urls that go to the homepage\n                if (req.url === 'https://apify.com/') return false;\n\n                return req;\n            },\n        });\n\n        // Save results as JSON to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Download the sitemap and add the URLs to the queue\nconst sitemap = await Sitemap.load('https://apify.com/sitemap.xml');\nfor (const url of sitemap.urls) {\n    await crawler.addRequests([{ url }]);\n}\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl all links on a website. It initializes a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue. The crawler processes each page, logging the title and URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks, log, $ }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            transformRequestFunction(req) {\n                // Add a custom label to the request\n                req.label = 'my-label';\n                return req;\n            },\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: URL Crawling and Data Extraction with JSDOMCrawler\nDESCRIPTION: Shows how to use JSDOMCrawler to process multiple URLs from an external file, make HTTP requests, parse HTML using jsdom, and extract page titles and h1 tags from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerSource}\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright\nDESCRIPTION: A comprehensive example that combines all the previous snippets to extract the full set of product information from a Shopify store page. The code retrieves the URL, manufacturer, title, SKU, price, and stock availability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Session Management with PlaywrightCrawler\nDESCRIPTION: Configuration of session management for PlaywrightCrawler, showing how to handle browser automation with session pooling and proxy rotation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Scraping with Crawlee's BasicCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Crawlee's BasicCrawler to download web pages and store their HTML content and URLs. It utilizes the sendRequest utility function for HTTP requests and saves the data to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log, request }) {\n        const { body } = await sendRequest();\n        log.info(`Got response from ${request.url}`);\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://SDK.apify.com',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler with Crawlee\nDESCRIPTION: A basic implementation of a web crawler using Crawlee's BasicCrawler class. The crawler downloads web pages using HTTP requests via the sendRequest utility function and stores the raw HTML and URLs in a default dataset. The data is stored as JSON files in the ./storage/datasets/default directory when running locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, createBasicRouter } from 'crawlee';\n\nconst router = createBasicRouter();\n\nrouter.addDefaultHandler(async ({ request, sendRequest, log }) => {\n    log.info(`Processing ${request.url}...`);\n\n    const response = await sendRequest();\n\n    log.info(`HTML length: ${response.body.length}`);\n\n    await Dataset.pushData({\n        url: request.url,\n        html: response.body,\n    });\n});\n\nconst crawler = new BasicCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run([\n    'https://apify.com',\n    'https://apify.com/store',\n    'https://apify.com/search',\n]);\n```\n\n----------------------------------------\n\nTITLE: CheerioCrawler URL Processing and HTML Parsing\nDESCRIPTION: Example showing how to use CheerioCrawler to process URLs from an external file, make HTTP requests, and parse HTML content using Cheerio library to extract page titles and h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler will automatically process each URL from the list.\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        // Extract data from the page using Cheerio.\n        const title = $('title').text();\n        const h1texts = [];\n        $('h1').each((_, el) => {\n            h1texts.push($(el).text());\n        });\n\n        // Log the extracted data.\n        log.info(`Title of ${request.url}: ${title}`);\n        log.info(`H1 texts for ${request.url}:`);\n        console.log(h1texts);\n    },\n});\n\n// Add URLs to the crawler's request queue.\nconst listOfUrls = await downloadListOfUrls({ url: 'https://example.com/list-of-urls.txt' });\nawait crawler.addRequests(listOfUrls);\n\n// Run the crawler.\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Requests Per Minute in CheerioCrawler\nDESCRIPTION: Sets up a CheerioCrawler with a limit of 60 requests per minute to avoid overwhelming the target website, while still allowing a high concurrent request limit of 50.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // This will limit our crawler to only 60 requests per minute\n    maxRequestsPerMinute: 60,\n    // But will still allow up to 50 concurrent requests\n    maxConcurrency: 50,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Filling and Submitting GitHub Search Forms with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to search for repositories on GitHub by filling in a search form with specific criteria like search term, repository owner, date range, and language. The crawler fills the form fields, submits it, extracts the search results, and saves them to a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, log }) {\n        log.info('Open GitHub...');\n        // Navigate to GitHub and wait for the page to fully load\n        await page.goto('https://github.com/search/advanced');\n\n        log.info('Filling in search form...');\n        // Fill form fields using Puppeteer's type and select methods\n        await page.type('#adv-search-qualifier-value', 'crawler OR scraper');\n        await page.type('#search_from', 'apify');\n        await page.type('#search_date', '>2015-01-01');\n        await page.select('#search_language', 'JavaScript');\n\n        log.info('Submitting form...');\n        // Submit the form and wait for the page to load\n        await Promise.all([\n            page.waitForNavigation(),\n            page.click('.btn-primary'),\n        ]);\n\n        log.info('Extracting data from results...');\n        // Extract data from the search results\n        const repos = await page.$$eval('.repo-list-item', (items) => {\n            return items.map((item) => {\n                const titleElement = item.querySelector('.f4');\n                const descriptionElement = item.querySelector('.mb-1');\n                const lastUpdatedElement = item.querySelector('relative-time');\n                const languageElement = item.querySelector('[itemprop=\"programmingLanguage\"]');\n                const starsElement = item.querySelector('#repo-stars-counter-star');\n\n                const title = titleElement ? titleElement.textContent.trim() : null;\n                const url = titleElement ? titleElement.querySelector('a').href : null;\n                const description = descriptionElement\n                    ? descriptionElement.textContent.trim()\n                    : null;\n                const lastUpdated = lastUpdatedElement\n                    ? lastUpdatedElement.getAttribute('datetime')\n                    : null;\n                const language = languageElement\n                    ? languageElement.textContent.trim()\n                    : null;\n                const stars = starsElement\n                    ? parseInt(starsElement.getAttribute('title').replace(/,/g, ''), 10)\n                    : null;\n\n                return {\n                    title,\n                    url,\n                    description,\n                    lastUpdated,\n                    language,\n                    stars,\n                };\n            });\n        });\n\n        log.info(`Found ${repos.length} repositories:`);\n        console.dir(repos);\n\n        // Save the results to the default dataset\n        await Dataset.pushData(repos);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Cheerio Crawler in Crawlee. It utilizes the Sitemap utility class to download and process URLs from a sitemap, then uses CheerioRequest to crawl those URLs and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from '@crawlee/cheerio';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerMinute: 100,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Cheerio code to extract data from a page\n    // and then store the results to the default dataset.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Choose a new link from the page but only from the same hostname\n        await enqueueLinks({\n            transformRequestFunction: (req) => {\n                // Ignore urls that go to the homepage\n                if (req.url === 'https://apify.com/') return false;\n\n                return req;\n            },\n        });\n\n        // Save results as JSON to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Download the sitemap and add the URLs to the queue\nconst sitemap = await Sitemap.load('https://apify.com/sitemap.xml');\nfor (const url of sitemap.urls) {\n    await crawler.addRequests([{ url }]);\n}\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler in Crawlee\nDESCRIPTION: This code snippet demonstrates a basic setup for a PlaywrightCrawler in Crawlee. It visits the start URL of an e-commerce site and logs the text content of category elements, serving as a sanity check before implementing full scraping logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${page.url()} is '${title}'`);\n\n        const categoryElements = await page.$$('.collection-block-item');\n        for (const element of categoryElements) {\n            const text = await element.textContent();\n            log.info('Category:', text);\n        }\n\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to scrape the Hacker News website. It initializes a RequestQueue, sets up the crawler with custom handling logic, and processes the scraped data. The crawler follows pagination links and stores the results in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestQueue to add more URLs to crawl.\n    requestQueue: await RequestQueue.open(),\n\n    // Here you can set options that are passed to the Playwright browser.\n    launchContext: {\n        launchOptions: {\n            headless: true,\n        },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 50,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Playwright scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Playwright within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link from each post on Hacker News.\n            $posts.forEach(($post) => {\n                scrapedData.push({\n                    title: $post.querySelector('.title a').innerText,\n                    rank: $post.querySelector('.rank').innerText,\n                    href: $post.querySelector('.title a').href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        const infos = await page.$$eval('.morelink', ($link) => {\n            return $link.length ? $link[0].href : null;\n        });\n        if (infos) {\n            await enqueueLinks({\n                urls: [infos],\n                label: 'LIST',\n            });\n        }\n    },\n\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times.`);\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(['https://news.ycombinator.com/']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright\nDESCRIPTION: This snippet shows how to use Playwright to extract a product title from a webpage. It uses a combined CSS selector to locate the h1 element within a div that has the 'product-meta' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright\nDESCRIPTION: This comprehensive snippet combines all the individual extraction techniques to collect complete product information including URL, manufacturer, title, SKU, price, and stock availability in a single code block.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Using Camoufox with Playwright for Bypassing Cloudflare Challenges\nDESCRIPTION: Example demonstrating how to use Camoufox, a stealthy Firefox build, with Playwright to bypass Cloudflare challenges. It utilizes a handleCloudflareChallenge helper to mimic user actions required to pass the protection.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/avoid_blocking.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, handleCloudflareChallenge, type PlaywrightCrawlerOptions } from 'crawlee';\nimport { firefox } from 'playwright';\nimport { getFirefoxExecutablePath } from 'camoufox-js';\n\n// By default the process searches for Camoufox in the following locations:\n//  - MacOS - /Applications/Camoufox.app/Contents/MacOS/firefox\n//  - Linux - /opt/camoufox/firefox\n//  - Unsupported platform\n// Otherwise you can specify the path to the executable via:\n//  CAMOUFOX_EXECUTABLE_PATH env var\n//  or\nconst CAMOUFOX_PATH = getFirefoxExecutablePath();\n\nconst crawler = new PlaywrightCrawler({\n    browserType: firefox,\n    // Specify the path to the Firefox executable\n    launchContext: {\n        launchOptions: {\n            executablePath: CAMOUFOX_PATH,\n        },\n    },\n    // Register the Cloudflare Hook\n    preNavigationHooks: [\n        // Here we wait for the Cloudflare challenge to resolve\n        async ({ request, browserController, session, log }) => {\n            const { context } = browserController;\n            const { page } = context;\n\n            await handleCloudflareChallenge({ page, request, session, log });\n        },\n    ],\n    async requestHandler({ page, request, log }) {\n        // Now we have passed the challenge and can continue with our processing\n        const title = await page.title();\n        log.info(`Title of ${request.url} is ${title}`);\n    },\n} as PlaywrightCrawlerOptions);\n\nawait crawler.run(['https://some-site-with-cf.com']);\n```\n\n----------------------------------------\n\nTITLE: Checking Stock Availability using Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to check if a product is in stock by looking for a specific element and counting its occurrences.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox browser in TypeScript\nDESCRIPTION: This code demonstrates how to configure and use PlaywrightCrawler with a headless Firefox browser. It sets up the crawler to use Firefox, processes pages, and extracts data from websites.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Firefox browser\n// and injects scripts from Crawlee.\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserPoolOptions: {\n        browserType: firefox,\n    },\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            title,\n            url: request.url,\n            succeeded: true,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-rendered Content with PuppeteerCrawler\nDESCRIPTION: This snippet uses PuppeteerCrawler to scrape JavaScript-rendered content, explicitly waiting for elements to appear before extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor card to render\n        await page.waitForSelector('.ActorStoreItem');\n        // Extract text content of an actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This example illustrates how to integrate SessionPool with PuppeteerCrawler in Crawlee. It shows proxy configuration and session management in a Puppeteer-based crawling environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ page, request, session }) {\n        await page.goto(request.url);\n        const title = await page.title();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // We can save cookies from the page to the session\n        const cookies = await page.cookies();\n        session.setCookies(cookies, request.url);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: Example showing how to use JSDOMCrawler to interact with a React calculator app. The script performs button clicks (1 + 1 =) and extracts the calculation result using jsdom DOM implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerRunScriptSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This example shows how to integrate SessionPool with HttpCrawler in Crawlee. It includes proxy configuration and demonstrates how to handle session-specific data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ request, response, session }) {\n        // Process the response...\n\n        // Optionally, we can add custom data to the session\n        session.userData.customField = 'someValue';\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to download and crawl URLs from a sitemap using Cheerio Crawler. It utilizes the downloadListOfUrls utility from @crawlee/utils to extract URLs from a sitemap and processes them with CheerioRouter.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioRouter, createCheerioRouter } from '@crawlee/cheerio';\nimport { downloadListOfUrls } from '@crawlee/utils';\nimport { CheerioCrawler, sleep } from 'crawlee';\n\n// Create the crawler and provide the required configuration\nconst crawler = new CheerioCrawler({\n    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, enqueueLinks, log, $ }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add any links from the page to the queue\n        await enqueueLinks();\n\n        // Wait for a random amount of time to avoid overloading the site\n        await sleep(Math.random() * 2000);\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.info(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Get the list of URLs from the sitemap\nconst urls = await downloadListOfUrls({ url: 'https://apify.com/sitemap.xml' });\n\n// Add the URLs to the crawler queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Complete Price Tracking Actor Implementation in Python\nDESCRIPTION: Full implementation of a price tracking actor using Crawlee and Apify SDK. It crawls a product page, extracts the name and price, and sends an email notification if the price is below a threshold. The code defines the main function, sets up a crawler, and handles request routing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\nfrom crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n\n\nasync def main() -> None:\n\n    # Enter the context of the Actor.\n    async with Actor:\n        # Create a crawler.\n        crawler = BeautifulSoupCrawler(\n            # Limit the crawl to max requests. Remove or increase it for crawling all links.\n            max_requests_per_crawl=50,\n        )\n\n        # Define a request handler, which will be called for every request.\n        @crawler.router.default_handler\n        async def request_handler(context: BeautifulSoupCrawlingContext) -> None:\n            url = context.request.url\n            Actor.log.info(f'Scraping {url}...')\n            \n            # Select the product name and price elements.\n            product_name_element = context.soup.find('div', class_='productname')\n            product_price_element = context.soup.find('span', id='product-price-395001')\n\n            # Extract the desired data.\n            data = {\n                'url': context.request.url,\n                'product_name': product_name_element.text.strip() if product_name_element else None,\n                'price': float(product_price_element['data-price-amount']) if product_price_element else None,\n            }\n            \n            price_threshold = 80\n            \n            if data['price'] < price_threshold:\n                actor_run = await Actor.start(\n                    actor_id=\"apify/send-mail\",\n                    run_input={\n                        \"to\": \"your_email@gmail.com\",\n                        \"subject\": \"Python Price Alert\",\n                        \"text\": f\"The price of '{data['product_name']}' has dropped below ${price_threshold} and is now ${data['price']}.\\n\\nCheck it out here: {data['url']}\",\n                    },\n                )\n                Actor.log.info(f\"Email sent with run ID: {actor_run.id}\")\n\n            # Store the extracted data to the default dataset.\n            await context.push_data(data)\n\n        # Run the crawler with the starting requests.\n        await crawler.run(['https://www.centralcomputer.com/raspberry-pi-5-8gb-ram-board.html'])\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler\nDESCRIPTION: Demonstrates how to use CheerioCrawler to crawl specific links on a website using glob patterns. The code shows how to configure the crawler, handle requests, and filter links using the globs property in enqueueLinks() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler will only fetch and process first 10 links\n    maxRequestsPerCrawl: 10,\n\n    // Let's try to handle the errors\n    async requestHandler({ $, request, enqueueLinks }) {\n        // Extract the page title\n        const title = $('title').text();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Add all links from page to RequestQueue\n        // but only if they match the provided glob pattern\n        await enqueueLinks({\n            // globs property enables URLs filtering using glob pattern matching\n            // for example 'https://www.example.com/**' - more info in README\n            globs: ['http?(s)://www.imdb.com/title/*'],\n        });\n    },\n});\n\n// Run the crawler with initial URL\nawait crawler.run(['https://www.imdb.com/chart/top']);\n```\n\n----------------------------------------\n\nTITLE: Setting up and using Dataset class in Crawlee\nDESCRIPTION: Example showing how to save website data to the default dataset. The dataset is automatically created if it doesn't exist. Data can also be saved to custom datasets using Dataset.open().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{AddDataToDatasetSource}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer Using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a single web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport puppeteer from 'puppeteer';\n\nawait Actor.init();\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nconst screenshot = await page.screenshot();\nconst key = 'screenshot-example.png';\nawait Actor.setValue(key, screenshot, { contentType: 'image/png' });\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom HTTP Client with Fetch API in TypeScript for Crawlee\nDESCRIPTION: A skeleton implementation of the BaseHttpClient interface using the standard fetch API. This provides the structure required for a custom HTTP client that can be used with Crawlee's BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BaseHttpClient, HttpClientOptions, HttpClientResponse, type HttpMethod } from '@crawlee/core';\n\nexport class FetchHttpClient extends BaseHttpClient {\n    async requestHandler<ResponseType>(\n        method: HttpMethod,\n        url: string,\n        options: HttpClientOptions,\n    ): Promise<HttpClientResponse<ResponseType>> {\n        const headers = options.headers ?? {};\n        const timeout = options.timeoutSecs ? options.timeoutSecs * 1000 : undefined;\n\n        // this part is totally up to you, this is just to show\n        // that you need to handle proxyUrl in the options\n        if (options.proxyUrl) {\n            const proxyUrl = new URL(options.proxyUrl);\n            process.env.HTTP_PROXY = options.proxyUrl;\n            process.env.HTTPS_PROXY = options.proxyUrl;\n\n            if (proxyUrl.username && proxyUrl.password) {\n                const auth = Buffer.from(`${proxyUrl.username}:${proxyUrl.password}`).toString('base64');\n                headers['Proxy-Authorization'] = `Basic ${auth}`;\n            }\n        }\n\n        let body: BodyInit | null = null;\n\n        if (options.payload) {\n            if (typeof options.payload === 'string') {\n                body = options.payload;\n            } else {\n                body = JSON.stringify(options.payload);\n            }\n        }\n\n        let controller: AbortController | undefined;\n\n        if (timeout) {\n            controller = new AbortController();\n            setTimeout(() => controller.abort(), timeout);\n        }\n\n        const response = await fetch(url, {\n            method,\n            headers,\n            body,\n            signal: controller?.signal,\n        });\n\n        const contentType = response.headers.get('content-type');\n\n        let data: ResponseType;\n\n        if (contentType?.includes('application/json')) {\n            data = await response.json() as ResponseType;\n        } else {\n            data = await response.text() as unknown as ResponseType;\n        }\n\n        return {\n            statusCode: response.status,\n            headers: Object.fromEntries(response.headers.entries()),\n            body: data,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers in Crawlee Router\nDESCRIPTION: This code demonstrates how to create a router with multiple handlers for different page types in a web scraper. It includes handlers for product detail pages, category pages, and a default handler for the start page, showing proper data extraction and link enqueueing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing User Input in a Crawlee Actor\nDESCRIPTION: This TypeScript code shows how to accept and log user input in a Crawlee-based actor. It uses Actor.getInput() to retrieve the input data and then logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of your actor\nconst input = await Actor.getInput();\nconsole.log('Input:', input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links from a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to find all anchor elements with href attributes on a page and extract those URLs into an array using Cheerio's selector, map, and get methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler in Crawlee\nDESCRIPTION: This code snippet demonstrates how to use Playwright Crawler to crawl a sitemap. It employs the Sitemap class for processing sitemap URLs and PlaywrightScraper for scraping each page. It's intended to run on the Apify Platform using the apify/actor-node-playwright-chrome image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightScraper, Dataset } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new PlaywrightScraper({\n    maxRequestsPerCrawl: 20,\n});\n\nconst sitemap = new Sitemap({\n    sitemapUrls: ['https://crawlee.dev/sitemap.xml'],\n});\n\nscraper.router.addDefaultHandler(async ({ page, request, enqueueLinks }) => {\n    const title = await page.title();\n    await Dataset.pushData({ url: request.url, title });\n    await enqueueLinks();\n});\n\nawait scraper.run(sitemap.urls());\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: Demonstrates how to create a custom Configuration object with a 10-second state persistence interval and pass it to a CheerioCrawler. The example shows a crawler that processes two requests with deliberate delays and uses the custom configuration to ensure statistics are persisted before the process exits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler from Crawlee. It includes handling of pagination and extraction of product details from a demo e-commerce site.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({ title, url: request.url });\n\n        // Extract links to crawler\n        // if on homepage enqueue links to paginated pages and categories\n        if (request.label === 'START') {\n            await enqueueLinks({\n                globs: ['https://demo-webstore.apify.org/?page=*'],\n                label: 'LIST',\n            });\n            await enqueueLinks({\n                selector: '.product-list > div > a',\n                label: 'DETAIL',\n            });\n        }\n\n        // If on a paginated page, enqueue product links\n        if (request.label === 'LIST') {\n            await enqueueLinks({\n                selector: '.product-list > div > a',\n                label: 'DETAIL',\n            });\n        }\n\n        // Extract data from a product page\n        if (request.label === 'DETAIL') {\n            const urlPart = request.url.split('/').slice(-1); // ['ssd-sandisk-plus-480gb']\n            const manufacturer = urlPart[0].split('-').slice(0, 2).join(' ');\n            const description = await page.$eval('.description', (el) => el.textContent);\n            const price = await page.$eval('.price', (el) => el.textContent);\n\n            // Save data to dataset\n            await Dataset.pushData({\n                manufacturer,\n                description,\n                price,\n            });\n        }\n    },\n    // Function to handle errors\n    failedRequestHandler({ request, error, log }) {\n        log.error(`Request ${request.url} failed with error: ${error.message}`);\n    },\n});\n\n// Run the crawler with initial url\nawait crawler.run(['https://demo-webstore.apify.org/']);\n```\n\n----------------------------------------\n\nTITLE: Initialize Cheerio Crawler for Link Crawling\nDESCRIPTION: Demonstrates how to use CheerioCrawler to crawl all links on a website. Uses enqueueLinks() to automatically add new discovered links to the RequestQueue for recursive crawling. Handles startup URLs and processes each link's content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler\nDESCRIPTION: Shows how to crawl a sitemap using Puppeteer Crawler with browser automation. Requires apify/actor-node-puppeteer-chrome image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for proxy usage and session creation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new BasicCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Set the maximum number of sessions to 100\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, sendRequest }) {\n        const { body } = await sendRequest({\n            url: 'https://example.com',\n            // Use the proxy URL and other options from the session\n            proxyUrl: session.proxyUrl,\n            ...(session.userData.cookieJar && { cookieJar: session.userData.cookieJar }),\n        });\n        // Process the response...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a PuppeteerCrawler Instance for Web Scraping in JavaScript\nDESCRIPTION: This example shows how to initialize and run a PuppeteerCrawler instance. It creates a crawler that processes web pages, extracts the page title and URL, and pushes the data to a Dataset. The crawler includes handlers for both successful requests and failed requests, and is run against specific URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/puppeteer-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // This function is called to extract data from a single web page\n        // 'page' is an instance of Puppeteer.Page with page.goto(request.url) already called\n        // 'request' is an instance of Request class with information about the page to load\n        await Dataset.pushData({\n            title: await page.title(),\n            url: request.url,\n            succeeded: true,\n        })\n    },\n    async failedRequestHandler({ request }) {\n        // This function is called when the crawling of a request failed too many times\n        await Dataset.pushData({\n            url: request.url,\n            succeeded: false,\n            errors: request.errorMessages,\n        })\n    },\n});\n\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file. It loads each URL using an HTTP request, parses the HTML with Cheerio, and extracts the page title and all h1 tags. The crawler is configured with various options including max requests, session pool, and request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Add URLs to a list\nconst urlList = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n];\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's limit our crawls to make our example brief.\n    maxRequestsPerCrawl: 20,\n\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        // Extract data from the page using Cheerio.\n        const title = $('title').text();\n        const h1texts = $('h1')\n            .map((_, el) => $(el).text())\n            .get();\n\n        // Log some information to the console.\n        log.info(`Title of ${request.url}: ${title}`);\n        log.info(`${h1texts.length} h1 tags found`);\n\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1texts,\n        });\n\n        // Crawl all links from the page.\n        await enqueueLinks();\n    },\n\n    // Add URLs to the queue, but only a specific amount\n    maxRequestsPerCrawl: 10,\n\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URLs to the queue and start the crawl.\nawait crawler.run(urlList);\n\n// Add URLs from text file\nconst content = await readFile('./path/to/urls.txt', { encoding: 'utf-8' });\nconst urls = content.split('\\n').map((line) => line.trim()).filter((line) => line.length > 0);\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Working with Key-Value Stores in Crawlee\nDESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, opening named stores, and handling different data types. Shows how JavaScript objects are automatically converted to JSON when stored.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Creating a CheerioCrawler with RequestQueue in Crawlee\nDESCRIPTION: Shows how to set up a CheerioCrawler with a RequestQueue and a basic requestHandler. The crawler visits the specified URL and extracts the page title using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Category and Product Pages with Crawlee\nDESCRIPTION: This snippet demonstrates a more complex crawling scenario using PlaywrightCrawler. It handles different types of pages (start page, category pages, and detail pages) based on request labels, enqueuing both product and pagination links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Crawlee Playwright Crawler for Store Collection Pages\nDESCRIPTION: A basic implementation of a PlaywrightCrawler that visits the store collections page and logs the text content of all category elements. This serves as a sanity check before implementing the full scraping logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        if (request.url.includes('/collections/all')) {\n            // We are now on a collection page.\n            log.info('I am on a collection page!');\n            const categories = await page.$$('.collection-block-item');\n            for (const category of categories) {\n                const categoryText = await category.textContent();\n                log.info(`CATEGORY TEXT: ${categoryText}`);\n            }\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n```\n\n----------------------------------------\n\nTITLE: Enqueuing All Links in TypeScript\nDESCRIPTION: This snippet shows how to use the enqueueLinks function to follow every single link, regardless of its domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Web Pages with PlaywrightCrawler\nDESCRIPTION: Example code showing how to use PlaywrightCrawler to perform a recursive crawl of the Crawlee website. PlaywrightCrawler uses a headless browser to crawl, controlled by the Playwright library.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee logic for extracting data from a page.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/**'],\n            exclude: ['.pdf', '?*']\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests in PlaywrightCrawler\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, identify image URLs, and fetch them directly without navigation. It utilizes the Request#skipNavigation option and the sendRequest method to efficiently handle image resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\nimport { createPlaywrightRouter } from 'crawlee';\n\nconst router = createPlaywrightRouter();\n\nrouter.addDefaultHandler(async ({ enqueueLinks, log, request }) => {\n    log.info(`enqueueing new URLs from ${request.url}`);\n    await enqueueLinks();\n\n    // Find all image elements on the page\n    const imageUrls = await request.page.evaluate(() => {\n        return Array.from(document.querySelectorAll('img'))\n            .map((img) => img.src)\n            .filter((src) => src.startsWith('http'));\n    });\n\n    // Enqueue each image URL with skipNavigation set to true\n    for (const url of imageUrls) {\n        await request.crawler.addRequests([{\n            url,\n            userData: { label: 'IMAGE' },\n            skipNavigation: true, // This is the important part!\n        }]);\n    }\n});\n\nrouter.addHandler('IMAGE', async ({ sendRequest, log, request }) => {\n    log.info(`Fetching image from ${request.url}`);\n    const imageBuffer = await sendRequest({ responseType: 'buffer' });\n\n    const keyValueStore = await KeyValueStore.open();\n    await keyValueStore.setValue(request.url, imageBuffer, { contentType: 'image/png' });\n});\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and continues until no more desired links are available. The results are stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';\n\n// createPuppeteerRouter() is only a helper to get better\n// intellisense when creating the routes object.\n// You can use a plain object too.\nconst router = createPuppeteerRouter();\n\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    const title = await page.title();\n    log.info(`${title}`, { url: request.loadedUrl });\n\n    await Dataset.pushData({\n        title,\n        url: request.loadedUrl,\n        html: await page.content(),\n    });\n\n    await enqueueLinks({\n        globs: ['https://news.ycombinator.com/*'],\n        label: 'detail',\n    });\n});\n\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    requestHandler: router,\n\n    // Uncomment this option to see the browser window.\n    // headless: false,\n\n    // Comment this option to scrape the full website.\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Session Management with PlaywrightCrawler and Proxies\nDESCRIPTION: Shows how to use session management with proxy configuration in PlaywrightCrawler. This helps maintain consistent browser fingerprints by pairing session IDs with specific proxy URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true, // This is the default\n    async requestHandler({ session, request, page }) {\n        console.log(`Using session ${session.id}`);\n        console.log(`Fetched ${request.url} with title: ${await page.title()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with Selectors in Crawlee\nDESCRIPTION: This snippet demonstrates how to crawl category pages by using the selector parameter of enqueueLinks() to target only specific links. It also introduces the label parameter to identify request types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Session Management with CheerioCrawler\nDESCRIPTION: Shows how to implement proxy session management in CheerioCrawler using SessionPool. This helps avoid blocks by maintaining consistent identity across requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    sessionPoolOptions: {\n        persistStateKeyValueStoreId: 'my-session-pool',\n        persistStateKey: 'cheerio-crawler-sessions',\n    },\n    async requestHandler({ request, $, session, log }) {\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Crawler in Crawlee with TypeScript\nDESCRIPTION: Demonstrates using BasicCrawler to download web pages with sendRequest and store their HTML and URL in the default dataset. This code shows the most basic usage of Crawlee's crawling capabilities, storing results as JSON files in the './storage/datasets/default' directory when run locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create a crawler that will use the sendRequest utility to download pages\nconst crawler = new BasicCrawler({\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ request, sendRequest, log }) {\n        const { url } = request;\n        log.info('Processing:', { url });\n        \n        // Use sendRequest utility to download the page\n        // It uses the got-scraping npm package internally\n        const response = await sendRequest();\n        \n        // Store the HTML content and URL to the default dataset\n        await Dataset.pushData({\n            url,\n            html: response.body,\n        });\n        \n        // Wait for a random amount of milliseconds to avoid overloading the server\n        await new Promise((resolve) => setTimeout(resolve, Math.random() * 1000));\n    },\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/docs/quick-start' },\n    { url: 'https://crawlee.dev/docs/guides/apify-platform' },\n    { url: 'https://crawlee.dev/docs/examples/basic-crawler' },\n]);\n\n// Run the crawler\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl all links on a website. It initializes the crawler, sets up a request queue, and processes each page to extract and enqueue new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of domain. It uses the 'All' enqueue strategy to process any URLs encountered.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // Alternatively, you can use the string 'all'\n            // strategy: 'all',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Implementation\nDESCRIPTION: Guide for implementing SessionPool independently without a crawler for manual session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{StandaloneSource}\n```\n\n----------------------------------------\n\nTITLE: Session Management with HttpCrawler\nDESCRIPTION: Demonstrates how to use proxy session management with HttpCrawler. This ensures consistent proxy usage for specific sessions, which helps to maintain the appearance of a real user.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, json, session }) {\n        // The session ensures that the same proxy is used for certain requests\n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright\nDESCRIPTION: A complete implementation that combines all the extraction techniques to collect product data including manufacturer, title, SKU, price, and stock availability from an e-commerce product page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to skip navigation for image requests using PlaywrightCrawler. It finds image links on a page, adds them to the request queue with the skipNavigation flag, and then downloads the images directly using the context.sendRequest method rather than navigating to them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore, log } from 'crawlee';\nimport { writeFile } from 'node:fs/promises';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, crawler, sendRequest }) {\n        // Handling a standard page request (not skipped)\n        if (!request.skipNavigation) {\n            log.info(`Handling standard page request for ${request.url}`);\n            \n            // Find all image elements on the page\n            const imgElements = await page.$$('img');\n            \n            // Process each image\n            for (const img of imgElements) {\n                const src = await img.getAttribute('src');\n                if (!src) continue;\n                \n                // Extract absolute URL for the image\n                const imgUrl = new URL(src, request.loadedUrl).toString();\n                \n                // Add the image to the request queue with skipNavigation set to true\n                await crawler.addRequests([{\n                    url: imgUrl,\n                    skipNavigation: true, // This will skip the browser navigation\n                    userData: { label: 'image' }\n                }]);\n            }\n            return;\n        }\n\n        // Handling image request (with skipNavigation)\n        if (request.userData.label === 'image') {\n            log.info(`Downloading image from ${request.url}`);\n            \n            // Use sendRequest to fetch the image without browser navigation\n            const imageResponse = await sendRequest({ url: request.url });\n            const imageBuffer = await imageResponse.body;\n            \n            // Save the image using Key-Value store\n            const filename = request.url.split('/').pop() || 'image.jpg';\n            const kvStore = await KeyValueStore.open();\n            await kvStore.setValue(filename, imageBuffer, { contentType: imageResponse.headers['content-type'] });\n            \n            log.info(`Image saved as ${filename}`);\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Setting up PlaywrightCrawler with Router in main.mjs\nDESCRIPTION: Main entry point file that initializes a PlaywrightCrawler with a router for request handling. This demonstrates the separation of crawler setup from routing logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Building a CheerioCrawler with RequestQueue in Crawlee\nDESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract and log the page title. It demonstrates the basic structure of a Crawlee crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling with CheerioCrawler\nDESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using CheerioCrawler. It demonstrates how to set up the crawler, define the request handler, and start the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit to 20 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Downloading Files with Crawlee's FileDownload Crawler in TypeScript\nDESCRIPTION: This script demonstrates how to download various file types (images, PDFs) using Crawlee's FileDownload crawler class. It makes HTTP requests to download files and saves them to the default key-value store, which in local configuration is stored in './storage/key_value_stores/default'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/file_download.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload } from 'crawlee';\n\nawait FileDownload.download({\n    // We will process the following URLs\n    urls: [\n        'https://cdn.jsdelivr.net/gh/apify/apify-js/website/public/img/apify.png',\n        'https://raw.githubusercontent.com/apify/apify-js/master/README.md',\n        'https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf',\n    ],\n\n    // Handle the download. This function is called for each downloaded page.\n    // It downloads images (png) and pdfs from S3 to the default key-value store.\n    // JSON and other text files are fully loaded to memory, parsed to text\n    // and saved.\n    downloadFunction: async ({ url, responseBody, response }) => {\n        // We use URL's pathname as a key in key-value store\n        // We would get, for example: '/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf'\n        // We strip the first slash somehow\n        let { pathname } = new URL(url);\n        pathname = pathname.startsWith('/') ? pathname.substring(1) : pathname;\n\n        // Normalize the pathname by replacing all slashes with underscores\n        // to make it safe for saving to the key-value store:\n        // For example: 'WAI_ER_tests_xhtml_testfiles_resources_pdf_dummy.pdf'\n        const key = pathname.replace(/\\//g, '_');\n\n        // For binary files (PDF, images), the responseBody is already\n        // the binary Buffer.\n        // For other files (text, JSON), the responseBody is a string.\n        console.log(`${url} -> ${key} (${response.headers.get('content-type')})`);\n\n        // Now save the response body to the key-value store\n        await FileDownload.defaultKeyValueStore.setValue(key, responseBody);\n    },\n});\n\nconsole.log('File download completed.');\n\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use the sendRequest function with BasicCrawler to make HTTP requests. The example shows the basic implementation where the crawler retrieves and logs the response body.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Building a Multi-Stage Docker Image for Crawlee Actors\nDESCRIPTION: This Dockerfile creates a multi-stage build for Crawlee actors. It first builds the application in a builder stage, then creates a minimal production image with only the necessary files and dependencies. This approach reduces the final image size and improves security by not including development dependencies in production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use JSDOMCrawler to interact with a React calculator app. It clicks buttons to perform a simple addition and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const { document } = window;\n        log.info(`Processing ${document.title}`);\n\n        // Click the buttons 1 + 1 =\n        document.querySelector('button[name=\"1\"]')?.click();\n        document.querySelector('button[name=\"+\"]')?.click();\n        document.querySelector('button[name=\"1\"]')?.click();\n        document.querySelector('button[name=\"=\"]')?.click();\n\n        // Extract the result\n        const result = document.querySelector('.component-display')?.textContent;\n\n        // Save the result to the dataset\n        await Dataset.pushData({ result });\n    },\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in Crawlee\nDESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl multiple specified URLs. It sets up the crawler, processes each page to extract the title, and saves the results to the default dataset. It requires the 'apify/actor-node-playwright-chrome' image for the Dockerfile when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with PlaywrightCrawler\nDESCRIPTION: Demonstrates how to configure proxies for PlaywrightCrawler to enable proxy usage for browser-based scraping with Playwright. Shows basic setup and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, log }) {\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Playwright Crawler in TypeScript\nDESCRIPTION: This code uses PlaywrightScraper to crawl all links on a website with a headless browser. It navigates to each page, extracts the title, and adds all discovered links to the request queue with enqueueLinks(). The crawler is limited to 100 requests maximum.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightScraper } from 'crawlee';\n\n// Create an instance of the PlaywrightScraper class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightScraper({\n    // Let's limit our crawls to make the example quick to run.\n    maxRequestsPerCrawl: 100,\n});\n\n// Crawlers come with various utilities, such as request queue support.\nawait crawler.run([\n    // Tell the crawler to start with this URL\n    'https://crawlee.dev',\n], {\n    // This function will be called for each URL\n    async requestHandler({ request, enqueueLinks, log, page }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Extract data from the page\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Add all links from the page to the crawling queue\n        await enqueueLinks();\n    },\n});\n\nlog.info('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler for Hacker News Scraping\nDESCRIPTION: Demonstrates setting up a PuppeteerCrawler to recursively scrape news articles from Hacker News. The crawler processes pages, extracts article data, handles pagination, and stores results in a dataset. Uses RequestQueue for managing URLs and implements error handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://news.ycombinator.com' });\n\n// Create the crawler and add the required logic\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page\n        const links = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // Process each post\n            $posts.forEach(($post) => {\n                const id = $post.id;\n                const rank = $post.querySelector('.rank')?.textContent;\n                const title = $post.querySelector('.titleline > a')?.textContent;\n                const url = $post.querySelector('.titleline > a')?.href;\n\n                scrapedData.push({\n                    id,\n                    rank,\n                    title,\n                    url,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Save the data to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            links,\n        });\n\n        // Add more URLs to the queue\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n});\n\n// Start the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Custom Configuration with CheerioCrawler in JavaScript\nDESCRIPTION: This example demonstrates how to create a custom configuration with a 10-second state persistence interval and pass it to a CheerioCrawler. The crawler processes two requests with deliberate delays, showing how the state persistence works before the process exits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser in TypeScript\nDESCRIPTION: This code demonstrates how to configure PlaywrightCrawler to use headless Firefox browser instead of the default Chromium. It sets up a crawler that visits a webpage and captures a screenshot using Firefox's browser engine.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { firefox } from 'playwright';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Firefox browser\n// and allows you to control it via a simple API.\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserType: firefox,\n    // Let's limit our crawls to make the run shorter\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ page, request, enqueueLinks }) {\n        log.info(`Processing: ${request.url}`);\n\n        // Wait for the page to fully render.\n        // This might not be as necessary for all pages.\n        await page.waitForLoadState('networkidle');\n\n        // Capture the screenshot with Firefox\n        const title = await page.title();\n        await page.screenshot({\n            path: `${title}.png`,\n            fullPage: true,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Initializing ProxyConfiguration with Static Proxy URLs in JavaScript\nDESCRIPTION: This code demonstrates how to set up a basic proxy configuration with a list of static proxy URLs. The ProxyConfiguration class will rotate through these URLs when newUrl() is called.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code snippet shows how to use PlaywrightCrawler to crawl a website, identify image URLs, and fetch them directly without navigation. It utilizes the Request#skipNavigation option and the sendRequest method to efficiently handle CDN-delivered images.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { Actor } from 'apify';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, crawler, request }) {\n        // Extract data from the page\n        const title = await page.title();\n        await Dataset.pushData({ title, url: request.url });\n\n        // Find all image URLs on the page\n        const imageUrls = await page.evaluate(() => {\n            return Array.from(document.querySelectorAll('img'))\n                .map((img) => img.src)\n                .filter((src) => src.startsWith('http'));\n        });\n\n        // Enqueue the image URLs with skipNavigation set to true\n        for (const url of imageUrls) {\n            await crawler.addRequests([{\n                url,\n                userData: { label: 'IMAGE' },\n                skipNavigation: true, // <-- This is the important part\n            }]);\n        }\n    },\n\n    async requestHandler({ sendRequest, request }) {\n        // This will only be called for image requests\n        if (request.userData.label === 'IMAGE') {\n            const imageBuffer = await sendRequest();\n            const kvStore = await Actor.openKeyValueStore('my-images');\n            await kvStore.setValue(request.url, imageBuffer, { contentType: 'image/png' });\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler\nDESCRIPTION: Initial crawler setup that visits the start URL and prints the text content of all categories on the page to verify the scraping approach works as expected.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Recursive Web Scraping with PlaywrightCrawler for Hacker News\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler with RequestQueue to recursively scrape the Hacker News website. It extracts post titles, authors, and scores, then follows pagination links. Results are stored to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API.\n        const results = await page.$$eval('.athing', ($posts) => {\n            return $posts.map($post => {\n                const id = $post.getAttribute('id');\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const author = $post.nextElementSibling.querySelector('.hnuser')?.innerText;\n\n                const points = $post.nextElementSibling.querySelector('.score')?.innerText;\n\n                return {\n                    id,\n                    title,\n                    rank,\n                    author,\n                    points,\n                };\n            });\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(results);\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Start the crawler and wait for it to finish.\nawait crawler.run(['https://news.ycombinator.com/']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: Demonstrates how to use SessionPool with BasicCrawler to manage sessions and proxy rotations. It includes configuration for the session pool and crawler, and shows how to handle requests and retries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new BasicCrawler({\n    // The `useSessionPool` enables the `sessionPool` configuration option\n    useSessionPool: true,\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ session, sendRequest }) {\n        // Use session in the sendRequest function\n        const { statusCode, body } = await sendRequest({\n            url: 'https://example.com',\n            proxyUrl: session.proxyUrl,\n        });\n\n        if (statusCode !== 200) {\n            throw new Error(`Request failed with status code ${statusCode}`);\n        }\n    },\n    async failedRequestHandler({ request, session }) {\n        // Invalidate session on error\n        await session.markBad();\n        // Add the failed request back to the queue\n        await crawler.addRequests([request]);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This example shows how to use SessionPool with PlaywrightCrawler for browser automation. It demonstrates configuring proxies, managing sessions, and handling page interactions while maintaining session integrity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // The default sessionPoolOptions are described in BasicCrawler example\n    // and it will work the same out of the box with PlaywrightCrawler\n    async requestHandler({ page, request, session }) {\n        // Process the data on the page using Playwright\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // With browser crawlers we can check if the page was actually blocked\n        // and we can retire the session and throw an error to retry the request\n        const isBlocked = await page.evaluate(() => {\n            const title = document.title.toLowerCase();\n            return title.includes('access denied') || title.includes('forbidden');\n        });\n\n        if (isBlocked) {\n            session!.retire();\n            throw new Error('We got blocked, lets use another session for these requests');\n        }\n\n        // Else we can mark the session good\n        session!.markGood();\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: ['http://user:password@proxy.com:8000']\n    }),\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Recursive Web Scraping with PlaywrightCrawler on Hacker News\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them and continues until no more desired links are available. Results are stored to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // The crawler will automatically process the requests\n    // added to the request queue and extend them with\n    // pre-processing and post-processing hooks.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // A function to be evaluated by Playwright within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link to the post\n            $posts.forEach(($post) => {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const href = $post.querySelector('.title a').href;\n\n                scrapedData.push({\n                    title,\n                    rank: Number(rank.replace('.', '')),\n                    href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    preNavigationHooks: [\n        // Pre-navigation hooks enable us to execute custom code before navigating to the URL\n        async ({ request, page, crawler }) => {\n            // ....\n        },\n    ],\n    postNavigationHooks: [\n        // Post-navigation hooks enable us to execute custom code after navigating to the URL\n        // and optionally waiting for various page events to happen\n        async ({ request, page, crawler }) => {\n            // ....\n        },\n    ],\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Plain JavaScript DOM Manipulation\nDESCRIPTION: This snippet compares how to extract text content from a title element and collect all href links using both plain JavaScript in the browser and Cheerio in Node.js. It demonstrates the syntax differences between the two approaches.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler to scrape the Hacker News website. It sets up a RequestQueue, defines a request handler to process pages, and extracts data from each page. The crawler follows pagination links to scrape multiple pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Use a CSS selector to extract the links from the page.\n        await enqueueLinks({\n            globs: ['https://news.ycombinator.com/*'],\n            label: 'detail',\n        });\n\n        // Extract data from the page using Playwright API.\n        const uniqueIdentifier = request.url.split('id=')[1];\n\n        const data = {\n            title: await page.locator('.title a').first().textContent(),\n            rank: await page.locator('.rank').textContent(),\n            href: await page.locator('.title a').first().getAttribute('href'),\n            score: await page.locator('.score').textContent(),\n            author: await page.locator('.hnuser').textContent(),\n            numberOfComments: await page.locator('.subline a:last-child').textContent(),\n        };\n\n        // Save the data to dataset.\n        await Dataset.pushData({\n            ...data,\n            url: request.url,\n            uniqueIdentifier,\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing CheerioCrawler with RequestQueue\nDESCRIPTION: Demonstrates a complete implementation of a CheerioCrawler that processes a webpage and extracts its title using RequestQueue for URL management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Cheerio Crawler in Crawlee. It uses the Sitemap utility to fetch and parse the sitemap from cnn.com, then processes each URL with the Cheerio Crawler to extract and log headlines from articles.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from '@crawlee/cheerio';\nimport { Sitemap } from '@crawlee/utils';\n\n// We'll use CNN as our sitemap provider\nconst sitemapUrl = 'https://www.cnn.com/sitemaps/cnn/index.xml';\n\n// Create and initialize the crawler\nconst crawler = new CheerioCrawler({\n    // We want to keep the crawler going even if there's an error in our code\n    ignoreSslErrors: true,\n    requestHandler: async ({ body, request, enqueueLinks, log }) => {\n        // If this is a regular document, we have to parse it to find all the links\n        log.info(`Processing ${request.url}...`);\n\n        // If we're crawling an article, extract some data from it.\n        if (/\\/[\\w-]+\\/\\d{4}\\/\\d{2}\\/\\d{2}\\/.+/i.test(request.url)) {\n            const title = body('h1[data-editable=\"headline\"]').text().trim();\n            log.info(`Found article with title: ${title}`);\n        }\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests.\n});\n\n// Initialize the Sitemap class from @crawlee/utils with the supplied URL\nconst sitemap = await Sitemap.load({ url: sitemapUrl });\n\n// Add the sitemap URLs to the crawler's request queue\nawait crawler.addRequests(\n    sitemap.urls.map((url) => ({ url, label: 'SITEMAP' })),\n    { strategy: EnqueueStrategy.SameHostname },\n);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Playwright Store Scraper\nDESCRIPTION: Initial scraping setup using PlaywrightCrawler to verify category scraping functionality\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const items = await page.$$('.collection-block-item');\n        for (const item of items) {\n            const content = await item.textContent();\n            console.log(content);\n        }\n    }\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Scraping and Parsing Product Price with Playwright in JavaScript\nDESCRIPTION: This code snippet shows how to scrape the product price, filter the correct element, and parse it into a number using Playwright and JavaScript string manipulation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Crawlee\nDESCRIPTION: This example demonstrates how to save data to the default dataset in Crawlee. It imports the Dataset class from Crawlee and pushes an object with title and url properties to the dataset. If the dataset doesn't exist, it will be automatically created.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.pushData({\n    title: 'Data item title',\n    url: 'https://crawlee.dev',\n});\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawlee Logic in AWS Lambda Handler\nDESCRIPTION: Example of wrapping Crawlee crawler initialization and execution in an AWS Lambda handler function. This ensures the code can be executed by AWS Lambda when triggered.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawlee Crawler in Express Server for GCP Cloud Run\nDESCRIPTION: Complete implementation showing how to wrap a Crawlee crawler in an Express HTTP server for GCP Cloud Run. The server listens on the port specified by GCP and returns crawler data in the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with HttpCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use HttpCrawler to crawl a list of URLs from a text file, make HTTP requests, and save the HTML content. It includes error handling and custom request headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset, log } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ request, body, $ }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            html: body,\n        });\n    },\n    // Uncomment this option to see the browser console output.\n    // headless: false,\n});\n\nconst start = async () => {\n    const urls = await readFile('urls.txt', 'utf8')\n        .then((text) => text.trim().split('\\n'))\n        .catch(() => []);\n\n    // Add URLs to a crawler's request queue\n    await crawler.addRequests(urls.map((url) => ({\n        url,\n        // Add custom data to the Request object that you can later use in requestHandler\n        userData: {\n            label: 'START',\n        },\n        // To override default Request headers:\n        // headers: {\n        //     'User-Agent': 'Apify HttpCrawler',\n        //     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n        // },\n    })));\n\n    // Run the crawler\n    await crawler.run();\n};\n\nstart();\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Title with Window API\nDESCRIPTION: Demonstrates how to access the page title using both browser JavaScript and JSDOM approaches\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Scraping with PlaywrightCrawler\nDESCRIPTION: Example implementation of a web crawler using PlaywrightCrawler to scrape Hacker News. The crawler starts from a single URL, discovers and enqueues links to subsequent pages, and stores the results in a dataset. It uses headless Chrome via Playwright for rendering JavaScript-heavy pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Instead of providing the requestQueue here, we will add it later.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Store the results to the default dataset.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Find all links and add them to the queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to use PuppeteerScraper to crawl URLs from a sitemap. It leverages the Sitemap utility to fetch sitemap URLs, then loads them in a headless Chrome browser with Puppeteer for full JavaScript rendering support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerScraper } from '@crawlee/puppeteer';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new PuppeteerScraper({\n    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests\n});\n\nscraper.router.addDefaultHandler(async ({ page, request, log, pushData }) => {\n    const title = await page.title();\n    log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n    // Save results as JSON to ./storage/datasets/default\n    await pushData({ title, url: request.loadedUrl });\n});\n\n// Create an instance of the Sitemap class\n// and download the sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Add URLs from the sitemap as requests to the queue\nawait scraper.addRequests(sitemap.urls);\n\nawait scraper.run();\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: Basic example of using the sendRequest function with Crawlee's BasicCrawler to make HTTP requests. This example demonstrates the simplest implementation that fetches and logs the response body.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering Links with CheerioCrawler using Glob Patterns\nDESCRIPTION: Demonstrates how to use CheerioCrawler to selectively crawl links that match specific glob patterns. The crawler processes each page and only enqueues links that match the defined pattern for further crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, $, request }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Only add links to actor detail pages\n        await enqueueLinks({\n            globs: ['https://apify.com/*/****'],\n        });\n    },\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Search Form with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to programmatically interact with GitHub's search form using PuppeteerCrawler. It fills in search parameters including term, owner, date, and language, then submits the form and processes the results. The extracted data is saved to either Apify's dataset or local storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    requestHandler: async ({ page }) => {\n        // Fill and submit the search form\n        await page.goto('https://github.com/search/advanced');\n        await page.type('#search_term', 'javascript');\n        await page.type('#search_user_name', 'apify');\n        await page.type('#search_from_date', '2015');\n        await page.select('#search_language', 'JavaScript');\n        await page.click('.btn');\n\n        // Wait for the results\n        await page.waitForSelector('.repo-list');\n\n        // Extract the data\n        const repos = await page.$$eval('.repo-list-item', ($posts) => {\n            const data = [];\n            // Process the results\n            for (const $post of $posts) {\n                const title = $post.querySelector('.f4').textContent.trim();\n                const description = $post.querySelector('.mb-1')?.textContent.trim();\n                const stars = $post.querySelector('.octicon-star')?.parentElement.textContent.trim();\n                data.push({ title, description, stars });\n            }\n            return data;\n        });\n\n        // Save the results\n        await Dataset.pushData(repos);\n    },\n});\n\nawait crawler.run(['https://github.com/search/advanced']);\n```\n\n----------------------------------------\n\nTITLE: Web Crawling with PlaywrightCrawler\nDESCRIPTION: Example demonstrating how to use PlaywrightCrawler for recursive crawling of the Crawlee website with a headless browser that can handle JavaScript rendering.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to just a few pages to prevent\n    // overwhelming the website and our dataset.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can perform the extraction logic.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the results to the dataset where we can access them later.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            // Consider only links that contain 'docs'.\n            globs: ['https://crawlee.dev/**'],\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: This snippet shows how to perform basic operations with key-value stores in Crawlee, including reading input, writing output, opening named stores, and manipulating values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use JSDOMCrawler to interact with a React calculator app. It clicks buttons to perform a simple addition and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, crawler }) {\n        // Wait for the calculator to load\n        await crawler.waitFor('.component-display');\n\n        // Click the buttons to perform 1 + 1\n        await crawler.click('.component-button:nth-child(16)');\n        await crawler.click('.component-button:nth-child(13)');\n        await crawler.click('.component-button:nth-child(16)');\n        await crawler.click('.component-button:nth-child(19)');\n\n        // Extract the result\n        const result = window.document.querySelector('.component-display').textContent;\n\n        // Save the result to the dataset\n        await Dataset.pushData({ result });\n    },\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Crawler in TypeScript using Crawlee\nDESCRIPTION: This code demonstrates how to create an HTTP crawler using the HttpCrawler class from Crawlee. It reads URLs from a text file, makes HTTP requests to each URL, and saves the HTML content. The crawler also handles pagination and implements a simple rate limiting mechanism.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, downloadListOfUrls } from 'crawlee';\n\n// Create an instance of the HttpCrawler class\nconst crawler = new HttpCrawler({\n    // Function to handle each HTTP request\n    async requestHandler({ body, request, enqueueLinks, log }) {\n        const title = body.match(/<title>([^<]*)<\\/title>/i)?.[1];\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save the HTML content to a file\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n\n        // Add all links from the page to the crawler's request queue\n        await enqueueLinks();\n    },\n    // Maximum number of requests per minute\n    maxRequestsPerMinute: 60,\n});\n\n// Load URLs from an external file\nconst urls = await downloadListOfUrls({ url: 'https://example.com/list-of-urls.txt' });\n\n// Add the URLs to the crawler's request queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with HttpCrawler in JavaScript/TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the HttpCrawler class to crawl a list of URLs from a text file, make HTTP requests to each URL, and save the HTML content. It utilizes the Crawlee library and includes error handling and logging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Create an instance of the HttpCrawler class\nconst crawler = new HttpCrawler({\n    // Function to handle each HTTP request\n    async requestHandler({ request, body, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Save the HTML content to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n    // Function to handle errors during crawling\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Read URLs from a text file\nconst urlsText = await readFile('urls.txt', 'utf8');\nconst urls = urlsText.trim().split('\\n');\n\n// Add the URLs to the crawler's queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a single web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport puppeteer from 'puppeteer';\n\nawait Actor.init();\n\nconst url = 'https://example.com';\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto(url);\n\nconst screenshot = await page.screenshot();\nconst key = `screenshot-${Date.now()}.png`;\nawait Actor.setValue(key, screenshot, { contentType: 'image/png' });\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator App using JSDOMCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to interact with a React calculator app. It navigates to the app, performs button clicks (1+1=), and extracts the calculation result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    // Let's limit our crawls to make our calculations on a safe side...\n    maxRequestsPerCrawl: 10,\n});\n\ncrawler.router.addDefaultHandler(async ({ window, enqueueLinks, log }) => {\n    const { document } = window;\n    \n    log.info(`Testing calculator`);\n    \n    // Click the calculator buttons 1 + 1 =\n    document.querySelector('.calculator-key[value=\"1\"]').click();\n    document.querySelector('.calculator-key[value=\"+\"]').click();\n    document.querySelector('.calculator-key[value=\"1\"]').click();\n    document.querySelector('.calculator-key[value=\"=\"]').click();\n    \n    // Extract the result\n    const result = document.querySelector('.calculator-display').textContent;\n    \n    // Store the result\n    await Dataset.pushData({\n        title: document.title,\n        result,\n        url: 'https://ahfarmer.github.io/calculator/',\n    });\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler in TypeScript\nDESCRIPTION: A basic implementation of PlaywrightCrawler that visits an e-commerce website's collections page and logs the text content of each category element. It serves as a sanity check before implementing the full scraping logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const title = await page.title();\n        console.log(`Title of the page: ${title}`);\n\n        const collectionElements = await page.$$('.collection-block-item');\n        for (const collectionElement of collectionElements) {\n            const textContent = await collectionElement.textContent();\n            console.log('Collection text content:', textContent);\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Cheerio Crawler in Crawlee\nDESCRIPTION: This code demonstrates how to use Crawlee's Cheerio Crawler to download and crawl URLs from a sitemap. It uses the downloadListOfUrls utility method to fetch the URLs and then processes them with the Cheerio Crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Cheeriorawler, Proxyonfiguration } from '@crawlee/cheerio';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\n// Configuration of the sitemap's URL\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new Cheeriorawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n        // Here you can process the HTML from the crawled URL using cheerio\n    },\n    // If you need to use proxy, you can use the proxy configuration\n    proxyConfiguration: new Proxyonfiguration({ proxyUrls: ['...'] }),\n});\n\n// This logic downloads the sitemap from the specified URL\nasync function crawlSitemap() {\n    // Download the sitemap and parse its URLs\n    const urls = await downloadListOfUrls({ url: startUrls[0] });\n\n    // Add all the URLs to the crawler's queue\n    await crawler.addRequests(urls.map((url) => ({ url })));\n\n    // Run the crawler\n    await crawler.run();\n}\n\n// Execute the crawlSitemap function\nawait crawlSitemap();\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Autoscaled Pool Options in Crawlee\nDESCRIPTION: Provides fine-grained control over the autoscaling behavior of the crawler through the autoscaledPoolOptions object. This includes settings for concurrency ratios, scaling steps, and check intervals.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Here we can pass all advanced options for the autoscaling pool\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.9,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 1,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 120,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Web Crawler with Crawlee in TypeScript\nDESCRIPTION: A basic example that demonstrates how to use BasicCrawler to download web pages with HTTP requests. It retrieves pages from a list of URLs, processes them, and stores their HTML content and URLs in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that just executes the request and processes the result.\nconst crawler = new BasicCrawler({\n    // Let's limit our crawls to only 10 requests in total\n    // to prevent excess usage of the service.\n    maxRequestsPerCrawl: 10,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the logic to process the downloaded pages.\n    async requestHandler({ request, sendRequest, log }) {\n        // 'request' contains an instance of the Request class\n        // Here we simply fetch the HTML of the page and store it to a dataset\n        log.info(`Processing ${request.url}...`);\n\n        // sendRequest() is asynchronous and returns a promise\n        const { body } = await sendRequest();\n\n        // Store the HTML and URL to the default dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n\n        log.info(`The crawler has crawled ${request.url}...`);\n    },\n});\n\n// Define the starting URLs\nconst startUrls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction',\n    'https://crawlee.dev/docs/guides/apify-platform',\n    'https://crawlee.dev/docs/guides/request-storage',\n    'https://crawlee.dev/docs/guides/result-storage',\n    'https://crawlee.dev/docs/guides/proxy-management',\n    'https://crawlee.dev/docs/guides/session-management',\n    'https://crawlee.dev/docs/guides/user-agent-spoofing',\n    'https://crawlee.dev/docs/guides/puppeteer-js',\n    'https://crawlee.dev/docs/guides/proxy-rotation',\n    'https://crawlee.dev/docs/guides/scaling-crawlers',\n];\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(startUrls);\n\nconsole.log('Crawler finished.');\n\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple TypeScript example of a Cheerio crawler that visits a single URL, extracts the page title, and prints it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using saveSnapshot() Utility\nDESCRIPTION: This example shows how to use the context-aware saveSnapshot() utility in PuppeteerCrawler to capture screenshots of multiple web pages. It automatically handles the saving of both screenshot and HTML content for each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Create a PuppeteerCrawler instance.\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL.\n    async requestHandler({ request, page, crawler }) {\n        const { url } = request;\n\n        // Create a key from the URL.\n        const key = url\n            .replace(/[:/]/g, '_')\n            .replace(/\\./g, '-');\n\n        // Capture the screenshot using the utility function.\n        // This will save both a screenshot and HTML to the default key-value store.\n        await crawler.utils.saveSnapshot(page, { key });\n\n        console.log(`Snapshot of ${url} saved as ${key}`);\n    },\n});\n\n// Start the crawler.\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://crawlee.dev/api/core/class/BasicCrawler',\n]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler Using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It creates a PuppeteerCrawler instance, defines a handler function that captures and saves screenshots for each page, and starts the crawler with a list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshot = await page.screenshot();\n        const key = `screenshot-${request.id}.png`;\n        await Actor.setValue(key, screenshot, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Recursive Web Scraping with PuppeteerCrawler\nDESCRIPTION: Demonstrates how to implement a recursive web scraper using PuppeteerCrawler to crawl Hacker News. The crawler starts with a single URL, finds links to next pages, enqueues them and stores results in a dataset. Uses headless Chrome via Puppeteer for rendering JavaScript and extracting data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as screenshots\n        await page.screenshot({\n            path: `screenshot-${Math.random()}.png`,\n            fullPage: true,\n        });\n\n        // Extract desirable links to the RequestQueue\n        await enqueueLinks({\n            // selector: 'a.storylink', // you can add explicit selector here\n            // pseudoUrls: [/^https:\\/\\/news\\.ycombinator\\.com\\/item.*/], // and explicit patterns for URLs\n            baseUrl: new URL(request.url).origin,\n        });\n\n        // Store results\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n    // Uncomment this option to see the browser window\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://news.ycombinator.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in Main File\nDESCRIPTION: Sets up the main Crawlee crawler entry point that uses a router for request handling instead of a monolithic handler function with conditional logic. This approach improves code organization and maintainability.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example of how to customize browser fingerprints in PlaywrightCrawler by specifying browser type, version, and operating system. This helps make scraping requests appear more like regular user traffic.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\nconst crawler = new PlaywrightCrawler({\n    // We need to specify the playwright browser just like before\n    browserType: firefox,\n    // Limit the number of concurrent requests to avoid overloading the target website\n    maxConcurrency: 5,\n    // Here comes the important part - you can override predefined fingerprints or their parts\n    browserPoolOptions: {\n        // This defines the fingerprints we want to use\n        fingerprintOptions: {\n            // We want to use the newest Firefox\n            browser: 'firefox',\n            // We want to pretend to be a visitor from Germany\n            locales: [\n                'de',\n            ],\n            // We want to pretend to be on Windows 11 (Win10 = Win11)\n            os: 'win10',\n        },\n    },\n    // Let's slow down each request a bit\n    requestHandlerTimeoutSecs: 30,\n    navigationTimeoutSecs: 30,\n    // Process the scraped data\n    async requestHandler({ request, page }) {\n        const data = {\n            url: request.url,\n            title: await page.title(),\n        };\n\n        // Save the data to the default dataset\n        await Dataset.pushData(data);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://playwright.dev']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PuppeteerCrawler - JavaScript\nDESCRIPTION: Example of SessionPool usage with PuppeteerCrawler for headless browser automation\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-Rendered Content with PuppeteerCrawler\nDESCRIPTION: This snippet uses PuppeteerCrawler to successfully scrape content from Apify Store that is rendered client-side with JavaScript. It demonstrates explicit waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor card to render\n        await page.waitForSelector('.ActorStoreItem');\n        // Extract text content of an actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Comparing DOM Selection in Browser JS vs Cheerio\nDESCRIPTION: Demonstrates the syntax differences between browser JavaScript and Cheerio for selecting and manipulating DOM elements, showing equivalent operations for title text extraction and href collection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler for Hacker News Scraping in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and use PuppeteerCrawler to scrape the Hacker News website. It includes the configuration of the crawler, definition of the request handler, and the logic for processing and storing the scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PuppeteerCrawler, RequestQueue } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://news.ycombinator.com'];\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue: await RequestQueue.open(),\n    requestHandler: router,\n});\n\nawait crawler.run(startUrls);\n\nawait Dataset.open().then((dataset) =>\n    dataset.exportToJSON('results.json'),\n);\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-rendered Content with PlaywrightCrawler\nDESCRIPTION: This snippet uses PlaywrightCrawler to scrape JavaScript-rendered content, automatically waiting for elements to appear before extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of an actor card\n        const actorText = await page.locator('.ActorStoreItem').first().innerText();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling with PlaywrightCrawler\nDESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using PlaywrightCrawler. It shows how to set up the crawler with Playwright, define the request handler, and start the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit to 20 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This example demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with page.screenshot(). It creates a crawler that processes a list of URLs, captures a screenshot for each page, and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\n// Create a PuppeteerCrawler instance.\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL.\n    async requestHandler({ request, page }) {\n        const { url } = request;\n\n        // Create a key from the URL.\n        const key = url\n            .replace(/[:/]/g, '_')\n            .replace(/\\./g, '-');\n\n        // Capture the screenshot.\n        const screenshotBuffer = await page.screenshot();\n\n        // Save the screenshot to the default key-value store.\n        await KeyValueStore.setValue(`${key}.png`, screenshotBuffer, {\n            contentType: 'image/png',\n        });\n\n        console.log(`Screenshot of ${url} saved as ${key}.png`);\n    },\n});\n\n// Start the crawler.\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://crawlee.dev/api/core/class/BasicCrawler',\n]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This code shows how to capture screenshots of multiple web pages when using PuppeteerCrawler. It creates a crawler that navigates to URLs in the request queue, takes screenshots using page.screenshot(), and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\n// Create the crawler\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page }) {\n        // Navigate to the URL.\n        await page.goto(request.url);\n\n        // Capture the screenshot.\n        const screenshotBuffer = await page.screenshot();\n\n        // Get a unique key for the screenshot from the URL.\n        const key = new URL(request.url).hostname;\n\n        // Save the screenshot to the default key-value store.\n        await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n\n        console.log(`Screenshot of ${request.url} saved!`);\n    },\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://apify.com' },\n]);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract a product SKU from a webpage using Playwright. It targets a span element with the specific class 'product-meta__sku-number'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Stage Dockerfile for Crawlee Actor with Playwright\nDESCRIPTION: Complete Dockerfile for setting up a Crawlee actor with a two-stage build process. The builder stage compiles the source code, while the final stage creates a minimal production image with only the necessary dependencies. The configuration includes performance optimizations like Docker layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PuppeteerCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with PuppeteerCrawler to handle IP rotation and session management. Shows how to configure browser-based crawling with session persistence and response monitoring.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    // This works directly with PuppeteerCrawler\n    useSessionPool: true,\n    // By default this is true. If true, all cookies will be included in the session.\n    // Then, when the session is reused, the cookies will be restored\n    // and thus you can keep the session alive over more requests.\n    persistCookiesPerSession: true,\n    async requestHandler({ request, page, session, log }) {\n        // Wait for the page to fully load\n        await page.waitForNetworkIdle();\n        \n        // Get the status code directly from the page\n        const statusCode = page.browser() \n            ? await page.evaluate(() => window.status || 200)\n            : 200;\n\n        // Based on the status code, mark the session as working, blocked, or failed.\n        if (statusCode === 200) {\n            session.markGood();\n            log.info(`Request ${request.url} succeeded and loaded ${await page.title()}`);\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n        \n        // Extract data from the page\n        // ...\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler for HTML Parsing and Data Extraction\nDESCRIPTION: Example demonstrates how to use CheerioCrawler to process URLs from an external source, make HTTP requests, and parse HTML using Cheerio. The code shows how to extract page titles and h1 tags from web pages while handling the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's limit the crawling to make the example quick and safe\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ $, request }) {\n        // Extract data from the page using Cheerio\n        const title = $('title').text();\n        const h1texts = $('h1')\n            .map((_, el) => $(el).text())\n            .get();\n\n        // Log the data\n        log.info(\n            'URL: ' + request.url + '\\n' +\n            'TITLE: ' + title + '\\n' +\n            'TEXT: ' + h1texts,\n        );\n    },\n});\n\n// This array contains the URLs to crawl\nconst startUrls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction/first-crawler',\n];\n\n// Run the crawler with the provided URLs\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration for Crawlee TypeScript Project\nDESCRIPTION: Full package.json configuration for a Crawlee TypeScript project, including all necessary scripts, dependencies, and project settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Multi-Stage Docker Build for Crawlee Actor with Playwright\nDESCRIPTION: Complete Dockerfile for configuring a Crawlee actor with Playwright and Chrome. Uses a multi-stage build approach to optimize the final image size by separating build dependencies from runtime dependencies. The build phase compiles the application while the final image contains only the necessary runtime files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using Map Method with Crawlee Dataset\nDESCRIPTION: Example of using the Dataset.map() method to filter and transform dataset items. This code checks if pages have more than 5 header elements and returns an array of heading counts that meet this condition.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Scraping in Crawlee\nDESCRIPTION: Modified category route handler that enqueues product URLs to a shared request queue instead of processing them directly. This separates the discovery of URLs from their processing, enabling parallelization.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Existing Routes object\nexport const router = createPlaywrightRouter();\n\n// Note: startUrls handler code would be here\n\nrouter.addHandler(Labels.CATEGORY, async ({ request, page, log, crawler }) {\n    const requestQueue = await getOrInitQueue();\n\n    const title = await page.title();\n    log.info(`${title}`, { url: request.loadedUrl });\n\n    // Get the products\n    const products = await page.$$('a.product-item');\n    \n    log.info(`Found ${products.length} products`)\n\n    // Instead of calling enqueueLinks, we'll manually add them to the separate queue\n    // We want to make them available for the other crawlers\n    // First we need to get the hrefs\n    const productUrls = [];\n    \n    for (const product of products) {\n        const href = await product.getAttribute('href');\n        productUrls.push(new URL(href, request.loadedUrl).href);\n    }\n\n    // Then we addRequest each URL individually to our request queue\n    // Request Queue addRequest supports batching, but for simplicity, we'll do them one at a time\n    for (const url of productUrls) {\n        await requestQueue.addRequest({\n            url,\n            userData: {\n                label: Labels.DETAIL,\n            },\n        });\n    }\n\n    // Then we do the same for pagination\n    // Get the pagination links\n    const paginationLinks = await page.$$('a.page-link[href]:not(.disabled)');\n    \n    if (paginationLinks.length > 0) {\n        log.info(`Found ${paginationLinks.length} pagination links`);\n    }\n\n    // Add them to the crawling queue of the crawler\n    for (const link of paginationLinks) {\n        const href = await link.getAttribute('href');\n        await crawler.addRequests([{\n            url: new URL(href, request.loadedUrl).href,\n            userData: {\n                label: Labels.CATEGORY,\n            },\n        }]);\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling All Website Links with Playwright Crawler in Crawlee\nDESCRIPTION: This code illustrates how to crawl all links on a website using Playwright Crawler in Crawlee. It configures a PlaywrightCrawler to navigate web pages, extracts the page title, and uses enqueueLinks() to discover and add new links to the request queue automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, enqueueLinks } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n    maxConcurrency: 10, // Maximum concurrency\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // This function is called for every page the crawler visits.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract desired data from the page using Playwright.\n\n        // This enqueues all links on the page that pass the filter function.\n        // The function loads the URLs using the same options as the crawler\n        await enqueueLinks();\n    },\n});\n\n// Start the crawler and wait for it to finish.\nawait crawler.run(['https://crawlee.dev']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to crawl all links on a website with PuppeteerCrawler. It uses headless Chrome to load pages, extracts and logs the page title, and uses enqueueLinks() to add new discovered links to the crawling queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome browser.\nconst crawler = new PuppeteerCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // On each request, call the following function\n    // which will extract data from the site and add new links to the queue.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract all links from the page.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a Request Queue including opening a queue, adding requests, getting request information, and retrieving the next request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\n// Add URL to the queue\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Add multiple URLs to the queue\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev/docs' },\n    { url: 'https://crawlee.dev/api' },\n]);\n\n// Get the queue information\nconst queueInfo = await requestQueue.getInfo();\nconsole.log(queueInfo);\n\n// Get the next request in the queue\nconst nextRequest = await requestQueue.fetchNextRequest();\nconsole.log(nextRequest);\nif (nextRequest) {\n    // Mark the request as handled\n    await requestQueue.markRequestHandled(nextRequest);\n\n    // Or if something went wrong, reclaim it back to the queue\n    await requestQueue.reclaimRequest(nextRequest);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Crawlee (JavaScript)\nDESCRIPTION: This code snippet demonstrates how to save data to the default dataset in Crawlee. If the dataset doesn't exist, it will be created automatically. The example uses the Dataset.pushData() method to add items to the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Dataset, CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        const h1 = $('h1').text();\n\n        // Save data to default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This example illustrates SessionPool implementation with CheerioCrawler for web scraping. It shows how to configure proxy settings, handle sessions, and process HTML content with cheerio while maintaining session integrity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // The default sessionPoolOptions and proxyConfiguration\n    // are described in BasicCrawler example\n    // and it will work the same out of the box with CheerioCrawler\n    async requestHandler({ $, request, session }) {\n        // Process the acquired data from $ using cheerio\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        \n        // To increase the chance that the session will continue to be valid when the proxy is blocked, we can do the following:\n        if (title === 'Access denied' || title === 'Forbidden') {\n            session!.retire();\n            throw new Error('We got blocked, lets use another session for these requests');\n        }\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: ['http://user:password@proxy.com:8000']\n    }),\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Basic HTTP Crawler Implementation\nDESCRIPTION: Complete example demonstrating how to initialize and run an HTTP crawler to process multiple URLs and save data to a dataset. Shows basic setup with requestHandler and URL processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/http-crawler/README.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, Dataset } from '@crawlee/http';\n\nconst crawler = new HttpCrawler({\n    requestList,\n    async requestHandler({ request, response, body, contentType }) {\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler with Crawlee\nDESCRIPTION: A basic crawler implementation that downloads web pages using HTTP requests and stores their HTML content and URLs in a default dataset. It demonstrates the usage of BasicCrawler and sendRequest utility function from Crawlee framework.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler\nconst crawler = new BasicCrawler({\n    // Define the request handler function that will be called for each URL\n    async requestHandler({ request, sendRequest }) {\n        // Download the page using the sendRequest utility\n        const response = await sendRequest();\n\n        // Store data in the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: response.body,\n        });\n    },\n});\n\n// Add requests to the queue\nawait crawler.run([\n    'http://example.com/page-1',\n    'http://example.com/page-2',\n    'http://example.com/page-3',\n]);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Crawlee with Browser Support\nDESCRIPTION: Full implementation of an AWS Lambda handler function for running a Crawlee crawler with browser support. Includes configuration for Chromium in the Lambda environment and returns the crawled data in the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Recursively Scraping Hacker News with PlaywrightCrawler\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler with RequestQueue to recursively scrape the Hacker News website. It extracts article titles, authors, and scores, handling pagination by finding and following 'More' links. Results are stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API.\n        const results = await page.$$eval('.athing', ($posts) => {\n            return $posts.map($post => {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const itemId = $post.id;\n\n                // Directly get the corresponding iteminfo & subtext elements using the itemId\n                const subtext = document.querySelector(`#${itemId} + tr .subtext`);\n                const points = subtext?.querySelector('.score')?.innerText;\n                const author = subtext?.querySelector('.hnuser')?.innerText;\n\n                return {\n                    title,\n                    rank,\n                    href: $post.querySelector('.title a').href,\n                    points: points || null,\n                    author: author || null,\n                };\n            });\n        });\n\n        // Save the data to default dataset.\n        await Dataset.pushData(results);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Apify Actor with Playwright and Chrome\nDESCRIPTION: This Dockerfile sets up an environment for an Apify actor using Node.js, Playwright, and Chrome. It installs npm packages, copies the project files, and specifies the command to run the actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee with Node.js 16\nDESCRIPTION: This Dockerfile sets up a Docker environment for a Crawlee project. It uses the apify/actor-node:16 base image, installs production dependencies only, copies the project files, and configures the container to start the application with npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: Example showing how to enable the request locking experiment in a CheerioCrawler instance. This allows the crawler to use the new request locking API for better parallelization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This example illustrates the setup of SessionPool with PuppeteerCrawler in Crawlee. It covers proxy configuration and session management within the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, page, browserController }) {\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in TypeScript\nDESCRIPTION: Implementation for crawling multiple specified URLs using the Cheerio Crawler in Crawlee. The code creates a new crawler instance, configures request handling that extracts the page title, and processes a list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, CheerioCrawlingContext } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the HTML and parses it using Cheerio.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerMinute: 100,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // The argument context contains information about the crawler,\n    // the URL, and other utilities.\n    async requestHandler({ request, $, log }: CheerioCrawlingContext) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n    },\n});\n\n// These links are from the Crawlee documentation.\nconst usingSiteNavigation = [\n    'https://crawlee.dev/',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/guides/request-storage',\n    'https://crawlee.dev/docs/guides/result-storage',\n];\n\n// Run the crawler with the defined requests.\nawait crawler.run(usingSiteNavigation);\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile Build for Crawlee Actor\nDESCRIPTION: A Dockerfile that implements a multi-stage build process for a Crawlee actor. The build stage handles dependency installation and project compilation, while the final stage creates an optimized production image with minimal dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:16\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using Page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It crawls a list of URLs and saves screenshots to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler, Configuration } from 'crawlee';\n\nawait Actor.init();\n\nconst urls = ['https://apify.com', 'https://crawlee.dev'];\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshotBuffer = await page.screenshot();\n\n        // The key is extracted from the URL\n        const key = request.url.replace(/[:/]/g, '_');\n\n        // Save the screenshot to the default key-value store\n        await Actor.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n        console.log(`Screenshot of ${request.url} saved as ${key}`);\n    },\n});\n\nawait crawler.run(urls);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator App using JSDOMCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to interact with a React calculator app by simulating button clicks and extracting the calculation result. It navigates to the calculator, performs a '1+1=' operation, and logs the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, log } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ window, request, enqueueLinks, pushData, session }) {\n        const { document } = window;\n        log.info(`Processing ${request.url}...`);\n\n        // Clicking the buttons on the calculator app\n        document.querySelector('.button:nth-child(13)')?.click(); // 1\n        document.querySelector('.button:nth-child(4)')?.click();  // +\n        document.querySelector('.button:nth-child(13)')?.click(); // 1\n        document.querySelector('.button:nth-child(16)')?.click(); // =\n\n        // Extracting the calculator result\n        const result = document.querySelector('.component-display')?.textContent;\n        log.info(`The result of 1+1 is ${result}`);\n    },\n});\n\nconst url = 'https://ahfarmer.github.io/calculator/';\n\n// Start the crawler with the provided URLs\nawait crawler.run([url]);\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores. Shows how to store and retrieve different data types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Screenshot Capture Using Crawler Utils\nDESCRIPTION: Shows how to capture screenshots using the utils.puppeteer.saveSnapshot() utility function, which automatically handles saving to the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { puppeteerUtils } from '@crawlee/puppeteer';\n\nawait Actor.init();\n\nconst url = 'https://example.com';\nconst page = await browser.newPage();\nawait page.goto(url);\n\n// Capture and save the screenshot\nawait puppeteerUtils.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up a PlaywrightCrawler that uses Firefox browser instead of the default Chromium. It configures the crawler to visit a webpage, extract data, and handle the response using Playwright with Firefox.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserType: firefox,\n    // Function called for each URL\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            title,\n            url: request.url,\n            succeeded: true,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://www.iana.org/']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler\nDESCRIPTION: This code snippet shows how to crawl multiple specified URLs using the Puppeteer Crawler in Crawlee. It requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform, sets up a request list, and uses Puppeteer to interact with each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler with RequestQueue\nDESCRIPTION: This snippet shows how to create a CheerioCrawler with a RequestQueue and a request handler. The crawler visits the specified URL, extracts the page title using Cheerio, and logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This code snippet illustrates the setup of SessionPool with CheerioCrawler in Crawlee. It includes proxy configuration and demonstrates session management in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, $, body }) {\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with CSS Selectors in Browser Console\nDESCRIPTION: This snippet demonstrates how to use document.querySelectorAll() with a CSS class selector to find all elements with the 'collection-block-item' class on a webpage. This is a common verification step when building web scrapers to ensure you're selecting only the desired elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Playwright Crawler in TypeScript\nDESCRIPTION: This example demonstrates using Playwright Crawler for processing sitemap URLs. It requires the 'apify/actor-node-playwright-chrome' image when running on the Apify Platform and leverages Playwright's browser automation capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add all links from the current page to the crawling queue.\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n});\n\n// Download and parse sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Get URLs from a sitemap\nconst urls = sitemap.getUrls();\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This code snippet shows how to extract all href attributes from anchor tags on a web page. It uses Cheerio's attribute selector, map function, and get method to collect the links into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet illustrates how to use Playwright Crawler to crawl all links on a website. It configures the crawler, sets up a request queue, and processes each page to extract the title and enqueue new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete Crawlee Data Extraction and Storage Example\nDESCRIPTION: A full example of a Crawlee script that crawls a website, extracts data, and saves it to a Dataset. It uses PlaywrightCrawler and includes error handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        const results = [];\n        const dataElements = await page.$$('.athing');\n\n        for (const element of dataElements) {\n            const rank = await element.$eval('.rank', (el) => el.textContent);\n            const titleElement = await element.$('.title a');\n            const title = await titleElement.textContent();\n            const url = await titleElement.getAttribute('href');\n\n            results.push({\n                rank: rank?.trim(),\n                title: title?.trim(),\n                url: url?.trim(),\n            });\n        }\n\n        await Dataset.pushData(results);\n\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with BasicCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with BasicCrawler to handle proxy rotation and session management. The code demonstrates creating a crawler that manually manages sessions when making requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new BasicCrawler({\n    // Basic crawler does not have proxy integration, but we can use\n    // SessionPool directly via `useSessionPool` option and then get the session\n    // via `crawler.sessionPool`.\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ request, crawler }) {\n        const session = await crawler.sessionPool.getSession();\n\n        try {\n            // By manually requesting the URL via got-scraping with the session\n            const { statusCode, body } = await got(request.url, {\n                headers: {\n                    // session automatically adds cookies to the cookieJar based on response\n                    cookieJar: session.cookieJar,\n                    // We can get a proxy URL directly from the session\n                    // that can be passed to got\n                    proxy: session.proxyUrl,\n                },\n            });\n\n            console.log(`URL: ${request.url}, Status code: ${statusCode}`);\n\n            session.markGood();\n        } catch (err) {\n            // This is where we handle the errors:\n            // We create a state object that will be saved to the session\n            const errorStatusCode = err.statusCode ?? err.response?.statusCode;\n\n            if (errorStatusCode) {\n                const statusMessage = err.statusMessage ?? err.response?.statusMessage;\n                session.markBad();\n            }\n\n            throw err;\n        } finally {\n            // We need to retire the session always!\n            session.retire();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Custom Cookie Jar with sendRequest\nDESCRIPTION: Demonstrates how to use a custom cookie jar from the tough-cookie package with sendRequest. This allows for better control over cookie management during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright in JavaScript\nDESCRIPTION: This snippet shows how to extract a product title using Playwright's locator API with a CSS selector. It targets an h1 element within a div with the class 'product-meta'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Custom Configuration in Crawlee\nDESCRIPTION: This code demonstrates how to create a custom Configuration object in Crawlee with a 10-second state persistence interval and pass it to a CheerioCrawler. The example includes a simple crawling scenario with two requests and specific timing control using sleep() to demonstrate state persistence behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Basic Browser Pool Usage with Playwright\nDESCRIPTION: Example demonstrating how to initialize a Browser Pool with Playwright, open a page, interact with it, and properly close resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BrowserPool, PlaywrightPlugin } from '@crawlee/browser-pool';\nimport playwright from 'playwright';\n\nconst browserPool = new BrowserPool({\n    browserPlugins: [new PlaywrightPlugin(playwright.chromium)],\n});\n\n// Launches Chromium with Playwright and returns a Playwright Page.\nconst page1 = await browserPool.newPage();\n// You can interact with the page as you're used to.\nawait page1.goto('https://example.com');\n// When you're done, close the page.\nawait page1.close();\n\n// Opens a second page in the same browser.\nconst page2 = await browserPool.newPage();\n// When everything's finished, tear down the pool.\nawait browserPool.destroy();\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL using the crawler. The crawler extracts the page title and URL, storing them as results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioSimpleCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new CheerioSimpleCrawler({\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await crawler.pushData({ title, url: request.url });\n    },\n    maxRequestsPerCrawl: 10, // Limit to 10 requests for demonstration purposes\n});\n\nconst { requestQueue } = await crawler.getRequestQueue();\n\n// Add sitemap.xml URLs to the request queue\nconst baseUrl = 'https://example.com';\nconst urls = await downloadListOfUrls({\n    url: `${baseUrl}/sitemap.xml`,\n});\nfor (const url of urls) {\n    await requestQueue.addRequest({ url });\n}\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Project with Playwright\nDESCRIPTION: This Dockerfile sets up a containerized environment for running a Crawlee web scraping actor. It builds upon a pre-configured image with Node.js and Playwright, optimizes dependency installation using Docker layer caching, and establishes the runtime environment for the actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Crawler Link Enqueuing\nDESCRIPTION: TypeScript implementation using Puppeteer Crawler to crawl all links on a website. Uses enqueueLinks() method with Puppeteer-specific page handling and request queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // The crawler will automatically process all enqueued links\n    maxRequestsPerCrawl: 100,\n    async requestHandler({ enqueueLinks, log, page }) {\n        log.info('Processing...');\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler\nDESCRIPTION: This code snippet illustrates how to crawl multiple specified URLs using the Playwright Crawler in Crawlee. It requires the apify/actor-node-playwright-chrome image when running on the Apify Platform, configures a request list, and uses Playwright to navigate and extract data from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue with Locking Support in JavaScript\nDESCRIPTION: Creates a shared request queue with locking support for parallel scraping. Includes functionality to initialize or reset the queue state.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\nexport async function getOrInitQueue(shouldPurge = false) {\n    const requestQueue = await RequestQueue.open('shared-queue', {\n        // Add support for locking requests\n        requestLocking: true,\n    });\n\n    if (shouldPurge) {\n        await requestQueue.drop();\n    }\n\n    return requestQueue;\n}\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: This snippet uses the Element API to find all anchor elements with href attributes on a page and extracts them into an array. It leverages DOM querySelectorAll and Array.from methods to collect all links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Initializing Proxy Configuration with Custom Proxy URLs in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a basic ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL. This is the foundational setup for using proxies in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Playwright Full Docker Configuration\nDESCRIPTION: Docker configuration for Node.js with full Playwright installation including all browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler. It sets specific options for the browser, operating system, and viewport, and uses a custom User-Agent.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // ...\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\"chrome\"],\n                devices: [\"desktop\"],\n                operatingSystems: [\"windows\"],\n            },\n            fingerprintInjector: async ({ page, fingerprint }) => {\n                // Set custom viewport\n                await page.setViewport(fingerprint.screen);\n                // Set custom user agent\n                await page.setUserAgent(fingerprint.userAgent);\n            },\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler in Crawlee\nDESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl a sitemap. It uses the Sitemap class to process sitemap URLs and PuppeteerScraper to scrape each page. It's designed to run on the Apify Platform using the apify/actor-node-puppeteer-chrome image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerScraper, Dataset } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new PuppeteerScraper({\n    maxRequestsPerCrawl: 20,\n});\n\nconst sitemap = new Sitemap({\n    sitemapUrls: ['https://crawlee.dev/sitemap.xml'],\n});\n\nscraper.router.addDefaultHandler(async ({ page, request, enqueueLinks }) => {\n    const title = await page.title();\n    await Dataset.pushData({ url: request.url, title });\n    await enqueueLinks();\n});\n\nawait scraper.run(sitemap.urls());\n\n```\n\n----------------------------------------\n\nTITLE: Basic JSDOMCrawler Implementation Example\nDESCRIPTION: A complete example demonstrating how to initialize and run JSDOMCrawler to extract page title data and save it to a Dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/jsdom-crawler/README.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ request, window }) {\n        await Dataset.pushData({\n            url: request.url,\n            title: window.document.title,\n        });\n    },\n});\n\nawait crawler.run([\n    'http://crawlee.dev',\n]);\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee\nDESCRIPTION: Code that saves scraped results to Crawlee's Dataset storage. This replaces the console.log() output with persistent storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox browser in TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize a PlaywrightCrawler with Firefox browser configuration. It shows how to specify browser type, launch options, and handle page navigation and data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Use firefox browser\n    browserType: 'firefox',\n    launchOptions: {\n        headless: true,\n    },\n    // Function called for each URL\n    async requestHandler({ request, page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page Using JSDOMCrawler\nDESCRIPTION: This JavaScript snippet demonstrates how to find all anchor (a) elements with href attributes on a page and extract their URLs into an array using the Element API with JSDOMCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Configuring PuppeteerCrawler with puppeteer-extra and stealth plugin\nDESCRIPTION: This snippet shows how to set up a PuppeteerCrawler using puppeteer-extra and the stealth plugin. It demonstrates the configuration of the crawler, including the use of a custom launcher function to integrate puppeteer-extra.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\nimport puppeteer from 'puppeteer-extra';\nimport StealthPlugin from 'puppeteer-extra-plugin-stealth';\n\npuppeteer.use(StealthPlugin());\n\nconst crawler = new PuppeteerCrawler({\n    // Instead of the default launcher, we use our own\n    // that uses puppeteer-extra under the hood\n    launchContext: {\n        launcher: puppeteer,\n    },\n    async requestHandler({ page, request, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Specifying Input JSON File Path\nDESCRIPTION: This example shows the path where the INPUT.json file should be placed to provide input to a Crawlee actor. The file should be placed in the default key-value store folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor\nDESCRIPTION: Sets up a Docker container for running Crawlee actors using the apify/actor-node-playwright-chrome:20 base image. The configuration optimizes build time through careful layering of commands, installing production dependencies only, and copying source code efficiently.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Data Return\nDESCRIPTION: Final implementation of the AWS Lambda handler that includes crawler initialization, execution, and returns the scraped data with proper HTTP status code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Results Output\nDESCRIPTION: Example JSON output showing the structure of crawled data stored by Crawlee\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: Demonstrates the usage of SessionPool with PuppeteerCrawler for managing sessions and proxy rotations. It includes setup for proxy configuration, session pool options, and handling of requests and blocked sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    // The `useSessionPool` is enabled by default\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ page, request, session }) {\n        const title = await page.title();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // Check if you got blocked here and invalidate the session\n        if (await isBlocked(page)) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Puppeteer Crawler in Crawlee\nDESCRIPTION: This code shows how to download URLs from a sitemap and crawl them using the Puppeteer Crawler in Crawlee. It requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with CheerioCrawler and Extracting Data using Cheerio in JavaScript\nDESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl a list of URLs from a file, make HTTP requests, parse HTML with Cheerio, and extract the page title and h1 tags. It utilizes the Crawlee library and filesystem operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\nimport { readFile, writeFile } from 'fs/promises';\n\n// Add URLs to a text file\nconst urls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction/first-crawler',\n];\nawait writeFile('urls.txt', urls.join('\\n'), { encoding: 'utf8' });\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's restrict our crawls to a single file for this example\n    maxRequestsPerCrawl: 3,\n\n    // The `$` argument is the Cheerio object\n    // which is used to extract data from the HTML.\n    async requestHandler({ $, request }) {\n        // Extract data from the page using Cheerio.\n        const title = $('title').text();\n        const h1 = $('h1').text();\n\n        // Store the results to the default dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n});\n\n// Add URLs to the queue and start the crawl.\n// We're using the `readFile` function to get the URLs from the text file.\nawait crawler.run([\n    await readFile('urls.txt', { encoding: 'utf8' })\n        .then((content) => content.split('\\n')),\n]);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with JSDOMCrawler to Extract Page Data in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to process URLs from an external file, loading each page with HTTP requests and parsing the HTML with jsdom. It extracts the page title and all h1 tags from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerSource}\n```\n\n----------------------------------------\n\nTITLE: Setting up PlaywrightCrawler with ProxyConfiguration in TypeScript\nDESCRIPTION: This snippet shows how to configure PlaywrightCrawler with ProxyConfiguration for browser-based scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, session }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Standalone Usage of SessionPool in Crawlee\nDESCRIPTION: This snippet demonstrates how to use SessionPool standalone in Crawlee without a crawler. It shows how to create and manage sessions manually, including creating a session pool, getting a session, and marking a session as retired.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool } from 'crawlee';\n\nconst sessionPool = await SessionPool.open({\n    maxPoolSize: 50,\n    sessionOptions: {\n        maxUsageCount: 5,\n    },\n});\n\nconst session = await sessionPool.getSession();\n\ntry {\n    // your custom request logic\n    const response = await fetch('https://example.com', {\n        headers: {\n            'User-Agent': session.userData.userAgent,\n        },\n    });\n\n    if (!response.ok) {\n        throw new Error(`Request failed with status ${response.status}`);\n    }\n\n    // Process the response\n    const body = await response.text();\n    // ...\n} catch (err) {\n    // This will trigger retirement of the session\n    session.markBad();\n    throw err;\n} finally {\n    session.markGood();\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with Playwright in Crawlee\nDESCRIPTION: Implementation example showing how to crawl multiple URLs using Playwright Crawler in Crawlee. Requires apify/actor-node-playwright-chrome image for execution on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to download and process URLs from a sitemap. It uses the downloadListOfUrls utility to fetch sitemap URLs and enqueues them for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioSimpleCrawler, ProxyConfiguration } from 'crawlee';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new CheerioSimpleCrawler({\n    // ProxyConfiguration is optional\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        await enqueueLinks();\n    },\n});\n\nconst listOfUrls = await downloadListOfUrls({ url: 'https://crawlee.dev/sitemap.xml' });\nawait crawler.addRequests(listOfUrls);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Building CheerioCrawler with RequestQueue\nDESCRIPTION: Demonstrates how to create a CheerioCrawler instance with a RequestQueue and implement a basic request handler to extract page titles using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Session Management with PuppeteerCrawler and Proxies\nDESCRIPTION: Shows how to use session management with PuppeteerCrawler and proxies to create and maintain persistent browsing sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    useSessionPool: true, // This is the default\n    persistCookiesPerSession: true, // This is the default\n    proxyConfiguration,\n    async requestHandler({ request, page, session }) {\n        // Process the response\n        // ...\n\n        // You can mark session as bad when it gets blocked\n        if (someCondition) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Comprehensive E-commerce Crawling with PlaywrightCrawler in TypeScript\nDESCRIPTION: Implements a full crawling strategy for an e-commerce site using PlaywrightCrawler. It handles the start page, category pages, and product detail pages. The code uses different selectors and labels to manage the crawling process and pagination.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with Cheerio for HTML Parsing in Web Scraping\nDESCRIPTION: This code demonstrates combining PlaywrightCrawler with Cheerio for more convenient HTML parsing. It loads the HTML content from the page into a Cheerio object and extracts category information using jQuery-like selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer for the websites\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract information from the page using Cheerio\n        const html = await page.content();\n        const $ = cheerio.load(html);\n        \n        // Use Cheerio's jQuery-like selectors to find the elements\n        $('.collection-block-item').each((i, el) => {\n            const categoryText = $(el).text();\n            log.info(`Category: ${categoryText}`);\n        });\n\n        // We don't need to do anything else here. Just continue with the next request\n    }\n});\n\n// add first URL to the queue and start the crawl\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PlaywrightCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with PlaywrightCrawler to handle IP rotation and session management. Demonstrates browser-based crawling with session-specific settings and status monitoring.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new PlaywrightCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    // This works directly with PlaywrightCrawler\n    useSessionPool: true,\n    // By default this is true. If true, all cookies will be included in the session.\n    // Then, when the session is reused, the cookies will be restored\n    // and thus you can keep the session alive over more requests.\n    persistCookiesPerSession: true,\n    async requestHandler({ request, page, session, log }) {\n        // Wait for the page to fully load\n        await page.waitForLoadState('networkidle');\n        \n        // Get the status code directly from the page\n        const statusCode = page.context().browser() \n            ? await page.evaluate(() => window.status || 200)\n            : 200;\n\n        // Based on the status code, mark the session as working, blocked, or failed.\n        if (statusCode === 200) {\n            session.markGood();\n            log.info(`Request ${request.url} succeeded and loaded ${await page.title()}`);\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n        \n        // Extract data from the page\n        // ...\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links from a Page with Cheerio\nDESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Cheerio Crawler to process a website's sitemap. It utilizes the Sitemap class from @crawlee/utils to download and parse the sitemap, then uses Cheerio Crawler to extract meta title and description from each page listed in the sitemap.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioConfiguration, createCheerioRouter, Dataset, CherioCrawler } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Prepare the crawler configuration\nconst crawler = new CherioCrawler({\n    async requestHandler({ request, $, log, pushData }) {\n        // Extract title and meta description\n        const title = $('title').text();\n        const h1 = $('h1').text();\n        const metaDescription = $('meta[name=description]').attr('content');\n\n        const metadata = {\n            url: request.url,\n            title,\n            h1,\n            metaDescription,\n        };\n\n        log.info('Page scraped', metadata);\n\n        // Store the results\n        await pushData(metadata);\n    },\n});\n\n// Create Sitemap instance\nconst sitemap = new Sitemap({ urls: startUrls });\n\n// Download and process sitemap\nawait sitemap.downloadAllSitemaps();\n\n// Get URLs from the sitemap\nconst urls = sitemap.getSitemapUrls().map((item) => ({ url: item.url }));\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Usage in JavaScript\nDESCRIPTION: A simple example of using CheerioCrawler in JavaScript to scrape product information from a webpage. It shows how to set up the crawler, define the request handler, and process the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Create the crawler and add the required handlers\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        // Extract data from the page using Cheerio\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Enqueue links to other pages\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Enqueueing Links from Same Domain with CheerioCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to crawl links from the same domain using CheerioCrawler. This strategy will enqueue links that share the same domain name, including links from any subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n\n        // Add all links from the page to the crawling queue that match the same domain (including subdomains)\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // We can also use the string representation of the strategy\n            // strategy: 'same-domain',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Implementing BlueskyApiScraper Class with Session Management\nDESCRIPTION: Defines the BlueskyApiScraper class with initialization parameters and session management methods. Handles authentication with the Bluesky API by creating a session with provided credentials.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass BlueskyApiScraper:\n    \"\"\"A scraper class for extracting data from Bluesky social network using their official API.\n\n    This scraper manages authentication, concurrent requests, and data collection for both\n    posts and user profiles. It uses separate datasets for storing post and user information.\n    \"\"\"\n\n    def __init__(self, mode: str, max_request: int | None) -> None:\n        self._crawler: HttpCrawler | None = None\n\n        self.mode = mode\n        self.max_request = max_request\n\n        # Variables for storing session data\n        self._service_endpoint: str | None = None\n        self._user_did: str | None = None\n        self._access_token: str | None = None\n        self._refresh_token: str | None = None\n        self._handle: str | None = None\n\n    def create_session(self, identifier: str, password: str) -> None:\n        \"\"\"Create credentials for the session.\"\"\"\n        url = 'https://bsky.social/xrpc/com.atproto.server.createSession'\n        headers = {\n            'Content-Type': 'application/json',\n        }\n        data = {'identifier': identifier, 'password': password}\n\n        response = httpx.post(url, headers=headers, json=data)\n        response.raise_for_status()\n\n        data = response.json()\n\n        self._service_endpoint = data['didDoc']['service'][0]['serviceEndpoint']\n        self._user_did = data['didDoc']['id']\n        self._access_token = data['accessJwt']\n        self._refresh_token = data['refreshJwt']\n        self._handle = data['handle']\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Handler Method for Bluesky API Data Collection\nDESCRIPTION: Implements the search handler method that processes search requests and extracts data based on the specified mode (posts or users). Handles pagination and adds new requests to the crawler's queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nasync def _search_handler(self, context: HttpCrawlingContext) -> None:\n    \"\"\"Handle search requests based on mode.\"\"\"\n    context.log.info(f'Processing search {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    if 'posts' not in data:\n        context.log.warning(f'No posts found in response: {context.request.url}')\n        return\n\n    user_requests = {}\n    posts = []\n\n    profile_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.actor.getProfile')\n\n    for post in data['posts']:\n        if self.mode == 'users' and post['author']['did'] not in user_requests:\n            user_requests[post['author']['did']] = Request.from_url(\n                url=str(profile_url.with_query(actor=post['author']['did'])),\n                user_data={'label': 'user'},\n            )\n        elif self.mode == 'posts':\n            posts.append(\n                {\n                    'uri': post['uri'],\n                    'cid': post['cid'],\n                    'author_did': post['author']['did'],\n                    'created': post['record']['createdAt'],\n                    'indexed': post['indexedAt'],\n                    'reply_count': post['replyCount'],\n                    'repost_count': post['repostCount'],\n                    'like_count': post['likeCount'],\n                    'quote_count': post['quoteCount'],\n                    'text': post['record']['text'],\n                    'langs': '; '.join(post['record'].get('langs', [])),\n                    'reply_parent': post['record'].get('reply', {}).get('parent', {}).get('uri'),\n                    'reply_root': post['record'].get('reply', {}).get('root', {}).get('uri'),\n                }\n            )\n\n    if self.mode == 'posts':\n        await context.push_data(posts)\n    else:\n        await context.add_requests(list(user_requests.values()))\n\n    if cursor := data.get('cursor'):\n        next_url = URL(context.request.url).update_query({'cursor': cursor})\n        await context.add_requests([str(next_url)])\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It configures logging and runs the crawler on a specific URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: JSON Response Handling\nDESCRIPTION: Shows how to configure sendRequest to handle JSON responses instead of default text.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Configuration\nDESCRIPTION: Docker configuration for Node.js with Playwright and Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Installing CheerioCrawler Manually\nDESCRIPTION: Command to install Crawlee for use with CheerioCrawler, a fast HTTP crawler that parses HTML using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Basic Node.js Docker Configuration\nDESCRIPTION: Basic Docker configuration for Node.js version 16 without browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This example shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes session pool setup and demonstrates how to use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new HttpCrawler({\n    async requestHandler({ session, $, body, json, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        maxPoolSize: 50,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Global Configuration Class\nDESCRIPTION: Example demonstrating how to use the Configuration class to adjust Crawlee settings programmatically. This sets the persistStateIntervalMillis option to 10 seconds using the global configuration singleton.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.reduce() Method in Crawlee\nDESCRIPTION: Shows how to use the reduce method to aggregate data from all dataset items into a single value, specifically calculating the total number of headers across all pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\nimport { KeyValueStore } from 'crawlee';\n\n// Fetch the dataset\nconst dataset = await Dataset.open();\n\n// Get the total number of headers for all pages\nconst pagesHeadingCount = await dataset.reduce((memo, item) => {\n    return memo + item.headingCount;\n}, 0);\n\n// Save the result to the default key-value store\nawait KeyValueStore.setValue('all-headers-count', pagesHeadingCount);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom configuration for a CheerioCrawler in Crawlee. It sets a custom 'persistStateIntervalMillis' option, creates a crawler with this configuration, and defines a router with different behaviors for two URLs. The example illustrates how the custom configuration affects state persistence.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Skip Navigation with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the skipNavigation option with PlaywrightCrawler to directly fetch and save an image from a CDN without navigating to it in the browser. The example shows how to enqueue requests with skipNavigation set to true and handle them in the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, sendRequest, log }) {\n        // Here we check if the request should skip navigation and just be fetched\n        if (request.skipNavigation) {\n            log.info(`Fetching image from ${request.url}`);\n\n            // We can use sendRequest to get the buffer of the image\n            const { body } = await sendRequest();\n\n            // And then we can save it to our key-value store\n            await KeyValueStore.getDefaultInstance().setValue(request.userData.imageFileName, body, {\n                contentType: request.userData.contentType,\n            });\n\n            return;\n        }\n\n        // If we don't have `skipNavigation` flag, we handle the request normally\n        log.info(`Processing ${request.url}...`);\n\n        // ...\n    },\n});\n\nawait crawler.run([\n    { url: 'https://example.com' },\n    {\n        url: 'https://cdn.example.com/images/image1.jpg',\n        // This is the flag that will make the crawler skip navigation and just fetch the resource\n        skipNavigation: true,\n        userData: {\n            imageFileName: 'image1.jpg',\n            contentType: 'image/jpeg',\n        },\n    },\n]);\n```\n\n----------------------------------------\n\nTITLE: Streaming File Downloads with Node.js and Crawlee\nDESCRIPTION: Implementation of a file download system using Node.js streams and the FileDownload crawler class from Crawlee. The code demonstrates how to download files efficiently, track progress, and store them in a key-value store. When running locally, files are saved to the ./storage/key_value_stores/default directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/file_download_stream.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload } from 'crawlee';\n\nawait FileDownload.download({\n    url: 'https://raw.githubusercontent.com/apify/crawlee/master/README.md',\n    key: 'test.txt',\n    progress: ({ percent }) => console.log(`Downloaded ${percent}%`),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function for Crawlee\nDESCRIPTION: This code wraps the Crawlee logic in an AWS Lambda handler function, ensuring a new crawler instance is created for each Lambda execution to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PuppeteerCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with PuppeteerCrawler for handling proxy rotation and maintaining sessions. The crawler uses Puppeteer to render pages in a real browser while managing sessions automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // Enable automatic proxy IP address rotation\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    proxyConfiguration,\n    // Limits connection for avoiding proxy ban\n    maxConcurrency: 50,\n    // Called for each URL\n    async requestHandler({ request, page, session }) {\n        console.log(`Processing ${request.url}...`);\n\n        try {\n            const title = await page.title();\n            console.log(`URL: ${request.url}, Title: ${title}`);\n\n            session.markGood();\n        } catch (error) {\n            session.markBad();\n            throw error;\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to use Playwright to extract the product SKU from a webpage using a specific CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Proxy Selection Function in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom proxy selection function that decides which proxy to use based on the request URL. It shows conditional logic to use different proxies for different domains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    newUrlFunction: (sessionId, { request }) => {\n        if (request?.url.includes('crawlee.dev')) {\n            return null; // for crawlee.dev, we don't use a proxy\n        }\n\n        return 'http://proxy-1.com'; // for all other URLs, we use this proxy\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: SendRequest API Implementation in TypeScript\nDESCRIPTION: Shows the full implementation of the sendRequest API function. The function wraps gotScraping with request parameters and supports override options for customization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Data to Default Dataset in Crawlee\nDESCRIPTION: This code snippet demonstrates how to save data to the default dataset using Crawlee. It shows how to push multiple items to the dataset and mentions that custom datasets can be created using Dataset.open().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.pushData({\n    id: 1,\n    title: '1st item',\n});\n\nawait Dataset.pushData({\n    id: 2,\n    title: '2nd item',\n});\n\n// Alternatively, you can push multiple items at once\nawait Dataset.pushData([\n    { id: 3, title: '3rd item' },\n    { id: 4, title: '4th item' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with JSDOMCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with JSDOMCrawler to handle IP rotation and session management. Shows configuration of proxy, session pool options, and handling of requests with DOM processing capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new JSDOMCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    // This works directly with JSDOMCrawler\n    useSessionPool: true,\n    // By default this is 3, which means a session is retired\n    // when it gets 3 error (400+) responses.\n    maxSessionUsageCount: 5,\n    // This is the default value and will be used to add\n    // Headers and options to fetch when using a session\n    sessionPoolOptions: {\n        sessionOptions: {\n            // This is used to identify the user agent of the session.\n            // It is used in the default createSessionFunction.\n            // The userAgent is also added automatically to the request\n            // headers when using sessions.\n            maxErrorScore: 1,\n            // This is the default value. It means that sessions with\n            // at least 3 successful responses will be preserved\n            // and reused when the crawler restarts or other sessions\n            // are not available.\n            maxUsageCount: 50,\n        },\n    },\n    async requestHandler({ request, window, response, session, log }) {\n        const { statusCode } = response;\n        const { document } = window;\n  \n        // Based on the status code, mark the session as working, blocked, or failed.\n        if (statusCode === 200) {\n            session.markGood();\n            log.info(`Request ${request.url} succeeded and loaded ${document.title}`);\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with Puppeteer in Crawlee\nDESCRIPTION: Implementation example showing how to crawl multiple URLs using Puppeteer Crawler in Crawlee. Requires apify/actor-node-puppeteer-chrome image for execution on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile for building and running Crawlee projects, optimizing for production deployment by excluding development dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Logic for Crawlee in JavaScript\nDESCRIPTION: Defines routing logic for a Crawlee web scraper. It includes handlers for detail pages, category pages, and a default handler. Each handler performs specific tasks such as data extraction, link enqueueing, and pagination handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Web Pages with CheerioCrawler\nDESCRIPTION: Example code showing how to use CheerioCrawler to perform a recursive crawl of the Crawlee website. CheerioCrawler is a fast HTTP crawler that parses HTML using the Cheerio library.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee logic for extracting data from a page.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/**'],\n            exclude: ['.pdf', '?*']\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This example shows how to implement SessionPool with HttpCrawler to handle proxy rotation and session management. It demonstrates how to handle requests, process responses, and manage sessions while making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration, Dataset } from 'crawlee';\n\nconst crawler = new HttpCrawler({\n    // The default sessionPoolOptions are described in BasicCrawler example\n    // and it will work the same out of the box with HTTPCrawler\n    async requestHandler({ json, request, session }) {\n        // Process the acquired json\n        await Dataset.pushData({\n            url: request.url,\n            json,\n        });\n        \n        // Mark session good so that it is not invalidated in case of an error\n        session!.markGood();\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: ['http://user:password@proxy.com:8000']\n    }),\n});\n\nawait crawler.run(['https://api.example.com/v2/user/1']);\n\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with CheerioCrawler\nDESCRIPTION: Shows how to set up a proxy configuration with CheerioCrawler to use proxies for all crawler requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $ }) {\n        // Process data using Cheerio API\n        // ...\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Main Function for LinkedIn Job Scraper\nDESCRIPTION: Core function that constructs and encodes the LinkedIn job search URL based on user inputs, initializes the PlaywrightCrawler, runs the crawler, and exports the scraped data to a CSV file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom .routes import router\nimport urllib.parse\n\nasync def main(title: str, location: str, data_name: str) -> None:\n    base_url = \"https://www.linkedin.com/jobs/search\"\n\n    # URL encode the parameters\n    params = {\n        \"keywords\": title,\n        \"location\": location,\n        \"trk\": \"public_jobs_jobs-search-bar_search-submit\",\n        \"position\": \"1\",\n        \"pageNum\": \"0\"\n    }\n\n    encoded_params = urlencode(params)\n\n    # Encode parameters into a query string\n    query_string = '?' + encoded_params\n\n    # Combine base URL with the encoded query string\n    encoded_url = urljoin(base_url, \"\") + query_string\n\n    # Initialize the crawler\n    crawler = PlaywrightCrawler(\n        request_handler=router,\n    )\n\n    # Run the crawler with the initial list of URLs\n    await crawler.run([encoded_url])\n\n    # Save the data in a CSV file\n    output_file = f\"{data_name}.csv\"\n    await crawler.export_data(output_file)\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It sets up a crawler that visits multiple URLs, captures screenshots, and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const key = `${(new URL(request.url)).hostname}`;\n        const screenshotBuffer = await page.screenshot();\n        await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Basic Crawlee Crawler Setup in TypeScript\nDESCRIPTION: Demonstrates the initial setup of a CheerioCrawler that crawls a single URL and extracts the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawling Strategy with PlaywrightCrawler\nDESCRIPTION: A comprehensive example showing how to crawl both category and product detail pages. This implementation uses request labels to differentiate between page types and implements pagination handling to navigate through multiple result pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Comparing DOM Manipulation in Cheerio vs Browser JavaScript\nDESCRIPTION: A comparison showing how to extract data from HTML elements using both plain browser JavaScript and Cheerio. The examples demonstrate getting text content from title elements and extracting all href links from a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Cheerio SessionPool Example with CheerioCrawler\nDESCRIPTION: Example showing SessionPool usage with CheerioCrawler for handling HTML parsing and proxy management\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration and Using CheerioCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a custom configuration in Crawlee, set the 'persistStateIntervalMillis' option, and use it with a CheerioCrawler. The crawler handles two different requests, implementing delays and conditional logic based on the URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This code shows how to use JSDOMCrawler to process a list of URLs, load them using HTTP requests, parse the HTML with jsdom, and extract data. It demonstrates crawling multiple pages and extracting the page title and h1 tags from each.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler();\n\nawait crawler.run([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/docs/quick-start' },\n]);\n\ncrawler.addHandler(async ({ window, request, enqueueLinks, log }) => {\n    const { document } = window;\n    const title = document.title;\n    log.info(`Title of ${request.loadedUrl} is: ${title}`);\n\n    // Extract h1 tags\n    const h1Texts = Array.from(document.querySelectorAll('h1'))\n        .map((node) => node.textContent?.trim());\n    log.info(`Found ${h1Texts.length} h1 tags on the page`);\n\n    const data = {\n        url: request.loadedUrl,\n        title,\n        h1Texts,\n    };\n\n    await Dataset.pushData(data);\n\n    // Enqueue all links from the page\n    await enqueueLinks();\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler\nDESCRIPTION: Illustrates sitemap crawling using Playwright Crawler for modern browser automation. Requires apify/actor-node-playwright-chrome image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction Script\nDESCRIPTION: Combined script showing all the scraping logic together for extracting product information from a webpage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Crawling Specific Links with CheerioCrawler in JavaScript\nDESCRIPTION: This code snippet shows how to set up a CheerioCrawler to crawl specific links on a website. It uses the 'globs' property to filter links based on patterns and demonstrates how to process the crawled data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}`);\n\n        // Save results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Enqueue links that match the specified globs\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/docs/**'],\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use the sendRequest function with BasicCrawler to handle requests. The function leverages got-scraping under the hood to mimic browser requests and avoid blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Crawlee\nDESCRIPTION: Basic example of using the enqueueLinks() function in Crawlee to add links to the crawling queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Pattern Matching in Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with URL pattern matching via globs. This simplifies filtering links for crawling by allowing specification of URL patterns that should be followed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Separate Configuration Instance\nDESCRIPTION: JavaScript code showing how to pass a new Configuration instance to a PlaywrightCrawler. This ensures each crawler instance has its own storage and won't interfere with other crawler instances in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function with PlaywrightCrawler\nDESCRIPTION: Complete example showing how to wrap the Crawlee crawler code in an AWS Lambda handler function, which executes the crawler and returns the results as the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler\nDESCRIPTION: A TypeScript example showing how to crawl category pages using PlaywrightCrawler. It uses the enqueueLinks() function with a selector parameter to target specific category links and applies a 'CATEGORY' label to them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n\n        // Only run this logic on the main category listing, not on sub-pages.\n        if (request.label !== 'CATEGORY') {\n\n          // Wait for the category cards to render,\n          // otherwise enqueueLinks wouldn't enqueue anything.\n          await page.waitForSelector('.collection-block-item');\n\n          // Add links to the queue, but only from\n          // elements matching the provided selector.\n          await enqueueLinks({\n              selector: '.collection-block-item',\n              label: 'CATEGORY',\n          });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PlaywrightCrawler - JavaScript\nDESCRIPTION: Guide for implementing SessionPool with PlaywrightCrawler for browser automation\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with HttpCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with HttpCrawler. The crawler will automatically use the configured proxies for all HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This example shows how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes setup for proxy configuration and session pool options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ page, request, session }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler Implementation\nDESCRIPTION: Full implementation of AWS Lambda handler function incorporating Crawlee crawler with AWS-specific configurations and proper response formatting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File using Crawlee\nDESCRIPTION: Example showing how to export an entire dataset to a single CSV file stored in the default key-value store using Dataset.exportToValue function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Export entire dataset into a single CSV file in the default key-value store.\nawait Dataset.exportToValue('RESULTS.csv', {\n    contentType: 'text/csv',\n});\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 with CheerioCrawler\nDESCRIPTION: Example demonstrating how to use a RequestQueueV2 instance with a CheerioCrawler. The experiment must be enabled in the crawler configuration to use the locking queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'SameHostname' enqueue strategy, which is the default.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Crawling!!');\n        await enqueueLinks({\n            // This is the default strategy, no need to specify it\n            // strategy: EnqueueStrategy.SameHostname,\n            // strategy: 'same-hostname',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for proxy usage and session pool management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new BasicCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ session, sendRequest }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // You can use `sendRequest` function which will\n        // correctly retire the session on failed requests\n        const { body } = await sendRequest({\n            url: 'https://example.com',\n            // You can override the session here\n            ...(session.userData.specialSession && {\n                session: session.userData.specialSession,\n            }),\n        });\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Worker Processes\nDESCRIPTION: Main script that spawns multiple worker processes to perform scraping in parallel. It handles communication between parent and child processes, sets up worker-specific configurations, and collects results from all workers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { fork } from 'child_process';\nimport { Configuration } from 'crawlee';\nimport { router } from './routes.mjs';\nimport { getOrInitQueue } from './requestQueue.mjs';\nimport { CheerioCrawler } from '@crawlee/cheerio';\n\n// If we're not a worker thread, we're the main process and should spawn workers\nif (!process.env.IS_WORKER_THREAD) {\n    // How many workers to spawn\n    const workerCount = 2;\n    const workerPromises = [];\n    const scrapedData = [];\n\n    // Spawn the workers\n    for (let i = 0; i < workerCount; i++) {\n        console.log(`Spawning worker ${i}...`);\n\n        const worker = fork(process.argv[1], [], {\n            env: {\n                ...process.env,\n                IS_WORKER_THREAD: 'true',\n                WORKER_INDEX: i.toString(),\n            },\n        });\n\n        // Collect the data from the workers\n        worker.on('message', (data) => {\n            console.log(`Received data from worker ${i}`);\n            scrapedData.push(data);\n        });\n\n        // Create a promise that resolves when the worker exits\n        const workerPromise = new Promise((resolve, reject) => {\n            worker.on('exit', (code) => {\n                if (code === 0) {\n                    console.log(`Worker ${i} exited successfully.`);\n                    resolve();\n                } else {\n                    console.error(`Worker ${i} exited with code ${code}.`);\n                    reject(new Error(`Worker ${i} exited with code ${code}.`));\n                }\n            });\n        });\n\n        workerPromises.push(workerPromise);\n    }\n\n    // Wait for all workers to exit\n    await Promise.all(workerPromises);\n\n    console.log(`All workers exited. Scraped ${scrapedData.length} products.`);\n    console.log('Scraped data:', scrapedData);\n} else {\n    // We're a worker thread, so we should start scraping\n    console.log(`Worker ${process.env.WORKER_INDEX} starting...`);\n\n    // Disable the automatic purge on start\n    // This is needed when running locally, as otherwise multiple processes will try to clear the default storage (and that will cause clashes)\n    Configuration.set('purgeOnStart', false);\n\n    // Get the request queue from the parent process\n    const requestQueue = await getOrInitQueue(false);\n\n    // Configure crawlee to store the worker-specific data in a separate directory (needs to be done AFTER the queue is initialized when running locally)\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    // Create the crawler\n    const crawler = new CheerioCrawler({\n        requestQueue,\n        // Prevent requests to the default queue, as we will be using our own queue\n        requestHandler: router,\n        maxRequestsPerMinute: 20,\n    });\n\n    // Run the crawler\n    await crawler.run();\n\n    console.log(`Worker ${process.env.WORKER_INDEX} finished.`);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This snippet shows how to use SessionPool with PlaywrightCrawler in Crawlee. It sets up the PlaywrightCrawler with SessionPool options and demonstrates session usage in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, session }) {\n        // Use session\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to customize browser fingerprints in PuppeteerCrawler by configuring specific browser parameters. It sets up fingerprint generation to use Chrome browser on macOS with US English locale.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // The default value is true\n        useFingerprints: true,\n        // Fingerprints for the default browser\n        fingerprintOptions: {\n            // Options for the fingerprint generation\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    {\n                        name: 'chrome',\n                        minVersion: 88,\n                    },\n                ],\n                devices: [\n                    'desktop',\n                ],\n                operatingSystems: [\n                    'macos',\n                ],\n                locales: ['en-US', 'en'],\n            },\n        },\n    },\n    // ... other crawler options\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects, optimizing for production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Crawlee TypeScript Projects\nDESCRIPTION: Provides a Dockerfile template for building and running Crawlee projects written in TypeScript, using a multi-stage build process.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Initializing and Exiting Actor in Crawlee (TypeScript)\nDESCRIPTION: Demonstrates how to initialize and exit an Actor using the new Crawlee syntax. This approach uses top-level await with Actor.init() and Actor.exit().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Crawlee Actor\nDESCRIPTION: A Dockerfile that implements a multi-stage build process for a Crawlee actor. The first stage builds the application with development dependencies, while the second stage creates a minimal production image with only required runtime dependencies. It includes optimizations like layer caching for package installation and minimal dependency inclusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:16\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Strategy Implementation\nDESCRIPTION: Implementation of a CheerioCrawler that follows all links found on pages regardless of their domain. Uses the 'all' enqueue strategy to process any discovered URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links found to the queue, regardless of domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using Apify Actor with Crawlee. Shows usage of forceCloud option to explicitly access platform storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with PlaywrightCrawler. It shows how to configure proxy rotation and session management for browser automation with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in TypeScript\nDESCRIPTION: Implementation for crawling multiple specified URLs using the Puppeteer Crawler in Crawlee. This version uses Puppeteer for browser automation, extracting the page title from each site in the list.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, PuppeteerCrawlingContext } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the web pages in a headless Chrome browser.\nconst crawler = new PuppeteerCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerMinute: 100,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // The argument context contains information about the crawler,\n    // the URL, and other utilities.\n    async requestHandler({ page, request, log }: PuppeteerCrawlingContext) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n    },\n});\n\n// These links are from the Crawlee documentation.\nconst usingSiteNavigation = [\n    'https://crawlee.dev/',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/guides/request-storage',\n    'https://crawlee.dev/docs/guides/result-storage',\n];\n\n// Run the crawler with the defined requests.\nawait crawler.run(usingSiteNavigation);\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Example of how to use the default request queue implicitly with a Crawler in Crawlee. The crawler automatically manages the request queue to crawl product links from an e-commerce site.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract product links from the page and add them to the queue\n        // The page is an e-shop category page with products\n        await enqueueLinks({\n            selector: 'a.product-detail-link',\n        });\n    },\n});\n\nawait crawler.addRequests(['https://crawlee.dev']);\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to use Puppeteer Crawler to process multiple URLs. It requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with Selective Link Enqueueing\nDESCRIPTION: A Crawler implementation that selectively enqueues only category links by using a CSS selector. The code waits for category elements to render before finding links with the collection-block-item class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: This snippet demonstrates how to use SessionPool standalone in Crawlee without a crawler. It shows manual session management, including creation, rotation, and retirement of sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool } from 'crawlee';\nimport got from 'got';\n\nconst sessionPool = new SessionPool({\n    maxPoolSize: 25,\n});\n\n// Create a new session\nconst session1 = await sessionPool.getSession();\nconsole.log(session1.id); // ID of the session, e.g. \"session_01\"\n\n// Obtain the session again\nconst session2 = await sessionPool.getSession();\nconsole.log(session2.id); // \"session_01\"\n\n// Create a new session\nconst session3 = await sessionPool.getSession();\nconsole.log(session3.id); // \"session_02\"\n\n// Retire session\nsession1.retire();\n\nconst response = await got('https://api.apify.com/v2/browser-info', {\n    headers: {\n        'User-Agent': session1.userData.userAgent,\n    },\n    proxyUrl: session1.proxyUrl,\n});\n\nconsole.log(response.body);\n\n// Destroy pool\nawait sessionPool.destroy();\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with BasicCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with BasicCrawler to handle IP rotation and session management. Demonstrates setup, request handling, and session marking as working/blocked based on response status codes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new BasicCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    async requestHandler({ request, session, log }) {\n        // Use session in request handler\n        const { statusCode } = await fetch(request.url, {\n            headers: {\n                // Use User-Agent from the session\n                'User-Agent': session.userData.userAgent,\n                // Add cookie based on the session ID\n                'Cookie': `sessionId=${session.id}`,\n            },\n            // Use proxy URLs as defined in proxyConfiguration\n            agent: session.proxyUrl ? new HttpsProxyAgent(session.proxyUrl) : undefined,\n        });\n        \n        // Based on the status code, mark the session as working,\n        // or retire it and throw an error.\n        if (statusCode === 200) {\n            session.markGood();\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Using Request List with PuppeteerCrawler in Crawlee\nDESCRIPTION: Shows how to create and use a Request List with a PuppeteerCrawler in Crawlee. This is useful for crawling a predefined list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use JSDOMCrawler to interact with a React calculator app. It navigates to the calculator, clicks on buttons to perform '1+1=' operation, and extracts the calculation result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerRunScriptSource}\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PuppeteerCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with PuppeteerCrawler. The crawler will automatically use the configured proxies for all browser requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        await page.goto(request.url);\n        // Process page\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with configuration from crawlee.json\nDESCRIPTION: Example code showing how a CheerioCrawler uses configuration from crawlee.json without explicitly importing or passing Configuration to the crawler. The example demonstrates state persistence based on the configured interval.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generator Options with sendRequest\nDESCRIPTION: Shows how to customize browser fingerprint generation by configuring headerGeneratorOptions. This allows for control over which devices, locales, operating systems, and browsers to emulate.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Configuration with CheerioCrawler in JavaScript\nDESCRIPTION: Shows how to create a custom configuration for CheerioCrawler with modified state persistence interval. The example demonstrates handling multiple requests with different sleep durations and includes state persistence configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Implementing JSDOMCrawler for Web Scraping in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to crawl URLs from a file, parse HTML using jsdom, and extract page titles and h1 tags. It utilizes the Crawlee library and includes error handling and logging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent;\n        const h1s = document.querySelectorAll('h1');\n        const h1Texts = Array.from(h1s).map((h1) => h1.textContent);\n\n        log.info(`Title: ${title}`);\n        log.info(`Number of h1 tags: ${h1s.length}`);\n\n        await Dataset.pushData({\n            title,\n            h1Texts,\n        });\n\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nconst main = async () => {\n    const listOfUrls = await readFile('urls.txt', 'utf-8');\n    await crawler.run(listOfUrls.split('\\n'));\n};\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest in BasicCrawler (TypeScript)\nDESCRIPTION: Shows how to use the context.sendRequest() helper to process requests through got-scraping in a BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This example demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It creates a crawler that visits multiple URLs, takes screenshots, and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\nimport { Actor } from 'apify';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshot = await page.screenshot();\n        const key = new URL(request.url).hostname;\n        await Actor.setValue(key, screenshot, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Using Session Management with PuppeteerCrawler and Proxies\nDESCRIPTION: Example of implementing session management with PuppeteerCrawler to maintain consistent proxy usage per session, creating more realistic browsing patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Let's create our crawler\nconst crawler = new PuppeteerCrawler({\n    // useSessionPool enables automatic session management\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ page, session, request }) {\n        // each request is automatically being assigned a random session\n        // and the proxy attached in proxyConfiguration is selected based\n        // on the session id\n        console.log(`Processing: ${request.url}`);\n        console.log(`Using session: ${session.id}`);\n        const title = await page.title();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function handler for CheerioCrawler\nDESCRIPTION: Implementation of an asynchronous handler function that initializes and runs the CheerioCrawler, then returns the crawled data as the Cloud Function response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    return res.send(await crawler.getData())\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using utils.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with the context-aware saveSnapshot() utility. It creates a crawler that visits multiple URLs and uses the saveSnapshot utility to capture and save screenshots.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, crawler }) {\n        await crawler.utils.saveSnapshot({\n            key: request.url,\n            saveHtml: false,\n            saveScreenshot: true,\n            screenshotQuality: 60,\n        });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Requests Per Minute in Crawlee\nDESCRIPTION: Sets the maximum number of requests that can be made per minute to prevent overloading target websites. This example sets the limit to 120 requests per minute.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // We want to do a maximum of 120 requests per minute in total\n    maxRequestsPerMinute: 120,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generation Options for Browser Fingerprinting\nDESCRIPTION: Example showing how to customize the browser fingerprinting behavior by configuring headerGeneratorOptions with specific devices, locales, operating systems, and browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Building Apify Actor Docker Image with Playwright and Chrome\nDESCRIPTION: This Dockerfile creates an optimized image for an Apify actor using Node.js, Playwright, and Chrome. It uses a multi-stage build process to minimize the final image size, installs dependencies, builds the project, and sets up the runtime environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20 AS builder\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY --chown=myuser . ./\n\nRUN npm run build\n\nFROM apify/actor-node-playwright-chrome:20\n\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using Apify's Docker image with Playwright and WebKit pre-installed, suitable for PlaywrightCrawler with WebKit.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_9\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:20\n```\n\n----------------------------------------\n\nTITLE: Getting a Public URL for an Item in Apify Key-Value Store\nDESCRIPTION: Code to store data in a Key-Value Store and obtain a publicly accessible URL for sharing the stored item. This is useful for generating shareable links to data on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Building Basic CheerioCrawler with RequestQueue\nDESCRIPTION: Shows how to create a CheerioCrawler instance with a RequestQueue and request handler. The crawler fetches a webpage and extracts its title using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Exporting an Entire Dataset to CSV File using Crawlee in TypeScript\nDESCRIPTION: This code demonstrates how to export a complete dataset to a single CSV file and save it to the default key-value store. It uses the Dataset and KeyValueStore classes from the Crawlee framework, exports the dataset to CSV format, and then stores the resulting file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\n// Open the default dataset\nawait Dataset.open().then(async (dataset) => {\n    // Export the entire dataset into a single CSV file\n    const csvData = await dataset.exportToValue('csv');\n\n    // Open the default key-value store\n    const store = await KeyValueStore.open();\n\n    // Save the CSV data to the key-value store\n    await store.setValue('OUTPUT.csv', csvData, { contentType: 'text/csv' });\n\n    console.log('Dataset was exported to a single CSV file called OUTPUT.csv in the default key-value store');\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to set up and use SessionPool with JSDOMCrawler in Crawlee. It includes configuration for the session pool and shows how to use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ session, $, window, document, body, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        maxPoolSize: 50,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee before starting a crawler. This ensures a clean state for each crawler run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: PlaywrightCrawler without Waiting (JavaScript)\nDESCRIPTION: Example demonstrating the need for waiting in PlaywrightCrawler. This code will fail because it doesn't wait for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be in the DOM yet\n        const actorText = await page.textContent('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'SameDomain' enqueue strategy to enqueue URLs that share the same domain name, regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add new links from page to the crawling queue\n        // Links that have the same domain name as the startUrls will be crawled,\n        // including links from any subdomain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData in Crawlee\nDESCRIPTION: Code snippet demonstrating how to save extracted data to Crawlee's default Dataset using the pushData() method instead of logging to console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Session Management with PuppeteerCrawler and Proxies\nDESCRIPTION: Demonstrates how to combine session management with proxy configuration in PuppeteerCrawler. Each session will be consistently paired with the same proxy URL to maintain browser identity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true, // This is the default\n    async requestHandler({ session, request, page }) {\n        console.log(`Using session ${session.id}`);\n        console.log(`Fetched ${request.url} with title: ${await page.title()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using saveSnapshot() Utility\nDESCRIPTION: This example demonstrates how to capture screenshots of multiple web pages using the context-aware saveSnapshot() utility within PuppeteerCrawler. The utility automatically handles naming and storing the screenshots in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page, crawler }) {\n        await page.setViewport({ width: 1920, height: 1080 });\n\n        // Use the context-aware saveSnapshot() utility\n        // This automatically saves to the default key-value store\n        // with an automatically generated key\n        const snapshot = await crawler.utils.saveSnapshot(page, {\n            // Override the key generation\n            key: `screenshot-${request.id}`,\n            // Don't save HTML in addition to the screenshot\n            saveHtml: false,\n            // Take a full-page screenshot\n            fullPage: true,\n            // Other puppeteer screenshot options can be added here\n        });\n\n        console.log(`Screenshot of ${request.url} saved to key-value store with key: ${snapshot.key}`);\n    },\n});\n\n// Start the crawler with a list of URLs\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Crawlee Actor with Playwright Chrome\nDESCRIPTION: Complete Dockerfile that sets up a Crawlee actor using a multi-stage build process. The first stage builds the application, and the second stage creates a production-ready image with minimal dependencies. The configuration includes proper file permissions, dependency installation, and startup commands for headful browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links using CheerioCrawler\nDESCRIPTION: This snippet shows how to crawl only links from the same hostname as the starting URL. This is the default strategy for enqueueLinks() and will only follow links pointing to the same hostname (but not subdomains).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        // Only add links from the same hostname to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n            // The selector can be omitted, and defaults to `a[href]`\n            selector: 'a[href]',\n        });\n    },\n});\n\n// Start with one request\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee Projects\nDESCRIPTION: TypeScript configuration file (tsconfig.json) that extends Apify's base configuration, enabling ES2022 features including top-level await and configuring the project structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Session Management with BasicCrawler\nDESCRIPTION: Example showing how to use SessionPool with BasicCrawler for handling proxy rotations and session management. The code demonstrates setup and configuration of BasicCrawler with session management capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{BasicSource}\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Request Queue with Locking Support in Crawlee\nDESCRIPTION: A utility function to get or initialize a request queue with locking support. It ensures the queue is initialized and optionally clears it if needed. This shared queue will be used by all parallel scraper instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n/**\n * Utility function to get the request queue\n * @param {boolean} clearQueue Whether the queue should be cleared\n * @returns {Promise<RequestQueue>} The request queue\n */\nexport async function getOrInitQueue(clearQueue = false) {\n    const queue = await RequestQueue.open('requests-with-locks', {\n        lockTimeoutSecs: 1800, // How long a lock will remain valid for an in-progress request\n    });\n\n    // If we want to clear, we need to purge the queue\n    if (clearQueue) {\n        await queue.drop();\n    }\n\n    return queue;\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables. Shows the usage of forceCloud option for accessing platform storages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Crawling Websites with CheerioCrawler in TypeScript\nDESCRIPTION: Example of using CheerioCrawler to crawl URLs from a file, parse HTML with Cheerio, and extract page titles and h1 tags. Uses HTTP requests for data fetching and demonstrates basic web scraping patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readFile } from 'fs/promises';\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Add URLs to a RequestList\nconst sourceUrls = await downloadListOfUrls({ url: 'https://example.com/my-url-list.txt' });\n\n// Create a CheerioCrawler\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Extract all h1 texts\n        const h1texts = [];\n        $('h1').each((index, el) => {\n            h1texts.push($(el).text());\n        });\n        log.info(`H1 texts: ${h1texts.join(', ')}`);\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(sourceUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Unified API for Cookie Management Across Browser Types\nDESCRIPTION: Shows how BrowserPool provides a unified API for common operations like cookie management that work the same way for both Playwright and Puppeteer, abstracting away their implementation differences.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// Playwright\nconst cookies = await context.cookies();\nawait context.addCookies(cookies);\n\n// Puppeteer\nconst cookies = await page.cookies();\nawait page.setCookie(...cookies);\n\n// BrowserPool uses the same API for all plugins\nconst cookies = await browserController.getCookies(page);\nawait browserController.setCookies(page, cookies);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Puppeteer Page.screenshot() Method\nDESCRIPTION: This snippet demonstrates how to use Puppeteer directly to capture a screenshot of a web page using the page.screenshot() method. It launches a browser, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { launchPuppeteer } from 'crawlee';\n\nawait Actor.init();\n\nconst url = 'https://apify.com';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\nawait page.goto(url);\n\nconst screenshotBuffer = await page.screenshot();\n\n// The key is extracted from the URL\nconst key = url.replace(/[:/]/g, '_');\n\n// Save the screenshot to the default key-value store\nawait Actor.setValue(key, screenshotBuffer, { contentType: 'image/png' });\nconsole.log(`Screenshot of ${url} saved as ${key}`);\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a website and enqueue all links found, regardless of domain, using the 'all' strategy. It initializes a CheerioCrawler and uses the enqueueLinks() method to find and process links on the website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // You can also use the string form\n            // strategy: 'all',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\n// Start the crawler with initial request\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with HttpCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use HttpCrawler to crawl URLs from an external file, make HTTP requests, and save the HTML content. It includes error handling and uses the Dataset class to store results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    // Let's limit the number of concurrency to not overload the server\n    maxConcurrency: 5,\n    // Function called for each URL\n    async requestHandler({ request, body, $ }) {\n        // Save results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request }) {\n        console.log(`Request ${request.url} failed twice.`);\n    },\n});\n\n// Load URLs from a text file\nconst urls = await readFile('urls.txt', 'utf8');\n\n// Add URLs to crawler's request queue\nawait crawler.addRequests(urls.trim().split('\\n'));\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to use PlaywrightCrawler to crawl category pages of a Shopify store. It uses enqueueLinks with a specific selector to target category links and labels the requests for later processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n\n        // Only run this logic on the main category listing, not on sub-pages.\n        if (request.label !== 'CATEGORY') {\n\n          // Wait for the category cards to render,\n          // otherwise enqueueLinks wouldn't enqueue anything.\n          await page.waitForSelector('.collection-block-item');\n\n          // Add links to the queue, but only from\n          // elements matching the provided selector.\n          await enqueueLinks({\n              selector: '.collection-block-item',\n              label: 'CATEGORY',\n          });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router - JavaScript\nDESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler and configures logging levels. The crawler uses a router for request handling instead of direct if-clause based routing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with HttpCrawler\nDESCRIPTION: Shows how to integrate ProxyConfiguration with HttpCrawler for handling HTTP requests through proxies, implementing a simple request handler that logs data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ json, request }) {\n        console.log(`Processing: ${request.url}`);\n        console.log(`Data: ${JSON.stringify(json)}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json\nDESCRIPTION: Example of a crawlee.json configuration file that sets persistence interval and log level options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating HTTP Server with CheerioCrawler for Web Scraping\nDESCRIPTION: Combines the HTTP server and CheerioCrawler to handle incoming requests, scrape web pages, and return page titles. It uses request queues and manages responses through a Map.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, RequestQueue, log } from 'crawlee';\nimport { createServer } from 'http';\n\nconst requestQueue = await RequestQueue.open();\nconst responseMap = new Map();\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        const response = responseMap.get(request.id);\n        if (response) {\n            response.writeHead(200, { 'Content-Type': 'application/json' });\n            response.end(JSON.stringify({ title }));\n            responseMap.delete(request.id);\n        }\n    },\n});\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    const url = new URL(req.url, `http://${req.headers.host}`);\n    const urlToScrape = url.searchParams.get('url');\n\n    if (!urlToScrape) {\n        res.writeHead(400, { 'Content-Type': 'text/plain' });\n        res.end('Missing URL parameter\\n');\n        return;\n    }\n\n    const request = await requestQueue.addRequest({ url: urlToScrape });\n    responseMap.set(request.id, res);\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating Express Server Handler for GCP Cloud Run\nDESCRIPTION: Implements an Express HTTP server that wraps the Playwright crawler for Cloud Run deployment. Includes port configuration and stateless request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport express from 'express';\nconst app = express();\n\nconst startUrls = ['https://crawlee.dev'];\n\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    return res.send(await crawler.getData());\n});\n\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawling with Crawlee in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Crawlee's BasicCrawler to crawl web pages, send HTTP requests, and store the results. It uses the sendRequest utility to fetch page content and saves the raw HTML and URL to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    // Function called for each URL\n    async requestHandler({ sendRequest, log }) {\n        const { body } = await sendRequest();\n        log.info(`HTML length: ${body.length}`);\n\n        // Save data to default dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\n// Run the crawler\nawait crawler.run([\n    'http://example.com/page-1',\n    'http://example.com/page-2',\n    'http://example.com/page-3',\n]);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News using PuppeteerCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to scrape Hacker News. It starts with the main page, extracts article information, and follows pagination links. The results are stored in the default dataset. It uses Puppeteer for browser automation and Crawlee for managing the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PuppeteerCrawler, RequestQueue } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestQueue to add more URLs to crawl.\n    requestQueue: await RequestQueue.open(),\n\n    // Here you can set options that are passed to the Puppeteer browser instance.\n    launchContext: {\n        // For example, use headless mode (default) or turn it off.\n        // launchOptions: {\n        //     headless: true,\n        // },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 50,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Puppeteer scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Puppeteer within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link from each post on Hacker News.\n            $posts.forEach(($post) => {\n                scrapedData.push({\n                    title: $post.querySelector('.title a').innerText,\n                    rank: $post.querySelector('.rank').innerText,\n                    href: $post.querySelector('.title a').href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        const infos = await page.$$eval('.morelink', (links) => links.map((link) => ({ href: link.href })));\n        for (const info of infos) {\n            await enqueueLinks({\n                urls: [info.href],\n                label: 'LIST',\n            });\n        }\n    },\n\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times.`);\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(['https://news.ycombinator.com/']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example showing how to customize browser fingerprints in PlaywrightCrawler by specifying browser, operating system, and other parameters to avoid detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            // Generate fingerprints based on Mac OS\n            operatingSystems: ['macos'],\n            // Use only Chrome browser\n            browsers: ['chrome'],\n            // Set language to US english\n            locales: ['en-US'],\n            // Use specific browser versions\n            browserVersions: {\n                chrome: ['100', '101', '102', '103']\n            }\n        }\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Cleaning Default Storages in Crawlee\nDESCRIPTION: Shows how to explicitly purge default storage directories to clean up before a crawler run. This removes all default results storage directories except the INPUT key in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in HttpCrawler\nDESCRIPTION: Shows how to access and inspect information about the currently used proxy in HttpCrawler's requestHandler function. This is useful for debugging or logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json, proxyInfo }) {\n        console.log(proxyInfo.url); // http://proxy-1.com\n        console.log(proxyInfo.hostname); // proxy-1.com\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function for Crawlee\nDESCRIPTION: This code wraps the Crawlee logic in a handler function, which is the entry point for AWS Lambda execution. It creates a new crawler instance for each Lambda invocation to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Using KeyValueStore Public URLs\nDESCRIPTION: Example showing how to get public URLs for items stored in Apify KeyValueStore.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Session Management with HttpCrawler\nDESCRIPTION: Implementation of session management using HttpCrawler, showing how to handle HTTP requests with session pooling and proxy rotation. Includes configuration for HTTP-specific session handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{HttpSource}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Puppeteer Utils saveSnapshot() Method\nDESCRIPTION: This example shows how to use the utils.puppeteer.saveSnapshot() utility function to take a screenshot. This method simplifies the process by handling the screenshot capture and storage in a single function call.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { launchPuppeteer, utils } from 'crawlee';\n\nawait Actor.init();\n\nconst url = 'https://apify.com';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\nawait page.goto(url);\n\n// Save a screenshot using the utility function\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nconsole.log(`Screenshot of ${url} saved as my-screenshot`);\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Initialize Playwright Crawler for Link Crawling\nDESCRIPTION: Demonstrates Playwright-based crawler implementation for website link crawling. Requires apify/actor-node-playwright-chrome image for deployment. Leverages modern browser automation to discover and process links across pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Proxy Configuration in Crawlee\nDESCRIPTION: Basic example of setting up proxy configuration with custom proxy URLs and getting a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Session Management with CheerioCrawler and Proxies\nDESCRIPTION: Shows how to use session management with proxy configuration in CheerioCrawler. Each session will be consistently paired with the same proxy URL to maintain a persistent identity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true, // This is the default\n    async requestHandler({ session, request, $ }) {\n        console.log(`Using session ${session.id}`);\n        console.log(`Fetched ${request.url} with title: ${$('title').text()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK with API Token\nDESCRIPTION: JavaScript code showing how to initialize the Apify SDK with an API token using Configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Setting Apify Token via Configuration\nDESCRIPTION: Example of configuring the Apify SDK with an API token programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in TypeScript\nDESCRIPTION: Shows the internal implementation of the sendRequest function, which uses got-scraping to send requests. It includes default options and allows for overriding with custom options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Installing puppeteer-extra and stealth plugin\nDESCRIPTION: Command for installing puppeteer-extra and puppeteer-extra-plugin-stealth packages using npm package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Creating an Express Server Wrapper for Crawlee on GCP Cloud Run\nDESCRIPTION: Implements an Express HTTP server that handles incoming requests, runs the crawler, and returns crawled data. This setup is necessary for GCP Cloud Run which expects containers to expose an HTTP endpoint on the specified PORT environment variable.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Actor with Apify CLI\nDESCRIPTION: Commands to create a new actor project and run it locally using Apify CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue for Batch URL Processing in Crawlee\nDESCRIPTION: Demonstrates how to use only Request Queue for batch processing URLs, leveraging the addRequests method to efficiently add multiple URLs at once.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Define the starting URLs\nconst initialRequests = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Initialize Request Queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add multiple requests to the queue at once\nawait requestQueue.addRequests(initialRequests);\n\n// Create a crawler that will use the requestQueue\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // optionally we could have used the `startUrls` option here\n    \n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // extract links and add them to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Playwright Crawler\nDESCRIPTION: Implementation of a web crawler using Playwright to navigate and extract all links from a website. Uses enqueueLinks() to automatically add discovered links to the RequestQueue while handling modern web applications.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Function called for each URL\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing new URLs...');\n        // Add new URLs to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl\nconst startUrls = await downloadListOfUrls({ url: 'https://example.com' });\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with JSDOMCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with JSDOMCrawler for handling proxy rotation and maintaining sessions. The crawler processes HTML content using JSDOM library while managing sessions automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new JSDOMCrawler({\n    // Enable automatic proxy IP address rotation\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    proxyConfiguration,\n    // Limits connection for avoiding proxy ban\n    maxConcurrency: 50,\n    // Called for each URL\n    async requestHandler({ request, response, window, session }) {\n        console.log(`Processing ${request.url}...`);\n\n        if (response.statusCode !== 200) {\n            session.markBad();\n            throw new Error(`Request ${request.url} failed with status code ${response.statusCode}`);\n        }\n\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent;\n        console.log(`URL: ${request.url}, Title: ${title}`);\n\n        session.markGood();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Input Schema JSON Definition\nDESCRIPTION: Example of a JSON schema file that can be imported into the actor. This defines the expected structure and validation rules for the actor's input.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/motivation.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"properties\": {\n        \"startUrls\": {\n            \"type\": \"array\",\n            \"editor\": \"stringList\",\n            \"description\": \"URLs to start with\",\n            \"prefill\": [\n                \"https://crawlee.dev\"\n            ]\n        },\n        \"maxItems\": {\n            \"type\": \"integer\",\n            \"description\": \"Maximum number of items\",\n            \"default\": 10\n        }\n    },\n    \"required\": [\"startUrls\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: This code demonstrates how to extract a product's SKU (Stock Keeping Unit) from a webpage using Playwright. It targets a span element with the specific class 'product-meta__sku-number'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet illustrates the usage of SessionPool with CheerioCrawler in Crawlee. It shows how to configure the crawler with proxy rotation and session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ $, request, response, session }) {\n        const title = $('title').text();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // We can mark the session as blocked if we receive an access denied page\n        if (response.statusCode === 403) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile configuration for building and running Crawlee projects, optimizing for production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to completely disable browser fingerprint generation in PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PlaywrightCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with PlaywrightCrawler. The crawler will automatically use the configured proxies for all browser requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        await page.goto(request.url);\n        // Process page\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'same-domain' enqueue strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links from page to RequestQueue\n        // only if they are from the same domain\n        // as the page that is currently open.\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links using CheerioCrawler\nDESCRIPTION: This snippet demonstrates crawling links from the same domain including subdomains. It starts from a specific URL and will follow links to the same domain including any subdomains, making it useful for crawling larger sites with multiple subdomains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        // Add links from the same domain to RequestQueue, including subdomains\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // The selector can be omitted, and defaults to `a[href]`\n            selector: 'a[href]',\n        });\n    },\n});\n\n// Start with one request\nawait crawler.run(['https://subdomain.example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Scraping with Crawlee's BasicCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Crawlee's BasicCrawler to scrape web pages. It initializes a crawler, defines a request list with URLs to scrape, and implements a handler function to process each page. The script downloads HTML content and stores it along with the URL in a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler that uses raw\n// HTTP requests to download web pages. The crawler will automatically parse\n// the HTML using the Cheerio library.\nconst crawler = new BasicCrawler({\n    // Let's limit our crawls to only 10 requests\n    maxRequestsPerCrawl: 10,\n\n    // Here we set up a crawling queue with the list of URLs to crawl.\n    // Instead of the `RequestList` we could also use `RequestQueue`,\n    // which downloads the URLs dynamically based on the crawler's\n    // actual needs.\n    requestList: await RequestList.open('my-list', [\n        { url: 'http://www.example.com/page-1' },\n        { url: 'http://www.example.com/page-2' },\n        { url: 'http://www.example.com/page-3' },\n        // ...\n        { url: 'http://www.example.com/page-100' },\n    ]),\n\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with options as:\n    // https://crawlee.dev/api/basic-crawler/interface/BasicCrawlingContext\n    async requestHandler({ sendRequest, request, log }) {\n        log.debug(`Processing ${request.url}...`);\n\n        // We use sendRequest() instead of Axios/Fetch/etc. to be able to\n        // take advantage of the automatic browser rotation and retries.\n        const { body } = await sendRequest();\n\n        // Store the HTML and URL to the default dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n\n        log.debug(`${request.url} succeeded.`);\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler in TypeScript\nDESCRIPTION: Implementation of a PlaywrightCrawler that visits a starting URL and enqueues only category links using a specific CSS selector and label. The code waits for elements to render before attempting to find links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to set up SessionPool with JSDOMCrawler in Crawlee. It includes proxy configuration and shows how to handle session-specific data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ window, request, response, session }) {\n        const title = window.document.querySelector('title').textContent;\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // We can mark the session as working if we successfully received the content\n        session.markGood();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing playwright-extra and stealth plugin dependencies\nDESCRIPTION: Command to install the required dependencies for using playwright-extra with the puppeteer stealth plugin via npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Using PlaywrightCrawler with Browser Context\nDESCRIPTION: Example of using PlaywrightCrawler to scrape data from a website. This demonstrates how to work with the browser context, interact with page elements, and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Browser context is available in the preNavigationHooks\n    preNavigationHooks: [\n        async ({ browserController, request }) => {\n            // We can modify browser settings before navigation\n            const context = browserController.getContext();\n            // Set geolocation, locale or permissions here\n        }\n    ],\n    async requestHandler({ request, page, enqueueLinks }) {\n        // Wait for the content to load\n        await page.waitForSelector('.content');\n        \n        // Extract data from the page\n        const title = await page.title();\n        const description = await page.$eval('meta[name=\"description\"]', (el) => el.getAttribute('content'));\n        \n        // Take a screenshot\n        await page.screenshot({ path: `${request.id}.png` });\n        \n        // Continue crawling through the website\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests in PlaywrightCrawler\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, identify image URLs, and fetch them directly without navigation. It utilizes the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\nimport { Actor } from 'apify';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, sendRequest, log }) {\n        // Extract all image URLs from the page\n        const imageUrls = await page.evaluate(() => {\n            return Array.from(document.querySelectorAll('img'))\n                .map(img => img.src)\n                .filter(src => src.startsWith('http'));\n        });\n\n        // Process each image URL\n        for (const imageUrl of imageUrls) {\n            log.info(`Processing image: ${imageUrl}`);\n\n            // Create a new request with skipNavigation set to true\n            const imageRequest = new Request({\n                url: imageUrl,\n                skipNavigation: true, // This is the key option\n            });\n\n            // Use sendRequest to fetch the image data\n            const response = await sendRequest(imageRequest);\n            const buffer = await response.buffer();\n\n            // Save the image to the default key-value store\n            const store = await KeyValueStore.open();\n            await store.setValue(imageUrl, buffer, { contentType: response.headers['content-type'] });\n        }\n    },\n});\n\nawait Actor.main(async () => {\n    await crawler.run(['https://example.com']);\n});\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-rendered website using Crawlee with Puppeteer\nDESCRIPTION: This JavaScript code demonstrates how to use Crawlee with Puppeteer to scrape a JavaScript-rendered website (Apify Store). It showcases Crawlee's flexibility in supporting different headless browser engines.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page }) {\n        await page.waitForSelector('.ActorStoreItem-title-wrapper');\n        const actorText = await page.$eval('.ActorStoreItem-title-wrapper', (el) => {\n            return el.textContent;\n        });\n        await crawler.pushData({\n            'actor': actorText\n        });\n    },\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with HttpCrawler in JavaScript/TypeScript\nDESCRIPTION: This example shows how to use HttpCrawler to load URLs from a text file, make HTTP requests to each URL, and save the HTML content. It demonstrates configuration of the crawler, request handling, and data saving.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// First we get the URLs from our text file\nconst urlsText = await readFile('./urls.txt', { encoding: 'utf-8' });\n// Split the text file by newlines and filter out empty rows\nconst urls = urlsText\n    .split('\\n')\n    .map((url) => url.trim())\n    .filter((url) => url.length > 0);\n\n// Create an instance of the HttpCrawler class\nconst crawler = new HttpCrawler({\n    // Instead of enabling `persistCookiesPerSession` or using `SessionPool` as with `CheerioCrawler` or `PlaywrightCrawler`,\n    // it's sufficient to set the `useSessionPool: true` option to maintain cookies between requests to same hostname\n    useSessionPool: true,\n\n    // Here we can set options for this specific crawler\n    // We need to override `maxRequestRetries`, because we know we want some 404s,\n    // which would be retried by default\n    maxRequestRetries: 0,\n\n    async requestHandler({ body, request, parseWithCheerio }) {\n        // `body` contains the raw HTML string of the page.\n        // We can use `parseWithCheerio()` to create a Cheerio object from the HTML\n        // and then use it to extract data from the page.\n        const $ = parseWithCheerio();\n        const data = {\n            url: request.url,\n            title: $('title').text(),\n        };\n\n        // Save the data in the default dataset\n        await Dataset.pushData(data);\n    },\n});\n\n// Enqueue start URLs and run the crawler\nawait crawler.run(urls);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Playwright Crawler\nDESCRIPTION: Implementation of a Playwright-based crawler for automated website link collection. Shows how to configure Playwright for web crawling with request handling and data storage capabilities. Includes specific configuration details for the Apify Platform deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawling to only 10 requests\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Save the HTML elements containing the text 'Example Domain'\n        await Dataset.pushData({\n            url: request.url,\n            title: await page.title(),\n        });\n\n        // Extract all links from the page and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Cheerio Crawler Link Enqueuing\nDESCRIPTION: TypeScript implementation using Cheerio Crawler to crawl all links on a website. Uses enqueueLinks() method to add new links to RequestQueue with a limit of 100 requests per crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // The crawler will automatically process all enqueued links\n    maxRequestsPerCrawl: 100,\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Processing...');\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Session Management with HttpCrawler\nDESCRIPTION: Demonstrates how to manage proxy sessions in HttpCrawler using the SessionPool feature. This ensures consistent proxy-session mapping for better crawling patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    sessionPoolOptions: {\n        persistStateKeyValueStoreId: 'my-session-pool',\n        persistStateKey: 'http-crawler-sessions',\n    },\n    async requestHandler({ request, response, session, body, log }) {\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json\nDESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. This file should be placed in the root of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: Shows how to integrate SessionPool with PlaywrightCrawler for managing sessions and proxy rotations. It includes configuration for proxy, session pool, and demonstrates handling of requests and blocked sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    // The `useSessionPool` is enabled by default\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ page, request, session }) {\n        const title = await page.title();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // Check if you got blocked here and invalidate the session\n        if (await isBlocked(page)) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a website and only enqueue links that point to the same hostname as the starting URL. This is the default strategy for enqueueLinks() and restricts crawling to the same hostname without including subdomains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all same-hostname links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n            // You can also use the string form\n            // strategy: 'same-hostname',\n        });\n\n        // Or even shorter, as it's the default strategy:\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\n// Start the crawler with initial request\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Cheerio Crawler to process a list of URLs. It creates a new Cheerio Crawler instance, defines URL handling logic, and executes the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Standalone Usage of SessionPool in Crawlee\nDESCRIPTION: Illustrates how to use SessionPool independently without a crawler for manual session management. It demonstrates creating a session pool, adding sessions, and using them for requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool } from 'crawlee';\n\n// Create a session pool\nconst sessionPool = new SessionPool({\n    maxPoolSize: 25,\n});\n\n// Add proxy URLs to the pool\nconst proxyUrls = [\n    'http://proxy-1',\n    'http://proxy-2',\n    'http://proxy-3',\n];\n\nfor (const proxyUrl of proxyUrls) {\n    await sessionPool.addSession({ sessionOptions: { proxyUrl } });\n}\n\n// Use a session from the pool\nconst session = await sessionPool.getSession();\n\n// Make a request using the session\nconst response = await fetch('https://example.com', {\n    proxy: session.proxyUrl,\n});\n\n// Check if the request was successful\nif (response.ok) {\n    // Mark the session as good\n    session.markGood();\n} else {\n    // Mark the session as bad\n    session.markBad();\n}\n\n// Return the session to the pool\nawait sessionPool.returnSession(session);\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Details from Amazon using TypeScript and Cheerio\nDESCRIPTION: This snippet defines a function to extract basic product details from an Amazon product page using Cheerio selectors. It includes type definitions and constant selectors for various product fields.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioAPI } from 'cheerio';\n\ntype ProductDetails = {\n    title: string;\n    price: string;\n    listPrice: string;\n    reviewRating: string;\n    reviewCount: string;\n};\n\n/**\n * CSS selectors for the product details. Feel free to figure out different variations of these selectors.\n */\nconst SELECTORS = {\n    TITLE: 'span#productTitle',\n    PRICE: 'span.priceToPay',\n    LIST_PRICE: 'span.basisPrice .a-offscreen',\n    REVIEW_RATING: '#acrPopover a > span',\n    REVIEW_COUNT: '#acrCustomerReviewText',\n} as const;\n\n/**\n * Scrapes the product details from the given Cheerio object.\n */\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\n    const title = $(SELECTORS.TITLE).text().trim();\n\n    const price = $(SELECTORS.PRICE).first().text();\n    const listPrice = $(SELECTORS.LIST_PRICE).first().text();\n    const reviewRating = $(SELECTORS.REVIEW_RATING).first().text();\n    const reviewCount = $(SELECTORS.REVIEW_COUNT).first().text();\n\n    return { title, price, listPrice, reviewRating, reviewCount };\n};\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet illustrates how to use Playwright Crawler to download and process URLs from a sitemap. It uses the downloadListOfUrls utility to fetch sitemap URLs and enqueues them for crawling with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new PlaywrightCrawler({\n    // ProxyConfiguration is optional\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        await enqueueLinks();\n    },\n});\n\nconst listOfUrls = await downloadListOfUrls({ url: 'https://crawlee.dev/sitemap.xml' });\nawait crawler.addRequests(listOfUrls);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Setting API Token using Configuration Object\nDESCRIPTION: JavaScript example showing how to authenticate with the Apify platform by creating an Actor instance with your API token in the configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy your local actor code to the Apify platform using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Proxy List with Null Option in JavaScript\nDESCRIPTION: Creates a ProxyConfiguration with a list of proxy URLs including a null option, which means some requests will be made without a proxy. Crawlee rotates through this list in round-robin fashion.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n        null // null means no proxy is used\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with Custom Proxy Configuration in TypeScript\nDESCRIPTION: Example showing how to override the default proxy settings when using sendRequest. This demonstrates passing a specific proxy URL for making requests through a proxy server.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler with Direct URL Addition\nDESCRIPTION: This simplified version of the CheerioCrawler example uses the crawler's built-in methods to handle the RequestQueue automatically. It demonstrates how to add URLs directly when running the crawler without manually creating a RequestQueue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Setting Apify Token in Configuration Object\nDESCRIPTION: Example of providing your Apify API token through a Configuration instance, which allows you to access Apify platform features programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to select all anchor elements with href attributes on a page and extract the href values into an array using Cheerio's selector, map, and get methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee Project to Apify Platform\nDESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Direct Puppeteer Usage\nDESCRIPTION: Demonstrates how to capture a webpage screenshot using Puppeteer's page.screenshot() method directly. The screenshot is saved to a key-value store with a key derived from the page URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { KeyValueStore } from '@crawlee/core';\n\nawait Actor.init();\n\nconst url = 'https://example.com';\nconst page = await browser.newPage();\nawait page.goto(url);\n\n// Create a key from the URL\nconst key = url.replace(/[:/]/g, '_');\n\n// Capture the screenshot\nconst screenshot = await page.screenshot();\n\n// Save the screenshot to the default key-value store\nawait KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Custom Configuration in GCP\nDESCRIPTION: Basic setup of a PlaywrightCrawler with disabled storage persistence for GCP Cloud Run. The configuration is adjusted for serverless environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Strategy Implementation\nDESCRIPTION: Example showing how to crawl all links on a website regardless of their domain using the 'all' strategy with CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links from page to queue, regardless of their domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generation with sendRequest in TypeScript\nDESCRIPTION: Demonstrates how to configure browser fingerprint generation using headerGeneratorOptions. This allows customizing the device types, locales, operating systems, and browsers used for generating headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: SendRequest Implementation Details\nDESCRIPTION: Shows the internal implementation of the sendRequest function with its configuration options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Crawling Context in SDK v1\nDESCRIPTION: Example showing the new unified crawling context approach in SDK v1, where all handlers share the same context object.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler for Crawlee\nDESCRIPTION: This complete example shows how to wrap Crawlee initialization and execution in an AWS Lambda handler function. It properly configures the crawler with AWS Chromium settings and returns the scraped data in the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor\nDESCRIPTION: Dockerfile that sets up a Node.js environment for running Crawlee actors. It uses a multi-stage build process to optimize caching, installs production NPM dependencies, and configures the runtime environment. The build process excludes development and optional dependencies to keep the image size minimal.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-Rendered Content with PlaywrightCrawler\nDESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content from Apify Store. Playwright automatically waits for elements to appear, making it easier to extract dynamically loaded content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        await page.waitForSelector('.ActorStoreItem');\n\n        // Extract text content of the first actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to use CheerioCrawler to crawl all links on a website, including those that go off-site. It uses the 'All' enqueue strategy to enqueue any URLs found during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links from page to the crawling queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // globs: [\"http?(s)://example.com/**\"], // <= Use this option to limit to specific domains\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy the local actor code to the Apify platform using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator Using JSDOMCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to interact with a React calculator application, performing button clicks and extracting results. It navigates to a calculator demo, enters '1+1=', and extracts the calculation result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    // Scrolls the page after loading to render all lazy-loaded elements\n    runScripts: true,\n});\n\nawait crawler.run([\n    // Start with a URL of our app\n    'https://ahfarmer.github.io/calculator/',\n]);\n\ncrawler.addHandler(async ({ window, enqueueLinks, log }) => {\n    // Get page's title\n    const { document } = window;\n    log.info(`The title of the page is: ${document.title}`);\n\n    // Click \"1 + 1 =\"\n    document.querySelector('.component-button:nth-of-type(4)')?.click(); // 1\n    document.querySelector('.component-button:nth-of-type(19)')?.click(); // +\n    document.querySelector('.component-button:nth-of-type(4)')?.click(); // 1\n    document.querySelector('.component-button:nth-of-type(20)')?.click(); // =\n\n    log.info(`1 + 1 = ${document.querySelector('.component-display')?.textContent}`);\n\n    // Save results\n    await Dataset.pushData({\n        title: document.title,\n        result: document.querySelector('.component-display')?.textContent,\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: Demonstrates how to use SessionPool independently without a crawler for manual session management and proxy rotation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst sessionPool = await SessionPool.open({\n    maxPoolSize: 100,\n});\n\n// Somewhere in your code\nconst session = await sessionPool.getSession();\nconst proxyUrl = await proxyConfiguration.newUrl(session.id);\n\nconsole.log(proxyUrl); // 'http://proxy-1.com'\nconsole.log(session.id); // '1'\n\nconst newSession = await sessionPool.getSession();\nconsole.log(newSession.id); // '2'\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to configure and use SessionPool with BasicCrawler to rotate proxy IPs and manage sessions. It shows how to create a session pool, configure proxy settings, and handle request failures.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, Session } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    // To use the SessionPool,\n    // we need to use SessionPool's session and getSession in our requestHandler.\n    async requestHandler({ session }) {\n        // We need to ensure the session is restored otherwise we lose it. This is the same as saving a session,\n        // but we don't need to modify it now so we only restore it.\n        session!.markGood();\n    },\n\n    // The default sessionPoolOptions are as follows\n    sessionPoolOptions: {\n        /**\n         * The maximum size of the pool. Indicates how many sessions are rotated.\n         */\n        maxPoolSize: 100,\n\n        /**\n         * SessionPool periodically passes a Session from the pool into this function\n         * so that you can decide if it's acceptable.\n         */\n        sessionOptions: {\n            maxUsageCount: 50,\n\n            /**\n             * You can define a custom predicate from your crawler\n             * to determine whether a session should be retired.\n             */\n            maxErrorScore: 1,\n\n            /**\n             * Sometimes, a session gets retired, but it's still a valid session that just doesn't\n             * fit our criteria. This function allows a handling mechanism for these sessions\n             * otherwise useful sessions that don't, for example, pass a maxUsageCount check.\n             */\n            // eslint-disable-next-line @typescript-eslint/no-unused-vars\n            sessionRetiredCallback: (session: Session): void => undefined,\n        },\n\n        /**\n         * Create a new session. This function is called when there are no available sessions.\n         * If it does not provide a valid session object or function, geen one will be created\n         * automatically\n         */\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        createSessionFunction: async (_sessionPool): Promise<Session | undefined> => undefined,\n\n        /**\n         * You can optionally supply a function to generate proxies globally for all sessions.\n         * This function is called for each session's creation and renewal.\n         */\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        proxyConfiguration: undefined as any,\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    //proxyConfiguration: new ProxyConfiguration({\n    //    proxyUrls: ['http://user:password@proxy.com:8000']\n    //}),\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Docker and Package.json Configuration for Playwright\nDESCRIPTION: Best practice configuration showing Dockerfile with proper versioning and package.json with wildcard dependencies for pre-installed packages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs Pattern in Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs within a PlaywrightCrawler request handler. This allows for targeting specific URL patterns without needing to manually pass the requestQueue or page instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Repository Search Forms with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to automate GitHub search form completion and submission using PuppeteerCrawler. It navigates to GitHub, fills in search parameters such as term, owner, date, and language, submits the form, and extracts search results to a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{CrawlSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to set up and use SessionPool with PuppeteerCrawler in Crawlee. It includes configuration for proxy usage and session pool management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ page, request, session }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: Illustrates the setup of SessionPool with PuppeteerCrawler for managing sessions and proxy rotations in Puppeteer-based crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request, page }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content with Cheerio\nDESCRIPTION: A simple example showing how to use Cheerio to find the first h2 element on a page and extract its text content using the text() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management in Crawlee\nDESCRIPTION: Example of using SessionPool directly without a crawler for manual session management. This approach allows for more granular control over session usage and proxy rotation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool, ProxyConfiguration, gotScraping } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the SessionPool directly\nconst sessionPool = await SessionPool.open({\n    // Define session options\n    sessionOptions: {\n        maxUsageCount: 5,\n    },\n    // Let's have 10 sessions in our pool\n    maxPoolSize: 10,\n});\n\n// Assign proxy configuration to the session pool\nsessionPool.setProxyConfiguration(proxyConfiguration);\n\nconst url = 'https://example.com/';\n\n// Get the session from the pool\nconst session = await sessionPool.getSession();\n\ntry {\n    // Use the gotScraping library with the session\n    const response = await gotScraping({\n        url,\n        proxyUrl: session.proxyUrl,\n        // Use session's cookie jar for this request\n        cookieJar: session.cookieJar,\n        // Other options like custom headers\n        headers: {\n            'User-Agent': session.userData.userAgent,\n        },\n    });\n\n    console.log(`URL: ${url}, Status code: ${response.statusCode}`);\n\n    // This step is extremely important!\n    // Mark the session as good so it can be used again\n    session.markGood();\n} catch (error) {\n    // If the request failed, mark the session as bad\n    session.markBad();\n    throw error;\n} finally {\n    // Whether the request succeeded or failed, we need to retire the session\n    session.retire();\n}\n\n// We can now destroy the pool\nawait sessionPool.destroy();\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: This snippet shows how to extract the manufacturer name from a product URL by splitting the string and accessing specific parts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in JSDOMCrawler\nDESCRIPTION: Shows how to access and use proxy information in JSDOMCrawler to track which proxy server is handling each request, useful for debugging and monitoring.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request }) {\n        if (proxyInfo) {\n            // Format of proxyInfo.url: <protocol>://<username>:<password>@<hostname>:<port>\n            console.log(`Processing ${request.url} using proxy ${proxyInfo.url}`);\n        } else {\n            console.log(`Processing ${request.url} directly`);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with Cheerio in Crawlee\nDESCRIPTION: Implementation example showing how to crawl multiple URLs using Cheerio Crawler in Crawlee. Uses Cheerio for HTML parsing which is suitable for static content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This snippet shows how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes session pool setup and demonstrates how to use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ session, page, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        maxPoolSize: 50,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler\nDESCRIPTION: Implementation of a crawler that targets specific category links using selectors and labels. Demonstrates waiting for elements and selective link enqueuing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Image URLs with Cheerio in TypeScript\nDESCRIPTION: Function to extract product image URLs from an Amazon product page using Cheerio selectors. It maps over all image elements and extracts the src attribute from each one.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst SELECTORS = {\n    ...\n    IMAGES: '#altImages .item img',\n} as const;\n\n/**\n * Extracts the product image URLs from the given Cheerio object.\n * - We have to iterate over the image elements and extract the `src` attribute.\n */\nconst extractImageUrls = ($: CheerioAPI): string[] => {\n    const imageUrls = $(SELECTORS.IMAGES)\n        .map((_, imageEl) => $(imageEl).attr('src'))\n        .get(); // `get()` - Retrieve all elements matched by the Cheerio object, as an array. Removes `undefined` values.\n\n    return imageUrls;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Skip Navigation with PlaywrightCrawler\nDESCRIPTION: Example showing how to use Request#skipNavigation with sendRequest to fetch and save CDN-hosted images without full crawler navigation. The code demonstrates setting up a PlaywrightCrawler and handling both regular page crawling and direct resource fetching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, sendRequest, log }) {\n        // Normal page processing\n        if (!request.skipNavigation) {\n            const title = await page.title();\n            log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n            // Find all images on the page\n            const images = await page.$$eval('img', (imgs) => {\n                return imgs.map((img) => img.src);\n            });\n\n            // Queue image URLs for processing, with skipNavigation enabled\n            await crawler.addRequests(\n                images.map((url) => ({\n                    url,\n                    skipNavigation: true,\n                    label: 'image',\n                })),\n            );\n            return;\n        }\n\n        // Direct image processing\n        const imageBuffer = await sendRequest({ responseType: 'buffer' });\n        await KeyValueStore.setValue(\n            request.url.split('/').pop()!,\n            imageBuffer,\n            { contentType: 'image/jpeg' },\n        );\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to find all anchor elements with href attributes and extract those URLs into an array using Cheerio's selector, map, and get methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Creating an Express HTTP Server for Crawlee on GCP Cloud Run\nDESCRIPTION: Wraps a Crawlee crawler in an Express HTTP server to handle requests from GCP Cloud Run. The server listens on the port specified by GCP's environment variable and returns the crawler's data as the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Docker configuration showing how to specify the Node.js version in Apify Docker images to ensure compatibility and prevent breaking changes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 16\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Working with Datasets in Crawlee\nDESCRIPTION: Demonstrates how to use Dataset class to store structured data. Shows how to push individual records or multiple rows at once to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor to Apify Platform\nDESCRIPTION: Command to deploy an actor to the Apify platform using CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest() in BasicCrawler\nDESCRIPTION: Shows how to use the context.sendRequest() helper for processing requests through got-scraping. This replaces the deprecated requestAsBrowser method and allows for easy HTTP requests with customizable options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler\nDESCRIPTION: Implementation of a crawler that enqueues all discovered links regardless of their domain. Uses the 'all' strategy in enqueueLinks() to process any URLs found during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // This will enqueue all discovered links regardless of domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to combine Request Queue and Request List in Crawlee for scenarios where you have a large initial list of URLs and need to add more dynamically during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Define a list of initial URLs\nconst sources = [\n    { url: 'http://example.com/start-1' },\n    { url: 'http://example.com/start-2' },\n    { url: 'http://example.com/start-3' },\n    // ... potentially thousands more ...\n];\n\n// Initialize both RequestList and RequestQueue\nconst requestList = await RequestList.open('my-list', sources);\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Initialize the crawler with both RequestList and RequestQueue\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    // Assuming we want to limit max requests for this example\n    maxRequestsPerCrawl: 100,\n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n        // Crawl the links from the page and add them to the RequestQueue\n        await enqueueLinks();\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating an Express HTTP server wrapper for Crawlee crawler in GCP Cloud Run\nDESCRIPTION: Complete implementation of an Express server that wraps a PlaywrightCrawler to handle HTTP requests in GCP Cloud Run. The server listens on the port specified by GCP's PORT environment variable and returns crawler data in the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to customize browser fingerprints in PlaywrightCrawler by setting specific browser and operating system parameters. It uses the fingerprints configuration to mimic Chrome browser on macOS with US English locale.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // The default value is true\n        useFingerprints: true,\n        // Fingerprints for the default browser\n        fingerprintOptions: {\n            // Options for the fingerprint generation\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    {\n                        name: 'chrome',\n                        minVersion: 88,\n                    },\n                ],\n                devices: [\n                    'desktop',\n                ],\n                operatingSystems: [\n                    'macos',\n                ],\n                locales: ['en-US', 'en'],\n            },\n        },\n    },\n    // ... other crawler options\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels with enqueueLinks\nDESCRIPTION: Demonstrates how to use the Request.label shortcut for categorizing requests, combined with enqueueLinks for efficient URL processing and classification.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Files with Crawlee's FileDownload Crawler\nDESCRIPTION: Example script that demonstrates downloading various file types (images, PDFs) using Crawlee's FileDownload crawler class. The files are downloaded via HTTP requests and stored in the default key-value store, which in local configuration maps to './storage/key_value_stores/default'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/file_download.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, RequestError } from 'crawlee';\nimport type { Dictionary } from '@crawlee/types';\n\n// Extend the HttpCrawler with a utility method for file downloads\nclass FileDownload extends HttpCrawler {\n    protected readyToInitiateFetch: boolean = true;\n\n    constructor(...args: ConstructorParameters<typeof HttpCrawler>) {\n        super(...args);\n    }\n\n    /**\n     * The 'downloadFile' method downloads and stores a URL to a file on a specified key-value store.\n     * @param url URL of the file to download\n     * @param options Options specifying where to store the file\n     */\n    async downloadFile(\n        url: string,\n        options: {\n            /**\n             * Unique key that identifies the file in the key-value store.\n             */\n            key: string;\n            /**\n             * If set, the function would add additional HTTP header to the request.\n             */\n            headers?: Dictionary<string>;\n            /**\n             * If set, the function will check if the file was already downloaded. If it was, it will not be\n             * downloaded again and the function will return null. \n             * This parameter is optional, default is false.\n             */\n            checkExists?: boolean;\n        },\n    ) {\n        const { key, headers = {}, checkExists = false } = options;\n        \n        try {\n            // Check if the file already exists in the key-value store\n            if (checkExists) {\n                const existingFile = await this.useRequestQueue().client.keyValueStore().getRecord(key);\n                if (existingFile) {\n                    return null;\n                }\n            }\n\n            // Create a Request object with the provided URL and headers\n            const request = {\n                url,\n                headers,\n                responseType: 'buffer' as const,\n            };\n\n            // Execute the request and get the response\n            const response = await this.httpClient.call(request);\n            const buffer = response.body;\n\n            // Store the downloaded file buffer in the key-value store\n            await this.useRequestQueue().client.keyValueStore().setRecord({\n                key,\n                value: buffer,\n                contentType: response.headers['content-type'],\n            });\n\n            return {\n                key,\n                buffer,\n            };\n        } catch (err) {\n            if (err instanceof RequestError) {\n                this.log.exception(err, `Failed to download file: ${url}`);\n            } else {\n                throw err;\n            }\n        }\n    }\n}\n\n// Initiate the custom FileDownload crawler\nconst crawler = new FileDownload({\n    // Define the maximum concurrency limit to 1 as an example\n    maxConcurrency: 1,\n});\n\n// Start the crawler and define what it should do\nawait crawler.run(async () => {\n    // Example: Download a few images and a PDF\n    await crawler.downloadFile('https://apify.com/favicon.ico', { key: 'favicon.ico' });\n    await crawler.downloadFile('https://sdk.apify.com/img/actors.png', { key: 'actors.png' });\n    await crawler.downloadFile('https://apify.com/robots.txt', { key: 'robots.txt' });\n    // Use key to store file under different name\n    await crawler.downloadFile('https://sdk.apify.com/img/hp_policymakers.svg', { key: 'policy.svg' });\n    await crawler.downloadFile('https://apify.com/misc/Exploring-the-Internet-with-Crawlee.pdf', { key: 'crawlee.pdf' });\n    \n    // Download a file only if it doesn't exist yet\n    await crawler.downloadFile('https://apify.com/misc/Exploring-the-Internet-with-Crawlee.pdf', {\n        key: 'crawlee-exists.pdf',\n        checkExists: true,\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Playwright\nDESCRIPTION: Commands to install Crawlee with Playwright dependency for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with persistStorage disabled\nDESCRIPTION: Updates the CheerioCrawler initialization to use a separate Configuration instance with persistStorage set to false, which is required for cloud environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return in JavaScript\nDESCRIPTION: This snippet shows the complete AWS Lambda handler function for a Cheerio Crawler, including crawler initialization, execution, and returning the scraped data as the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Basic Request Handling with BasicCrawler\nDESCRIPTION: Demonstrates how to use BasicCrawler with the sendRequest function for basic HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Request List in Crawlee\nDESCRIPTION: Shows how to create and use a Request List with a Crawler in Crawlee. This is useful for processing a known list of URLs without dynamic additions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: Example demonstrating how to use JSDOMCrawler to process URLs from an external file, make HTTP requests, and extract page title and h1 tags using jsdom DOM implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerSource}\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Demonstrates how to customize browser fingerprint settings in PlaywrightCrawler including browser type, version, and operating system specifications.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nnew PlaywrightCrawler({\n    browserPoolOptions: {\n        // Choose a specific fingerprint combination\n        fingerprintOptions: {\n            // Allows to define a specific browser\n            browsers: [{\n                name: 'chrome',\n                minVersion: 90,\n            }],\n            // Use only specified operating systems\n            operatingSystems: ['windows', 'linux'],\n            // Limit the generated fingerprints by the device\n            devices: ['desktop'],\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PuppeteerCrawler\nDESCRIPTION: Shows how to access proxy information within PuppeteerCrawler's request handler. This helps track which proxy is being used for each browser instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, page }) {\n        // Prints information about the proxy used for the request\n        if (proxyInfo) {\n            console.log(proxyInfo.url);\n        }\n        \n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a list of URLs using Puppeteer Crawler. It sets up a RequestQueue with multiple URLs, configures the crawler to process each page by extracting the title using browser automation, and stores the results using Dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create a request queue\nconst requestQueue = await RequestQueue.open();\n\n// Add multiple URLs to the queue\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://apify.com' },\n    { url: 'https://sdk.apify.com' },\n]);\n\n// Create a crawler\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        // Extract data from the page\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the data to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to set up and use SessionPool with JSDOMCrawler in Crawlee. It covers proxy configuration and session management within the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, window, body }) {\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to utilize Request Queue implicitly with a Crawler in Crawlee, where the crawler automatically manages the request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// The crawler will automatically initialize and use the default request queue\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        // Extract data from the page\n        // ...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add first requests to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Data Return\nDESCRIPTION: Final implementation including data return functionality from the crawler execution with proper AWS Lambda response format.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Extracting URL and Manufacturer from Product URL in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the URL and manufacturer information from a product URL using string manipulation in JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot Using Puppeteer's page.screenshot()\nDESCRIPTION: Direct implementation of screenshot capture using Puppeteer's built-in screenshot method. The code launches a browser, navigates to a URL, and saves the screenshot to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from '@crawlee/core';\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://apify.com');\n\nconst screenshotBuffer = await page.screenshot();\nconst key = 'screenshot-' + (new Date().toISOString());\n\nawait KeyValueStore.setValue(key, screenshotBuffer, {\n    contentType: 'image/png',\n});\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Puppeteer Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to use PuppeteerScraper to crawl all links on a website. It opens a headless browser, navigates to each page, extracts the title with JavaScript, and uses enqueueLinks() to add new discovered links to the request queue, limiting to 100 requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerScraper } from 'crawlee';\n\n// Create an instance of the PuppeteerScraper class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerScraper({\n    // Let's limit our crawls to make the example quick to run.\n    maxRequestsPerCrawl: 100,\n});\n\n// Crawlers come with various utilities, such as request queue support.\nawait crawler.run([\n    // Tell the crawler to start with this URL\n    'https://crawlee.dev',\n], {\n    // This function will be called for each URL\n    async requestHandler({ request, enqueueLinks, log, page }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Extract data from the page\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Add all links from the page to the crawling queue\n        await enqueueLinks();\n    },\n});\n\nlog.info('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 for Request Locking\nDESCRIPTION: Example showing how to create and use a RequestQueueV2 instance that supports request locking. This replaces the standard RequestQueue import with RequestQueueV2.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with PuppeteerCrawler. It shows how to configure proxy rotation and session management for browser automation with Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Project for Apify Platform\nDESCRIPTION: Modified main script that includes Apify Actor initialization and cleanup for cloud deployment. Implements proper storage handling and platform integration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Using a Custom HTTP Client with BasicCrawler in TypeScript\nDESCRIPTION: Example showing how to instantiate a custom HTTP client and pass it to a BasicCrawler constructor. This allows replacing the default got-scraping implementation with a custom one.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from '@crawlee/basic';\nimport { FetchHttpClient } from './fetch-http-client';\n\nconst crawler = new BasicCrawler({\n    httpClient: new FetchHttpClient(),\n    async requestHandler({ request, sendRequest }) {\n        // The `sendRequest` function will use your custom client\n        const response = await sendRequest({\n            url: request.url,\n            method: 'GET',\n        });\n\n        console.log(response.body);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: Shows how to set up SessionPool with HttpCrawler, including proxy configuration and session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request, body }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links using CheerioCrawler\nDESCRIPTION: Example showing how to crawl all links on a website regardless of domain using the 'all' enqueue strategy. Uses CheerioCrawler to process discovered URLs and log their titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Add all discovered links to the queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in TypeScript\nDESCRIPTION: This code snippet shows the implementation of the sendRequest API. It uses got-scraping to make HTTP requests with various options including URL, method, headers, proxy, and cookie handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL\nDESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string at appropriate points.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1);\nconst manufacturer = urlPart[0].split('-')[0];\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI tool, which sets up all necessary dependencies and adds boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Puppeteer Crawler\nDESCRIPTION: Implementation of a Puppeteer-based crawler that follows and processes all links within a website's subdomain. Uses the enqueueLinks() method with Puppeteer for browser-based crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ enqueueLinks, page, log }) {\n        log.info('enqueueing new URLs');\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to use SessionPool with BasicCrawler in Crawlee. It sets up a SessionPool with custom options and uses it in the BasicCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ session, sendRequest }) {\n        // Use session\n        const { body } = await sendRequest({\n            useExtendedUniqueKey: true,\n            session,\n        });\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Targeted Category Page Crawling with PlaywrightCrawler\nDESCRIPTION: Demonstrates how to crawl category pages by using a specific CSS selector to target only relevant links. It also shows how to apply labels to categorize the requests being enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links Strategy\nDESCRIPTION: Implementation demonstrating how to crawl links from the same hostname using CheerioCrawler. This is the default strategy that matches relative URLs and URLs pointing to the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{SameHostnameSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler\nDESCRIPTION: This code snippet demonstrates how to crawl multiple specified URLs using the Cheerio Crawler in Crawlee. It sets up a request list with predefined URLs, runs the crawler to process each URL, and extracts the page title from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawler Implementation\nDESCRIPTION: Full implementation of a crawler that handles both category and product detail pages. Includes pagination handling and differentiation between page types using request labels.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawler Implementation\nDESCRIPTION: Full implementation of a crawler that handles both category and product detail pages. Uses request labels to differentiate between page types and includes pagination handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with CheerioCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with CheerioCrawler. The crawler will automatically use the configured proxies for all HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, body }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Using PuppeteerCrawler with Explicit Element Waiting\nDESCRIPTION: This code shows how to scrape JavaScript-rendered content using PuppeteerCrawler, which requires explicit waiting for elements to appear in the DOM. It properly extracts text from actor cards on Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// PuppeteerCrawler will properly extract JavaScript-rendered content\n// but we need to wait for elements to appear manually\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // We need to wait for the selector or else\n        // the element might not be in the DOM yet\n        await page.waitForSelector('.ActorStoreItem');\n        const actorText = await page.evaluate(() => {\n            return document.querySelector('.ActorStoreItem').textContent;\n        });\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return\nDESCRIPTION: This is the final version of the AWS Lambda handler function. It initializes a CheerioCrawler, runs it, and returns the scraped data as the Lambda function's response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee using CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country\nDESCRIPTION: Demonstrates how to create an Apify Proxy configuration with specific proxy groups and country selection. This allows targeting connections from specific locations for better scraping performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Browser Dependencies\nDESCRIPTION: Commands to install Apify SDK v1 with either Puppeteer or Playwright as browser automation dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Example showing how to disable browser fingerprinting functionality in PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Disable fingerprints completely\n        useFingerprints: false\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler with SendRequest Implementation\nDESCRIPTION: Demonstrates how to use the sendRequest helper with BasicCrawler for making HTTP requests using got-scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'SameDomain' enqueue strategy to process URLs from the same domain during the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing links with same domain.');\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            selector: 'a[href]',\n        });\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Intercepting GraphQL Persisted Queries with mitmproxy\nDESCRIPTION: This Python script for mitmproxy intercepts GraphQL requests and replaces the SHA256 hash with an invalid value. When the server returns a 'PersistedQueryNotFound' error, the client will resend the complete query text, allowing you to capture the full GraphQL query for scraping purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/11-15-graphql-persisted-query/index.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef request(flow):\n    try:\n        dat = json.loads(flow.request.text)\n        dat[0][\"extensions\"][\"persistedQuery\"][\"sha256Hash\"] = \"0d9e\" # any bogus hex string here\n        flow.request.text = json.dumps(dat)\n    except:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Limiting Crawlee Crawler Requests\nDESCRIPTION: Setting a maximum limit on the number of requests a Crawlee crawler can make using the maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Custom Configuration\nDESCRIPTION: Setting up CheerioCrawler with disabled storage persistence for cloud function compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Fetching JSON Response with BasicCrawler in TypeScript\nDESCRIPTION: This snippet shows how to configure BasicCrawler to fetch and parse JSON responses. It demonstrates setting the responseType option to 'json' when making a request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl all links on a website. It sets up the crawler, initializes a request queue, and processes each page by extracting the title, storing data, and enqueueing new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to use Puppeteer Crawler to download and process URLs from a sitemap. It uses the downloadListOfUrls utility to fetch sitemap URLs and enqueues them for crawling with Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new PuppeteerCrawler({\n    // ProxyConfiguration is optional\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        await enqueueLinks();\n    },\n});\n\nconst listOfUrls = await downloadListOfUrls({ url: 'https://crawlee.dev/sitemap.xml' });\nawait crawler.addRequests(listOfUrls);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Providing Input to a Crawlee Actor via INPUT.json\nDESCRIPTION: This snippet shows the file path structure for providing input to an actor through an INPUT.json file in the default key-value store. This is commonly used during local development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Scraping Dynamic Content with PuppeteerCrawler\nDESCRIPTION: This snippet demonstrates using PuppeteerCrawler to scrape JavaScript-rendered content, explicitly waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        await page.waitForSelector('.ActorStoreItem');\n        // Extract text content of the first actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It configures logging and runs the crawler on a specific URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Project\nDESCRIPTION: This Dockerfile sets up a Node.js environment for a Crawlee project. It uses the apify/actor-node:20 base image, installs dependencies efficiently with layer caching, copies the source code, and configures the container to run the project with npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio\nDESCRIPTION: Implementation of sitemap crawling using Cheerio Crawler, which is a fast and lightweight solution for HTML parsing. The code utilizes the Sitemap utility class from @crawlee/utils to process sitemap URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Capturing Multiple Screenshots with PuppeteerCrawler using saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware saveSnapshot() utility. It creates a crawler that visits multiple URLs and uses the saveSnapshot() function to capture and save screenshots.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log, saveSnapshot }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await saveSnapshot();\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Example for JavaScript-rendered Content\nDESCRIPTION: A simple example showing why CheerioCrawler fails to extract content from JavaScript-rendered websites. This crawler attempts to scrape actor cards from Apify Store but returns empty results because the content is rendered client-side.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This example shows how to integrate SessionPool with HttpCrawler in Crawlee. It covers proxy configuration and session management within the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, body, response }) {\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Managing Browser Lifecycle with Hooks\nDESCRIPTION: Example demonstrating how to use pre-launch and post-page-create hooks to manage the browser and page lifecycle, including the use of page IDs for tracking.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n    browserPlugins: [\n        new PlaywrightPlugin(playwright.chromium),\n    ],\n    preLaunchHooks: [(pageId, launchContext) => {\n        // You can use pre-launch hooks to make dynamic changes\n        // to the launchContext, such as changing a proxyUrl\n        // or updating the browser launchOptions\n\n        pageId === 'my-page' // true\n    }],\n    postPageCreateHooks: [(page, browserController) => {\n        // It makes sense to make global changes to pages\n        // in post-page-create hooks. For example, you can\n        // inject some JavaScript library, such as jQuery.\n\n        browserPool.getPageId(page) === 'my-page' // true\n    }]\n});\n\nawait browserPool.newPage({ id: 'my-page' });\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to crawl URLs from an external file, load each page using HTTP requests, and parse the HTML using jsdom. It extracts the page title and all h1 tags from each page and stores the data in a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readFile } from 'node:fs/promises';\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the JSDOMCrawler class\nconst crawler = new JSDOMCrawler({\n    // Let's limit the crawling to only 10 requests\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ window, request, enqueueLinks, log }) {\n        const { document } = window;\n        log.info(`Processing ${request.url}...`);\n\n        // A function to extract data from the page\n        const title = document.querySelector('title')?.textContent ?? '';\n        const h1Texts = Array.from(document.querySelectorAll('h1'))\n            .map((el) => el.textContent?.trim());\n\n        // Store the results to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1Texts,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue\n        await enqueueLinks();\n    },\n});\n\n// Read the list of URLs from a text file\nconst urls = (await readFile('urls.txt', 'utf8'))\n    .trim()\n    .split('\\n')\n    .map((url) => url.trim());\n\n// Run the crawler with initial list of URLs\nawait crawler.run(urls);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler in Crawlee\nDESCRIPTION: Shows how to use PlaywrightCrawler to crawl category pages, waiting for elements to render and enqueueing specific links using selectors and labels.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Crawlee (JavaScript)\nDESCRIPTION: This code snippet demonstrates how to save data to the default dataset in Crawlee. It uses the Dataset.pushData() method to add items to the dataset. If the dataset doesn't exist, it will be automatically created.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.pushData({\n    title: 'Noname',\n    url: 'https://example.com',\n    modifiedDate: new Date(),\n});\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: Demonstrates the configuration of SessionPool with JSDOMCrawler for handling sessions and proxy rotations in JSDOM-based crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request, window, document, body }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced autoscaledPoolOptions in Crawlee\nDESCRIPTION: Demonstrates how to use advanced autoscaling options in CheerioCrawler to fine-tune crawler performance, including scaling ratios, intervals, and concurrency parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // other configuration options\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.9,\n        scaleUpStepRatio: 0.1,\n        scaleDownStepRatio: 0.1,\n        maybeRunIntervalSecs: 1,\n        loggingIntervalSecs: 30,\n        autoscaleIntervalSecs: 15,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU\nDESCRIPTION: Demonstrates getting the product SKU using a specific class selector with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: NPM installation commands for different Crawlee crawler types including necessary dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Cheerio Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to use Cheerio Crawler to process sitemap URLs. It imports the Sitemap utility, sets up a CheerioCrawler, and handles the download and processing of sitemap entries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add all links from the current page to the crawling queue.\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n});\n\n// Download and parse sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Get URLs from a sitemap\nconst urls = sitemap.getUrls();\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Basic SessionPool Example with BasicCrawler\nDESCRIPTION: Example showing how to use SessionPool with BasicCrawler for managing proxy sessions and handling request failures\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{BasicSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler in TypeScript\nDESCRIPTION: This code shows how to implement a sitemap crawler using Puppeteer for browser automation. It downloads the sitemap XML, extracts URLs, and then uses Puppeteer to visit each page, collecting the title and meta description while handling JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Prepare the crawler configuration\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, log, pushData }) {\n        // Wait for the content to load\n        await page.waitForSelector('title');\n\n        // Extract title and meta description\n        const title = await page.title();\n        const h1 = await page.$eval('h1', (el) => el.textContent);\n        const metaDescription = await page.$eval('meta[name=description]', (el) => el.getAttribute('content'));\n\n        const metadata = {\n            url: request.url,\n            title,\n            h1,\n            metaDescription,\n        };\n\n        log.info('Page scraped', metadata);\n\n        // Store the results\n        await pushData(metadata);\n    },\n});\n\n// Create Sitemap instance\nconst sitemap = new Sitemap({ urls: startUrls });\n\n// Download and process sitemap\nawait sitemap.downloadAllSitemaps();\n\n// Get URLs from the sitemap\nconst urls = sitemap.getSitemapUrls().map((item) => ({ url: item.url }));\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability Using Playwright\nDESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of a specific element. The code uses filtering and counting to verify if a 'In stock' indicator exists on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Web Crawler with BasicCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use BasicCrawler to download multiple web pages using HTTP requests. It shows request handling, processing the response, and storing the results to the default dataset. The crawler visits several URLs, extracts their HTML content, and saves it along with the URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that just downloads the resources and doesn't do any special handling.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, sendRequest, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n\n        // Here you can use the sendRequest utility to get the HTML\n        // of the page. Unlike more advanced crawlers, this one\n        // doesn't use browser automation, so the returned HTML\n        // is what you would get with a simple curl command.\n        const response = await sendRequest();\n\n        await Dataset.pushData({\n            url,\n            html: response.body,\n            // The HTTP response status code\n            status: response.statusCode,\n        });\n    },\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/docs/quick-start' },\n    { url: 'https://crawlee.dev/docs/guides/apify-platform' },\n    { url: 'https://crawlee.dev/docs/guides/authentication' },\n    { url: 'https://crawlee.dev/docs/guides/proxy-management' },\n]);\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example\nDESCRIPTION: Shows the file structure for request queue storage on disk using CRAWLEE_STORAGE_DIR environment variable\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Setting Apify Token via Configuration Instance\nDESCRIPTION: Example of configuring an Apify actor with an API token using the Configuration instance, allowing programmatic authentication with the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Stage Build for Crawlee Actor with Node.js\nDESCRIPTION: This Dockerfile implements a multi-stage build process for a Crawlee actor. It first creates a builder stage to compile the TypeScript code, then creates a production image with only the necessary files and dependencies. The build process is optimized for Docker layer caching and minimizes the final image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with JSON Response Type in TypeScript\nDESCRIPTION: Example of configuring sendRequest to handle JSON responses instead of the default text/HTML response type. This is useful when working with JSON APIs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling with PlaywrightCrawler\nDESCRIPTION: Example of recursive crawling of the Crawlee website using PlaywrightCrawler. It extracts the title of each page and saves the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CheerioCrawler\nDESCRIPTION: Command for manually installing Crawlee with npm, which includes CheerioCrawler for HTML parsing using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer Chrome Image\nDESCRIPTION: Using the Docker image that includes Puppeteer and Chrome browser, compatible with CheerioCrawler and PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP Cloud Function handler with CheerioCrawler\nDESCRIPTION: Wraps the crawler execution in an exported handler function that accepts request and response objects, which is the required format for GCP Cloud Functions. The function returns crawled data in the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with Custom Cookie Jar in TypeScript\nDESCRIPTION: Example showing how to use a custom Cookie Jar with sendRequest for managing cookies across requests. This uses the tough-cookie package for cookie management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with JSDOMCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with JSDOMCrawler. The crawler will automatically use the configured proxies for all HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, document, body }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Specifying Playwright Version in Docker Image\nDESCRIPTION: Demonstrates how to use a specific version of Playwright (1.10.0-beta) with Node.js 16 in the Apify actor-node-playwright-chrome Docker image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Automated GitHub Form Submission with PuppeteerCrawler\nDESCRIPTION: Example code showing how to use PuppeteerCrawler to automate GitHub repository search by filling and submitting forms. The crawler handles search parameters like term, owner, date, and language, then processes and stores the results in a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PuppeteerCrawler, log } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page }) {\n        // Fill and submit the form\n        await page.goto('https://github.com/search/advanced');\n\n        // Fill form\n        await page.type('#search_terms', 'crawler');\n        await page.type('#search_from_owner', 'apify');\n        await page.type('#search_date', '>2015');\n        await page.select('#search_language', 'JavaScript');\n\n        // Submit form\n        await Promise.all([\n            page.waitForNavigation(),\n            page.click('.btn-primary'),\n        ]);\n\n        // Extract the data\n        const repos = await page.$$eval('.repo-list-item', (elements) => {\n            const repoData = [];\n            \n            for (const el of elements) {\n                const titleElement = el.querySelector('.f4');\n                const descriptionElement = el.querySelector('.mb-1');\n                const metadataElements = el.querySelectorAll('.text-small');\n                \n                const title = titleElement ? titleElement.textContent.trim() : '';\n                const description = descriptionElement ? descriptionElement.textContent.trim() : '';\n                const metadata = {};\n                \n                for (const metaEl of metadataElements) {\n                    const text = metaEl.textContent.trim();\n                    if (text.includes('Updated')) metadata.updated = text;\n                    if (text.includes('')) metadata.stars = text;\n                }\n                \n                repoData.push({\n                    title,\n                    description,\n                    ...metadata,\n                });\n            }\n            \n            return repoData;\n        });\n\n        // Save the data\n        await Dataset.pushData(repos);\n    },\n});\n\nawait crawler.run(['https://github.com/search/advanced']);\n\nlog.info('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler for Cheerio Crawler in JavaScript\nDESCRIPTION: This code demonstrates how to wrap the Cheerio Crawler logic in an AWS Lambda handler function, ensuring a new crawler instance is created for each invocation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Configuration\nDESCRIPTION: Docker configuration for Node.js with Playwright and WebKit browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with Custom Selector in TypeScript\nDESCRIPTION: This snippet shows how to use the enqueueLinks function with a custom selector to find specific links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to crawl all links on a website using Puppeteer Crawler. It sets up a crawler with Puppeteer browser automation, handles requests by enqueueing links, and captures screenshots of each page. This approach is useful when JavaScript rendering is required.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Here you can set options that are passed to the launchPuppeteer() function.\n    launchContext: {\n        launchOptions: {\n            headless: true,\n            // Other Puppeteer options\n        },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee configuration and handlers\n    // that will be used for each of the requests.\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Puppeteer within the browser context.\n        const pageTitle = await page.title();\n        log.info(`Title of ${request.url}: ${pageTitle}`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n\n        // Save a screenshot of the current page to the default key-value store\n        // under the OUTPUT tab in the app.\n        await page.screenshot({\n            path: `${request.url.replace(/[^a-zA-Z0-9]/g, '')}.jpg`,\n            fullPage: true,\n        });\n\n        // Save the HTML content of the page to dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: await page.content()\n        });\n    },\n});\n\n// Add first URL to a RequestQueue\nconst url = 'https://apify.com';\nawait crawler.run([url]);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Website Links with Puppeteer Crawler in Crawlee\nDESCRIPTION: This example demonstrates how to crawl all links on a website using Puppeteer Crawler in Crawlee. It configures a PuppeteerCrawler to navigate pages, uses enqueueLinks() to add new discovered links to the request queue, and extracts the title from each page using browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, enqueueLinks } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n    maxConcurrency: 10, // Maximum concurrency\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // This function is called for every page the crawler visits.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract desired data from the page using Puppeteer.\n\n        // This enqueues all links on the page that pass the filter function.\n        // The function loads the URLs using the same options as the crawler\n        await enqueueLinks();\n    },\n});\n\n// Start the crawler and wait for it to finish.\nawait crawler.run(['https://crawlee.dev']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Apify Actor\nDESCRIPTION: Commands to create and run a new Apify actor project using the CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Using CookieJar with BasicCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use a CookieJar with BasicCrawler for managing cookies. It shows how to create a CookieJar instance and pass it to the sendRequest function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Request Queue with Locking Support in JavaScript\nDESCRIPTION: This code creates a function to initialize or retrieve a request queue that supports request locking. It uses the Crawlee RequestQueue class and handles both local and Apify Platform environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\nimport { Actor } from 'apify';\n\nexport const getOrInitQueue = async (purge = false) => {\n    const requestQueue = await RequestQueue.open();\n\n    if (purge) {\n        if (Actor.isAtHome()) {\n            await requestQueue.drop();\n        } else {\n            await requestQueue.drop();\n            await requestQueue.delete();\n        }\n    }\n\n    return requestQueue;\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This snippet shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ $, request, session }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Meta-Package\nDESCRIPTION: Command to install the complete Crawlee meta-package that contains all crawler implementations and utilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Implementing ErrorSnapshotter in JavaScript\nDESCRIPTION: New feature to implement ErrorSnapshotter for error context capture.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nErrorSnapshotter\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Playwright Crawler in Crawlee\nDESCRIPTION: This code demonstrates how to download URLs from a sitemap and crawl them using the Playwright Crawler in Crawlee. It requires the apify/actor-node-playwright-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with JSDOMCrawler and Extracting Data in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to crawl a list of URLs from an external file, parse HTML using jsdom, and extract the page title and h1 tags. It demonstrates URL processing, error handling with sessions, and data extraction from web pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Create an instance of the JSDOMCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the\n// jsdom npm package.\nconst crawler = new JSDOMCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ window, request, log, pushData }) {\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent;\n        const h1texts = [];\n\n        // We're only extracting text from the <h1> elements.\n        // To extract text content from other elements, simply add more selectors.\n        // The Element.querySelectorAll() method returns a NodeList of matching elements.\n        for (const h1 of document.querySelectorAll('h1')) {\n            h1texts.push(h1.textContent);\n        }\n\n        log.info(`The title of ${request.url} is: ${title}`);\n\n        // Store the results as a record in the default dataset.\n        // Alternatively, we could push the data into a database,\n        // or just process it in memory.\n        await pushData({\n            url: request.url,\n            title,\n            h1texts,\n        });\n    },\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Automatically saves cookies to disk and reloads them for each new page open.\n    // This is required to keep the authentication cookies for sites that need them.\n    persistCookiesPerSession: true,\n});\n\n// Add first URL to the queue and start the crawl.\nconst urls = JSON.parse(await readFile('./urls.json', 'utf8'));\nawait crawler.run(urls);\n\n// Display results from the default Dataset.\nconst data = await Dataset.open();\nconst { items } = await data.getData();\nfor (const item of items) {\n    console.log(item);\n}\n```\n\n----------------------------------------\n\nTITLE: HTTP Crawler Implementation with TypeScript\nDESCRIPTION: A TypeScript implementation showing how to create an HTTP crawler that loads URLs from a file, makes HTTP requests, and saves HTML content. Uses the HttpCrawler class from Crawlee framework.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the HttpCrawler class\nconst crawler = new HttpCrawler({\n    // Function called for each URL\n    async requestHandler({ request, body }) {\n        // Save HTML content of each crawled page to the dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body\n        });\n    },\n});\n\n// Add URLs to the crawler's queue and run it\nawait crawler.run([\n    'http://example.com/page-1',\n    'http://example.com/page-2',\n    'http://example.com/page-3',\n]);\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee as an Actor Using init() and exit()\nDESCRIPTION: Example showing how to run a Cheerio crawler with explicit Actor.init() and Actor.exit() calls for Apify platform compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Initialize the Apify SDK\nawait Actor.init();\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save results\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Add new links from page to request queue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to request queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n\n// Exit successfully\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Proxy Integration with PlaywrightCrawler\nDESCRIPTION: Example of integrating a ProxyConfiguration instance with PlaywrightCrawler. This allows the crawler to use proxies when navigating websites with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        // Process the data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using BrowserPool Lifecycle Hooks\nDESCRIPTION: Example demonstrating how to use BrowserPool lifecycle hooks to customize browser behavior, such as conditionally launching browsers in headful mode based on request data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Basic Operations with Request Queue in Crawlee\nDESCRIPTION: Demonstrates basic operations with a request queue such as adding requests, getting request info, and handling request states. Shows how to create, manipulate, and track requests in a queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add requests to queue\nawait requestQueue.addRequest({ url: 'http://example.com/a-1' });\n\n// Get the info and ID of a request\nconst requestA = await requestQueue.addRequest({ url: 'http://example.com/a-2' });\nconsole.dir(requestA);\n// {\n//   id: 'YVXC8'                                      // ID of the added request\n//   requestId: 'YVXC8'                              // Same as id but kept for backwards compatibility\n//   wasAlreadyPresent: false,                        // Indicates a request with the same URL was not already present in the queue\n//   uniqueKey: 'http://example.com/a-2'              // The uniqueKey of the request, by default it equals to URL\n// }\n\n// Add a request with a custom uniqueKey\nconst requestB = await requestQueue.addRequest({\n    url: 'http://example.com/b',\n    uniqueKey: 'my-unique-key',\n});\nconsole.dir(requestB);\n// {\n//   id: 'YVXC9'                                      // ID of the added request\n//   requestId: 'YVXC9'                              // Same as id but kept for backwards compatibility\n//   wasAlreadyPresent: false,                        // Indicates a request with the same uniqueKey was not already present in the queue\n//   uniqueKey: 'my-unique-key'                       // The custom uniqueKey of the request\n// }\n\n// Attempt to add a request that's already present will result in the request not being updated\n// and wasAlreadyPresent === true, but the existing request will be returned (or null if it had already been processed).\nconst requestC = await requestQueue.addRequest({ url: 'http://example.com/a-2' });\nconsole.dir(requestC);\n// {\n//   id: 'YVXC8'                                      // ID of the existing request\n//   requestId: 'YVXC8'                              // Same as id but kept for backwards compatibility\n//   wasAlreadyPresent: true,                         // Indicates a request with the same URL was already present in the queue\n//   uniqueKey: 'http://example.com/a-2'              // The uniqueKey of the request, by default it equals to URL\n// }\n\n// Add more requests with custom uniqueKey\nawait requestQueue.addRequest({\n    url: 'http://example.com/c-1',\n    uniqueKey: 'foo',\n});\nawait requestQueue.addRequest({\n    url: 'http://example.com/c-2',\n    uniqueKey: 'foo',\n});\n// Only the first one gets added, the other one with the same uniqueKey is ignored\n\n// Mark a request as handled\nawait requestQueue.markRequestHandled({ url: 'http://example.com/a-1' });\n\n// Get the number of handled requests in the queue\nconst { handledCount } = await requestQueue.getInfo();\n// handledCount = 1\n\n// Get all requests from queue\nconst allRequests = await requestQueue.fetchNextRequest();\n\n// Delete queue\nawait requestQueue.drop();\n\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawler Logic in AWS Lambda Handler Function\nDESCRIPTION: Creating an AWS Lambda handler function that instantiates a new crawler instance for each execution to maintain statelessness. This is crucial to avoid issues with AWS Lambda's environment reuse.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's selector and mapping functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function Handler for CheerioCrawler\nDESCRIPTION: Wraps the crawler initialization and execution in an async handler function that accepts request and response objects from GCP. The function returns the crawler data as the response, which is required for cloud function execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Stage Docker Build for Crawlee Actor\nDESCRIPTION: A complete Dockerfile that implements a multi-stage build process for a Crawlee web scraping project. It separates the build environment from the production environment to create a smaller final image with only the necessary runtime dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Extracting and Formatting Product Price with Playwright\nDESCRIPTION: This snippet shows how to extract and format a product's price from an HTML element. It filters elements with the 'price' class that contain the '$' character, then processes the text to convert it to a numeric value.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Enqueueing Links from Same Hostname with CheerioCrawler in TypeScript\nDESCRIPTION: This code snippet shows how to crawl links with the same hostname using CheerioCrawler. This strategy (the default) will only enqueue links that point to the same hostname as the current URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n\n        // Add all links from the page to the crawling queue that match the same hostname\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n            // We can also use the string representation of the strategy\n            // strategy: 'same-hostname',\n\n            // Or not specify the strategy at all since SameHostname is the default\n            // await enqueueLinks();\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Standalone Proxy Session Management\nDESCRIPTION: Demonstrates how to manually manage proxy sessions outside of crawler instances. Shows how to create sessions and maintain consistent proxy usage per session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: js\nCODE:\n```\nimport { ProxyConfiguration, SessionPool } from 'crawlee';\nimport got from 'got';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst sessionPool = await SessionPool.open();\n\n// Create a new session or get existing one\nconst session1 = await sessionPool.getSession();\nconst proxyUrl1 = await proxyConfiguration.newUrl(session1.id);\n\n// This fetches the page using the selected proxy.\nconst response = await got('https://example.com', {\n    headers: { 'User-Agent': session1.userData.userAgent },\n    proxyUrl: proxyUrl1, // Using the proxy URL from our ProxyConfiguration\n});\n\n// Now if we get the proxy URL for the same session again\nconst proxyUrl1again = await proxyConfiguration.newUrl(session1.id);\n// We get the same one\nconsole.log(proxyUrl1 === proxyUrl1again); // true\n\n// Getting a URL for a different session may return a different proxy URL\nconst session2 = await sessionPool.getSession();\nconst proxyUrl2 = await proxyConfiguration.newUrl(session2.id);\n// This will be often true, but depends on the number of available proxies\nconsole.log(proxyUrl1 !== proxyUrl2);\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for CDN Images in PlaywrightCrawler\nDESCRIPTION: This code shows how to skip navigation for image resources from CDNs while still fetching and saving them. It uses Request#skipNavigation with sendRequest to optimize the crawling process by avoiding full browser navigation for simple resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PlaywrightCrawler } from 'crawlee';\n\n// Create a new PlaywrightCrawler\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, log, sendRequest, crawler }) {\n        // Extract the URL from the request\n        const { url } = request;\n\n        log.info(`Processing ${url}`);\n\n        // Get all images that are served through a CDN\n        const images = await page.$$eval('img[src*=\"cdn\"]', (imgs) => {\n            return imgs.map((img) => img.src);\n        });\n\n        log.info(`Found ${images.length} images on ${url}`);\n\n        // Enqueue each image as a request, but skip the navigation\n        for (const imageUrl of images) {\n            await crawler.addRequests([{\n                url: imageUrl,\n                skipNavigation: true,\n            }]);\n        }\n    },\n\n    // When skipNavigation is true, the requestHandler will not be called, and instead the failedRequestHandler will be used\n    async failedRequestHandler({ request, log, sendRequest }) {\n        // We only want to handle requests that are skipped\n        if (!request.skipNavigation) {\n            log.warning(`Request ${request.url} failed and will not be retried...`);\n            return;\n        }\n\n        const { url } = request;\n\n        // Extract the image name from the URL\n        const imageName = url.split('/').pop();\n\n        // Use sendRequest to fetch the image\n        const response = await sendRequest();\n\n        if (!response.ok) {\n            log.error(`Failed to fetch image at ${url}`);\n            return;\n        }\n\n        // Convert the response to a buffer\n        const buffer = await response.buffer();\n\n        // Save the image to the default key-value store\n        await KeyValueStore.setValue(imageName, buffer, { contentType: response.headers.get('content-type') });\n\n        log.info(`Image ${imageName} saved to key-value store`);\n    },\n\n    // Define the maximum concurrent requests\n    maxRequestsPerCrawl: 10,\n});\n\n// Enqueue the start URL\nawait crawler.addRequests(['https://crawlee.dev']);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use CheerioCrawler to crawl specific links on a website using glob patterns. The crawler processes links matching specified patterns and skips others, controlling which pages get added to the RequestQueue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, createCheerioRouter, CheerioCrawler } from 'crawlee';\n\nconst router = createCheerioRouter();\n\nrouter.addHandler('DETAIL', async ({ $, request }) => {\n    await Dataset.pushData({\n        url: request.url,\n        title: $('title').text(),\n    });\n});\n\nrouter.addDefaultHandler(async ({ enqueueLinks, log }) => {\n    log.info('Enqueueing new links to scrape.');\n    await enqueueLinks({\n        globs: ['**/sections/people/*.html'],\n        label: 'DETAIL',\n    });\n});\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Request List Basic Operations\nDESCRIPTION: Demonstrates how to initialize and use a RequestList with a PuppeteerCrawler, including setting up source URLs and handling requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() utility function. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot of the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { launchPuppeteer } from 'crawlee';\nimport { utils } from 'crawlee';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\n\nawait page.goto('https://crawlee.dev');\n\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with SendRequest in Crawlee\nDESCRIPTION: Demonstrates how to use a proxy with the sendRequest function by manually passing the proxyUrl option. Proxies are used to hide the real IP address when making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels with enqueueLinks\nDESCRIPTION: Demonstrates the Request.label shortcut for labeling requests, which is easier than using Request.userData. This example shows how to label requests when enqueuing links for categorization and processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Crawlee Actor Docker Image\nDESCRIPTION: This Dockerfile creates a Docker image for a Crawlee actor using a multi-stage build process. It installs dependencies, builds the project, and sets up the final image with only the necessary components for production, including Node.js and Playwright with Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY --chown=myuser . ./\n\nRUN npm run build\n\nFROM apify/actor-node-playwright-chrome:16\n\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP deployment\nDESCRIPTION: Sets the main field in package.json to point to the main.js file, which is necessary for GCP Cloud Functions to identify the entry point of the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels in Crawler Handler\nDESCRIPTION: Demonstrates the use of the new Request.label shortcut for labeling requests and using it in the enqueueLinks options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting Crawler Requests in TypeScript\nDESCRIPTION: This snippet demonstrates how to set a maximum limit of crawled pages using the maxRequestsPerCrawl option in CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Proxy Integration with CheerioCrawler\nDESCRIPTION: Example of integrating a ProxyConfiguration instance with CheerioCrawler. This allows the crawler to use proxies when scraping websites with Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $ }) {\n        // Process the data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawler in AWS Lambda Handler\nDESCRIPTION: Creates an AWS Lambda handler function that instantiates and runs the Cheerio crawler for each Lambda execution to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-stage Docker Build for TypeScript Projects\nDESCRIPTION: Dockerfile configuration using multi-stage build to compile TypeScript code and create a production image without development dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Crawlee Actor\nDESCRIPTION: A Dockerfile that implements a two-stage build process for a Crawlee actor. The first stage builds the application with development dependencies, while the second stage creates a minimal production image with only required runtime dependencies. It uses the apify/actor-node:16 base image and includes optimizations for Docker layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:16\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Proxy Configuration in Crawlee\nDESCRIPTION: Shows how to set up and use Apify Proxy in a Crawlee project. This code creates a proxy configuration using the Actor class and generates a new proxy URL for making requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawler Logic in Lambda Handler Function\nDESCRIPTION: Adding the AWS Lambda handler function to wrap the crawler logic, ensuring a new crawler instance is created for each Lambda execution to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to use both Request Queue and Request List together in a Crawler. This approach can be useful for processing initial URLs from a list while allowing dynamic addition of new URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\nconst requestList = await RequestList.open('my-list', [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n]);\n\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(request.url);\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with HttpCrawler in JavaScript\nDESCRIPTION: This code demonstrates how to use the HttpCrawler class to crawl a list of URLs from an external file. It makes HTTP requests to each URL, processes the responses, and saves the HTML content. The crawler is configured with various options including maximum requests, concurrent requests limit, and session pool options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, downloadListOfUrls } from 'crawlee';\nimport { readFile, writeFile } from 'fs/promises';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    // Let's limit our crawls to only 10 requests (do not use in production)>\n    maxRequestsPerCrawl: 10,\n    // Alternatively, we can limit the number of requests per minute\n    maxRequestsPerMinute: 60,\n    // Instead of the two options above, we can limit the number of concurrent requests\n    maxConcurrency: 5,\n    // Timeout each request after 30 seconds\n    requestHandlerTimeoutSecs: 30,\n    // Retry each failed request once\n    maxRequestRetries: 1,\n    // Here we can set options for Crawlee's RequestQueue\n    // This queue is automatically created by the HttpCrawler\n    requestQueue: {\n        // Limit the number of requests in the queue to 100\n        maxRequestsPerMinute: 100,\n    },\n    // Here you can set options for the session pool\n    sessionPool: {\n        // Let's use up to 100 signed in accounts\n        maxPoolSize: 100,\n    },\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with options\n    // and methods as seen below.\n    // The function MUST be defined as `async`.\n    async requestHandler({ request, sendRequest, log, crawler }) {\n        // Get the text of the body of the HTTP response\n        const response = await sendRequest();\n        const html = response.body;\n\n        // Store the HTML to a file\n        const outputPath = `${__dirname}/output/${request.id}.html`;\n        await writeFile(outputPath, html);\n        log.info(`Saved HTML to ${outputPath}`);\n    },\n});\n\n// Enqueue all URLs from the text file\nconst urls = await downloadListOfUrls({\n    url: 'https://api.apify.com/v2/key-value-stores/bf581ba5-15f3-4cd8-9f91-ccaddc2591ae/records/urls.txt?disableRedirect=true',\n});\nawait crawler.addRequests(urls);\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for CDN Resources with PlaywrightCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Request#skipNavigation with PlaywrightCrawler to efficiently fetch resources like images from a CDN without performing a full page navigation. The code shows enqueuing a request with skipNavigation set to true and then using the sendRequest method to fetch the image data and save it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, sendRequest, log, crawler }) {\n        // Normal request handling code\n        log.info(`Processing ${request.url}`);\n\n        // The crawler found an image we want to save\n        // We enqueue it as a separate request with skipNavigation: true\n        await crawler.addRequests([{\n            url: 'https://cdn.example.com/image.jpg',\n            skipNavigation: true, // this makes sure that we won't trigger a new page navigation\n            userData: {\n                // Add any information you need to identify the image request\n                label: 'image',\n            },\n        }]);\n    },\n});\n\ncrawler.router.addHandler('image', async ({ request, sendRequest, log }) => {\n    // Use sendRequest to fetch the resource directly\n    log.info(`Downloading image from ${request.url}`);\n\n    // We can use sendRequest to get the image data\n    const imageResponse = await sendRequest();\n    const imageBuffer = await imageResponse.body;\n\n    // Now we can save the image to the default key-value store\n    await crawler.useState(async (state) => {\n        state.images = state.images || [];\n        state.images.push(request.url);\n        // Here we could also save the actual image buffer to the default key-value store\n    });\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Implementing CustomCrawler Class with curl_impersonate\nDESCRIPTION: Main implementation of the CustomCrawler class that uses CurlImpersonateHttpClient for HTTP requests and defines a context pipeline for processing responses. Includes initialization with impersonate option and headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# custom_crawler.py\n\nfrom __future__ import annotations\n\nimport logging\nfrom re import search\nfrom typing import TYPE_CHECKING, Any, Unpack\n\nfrom crawlee import Request\nfrom crawlee.basic_crawler import (\n    BasicCrawler,\n    BasicCrawlerOptions,\n    BasicCrawlingContext,\n    ContextPipeline,\n)\nfrom crawlee.errors import SessionError\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\nfrom crawlee.http_crawler import HttpCrawlingContext\nfrom orjson import loads\n\nfrom afs_crawlee.constants import BASE_TEMPLATE, HEADERS\n\nfrom .custom_context import CustomContext\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncGenerator, Iterable\n\n\nclass CustomCrawler(BasicCrawler[CustomContext]):\n    \"\"\"A crawler that fetches the request URL using `curl_impersonate` and parses the result with `orjson` and `re`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        impersonate: str = 'chrome124',\n        additional_http_error_status_codes: Iterable[int] = (),\n        ignore_http_error_status_codes: Iterable[int] = (),\n        **kwargs: Unpack[BasicCrawlerOptions[CustomContext]],\n    ) -> None:\n\n        self._build_id = None\n        self._base_url = BASE_TEMPLATE\n\n        kwargs['_context_pipeline'] = (\n            ContextPipeline()\n            .compose(self._make_http_request)\n            .compose(self._handle_blocked_request)\n            .compose(self._parse_http_response)\n        )\n\n        # Initialize curl_impersonate http client using TLS preset and necessary headers\n        kwargs.setdefault(\n            'http_client',\n            CurlImpersonateHttpClient(\n                additional_http_error_status_codes=additional_http_error_status_codes,\n                ignore_http_error_status_codes=ignore_http_error_status_codes,\n                impersonate=impersonate,\n                headers=HEADERS,\n            ),\n        )\n\n        kwargs.setdefault('_logger', logging.getLogger(__name__))\n\n        super().__init__(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Configuration\nDESCRIPTION: Docker configuration for Node.js with Playwright and Firefox browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration in Crawlee\nDESCRIPTION: Demonstrates how to set up a basic proxy configuration using ProxyConfiguration class with custom proxy URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK with API Token\nDESCRIPTION: JavaScript code to initialize the Apify SDK with an API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Batch Adding Requests with crawler.addRequests()\nDESCRIPTION: Shows how to efficiently add multiple requests in batches using the crawler.addRequests() method. This approach handles large volumes of requests efficiently by processing them in batches of 1000, allowing crawling to start quickly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser\nDESCRIPTION: Example showing how to set up PlaywrightCrawler to use Firefox browser for web scraping. This demonstrates proper configuration when running on the Apify Platform using the apify/actor-node-playwright-firefox image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Configuration } from 'crawlee';\n\nawait PlaywrightCrawler.create({\n    browserPoolOptions: {\n        // See https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n        launchOptions: {\n            headless: true,\n            // The firefox browser is already installed on the apify/actor-node-playwright-firefox image.\n            // If you use your own image, make sure to install firefox-esr and playwright-firefox\n            // or use playwright CLI: npx playwright install firefox\n            channel: 'firefox', // or 'firefox-esr' depending on what you have installed\n            // For local development, if you installed firefox via playwright,\n            // you can also use executablePath: '/path/to/firefox',\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple crawler implementation that downloads a single page, extracts its title, and prints it to the console. This serves as the starting point for the tutorial.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple crawler that visits a single URL (crawlee.dev), extracts the page title, and logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Running Headful Browsers with PlaywrightCrawler\nDESCRIPTION: JavaScript code snippet demonstrating how to configure PlaywrightCrawler to run in headful mode for development purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    headless: false,\n    requestHandler: async ({ page, request, enqueueLinks, log }) => {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n        await Dataset.pushData({ title, url: request.loadedUrl });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Operations in Crawlee\nDESCRIPTION: This snippet illustrates how to perform basic operations with datasets in Crawlee, including writing single and multiple rows to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with CheerioCrawler\nDESCRIPTION: Demonstrates how to integrate ProxyConfiguration with CheerioCrawler to enable proxy usage for web scraping. The example shows basic setup and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, log }) {\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Setting Crawl Strategy to Include Subdomains in Crawlee\nDESCRIPTION: Configures enqueueLinks to include subdomains in the crawl by using the 'same-domain' strategy instead of the default 'same-hostname'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Scraping a single URL with got-scraping in Crawlee\nDESCRIPTION: This code demonstrates how to fetch HTML content from a specific URL using the got-scraping package. It uses the asynchronous gotScraping function to retrieve a web page and log its content to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\n/**\n * This example uses the `got-scraping` npm package\n * to grab the HTML of a web page.\n */\nconst url = 'https://apify.com';\n\nasync function run() {\n    console.log(`Downloading HTML from ${url}...`);\n\n    // Get HTML of the page\n    const { body } = await gotScraping(url);\n\n    console.log(`HTML from ${url} downloaded! It's ${body.length} characters long.`);\n    console.log('');\n    console.log(body);\n}\n\nrun();\n```\n\n----------------------------------------\n\nTITLE: Custom URL Selection with enqueueLinks\nDESCRIPTION: Shows how to override the default link selector in enqueueLinks to target specific elements on a page that contain links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Puppeteer Crawler in Crawlee\nDESCRIPTION: This code shows how to use Crawlee's Puppeteer Crawler to download and crawl URLs from a sitemap. It uses the downloadListOfUrls utility to retrieve the sitemap URLs and processes them with Puppeteer Crawler, which allows for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from '@crawlee/puppeteer';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\n// Configuration of the sitemap's URL\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n        const title = await page.title();\n        log.info(`Title of ${url}: ${title}`);\n        // Here you process each URL using Puppeteer - extract data, take screenshots, etc.\n    },\n    // If you need to use proxy, you can use the proxy configuration\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n});\n\n// This logic downloads the sitemap from the specified URL\nasync function crawlSitemap() {\n    // Download the sitemap and parse its URLs\n    const urls = await downloadListOfUrls({ url: startUrls[0] });\n\n    // Add all the URLs to the crawler's queue\n    await crawler.addRequests(urls.map((url) => ({ url })));\n\n    // Run the crawler\n    await crawler.run();\n}\n\n// Execute the crawlSitemap function\nawait crawlSitemap();\n```\n\n----------------------------------------\n\nTITLE: Integrating RequestQueueV2 with Crawler Configuration\nDESCRIPTION: Demonstrates how to combine a custom RequestQueueV2 instance with a crawler while enabling the request locking experiment. This setup allows for customized queue management with locking support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Extracting and Formatting Product Price with Playwright\nDESCRIPTION: Retrieves the product price from an HTML page using Playwright and formats it as a number. The code filters spans with the 'price' class, extracts the numeric value, and removes formatting characters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Deployment\nDESCRIPTION: Package.json configuration showing the main entry point for the GCP function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet shows how to use CheerioCrawler to crawl links with the same hostname as the current page. It uses the 'same-hostname' enqueue strategy, which is the default.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add new links from page to RequestQueue\n        // only if they are from the same hostname\n        // as the page that is currently open.\n        // This is the default behaviour.\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management with Proxies\nDESCRIPTION: Shows how to manually handle session-proxy pairing without a crawler. This creates a consistent relationship between session IDs and proxy URLs for maintaining persistent identities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Requesting a proxy URL for the same session will always return the same proxy URL.\nconst proxyUrl1 = await proxyConfiguration.newUrl('session_1');\nconst proxyUrl2 = await proxyConfiguration.newUrl('session_1');\n// proxyUrl1 === proxyUrl2\nconsole.log('proxyUrl1', proxyUrl1);\nconsole.log('proxyUrl2', proxyUrl2);\n\n// Requesting a proxy URL for a different session will return a different proxy URL.\nconst proxyUrl3 = await proxyConfiguration.newUrl('session_2');\n// proxyUrl1 !== proxyUrl3\nconsole.log('proxyUrl3', proxyUrl3);\n\n// Requesting a proxy URL without a session will rotate through the proxy URLs.\nconst proxyUrl4 = await proxyConfiguration.newUrl();\nconst proxyUrl5 = await proxyConfiguration.newUrl();\n// proxyUrl4 !== proxyUrl5\nconsole.log('proxyUrl4', proxyUrl4);\nconsole.log('proxyUrl5', proxyUrl5);\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for DOM Elements\nDESCRIPTION: This snippet demonstrates how to select and manipulate DOM elements using Cheerio compared to plain browser JavaScript. It shows examples of extracting text content from title elements and collecting all href links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Setting minConcurrency and maxConcurrency in CheerioCrawler\nDESCRIPTION: This code example shows how to configure the minimum and maximum concurrency for parallel requests in CheerioCrawler, setting minConcurrency to 5 and maxConcurrency to 10.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    minConcurrency: 5,\n    maxConcurrency: 10,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Skip Navigation with PlaywrightCrawler\nDESCRIPTION: Shows how to skip navigation for specific requests when crawling, particularly useful for directly downloading resources like images from CDNs. Uses PlaywrightCrawler with Request#skipNavigation and sendRequest to optimize resource fetching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, sendRequest }) {\n        // Check if the URL points to an image\n        if (request.url.endsWith('.jpg')) {\n            // We don't need to load the browser page for this request\n            request.skipNavigation = true;\n\n            // Fetch the image data directly\n            const response = await sendRequest();\n            const buffer = await response.body;\n\n            // Save the image to our key-value store\n            await KeyValueStore.setValue('image', buffer, {\n                contentType: response.headers['content-type'],\n            });\n\n            return;\n        }\n\n        // Handle other URLs normally\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use the Request Queue implicitly with a Crawler in Crawlee. The crawler automatically uses the default request queue to manage URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Add new requests to the queue\n        await enqueueLinks();\n\n        // ... process the page\n    },\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    'https://crawlee.dev',\n]);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Adding Production Script for Compiled Code\nDESCRIPTION: Package.json script configuration for running the compiled JavaScript code in production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Puppeteer's page.screenshot() Method\nDESCRIPTION: This code demonstrates how to capture a screenshot of a web page using Puppeteer's native page.screenshot() method and save it to a key-value store. It loads a URL, takes the screenshot, and stores it with a key derived from the URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser\nconst browser = await launch();\n\ntry {\n    // Open a new page\n    const page = await browser.newPage();\n    await page.goto('https://example.com');\n\n    // Create a key from the URL\n    const key = 'screenshot-example.png';\n\n    // Capture the screenshot\n    const screenshotBuffer = await page.screenshot();\n    \n    // Save the screenshot to the default key-value store\n    await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n\n    console.log(`Screenshot of 'example.com' saved to Key-value store.`);\n} finally {\n    // Close the browser\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.puppeteer.saveSnapshot() utility. It sets up a crawler to visit multiple URLs and save snapshots using the utility function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log, crawler: { saveSnapshot } }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // The saveSnapshot function automatically saves a full screenshot\n        // to a key-value store under the key `SNAPSHOT-[hash]`.\n        // To save it under a different key, pass a `key` option.\n        await saveSnapshot();\n    },\n});\n\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Transform and Filter Requests with transformRequestFunction\nDESCRIPTION: Advanced example showing how to use the transformRequestFunction to dynamically filter or modify requests before they are enqueued, in this case ignoring PDF files.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl in TypeScript\nDESCRIPTION: Shows how to limit the number of requests processed by the crawler using the maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Link Selection in Crawlee's enqueueLinks\nDESCRIPTION: Overriding the default link selection in Crawlee's enqueueLinks function using a custom selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Command to install TypeScript as a development dependency in the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Scraping with PuppeteerCrawler (JavaScript)\nDESCRIPTION: Example of using PuppeteerCrawler to successfully scrape JavaScript-rendered content from Apify Store. Puppeteer requires explicit waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        await page.waitForSelector('.ActorStoreItem');\n        // Extract text content of the first actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Setup with Inline URL Addition in Crawlee\nDESCRIPTION: This snippet shows a more concise way to set up a CheerioCrawler by directly adding URLs to crawl using the run method, eliminating the need for explicit RequestQueue initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Dependency Overrides for Apify SDK with Beta Crawlee\nDESCRIPTION: JSON configuration to specify dependency overrides in package.json when using beta versions of Crawlee with the Apify SDK to prevent multiple versions of Crawlee from being installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/README.md#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"overrides\": {\n       \"apify\": {\n           \"@crawlee/core\": \"3.12.3-beta.13\",\n           \"@crawlee/types\": \"3.12.3-beta.13\",\n           \"@crawlee/utils\": \"3.12.3-beta.13\"\n       }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code as Actor Using Actor.init() and Actor.exit()\nDESCRIPTION: Example showing how to use Actor.init() and Actor.exit() explicitly with a CheerioCrawler implementation for the Apify platform. This approach gives more control over the initialization and cleanup process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, $ }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Cloud Functions\nDESCRIPTION: Sets the main entry point in package.json to point to the src/main.js file, which is necessary for GCP Cloud Functions to correctly locate the handler function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright\nDESCRIPTION: Retrieves the product title from an HTML page using Playwright's locator API. The code targets the h1 element within a div that has the 'product-meta' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Session Management with JSDOMCrawler\nDESCRIPTION: Integration of session management with JSDOMCrawler for DOM-based scraping. Demonstrates session configuration in combination with JSDOM's full DOM implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{JSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Session Management with Proxy Configuration\nDESCRIPTION: Demonstrates how to maintain consistent proxy-session pairs by using sessionId parameter with ProxyConfiguration. Shows standalone implementation of proxy session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\n\n// Getting a proxy URL for our session\nconst sessionId = 'session-1';\nconst proxyUrl = await proxyConfiguration.newUrl(sessionId);\n// This will return the same proxy URL as before\nconst proxyUrl2 = await proxyConfiguration.newUrl(sessionId);\n// This will return a different proxy URL\nconst proxyUrl3 = await proxyConfiguration.newUrl('session-2');\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee\nDESCRIPTION: Demonstrates how to update event handling code when migrating from Apify SDK to Crawlee. Instead of using Apify.events, Crawlee uses Actor.on() and Actor.off() methods for event management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl all links on a website. It initializes the crawler, sets up a request queue, and processes each page by enqueueing new links and storing the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File using Crawlee in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use Crawlee's Dataset API to export an entire dataset to a single CSV file and store it in a key-value store. It utilizes the `exportToValue` function, specifies CSV as the output format, and saves the result to a key-value store named 'my-data'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst csv = await dataset.exportToValue('csv');\nawait KeyValueStore.setValue('OUTPUT', csv, { contentType: 'text/csv' });\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This example illustrates how to implement SessionPool with PuppeteerCrawler in Crawlee. It configures the PuppeteerCrawler with SessionPool options and shows session usage in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, session }) {\n        // Use session\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates a basic setup for crawling the start URL of an e-commerce website and printing the text content of category elements. It uses PlaywrightCrawler to navigate the page and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, log }) {\n        const categoryElements = await page.$$('.collection-block-item');\n\n        log.info(`Number of categories found: ${categoryElements.length}`);\n\n        for (const categoryElement of categoryElements) {\n            const categoryText = await categoryElement.textContent();\n            console.log(categoryText);\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Successful Scraping Log Output\nDESCRIPTION: The console output showing the successful extraction of content from a JavaScript-rendered page using a headless browser crawler. The text is extracted but poorly formatted.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator Using JSDOMCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to interact with a React calculator app. It clicks on calculator buttons to perform a 1+1 calculation and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const { document } = window;\n        log.info(`Testing calculator`);\n\n        // Execute the calculation 1 + 1 = by clicking the buttons\n        const buttonOne = document.querySelector('#calculator > div.react-calculator > div.calculator-keypad > div.input-keys > div.numbers-container > button:nth-child(7)');\n        const buttonPlus = document.querySelector('#calculator > div.react-calculator > div.calculator-keypad > div.operator-keys > button:nth-child(4)');\n        const buttonEqual = document.querySelector('#calculator > div.react-calculator > div.calculator-keypad > div.operator-keys > button:nth-child(5)');\n\n        if (buttonOne && buttonPlus && buttonEqual) {\n            // Click 1 + 1 =\n            buttonOne.click();\n            buttonPlus.click();\n            buttonOne.click();\n            buttonEqual.click();\n\n            // Get the result\n            const result = document.querySelector('#calculator > div.react-calculator > div.auto-scaling-text');\n            if (result) {\n                log.info(`1 + 1 = ${result.textContent}`);\n                await Dataset.pushData({\n                    title: document.title,\n                    result: result.textContent,\n                });\n            }\n        }\n    },\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n\n```\n\n----------------------------------------\n\nTITLE: Comparing DOM Selection in Cheerio vs Browser JavaScript\nDESCRIPTION: Demonstrates equivalent DOM selection operations using both Cheerio and plain browser JavaScript, showing how to extract title text and href attributes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command for deploying an actor from a local development environment to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Session Management with CheerioCrawler\nDESCRIPTION: Shows how to implement proxy session management with CheerioCrawler. This maintains proxy consistency across requests in the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, $, session }) {\n        // The session ensures that the same proxy is used for certain requests\n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Custom Settings in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom Configuration object for Crawlee, set up a CheerioCrawler with this configuration, and define a router with handlers for specific URLs. It showcases state persistence control and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency Limits in Crawlee\nDESCRIPTION: Example of setting minConcurrency and maxConcurrency for a CheerioCrawler to control parallel request execution. This snippet sets a minimum of 5 and maximum of 100 parallel requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n\tminConcurrency: 5, // default is 1\n\tmaxConcurrency: 100, // default is 200\n\t// ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring a Playwright Crawler with Required Settings for GCP Cloud Run\nDESCRIPTION: Sets up a basic PlaywrightCrawler instance with a router for request handling and disables persistent storage which is important for serverless environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Running CheerioCrawler as an Actor Using Actor.main()\nDESCRIPTION: Example of a CheerioCrawler implementation using Actor.main() which handles initialization and proper termination of the actor on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, createCheerioRouter } from 'crawlee';\n\n// Initialize the Actor\nawait Actor.main(async () => {\n    // Create a request list\n    const startUrls = ['https://crawlee.dev'];\n\n    // Create a router\n    const router = createCheerioRouter();\n\n    // Add a route for the start URL\n    router.addDefaultHandler(async ({ $, request, enqueueLinks }) => {\n        const title = $('title').text();\n        console.log(`The title of ${request.url} is '${title}'`);\n\n        // Save results\n        await Actor.pushData({\n            title,\n            url: request.url,\n        });\n\n        // Enqueue links to more pages\n        await enqueueLinks();\n    });\n\n    // Create a crawler\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    });\n\n    // Run the crawler\n    await crawler.run(startUrls);\n});\n```\n\n----------------------------------------\n\nTITLE: Request Transform Function\nDESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a request queue with a Crawler in Crawlee. This approach allows more control over queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequests(['https://crawlee.dev']);\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Proxy Configuration in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a simple ProxyConfiguration with a list of proxy URLs that will be rotated in a round-robin fashion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Scraping and Processing Product Price\nDESCRIPTION: Shows how to extract and clean up the product price, including filtering for the correct element and converting the string price to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Setting Domain Strategy for enqueueLinks\nDESCRIPTION: Example of configuring enqueueLinks to include subdomains in the crawl by using the 'same-domain' strategy instead of the default hostname-only strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nconst screenshotBuffer = await page.screenshot();\nawait KeyValueStore.setValue('my-screenshot', screenshotBuffer, { contentType: 'image/png' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests in PlaywrightCrawler\nDESCRIPTION: This code demonstrates how to skip navigation for image requests while crawling a website using PlaywrightCrawler. It shows how to use the skipNavigation option combined with sendRequest to efficiently fetch images from a CDN without requiring a full browser navigation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\nimport { writeFileSync } from 'fs';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, enqueueLinks, sendRequest, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // If the page has an image\n        const imageElement = await page.$('.some-image');\n        \n        if (imageElement) {\n            // Get the image URL\n            const imageUrl = await imageElement.getAttribute('src');\n            \n            if (imageUrl) {\n                log.info(`Found image at ${imageUrl}`);\n                \n                // We don't need to navigate to the image with a browser,\n                // we can just send a plain HTTP request\n                const imageRequest = { \n                    url: imageUrl,\n                    // This is the important part - we're telling the crawler\n                    // to not use a browser for this request\n                    skipNavigation: true,\n                };\n                \n                // Send the request and get the response\n                const imageResponse = await sendRequest(imageRequest);\n                \n                // Save the image to the default key-value store\n                // under the image-1.jpg key\n                await KeyValueStore.setValue('image-1.jpg', imageResponse.body, { contentType: imageResponse.headers['content-type'] });\n                \n                // You can also save it locally\n                // writeFileSync('image-1.jpg', imageResponse.body);\n            }\n        }\n        \n        // Continue crawling the page as normal\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Basic LinkeDOMCrawler Implementation\nDESCRIPTION: Demonstrates basic usage of LinkeDOMCrawler to crawl a website and extract the page title, storing results in a Dataset\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/linkedom-crawler/README.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new LinkeDOMCrawler({\n    async requestHandler({ request, window }) {\n        await Dataset.pushData({\n            url: request.url,\n            title: window.document.title,\n        });\n    },\n});\n\nawait crawler.run([\n    'http://crawlee.dev',\n]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler Using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and the page.screenshot() method. It creates a crawler that visits multiple URLs, takes screenshots, and saves them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshotBuffer = await page.screenshot();\n        const key = new URL(request.url).hostname;\n        await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Handling Paginated Search Results from Crunchbase API in Python\nDESCRIPTION: Processes search results with pagination support, extracting company details and automatically requesting subsequent pages. Includes data transformation and storage logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('search')\nasync def search_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Search results handler with pagination support.\"\"\"\n    context.log.info(f'search_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    last_entity = None\n    results = []\n\n    for entity in data['entities']:\n        last_entity = entity['uuid']\n        results.append(\n            {\n                'Company Name': entity['properties']['identifier']['value'],\n                'Short Description': entity['properties']['short_description'],\n                'Website': entity['properties'].get('website_url'),\n                'Location': '; '.join([item['value'] for item in entity['properties'].get('location_identifiers', [])]),\n            }\n        )\n\n    if results:\n        await context.push_data(results)\n\n    if last_entity:\n        payload = json.loads(context.request.payload)\n        payload['after_id'] = last_entity\n        payload = json.dumps(payload)\n\n        await context.add_requests(\n            [\n                Request.from_url(\n                    url='https://api.crunchbase.com/api/v4/searches/organizations',\n                    method='POST',\n                    payload=payload,\n                    use_extended_unique_key=True,\n                    headers=HttpHeaders({'Content-Type': 'application/json'}),\n                    label='search',\n                )\n            ]\n        )\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright in JavaScript\nDESCRIPTION: This snippet shows how to extract a product title from a webpage using Playwright's locator API. It selects an h1 element within a container with the class 'product-meta'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.reduce() method in Crawlee\nDESCRIPTION: Example showing how to use the Dataset reduce method to aggregate data. This code calculates the total number of headers across all pages by summing the headingCount values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Persistence Disabled for GCP Cloud Run\nDESCRIPTION: This snippet shows how to create a PlaywrightCrawler instance with storage persistence disabled to work properly in a serverless environment. It configures the crawler with a router for request handling and defines the starting URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: HTML Anchor Tag Example\nDESCRIPTION: An example of an HTML anchor tag that contains an href attribute which the crawler would find and potentially follow during the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://crawlee.dev/js/docs/introduction\">This is a link to Crawlee introduction</a>\n```\n\n----------------------------------------\n\nTITLE: Implementing Router for Crawlee Scraping in JavaScript\nDESCRIPTION: Defines a router for handling different types of pages in a Crawlee scraping project. It includes handlers for product details, category pages, and a default handler for the start page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\nexport const router = createPlaywrightRouter();\n\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1);\n    const manufacturer = urlPart[0].split('-')[0];\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL',\n    });\n\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY',\n        });\n    }\n});\n\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Importing JSON Schema from External File\nDESCRIPTION: This snippet demonstrates importing an input schema defined in a separate JSON file. This approach keeps the schema separate from the main actor code for better organization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { inputSchema } from './input-schema.json';\n\nawait Actor.main(async () => {\n    // Use the imported input schema\n    const input = await Actor.getInput<{\n        startUrls: string[];\n        maxItems: number;\n    }>(inputSchema);\n\n    // ... rest of the actor code\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Batch Requests to a Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method. This method handles request batching automatically, adding the first 1000 requests immediately and continuing with the rest in the background.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Crawlee\nDESCRIPTION: Shows basic operations with Crawlee's key-value store including reading input, writing output, and managing named stores. Demonstrates how to store and retrieve different data types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Advanced Autoscaled Pool Configuration in CheerioCrawler\nDESCRIPTION: This example shows how to use the advanced autoscaledPoolOptions to fine-tune the scaling behavior of a CheerioCrawler. It demonstrates configuring scaling ratios, intervals, and other parameters that control the autoscaling mechanism.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Advanced options for the autoscaled pool\n    autoscaledPoolOptions: {\n        // Initial number of requests run in parallel\n        desiredConcurrency: 10,\n        // How far we need to reach to attempt scaling up/down (between 0-1)\n        desiredConcurrencyRatio: 0.95,\n        // How fast we'll scale up\n        scaleUpStepRatio: 0.05,\n        // How fast we'll scale down\n        scaleDownStepRatio: 0.05,\n        // How often we'll check to run new requests (in seconds)\n        maybeRunIntervalSecs: 0.5,\n        // How often we'll log the status of the autoscaled pool (in seconds, set to null to disable)\n        loggingIntervalSecs: 60,\n        // How often we'll attempt to scale up/down (in seconds)\n        autoscaleIntervalSecs: 10,\n        // Maximum number of requests per minute\n        maxTasksPerMinute: 60,\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to use Puppeteer Crawler to process sitemap URLs. It requires the 'apify/actor-node-puppeteer-chrome' image when running on the Apify Platform and uses browser automation for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add all links from the current page to the crawling queue.\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl a sitemap)\n});\n\n// Download and parse sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Get URLs from a sitemap\nconst urls = sitemap.getUrls();\n\n// Add URLs to the crawling queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Creating a CheerioCrawler with keepAlive for Web Server Integration\nDESCRIPTION: Implements a CheerioCrawler with the keepAlive option set to true, allowing it to remain active while waiting for new requests. The crawler extracts page titles which will later be sent as HTTP responses.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        // We will send the response here later\n        log.info(`Page title: ${title} on ${request.url}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image for Crawlee\nDESCRIPTION: Example of using a beta pre-release version of Node.js 20 Docker image for testing purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:20-beta\n```\n\n----------------------------------------\n\nTITLE: Implementing Entry Point and Actor Input Handling for Bluesky API Scraper\nDESCRIPTION: Defines the Actor input schema, main execution function, and entry point for the Bluesky API scraper. Handles async execution, input parsing, and error handling for the scraper lifecycle.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nimport traceback\nfrom dataclasses import dataclass\n\nimport httpx\nfrom apify import Actor\nfrom yarl import URL\n\nfrom crawlee import ConcurrencySettings, Request\nfrom crawlee.crawlers import HttpCrawler, HttpCrawlingContext\nfrom crawlee.http_clients import HttpxHttpClient\n\n\n@dataclass\nclass ActorInput:\n    \"\"\"Actor input schema.\"\"\"\n    identifier: str\n    app_password: str\n    queries: list[str]\n    mode: str\n    max_requests_per_crawl: Optional[int] = None\n\n\nasync def run() -> None:\n    \"\"\"Main execution function that orchestrates the crawling process.\n\n    Creates a scraper instance, manages the session, and handles the complete\n    crawling lifecycle including proper cleanup on completion or error.\n    \"\"\"\n    async with Actor:\n        raw_input = await Actor.get_input()\n        actor_input = ActorInput(\n            identifier=raw_input.get('indentifier', ''),\n            app_password=raw_input.get('appPassword', ''),\n            queries=raw_input.get('queries', []),\n            mode=raw_input.get('mode', 'posts'),\n            max_requests_per_crawl=raw_input.get('maxRequestsPerCrawl')\n        )\n        scraper = BlueskyApiScraper(actor_input.mode, actor_input.max_requests_per_crawl)\n        try:\n            scraper.create_session(actor_input.identifier, actor_input.app_password)\n\n            await scraper.init_crawler()\n            await scraper.crawl(actor_input.queries)\n        except httpx.HTTPError as e:\n            Actor.log.error(f'HTTP error occurred: {e}')\n            raise\n        except Exception as e:\n            Actor.log.error(f'Unexpected error: {e}')\n            traceback.print_exc()\n        finally:\n            scraper.delete_session()\n\ndef main() -> None:\n    \"\"\"Entry point for the scraper application.\"\"\"\n    asyncio.run(run())\n```\n\n----------------------------------------\n\nTITLE: Using Custom Modules with Apify Launch Functions (JavaScript)\nDESCRIPTION: This example demonstrates how to use custom modules with Apify's launch functions for both Puppeteer and Playwright. It shows the normalization of the 'launcher' option across both implementations, allowing for greater flexibility in browser selection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links Strategy Implementation\nDESCRIPTION: Example demonstrating how to crawl links from the same hostname using CheerioCrawler, which is the default strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add links from the same hostname to queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler\nDESCRIPTION: Implementation of a crawler that specifically targets category links using selectors and labels. Demonstrates waiting for elements and selective link enqueuing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tiered Proxy Configuration in JavaScript\nDESCRIPTION: Configures tiered proxy URLs that automatically upgrade to better proxies when blocking is detected. The system starts with the lowest tier and moves to higher tiers as needed, periodically checking if lower tiers are usable again.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    tieredProxyUrls: [\n        [null], // At first, we try to connect without a proxy\n        ['http://okay-proxy.com'],\n        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],\n        ['http://very-good-and-expensive-proxy.com'],\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates fundamental operations with the RequestQueue class including creating a queue, adding requests, and processing them to extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add requests to the queue\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\nawait requestQueue.addRequest(\n    { url: 'https://crawlee.dev/docs/guides/request-storage' },\n    { forefront: true } // This request will be processed first\n);\n\n// Get the next request from the queue\nconst request1 = await requestQueue.fetchNextRequest();\nconst request2 = await requestQueue.fetchNextRequest();\n\nconsole.log(request1.url); // https://crawlee.dev/docs/guides/request-storage\nconsole.log(request2.url); // https://crawlee.dev\n\n// We're done with processing the requests\nawait requestQueue.markRequestHandled(request1);\nawait requestQueue.markRequestHandled(request2);\n\n// Check if the queue has more requests\nconst queueInfo = await requestQueue.getInfo();\nconsole.log(queueInfo);\n// {\n//   id: 'my-queue',\n//   name: 'my-queue',\n//   totalRequestCount: 2,\n//   handledRequestCount: 2,\n//   pendingRequestCount: 0,\n//   ...\n// }\n\n```\n\n----------------------------------------\n\nTITLE: Creating New Crawlee Project\nDESCRIPTION: Command to create a new Crawlee project using the CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Session Management with JSDOMCrawler and Proxies\nDESCRIPTION: Illustrates how to implement session management with JSDOMCrawler and proxies to create more human-like scraping patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    useSessionPool: true, // This is the default\n    persistCookiesPerSession: true, // This is the default\n    proxyConfiguration,\n    async requestHandler({ request, window, session }) {\n        // Process the response\n        // ...\n\n        // You can mark session as bad when it gets blocked\n        if (someCondition) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for Production Execution\nDESCRIPTION: JSON snippet showing how to set up a start:prod script for running compiled JavaScript in production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Empty Output Log from CheerioCrawler\nDESCRIPTION: The console output showing the empty result when using CheerioCrawler to scrape JavaScript-rendered content. This demonstrates why a different approach is needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Content with JSDOMCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use JSDOMCrawler to crawl a list of URLs from an external file. For each URL, it loads the page via HTTP request, parses the HTML using jsdom, and extracts the page title and all h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// Create an instance of the JSDOMCrawler class\nconst crawler = new JSDOMCrawler({\n    // Let's limit our crawls to just a few requests\n    maxRequestsPerCrawl: 20,\n});\n\n// This function will be called for each URL to crawl.\ncrawler.router.addDefaultHandler(async ({ request, window, log }) => {\n    const { document } = window;\n    const title = document.title;\n    log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n    // Extract h1 tags\n    const h1Texts = Array.from(document.querySelectorAll('h1'))\n        .map((element) => element.textContent);\n\n    // Save the results to Dataset\n    await Dataset.pushData({\n        url: request.loadedUrl,\n        title,\n        h1Texts,\n    });\n});\n\n// Read the list of URLs from a text file, one URL per line\nconst urlsText = await readFile('./urls.txt', 'utf-8');\nconst urls = urlsText\n    .split('\\n')\n    .map((line) => line.trim())\n    .filter((line) => line.length > 0 && !line.startsWith('#'));\n\n// Execute the crawler with the defined options\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use the Request Queue implicitly with a Crawler in Crawlee. The crawler automatically manages the queue, adding new requests as they are discovered during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}`);\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Request Labeling and Link Enqueuing\nDESCRIPTION: Example of using the new Request.label shortcut with enqueueLinks for request categorization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in HttpCrawler\nDESCRIPTION: Shows how to access and inspect the current proxy information within the HttpCrawler's request handler. This allows for verification and debugging of proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, json }) {\n        // Prints information about the proxy used for the request\n        if (proxyInfo) {\n            console.log(proxyInfo.url);\n        }\n        \n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: This code demonstrates how to create a custom Configuration object with a 10-second state persistence interval and pass it to a CheerioCrawler. The crawler processes two example URLs with specific waiting times, showing how custom configurations affect crawler behavior, particularly for state persistence.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Accessing ProxyInfo in PlaywrightCrawler\nDESCRIPTION: Example showing how to access proxy information in PlaywrightCrawler's requestHandler using the proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionPlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from an Element Using Cheerio\nDESCRIPTION: A simple example showing how to find the first h2 element on a page and extract its text content using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use the context-aware sendRequest() function with BasicCrawler to make HTTP requests. The function uses got-scraping under the hood to mimic browser requests and avoid being blocked.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with BasicCrawler. It shows how to configure the SessionPool, handle session rotation, and manage proxy IPs with proper error handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\n{BasicSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue in Crawlee\nDESCRIPTION: Shows how to create a RequestQueue instance and add a URL to crawl. This demonstrates the basic setup for managing crawling requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright\nDESCRIPTION: This snippet shows how to use Playwright's locator to extract a product title from an HTML element. It specifically targets an h1 element within a div with the 'product-meta' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Configuring enqueueLinks to Include Subdomains\nDESCRIPTION: Using the 'same-domain' strategy with enqueueLinks to include subdomains in the crawl instead of the default behavior of staying on the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Implementation in TypeScript\nDESCRIPTION: A simple crawler that downloads HTML, extracts the page title, and prints it to the console. This is the starting point for the tutorial's enhancements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Repository Search Form with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to automate GitHub repository search by filling and submitting a search form. It handles search parameters like term, owner, date, and language, then saves the results to a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PuppeteerCrawler, log } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page }) {\n        // Choose value in select element\n        await page.select('#search_language', 'JavaScript');\n\n        // Type text in search input\n        await page.type('#search_q', 'puppeteer-crawler');\n\n        // Type text in user input\n        await page.type('#search_user', 'apify');\n\n        // Type start date\n        await page.type('#search_created', '2021-01-01');\n\n        // Submit form\n        await page.click('form[action=\"/search\"] button[type=\"submit\"]');\n\n        // Wait for results\n        await page.waitForSelector('.repo-list li');\n\n        // Extract data from the page\n        const repositories = await page.$$eval('.repo-list li', ($repositories) => {\n            const repoData = [];\n\n            // Process each repository\n            for (const $repository of $repositories) {\n                const title = $repository.querySelector('h3')?.textContent?.trim();\n                const description = $repository.querySelector('p')?.textContent?.trim();\n                const stars = $repository.querySelector('[href$=\"/stargazers\"]')?.textContent?.trim();\n                const lastUpdate = $repository.querySelector('relative-time')?.textContent?.trim();\n\n                if (!title) continue;\n\n                repoData.push({\n                    title,\n                    description,\n                    stars,\n                    lastUpdate,\n                });\n            }\n\n            return repoData;\n        });\n\n        // Save the data to dataset\n        await Dataset.pushData(repositories);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://github.com/search']);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Country-Specific Residential Proxies\nDESCRIPTION: Demonstrates how to configure Apify Proxy to use specific proxy groups and geographic locations. Sets up residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Crawling with PuppeteerCrawler\nDESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using PuppeteerCrawler. It demonstrates how to set up the crawler with Puppeteer, define the request handler, and start the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit to 20 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Scraping with PlaywrightCrawler (JavaScript)\nDESCRIPTION: Example of using PlaywrightCrawler to successfully scrape JavaScript-rendered content from Apify Store. Playwright automatically waits for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of the first actor card\n        const actorText = await page.textContent('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Crawler with Router in Crawlee\nDESCRIPTION: This snippet shows how to set up a PlaywrightCrawler with a router to handle different types of pages. It demonstrates the main entry point of a refactored scraper that uses proper logging and a clean structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: This snippet shows how to perform basic operations with key-value stores in Crawlee, including reading input, writing output, and working with named stores.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Proxy Configuration in JavaScript\nDESCRIPTION: Shows how to create a proxy configuration for Apify Proxy using the Actor.createProxyConfiguration() method and generate a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PuppeteerCrawler\nDESCRIPTION: Example configuration of SessionPool with PuppeteerCrawler for headless browser automation with session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with PuppeteerCrawler in Crawlee\nDESCRIPTION: This example illustrates the configuration and usage of SessionPool with PuppeteerCrawler in Crawlee. It shows how to set up the session pool and use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ session, page, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        maxPoolSize: 50,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with Proxy Configuration in CheerioCrawler\nDESCRIPTION: Demonstrates how to manage sessions in conjunction with proxy configuration in CheerioCrawler to maintain consistent proxy usage per session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // When a proxyConfiguration is used together with SessionPool,\n    // the created sessions will be associated with particular proxy URLs\n    // from the pool, improving your scraper's performance.\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, response, proxyInfo, session }) {\n        // You can access the proxy URL used for the request with proxyInfo.url,\n        // and the session that holds the proxy URL assignment with session.id\n        console.log(`Using proxy: ${proxyInfo?.url} for session: ${session.id}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with JSDOMCrawler\nDESCRIPTION: Shows how to integrate ProxyConfiguration with JSDOMCrawler for web scraping using JSDOM. The example demonstrates setting up proxies for the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, log }) {\n        const { document } = window;\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawler with crawlee.json Configuration\nDESCRIPTION: JavaScript example showing how to implement a crawler that uses the configuration from crawlee.json without explicit configuration setup in code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Commands to install Apify CLI and log in with an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use Playwright Crawler to process a list of URLs. It requires the apify/actor-node-playwright-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure in Crawlee\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing dataset items as individual JSON files\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to access proxy information in PlaywrightCrawler to monitor which proxy is being used during browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo }) {\n        // Print information about the currently used proxy directly to the terminal\n        console.log(proxyInfo);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Custom Browser Modules with Launch Functions\nDESCRIPTION: Demonstrates how to use custom browser modules with the launchPuppeteer and launchPlaywright functions, showing the normalized 'launcher' option pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in Crawlee\nDESCRIPTION: Internal implementation of the sendRequest function showing default parameters and how it integrates with got-scraping. The function handles request configuration including URLs, methods, headers, proxies, and cookies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Cheerio Module\nDESCRIPTION: Command to install only the Cheerio module for lightweight HTML parsing functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This snippet shows how to disable the browser fingerprinting feature in PuppeteerCrawler by setting useFingerprints to false in the browserPoolOptions. This allows developers to control their own fingerprinting or avoid it completely.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Disable the fingerprinting feature\n        useFingerprints: false,\n    },\n    // Function to process the crawled data\n    async requestHandler({ page, parseWithCheerio, request }) {\n        const $ = await parseWithCheerio();\n        // ... processing logic\n    },\n});\n\n// Run the crawler\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example demonstrating how to customize browser fingerprints in PlaywrightCrawler by specifying browser type, operating system, and other fingerprint parameters to avoid getting blocked during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Configures fingerprints generation\n        fingerprintOptions: {\n            // Use only Chrome browsers\n            browsers: [\n                { name: 'chrome', minVersion: 88 },\n            ],\n            // Use only desktop devices\n            devices: [\n                'desktop',\n            ],\n            // Use only specified operating systems\n            operatingSystems: [\n                'windows',\n                'macos',\n            ],\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Group and Country Selection\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country code selection. This example shows how to use only residential proxies from the United States for better proxy performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using Page Method\nDESCRIPTION: This code uses PuppeteerCrawler to process multiple URLs and capture screenshots with page.screenshot(). The crawler handles the browser lifecycle while the handler function processes each page by creating a key based on the URL and saving the screenshot.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Get a unique key for the screenshot based on the URL\n        const key = `screenshot-${request.url.replace(/[:/]/g, '_')}`;\n\n        // Take the screenshot\n        const screenshot = await page.screenshot();\n\n        // Save the screenshot to the default key-value store\n        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });\n    },\n    // Let's limit our crawls to just a few pages\n    maxRequestsPerCrawl: 10,\n});\n\n// Define the starting URLs\nawait crawler.run(['https://apify.com', 'https://crawlee.dev']);\n\nconsole.log('Crawler finished!');\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates using CheerioCrawler to crawl links that point to the same domain, including any subdomains. It uses the 'SameDomain' strategy to include all links within the same domain regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Add all same-domain links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // Alternatively, you can use the string 'same-domain'\n            // strategy: 'same-domain',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Pool with Multiple Browser Types in Crawlee\nDESCRIPTION: This code demonstrates how to set up a browser pool with both Chrome and Firefox browsers. It defines different launch contexts, configurations, and uses options like proxy configuration and user data directories.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { BrowserPool, PlaywrightPlugin } = require('crawlee');\n\nconst browserPool = new BrowserPool({\n    preLaunchHooks: [(pageId, launchContext) => {\n        // called before a browser is launched\n        console.log(`Launching browser for page ${pageId}`);\n    }],\n    postLaunchHooks: [(pageId, browserController) => {\n        // called after a browser is launched\n        console.log(`Browser for page ${pageId} launched in PID ${browserController.process.pid}`);\n    }],\n    prePageCreateHooks: [(pageId, browserController, pageOptions) => {\n        // called before a page is created in the browser\n        console.log(`Creating page ${pageId}`);\n    }],\n    postPageCreateHooks: [(pageId, browserController, page) => {\n        // called after a page is created in the browser\n        console.log(`Page ${pageId} created`);\n    }],\n    prePageCloseHooks: [(pageId, browserController, page) => {\n        // called before a page is closed\n        console.log(`Closing page ${pageId}`);\n    }],\n    postPageCloseHooks: [(pageId, browserController, page) => {\n        // called after a page is closed\n        console.log(`Page ${pageId} closed`);\n    }],\n\n    browserPlugins: [\n        new PlaywrightPlugin({\n            launchOptions: {\n                headless: true,\n            },\n            proxyUrl: 'http://proxy.example.com',\n            useIncognitoPages: true,\n        }, { browserType: 'chromium' }),\n        new PlaywrightPlugin({\n            launchOptions: {\n                headless: false,\n                userDataDir: './tmp/chromium-1',\n            },\n        }, { browserType: 'chromium' }),\n        new PlaywrightPlugin({\n            launchOptions: {\n                headless: true,\n            },\n        }, { browserType: 'firefox' }),\n    ],\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Global Configuration Class\nDESCRIPTION: Example demonstrating how to use the Configuration class to modify the global configuration. This code sets the state persistence interval to 10 seconds programmatically rather than using a configuration file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Creating and Adding Requests to a RequestQueue in Crawlee\nDESCRIPTION: Creates a RequestQueue instance and adds a URL to be crawled. This demonstrates the foundational step of initializing the queue that will store all target URLs for the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in JavaScript\nDESCRIPTION: This example uses JSDOMCrawler to crawl a list of URLs from an external file, parse the HTML using jsdom, and extract the page title and all h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const { document } = window;\n\n        const title = document.querySelector('title')?.textContent;\n        const h1texts = Array.from(document.querySelectorAll('h1'))\n            .map((el) => el.textContent);\n\n        log.info(`The title is: ${title}`);\n        log.info(`The h1 texts are: ${h1texts}`);\n\n        await Dataset.pushData({\n            title,\n            h1texts,\n        });\n    },\n});\n\nconst urlList = await readFile('urls.txt', 'utf8');\nconst urls = urlList.split('\\n').map((url) => url.trim());\n\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Actor Configuration JSON for Apify Platform\nDESCRIPTION: Configuration file defining the Apify Actor's metadata. This includes details like name, memory requirements, description, and references to input schema and Dockerfile.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"actorSpecification\": 1,\n  \"name\": \"Bluesky-Crawlee\",\n  \"title\": \"Bluesky - Crawlee\",\n  \"minMemoryMbytes\": 128,\n  \"maxMemoryMbytes\": 2048,\n  \"description\": \"Scrape data products from bluesky\",\n  \"version\": \"0.1\",\n  \"meta\": {\n    \"templateId\": \"bluesky-crawlee\"\n  },\n  \"input\": \"./input_schema.json\",\n  \"dockerfile\": \"./Dockerfile\"\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting Request Rate with maxRequestsPerMinute in Crawlee\nDESCRIPTION: Example of configuring a CheerioCrawler with maxRequestsPerMinute set to 120, limiting the crawler to make a maximum of 120 requests per minute to avoid overwhelming the target website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n\tmaxRequestsPerMinute: 120, // 2 requests per second\n\t// ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Child Processes in JavaScript\nDESCRIPTION: This code implements a parallel scraper using Node.js child processes. It forks multiple instances of the scraper and manages communication between parent and child processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, CheerioCrawler } from 'crawlee';\nimport { fork } from 'child_process';\nimport { fileURLToPath } from 'url';\nimport { getOrInitQueue } from './requestQueue.mjs';\nimport { router } from './routes.mjs';\n\nif (process.env.IS_WORKER_THREAD) {\n    Configuration.set('purgeOnStart', false);\n    const requestQueue = await getOrInitQueue(false);\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n    const crawler = new CheerioCrawler({\n        requestQueue,\n        requestHandler: router,\n        maxConcurrency: 15,\n    }, config);\n    await crawler.run();\n    process.exit(0);\n} else {\n    const currentFilePath = fileURLToPath(import.meta.url);\n    const NUM_PROCESSES = 2;\n    const promises = [];\n    for (let i = 0; i < NUM_PROCESSES; i++) {\n        const promise = new Promise((resolve) => {\n            const child = fork(currentFilePath, [], {\n                env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },\n            });\n            child.on('message', (message) => {\n                console.log(`Received data from worker ${i}:`, message);\n            });\n            child.on('exit', (code) => {\n                console.log(`Worker ${i} exited with code ${code}`);\n                resolve();\n            });\n        });\n        promises.push(promise);\n    }\n    await Promise.all(promises);\n    console.log('All workers finished');\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from an Element using Cheerio\nDESCRIPTION: This snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Building and Running a Crawlee Actor\nDESCRIPTION: This Dockerfile defines a two-stage build process. The first stage (builder) installs dependencies and builds the project. The second stage creates a minimal production image with only the built files and production dependencies, optimizing for smaller image size and better security.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with CheerioCrawler. It shows how to configure proxy rotation and session management for web scraping with Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler\nDESCRIPTION: Playwright-based crawler implementation for handling multiple URLs. Requires 'apify/actor-node-playwright-chrome' Docker image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management with ProxyConfiguration\nDESCRIPTION: Shows how to use ProxyConfiguration with sessions in a standalone manner, outside of crawler implementations, for consistent proxy-session pairing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Getting a proxyUrl for session 1\nconst url1 = await proxyConfiguration.newUrl('session_1');\nconsole.log(`Proxy URL for session_1: ${url1}`);\n\n// Getting a proxyUrl for session 2\nconst url2 = await proxyConfiguration.newUrl('session_2');\nconsole.log(`Proxy URL for session_2: ${url2}`);\n\n// Getting the next proxyUrl for session 1 again\nconst url1Again = await proxyConfiguration.newUrl('session_1');\nconsole.log(`Proxy URL for session_1 again: ${url1Again}`);\n\n// url1 will equal url1Again\n```\n\n----------------------------------------\n\nTITLE: Configuring enqueueLinks for Same-Domain Crawling\nDESCRIPTION: This snippet demonstrates how to configure enqueueLinks to include subdomains in the crawl using the 'same-domain' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Configuration with CheerioCrawler in JavaScript\nDESCRIPTION: This snippet demonstrates creating a custom Configuration object with a modified 'persistStateIntervalMillis' value, passing it to a CheerioCrawler instance, and setting up a simple crawling scenario. It showcases how to handle different URLs, use sleep functions, and manage crawler state persistence.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Interacting with a React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to interact with a React calculator application. The script navigates to the calculator, performs a sequence of button clicks (1+1=), and extracts the calculation result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler();\n\ncrawler.router.addDefaultHandler(async ({ window, enqueueLinks, log }) => {\n    const { document } = window;\n\n    // Click the number 1\n    document.querySelector('.calculator .digit-1')?.dispatchEvent(\n        new window.MouseEvent('click', { bubbles: true })\n    );\n\n    // Click the + button\n    document.querySelector('.calculator .op-add')?.dispatchEvent(\n        new window.MouseEvent('click', { bubbles: true })\n    );\n\n    // Click the number 1 again\n    document.querySelector('.calculator .digit-1')?.dispatchEvent(\n        new window.MouseEvent('click', { bubbles: true })\n    );\n\n    // Click the = button\n    document.querySelector('.calculator .eq')?.dispatchEvent(\n        new window.MouseEvent('click', { bubbles: true })\n    );\n\n    // Extract the result\n    const display = document.querySelector('.calculator .display');\n    const result = display?.textContent;\n\n    log.info(`1 + 1 = ${result}`);\n\n    await Dataset.pushData({\n        result,\n    });\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Crawler with Crawlee in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use HttpCrawler from Crawlee to crawl URLs from an external file, make HTTP requests, and save HTML content. It includes setting up the crawler, defining the request handler, and running the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ body, request, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Save results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\n// Read the list of URLs from the text file\nconst urlList = await readFile('urls.txt', 'utf8');\nconst urls = urlList.split('\\n').map((url) => url.trim());\n\n// Add URLs to the crawler's queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring maxRequestsPerMinute in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in CheerioCrawler to limit the number of requests made per minute to 120.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to use Cheerio to find all anchor elements with href attributes and extract those URLs into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for CDN Resources with PlaywrightCrawler\nDESCRIPTION: This example shows how to use the skipNavigation option with sendRequest to fetch images from a CDN without navigating to them. It demonstrates creating a PlaywrightCrawler that identifies image URLs, adds them to the request queue with skipNavigation set to true, and then processes them by fetching and saving to the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\nimport { Actor } from 'apify';\n\n// Initialize the PlaywrightCrawler\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, sendRequest, log, crawler }) {\n        // If we've set skipNavigation to true on this request\n        // we'll use sendRequest to fetch it instead of navigating to it\n        if (request.skipNavigation) {\n            log.info(`Fetching image from ${request.url}`);\n            const response = await sendRequest({ url: request.url });\n\n            const contentType = response.headers['content-type'];\n\n            if (!contentType?.startsWith('image/')) {\n                log.warning(`Resource at ${request.url} is not an image`);\n                return;\n            }\n\n            // The response from sendRequest contains the buffer.\n            // We can even store the raw image data to a key-value store\n            const imageBuffer = response.body;\n\n            // Store the image using the URL as the key\n            // This will create a file with the image, ensuring duplicates are only saved once\n            await KeyValueStore.getDefaultInstance().setValue(request.userData.key, imageBuffer, {\n                contentType,\n            });\n\n            return;\n        }\n\n        // Here's our normal crawling logic\n        const title = await crawler.page.title();\n        log.info(`Title of ${request.url} is ${title}`);\n\n        // We'll just extract all the image URLs on the page using Playwright\n        const imageUrls = await crawler.page.$$eval('img', (imgs) => imgs.map((img) => img.src));\n\n        // For each image, we'll add it to the queue with skipNavigation set to true\n        for (const imageUrl of imageUrls) {\n            // It's a good idea to check if the URL is valid\n            if (!imageUrl) continue;\n\n            await crawler.addRequests([{\n                url: imageUrl,\n                // This is the important part!\n                // We set skipNavigation to true to tell the crawler to\n                // use sendRequest instead of navigating to the URL\n                skipNavigation: true,\n                userData: {\n                    // We'll use a hash of the URL as the key to prevent duplicates\n                    // and ensure valid filenames\n                    key: Buffer.from(imageUrl).toString('base64url'),\n                },\n            }]);\n        }\n    },\n});\n\nactor.main(async () => {\n    // Let's start with a single URL\n    await crawler.run(['https://crawlee.dev']);\n    console.log('Crawler finished.');\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with HttpCrawler\nDESCRIPTION: Shows how to integrate ProxyConfiguration with HttpCrawler to enable proxy usage for all HTTP requests. The example includes setting up the proxy and creating a basic crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, response, body, log }) {\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Running PuppeteerCrawler in Headful Mode\nDESCRIPTION: Example showing how to configure PuppeteerCrawler to run in headful mode (with a visible browser window) which is useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Let's open browsers in headful mode so that we can see what's going on.\n    headless: false,\n    // Let's slow it down a bit to make it more visible.\n    preNavigationHooks: [() => new Promise(resolve => setTimeout(resolve, 300))],\n\n    // Your logic here\n});\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Location in File System\nDESCRIPTION: This snippet shows the file path where individual dataset items are stored when using Crawlee. Each item is saved to its own file in the default dataset directory within the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PuppeteerCrawler\nDESCRIPTION: Shows how to access and log proxy details within PuppeteerCrawler's request handler to track proxy usage during browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo }) {\n        // Print information about the currently used proxy directly to the terminal\n        console.log(proxyInfo);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Base Package\nDESCRIPTION: Commands for installing the main Crawlee package which contains all crawler implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Crawling with CheerioCrawler\nDESCRIPTION: Example of recursive crawling of the Crawlee website using CheerioCrawler. It extracts the title of each page and saves the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json File\nDESCRIPTION: A JSON configuration file that specifies global Crawlee options. This example sets persistStateIntervalMillis to 10000 and logLevel to DEBUG.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using Configuration Class\nDESCRIPTION: Example demonstrating how to use the Configuration class to set global configuration options programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image\nDESCRIPTION: Shows how to use a pre-release (beta) version of the Apify actor-node Docker image with Node.js 16.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16-beta\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue for Batch Adding Requests in Crawlee\nDESCRIPTION: Demonstrates how to use the request queue to add multiple requests in a batch, which is more efficient than adding them one by one.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Prepare a list of URLs\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add multiple requests to the queue in one call\nawait requestQueue.addRequests(sources);\n\n// Create a crawler that will process the requests from the queue\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request }) {\n        // Process the page...\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Custom Settings\nDESCRIPTION: Demonstrates how to create a proxy configuration with specific proxy groups and country code settings. This example configures the proxy to use residential IPs from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee and Playwright for PlaywrightCrawler\nDESCRIPTION: Install both Crawlee and Playwright packages for projects using PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Base Package\nDESCRIPTION: Command to install the main Crawlee package which contains most @crawlee/* packages and crawler classes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image Tags\nDESCRIPTION: Examples of how to use pre-release (beta) versions of Docker images, both with and without specifying the automation library version.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:20-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:20-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects with TypeScript support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This example demonstrates how to customize browser fingerprints in PlaywrightCrawler by specifying browser type, operating system, and language preferences to avoid detection when scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Browser fingerprints are generated automatically by default.\n    // Here we customize them to match browsers used in the US.\n    browserPoolOptions: {\n        fingerprintOptions: {\n            // Use desktop browser fingerprints (the default)\n            mobile: false,\n            // Restrict to just Chrome browser fingerprints\n            browsers: ['chrome'],\n            // Use only fingerprints from the latest Chrome version\n            // Other options are 'random' (default) and 'randomized'\n            browserVersion: 'latest',\n            // Only generate fingerprints for Windows\n            operatingSystems: ['windows'],\n            // Use locales typical for US users\n            locales: ['en-US', 'en'],\n        },\n    },\n    // Other crawler options...\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PlaywrightCrawler\nDESCRIPTION: Example of how to access proxy information in PlaywrightCrawler to log which proxy is being used for browser automation, helping with troubleshooting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request }) {\n        if (proxyInfo) {\n            // Format of proxyInfo.url: <protocol>://<username>:<password>@<hostname>:<port>\n            console.log(`Processing ${request.url} using proxy ${proxyInfo.url}`);\n        } else {\n            console.log(`Processing ${request.url} directly`);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset from Crawlee in TypeScript\nDESCRIPTION: Importing the PlaywrightCrawler and Dataset classes from the Crawlee library. The Dataset class is used for saving scraped data to disk or other storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: JSDOM SessionPool Example with JSDOMCrawler\nDESCRIPTION: Example illustrating SessionPool implementation with JSDOMCrawler for DOM manipulation and session management\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{JSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use CheerioCrawler to crawl specific links on a website that match given glob patterns. Uses enqueueLinks() with globs property to filter which URLs should be added to the RequestQueue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // enqueueLinks() will add more links into the RequestQueue for crawling\n        // based on the matching glob pattern - only sub-pages of quotes.toscrape.com\n        // will be included\n        await enqueueLinks({\n            globs: ['http://quotes.toscrape.com/*']\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n    // ProxyConfiguration can be used to set proxy servers\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] })\n});\n\nawait crawler.run(['http://quotes.toscrape.com']);\n```\n\n----------------------------------------\n\nTITLE: Web Crawling with CheerioCrawler\nDESCRIPTION: Example showing how to perform recursive crawling of the Crawlee website using CheerioCrawler, which is a fast HTTP-based crawler for static websites.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // Let's limit our crawls to just a few pages to prevent\n    // overwhelming the website and our dataset.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can perform the extraction logic.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the results to the dataset where we can access them later.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            // Consider only links that contain 'docs'.\n            globs: ['https://crawlee.dev/**'],\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Adding Requests in Batches to Request Queue in Crawlee\nDESCRIPTION: Demonstrates how to add multiple requests to a request queue in a single batch operation using the addRequests() method in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\n// Add multiple requests to the queue in a single batch\nawait requestQueue.addRequests([\n    { url: 'https://example.com/1' },\n    { url: 'https://example.com/2' },\n    { url: 'https://example.com/3' },\n]);\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // ...\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring a PlaywrightCrawler with disabled storage persistence for GCP Cloud Run\nDESCRIPTION: Basic setup of a PlaywrightCrawler with a router for request handling and a Configuration instance that disables storage persistence, which is important for serverless environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Actor Project\nDESCRIPTION: Command to create a new Apify Actor project using a pre-built template.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\napify create price-tracking-actor\n```\n\n----------------------------------------\n\nTITLE: PlaywrightCrawler Implementation with Proper Element Waiting\nDESCRIPTION: An implementation using PlaywrightCrawler that successfully scrapes JavaScript-rendered content by waiting for elements to appear on the page before extracting data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        // Playwright will do this automatically\n        const element = await page.locator('.ActorStoreItem').first();\n        const actorText = await element.textContent();\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency Limits in CheerioCrawler\nDESCRIPTION: This example demonstrates how to set minimum and maximum concurrency for a CheerioCrawler. These parameters control how many parallel requests can run at any time during the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Minimum parallel requests - crawler will never do less than 5 parallel requests\n    minConcurrency: 5,\n    // Maximum parallel requests - crawler will never do more than 10 parallel requests\n    maxConcurrency: 10,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Platform Datasets with Actor in Crawlee\nDESCRIPTION: This snippet demonstrates how to access both local and platform-based datasets using the Actor class. The example shows how to open a local dataset by default and a remote dataset using the forceCloud option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Playwright SessionPool Example with PlaywrightCrawler\nDESCRIPTION: Example demonstrating SessionPool setup with PlaywrightCrawler for browser automation and session handling\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP Cloud Function handler for CheerioCrawler\nDESCRIPTION: Wraps the crawler initialization and execution in an async handler function that receives request and response objects, and returns the crawler data as the function response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet illustrates the configuration and usage of SessionPool with CheerioCrawler in Crawlee. It shows how to set up the session pool and use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ session, $, body, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        maxPoolSize: 50,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Repository Search with PuppeteerCrawler in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills in a search form with specific criteria, submits it, extracts the results, and saves them to a dataset. The crawler handles pagination and uses Puppeteer for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, log }) {\n        if (page.url() === 'https://github.com/search') {\n            // Fill and submit the search form\n            await page.type('input[name=\"q\"]', 'crawler');\n            await page.type('#search_from', '2020-01-01');\n            await page.select('select[name=\"l\"]', 'JavaScript');\n            await page.type('input[name=\"o\"]', 'apify');\n            await Promise.all([\n                page.waitForNavigation(),\n                page.click('button[type=\"submit\"]'),\n            ]);\n        } else {\n            // Extract the data from the page\n            const data = await page.$$eval('.repo-list-item', (elements) => {\n                return elements.map((el) => ({\n                    title: el.querySelector('.f4 a').innerText,\n                    description: el.querySelector('.col-9').innerText,\n                    url: el.querySelector('.f4 a').href,\n                }));\n            });\n\n            // Save the data to dataset\n            await Dataset.pushData(data);\n\n            // Get the next page URL\n            const nextPageUrl = await page.$eval('a.next_page', (el) => el.href);\n            if (nextPageUrl) {\n                // Enqueue the next page URL\n                await crawler.addRequests([nextPageUrl]);\n            }\n        }\n    },\n});\n\nawait crawler.run(['https://github.com/search']);\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in HttpCrawler\nDESCRIPTION: Shows how to access and inspect the current proxy information in HttpCrawler's request handler, providing details like the proxy URL being used.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request }) {\n        if (proxyInfo) {\n            // Format of proxyInfo.url: <protocol>://<username>:<password>@<hostname>:<port>\n            console.log(`Processing ${request.url} using proxy ${proxyInfo.url}`);\n        } else {\n            console.log(`Processing ${request.url} directly`);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests with transformRequestFunction\nDESCRIPTION: Using transformRequestFunction with enqueueLinks to dynamically filter or modify requests before they are enqueued, such as ignoring PDF files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json Configuration\nDESCRIPTION: JavaScript example demonstrating how to use Crawlee with configuration set in crawlee.json. This code doesn't explicitly import or pass Configuration to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Puppeteer SessionPool Example with PuppeteerCrawler\nDESCRIPTION: Example showing SessionPool configuration with PuppeteerCrawler for headless browser automation\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Crawler with Crawlee\nDESCRIPTION: Example showing how to configure and use HttpCrawler to crawl URLs from an external file. The crawler makes HTTP requests to each URL and saves the resulting HTML. Demonstrates core HTTP crawling functionality with error handling and request configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler } from 'crawlee';\n\nconst crawler = new HttpCrawler({\n    async requestHandler({ request, body }) {\n        // Save the loaded HTML to a file\n        await Dataset.pushData({\n            url: request.url,\n            html: body\n        });\n    },\n});\n\n// Add requests to the crawler\nconst listOfUrls = [\n    'http://example.com',\n    'http://google.com',\n    'http://example.com/something'\n];\n\nawait crawler.run(listOfUrls);\n```\n\n----------------------------------------\n\nTITLE: Session Management with HttpCrawler and Proxies\nDESCRIPTION: Demonstrates how to use session management with HttpCrawler and proxies to maintain persistent sessions for better scraping results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    useSessionPool: true, // This is the default\n    persistCookiesPerSession: true, // This is the default\n    proxyConfiguration,\n    async requestHandler({ request, json, session }) {\n        // Process the response\n        // ...\n\n        // You can mark session as bad when it gets blocked\n        if (someCondition) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Downloading Files with Crawlee's FileDownload Class in TypeScript\nDESCRIPTION: This script demonstrates how to download various file types (images, PDF, HTML) using the FileDownload crawler class from Crawlee. It downloads files via HTTP requests and saves them to the default key-value store (locally in ./storage/key_value_stores/default).\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/file_download.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload } from 'crawlee';\n\nconst crawler = new FileDownload();\n\nawait crawler.downloadFiles([\n    // You can specify files as URLs by passing strings...\n    'https://www.example.com/image.png',\n    'https://www.example.com/document.pdf',\n    \n    // ...or as full RequestOptions objects\n    {\n        url: 'https://www.example.com/page.html',\n        // Optionally override the default key under which the file will be stored\n        key: 'my-page.html',\n        // Optionally add HTTP request options like headers\n        headers: {\n            'X-Requested-With': 'crawler',\n        },\n    },\n]);\n\n// Files get stored automatically with a key derived from the URL\n// Example: '61d3a2f06ea67f29c8ec3d95.png' for the image.png file\n\nconsole.log('Files were stored in the default key-value store');\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in TypeScript\nDESCRIPTION: Shows the implementation of the sendRequest function. It merges request parameters with override options and configures got-scraping with sensible defaults for web scraping, including session management and proxy support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Request Transformation Function\nDESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Options in Crawlers\nDESCRIPTION: Demonstrates how to configure BrowserPool options, including lifecycle hooks, in PuppeteerCrawler or PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Sanity Check Crawler with Playwright and Cheerio\nDESCRIPTION: This code snippet creates a PlaywrightCrawler that uses Cheerio for HTML parsing. It visits the start URL, extracts the HTML content, and then uses Cheerio to select and print the text content of all category elements on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const html = await page.content();\n        const $ = cheerio.load(html);\n\n        $('.collection-block-item').each((_, el) => {\n            console.log($(el).text());\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy your local actor code to the Apify platform using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Dataset for Map and Reduce Operations in Crawlee\nDESCRIPTION: A sample JSON array representing scraped data from multiple web pages. Each item contains a URL and a count of heading elements (h1-h3) found on that page. This data structure is stored in the default dataset folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting maxRequestsPerMinute in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option when initializing a CheerioCrawler. This option controls the total number of requests that can be made per minute, preventing bursts of requests followed by long waiting periods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Using Custom CookieJar with sendRequest\nDESCRIPTION: Example demonstrating how to use a custom cookie jar with the sendRequest function. This uses the tough-cookie package to manage cookies during HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Running PlaywrightCrawler in Headful Mode\nDESCRIPTION: Example of how to run PlaywrightCrawler in headful mode, which shows the browser UI during crawling. This is useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // ... other options\n    headless: false,\n    requestHandlerTimeoutSecs: 30,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Puppeteer Crawler\nDESCRIPTION: Implementation of a Puppeteer-based crawler for website link traversal. It shows how to use Puppeteer for browser automation while crawling links, with request handling and data collection functionality. Includes configuration for running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Let's limit our crawling to only 10 requests\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Save the HTML elements containing the text 'Example Domain'\n        await Dataset.pushData({\n            url: request.url,\n            title: await page.title(),\n        });\n\n        // Extract all links from the page and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Composing Browser Launch Behavior with Pre-Launch Hooks\nDESCRIPTION: Example demonstrating how to compose multiple browser pre-launch hooks for flexible browser configuration and how to access the crawling context from hooks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Same-Domain Strategy with enqueueLinks\nDESCRIPTION: Setting the enqueueLinks strategy to 'same-domain' to include subdomains in the crawl. By default, enqueueLinks stays on the same hostname without including subdomains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Map Method in Crawlee\nDESCRIPTION: Example of using the Dataset map method to transform dataset items, filtering for pages with more than 5 header elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst moreThan5headers = await dataset.map((item) => {\n    if (item.headingCount > 5) {\n        return item.headingCount;\n    }\n    return;\n});\n\n// Save the result to the default key-value store\nconst store = await KeyValueStore.open();\nawait store.setValue('more-than-5-headers', moreThan5headers);\n```\n\n----------------------------------------\n\nTITLE: Configuring header generation with sendRequest in TypeScript\nDESCRIPTION: Demonstrates how to configure browser fingerprint generation when making requests. This example shows how to specify device types, locales, operating systems, and browser types to be used.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating TypeScript Configuration File\nDESCRIPTION: TypeScript configuration (tsconfig.json) that extends @apify/tsconfig and sets up compilation options for ES2022 modules with top-level await support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example of disabling dynamic fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in Handler Functions\nDESCRIPTION: Shows how to access crawler properties like requestQueue and autoscaledPool from within handler functions in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers with PlaywrightRouter\nDESCRIPTION: Sets up a router with specific handlers for different page types (detail pages, category pages, and a default handler). Each handler is responsible for its own data extraction or link enqueueing logic based on the page type.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler with ProxyConfiguration in TypeScript\nDESCRIPTION: This code demonstrates how to use ProxyConfiguration with PuppeteerCrawler for browser automation and scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, session }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json Configuration\nDESCRIPTION: Example of using a CheerioCrawler that inherits configuration from a crawlee.json file without explicitly importing or passing Configuration. This demonstrates how Crawlee automatically picks up settings from the configuration file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Implementing CheerioCrawler for Web Scraping in TypeScript\nDESCRIPTION: This code demonstrates the use of CheerioCrawler to crawl URLs from a file, make HTTP requests, parse HTML with Cheerio, and extract the page title and h1 tags. It uses the Crawlee library and includes error handling and data saving functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\nimport { Dataset } from '@crawlee/core';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`The title of \"${request.url}\" is: ${title}`);\n\n        // Extract all h1 texts.\n        const h1Texts = $('h1')\n            .map((_, el) => $(el).text())\n            .get();\n\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1Texts,\n        });\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run([\n    'https://crawlee.dev',\n]);\n\n// Alternatively, we can enqueue URLs from a text file.\n// await crawler.run(await downloadListOfUrls({ url: 'https://example.com/list-of-urls.txt' }));\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PlaywrightCrawler\nDESCRIPTION: Shows how to access and inspect information about the currently used proxy in PlaywrightCrawler's requestHandler function. This is useful for debugging or logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo }) {\n        console.log(proxyInfo.url); // http://proxy-1.com\n        console.log(proxyInfo.hostname); // proxy-1.com\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Crawlee (TypeScript)\nDESCRIPTION: This snippet demonstrates how to push data items to a default dataset in Crawlee. It includes importing the Dataset class, pushing single items, and pushing multiple items at once. If the dataset doesn't exist, it will be created automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Save one item to default dataset\nawait Dataset.pushData({ id: 123, name: 'John Doe' });\n\n// Save multiple items to default dataset\nawait Dataset.pushData([\n    { id: 124, name: 'John Smith' },\n    { id: 125, name: 'Jack Brown' },\n]);\n\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler\nDESCRIPTION: Demonstrates how to use the sendRequest helper method with BasicCrawler for making HTTP requests using got-scraping\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Declarations\nDESCRIPTION: Command to install TypeScript type declarations for Node.js, enabling type-checking for Node.js features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router\nDESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling. Configures logging level and initiates the crawler with a starting URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with all necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows how to create and use Apify Proxy configuration for making HTTP requests. Uses Actor.createProxyConfiguration() to set up proxy access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Autoscaled Pool Options in CheerioCrawler\nDESCRIPTION: Demonstrates how to set advanced autoscaling parameters including scale ratios, checking intervals, and desired concurrency settings to fine-tune crawler performance based on system resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Set the minimum and maximum concurrency\n    minConcurrency: 5,\n    maxConcurrency: 100,\n    // Configure the autoscaled pool\n    autoscaledPoolOptions: {\n        // How quickly to scale up and down (5% at a time)\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        // Minimum ratio (95%) of desired concurrency to reach before more scaling up is allowed\n        desiredConcurrencyRatio: 0.95,\n        // How many requests should ideally be running in parallel\n        desiredConcurrency: 5,\n        // How often to check if we can start more requests (in seconds)\n        maybeRunIntervalSecs: 0.5,\n        // How often to log the state of the autoscaled pool (in seconds)\n        loggingIntervalSecs: 60,\n        // How often to check if we should scale up or down (in seconds)\n        autoscaleIntervalSecs: 10,\n        // Limit how many requests per minute can be processed\n        maxTasksPerMinute: 60,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Implementation\nDESCRIPTION: Demonstrates a basic CheerioCrawler setup that fails to extract JavaScript-rendered content from Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Working with Datasets in Crawlee\nDESCRIPTION: Shows how to use Crawlee's Dataset class to store structured data, including writing single and multiple rows to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This snippet shows how to use SessionPool with PlaywrightCrawler in Crawlee. It demonstrates proxy configuration and session management in a browser-based crawling scenario.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    // The `useSessionPool` option will enable the SessionPool\n    useSessionPool: true,\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ page, request, session }) {\n        await page.goto(request.url);\n        const title = await page.title();\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // We can save cookies from the page to the session\n        const cookies = await page.context().cookies();\n        session.setCookies(cookies, request.url);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawlee Crawler in Express Server for GCP Cloud Run\nDESCRIPTION: Modifies the main script to create an Express server that wraps the Crawlee crawler. This allows the crawler to communicate via HTTP, which is required for GCP Cloud Run. The server listens on the port specified by GCP's environment variable.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport express from 'express';\nconst app = express();\n\nconst startUrls = ['https://crawlee.dev'];\n\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    return res.send(await crawler.getData());\n});\n\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Including Subdomains in Crawl using Strategy Parameter\nDESCRIPTION: Demonstrates how to modify the default crawling behavior to include subdomains by using the 'same-domain' strategy instead of the default 'same-hostname'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PuppeteerCrawler\nDESCRIPTION: Shows how to access and log proxy information in PuppeteerCrawler to monitor which proxy server is handling each browser request for debugging purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request }) {\n        if (proxyInfo) {\n            // Format of proxyInfo.url: <protocol>://<username>:<password>@<hostname>:<port>\n            console.log(`Processing ${request.url} using proxy ${proxyInfo.url}`);\n        } else {\n            console.log(`Processing ${request.url} directly`);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: Simple example of using the enqueueLinks() function without parameters to find all links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Processing\nDESCRIPTION: Modifies the category route handler to enqueue product URLs to a shared queue for parallel processing instead of immediate scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getOrInitQueue } from './requestQueue.mjs';\n\nconst requestQueue = await getOrInitQueue(true);\n\nconst routes = {\n    CATEGORY: async ({ enqueueLinks, $ }) => {\n        const productLinks = $('a.product');\n        await enqueueLinks({\n            selector: 'a.product',\n            label: 'DETAIL',\n            requestQueue,\n        });\n    },\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This code snippet illustrates the use of SessionPool with CheerioCrawler in Crawlee. It sets up the CheerioCrawler with SessionPool options and shows how to use the session in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, session }) {\n        // Use session\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Basic HTTP Crawling with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use BasicCrawler to make HTTP requests to specific URLs and save the results to the default dataset. This example uses the sendRequest utility to download web pages and stores their raw HTML and URL as JSON files in the local storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that just downloads plain HTTP requests and doesn't do any special handling.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with the following fields:\n    // - request: an object holding information about the URL to crawl\n    // - enqueueLinks: a function to add more links to the queue\n    // - sendRequest: a function to make the HTTP request and return the response\n    // - pushData: a function to store data in the dataset\n    async requestHandler({ request, sendRequest }) {\n        console.log(`Processing ${request.url}...`);\n\n        // Fetch the page HTML via the crawlee sendRequest utility function\n        // This automatically uses a HTTP request library called got-scraping\n        // which uses headers that mimic a real browser to avoid detection\n        const { body } = await sendRequest();\n\n        // Store the results as JSON to the default dataset.\n        // In local configuration, the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n\n        console.log(`The crawling finished.`);\n    },\n});\n\n// Add URLs to the queue and start the crawl\nawait crawler.run([\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Specifying Dataset Storage Location in Crawlee\nDESCRIPTION: Shows the default storage location for dataset items in a Crawlee project. Each item is saved as a separate file in the project's storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Session Management with HttpCrawler and ProxyConfiguration\nDESCRIPTION: Shows how to use SessionPool with HttpCrawler and ProxyConfiguration for improved session management. The session ID is used to consistently assign the same proxy to the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // By default, the crawler will use the proxy configuration\n    // and select a proxy based on session ID\n    async requestHandler({ request, json, session }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Initializing BasicCrawler with sendRequest in TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize a BasicCrawler and use the sendRequest function to make HTTP requests. It showcases the basic usage of sendRequest within the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Location-Based Search Request with Crunchbase API in Python\nDESCRIPTION: Creates a search request payload for finding companies in Prague using Crunchbase's search endpoint. Configures field selection, result limits, and location filtering criteria.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npayload = {\n    'field_ids': ['identifier', 'location_identifiers', 'short_description', 'website_url'],\n    'limit': 200,\n    'order': [{'field_id': 'rank_org', 'sort': 'asc'}],\n    'query': [\n        {\n            'field_id': 'location_identifiers',\n            'operator_id': 'includes',\n            'type': 'predicate',\n            'values': ['e0b951dc-f710-8754-ddde-5ef04dddd9f8'],\n        },\n        {'field_id': 'facet_ids', 'operator_id': 'includes', 'type': 'predicate', 'values': ['company']},\n    ],\n}\n\nserialiazed_payload = json.dumps(payload)\nawait crawler.run(\n    [\n        Request.from_url(\n            url='https://api.crunchbase.com/api/v4/searches/organizations',\n            method='POST',\n            payload=serialiazed_payload,\n            use_extended_unique_key=True,\n            headers=HttpHeaders({'Content-Type': 'application/json'}),\n            label='search',\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Limit for a Crawler\nDESCRIPTION: Setting the maxRequestsPerCrawl option to limit the number of pages crawled during testing or to prevent excessive crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result in JavaScript\nDESCRIPTION: The expected output of the reduce method example, showing the total count of all headers across all pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Updated BrowserPool Methods Compared to PuppeteerPool\nDESCRIPTION: Examples comparing the old PuppeteerPool methods with their new BrowserPool equivalents, showing the syntax changes needed when upgrading to SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: Shows how to enable the request locking experiment in a CheerioCrawler instance. The crawler will process URLs using the new locking mechanism.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Unique Storage Configuration\nDESCRIPTION: Code example showing how to pass a new Configuration instance to the PlaywrightCrawler to ensure each crawler instance has its own storage and won't interfere with others in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Recursive Web Scraping with PuppeteerCrawler\nDESCRIPTION: Demonstrates setting up PuppeteerCrawler to recursively scrape Hacker News website, storing results in a dataset. The crawler processes each page by extracting article titles and URLs, then enqueues additional pages for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Dataset, PuppeteerCrawler, RequestQueue } from 'crawlee';\n\n// Create an instance of the RequestQueue class\nconst requestQueue = await RequestQueue.open();\n// Add first URL to queue\nawait requestQueue.addRequest({ url: 'https://news.ycombinator.com' });\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // Here you can set options that are passed to the Puppeteer browser instance\n    launchContext: {\n        launchOptions: {\n            headless: true,\n            // Other Puppeteer options\n        },\n    },\n    // This function will be called for each URL to crawl.\n    // Here you can write the Puppeteer scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by Crawlee.\n    // The function accepts a single parameter, which is an object with the following fields:\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Puppeteer within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link to post\n            // from each post on Hacker News.\n            $posts.forEach(($post) => {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const href = $post.querySelector('.title a').href;\n\n                scrapedData.push({\n                    title,\n                    rank,\n                    href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times.`);\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json File\nDESCRIPTION: Example of a crawlee.json file that sets the persistStateIntervalMillis to 10000 milliseconds and logLevel to DEBUG. This file should be placed in the root of your project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced AutoscaledPool Options in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to configure advanced autoscaling options that control how the crawler scales up and down based on system resources and performance metrics.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Advanced options for fine-tuning the crawler's autoscaling behavior\n    autoscaledPoolOptions: {\n        // Start with this concurrency\n        desiredConcurrency: 10,\n        // Minimum ratio of concurrency to reach before more scaling is allowed\n        desiredConcurrencyRatio: 0.90,\n        // How much to scale up by at each step (fraction of desired concurrency)\n        scaleUpStepRatio: 0.05,\n        // How much to scale down by at each step (fraction of desired concurrency)\n        scaleDownStepRatio: 0.05,\n        // How often to run autoscaling checks (in seconds)\n        autoscaleIntervalSecs: 10,\n        // How often to log the state of the autoscaled pool (in seconds)\n        loggingIntervalSecs: 60,\n        // How often to check for new tasks when scaling up or down (in seconds)\n        maybeRunIntervalSecs: 0.5,\n        // Maximum number of tasks per minute (same as maxRequestsPerMinute)\n        maxTasksPerMinute: 120,\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to upload and deploy your actor code to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Custom Link Selector Configuration\nDESCRIPTION: Example of using a custom CSS selector with enqueueLinks to find specific elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with datasets including writing single and multiple rows of data to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options for PuppeteerCrawler in JavaScript\nDESCRIPTION: This code shows how to update the launch options for a PuppeteerCrawler from the old launchPuppeteerOptions format to the new launchContext object. It separates Apify-specific options from Puppeteer launch options for clarity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with a Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use a Request Queue implicitly with a Crawler class in Crawlee. The crawler automatically manages the queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(request.url);\n        // Add new requests to the automatically managed request queue\n        await enqueueLinks();\n    },\n});\n\n// Add requests to the automatically managed request queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n]);\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Creating a CheerioCrawler for web scraping\nDESCRIPTION: Sets up a CheerioCrawler with 'keepAlive' option to continuously wait for new requests. The crawler extracts the page title from each scraped page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        // We will send the response here later\n        log.info(`Page title: ${title} on ${request.url}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Lifecycle Hooks\nDESCRIPTION: Example of configuring lifecycle hooks for BrowserPool through browserPoolOptions in a crawler, including setting browser retirement policy and modifying launch options based on request data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with BasicCrawler - JavaScript\nDESCRIPTION: Shows how to use SessionPool with BasicCrawler to manage proxy rotations and handle sessions\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{BasicSource}\n```\n\n----------------------------------------\n\nTITLE: Proxy Inspection in CheerioCrawler\nDESCRIPTION: Demonstrates accessing proxy information in CheerioCrawler's requestHandler using proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nInspectionCheerioSource\n```\n\n----------------------------------------\n\nTITLE: Using HttpCrawler to Process URLs and Save HTML\nDESCRIPTION: This code demonstrates how to use the HttpCrawler class to process a list of URLs from a text file, make HTTP requests, and save the resulting HTML. It shows configuration options, request handling, and error handling patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// This example shows how to use HttpCrawler to crawl a list of URLs\n// from an external file, load each URL using a plain HTTP request\n// and save HTML.\nimport { Dataset, HttpCrawler, log } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// Prepare a list of URLs to crawl.\n// In this example, the list contains just a few URLs.\nconst urlListText = await readFile('./url-list.txt', { encoding: 'utf8' });\nconst urlList = urlListText.trim().split('\\n');\n\nlog.info(`Going to crawl ${urlList.length} URLs`);\n\n// Create and run the crawler\nconst crawler = new HttpCrawler({\n    // To speed up the crawl, we set a high concurrency.\n    // But note that many websites might enforce low rate limits,\n    // If you experience timeout errors, decrease the concurrency.\n    maxConcurrency: 20,\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 20,\n\n    // Use only the request queue for getting new URLs\n    useSessionPool: false,\n\n    // Crawl the website depth-first (rather than breadth-first)\n    // to better handle large crawls\n    strategy: 'same-hostname',\n\n    // The default timeout is 30 seconds, some sites need more\n    requestHandlerTimeoutSecs: 60,\n\n    // Print debug lines for each request\n    logLevel: log.LEVELS.DEBUG,\n\n    // Here you can set options for Got.js, check their docs\n    // for all available options: https://github.com/sindresorhus/got/blob/v11/documentation/2-options.md\n    requestHandler: {\n        // Below options are automatically passed to the got() call\n        // They can be overridden per-request in the crawler.requestAsBrowser() call.\n        useHeaderGenerator: false,\n        headerGeneratorOptions: {\n            browsers: [\n                {\n                    name: 'chrome',\n                    minVersion: 87,\n                    maxVersion: 89,\n                },\n            ],\n            devices: ['desktop'],\n            locales: ['en-US'],\n            operatingSystems: ['windows', 'linux'],\n        },\n    },\n\n    // Each request is processed in a FileHandler-style (see FileProcessor.processRequest)\n    async requestHandler({ request, responseBody, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Save the data to dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: responseBody.toString(),\n        });\n    },\n\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(urlList);\n\nconsole.log('Crawler finished.');\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in CheerioCrawler\nDESCRIPTION: Demonstrates how to access proxy information within CheerioCrawler's request handler to monitor and log proxy usage during scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, proxyInfo }) {\n        // Print information about the currently used proxy directly to the terminal\n        console.log(proxyInfo);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Crawler with Disabled Storage Persistence for GCP\nDESCRIPTION: Basic setup of a PlaywrightCrawler with configuration that disables storage persistence, which is necessary for serverless environments like GCP Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Crawler with Router in Crawlee (JavaScript)\nDESCRIPTION: Demonstrates how to set up a PlaywrightCrawler with a router for better code organization. This main file initializes the crawler, configures logging, and starts the crawling process with a specified start URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in TypeScript\nDESCRIPTION: The implementation of the sendRequest function, showing how it combines request parameters with got-scraping options. It handles URL, method, body, headers, proxy settings, and cookie management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TypeScript configuration file (tsconfig.json) setup with ES2022 module system and custom compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country\nDESCRIPTION: Demonstrates how to create a more specific Apify Proxy configuration by selecting proxy groups and country. This example configures the proxy to use residential IPs from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's chaining methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Accessing Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores. Shows how to get/set values and handle different data types automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration\nDESCRIPTION: Shows how to initialize and create a basic proxy configuration using Apify Proxy service. This requires being logged into an Apify account with proxy access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with CheerioCrawler - JavaScript\nDESCRIPTION: Example of using SessionPool with CheerioCrawler for HTML parsing with managed sessions\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl all links on a website. It sets up a PuppeteerCrawler, uses the enqueueLinks() method to add new links, and processes each page by extracting the title and URL. It also demonstrates how to add custom labels to requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            transformRequestFunction(req) {\n                // Add a custom label to the request\n                req.label = 'my-label';\n                return req;\n            },\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns\nDESCRIPTION: Using glob patterns with enqueueLinks to filter URLs for more precise control over which links are followed during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee\nDESCRIPTION: Imports the necessary PlaywrightCrawler and Dataset modules from Crawlee library to enable data extraction and storage functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in Typescript\nDESCRIPTION: This code demonstrates using the 'All' strategy with enqueueLinks() in CheerioCrawler to enqueue any URLs found, even if they go off the site you are currently crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // The crawler will automatically process the requests in the queue\n    async requestHandler({ request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // Optional: Provide a link selector to use for calculating the urls\n            // LinkSelectorToBeAdded\n        });\n    },\n});\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: Illustrates how to use SessionPool with JSDOMCrawler for managing sessions and proxy rotations. It includes setup for proxy configuration, session pool options, and handling of requests and blocked sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    // The `useSessionPool` is enabled by default\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ window, request, session }) {\n        const { document } = window;\n        const title = document.querySelector('title').textContent;\n        console.log(`The title of ${request.url} is: ${title}`);\n\n        // Check if you got blocked here and invalidate the session\n        if (await isBlocked(document)) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Globs in TypeScript\nDESCRIPTION: This snippet demonstrates how to use globs to filter URLs when enqueuing links in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple crawler that downloads the HTML of a single page, extracts its title, and prints it to the console using CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio Crawler\nDESCRIPTION: Demonstrates how to crawl a sitemap using Cheerio Crawler by downloading URLs and processing them. Uses downloadListOfUrls utility from @crawlee/utils to fetch sitemap contents.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels in Crawler Handler (TypeScript)\nDESCRIPTION: Illustrates the use of the new Request.label shortcut for labeling requests and using it in enqueueLinks options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Reduce Method in Crawlee\nDESCRIPTION: Shows how to use the Dataset reduce method to calculate the total number of headers across all scraped pages. The result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst pagesHeadingCount = await dataset.reduce((memo, item) => {\n    return memo + item.headingCount;\n}, 0);\n\nconst kvStore = await KeyValueStore.open();\nawait kvStore.setValue('pages-heading-count', pagesHeadingCount);\n```\n\n----------------------------------------\n\nTITLE: Domain Strategy Configuration\nDESCRIPTION: Configuration for including subdomains in the crawl using the same-domain strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Example showing how to completely disable browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false, which may be useful in specific scraping scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Disables fingerprints generation\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing puppeteer-extra and stealth plugin dependencies\nDESCRIPTION: Command to install the required dependencies for using puppeteer-extra with the stealth plugin via npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Configuring Residential Proxies with Crawlee for Amazon Scraping\nDESCRIPTION: Sets up Apify's residential proxies with a specific country code for Amazon scraping. This approach helps prevent IP-based blocking by using proxies from residential networks in the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration } from 'apify';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US', // Optionally, you can specify the proxy country code.\n    // This is useful for sites like Amazon, which display different content based on the user's location.\n});\n\nconst crawler = new CheerioCrawler({ requestHandler, proxyConfiguration });\n\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json Build Scripts\nDESCRIPTION: Basic package.json configuration for TypeScript build process and main entry point.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with JSDOMCrawler\nDESCRIPTION: Example of configuring JSDOMCrawler with proxies to parse and interact with web pages using a full DOM implementation, allowing access to window and document objects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ window, document, request }) {\n        console.log(`Processing: ${request.url}`);\n        const title = document.querySelector('title')?.textContent;\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing and Authenticating with Apify CLI\nDESCRIPTION: Commands to install the Apify CLI and log in with your API token, which allows you to run your scrapers with your Apify account credentials.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with AWS Chromium Settings\nDESCRIPTION: Updates the crawler configuration to use the Chromium executable provided by @sparticuz/chromium and adds necessary launch arguments for running in AWS Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to customize browser fingerprints for PuppeteerCrawler in Crawlee. It configures specific parameters for fingerprint generation, including browser, operating system, and screen size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [{ name: 'firefox', minVersion: 80 }],\n                operatingSystems: ['windows'],\n                devices: ['desktop'],\n                locales: ['en-US', 'en-GB'],\n                resolution: { width: 1920, height: 1080 },\n            },\n        },\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests in Crawlee's enqueueLinks\nDESCRIPTION: Using the transformRequestFunction in Crawlee's enqueueLinks to modify or skip requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build script and main entry point\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Cheerio Crawler to crawl multiple specified URLs. It extracts the title from each page and stores the results using the Dataset API.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title using Playwright in JavaScript\nDESCRIPTION: This code uses Playwright to locate and extract the product title from the page using a CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Launch Context Information\nDESCRIPTION: Shows how to access information about the browser's launch context, such as proxy information and session data, through the browserController object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Sessions with CheerioCrawler in TypeScript\nDESCRIPTION: This code shows how to implement proxy sessions with CheerioCrawler to maintain consistent IP addresses for scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ session }) {\n        const proxyUrl = await proxyConfiguration.newUrl(session.id);\n        console.log(proxyUrl);\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Setting up PlaywrightCrawler with AWS Chromium\nDESCRIPTION: Code demonstrating how to supply Chromium path from the @sparticuz/chromium package and pass necessary arguments to disable hardware acceleration features not available in AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Installing playwright-extra and stealth plugin with npm\nDESCRIPTION: Command to install the playwright-extra and puppeteer-extra-plugin-stealth packages via npm, which are required for the Playwright implementation example.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy in HttpCrawler\nDESCRIPTION: Variable reference InspectionHttpSource showing proxy inspection in HttpCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionHttpSource}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee manually for PlaywrightCrawler\nDESCRIPTION: Command to manually install Crawlee and Playwright for use with PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple Pre-launch Hooks in JavaScript\nDESCRIPTION: Example showing how multiple browser pre-launch hooks can be composed together to create reusable browser configuration logic, demonstrating the modularity of the new approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Unique Configuration for AWS Lambda\nDESCRIPTION: This snippet demonstrates how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for in-memory storage on AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Shows how to log into the Apify platform using the Apify CLI tool. This allows you to use your Apify account credentials when running scrapers locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-firefox Docker Image\nDESCRIPTION: Demonstrates how to use the Apify Docker image that includes Playwright and Firefox.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Setting Up PlaywrightCrawler with Router in Crawlee\nDESCRIPTION: Demonstrates the main crawler setup in a separate file, importing a router from another module. This approach creates a clean entry point that sets up logging and initializes the crawler with the router as the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Using proxyUrl with sendRequest in Crawlee\nDESCRIPTION: Shows how to specify a proxy server when using sendRequest. This is useful for avoiding IP blocks and distributing requests across multiple IP addresses.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PlaywrightCrawler\nDESCRIPTION: Shows how to use proxy configuration with PlaywrightCrawler for browser automation through proxy servers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    // Remove the launchContext option for automatic browser launch\n    // with provided proxy configuration\n    async requestHandler({ request, page }) {\n        // Extract data using Playwright API\n        // ...\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Finding Links with enqueueLinks Function\nDESCRIPTION: Implementation of a crawler that automatically finds and enqueues links on each page using the enqueueLinks helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // find all links and add them to the crawling queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: This snippet shows how to perform basic operations with key-value stores in Crawlee, including getting input, setting output, opening a named store, and reading/writing/deleting records.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, Puppeteer is no longer bundled with the SDK and must be installed separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Session Management with CheerioCrawler\nDESCRIPTION: Example of using SessionPool with CheerioCrawler for HTML parsing and scraping with session management. Shows how to configure sessions with Cheerio's lightweight parsing approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Initialize Puppeteer Crawler for Link Crawling\nDESCRIPTION: Shows implementation of PuppeteerCrawler for crawling website links. Requires apify/actor-node-puppeteer-chrome image for deployment. Uses browser automation to discover and process links while handling page navigation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing User Handler for Bluesky API Crawling in Python\nDESCRIPTION: This method processes user profile data from the Bluesky API. It extracts relevant user information and stores it in a dedicated dataset for users.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def _user_handler(self, context: HttpCrawlingContext) -> None:\n    context.log.info(f'Processing user {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    user_item = {\n        'did': data['did'],\n        'created': data['createdAt'],\n        'avatar': data.get('avatar'),\n        'description': data.get('description'),\n        'display_name': data.get('displayName'),\n        'handle': data['handle'],\n        'indexed': data.get('indexedAt'),\n        'posts_count': data['postsCount'],\n        'followers_count': data['followersCount'],\n        'follows_count': data['followsCount'],\n    }\n\n    await self._users.push_data(user_item)\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code snippet shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'SameHostname' enqueue strategy, which is the default behavior of enqueueLinks().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing links with same hostname.');\n        await enqueueLinks({\n            selector: 'a[href]',\n        });\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using Utils Method\nDESCRIPTION: This example uses PuppeteerCrawler with the context-aware saveSnapshot() utility function. This approach provides a simpler way to capture screenshots with automatic naming and storage, while the crawler manages browser instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page, puppeteerUtils }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Get a unique key for the screenshot based on the URL\n        const key = `screenshot-${request.url.replace(/[:/]/g, '_')}`;\n\n        // Capture and save the screenshot using the utility function\n        // The context is already available in the requestHandler\n        await puppeteerUtils.saveSnapshot({\n            key,\n        });\n    },\n    // Let's limit our crawls to just a few pages\n    maxRequestsPerCrawl: 10,\n});\n\n// Define the starting URLs\nawait crawler.run(['https://apify.com', 'https://crawlee.dev']);\n\nconsole.log('Crawler finished!');\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with JSDOMCrawler. It shows how to configure proxy rotation and session management for web scraping with JSDOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\n{JSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Finding Links without enqueueLinks Function\nDESCRIPTION: Manual implementation of link discovery and enqueueing without using the enqueueLinks helper, demonstrating the underlying process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, crawler }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // find all links and add them to the crawling queue\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n\n        for (const url of links) {\n            await crawler.addRequests([{ url }]);\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset in Crawlee\nDESCRIPTION: Code snippet showing how to import the Dataset class from Crawlee, which is needed for saving extracted data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Basic example of how to use the sendRequest function within BasicCrawler to make HTTP requests. This function uses got-scraping under the hood to mimic browser requests and avoid blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for HTML Element Selection\nDESCRIPTION: Demonstrates the syntax differences between plain browser JavaScript and Cheerio for extracting text content from title elements and collecting all href links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Command to install ts-node for development runtime\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Session Management with JSDOMCrawler\nDESCRIPTION: Example of using proxy session management with JSDOMCrawler. This maintains the same proxy for requests within the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, window, session }) {\n        // The session ensures that the same proxy is used for certain requests\n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using utils.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with the context-aware utils.saveSnapshot() utility. It sets up a crawler that visits multiple URLs and saves snapshots using the utility function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log, utils }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        const key = `${(new URL(request.url)).hostname}`;\n        await utils.saveSnapshot(page, { key });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: This snippet demonstrates how to find all anchor elements (<a>) with href attributes on a page and extract those URLs into an array using the Element API in JSDOMCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Browser Fingerprint Configuration\nDESCRIPTION: TypeScript code example showing how to configure browser fingerprint settings in PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Browser Automation Libraries\nDESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright as the browser automation library.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of Crawlee with Playwright\nDESCRIPTION: Command to manually install Crawlee and Playwright as dependencies in an existing project. Playwright is required separately as it's not bundled with Crawlee to keep the installation size smaller.\nSOURCE: https://github.com/apify/crawlee/blob/master/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Cookie Jar Implementation\nDESCRIPTION: Demonstrates how to use a custom cookie jar with the sendRequest function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Using Full Playwright Docker Image\nDESCRIPTION: Example of using the comprehensive Playwright image that includes all supported browsers (Chromium, Chrome, Firefox, WebKit). This large image is best for multi-browser development or testing.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:20\n```\n\n----------------------------------------\n\nTITLE: Updated Browser Management Methods\nDESCRIPTION: Comparison of old PuppeteerPool methods with new BrowserPool equivalents for common browser management tasks like recycling pages and retiring browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for TypeScript Project\nDESCRIPTION: Sets up a multi-stage Dockerfile to build the TypeScript project and create a production-ready image without development dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Standalone Session and Proxy Management\nDESCRIPTION: Demonstrates how to manually manage sessions with proxy configuration outside of a crawler context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Getting a proxy URL with a specific session ID\nconst sessionId = 'my-session-1';\nconst proxyUrl = await proxyConfiguration.newUrl(sessionId);\nconsole.log(`Proxy URL for session ${sessionId}: ${proxyUrl}`);\n\n// When called with the same session ID, the same proxy URL will be returned\nconst sameProxyUrl = await proxyConfiguration.newUrl(sessionId);\nconsole.log(`Same proxy URL for session ${sessionId}: ${sameProxyUrl}`);\n\n// We can create multiple proxy URLs for multiple sessions\nconst proxyUrl2 = await proxyConfiguration.newUrl('my-session-2');\nconsole.log(`Proxy URL for session my-session-2: ${proxyUrl2}`);\n\n// Creates a proxy URL without a session\n// These proxy URLs are rotated in a round-robin fashion\nconst proxyUrl3 = await proxyConfiguration.newUrl();\nconsole.log(`Proxy URL without session: ${proxyUrl3}`);\n\n// ProxyInfo object provides more information about the proxy and the session\nconst proxyInfo = await proxyConfiguration.newProxyInfo(sessionId);\nconsole.log(`Proxy URL for session ${sessionId}: ${proxyInfo.url}`);\nconsole.log(`Proxy session: ${proxyInfo.sessionId}`);\n\n// Without a sessionId, you get the next proxy in the round-robin\nconst proxyInfo2 = await proxyConfiguration.newProxyInfo();\nconsole.log(`Proxy URL without session: ${proxyInfo2.url}`);\n```\n\n----------------------------------------\n\nTITLE: Adding Batch Requests to Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method, with support for handling large numbers of requests efficiently\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee, providing more control over the queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Use the request queue with a crawler\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        // Extract data from the page\n        // ...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\n// Start the crawl\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Documenting Crawlee Release 2.0.0 in Markdown\nDESCRIPTION: This snippet details the major changes in Crawlee version 2.0.0, including breaking changes such as requiring Node.js >=15.10.0, updating cheerio, and removing deprecated features.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_30\n\nLANGUAGE: markdown\nCODE:\n```\n## [2.0.0](https://github.com/apify/crawlee/compare/v1.3.4...v2.0.0) (2021-08-05)\n\n* **BREAKING**: Require Node.js >=15.10.0 because HTTP2 support on lower Node.js versions is very buggy.\n* **BREAKING**: Bump `cheerio` to `1.0.0-rc.10` from `rc.3`. There were breaking changes in `cheerio` between the versions so this bump might be breaking for you as well.\n* Remove `LiveViewServer` which was deprecated before release of SDK v1.\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function Handler\nDESCRIPTION: Complete handler function implementation for GCP Cloud Functions with CheerioCrawler integration and response handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    return res.send(await crawler.getData())\n}\n```\n\n----------------------------------------\n\nTITLE: Setting responseType to JSON in sendRequest\nDESCRIPTION: Demonstrates how to configure sendRequest to handle JSON responses. By changing the responseType to 'json', the response body will be automatically parsed as JSON.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Legacy vs. New Launch Options Pattern\nDESCRIPTION: Comparison of old and new patterns for launching Puppeteer, showing the more explicit separation of Apify options and browser launch options in the new pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Scraping in Crawlee\nDESCRIPTION: Modified route handler for category pages that enqueues product URLs to a request queue with locking support instead of directly processing them, enabling parallel scraping of product details.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CATEGORY } = Labels;\nroutes[CATEGORY] = async ({ log, request, $, crawler }) => {\n    log.info(`Extracting product urls from category: ${request.url}`);\n\n    // Get the request queue that supports locking\n    const requestQueue = await getOrInitQueue();\n\n    // Get the product links in the category and enqueue them to the queue with locking support\n    const productLinks = $('a.product');\n    log.info(`Found ${productLinks.length} products in category ${request.url}`);\n\n    for (const link of productLinks) {\n        const $link = $(link);\n        // Extract product ID and URL\n        const url = new URL($link.attr('href'), request.loadedUrl).href;\n        const urlPaths = url.split('/');\n        const id = urlPaths[urlPaths.length - 2];\n\n        // Add the URL to the queue with locking support\n        await requestQueue.addRequest({\n            url,\n            userData: {\n                label: Labels.DETAIL,\n                id,\n            },\n        });\n    }\n\n    // Find the \"next page\" link for this category pagination\n    const nextPageUrl = $('a.pagination__next').attr('href');\n    // If there's a next page, enqueue it in the current crawler\n    if (nextPageUrl) {\n        await crawler.addRequests([{\n            url: new URL(nextPageUrl, request.loadedUrl).href,\n            userData: {\n                label: Labels.CATEGORY,\n            },\n        }]);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in JSDOMCrawler\nDESCRIPTION: Demonstrates how to access and inspect the current proxy information within the JSDOMCrawler's request handler. This allows retrieving the proxy URL used for the request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request, window }) {\n        console.log(`Used proxy URL: ${proxyInfo.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain with enqueueLinks\nDESCRIPTION: Shows how to filter links to ensure the crawler only visits pages within the same domain using enqueueLinks, which by default only follows links pointing to the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Set a limit for the number of pages we'll crawl\n    maxRequestsPerCrawl: 20,\n    \n    async requestHandler({ $, request, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // Extract the title of the page\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // The default strategy is to enqueue all links from the same hostname (domain):\n        // hostname for https://crawlee.dev/api/cheerio-crawler/class/CheerioCrawler is crawlee.dev\n        // Cheerio will help us find all URLs, filter only those we want to follow\n        // and enqueue them to the RequestQueue.\n        await enqueueLinks();\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Navigating Key-Value Store Directory Structure in Crawlee\nDESCRIPTION: Shows the directory structure used by Crawlee's key-value stores for storing data. Files are saved in a path that includes the storage directory, store ID, key name, and appropriate file extension.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Request List Basic Implementation\nDESCRIPTION: Demonstrates how to initialize and use a RequestList with a PuppeteerCrawler, including setting up source URLs and basic crawler configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Context Information with BrowserController\nDESCRIPTION: Example showing how to access browser context information like proxy details and session through the BrowserController in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet illustrates how to use Playwright Crawler to crawl all links on a website. It configures the crawler, sets up a request queue, and processes each page by extracting the title, storing data, and enqueueing new links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It creates a PuppeteerCrawler instance, defines a handler function to capture and save screenshots, and starts the crawler with a list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        const screenshotBuffer = await page.screenshot();\n        const key = `screenshot-${request.id}`;\n        await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Context in Pre-launch Hooks\nDESCRIPTION: Example showing how to access the crawlingContext of a request that triggered a browser launch, enabling request-specific browser configuration through crawler.crawlingContexts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with URL Pattern Matching in PlaywrightCrawler\nDESCRIPTION: Example of using the enqueueLinks method with URL glob patterns to filter crawled URLs. The crawler will only follow links matching the specified pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Base Node.js Docker Configuration\nDESCRIPTION: Configuration for the basic Node.js Alpine Linux-based Docker image without browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for proxy usage and session pool management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new BasicCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, sendRequest }) {\n        const { body, headers, statusCode } = await sendRequest();\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Adapting Crawlee Routes for Parallel Scraping in JavaScript\nDESCRIPTION: This code modifies the CATEGORY route handler to enqueue product URLs to a shared request queue instead of processing them immediately. It prepares the scraper for parallel execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getOrInitQueue } from './requestQueue.mjs';\n\n// ...\n\nrouter.addHandler('CATEGORY', async ({ log, $ }) => {\n    log.info('Category page opened!');\n\n    const productCards = $('a.product-item');\n    log.info(`Number of products found: ${productCards.length}`);\n\n    const queue = await getOrInitQueue();\n\n    for (const card of productCards) {\n        const $card = $(card);\n        const url = $card.attr('href');\n        await queue.addRequest({\n            url,\n            label: 'DETAIL',\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler\nDESCRIPTION: Implementation of a crawler that enqueues links from the same domain including subdomains. Matches relative URLs and URLs that share the same domain name regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // This will enqueue links from the same domain including subdomains\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Request Locking Queue with CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to use a custom RequestQueueV2 instance with a crawler by passing it as the requestQueue option. The requestLocking experiment must also be enabled in the crawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Setting up a Basic HTTP Server with Node.js for Crawlee\nDESCRIPTION: Creates a simple HTTP server using Node.js built-in http module that listens on port 3000 and returns a placeholder response. The server logs incoming requests and will later be modified to return scraped page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createServer } from 'http';\nimport { log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    // We will return the page title here later instead\n    res.end('Hello World\\n');\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Cheerio Crawler\nDESCRIPTION: Implementation using CheerioCrawler to enumerate and process all links on a website. Uses enqueueLinks() method to add discovered URLs to the RequestQueue and processes them iteratively. Includes maxRequestsPerCrawl limit and custom data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Using CookieJar with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use a CookieJar from the tough-cookie package with BasicCrawler. This allows for managing cookies across multiple requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example showing the correct way to manage browser resources using BrowserController, which provides a unified API for working with both Puppeteer and Playwright browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Old vs New Launch Function Syntax in JavaScript\nDESCRIPTION: Comparison showing the updated launch options pattern that separates browser-specific options into the launchOptions object, improving clarity and consistency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development in Crawlee Project\nDESCRIPTION: Install ts-node as a development dependency to run TypeScript code directly during development of a Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Using Node.js with Playwright Chrome Docker Image\nDESCRIPTION: Example of using the Docker image with Node.js 20 and Playwright Chrome, along with the recommended package.json configuration using asterisk for the pre-installed library version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Cheerio Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to use CheerioScraper to crawl URLs from a sitemap. It utilizes the Sitemap utility class to download and parse the sitemap, then processes each URL with Cheerio for HTML parsing without browser rendering.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioScraper } from '@crawlee/cheerio';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new CheerioScraper({\n    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests\n});\n\nscraper.router.addDefaultHandler(async ({ $, request, log, pushData }) => {\n    const title = $('title').text();\n    log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n    // Save results as JSON to ./storage/datasets/default\n    await pushData({ title, url: request.loadedUrl });\n});\n\n// Create an instance of the Sitemap class\n// and download the sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Add URLs from the sitemap as requests to the queue\nawait scraper.addRequests(sitemap.urls);\n\nawait scraper.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring headerGeneratorOptions in TypeScript\nDESCRIPTION: Demonstrates how to customize browser fingerprint generation by configuring specific devices, locales, operating systems, and browsers. This helps simulate different types of browser environments for more effective scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generation for BasicCrawler in TypeScript\nDESCRIPTION: This snippet shows how to configure header generation options for BasicCrawler. It demonstrates setting specific devices, locales, operating systems, and browsers for generating browser-like headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: New Launch Context Configuration in JavaScript\nDESCRIPTION: Example of the improved launchContext pattern that explicitly separates Apify-specific options from browser-specific launchOptions, making the configuration more intuitive and consistent with browser-pool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer\nDESCRIPTION: Implementation of multi-URL crawling using Puppeteer Crawler in Crawlee. Shows how to configure a Puppeteer crawler to handle multiple URLs and extract data using browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';\n\nconst sources = [\n    'https://crawlee.dev',\n    'https://apify.com',\n];\n\nconst router = createPuppeteerRouter();\n\nrouter.addDefaultHandler(async ({ page, request, log }) => {\n    const title = await page.title();\n    log.info(`Title of ${request.url} is '${title}'`);\n\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\nconst crawler = new PuppeteerCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(sources);\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue with Locking Support\nDESCRIPTION: Creates a shared request queue with locking support for parallel scraping operations. Includes functionality to initialize or retrieve an existing queue and optionally purge existing requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration } from 'crawlee';\n\nexport async function getOrInitQueue(shouldClear = false) {\n    const config = new Configuration();\n    const queue = await config.getRequestQueue();\n    \n    if (shouldClear) {\n        await queue.drop();\n    }\n    \n    return queue;\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with Browser Lifecycle Hooks\nDESCRIPTION: Demonstrates migrating from the custom launchPuppeteerFunction to browser-pool's preLaunchHooks, which provide a more consistent and flexible approach across browser types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with HttpCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with HttpCrawler to handle IP rotation and session management. Shows how to automatically manage sessions through the HttpCrawler configuration, including handling status codes for session health management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\n// Initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Initialize the crawler\nconst crawler = new HttpCrawler({\n    // To use the proxy IP rotation, you need to set the proxyConfiguration\n    // option. It receives the ProxyConfiguration instance.\n    proxyConfiguration,\n    // This works directly with HttpCrawler\n    useSessionPool: true,\n    // By default this is 3, which means a session is retired\n    // when it gets 3 error (400+) responses.\n    maxSessionUsageCount: 5,\n    // This is the default value and will be used to add\n    // Headers and options to fetch when using a session\n    sessionPoolOptions: {\n        sessionOptions: {\n            // This is used to identify the user agent of the session.\n            // It is used in the default createSessionFunction.\n            // The userAgent is also added automatically to the request\n            // headers when using sessions.\n            maxErrorScore: 1,\n            // This is the default value. It means that sessions with\n            // at least 3 successful responses will be preserved\n            // and reused when the crawler restarts or other sessions\n            // are not available.\n            maxUsageCount: 50,\n        },\n    },\n    async requestHandler({ request, response, session, log }) {\n        const { statusCode } = response;\n  \n        // Based on the status code, mark the session as working, blocked, or failed.\n        if (statusCode === 200) {\n            session.markGood();\n            log.info(`Request ${request.url} succeeded.`);\n        } else if (statusCode === 403) {\n            session.retire();\n            throw new Error('Access forbidden');\n        } else {\n            session.markBad();\n            throw new Error(`Request failed with status code: ${statusCode}`);\n        }\n    },\n});\n\n// Run the crawler\nawait crawler.run([/* list of URLs */]);\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Implementation\nDESCRIPTION: Demonstrates a basic CheerioCrawler setup that fails to extract JavaScript-rendered content from Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Integration with HttpCrawler\nDESCRIPTION: Example of integrating a ProxyConfiguration instance with HttpCrawler. This sets up the HttpCrawler to use proxies when making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json }) {\n        // Process the data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PuppeteerCrawler\nDESCRIPTION: Shows how to access and utilize proxy information during PuppeteerCrawler requests. Helps monitor which proxy is being used during browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo, log }) {\n        // Log information about the currently used proxy\n        if (proxyInfo) {\n            log.info(`Currently using proxy: ${proxyInfo.url}`);\n        }\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Wrapping Cheerio Crawler in AWS Lambda Handler Function\nDESCRIPTION: Modifying the crawler code by wrapping it in an AWS Lambda handler function to make it executable as a Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler\nDESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script performs button clicks for basic arithmetic (1 + 1 =) and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{JSDOMCrawlerRunScriptSource}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Package in TypeScript\nDESCRIPTION: Shows how to install the Crawlee package and its dependencies using npm. It includes options for installing the full meta-package or individual components.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Initializing Crawler with Separate Configuration for AWS Lambda\nDESCRIPTION: Initializes a PlaywrightCrawler with a new Configuration instance to ensure each crawler has its own storage in Lambda environments. The persistStorage option is set to false to prevent interference between crawler instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation\nDESCRIPTION: Shows the full implementation of the sendRequest function that uses gotScraping to make HTTP requests. It includes default parameters and how it integrates with the current request and session context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright\nDESCRIPTION: A comprehensive example that combines all the previous snippets to extract all required product data including URL, manufacturer, title, SKU, price, and stock availability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Input Schema JSON for Bluesky API Actor\nDESCRIPTION: Schema defining the input parameters for the Apify Actor. This includes credential fields (marked as secret), search queries configuration, and options for limiting requests and selecting the operating mode.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"title\": \"Bluesky - Crawlee\",\n  \"type\": \"object\",\n  \"schemaVersion\": 1,\n  \"properties\": {\n    \"identifier\": {\n      \"title\": \"Bluesky identifier\",\n      \"description\": \"Bluesky identifier for API login\",\n      \"type\": \"string\",\n      \"editor\": \"textfield\",\n      \"isSecret\": true\n    },\n    \"appPassword\": {\n      \"title\": \"Bluesky app password\",\n      \"description\": \"Bluesky app password for API\",\n      \"type\": \"string\",\n      \"editor\": \"textfield\",\n      \"isSecret\": true\n    },\n    \"maxRequestsPerCrawl\": {\n      \"title\": \"Max requests per crawl\",\n      \"description\": \"Maximum number of requests for crawling\",\n      \"type\": \"integer\"\n    },\n    \"queries\": {\n      \"title\": \"Queries\",\n      \"type\": \"array\",\n      \"description\": \"Search queries\",\n      \"editor\": \"stringList\",\n      \"prefill\": [\n        \"apify\"\n      ],\n      \"example\": [\n        \"apify\",\n        \"crawlee\"\n      ]\n    },\n    \"mode\": {\n      \"title\": \"Mode\",\n      \"type\": \"string\",\n      \"description\": \"Collect posts or users who post on a topic\",\n      \"enum\": [\n        \"posts\",\n        \"users\"\n      ],\n      \"default\": \"posts\"\n    }\n  },\n  \"required\": [\n    \"identifier\",\n    \"appPassword\",\n    \"queries\",\n    \"mode\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests with enqueueLinks\nDESCRIPTION: Demonstrates how to use the transformRequestFunction with enqueueLinks to modify or filter requests before they are enqueued, allowing for dynamic updates to URLs or metadata.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Handling JSON Response Type with sendRequest\nDESCRIPTION: Shows how to configure the sendRequest function to parse JSON responses. By default, responseType is set to 'text', but it can be changed to 'json' for API requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example showing how to customize browser fingerprints in PlaywrightCrawler by specifying operating system, browser version, and other parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                operatingSystems: ['windows'],\n                browsers: [{\n                    name: 'firefox',\n                    minVersion: 88,\n                }],\n            },\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links Strategy\nDESCRIPTION: Implementation showing how to crawl links from the same domain including subdomains using CheerioCrawler. Matches URLs from the same domain name regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{SameDomainSource}\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in Crawlee\nDESCRIPTION: Example of how to disable dynamic browser fingerprints in a PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Custom Browser Module Configuration in JavaScript\nDESCRIPTION: Examples showing how to use custom modules for browser launching, demonstrating the standardized approach for both Puppeteer and Playwright with the launcher option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with URL Pattern Matching in Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with URL pattern matching via globs in a PlaywrightCrawler. This allows filtering URLs to only process those matching specific patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with Chrome Docker Image\nDESCRIPTION: Example of using a Docker image with pre-installed Playwright and Chrome browser, supporting both headless and headful browser modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Implementation\nDESCRIPTION: Example of manual SessionPool implementation without a crawler for custom session management scenarios\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{StandaloneSource}\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Limit for Crawlers\nDESCRIPTION: Example of setting a maximum limit on the number of requests processed by a crawler using the maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: Simple example showing basic link enqueuing functionality in Crawlee without parameters\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Session Management with JSDOMCrawler and ProxyConfiguration\nDESCRIPTION: Shows how to use SessionPool with JSDOMCrawler and ProxyConfiguration for improved session management. The session ID is used to consistently assign the same proxy to the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // By default, the crawler will use the proxy configuration\n    // and select a proxy based on session ID\n    async requestHandler({ request, window, session }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with PuppeteerCrawler\nDESCRIPTION: Shows how to set up proxy configuration with PuppeteerCrawler. The crawler automatically uses the configured proxies for browser sessions and rotates them as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        console.log(`Fetched ${request.url} with title: ${await page.title()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.map() Method in Crawlee\nDESCRIPTION: Demonstrates how to use the map method to transform dataset items, specifically filtering to find pages with more than 5 header elements and extracting their heading counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\nimport { KeyValueStore } from 'crawlee';\n\n// Fetch the dataset\nconst dataset = await Dataset.open();\n\n// Map all items\nconst moreThan5headers = await dataset.map((item) => {\n    // If there are more than 5 headers, return the count\n    if (item.headingCount > 5) {\n        return item.headingCount;\n    }\n    // Otherwise return undefined which will be filtered out\n    return undefined;\n});\n\n// Save the result to the default key-value store\nawait KeyValueStore.setValue('header-count-more-than-5', moreThan5headers);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Strategy Implementation\nDESCRIPTION: Implementation of a CheerioCrawler that follows links from the same domain including subdomains. This strategy matches relative URLs and URLs that point to the same domain name regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Enqueue links that share the same domain (including subdomains)\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Element Not Found Error Output\nDESCRIPTION: Shows the error message when attempting to access elements without proper waiting mechanisms in headless browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Browser Modules\nDESCRIPTION: Examples of using custom browser modules with both Puppeteer and Playwright launchers, showing the standardization of the launcher option across browser types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: Shows how to integrate SessionPool with HttpCrawler for managing sessions and proxy rotations. It includes setup for the proxy configuration, session pool options, and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    // The `useSessionPool` is enabled by default\n    // Optionally, you can set the maximum number of sessions\n    // to be used in the pool. The default is 1000.\n    sessionPoolOptions: { maxPoolSize: 100 },\n    // Set up proxy rotation using the defined ProxyConfiguration\n    proxyConfiguration,\n    // Function to handle each request\n    async requestHandler({ session, json }) {\n        const data = await json();\n        // Process data...\n\n        // You can mark the session as bad if the request failed\n        // await session.markBad();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using puppeteerUtils.blockRequests in PuppeteerCrawler\nDESCRIPTION: Example showing how to use puppeteerUtils.blockRequests to optimize crawling by blocking unnecessary resource types. This helps improve performance by preventing loading of images, fonts, or other resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, puppeteerUtils } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    preNavigationHooks: [\n        // Block requests to improve performance\n        async ({ page }) => {\n            await puppeteerUtils.blockRequests(page, {\n                urlPatterns: ['.css', '.jpg', '.jpeg', '.png', '.svg', '.gif', '.woff', '.pdf', '.zip'],\n            });\n        },\n    ],\n    async requestHandler({ request, page, log }) {\n        log.info(`Processing ${request.url}...`);\n        // The page loads faster now as resources are blocked\n        // Continue with your scraping logic\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Container for Crawlee Actor with Playwright and Chrome\nDESCRIPTION: This Dockerfile sets up a container environment for a Crawlee-based web scraping actor. It uses the apify/actor-node-playwright-chrome base image, installs production dependencies, copies the application code, and specifies the startup command. It implements Docker layer caching for faster builds during development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Example showing how to completely disable browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Choose from chrome, firefox or safari\n        browserType: 'chrome',\n        // Disable fingerprints\n        useFingerprints: false,\n    },\n    async requestHandler({ page }) {\n        // ... page handling\n    },\n});\n\nawait crawler.run(['https://www.example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for TypeScript Project\nDESCRIPTION: Multi-stage Dockerfile that builds the TypeScript code in the first stage and creates a minimal production image in the second stage. It installs only production dependencies in the final image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with Playwright\nDESCRIPTION: Shows how to use Playwright to find and extract the product title from an H1 element within a div with class 'product-meta'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Scraper Implementation\nDESCRIPTION: Sets up parallel scraping using Node.js child processes, configuring storage and request queue handling for multiple workers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nConfiguration.set('purgeOnStart', false);\n\nconst requestQueue = await getOrInitQueue(false);\n\nconst config = new Configuration({\n    storageClientOptions: {\n        localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: Demonstrates how to configure and use SessionPool with BasicCrawler for managing sessions and proxy rotations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new BasicCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links in a Website using CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to crawl all links found on a website regardless of their domain using the 'All' enqueue strategy with CheerioCrawler. It starts from example.com and will follow any links it finds, including those going to different domains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        // Add all links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // The selector can be omitted, and defaults to `a[href]`\n            selector: 'a[href]',\n        });\n    },\n});\n\n// Start with one request\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Comparing BrowserPool and PuppeteerPool Methods\nDESCRIPTION: Examples showing the difference between PuppeteerPool methods in previous SDK versions and their BrowserPool counterparts in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State Management\nDESCRIPTION: Demonstrates the use of the new useState method for automatic state management in a crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Actor Migration in SuperScraper with TypeScript\nDESCRIPTION: This code snippet sets up an event listener for Actor migration. When a migration event occurs, it calls the function to add timeouts to all pending responses, ensuring proper handling of requests during server transitions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nActor.on('migrating', ()=>{\n    addTimeoutToAllResponses(60);\n});\n```\n\n----------------------------------------\n\nTITLE: Error Output When Not Waiting for Elements\nDESCRIPTION: The console output when attempting to scrape without properly waiting for elements to render, resulting in failure after multiple retries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. This approach allows more control over queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Create a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add requests to queue\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev' },\n]);\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        await enqueueLinks();\n        // ... process the page\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Storage Directory Structure for Key-Value Stores\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing key-value data on disk, where files are organized by store ID and key with appropriate extensions\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using Apify's Docker image with Puppeteer and Chrome pre-installed, suitable for PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It sets up a PuppeteerCrawler, navigates to a URL, and uses the saveSnapshot utility to capture and save the screenshot.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log, puppeteerUtils }) {\n        log.info(`Capturing screenshot of ${request.url}`);\n        await puppeteerUtils.saveSnapshot({\n            key: 'my-screenshot',\n            saveHtml: false,\n            saveScreenshot: true,\n            screenshotQuality: 60,\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Batch Adding Requests with crawler.addRequests()\nDESCRIPTION: Demonstrates how to add multiple requests in batches using the crawler.addRequests() method. This approach allows adding thousands of requests while starting crawling immediately with the first batch.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler for Crawlee Crawler\nDESCRIPTION: JavaScript code wrapping the Crawlee crawler setup and execution in an AWS Lambda handler function, which will be executed by AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: Illustrates the setup of SessionPool with CheerioCrawler for managing sessions and proxy rotations in web scraping tasks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request, $, body }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Screenshot Capture Using Puppeteer Utils saveSnapshot\nDESCRIPTION: Alternative approach using Crawlee's utility function to capture screenshots. This method provides additional context and automatic naming based on the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, utils } from '@crawlee/puppeteer';\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://apify.com');\n\nawait utils.puppeteer.saveSnapshot(page, {\n    key: 'my-screenshot',\n    saveHtml: false,\n});\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for CDN Resources with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to skip navigation for CDN-delivered images while crawling a website. It uses PlaywrightCrawler with the Request#skipNavigation option to efficiently fetch and save images without using the full crawler capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PlaywrightCrawler } from 'crawlee';\nimport { randomUUID } from 'crypto';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, enqueueLinks, sendRequest, log, crawler }) {\n        log.info(`Processing: ${request.url}`);\n\n        if (request.skipNavigation) {\n            // We should have access to the crawler instance\n            log.info('Crawler name:', crawler.name);\n\n            // We will want to fetch the image data without using a browser\n            const imageBuffer = await sendRequest({ url: request.url, responseType: 'buffer' });\n            const filename = `image-${randomUUID()}.jpg`;\n\n            // And save it to the default key-value store\n            await KeyValueStore.getValue('default').setValue(filename, imageBuffer, { contentType: 'image/jpeg' });\n\n            log.info(`Image saved as: ${filename}`);\n\n            return;\n        }\n\n        // For navigation requests, we will want to extract the image URLs\n        // and then enqueue them with the skipNavigation flag\n        const imageUrls = [\n            'https://picsum.photos/200',\n            'https://picsum.photos/300',\n            'https://picsum.photos/400'\n        ];\n\n        // We will enqueue these images with the skipNavigation flag\n        // so that we don't navigate to them, but just fetch them\n        // in the next round of crawling\n        await crawler.addRequests(imageUrls.map((url) => ({\n            url,\n            skipNavigation: true,\n            label: 'image'\n        })));\n\n        // We will also check for links on the page and enqueue those normally\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example showing how to customize browser fingerprints in PlaywrightCrawler by specifying browser versions, operating systems and other parameters to avoid detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            // restrict the websites that can be displayed in the browser's history\n            browserPlugins: [],\n            locales: ['en-US', 'en'],\n            operatingSystems: ['windows', 'linux'],\n            // restrict to only Chrome browsers\n            browsers: [{\n                name: 'chrome',\n                minVersion: 87,\n                maxVersion: 89\n            }]\n        }\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl\nDESCRIPTION: Example showing how to limit the number of requests processed in a crawl using maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: sendRequest Function Implementation in TypeScript\nDESCRIPTION: The internal implementation of the sendRequest function showing how it uses got-scraping with default options. It handles URL, method, body, headers, proxy configuration, session token, and cookie management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController in Handler Functions\nDESCRIPTION: Shows how to properly use BrowserController for managing browser instances and performing browser-specific operations in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Using proxies with sendRequest in TypeScript\nDESCRIPTION: Shows how to specify a proxy URL when making HTTP requests with sendRequest. This example demonstrates passing a proxy server in the format of protocol://username:password@hostname:port.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result Format in Crawlee Dataset\nDESCRIPTION: The expected result of the reduce method demonstration, showing a single value representing the total heading count across all pages. This result would be saved to the key-value store after processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'All' enqueue strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Crawling!!');\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // We can also use the following:\n            // strategy: 'all',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee (TypeScript)\nDESCRIPTION: This snippet shows how to import the PlaywrightCrawler and Dataset modules from Crawlee. These imports are necessary for creating a crawler and saving data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Information via BrowserController\nDESCRIPTION: Example showing how to access information about the browser such as proxy settings and session details through the BrowserController.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Crawlee Package\nDESCRIPTION: Command to install only the Cheerio implementation from Crawlee for lightweight scraping needs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: Shows how to set up SessionPool with PlaywrightCrawler for managing sessions and proxy rotations in browser automation tasks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    // Use the proxyConfiguration\n    proxyConfiguration,\n    async requestHandler({ session, request, page }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Playwright Configuration with Stealth Plugin\nDESCRIPTION: Sample code showing Playwright crawler setup with playwright-extra and stealth plugin integration in a TypeScript environment. Located in src/crawler.ts\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawler-plugins/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightExtraSource}\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Example demonstrating how to explicitly create and use a request queue with a Crawler in Crawlee. This approach creates a named queue and passes it to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, RequestQueue } from 'crawlee';\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add initial requests to the queue\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\nconst crawler = new PlaywrightCrawler({\n    requestQueue,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract product links from the page and add them to the queue\n        // The page is an e-shop category page with products\n        await enqueueLinks({\n            selector: 'a.product-detail-link',\n            requestQueue,\n        });\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Proxy Configuration in Crawlee\nDESCRIPTION: Basic setup of ProxyConfiguration class with custom proxy URLs. Shows how to create a proxy configuration instance and obtain a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in JSDOMCrawler\nDESCRIPTION: Shows how to access proxy details in JSDOMCrawler's request handler to track which proxy is being used for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, proxyInfo }) {\n        // Print information about the currently used proxy directly to the terminal\n        console.log(proxyInfo);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PlaywrightCrawler\nDESCRIPTION: Shows how to access and inspect the current proxy information within the PlaywrightCrawler's request handler. This allows retrieving the proxy URL used for the browser session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request, page }) {\n        console.log(`Used proxy URL: ${proxyInfo.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawling Context in Handler Functions\nDESCRIPTION: Example showing how to access the unified crawling context in handler functions, which now provides consistent access to properties across different handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Finding Links without enqueueLinks\nDESCRIPTION: Alternative approach to finding and enqueuing links without using the enqueueLinks helper. It manually selects all <a> elements with href attributes and adds them to the request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Set a limit for the number of pages we'll crawl\n    maxRequestsPerCrawl: 20,\n    \n    async requestHandler({ $, crawler, request }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // Extract the title of the page\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // Find all links and add them to the crawling queue\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n        \n        // Add all links to the queue\n        await crawler.addRequests(links);\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Configuration in Crawlee\nDESCRIPTION: Sets up a PlaywrightCrawler instance with a custom Configuration that disables storage persistence, which is necessary for serverless environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PlaywrightCrawler in Crawlee\nDESCRIPTION: This snippet shows how to integrate SessionPool with PlaywrightCrawler in Crawlee. It includes proxy configuration and demonstrates session management in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    // The `useSessionPool` option will enable the automatic session management.\n    useSessionPool: true,\n    // Optionally, you can pass session pool configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // Set up proxy servers\n    proxyConfiguration,\n    async requestHandler({ request, session, page, browserController }) {\n        // Do something with the data.\n        // ...\n\n        // Rotate session\n        session.retire();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File Path for Apify Actor\nDESCRIPTION: Demonstrates the correct file path structure for providing input to an Apify actor through the default key-value store. The INPUT.json file needs to be placed in the project's storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This example demonstrates how to completely disable browser fingerprinting in PlaywrightCrawler by setting the useFingerprints option to false in browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // Other crawler options...\n});\n\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with datasets including writing single and multiple rows of data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests before Enqueuing in TypeScript\nDESCRIPTION: This snippet shows how to use the transformRequestFunction to modify or skip requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Min and Max Concurrency for CheerioCrawler\nDESCRIPTION: This snippet shows how to configure minimum and maximum concurrency limits for a crawler. It sets the crawler to operate between 10 and 50 parallel requests, allowing it to scale within these boundaries based on system resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\n// Set the crawler to start with 10 requests, and scale up to a maximum of 50 if possible\nconst crawler = new CheerioCrawler({\n    minConcurrency: 10,\n    maxConcurrency: 50,\n    // other options...\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee\nDESCRIPTION: Code snippet showing how to import the Dataset module along with PlaywrightCrawler from Crawlee, which is necessary for saving scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File Path for Apify Actor\nDESCRIPTION: Shows the file path structure for providing input to an Apify actor via INPUT.json file in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in Apify SDK v1\nDESCRIPTION: Shows how to use the new Crawling Context in SDK v1, which provides a consistent object across different handler functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Installing and Zipping Dependencies for AWS Lambda\nDESCRIPTION: This snippet shows how to install the @sparticuz/chromium package and zip the node_modules folder for use as a Lambda Layer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Running Actor on Apify Platform\nDESCRIPTION: Command to deploy and run an actor on the Apify platform\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest in BasicCrawler\nDESCRIPTION: Shows how to use the new context.sendRequest() helper to process requests through got-scraping in a BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Failed Cheerio Crawler Output\nDESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating that it cannot access JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to download and crawl URLs from a sitemap using Playwright Crawler. It leverages the downloadListOfUrls utility from @crawlee/utils to extract URLs from a sitemap and processes them with Playwright for modern browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { downloadListOfUrls } from '@crawlee/utils';\nimport { PlaywrightCrawler, sleep } from 'crawlee';\n\n// Create the crawler and provide the required configuration\nconst crawler = new PlaywrightCrawler({\n    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, enqueueLinks, page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add any links from the page to the queue\n        await enqueueLinks();\n\n        // Wait for a random amount of time to avoid overloading the site\n        await sleep(Math.random() * 2000);\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.info(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Get the list of URLs from the sitemap\nconst urls = await downloadListOfUrls({ url: 'https://apify.com/sitemap.xml' });\n\n// Add the URLs to the crawler queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in SDK v1 Crawlers\nDESCRIPTION: Example showing how to access the new BrowserPool instance in PuppeteerCrawler and PlaywrightCrawler, which replaces the previous PuppeteerPool implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to use both Request Queue and Request List together in a Crawlee crawler. This approach combines the benefits of both storage types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, RequestList, PuppeteerCrawler } from 'crawlee';\n\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\nconst requestList = await RequestList.open('my-list', sources);\nconst requestQueue = await RequestQueue.open();\n\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests with Crawlee v3\nDESCRIPTION: Demonstrates how to use the new addRequests method to efficiently add multiple requests in batches, with the option to wait for all requests to be added.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function Handler\nDESCRIPTION: Complete implementation of the cloud function handler that initializes and runs the crawler, then returns the collected data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    return res.send(await crawler.getData())\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Custom Storage Settings for AWS Lambda\nDESCRIPTION: JavaScript code demonstrating how to initialize a PlaywrightCrawler with a custom Configuration instance to ensure each crawler uses its own storage and doesn't interfere with other Lambda instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Running PuppeteerCrawler in Headful Mode\nDESCRIPTION: Example showing how to configure PuppeteerCrawler to run in headful mode, which displays the browser UI during development for easier debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headful Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Let the crawler run headful so that you can see the browser.\n    headless: false,\n    // Slow down the crawler to make it easier to see what's happening in the browser.\n    navigationTimeoutSecs: 60,\n    requestHandlerTimeoutSecs: 60,\n    preNavigationHooks: [\n        async ({ page }) => {\n            // Wait for random amount of milliseconds to slow down the crawler.\n            // This is just to make the headful mode more visible for the tutorial.\n            await new Promise((resolve) => setTimeout(resolve, Math.random() * 3000));\n        },\n    ],\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Enqueue links but don't save results\n        // because they would be the same as with CheerioCrawler.\n        await enqueueLinks({\n            // Consider only links that contain 'docs'.\n            globs: ['https://crawlee.dev/**'],\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Basic enqueueLinks Usage in Crawlee\nDESCRIPTION: A basic example of using the enqueueLinks() function without parameters to find all links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Worker Processes\nDESCRIPTION: Main scraper implementation that spawns multiple worker processes for parallel scraping. Includes configuration for worker-specific storage and communication between parent and child processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration } from 'crawlee';\nimport { fork } from 'child_process';\nimport { getOrInitQueue } from './requestQueue.mjs';\n\nConfiguration.set('purgeOnStart', false);\nconst requestQueue = await getOrInitQueue(false);\n\nconst config = new Configuration({\n    storageClientOptions: {\n        localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Disabled Storage Persistence for GCP Cloud Run\nDESCRIPTION: Creates a PlaywrightCrawler instance with storage persistence disabled to accommodate the ephemeral nature of cloud environments. This configuration is essential when deploying to GCP Cloud Run to avoid issues with file system access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to integrate SessionPool with JSDOMCrawler for web scraping using JSDOM. It covers proxy configuration, session management, and handling blocked requests while parsing HTML with JSDOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    // The default sessionPoolOptions are described in BasicCrawler example\n    // and it will work the same out of the box with JSDOMCrawler\n    async requestHandler({ window, request, session }) {\n        const { document } = window;\n\n        // Process the data from the page using JSDOM's window and document\n        const title = document.querySelector('title')?.textContent ?? '';\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        \n        // On blocked page we throw an error to retry the request with a different session\n        if (title === 'Access denied' || title === 'Forbidden') {\n            session!.retire();\n            throw new Error('We got blocked, lets use another session for these requests');\n        }\n\n        session!.markGood();\n    },\n\n    // In case you need to specify your proxy, you need to use proxyConfiguration\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: ['http://user:password@proxy.com:8000']\n    }),\n});\n\nawait crawler.run(['https://crawlee.dev', 'https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Complete Google Maps Scraper Implementation in Python\nDESCRIPTION: Full implementation combining PlaywrightCrawler setup, request handler, and main function to create a working Google Maps scraper that searches for hotels in Bengaluru.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom datetime import timedelta\nimport asyncio\n\nasync def scrape_google_maps(context):\n    \"\"\"\n    Establishes connection to Google Maps and handles the initial page load\n    \"\"\"\n    page = context.page\n    await page.goto(context.request.url)\n    context.log.info(f\"Processing: {context.request.url}\")\n\nasync def main():\n    \"\"\"\n    Configures and launches the crawler with custom settings\n    \"\"\"\n    # Initialize crawler with browser visibility and timeout settings\n    crawler = PlaywrightCrawler(\n        headless=False,  # Shows the browser window while scraping\n        request_handler_timeout=timedelta(\n            minutes=5\n        ),  # Allows plenty of time for page loading\n    )\n\n    # Tell the crawler how to handle each page it visits\n    crawler.router.default_handler(scrape_google_maps)\n\n    # Prepare the search URL\n    search_query = \"hotels in bengaluru\"\n    start_url = f\"https://www.google.com/maps/search/{search_query.replace(' ', '+')}\"\n\n    # Start the scraping process\n    await crawler.run([start_url])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Console Output from Successful Headless Browser Scraping\nDESCRIPTION: This log shows the successful output when using a headless browser with proper waiting for elements to render, containing the text from the actor card.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use a request queue implicitly with a Crawler in Crawlee. The crawler automatically creates and manages the default request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Getting a Public URL for a Key-Value Store Item\nDESCRIPTION: Example showing how to store an item in Key-Value store and generate a public URL for sharing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: js\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Basic Request List Operations with PuppeteerCrawler\nDESCRIPTION: Demonstrates how to initialize and use a RequestList with PuppeteerCrawler, including defining source URLs and basic crawler setup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Actor Usage with Init/Exit in TypeScript\nDESCRIPTION: Shows how to initialize and close an Actor directly using init() and exit() methods. This is the explicit way to handle Actor lifecycle.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox browser in TypeScript\nDESCRIPTION: This example shows how to initialize PlaywrightCrawler with Firefox browser configuration. It demonstrates setting up the crawler to visit a URL, extract data from the page, and handle the crawling process. It also includes configuration for using a headless Firefox browser instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserType: 'firefox',\n    // Let's limit our crawls to make our example run faster\n    maxRequestsPerCrawl: 10,\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Extract valid links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform via CLI\nDESCRIPTION: Command to install Apify CLI and authenticate with your API token to access Apify platform features from your local environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Configuring Navigation Hooks in PuppeteerCrawler (JavaScript)\nDESCRIPTION: Shows the transition from using gotoFunction to preNavigationHooks and postNavigationHooks for more granular control over page navigation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Browser Dependencies\nDESCRIPTION: Commands to install Crawlee with explicit browser dependencies like Playwright or Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Comparing DOM Selection in Browser JavaScript vs Cheerio\nDESCRIPTION: This code snippet demonstrates the difference between using plain browser JavaScript and Cheerio for common DOM selection operations, including getting text content from elements and extracting attributes from multiple elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: Example showing how to customize browser fingerprints in PuppeteerCrawler by configuring specific browser types, device types, and operating systems to reduce the chance of getting blocked during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Configures fingerprints generation\n        fingerprintOptions: {\n            // Use only Chrome browsers\n            browsers: [\n                { name: 'chrome', minVersion: 88 },\n            ],\n            // Use only desktop devices\n            devices: [\n                'desktop',\n            ],\n            // Use only specified operating systems\n            operatingSystems: [\n                'windows',\n                'macos',\n            ],\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use a request queue implicitly with a Crawler in Crawlee. The crawler automatically creates and manages the default request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: This code demonstrates how to extract a product's SKU (Stock Keeping Unit) using Playwright. It targets a span element with the specific class 'product-meta__sku-number'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PuppeteerCrawler\nDESCRIPTION: Demonstrates setting up PuppeteerCrawler with proxy configuration to automate Chrome/Chromium browser interactions through proxies for web scraping dynamic content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ page, request }) {\n        console.log(`Processing: ${request.url}`);\n        const title = await page.title();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling and configures logging. The crawler is then run with a starting URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with CheerioCrawler\nDESCRIPTION: Demonstrates setting up proxy configuration with CheerioCrawler. The crawler will automatically use the configured proxies for all requests and rotate them appropriately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $}) {\n        console.log(`Fetched ${request.url} with title: ${$('title').text()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: This code snippet uses the Element API to find all <a> elements with an href attribute on a page and extracts their href values into an array. It demonstrates the use of querySelectorAll and Array.from methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Creating INPUT.json File for Actor Input in Bash\nDESCRIPTION: This bash snippet shows the file path where the INPUT.json file should be created to provide input to the actor. The file should be placed in the default key-value store of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Crawling Context IDs\nDESCRIPTION: Demonstrates how to use the new 'id' property of crawling contexts to maintain cross-context access, useful for managing master and child pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title with Playwright\nDESCRIPTION: Using Playwright to extract the product title from an HTML page. It locates the h1 element inside a div with the class 'product-meta' and extracts its text content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Complete CheerioCrawler Lambda Implementation with Data Return\nDESCRIPTION: Full implementation of an AWS Lambda handler using CheerioCrawler, which runs the crawler and returns the scraped data in the Lambda response. This demonstrates the complete pattern for running Crawlee on AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Installing PlaywrightCrawler Manually\nDESCRIPTION: Command to manually install Crawlee with Playwright for using PlaywrightCrawler in an existing Node.js project.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Creating Input JSON File for Crawlee Projects\nDESCRIPTION: Bash command showing the file path for creating an INPUT.json file in the default key-value store. This file is used to provide input data to the actor when it runs locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Using BrowserPool with Lifecycle Hooks in Crawlers\nDESCRIPTION: Shows how to configure a PuppeteerCrawler with BrowserPool options, including lifecycle hooks like preLaunchHooks to dynamically modify browser launch settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Context IDs\nDESCRIPTION: Example demonstrating how to access and manipulate data across different crawling contexts using context IDs.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Commands to install the Apify CLI and log in with your API token to access Apify platform features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Puppeteer Crawler in Crawlee\nDESCRIPTION: This code shows how to use Puppeteer Crawler to extract and crawl all links from a website. It creates a RequestQueue, initializes a PuppeteerCrawler, and uses enqueueLinks() to add new links to the queue as it navigates through the site. Note that this requires the apify/actor-node-puppeteer-chrome image on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content with Cheerio\nDESCRIPTION: This snippet shows how to find the first h2 element on a page and extract its text content using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler for Cheerio Crawler\nDESCRIPTION: Wraps the crawler initialization and execution in an AWS Lambda handler function for serverless deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Git Commit Reference\nDESCRIPTION: Git commit hash reference for tracking specific changes.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n([acc6344](https://github.com/apify/crawlee/commit/acc6344f0e52854b4c4c833dbf7aede2547c111e))\n```\n\n----------------------------------------\n\nTITLE: Migrating from Apify.events to Actor.on\nDESCRIPTION: Example showing how to migrate from the old Apify.events system to the new Actor.on pattern for event handling in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example showing best practices for managing browsers and pages with BrowserController, including proper browser closing and cross-browser compatible cookie management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler\nDESCRIPTION: Code example demonstrating how to crawl category pages by enqueuing only specific links matching a CSS selector. This uses the selector parameter to target only collection items and labels them as 'CATEGORY'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure in Crawlee\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing dataset records on disk.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue Implicitly with Crawler in Crawlee\nDESCRIPTION: Shows how to use a Request Queue implicitly with a Crawler by providing initial URLs that are automatically added to the default queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Initial list of URLs to crawl\n    // The crawler will automatically create and use a request queue for these URLs\n    startUrls: ['https://example.com'],\n    // This function is called for each URL\n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        \n        // Extract links from the current page\n        // and add them to the crawling queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler\nDESCRIPTION: Example code showing how to use CheerioCrawler to crawl only specific links that match a glob pattern. The crawler processes links from crawlee.dev and only enqueues URLs that match the documentation patterns using the globs property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('enqueueing new URLs');\n        // Add new URLs to the queue according to our glob pattern\n        await enqueueLinks({\n            globs: [\n                'https://crawlee.dev/docs/**', // all documentation pages\n                'https://crawlee.dev/api/**', // all API documentation pages\n            ],\n        });\n    },\n    // Limit the number of connections\n    maxRequestsPerCrawl: 20,\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: This example illustrates standalone usage of SessionPool in Crawlee without a crawler. It demonstrates manual session management and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool, Session, ProxyConfiguration } from 'crawlee';\nimport got from 'got';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst sessionPool = new SessionPool({\n    maxPoolSize: 100,\n});\n\nconst makeRequest = async (url) => {\n    const session = await sessionPool.getSession();\n    const proxyUrl = await proxyConfiguration.newUrl(session.id);\n\n    try {\n        const response = await got(url, {\n            headers: {\n                'User-Agent': session.userData.userAgent,\n            },\n            proxyUrl,\n            ...(session.getCookieString(url) && {\n                headers: {\n                    Cookie: session.getCookieString(url),\n                },\n            }),\n        });\n\n        session.markGood();\n\n        // update cookies\n        const cookies = response.headers['set-cookie'];\n        if (cookies) {\n            session.setCookies(cookies, url);\n        }\n\n        return response;\n    } catch (error) {\n        session.markBad();\n        throw error;\n    }\n};\n\nconst url = 'https://example.com';\nconst response = await makeRequest(url);\nconsole.log(response.body);\n\n// we can create sessions manually\nconst session = new Session({\n    id: '123',\n    // `userData` can be used for storing custom data\n    // related to session\n    userData: {\n        // for example we can use it for storing and rotating \n        // user agents\n        userAgent: 'Custom User Agent',\n    },\n});\n\n// and add it to the pool\nawait sessionPool.addSession(session);\n\n// we need to destroy the pool when we're done\n// otherwise node.js might hang\nawait sessionPool.teardown();\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Configuration\nDESCRIPTION: Docker configuration for Playwright with WebKit browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for CheerioCrawler with Data Return\nDESCRIPTION: A complete implementation of the Lambda handler function that instantiates a crawler, runs it, and returns the scraped data with a successful status code. This ensures the Lambda returns useful data after execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool with Lifecycle Hooks\nDESCRIPTION: Example of configuring BrowserPool options and lifecycle hooks in crawler initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to use both request queue and request list together in a crawler. This approach can be useful for handling both initial and dynamically added URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Prepare initial list of URLs\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open request list and request queue\nconst requestList = await RequestList.open('my-list', sources);\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Create a crawler that will process requests from both list and queue\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page...\n\n        // Optionally, add more requests to the queue\n        await enqueueLinks();\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Downloading Files Using FileDownload Crawler in Crawlee with TypeScript\nDESCRIPTION: This code demonstrates how to download files using Crawlee's FileDownload crawler class and save them to the default key-value store. It downloads three different file types (image, PDF, and CSV) by making HTTP requests to specified URLs and stores them with appropriate content types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/file_download.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownload, KeyValueStore } from 'crawlee';\n\n// First we need to create the file download instance\nconst fileDownload = new FileDownload();\n\n// Next we need to create the download requests\n// which have mandatory URL and saveAs properties.\nconst requests = [\n    {\n        url: 'https://apify.com/img/favicon.svg',\n        saveAs: 'favicon.svg',\n        contentType: 'image/svg+xml',\n    },\n    {\n        url: 'https://apify.com/docs/crawlee-simple-graph.png',\n        saveAs: 'crawlee-simple-graph.png',\n        contentType: 'image/png',\n    },\n    {\n        url: 'https://apify.com/docs/crawlee.pdf',\n        saveAs: 'crawlee.pdf',\n        contentType: 'application/pdf',\n    },\n    {\n        url: 'https://cdn.jsdelivr.net/gh/apify/apify-js@master/README.md',\n        saveAs: 'apify-js-readme.md',\n        contentType: 'text/markdown',\n    },\n    {\n        url: 'https://datasets.imdbws.com/title.ratings.tsv.gz',\n        saveAs: 'title.ratings.tsv.gz',\n        contentType: 'text/tab-separated-values',\n    },\n];\n\n// Run the crawler with the specified requests\nawait fileDownload.run(requests);\n\n// Print a message after all files have been downloaded\nconsole.log('Files have been downloaded and saved to the default key-value store!');\n\n// We can also download a specific file directly\n// to the default keyValueStore with the `downloadFileToKeyValueStore()` function\nawait FileDownload.downloadFileToKeyValueStore({\n    url: 'https://raw.githubusercontent.com/apify/crawlee/master/packages/core/package.json',\n    saveAs: 'package.json',\n    contentType: 'application/json',\n    keyValueStore: await KeyValueStore.open('default'),\n});\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain with enqueueLinks\nDESCRIPTION: Shows how to use enqueueLinks with its default behavior to stay on the same hostname, filtering out links that would lead to external domains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // By default, enqueueLinks will only enqueue new URLs from the same hostname.\n        await enqueueLinks();\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio and Browser JavaScript DOM Manipulation\nDESCRIPTION: This snippet demonstrates how to use Cheerio's $ function to manipulate HTML elements, compared to plain JavaScript in a browser environment. It shows examples of selecting the title element and extracting href attributes from links.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser in TypeScript\nDESCRIPTION: This example demonstrates how to configure PlaywrightCrawler to use Firefox browser instead of the default browser. It shows how to set the browser option to 'firefox' and properly configure the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Firefox browser.\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserType: 'firefox',\n    // Let's limit our crawls to make the example quick.\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            title,\n            url: request.url,\n            succeeded: true,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Apify Actor\nDESCRIPTION: CLI commands to create a new actor project and run it locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Updated Launch Context Configuration (JavaScript)\nDESCRIPTION: Demonstrates the transition from launchPuppeteerOptions to the new launchContext object that explicitly separates Apify and Puppeteer options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() in Crawlee\nDESCRIPTION: Example showing how to manually initialize and exit the Actor environment using the explicit init() and exit() methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the URL string. It takes advantage of the URL structure where manufacturer names appear in a predictable pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Example showing how to completely disable the browser fingerprinting feature in PuppeteerCrawler by setting useFingerprints to false in the browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Here comes the important part - we disable fingerprints completely\n    browserPoolOptions: {\n        // This will tell the crawler to not use fingerprints at all\n        useFingerprints: false,\n    },\n    // Process the scraped data\n    async requestHandler({ request, page }) {\n        const data = {\n            url: request.url,\n            title: await page.title(),\n        };\n\n        // Save the data to the default dataset\n        await Dataset.pushData(data);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://puppeteer.dev']);\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue with Request List in Crawlee\nDESCRIPTION: Example showing how to combine Request Queue with Request List to handle both initial batch URLs and dynamically added URLs during crawling. Uses the Request List for batch initialization and the Queue for dynamic additions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, RequestList, RequestQueue } from 'crawlee';\n\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Initialize the request list\nconst requestList = await RequestList.open('my-list', sources);\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\nconst crawler = new PlaywrightCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract links from the page and add them to the queue\n        await enqueueLinks({\n            requestQueue,\n        });\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country in Crawlee\nDESCRIPTION: Illustrates how to create an Apify Proxy configuration with specific proxy groups and country selection. This example sets up residential proxies from the US.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PlaywrightCrawler\nDESCRIPTION: Shows how to use PlaywrightCrawler with proxy configuration to automate browser interactions through proxies, enabling interaction with dynamic web content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ page, request }) {\n        console.log(`Processing: ${request.url}`);\n        const title = await page.title();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests in PlaywrightCrawler\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It utilizes the Request#skipNavigation option and sendRequest function.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, sendRequest, log }) {\n        if (request.skipNavigation) {\n            log.info(`Skipping navigation for ${request.url}`);\n            const imageBuffer = await sendRequest({ url: request.url });\n            await KeyValueStore.setValue(request.url, imageBuffer, { contentType: 'image/jpeg' });\n            return;\n        }\n\n        // Process the page as normal\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Find all image URLs on the page\n        const imageUrls = await page.evaluate(() => {\n            return Array.from(document.querySelectorAll('img')).map((img) => img.src);\n        });\n\n        // Enqueue the image URLs with skipNavigation set to true\n        for (const imageUrl of imageUrls) {\n            await crawler.addRequests([{\n                url: imageUrl,\n                skipNavigation: true,\n            }]);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links using CheerioCrawler\nDESCRIPTION: Example demonstrating how to crawl links from the same hostname using the 'same-hostname' strategy. Only processes URLs that match the original hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Only add links that share the same hostname\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Stage Dockerfile for Crawlee Project with Playwright & Chrome\nDESCRIPTION: This Dockerfile defines a complete build process for a Crawlee project that uses Playwright with Chrome. It employs a multi-stage build to optimize the final image size, first building the project with development dependencies, then creating a lean production image. The configuration handles proper file ownership, dependency management, and includes XVFB setup for headful browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Rotation Middleware in Scrapy\nDESCRIPTION: Configuration in settings.py to set up rotating proxies in Scrapy. This includes adding the appropriate middleware to DOWNLOADER_MIDDLEWARES and defining a list of proxy servers to use.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOADER_MIDDLEWARES = {\n    # Lower value means higher priority\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,\n    'scrapy_rotating_proxies.middlewares.RotatingProxyMiddleware': 610,\n    'scrapy_rotating_proxies.middlewares.BanDetectionMiddleware': 620,\n}\n\nROTATING_PROXY_LIST = [\n    'proxy1.com:8000',\n    'proxy2.com:8031',\n    # Add more proxies as needed\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up a PlaywrightCrawler with Router in Crawlee\nDESCRIPTION: Main entry point file that initializes a PlaywrightCrawler using a router for request handling. This demonstrates how to set up the crawler with proper logging and execute it against a starting URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Node.js Version Tag Specification in Dockerfile\nDESCRIPTION: Example of specifying Node.js version 16 in a Dockerfile using the actor-node base image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access with Crawling Context IDs\nDESCRIPTION: Example showing how to use the new crawling context ID property to access data across different contexts, which is useful for cross-context operations like manipulating a master page from another handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Cheerio URL Crawler Implementation\nDESCRIPTION: Web scraping implementation using Cheerio Crawler to handle multiple URLs. Uses basic HTTP requests to fetch and parse content.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore, createCheerioRouter, Cheeriorawler } from 'crawlee';\n\n// Example data structure\ninterface Article {\n    url: string;\n    title: string;\n    description: string;\n}\n\n// Add URLs to the list here\nconst START_URLS = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction',\n];\n```\n\n----------------------------------------\n\nTITLE: Checking Stock Availability with Playwright in JavaScript\nDESCRIPTION: Demonstrates how to check if a product is in stock by locating a specific element and counting its occurrences.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Crawling a Single URL using got-scraping in TypeScript\nDESCRIPTION: This code demonstrates how to use the got-scraping package to fetch the HTML content of a web page. It imports the necessary module, makes a request to a specific URL, and logs the HTML content to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\n// Fetch the HTML of a web page\nconst response = await gotScraping('https://crawlee.dev');\n\n// Print the HTML to terminal\nconsole.log(response.body);\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK with API Token\nDESCRIPTION: JavaScript code snippet showing how to configure the Apify SDK with an API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management - JavaScript\nDESCRIPTION: Demonstrates how to use SessionPool independently without a crawler for manual session management\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{StandaloneSource}\n```\n\n----------------------------------------\n\nTITLE: Running PuppeteerCrawler in Headful Mode\nDESCRIPTION: Example of how to run PuppeteerCrawler in headful mode, which shows the browser UI during crawling. This is useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // ... other options\n    headless: false,\n    requestHandlerTimeoutSecs: 30,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Setting minConcurrency and maxConcurrency in Crawlee\nDESCRIPTION: Configures a CheerioCrawler with minimum and maximum concurrency limits of 10 and 50 respectively, controlling the range of parallel requests the crawler can make.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // other configuration options\n    minConcurrency: 10,\n    maxConcurrency: 50,\n});\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler and Cheerio in TypeScript\nDESCRIPTION: An alternative implementation that combines PlaywrightCrawler with Cheerio for HTML parsing. This approach uses Playwright to render the JavaScript-based website and Cheerio to more easily extract the category data from the resulting HTML.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const title = await page.title();\n        console.log(`Title of the page: ${title}`);\n\n        const html = await page.content();\n        const $ = cheerio.load(html);\n\n        $('.collection-block-item').each((_, el) => {\n            console.log('Collection text content:', $(el).text());\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Event Handler Migration Example (TypeScript)\nDESCRIPTION: Demonstrates the migration from Apify.events to Actor.on for event handling in the new version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer with utils.puppeteer.saveSnapshot() Helper\nDESCRIPTION: This example shows how to use the utils.puppeteer.saveSnapshot() helper function to capture and save a screenshot. This utility simplifies the process by handling the key generation and storage operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\nimport { puppeteerUtils } from 'crawlee';\n\n// Launch the browser\nconst browser = await launch();\n\ntry {\n    // Open a new page\n    const page = await browser.newPage();\n    await page.goto('https://example.com');\n\n    // Use puppeteerUtils to save a screenshot\n    await puppeteerUtils.saveSnapshot(page, { key: 'detail' });\n    \n    console.log(`Screenshot of 'example.com' saved to Key-value store.`);\n} finally {\n    // Close the browser\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Session Management with PuppeteerCrawler and ProxyConfiguration\nDESCRIPTION: Shows how to use SessionPool with PuppeteerCrawler and ProxyConfiguration for improved session management. The session ID is used to consistently assign the same proxy to the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // By default, the crawler will use the proxy configuration\n    // and select a proxy based on session ID\n    async requestHandler({ request, page, session }) {\n        // Process page\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: BrowserPool Methods vs PuppeteerPool Methods\nDESCRIPTION: Comparison of PuppeteerPool methods with their BrowserPool equivalents, showing the changes in API for recycling pages, retiring browsers, and other browser management operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Logging with Crawlee's Context-Aware Logger\nDESCRIPTION: Shows how to use Crawlee's built-in logging system within a request handler. The context-scoped log instance automatically prefixes messages with the crawler name for better log organization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Custom Configuration for AWS Lambda\nDESCRIPTION: Sets up a basic Cheerio crawler with a custom Configuration instance to prevent storage persistence on Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Crawling a Single URL with Cheerio in JavaScript\nDESCRIPTION: This snippet demonstrates how to use got-scraping to fetch HTML from a specific URL and then process it using Cheerio. It extracts the page title and stores it in the dataset. The code is designed to run within the Crawlee framework.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\nimport { gotScraping } from 'got-scraping';\nimport cheerio from 'cheerio';\n\n// Add URL you want to scrape\nconst url = 'https://example.com';\n\nconsole.log('Fetching URL: ', url);\n\nconst response = await gotScraping(url);\nconst html = response.body;\nconst $ = cheerio.load(html);\n\nconst title = $('title').text();\n\nconsole.log('Page title: ', title);\n\nawait Dataset.pushData({\n    url,\n    title,\n});\n\nconsole.log('Data saved!');\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler with JavaScript-rendered Content\nDESCRIPTION: This code attempts to scrape content from a JavaScript-rendered page using CheerioCrawler, which will fail because CheerioCrawler can't execute client-side JavaScript. It tries to extract text from actor cards on Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Standalone Usage of SessionPool in Crawlee\nDESCRIPTION: This snippet demonstrates how to use SessionPool standalone in Crawlee without a crawler. It shows how to create a SessionPool, configure it, and manage sessions manually.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool } from 'crawlee';\n\nconst sessionPool = new SessionPool({\n    maxPoolSize: 100,\n});\n\nconst session1 = await sessionPool.getSession();\nsession1.userData.foo = 'bar';\n\nconst session2 = await sessionPool.getSession();\nconsole.log(session2.userData.foo); // undefined\n\nawait sessionPool.persistState();\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Actor Manually with TypeScript\nDESCRIPTION: Example of manually initializing and exiting an Apify Actor using the init() and exit() methods. This approach requires explicit initialization and cleanup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP Cloud Functions with Crawlee\nDESCRIPTION: Sets the 'main' field in package.json to point to the entry point file for GCP Cloud Functions. This configuration tells GCP where to find the handler function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Proxy Information in CheerioCrawler with TypeScript\nDESCRIPTION: This code demonstrates how to inspect and log the current proxy details within the CheerioCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo }) {\n        if (proxyInfo) {\n            console.log(`Proxy URL: ${proxyInfo.url}`);\n        }\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest() Helper with BasicCrawler\nDESCRIPTION: Demonstrates the context.sendRequest() helper which allows processing Request objects through got-scraping. This replaces the deprecated requestAsBrowser functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Filtering\nDESCRIPTION: Example of using glob patterns to filter URLs for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Request List with a Crawler\nDESCRIPTION: Demonstrates how to use a request list to process a predefined set of URLs with a crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating AWS Chromium with Crawlee\nDESCRIPTION: Implementation showing how to configure Crawlee to use AWS-compatible Chromium with necessary launch options and hardware settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Using Only Request Queue with Batch Additions in Crawlee\nDESCRIPTION: Demonstrates how to use just a Request Queue with batch additions of requests, making it more efficient for handling large numbers of initial URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, RequestQueue } from 'crawlee';\n\n// Create a request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// We can now add all the URLs we want to crawl in batches\nawait requestQueue.addRequests([\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n    // ... you can add thousands more URLs here\n]);\n\n// And then we are good to go\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    async requestHandler({ request }) {\n        console.log(`Processing ${request.url}...`);\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Proxy Configuration in Crawlee\nDESCRIPTION: Shows how to initialize the Apify Proxy configuration using the Actor class, which allows using Apify's proxy infrastructure for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Screenshot Capture with page.screenshot()\nDESCRIPTION: Implementation of screenshot capture within a PuppeteerCrawler context using the page.screenshot() method. Handles multiple pages and saves screenshots with URL-based keys.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from '@crawlee/puppeteer';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const screenshotBuffer = await page.screenshot();\n        const key = `screenshot-${request.urlDetails.domain}`;\n\n        await KeyValueStore.setValue(key, screenshotBuffer, {\n            contentType: 'image/png',\n        });\n    },\n});\n\nawait crawler.run(['https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code as Apify Actor using init() and exit()\nDESCRIPTION: Example of using Actor.init() and Actor.exit() to run Crawlee code as an Apify actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, which enables automation of Firefox and Webkit browsers in addition to Chromium.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Updating Event Handler Registration in TypeScript\nDESCRIPTION: Demonstrates the migration from Apify.events to Actor.on for event handling in the new version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Extracting and Processing Product Price with Playwright in JavaScript\nDESCRIPTION: This snippet shows how to extract a product price from a webpage, filter the element containing the price, extract the numeric part, and convert it to a number. It handles formatting issues like commas in the price.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with PlaywrightCrawler\nDESCRIPTION: Demonstrates setting up proxy configuration with PlaywrightCrawler. The crawler will automatically use the configured proxies for browser sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        console.log(`Fetched ${request.url} with title: ${await page.title()}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Managing Default Request Queue Path Structure\nDESCRIPTION: Shows the file path structure used by Crawlee to store request queue data on the local disk.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Setting Up a RequestQueueV2 for Request Locking\nDESCRIPTION: Example of creating a RequestQueueV2 instance that supports request locking. This new queue type is required for the locking functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Session-Based Proxy Management Example\nDESCRIPTION: Demonstrates standalone proxy configuration with session management for consistent IP usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: ['http://proxy-1.com', 'http://proxy-2.com']\n});\n\n// Getting proxy URL for a specific session\nconst sessionId1 = 'session1';\nconst proxyUrl1 = await proxyConfiguration.newUrl(sessionId1);\nconsole.log(proxyUrl1); // http://proxy-1.com\n\n// Same session = same proxy URL\nconst proxyUrl2 = await proxyConfiguration.newUrl(sessionId1);\nconsole.log(proxyUrl2); // http://proxy-1.com\n\n// Different session = different proxy URL\nconst proxyUrl3 = await proxyConfiguration.newUrl('session2');\nconsole.log(proxyUrl3); // http://proxy-2.com\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer Using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore, launchPuppeteer } from 'crawlee';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nconst screenshotBuffer = await page.screenshot();\nawait KeyValueStore.setValue('my-screenshot', screenshotBuffer, { contentType: 'image/png' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Implementation in TypeScript\nDESCRIPTION: Complete example of initializing and running a CheerioCrawler to extract data from web pages. The example shows how to set up a requestHandler that extracts data using Cheerio selectors and saves it to a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/cheerio-crawler/README.md#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    requestList,\n    async requestHandler({ request, response, body, contentType, $ }) {\n        const data = [];\n\n        // Do some data extraction from the page with Cheerio.\n        $('.some-collection').each((index, el) => {\n            data.push({ title: $(el).find('.some-title').text() });\n        });\n\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n            data,\n        })\n    },\n});\n\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 in CheerioCrawler\nDESCRIPTION: This example demonstrates how to use a RequestQueueV2 instance with locking support in a CheerioCrawler. It includes enabling the request locking experiment and passing the custom queue to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Accessing ProxyInfo in HttpCrawler\nDESCRIPTION: Example showing how to access proxy information in HttpCrawler's requestHandler using the proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionHttpSource}\n```\n\n----------------------------------------\n\nTITLE: Scraping and Processing Product Price\nDESCRIPTION: Shows how to extract and process the product price by filtering elements, handling currency symbols and converting to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler and Cheerio in TypeScript\nDESCRIPTION: This code snippet shows an alternative approach using PlaywrightCrawler with Cheerio for HTML parsing. It visits the start URL, extracts the HTML content, and uses Cheerio to select and print the text of all category elements. This method combines the benefits of Playwright for JavaScript rendering and Cheerio for efficient HTML parsing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const html = await page.content();\n        const $ = cheerio.load(html);\n\n        $('.collection-block-item').each((_, el) => {\n            console.log($(el).text());\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: CLI command to deploy the actor code to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Docker Deployment Configuration\nDESCRIPTION: Multi-stage Dockerfile for building and running TypeScript project\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\nFROM apify/actor-node:16\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Installing and Zipping Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for uploading as a Lambda Layer to AWS.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns\nDESCRIPTION: Demonstrates how to use glob patterns to filter which URLs should be enqueued, providing more specific control than the built-in strategies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData\nDESCRIPTION: Demonstrates how to save extracted data to Crawlee's Dataset storage using the pushData method\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Adjusting Crawlee Code for Apify Platform\nDESCRIPTION: Modified main script to integrate with Apify Platform, including Actor initialization and exit calls.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Request Transform Function\nDESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use JSDOMCrawler to interact with a React calculator app. It clicks buttons to perform a calculation and extracts the result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const calculatorButtons = window.document.querySelectorAll('button');\n        const findButton = (text: string) => Array.from(calculatorButtons).find((button) => button.textContent === text);\n\n        // Click buttons 1 + 1 =\n        await findButton('1')!.click();\n        await findButton('+')!.click();\n        await findButton('1')!.click();\n        await findButton('=')!.click();\n\n        // Extract the result\n        const result = window.document.querySelector('.component-display')!.textContent;\n        log.info(`The result is: ${result}`);\n\n        await Dataset.pushData({ result });\n    },\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with a crawlee.json Configuration\nDESCRIPTION: Example showing how to use Crawlee with configuration coming from a crawlee.json file. No explicit configuration is passed to the crawler in the code, but it will pick up settings from the crawlee.json file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in CheerioCrawler\nDESCRIPTION: Demonstrates how to access proxy information in CheerioCrawler's request handler to log and track which proxy is being used for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request }) {\n        if (proxyInfo) {\n            // Format of proxyInfo.url: <protocol>://<username>:<password>@<hostname>:<port>\n            console.log(`Processing ${request.url} using proxy ${proxyInfo.url}`);\n        } else {\n            console.log(`Processing ${request.url} directly`);\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Function in TypeScript\nDESCRIPTION: Example of using the Actor.main() method, which is a wrapper that handles initialization and cleanup automatically. This is equivalent to the manual approach but with less boilerplate.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Including Subdomains in Crawling with enqueueLinks\nDESCRIPTION: Configuration of the enqueueLinks function to include subdomains in the crawl by using the 'same-domain' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Making HTTP requests with sendRequest() in BasicCrawler\nDESCRIPTION: Shows how to use the context.sendRequest() helper that processes context-bound Request objects through got-scraping. This replaces the deprecated requestAsBrowser functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() in Crawlee\nDESCRIPTION: Example showing the simplified approach to running a Crawlee actor using Actor.main(). This method wraps the user code in try/catch and automatically handles initialization and cleanup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State in CheerioCrawler (TypeScript)\nDESCRIPTION: Demonstrates the use of the useState method to manage and automatically save crawler state in a CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue for Batch Request Addition in Crawlee\nDESCRIPTION: Demonstrates how to use the Request Queue to add a batch of requests efficiently, replacing the need for combining Request Queue and Request List in some scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Open the default request queue\nconst requestQueue = await RequestQueue.open();\n\n// Prepare a list of URLs\nconst urls = [\n    'https://example.com/page-1',\n    'https://example.com/page-2',\n    'https://example.com/page-3',\n    // ... even more URLs\n];\n\n// Add all the URLs to the queue at once\nawait requestQueue.addRequests(urls);\n\n// Create a crawler that will process URLs from the queue\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    // ... other options\n    async requestHandler({ request, page }) {\n        // ... process the page\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Scraping Static Content with CheerioCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Initializing Playwright Crawler with Basic Configuration\nDESCRIPTION: Sets up a basic Playwright crawler instance with persistent storage disabled, targeting the Crawlee documentation website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Meta-Package\nDESCRIPTION: Command to install the complete Crawlee package which contains all crawler classes and functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to customize browser fingerprints in PlaywrightCrawler by specifying particular browser versions, operating systems, and other fingerprint parameters. The example shows configuration for Chrome on Windows 10 with a specified locale.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Set the fingerprint generation strategy\n        fingerprintOptions: {\n            // Use desktop fingerprints\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'chrome', minVersion: 88 },\n                    // { name: 'firefox', minVersion: 69 } // you can add more browsers\n                ],\n                devices: [\n                    'desktop',\n                ],\n                operatingSystems: [\n                    'windows',\n                ],\n            },\n            // Use the same fingerprint for the same session\n            fingerprintCacheSize: 1,\n        },\n    },\n    // Playwright native options, automatically passed to Playwright\n    launchContext: {\n        // In Playwright we can set the language and geolocation directly\n        userPreferences: {\n            // Set the language to US English\n            languages: ['en-US'],\n        },\n        geolocation: {\n            // Set the geolocation to New York\n            longitude: -74.006,\n            latitude: 40.7128,\n            accuracy: 100,\n        },\n    },\n    // Function to process the crawled data\n    async requestHandler({ page, parseWithCheerio, request }) {\n        const $ = await parseWithCheerio();\n        // ... processing logic\n    },\n});\n\n// Run the crawler\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in JSDOMCrawler\nDESCRIPTION: Shows how to access proxy information within JSDOMCrawler's request handler. This allows for monitoring which proxy is being used for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, window }) {\n        // Prints information about the proxy used for the request\n        if (proxyInfo) {\n            console.log(proxyInfo.url);\n        }\n        \n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Running Apify Actor with Purge\nDESCRIPTION: Command to run the Apify Actor and purge previous data to ensure clean results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\napify run --purge\n```\n\n----------------------------------------\n\nTITLE: Using transformRequestFunction in enqueueLinks\nDESCRIPTION: This snippet shows how to use the transformRequestFunction to have fine-grained control over which URLs are enqueued and how they are modified.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating RequestQueueV2 with CheerioCrawler\nDESCRIPTION: Example demonstrating how to use a custom RequestQueueV2 instance with a crawler. When using a locking-enabled queue, the requestLocking experiment must also be enabled in the crawler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Path\nDESCRIPTION: Shows the default storage path where Crawlee saves dataset files in the project directory\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Processing\nDESCRIPTION: Modified route handler that enqueues product URLs to a shared request queue instead of processing them directly. Uses request locking for parallel processing coordination.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getOrInitQueue } from './requestQueue.mjs';\n\n// Handler code implementation for CATEGORY label\n```\n\n----------------------------------------\n\nTITLE: Extracting API Authentication Tokens using JSDOM\nDESCRIPTION: Function that uses JSDOM to load the TikTok page and intercept the API verification headers. It overrides XMLHttpRequest methods to capture the required authentication tokens needed for API calls.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-30-jsdom-based-scraping/index.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst getApiUrlWithVerificationToken = async (body, url) => {\n    log.info(`Getting API session`);\n    const virtualConsole = new VirtualConsole();\n    const { window } = new JSDOM(body, {\n        url,\n        contentType: 'text/html',\n        runScripts: 'dangerously',\n        resources: 'usable' || new CustomResourceLoader(),\n        // ^ 'usable' faster than custom and works without canvas\n        pretendToBeVisual: false,\n        virtualConsole,\n    });\n    virtualConsole.on('error', () => {\n        // ignore errors cause by fake XMLHttpRequest\n    });\n\n    const apiHeaderKeys = ['anonymous-user-id', 'timestamp', 'user-sign'];\n    const apiValues = {};\n    let retries = 10;\n    // api calls made outside of fetch, hack below is to get URL without actual call\n    window.XMLHttpRequest.prototype.setRequestHeader = (name, value) => {\n        if (apiHeaderKeys.includes(name)) {\n            apiValues[name] = value;\n        }\n        if (Object.values(apiValues).length === apiHeaderKeys.length) {\n            retries = 0;\n        }\n    };\n    window.XMLHttpRequest.prototype.open = (method, urlToOpen) => {\n        if (\n            ['static', 'scontent'].find((x) =>\n                urlToOpen.startsWith(`https://${x}`),\n            )\n        )\n        log.debug('urlToOpen', urlToOpen);\n    };\n    do {\n        await sleep(4000);\n        retries--;\n    } while (retries > 0);\n\n    await window.close();\n    return apiValues;\n};\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, captures a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launchPuppeteer } from 'crawlee';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\n\nawait page.goto('https://crawlee.dev');\n\nconst screenshotBuffer = await page.screenshot();\nawait KeyValueStore.setValue('my-screenshot', screenshotBuffer, { contentType: 'image/png' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Updated Path to AutoscaledPool in SDK v1\nDESCRIPTION: Example showing the change in how to access the autoscaledPool in SDK v1, which has been moved under the crawler property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in SDK v1\nDESCRIPTION: Example demonstrating the new Crawling Context in SDK v1, which provides a single shared object across different handler functions, making it easier to track values between handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Failed Cheerio Crawler Output\nDESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating why it fails with JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Session Management with CheerioCrawler and ProxyConfiguration\nDESCRIPTION: Shows how to use SessionPool with CheerioCrawler and ProxyConfiguration for improved session management. The session ID is used to consistently assign the same proxy to the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // By default, the crawler will use the proxy configuration\n    // and select a proxy based on session ID\n    async requestHandler({ request, $, session }) {\n        // Process data\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the URL string. It includes a note about using request.url versus request.loadedUrl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with CheerioCrawler in Crawlee\nDESCRIPTION: This example demonstrates the configuration and usage of SessionPool with CheerioCrawler in Crawlee. It includes proxy setup and session pool management options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ $, request, session }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Global Configuration Class\nDESCRIPTION: Example of using the Configuration class to programmatically configure Crawlee. This code accesses the global configuration singleton and sets persistStateIntervalMillis to 10000 milliseconds. The crawler will use these settings without explicit configuration passing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Pre and Post Navigation Hooks in JavaScript\nDESCRIPTION: Demonstrates the transition from using a custom gotoFunction to using preNavigationHooks and postNavigationHooks in Apify's PuppeteerCrawler. This change simplifies navigation customization and improves code readability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Session Management with JSDOMCrawler and Proxies\nDESCRIPTION: Demonstrates session management with JSDOMCrawler to maintain persistent proxy-session pairs, helping to emulate real user browsing patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Let's create our crawler\nconst crawler = new JSDOMCrawler({\n    // useSessionPool enables automatic session management\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ window, document, session, request }) {\n        // each request is automatically being assigned a random session\n        // and the proxy attached in proxyConfiguration is selected based\n        // on the session id\n        console.log(`Processing: ${request.url}`);\n        console.log(`Using session: ${session.id}`);\n        const title = document.querySelector('title')?.textContent;\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Building Basic CheerioCrawler with RequestQueue\nDESCRIPTION: Shows how to create a CheerioCrawler with a RequestQueue, including a request handler that extracts page titles using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Setting enqueueLinks Strategy to 'All' in TypeScript\nDESCRIPTION: Demonstrates how to configure enqueueLinks to follow all links, regardless of their domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests to Crawler\nDESCRIPTION: Shows how to use the new addRequests method to efficiently add a large number of requests to a crawler, with the option to wait for all requests to be added.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple implementation of a CheerioCrawler that visits a single URL, extracts the page title, and logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler Implementation with crawlee.json\nDESCRIPTION: Example showing how to use a CheerioCrawler with configuration loaded from crawlee.json. Demonstrates request handling and sleep functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Method with Callback Function in Crawlee\nDESCRIPTION: Shows the alternative approach using Actor.main() which wraps the initialization and exit logic around a callback function, providing equivalent functionality to the explicit approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Puppeteer URL Crawler Implementation\nDESCRIPTION: Web scraping implementation using Puppeteer Crawler for headless browser automation. Requires apify/actor-node-puppeteer-chrome image for execution on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';\n\n// Example data structure\ninterface Article {\n    url: string;\n    title: string;\n    description: string;\n}\n\n// Add URLs to the list here\nconst START_URLS = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction',\n];\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler\nDESCRIPTION: Example demonstrating how to crawl URLs from the same hostname using the 'SameHostname' strategy. This will match relative URLs and URLs pointing to the same hostname but not subdomains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add links from the same hostname to the queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Demonstrates a more concise way to create a crawler using the crawler.run() method with direct URL input, eliminating the need for explicit RequestQueue initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Main Execution Functions for Bluesky API Crawler\nDESCRIPTION: These functions orchestrate the complete crawling process. The 'run' function initializes the scraper, manages the session, and handles the crawling lifecycle. The 'main' function serves as the entry point for the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nasync def run() -> None:\n    \"\"\"Main execution function that orchestrates the crawling process.\n\n    Creates a scraper instance, manages the session, and handles the complete\n    crawling lifecycle including proper cleanup on completion or error.\n    \"\"\"\n    scraper = BlueskyApiScraper()\n    scraper.create_session()\n    try:\n        await scraper.init_crawler()\n        await scraper.crawl(['python', 'apify', 'crawlee'])\n        await scraper.save_data()\n    except Exception:\n        traceback.print_exc()\n    finally:\n        scraper.delete_session()\n\n\ndef main() -> None:\n    \"\"\"Entry point for the crawler application.\"\"\"\n    asyncio.run(run())\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Shows how to clean up default storage directories using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production environment execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-Rendered Content with PuppeteerCrawler\nDESCRIPTION: This snippet demonstrates how to use PuppeteerCrawler to scrape JavaScript-rendered content from Apify Store. Unlike Playwright, Puppeteer requires explicit waiting for elements to appear.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        await page.waitForSelector('.ActorStoreItem');\n\n        // Extract text content of the first actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Session Management with PlaywrightCrawler and ProxyConfiguration\nDESCRIPTION: Shows how to use SessionPool with PlaywrightCrawler and ProxyConfiguration for improved session management. The session ID is used to consistently assign the same proxy to the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n    },\n    // By default, the crawler will use the proxy configuration\n    // and select a proxy based on session ID\n    async requestHandler({ request, page, session }) {\n        // Process page\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests Before Enqueuing\nDESCRIPTION: Shows how to use the transformRequestFunction to modify requests before they're added to the queue, allowing dynamic updates to request properties or filtering based on complex conditions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components via the Crawler Property\nDESCRIPTION: Example showing how to access key components like requestQueue and autoscaledPool via the new crawler property in the crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Options in Crawler\nDESCRIPTION: Shows how to configure BrowserPool options, including lifecycle hooks, in a PuppeteerCrawler instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs in a Playwright crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Starting Crawlee Project\nDESCRIPTION: Commands to navigate to project directory and start the crawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with PlaywrightCrawler\nDESCRIPTION: Instructions for setting up SessionPool with PlaywrightCrawler for browser automation with session handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Groups and Country Selection\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection. This example sets up a proxy configuration that uses only Residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Modifying Crawlee Detail Route for Parallel Processing in JavaScript\nDESCRIPTION: This code adapts the DETAIL route handler to send scraped data back to the parent process instead of pushing it directly. This modification is necessary for parallel processing with child processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler('DETAIL', async ({ request, log, $ }) => {\n    log.info(`Fetching details for ${request.url}`);\n\n    const title = $('h1.pdp-title').text().trim();\n    const price = $('div.pdp-price').text().trim();\n    const description = $('#tab-description').text().trim();\n\n    const data = {\n        title,\n        price,\n        description,\n        url: request.url,\n    };\n\n    // Instead of context.pushData, we use process.send\n    process.send(data);\n});\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Pre and Post Navigation Hooks in Apify SDK (JavaScript)\nDESCRIPTION: This snippet demonstrates the transition from using a custom 'gotoFunction' to utilizing 'preNavigationHooks' and 'postNavigationHooks' in Apify's PuppeteerCrawler. It simplifies navigation customization and improves code readability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction using Playwright in JavaScript\nDESCRIPTION: This code block combines all the previous snippets to extract all required product information from a page using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This snippet shows how to customize browser fingerprints in PuppeteerCrawler by defining specific browser parameters such as version, operating system, and locale. The example configures Chrome on Windows with an English US locale.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Set the fingerprint generation strategy\n        fingerprintOptions: {\n            // Use desktop fingerprints\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'chrome', minVersion: 88 },\n                    // { name: 'firefox', minVersion: 69 } // you can add more browsers\n                ],\n                devices: [\n                    'desktop',\n                ],\n                operatingSystems: [\n                    'windows',\n                ],\n            },\n            // Use the same fingerprint for the same session\n            fingerprintCacheSize: 1,\n        },\n    },\n    // Function to process the crawled data\n    async requestHandler({ page, parseWithCheerio, request }) {\n        const $ = await parseWithCheerio();\n        // ... processing logic\n    },\n});\n\n// Run the crawler\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Inspection in PlaywrightCrawler\nDESCRIPTION: Illustrates accessing proxy information in PlaywrightCrawler's requestHandler using proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nInspectionPlaywrightSource\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with Lifecycle Hooks in JavaScript\nDESCRIPTION: Illustrates the shift from using a custom launchPuppeteerFunction to using browser-pool's lifecycle hooks in Apify SDK. This change provides more flexibility and consistency across Puppeteer and Playwright implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler with CheerioCrawler\nDESCRIPTION: A simple crawler that downloads the HTML of a single page, extracts its title, and prints it to the console. This serves as the basis for the improvements in this lesson.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File using Crawlee in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the 'exportToValue' function from Crawlee to export the entire default dataset to a single CSV file. The file is then stored in the default key-value store with the key 'OUTPUT'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\nawait Actor.init();\n\n// Export the entire default dataset to a single CSV file\nconst csvData = await Dataset.exportToValue('OUTPUT', {\n    contentType: 'text/csv',\n    csvFullObject: true,\n});\n\n// Save the CSV file to the default key-value store\nawait Actor.setValue('OUTPUT', csvData, { contentType: 'text/csv' });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV in Crawlee\nDESCRIPTION: Demonstrates how to export a complete dataset to a single CSV file stored in a key-value store. The example shows using Dataset.exportToValue() to export the default dataset with CSV format specification.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from '@crawlee/core';\n\nawait Dataset.exportToValue('my-data:data.csv', {\n    format: 'csv',\n    data: await Dataset.getData(),\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Chromium for AWS Lambda\nDESCRIPTION: JavaScript code demonstrating how to supply Crawlee with the Chromium path from the @sparticuz/chromium package. It includes passing the necessary browser arguments to handle AWS Lambda's hardware limitations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests to Crawler (TypeScript)\nDESCRIPTION: Shows how to add multiple requests to a crawler using the new addRequests method, which handles batching automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Crawling with PuppeteerCrawler\nDESCRIPTION: Demonstrates how to use PuppeteerCrawler for recursive website crawling. The code imports necessary dependencies and configures crawler settings. This example shows how to set up and run a crawler that can navigate through multiple pages recursively.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Define crawler configuration\n});\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Wrapper in Crawlee\nDESCRIPTION: Shows how to use the Actor.main() method which is a convenience wrapper that handles initialization and cleanup automatically. It wraps user code in a try/catch block and accepts an optional status message.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Using Navigation Hooks in PuppeteerCrawler (New Pattern)\nDESCRIPTION: Implementation of navigation hooks that replace gotoFunction with more intuitive preNavigationHooks and postNavigationHooks arrays, simplifying pre and post-processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Context IDs\nDESCRIPTION: Example of using the new crawling context ID system to maintain references between different page contexts, allowing for manipulation of master data from other handler functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It configures logging and starts the crawler with a specific URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Proxy Session Management with PuppeteerCrawler\nDESCRIPTION: Demonstrates proxy session management implementation in PuppeteerCrawler. Uses SessionPool to maintain consistent identities and avoid blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    sessionPoolOptions: {\n        persistStateKeyValueStoreId: 'my-session-pool',\n        persistStateKey: 'puppeteer-crawler-sessions',\n    },\n    async requestHandler({ request, page, session, log }) {\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Cheerio Crawler with In-Memory Storage\nDESCRIPTION: Initial setup of CheerioCrawler with a unique Configuration instance using in-memory storage to ensure Lambda statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in Playwright Crawler (TypeScript)\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs in a Playwright crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 with CheerioCrawler\nDESCRIPTION: This example demonstrates how to use a custom RequestQueueV2 instance with a CheerioCrawler. It shows the required configuration to enable the request locking experiment and pass the queue to the crawler instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using BasicCrawler with Proxy in TypeScript\nDESCRIPTION: This example demonstrates how to use BasicCrawler with a proxy server. It shows how to pass the proxyUrl option to the sendRequest function when making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting responseType to JSON in sendRequest\nDESCRIPTION: Example showing how to configure the responseType option to parse JSON responses automatically. By default, the response type is 'text' for HTML websites.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This snippet illustrates how to set up and use SessionPool with JSDOMCrawler in Crawlee. It includes configuration for proxy usage and session pool management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    // The `useSessionPool` option will enable the `SessionPool` usage.\n    // by default the pool is created with the default configuration\n    useSessionPool: true,\n    // Alternatively, you can provide a custom configuration\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        sessionOptions: {\n            maxUsageCount: 5,\n        },\n    },\n    async requestHandler({ window, document, request, session }) {\n        // `session.userData` can be used to store custom data\n        session.userData.example = 123;\n\n        // process the result\n    },\n    // This function is called when the session is marked as blocked\n    failedRequestHandler({ request, session }) {\n        console.log(`Request ${request.url} failed`)\n        session.retire()\n    },\n    proxyConfiguration,\n});\n\nawait crawler.run(['https://example.com/1', 'https://example.com/2', 'https://example.com/3']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Function for Crawlee Crawler in Python\nDESCRIPTION: Defines the main function that initializes and runs the CustomCrawler with a specified request handler and maximum number of requests. It also exports the crawled data to a JSON file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# main.py\n\nfrom .custom_crawler import CustomCrawler\nfrom .router import router\n\n\nasync def main() -> None:\n    \"\"\"The main function that starts crawling.\"\"\"\n    crawler = CustomCrawler(max_requests_per_crawl=50, request_handler=router)\n\n    # Run the crawler with the initial list of URLs.\n    await crawler.run(['https://www.accommodationforstudents.com/'])\n\n    await crawler.export_data('results.json')\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Operations in Crawlee\nDESCRIPTION: This snippet illustrates how to perform basic operations with datasets in Crawlee, including writing single and multiple rows to the default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Scraping Hacker News with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use PlaywrightCrawler to scrape Hacker News. It starts with the main page, extracts article details, and follows pagination links. The crawler uses a RequestQueue to manage URLs and stores results in the default dataset. It includes error handling and custom navigation timeouts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestQueue to automatically manage the crawler's requests\n    requestQueue: new RequestQueue(),\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Playwright code to extract data from the page.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Playwright within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n\n            // We're getting the title, rank and link from each post on Hacker News.\n            $posts.forEach(($post) => {\n                scrapedData.push({\n                    title: $post.querySelector('.title a').innerText,\n                    rank: $post.querySelector('.rank').innerText,\n                    href: $post.querySelector('.title a').href,\n                });\n            });\n\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset.\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        const infos = await enqueueLinks({\n            selector: '.morelink',\n        });\n\n        if (infos.processedRequests.length === 0) {\n            log.info(`${request.url} is the last page!`);\n        }\n    },\n\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times.`);\n    },\n\n    // Let's limit our crawler to only 50 requests\n    maxRequestsPerCrawl: 50,\n\n    // Increase the timeout for each navigation.\n    navigationTimeoutSecs: 120,\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(['https://news.ycombinator.com/']);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Sample Crawler Log Output\nDESCRIPTION: Example log output from a running Crawlee crawler, showing the titles of pages being crawled on the Crawlee website.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country in Crawlee\nDESCRIPTION: This snippet shows how to configure Apify Proxy with specific proxy groups and country selection. It creates a proxy configuration that uses only residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Failed Attempt to Scrape with PlaywrightCrawler Without Waiting\nDESCRIPTION: This snippet demonstrates a failed attempt to scrape content using PlaywrightCrawler without waiting for elements to render, resulting in an error.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Try to extract text content of an actor card without waiting\n        const actorText = await page.textContent('.ActorStoreItem', { timeout: 1 });\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development runtime using ts-node\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Router and Handlers for Crawlee in Python\nDESCRIPTION: Defines a Router object with handlers for default, search, and listing URLs. Each handler processes specific types of pages and enqueues new requests or extracts data as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# router.py\n\nfrom crawlee.router import Router\n\nfrom .constants import LISTING_PATH, SEARCH_PATH, TARGET_LOCATIONS\nfrom .custom_crawler import CustomContext\n\nrouter = Router[CustomContext]()\n\n\n@router.default_handler\nasync def default_handler(context: CustomContext) -> None:\n    \"\"\"Handle the start URL to get the Build ID and create search links.\"\"\"\n    context.log.info(f'default_handler is processing {context.request.url}')\n\n    await context.enqueue_links(\n        path_template=SEARCH_PATH, items=TARGET_LOCATIONS, label='SEARCH', user_data={'page': 1}\n    )\n\n\n@router.handler('SEARCH')\nasync def search_handler(context: CustomContext) -> None:\n    \"\"\"Handle the SEARCH URL generates links to listings and to the next search page.\"\"\"\n    context.log.info(f'search_handler is processing {context.request.url}')\n\n    max_pages = context.page_data['pageProps']['initialPageCount']\n    current_page = context.request.user_data['page']\n    if current_page < max_pages:\n\n        await context.enqueue_links(\n            path_template=SEARCH_PATH,\n            items=[context.request.user_data['location']],\n            label='SEARCH',\n            user_data={'page': current_page + 1},\n        )\n    else:\n        context.log.info(f'Last page for {context.request.user_data[\"location\"]} location')\n\n    listing_ids = [\n        listing['property']['id']\n        for group in context.page_data['pageProps']['initialListings']['groups']\n        for listing in group['results']\n        if listing.get('property')\n    ]\n\n    await context.enqueue_links(path_template=LISTING_PATH, items=listing_ids, label='LISTING')\n\n\n@router.handler('LISTING')\nasync def listing_handler(context: CustomContext) -> None:\n    \"\"\"Handle the LISTING URL extracts data from the listings and saving it to a dataset.\"\"\"\n    context.log.info(f'listing_handler is processing {context.request.url}')\n\n    listing_data = context.page_data['pageProps']['viewModel']['propertyDetails']\n    if not listing_data['exists']:\n        context.log.info(f'listing_handler, data is not available for url {context.request.url}')\n        return\n    property_data = {\n        'property_id': listing_data['id'],\n        'property_type': listing_data['propertyType'],\n        'location_latitude': listing_data['coordinates']['lat'],\n        'location_longitude': listing_data['coordinates']['lng'],\n        'address1': listing_data['address']['address1'],\n        'address2': listing_data['address']['address2'],\n        'city': listing_data['address']['city'],\n        'postcode': listing_data['address']['postcode'],\n        'bills_included': listing_data.get('terms', {}).get('billsIncluded'),\n        'description': listing_data.get('description'),\n        'bathrooms': listing_data.get('numberOfBathrooms'),\n        'number_rooms': len(listing_data['rooms']) if listing_data.get('rooms') else None,\n        'rent_ppw': listing_data.get('terms', {}).get('rentPpw', {}).get('value', None),\n    }\n\n    await context.push_data(property_data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Router for Google Search Crawling with Crawlee and BeautifulSoup in Python\nDESCRIPTION: This snippet defines a router with a default handler for processing Google Search results pages. The handler extracts search result data (title, URL, text) from each result item, tracks the order of results across pagination, and enqueues links to the next results pages. It includes error handling for both attribute errors and unexpected exceptions during extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext\nfrom crawlee.router import Router\n\nrouter = Router[BeautifulSoupCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n    context.log.info(f'Processing {context.request.url} ...')\n\n    order = context.request.user_data.get(\"last_order\", 1)\n    query = context.request.user_data.get(\"query\")\n    for item in context.soup.select(\"div#search div#rso div[data-hveid][lang]\"):\n        try:\n            data = {\n                \"query\": query,\n                \"order_no\": order,\n                'title': item.select_one(\"h3\").get_text(),\n                \"url\": item.select_one(\"a\").get(\"href\"),\n                \"text_widget\": item.select_one(\"div[style*='line']\").get_text(),\n            }\n            await context.push_data(data)\n            order += 1\n        except AttributeError as e:\n            context.log.warning(f'Attribute error for query \"{query}\": {str(e)}')\n        except Exception as e:\n            context.log.error(f'Unexpected error for query \"{query}\": {str(e)}')\n\n    await context.enqueue_links(selector=\"div[role='navigation'] td[role='heading']:last-of-type > a\",\n                                user_data={\"last_order\": order, \"query\": query})\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Crawlee Package\nDESCRIPTION: Command to install only the Cheerio package from Crawlee for lightweight HTML parsing needs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State in CheerioCrawler\nDESCRIPTION: Demonstrates the use of the new useState() method to maintain and automatically save crawler state.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Country and Group Specific Apify Proxy in Crawlee\nDESCRIPTION: Demonstrates how to create an Apify Proxy configuration with specific proxy groups and country selection. This example shows how to limit proxies to residential IPs from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Checking NPM Version\nDESCRIPTION: Command to verify that NPM package manager is installed, which is required for Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with JSDOMCrawler\nDESCRIPTION: Guide for implementing SessionPool with JSDOMCrawler for DOM manipulation with session management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{JSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Browser, launchPuppeteer } from 'crawlee';\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\n\nconst url = 'https://crawlee.dev';\nawait page.goto(url);\n\nconst screenshot = await page.screenshot();\n\nconst key = new URL(url).hostname;\nawait Actor.setValue(key, screenshot, { contentType: 'image/png' });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Session Management with CheerioCrawler and Proxies\nDESCRIPTION: Shows how to combine session management with CheerioCrawler and proxies to improve scraping efficiency and avoid blocks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    useSessionPool: true, // This is the default\n    persistCookiesPerSession: true, // This is the default\n    proxyConfiguration,\n    async requestHandler({ request, $, session }) {\n        // Process the response\n        // ...\n\n        // You can mark session as bad when it gets blocked\n        if (someCondition) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Fetching HTML from a Single URL using got-scraping in Crawlee\nDESCRIPTION: This snippet demonstrates how to use the got-scraping package to fetch HTML content from a single URL. It makes a GET request to example.com and logs the response HTML to the console. The code is designed to be used in a Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\n/**\n * This example uses the got-scraping package to grab the HTML of a web page.\n */\nconst response = await gotScraping({\n    url: 'https://example.com',\n    // Other options can be passed here\n});\n\nconsole.log(response.body);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example of disabling browser fingerprints in Playwright crawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest Helper with BasicCrawler in TypeScript\nDESCRIPTION: Shows how to use the context.sendRequest() helper with BasicCrawler to process requests using got-scraping. This helper allows easy handling of the context-bound Request object with customizable options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs with JSDOMCrawler to Extract Page Titles and H1 Tags\nDESCRIPTION: This example shows how to use JSDOMCrawler to crawl URLs from an external file. It loads each page using HTTP requests, parses the HTML with jsdom, and extracts the page title and all h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readFile } from 'node:fs/promises';\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the JSDOMCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the\n// JSDOM library (https://www.npmjs.com/package/jsdom).\nconst crawler = new JSDOMCrawler({\n    // Let's limit paralelism to 1 request at a time, to not overload the system.\n    maxRequestsPerMinute: 60,\n\n    // This function will be called for each URL to crawl.\n    // The \"window\" parameter is Cheerio's representation of the web page.\n    async requestHandler({ request, window, log }) {\n        const { document } = window;\n\n        log.info(`Processing ${request.url}...`);\n\n        // Extract data from the page using DOM manipulation.\n        const title = document.querySelector('title')?.textContent;\n        const h1s = Array.from(document.querySelectorAll('h1'))\n            .map((node) => node.textContent);\n\n        // Store the results to the default dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1s,\n        });\n    },\n});\n\n// Read the list of URLs from a text file\n// where each line contains a URL\nconst contents = await readFile('./urls.txt', { encoding: 'utf8' });\nconst urls = contents.trim().split('\\n');\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(urls);\n\nlog.info('Crawler finished.');\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support\nDESCRIPTION: Shows how to create a request queue that supports locking by using the RequestQueueV2 class instead of the standard RequestQueue class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Shows how to completely disable browser fingerprint generation in PlaywrightCrawler by setting useFingerprints to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nnew PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json\nDESCRIPTION: Example of a crawlee.json file that sets persistStateIntervalMillis to 10000 and logLevel to DEBUG. This file should be placed in the root of your project for Crawlee to use these options as global configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Domain Strategy Configuration\nDESCRIPTION: Configuration example for including subdomains in crawling using enqueueLinks strategy option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Shows how to disable browser fingerprint generation in PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing ProxyInfo in CheerioCrawler\nDESCRIPTION: Example showing how to access proxy information in CheerioCrawler's requestHandler using the proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionCheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Adding All Shoes Links to Request Queue\nDESCRIPTION: Code to find all 'All shoes' links on the page and add them to the request queue with the 'listing' label.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    shoe_listing_links = (\n        await context.page.get_by_test_id('link').filter(has_text='All shoes').all()\n    )\n    await context.add_requests(\n        [\n            Request.from_url(url, label='listing')\n            for link in shoe_listing_links\n            if (url := await link.get_attribute('href'))\n        ]\n    )\n\n@router.handler('listing')\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Handler for shoe listings.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using RequestList with PuppeteerCrawler in Crawlee\nDESCRIPTION: Shows how to initialize and use a RequestList with a PuppeteerCrawler to process a predefined list of URLs in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Company Data from Crunchbase API with Crawlee for Python\nDESCRIPTION: This snippet defines the company_handler function to extract detailed information about each company from the Crunchbase API response. It processes the JSON data and pushes the extracted information to the crawler's data storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('company')\nasync def company_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Company request handler.\"\"\"\n    context.log.info(f'company_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    await context.push_data(\n        {\n            'Company Name': data['properties']['identifier']['value'],\n            'Short Description': data['properties']['short_description'],\n            'Website': data['properties'].get('website_url'),\n            'Location': '; '.join([item['value'] for item in data['properties'].get('location_identifiers', [])]),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File - Crawlee Dataset API\nDESCRIPTION: Demonstrates using the exportToValue function to export the default dataset to a single CSV file in the key-value store. The example shows how to access and export dataset contents programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.exportToValue('OUTPUT.csv', {\n    // The items will be exported using the CSV format\n    format: 'csv',\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a Request Queue in Crawlee, including opening a queue, adding requests, and processing them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst queue = await RequestQueue.open('my-queue');\n\n// Add requests to the queue\nawait queue.addRequest({ url: 'https://example.com/1' });\nawait queue.addRequest({ url: 'https://example.com/2' });\nawait queue.addRequest({ url: 'https://example.com/3' });\n\n// Get request from queue and process it\nwhile (requests = await queue.fetchNextRequest()) {\n    // Process the request...\n    console.log(request.url);\n\n    // Mark the request as handled so it's not processed again\n    await queue.markRequestHandled(request);\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with HttpCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with HttpCrawler to handle requests through proxies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, response, proxyInfo }) {\n        console.log(`Using proxy: ${proxyInfo.url}`);\n    },\n});\n\nconst proxyInfo = await proxyConfiguration.newProxyInfo();\nconsole.log(`Got proxy URL: ${proxyInfo.url}`);\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Basic Proxy Configuration in Crawlee\nDESCRIPTION: Creates a proxy configuration with a list of custom proxy URLs and retrieves a new proxy URL for use in requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Using Apify Platform Storage in Local Development\nDESCRIPTION: JavaScript code demonstrating how to use Apify platform storage features in a local environment, including getting public URLs for stored items.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Cheerio\nDESCRIPTION: Demonstrates how to download and process URLs from a sitemap using Cheerio Crawler. Uses downloadListOfUrls utility from @crawlee/utils to fetch sitemap URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioSitemapCrawler } from '@crawlee/cheerio';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new CheerioSitemapCrawler({\n    // ...\n});\n\nconst sitemapUrls = await downloadListOfUrls({ url: 'https://example.com/sitemap.xml' });\n\nawait crawler.run(sitemapUrls);\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple Pre-Launch Hooks\nDESCRIPTION: Example showing how to combine multiple pre-launch hooks into an array for more modular browser configuration management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments Before SDK v1\nDESCRIPTION: Example showing how handler arguments were implemented before SDK v1, where separate argument objects were created for different handlers, making it difficult to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Updated Access Path for AutoscaledPool\nDESCRIPTION: Example showing how the path to access the autoscaledPool has changed in SDK v1, moving from direct access on the context to access through the crawler property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows how to initialize and create a basic proxy configuration using Apify Proxy service. This requires being subscribed to Apify Proxy and being logged into the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Sessions with PuppeteerCrawler in TypeScript\nDESCRIPTION: This snippet shows how to implement proxy sessions with PuppeteerCrawler to maintain consistent IP addresses for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ session }) {\n        const proxyUrl = await proxyConfiguration.newUrl(session.id);\n        console.log(proxyUrl);\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with RequestQueueV2\nDESCRIPTION: Demonstrates full configuration of a CheerioCrawler using a custom RequestQueueV2 instance with request locking enabled. Shows integration between the queue and crawler components.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Crawlee in TypeScript\nDESCRIPTION: Basic example of using enqueueLinks() function in Crawlee to add links to the crawler's queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: This code demonstrates how to enable the request locking experiment option in a CheerioCrawler instance. The experiment is enabled by setting the requestLocking flag to true in the crawler's configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in JSDOMCrawler\nDESCRIPTION: Shows how to access and inspect information about the currently used proxy in JSDOMCrawler's requestHandler function. This is useful for debugging or logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, document, proxyInfo }) {\n        console.log(proxyInfo.url); // http://proxy-1.com\n        console.log(proxyInfo.hostname); // proxy-1.com\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet shows how to find all anchor elements with href attributes on a page and extract those URLs into an array using Cheerio's selector syntax and map/get methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with BasicCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for the session pool and shows how to use sessions in the requestHandler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { BasicCrawler, createSessionPool } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ session, $, request }) {\n        // Use session\n        const userAgent = session.userData.userAgent;\n        // ...\n    },\n    sessionPool: await createSessionPool({\n        // defined max count of sessions\n        maxPoolSize: 50,\n        sessionOptions: {\n            // name of cookie\n            name: 'session_id',\n            // max usage count of session\n            maxUsageCount: 5,\n        },\n    }),\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Apify SDK with Crawlee v3\nDESCRIPTION: Shows two equivalent ways to initialize and exit an Apify actor using the new Apify SDK integration in Crawlee v3.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content from an Element with Cheerio\nDESCRIPTION: A simple code snippet demonstrating how to find the first h2 element on a page and extract its text content using Cheerio's jQuery-like selector syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route Handler for Worker Process Communication\nDESCRIPTION: Updates the detail route handler to send scraped data back to the parent process instead of storing it directly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nDETAIL: async ({ $, crawler }) => {\n    const data = {\n        title: $('h1').text(),\n        price: $('span.price').text(),\n    };\n    \n    if (process.send) {\n        process.send(data);\n    } else {\n        await crawler.pushData(data);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AdaptivePlaywrightCrawler in Python\nDESCRIPTION: Example of using the new AdaptivePlaywrightCrawler to handle both static and dynamic content. It demonstrates request handling, element selection, and data pushing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-06/index.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom datetime import timedelta\n\nfrom crawlee.crawlers import AdaptivePlaywrightCrawler, AdaptivePlaywrightCrawlingContext\n\n\nasync def main() -> None:\n    crawler = AdaptivePlaywrightCrawler.with_beautifulsoup_static_parser(\n        max_requests_per_crawl=5,\n        playwright_crawler_specific_kwargs={'browser_type': 'chromium'},\n    )\n\n    @crawler.router.default_handler\n    async def request_handler(context: AdaptivePlaywrightCrawlingContext) -> None:\n        # Do some processing using `parsed_content`\n        context.log.info(context.parsed_content.title)\n\n        # Locate element h2 within 5 seconds\n        h2 = await context.query_selector_one('h2', timedelta(seconds=5))\n        # Do stuff with element found by the selector\n        context.log.info(h2)\n\n        # Find more links and enqueue them.\n        await context.enqueue_links()\n        # Save some data.\n        await context.push_data({'Visited url': context.request.url})\n\n    await crawler.run(['https://www.crawlee.dev/'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-stage Docker Build for Crawlee Actor\nDESCRIPTION: Dockerfile that sets up a production-ready Crawlee actor environment using Node.js 20. It implements a multi-stage build process to optimize the final image size and includes only necessary production dependencies. The build process includes dependency installation, source code compilation, and production environment setup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:20\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using Apify Platform Storage in Local Development\nDESCRIPTION: JavaScript code demonstrating how to use Apify platform storage (Key-Value Store) in a local environment and get a public URL for a stored item.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with PuppeteerCrawler\nDESCRIPTION: Shows how to integrate ProxyConfiguration with PuppeteerCrawler to enable proxy usage for browser-based scraping with Puppeteer. Includes setup and basic request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, log }) {\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Converting Between Tough Cookies and Browser Pool Cookies in JavaScript\nDESCRIPTION: Fix for conversion between tough cookies and browser pool cookies in the core module.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Conversion between tough cookies and browser pool cookies\n```\n\n----------------------------------------\n\nTITLE: Using launchContext instead of launchPuppeteerOptions in Apify SDK (JavaScript)\nDESCRIPTION: This code snippet shows how to use the new 'launchContext' object instead of 'launchPuppeteerOptions' in Apify's PuppeteerCrawler. It provides a clearer separation between Apify-specific options and Puppeteer launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a new Crawlee project with CLI extras using pipx\nDESCRIPTION: Command to create a new Crawlee project from a template using pipx with CLI extras installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-06/index.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npipx run 'crawlee[cli]' create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Building Apify Crawlee Project Docker Image\nDESCRIPTION: This Dockerfile creates a multi-stage build for an Apify Crawlee project. It first builds the project in a builder stage, then creates a lightweight production image with only the necessary files and dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:16\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of sendRequest with BasicCrawler\nDESCRIPTION: Demonstrates the basic usage of sendRequest function within BasicCrawler's request handler to make HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Running PuppeteerCrawler in Headful Mode\nDESCRIPTION: Example of running PuppeteerCrawler in headful mode (with visible browser window) for development and debugging purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    headless: false,\n    requestHandlerTimeoutSecs: 30,\n    async requestHandler({ page, request, enqueueLinks }) {\n        await page.waitForSelector('title');\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n        await new Promise((r) => setTimeout(r, 1000));\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in HttpCrawler\nDESCRIPTION: Shows how to access and inspect the current proxy details within the HttpCrawler's request handler to gain insights about the proxy being used.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json, proxyInfo }) {\n        // Print information about the currently used proxy directly to the terminal\n        console.log(proxyInfo);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Recent Version Update Notes\nDESCRIPTION: Recent changelog entries showing version bumps, bug fixes and feature additions for the memory storage module\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/memory-storage/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.5.3](https://github.com/apify/crawlee/compare/v3.5.2...v3.5.3) (2023-08-31)\n\n### Bug Fixes\n\n* pin all internal dependencies ([#2041](https://github.com/apify/crawlee/issues/2041)) ([d6f2b17](https://github.com/apify/crawlee/commit/d6f2b172d4a6776137c7893ca798d5b4a9408e79)), closes [#2040](https://github.com/apify/crawlee/issues/2040)\n```\n\n----------------------------------------\n\nTITLE: Implementing transformRequestFunction with enqueueLinks in TypeScript\nDESCRIPTION: Demonstrates how to use the transformRequestFunction to have fine-grained control over which requests are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Updated Property Access in Crawling Context\nDESCRIPTION: Example demonstrating how property access has changed in SDK v1, with some properties like autoscaledPool moving under the crawler object instead of being directly accessible on the context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Playwright with Incognito Pages\nDESCRIPTION: Example showing how to enable proxy-per-page feature in Playwright crawler by setting useIncognitoPages flag in launchContext.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_26\n\nLANGUAGE: typescript\nCODE:\n```\nnew Apify.Playwright({\n    launchContext: {\n        useIncognitoPages: true,\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler\nDESCRIPTION: Implementation of a crawler that only enqueues links from the same hostname. This is the default strategy that matches relative URLs and URLs pointing to the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // This will enqueue only links from the same hostname\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Playwright Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to crawl a sitemap using Playwright Crawler. It leverages the Sitemap utility to extract URLs from the sitemap, enqueues them for processing, and then uses Playwright to navigate to each page and extract its title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore, PlaywrightCrawler, PlaywrightConfiguration, PlaywrightCrawlingContext, createPlaywrightRouter } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create a router\nconst router = createPlaywrightRouter();\n\n// Add a route for the sitemap page\nrouter.addHandler('sitemap', async ({ enqueueLinks, log, request }) => {\n    const sitemapUrl = request.url;\n\n    log.info(`Processing sitemap at ${sitemapUrl}`);\n\n    // Initialize the Sitemap object with the URL\n    const sitemap = await Sitemap.load({ url: sitemapUrl });\n\n    // Get an array of URLs from the sitemap\n    const urls = sitemap.urls;\n\n    log.info(`Found ${urls.length} URLs in the sitemap`);\n\n    // Add the URLs to the crawler's request queue\n    await enqueueLinks({\n        urls: urls.map((u) => u.url),\n        label: 'detail',\n    });\n});\n\n// Add a route for the detail pages\nrouter.addHandler('detail', async ({ page, log, request }) => {\n    const title = await page.title();\n    log.info(`Processing detail page: ${title}`, { url: request.url });\n\n    // Save the results to the default dataset\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\n// Define crawler configuration\nconst configuration: PlaywrightConfiguration = {\n    // Use the router to handle requests\n    requestHandler: router,\n\n    // Start with the sitemap URL\n    startUrls: [\n        {\n            url: 'https://crawlee.dev/sitemap.xml',\n            userData: {\n                label: 'sitemap',\n            },\n        },\n    ],\n};\n\n// Set up the crawler\nconst crawler = new PlaywrightCrawler(configuration);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability using Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to check if a product is in stock by searching for a specific element on the web page using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: Demonstrates how to enable the experimental request locking feature in a CheerioCrawler instance by setting the requestLocking experiment flag to true.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code as Actor Using Actor.main()\nDESCRIPTION: Example of using Actor.main() to encapsulate a CheerioCrawler implementation for running on the Apify platform. This approach handles initialization and cleanup automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.main(async () => {\n    const crawler = new CheerioCrawler({\n        async requestHandler({ request, $ }) {\n            const title = $('title').text();\n            await Dataset.pushData({\n                url: request.url,\n                title,\n            });\n        },\n    });\n\n    await crawler.run(['https://crawlee.dev']);\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Groups and Country Filtering\nDESCRIPTION: Demonstrates how to create an advanced proxy configuration with specific proxy groups and country code filtering. This example configures the proxy to use only residential IPs from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Managing Standalone Proxy Sessions in TypeScript\nDESCRIPTION: This code demonstrates how to manage proxy sessions independently of a crawler for flexible proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\n// Create a session\nconst sessionId = 'my-session';\n\n// Get a proxy URL for the session\nconst proxyUrl = await proxyConfiguration.newUrl(sessionId);\nconsole.log(proxyUrl);\n\n// Use the same proxy URL again\nconst sameProxyUrl = await proxyConfiguration.newUrl(sessionId);\nconsole.log(sameProxyUrl);\n```\n\n----------------------------------------\n\nTITLE: Browser Fingerprint Configuration\nDESCRIPTION: Example of disabling browser fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Crawlee\nDESCRIPTION: This snippet demonstrates how to save data to the default dataset in Crawlee. It shows how to push a single item and multiple items to a dataset, and notes that custom datasets can be created using Dataset.open().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Dataset is a class that represents a dataset\n// Let's push one item to the default dataset\nawait Dataset.pushData({\n    id: '1',\n    name: 'Cheese',\n    price: 10,\n});\n\n// You can push multiple items\nawait Dataset.pushData([\n    {\n        id: '2',\n        name: 'Ham',\n        price: 10,\n    },\n    {\n        id: '3',\n        name: 'Bread',\n        price: 10,\n    },\n]);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: A code snippet demonstrating how to locate all anchor elements with href attributes on a page and extract them into an array using Cheerio's selector, map, and get methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Initializing ProxyConfiguration with Static Proxy URLs in JavaScript\nDESCRIPTION: Sets up a ProxyConfiguration instance with a static list of proxy URLs that will be rotated when making requests. This is the basic setup for using proxies with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Minute in CheerioCrawler\nDESCRIPTION: This example shows how to limit the number of requests per minute to 60 when configuring a CheerioCrawler. This prevents the crawler from overwhelming the target website with requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // We'll allow only 1 request per second (60 per minute)\n    maxRequestsPerMinute: 60,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Cheerio Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to crawl a sitemap using Cheerio Crawler. It utilizes the Sitemap utility class to parse the sitemap, enqueues the discovered URLs, and then processes each page to extract its title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioConfiguration, CheerioCrawlingContext, CheerioSource, Dataset, KeyValueStore, createCheerioRouter, CheerioCrawler } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create a router\nconst router = createCheerioRouter();\n\n// Add a route for the sitemap page\nrouter.addHandler('sitemap', async ({ enqueueLinks, log, request }) => {\n    const sitemapUrl = request.url;\n\n    log.info(`Processing sitemap at ${sitemapUrl}`);\n\n    // Initialize the Sitemap object with the URL\n    const sitemap = await Sitemap.load({ url: sitemapUrl });\n\n    // Get an array of URLs from the sitemap\n    const urls = sitemap.urls;\n\n    log.info(`Found ${urls.length} URLs in the sitemap`);\n\n    // Add the URLs to the crawler's request queue\n    await enqueueLinks({\n        urls: urls.map((u) => u.url),\n        label: 'detail',\n    });\n});\n\n// Add a route for the detail pages\nrouter.addHandler('detail', async ({ $, log, request }) => {\n    const title = $('title').text().trim();\n    log.info(`Processing detail page: ${title}`, { url: request.url });\n\n    // Save the results to the default dataset\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\n// Define crawler configuration\nconst configuration: CheerioConfiguration = {\n    // Use the router to handle requests\n    requestHandler: router,\n\n    // Start with the sitemap URL\n    startUrls: [\n        {\n            url: 'https://crawlee.dev/sitemap.xml',\n            userData: {\n                label: 'sitemap',\n            },\n        },\n    ],\n};\n\n// Set up the crawler\nconst crawler = new CheerioCrawler(configuration);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Updated Launch Functions Configuration (JavaScript)\nDESCRIPTION: Demonstrates the updated launch options format and custom module configuration for both Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawling Context in Handler Functions\nDESCRIPTION: Demonstrates how crawling context is now shared between handler functions, allowing for consistent access to properties like proxyInfo across different handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example showing how to properly use BrowserController for browser management tasks like closing browsers and setting cookies, which provides a unified API for both Puppeteer and Playwright browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee as an Actor Using Actor.main()\nDESCRIPTION: Example showing how to run a Cheerio crawler using the Actor.main() method for Apify platform compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.main(async () => {\n    // Initialize the crawler\n    const crawler = new CheerioCrawler({\n        // Function called for each URL\n        async requestHandler({ request, $, enqueueLinks, log }) {\n            const title = $('title').text();\n            log.info(`Title of ${request.url} is '${title}'`);\n\n            // Save results\n            await Dataset.pushData({\n                url: request.url,\n                title,\n            });\n\n            // Add new links from page to request queue\n            await enqueueLinks();\n        },\n    });\n\n    // Add first URL to request queue and start the crawl\n    await crawler.run(['https://crawlee.dev']);\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Sessions with PlaywrightCrawler in TypeScript\nDESCRIPTION: This code illustrates how to set up proxy sessions with PlaywrightCrawler for consistent IP usage in browser-based scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ session }) {\n        const proxyUrl = await proxyConfiguration.newUrl(session.id);\n        console.log(proxyUrl);\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Example demonstrating how to turn off browser fingerprints in PuppeteerCrawler by setting the useFingerprints property to false in the browserPoolOptions configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Disables fingerprints generation\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Element Not Found Error\nDESCRIPTION: Shows the error message when attempting to access elements before they are rendered in the DOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to customize browser fingerprints for PlaywrightCrawler in Crawlee. It sets specific parameters for the fingerprint generation, including browser, operating system, and screen size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [{ name: 'firefox', minVersion: 80 }],\n                operatingSystems: ['windows'],\n                devices: ['desktop'],\n                locales: ['en-US', 'en-GB'],\n                resolution: { width: 1920, height: 1080 },\n            },\n        },\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Session Management with HttpCrawler and Proxies\nDESCRIPTION: Example of using session management with HttpCrawler to maintain consistent proxy usage per session, which helps create realistic user behavior and avoid blocks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Let's create our crawler\nconst crawler = new HttpCrawler({\n    // useSessionPool enables automatic session management\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ json, session, request }) {\n        // each request is automatically being assigned a random session\n        // and the proxy attached in proxyConfiguration is selected based\n        // on the session id\n        console.log(`Processing: ${request.url}`);\n        console.log(`Using session: ${session.id}`);\n        console.log(`Data: ${JSON.stringify(json)}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Handling Infinite Scroll in Listing Handler\nDESCRIPTION: Updated listing handler that uses the accept_cookies context manager and infinite_scroll helper to navigate through the entire product catalog.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('listing')\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Handler for shoe listings.\"\"\"\n\n    async with accept_cookies(context.page):\n        await context.page.wait_for_load_state('networkidle')\n        await context.infinite_scroll()\n        await context.enqueue_links(\n            selector='a.product-card__link-overlay', label='detail'\n        )\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with JSDOMCrawler - JavaScript\nDESCRIPTION: Implementation of SessionPool with JSDOMCrawler for DOM manipulation with session management\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{JSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Installing curl_cffi for TLS Fingerprinting Bypass in Python\nDESCRIPTION: This snippet shows how to install the curl_cffi library using pip. curl_cffi is a wrapper around the C library that patches curl and provides an API similar to requests for bypassing TLS fingerprinting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install curl_cffi\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Disabled Storage Persistence for GCP Cloud Run\nDESCRIPTION: Basic setup for a PlaywrightCrawler instance with disabled storage persistence, which is necessary for serverless environments like GCP Cloud Run where persistent file storage isn't reliable.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Handling Cookie Dialog in Default Router Handler\nDESCRIPTION: Default request handler that clicks the accept cookies button on the Nike website using the test_id attribute.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.router import Router\nfrom crawlee.playwright_crawler import PlaywrightCrawlingContext\n\nrouter = Router[PlaywrightCrawlingContext]()\n\n@router.default_handler\nasync def default_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n\n    # Wait for the popup to be visible to ensure it has loaded on the page.\n    await context.page.get_by_test_id('dialog-accept-button').click()\n```\n\n----------------------------------------\n\nTITLE: Using Platform Storage in Local Actor\nDESCRIPTION: JavaScript code demonstrating how to get a public URL for an item stored in a Key-Value Store on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with HttpCrawler - JavaScript\nDESCRIPTION: Demonstrates SessionPool implementation with HttpCrawler for handling HTTP requests with session management\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{HttpSource}\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Installs ts-node as a development dependency to run TypeScript code directly during development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Using cURL with TLSv1.3 to Bypass Cloudflare Protection\nDESCRIPTION: This improved cURL command uses the --tlsv1.3 flag to specify the TLS version, successfully bypassing Cloudflare's protection where the standard request failed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XGET -H 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\"' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8' -H 'Accept-Language: en-US,en;q=0.5' -H 'Connection: keep-alive' 'https://www.g2.com/' --tlsv1.3 -s -o /dev/null -w \"%{http_code}\\n\"\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawlee Crawler in Express Server for GCP Cloud Run\nDESCRIPTION: This code demonstrates how to wrap a Crawlee crawler in an Express HTTP server handler for compatibility with GCP Cloud Run. It sets up a GET route that runs the crawler and returns the scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport express from 'express';\nconst app = express();\n\nconst startUrls = ['https://crawlee.dev'];\n\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    return res.send(await crawler.getData());\n});\n\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Updating launchPuppeteerOptions to launchContext in Apify PuppeteerCrawler (JavaScript)\nDESCRIPTION: This code snippet shows how to migrate from the old 'launchPuppeteerOptions' to the new 'launchContext' object in Apify's PuppeteerCrawler. The new structure clearly separates Apify-specific options from Puppeteer launch options, reducing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue and Adding Requests in Crawlee\nDESCRIPTION: Demonstrates how to create a RequestQueue instance and add a request to it. This is the basic setup for defining the URLs to be crawled.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Setting up Docker Image for Crawlee Actor with Playwright Chrome\nDESCRIPTION: This Dockerfile creates a containerized environment for running Crawlee web scraping actors. It implements a two-stage build process to optimize the final image size, first building the application and then creating a production image with only the necessary files and dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Storage in Crawlee\nDESCRIPTION: JavaScript implementation for storing scraped data in Crawlee using the built-in dataset functionality. This example shows how to push data from a PlaywrightCrawler to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page }) => {\n\n        const title = await page.title();\n        const price = await page.textContent('.price');\n\n        await crawler.pushData({\n            url: request.url,\n            title,\n            price\n        });\n    }\n})\n\nawait crawler.run(['http://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Pre-release Docker Image Configuration\nDESCRIPTION: Examples of using pre-release beta versions of Docker images, both with and without specific library versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Command to install ts-node as a development dependency, allowing direct execution of TypeScript files during development without pre-compilation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with PuppeteerCrawler\nDESCRIPTION: Demonstrates how to set up PuppeteerCrawler with proxy configuration to use proxies for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    // Remove the launchContext option for automatic browser launch\n    // with provided proxy configuration\n    async requestHandler({ request, page }) {\n        // Extract data using Puppeteer API\n        // ...\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using cookieJar with sendRequest in Crawlee\nDESCRIPTION: Shows how to use a custom cookie jar with sendRequest for managing cookies across requests. This is useful for maintaining session state when navigating through websites.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Migrating from Apify events to Actor events\nDESCRIPTION: Code diff showing how to update event subscription from the old Apify.events system to the new Actor-based event management system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content with Cheerio\nDESCRIPTION: Simple example showing how to extract text content from an h2 element using Cheerio's selector syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in Playwright Crawler (TypeScript)\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs in a Playwright crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to crawl a list of URLs from an external file, load each using HTTP requests, and extract data (page titles and h1 tags) using the jsdom DOM implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, JSDOMCrawler, log } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// Create an instance of the JSDOMCrawler class\nconst crawler = new JSDOMCrawler({\n    // Let's limit our crawls to just a few sites\n    maxRequestsPerCrawl: 20,\n});\n\n// Prepare a list of URLs to crawl\nconst fileContents = await readFile('./urls.txt', 'utf-8');\nconst urls = fileContents.trim().split('\\n');\n\n// Define a route to process the URLs\ncrawler.router.addDefaultHandler(async ({ window, request, enqueueLinks, pushData, log }) => {\n    const { document } = window;\n    const title = document.title;\n    log.info(`Title of ${request.loadedUrl} is: ${title}`);\n\n    // Extract h1 tags from the page\n    const h1Texts = Array.from(document.querySelectorAll('h1'))\n        .map((el) => el.textContent?.trim());\n\n    // Save the results to dataset\n    await pushData({\n        url: request.loadedUrl,\n        title,\n        h1Texts,\n    });\n\n    // Enqueue links from the page for further crawling\n    await enqueueLinks();\n});\n\n// Start the crawler with the URLs\nawait crawler.run(urls);\n\n// Display the results\nconst data = await Dataset.getData();\nlog.info('Results:', data);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This code demonstrates how to completely disable browser fingerprinting in PlaywrightCrawler by setting the useFingerprints option to false. This can be useful when fingerprinting causes issues or when a custom fingerprinting solution is preferred.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Disable the fingerprinting feature\n        useFingerprints: false,\n    },\n    // Function to process the crawled data\n    async requestHandler({ page, parseWithCheerio, request }) {\n        const $ = await parseWithCheerio();\n        // ... processing logic\n    },\n});\n\n// Run the crawler\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File Location for Apify Actor\nDESCRIPTION: Demonstrates the file path structure for providing input to an Apify actor through the INPUT.json file in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler in JavaScript\nDESCRIPTION: This snippet demonstrates how to use JSDOMCrawler to find all <a> elements with href attributes on a page and extract their URLs into an array. It utilizes the querySelectorAll method and Array.from for conversion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection. This example shows setting up residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Proxy Session Management with JSDOMCrawler\nDESCRIPTION: Demonstrates how to implement proxy session management in JSDOMCrawler. Utilizes SessionPool to maintain consistent browser identity across requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    sessionPoolOptions: {\n        persistStateKeyValueStoreId: 'my-session-pool',\n        persistStateKey: 'jsdom-crawler-sessions',\n    },\n    async requestHandler({ request, window, session, log }) {\n        const { document } = window;\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Example of Properly Decompressed Response Output\nDESCRIPTION: This bash snippet shows the properly decompressed output after installing the Brotli library, displaying the beginning of an HTML document.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nb'<!DOCTYPE '\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Request Queue with Locking Support in JavaScript\nDESCRIPTION: This code defines a function to initialize or retrieve a request queue that supports locking. It uses the Crawlee library and ensures the queue is created with the correct name and options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\nlet requestQueue;\n\nexport async function getOrInitQueue(purgeExisting = false) {\n    if (!requestQueue) {\n        requestQueue = await RequestQueue.open('locking-queue', {\n            clientKey: 'locking-queue',\n            name: 'locking-queue',\n            forceCloud: true,\n            // This is a separate config for the request queue\n            requestQueueOptions: {\n                // We need to use the same clientKey for locking to work\n                clientKey: 'locking-queue',\n                name: 'locking-queue',\n                forceCloud: true,\n            },\n        });\n    }\n\n    if (purgeExisting) {\n        await requestQueue.drop();\n    }\n\n    return requestQueue;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Response Map in SuperScraper with TypeScript\nDESCRIPTION: This code initializes a Map to store response objects. Each response is associated with a unique key, allowing SuperScraper to manage multiple concurrent requests and their corresponding responses.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst responses = new Map<string, ServerResponse>();\n```\n\n----------------------------------------\n\nTITLE: Example Dataset Structure in JSON\nDESCRIPTION: Sample dataset structure stored in the default dataset location representing scraped URLs and their heading counts. This structure can be created using the dataset.pushData() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Saving Data to Default Dataset in Crawlee\nDESCRIPTION: This code snippet demonstrates how to save data to the default dataset in Crawlee. It uses the Dataset.pushData() method to add items to the dataset. If the dataset doesn't exist, it will be automatically created.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.pushData({\n    id: 1,\n    title: 'Example item',\n});\n\n// Or add multiple items at once\nawait Dataset.pushData([\n    { id: 2, title: 'Example item 2' },\n    { id: 3, title: 'Example item 3' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Building an Express Server Wrapper for PlaywrightCrawler in GCP Cloud Run\nDESCRIPTION: Complete implementation of an Express HTTP server that wraps the Crawlee crawler for use in GCP Cloud Run. The server listens on the port specified by the PORT environment variable, executes the crawler on HTTP requests, and returns crawled data as the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Shows how to explicitly purge default storages (including request queue and request list data) before starting a crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Page Title with JSDOM in JavaScript\nDESCRIPTION: Demonstrates how to get the page title using JSDOM, comparing it with browser JavaScript. This showcases the similarity between JSDOM and browser APIs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Crawling a Single URL with Crawlee and got-scraping in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use got-scraping to fetch the HTML of a web page, process it using Cheerio, and log the page title. It requires the got-scraping npm package and uses Crawlee's Dataset for storing results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport cheerio from 'cheerio';\nimport { Dataset } from 'crawlee';\n\n// Specify the URL to crawl\nconst url = 'https://crawlee.dev';\n\n// Fetch the HTML content of the page\nconst response = await gotScraping(url);\nconst html = response.body;\n\n// Parse the HTML content with Cheerio\nconst $ = cheerio.load(html);\n\n// Extract data from the page (e.g. the page title)\nconst pageTitle = $('title').text();\n\nconsole.log(`Page title: ${pageTitle}`);\n\n// Save the results to the default dataset\nawait Dataset.pushData({\n    url,\n    pageTitle,\n});\n```\n\n----------------------------------------\n\nTITLE: Using Custom Modules with Apify Launch Functions (JavaScript)\nDESCRIPTION: This snippet illustrates how to use custom modules with Apify's launch functions for both Puppeteer and Playwright. It shows the normalization of the 'launcher' option across both libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Lifecycle Hooks\nDESCRIPTION: Example showing how to use the new lifecycle hooks in BrowserPool, which allow customizing browser behavior through the browserPoolOptions configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example showing the proper way to close browsers and set cookies using the new BrowserController, which provides a unified API for both Puppeteer and Playwright browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Reduce Method in Crawlee\nDESCRIPTION: Example of using the Dataset.reduce() method to aggregate data by calculating the total number of headers across all scraped pages. The method iteratively processes each item and accumulates a final result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Proxy Selection Function in JavaScript\nDESCRIPTION: Demonstrates how to use a custom function to select a proxy URL based on the request details. This allows for conditional proxy usage depending on the target URL or other factors.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    newUrlFunction: (sessionId, { request }) => {\n        if (request?.url.includes('crawlee.dev')) {\n            return null; // for crawlee.dev, we don't use a proxy\n        }\n\n        return 'http://proxy-1.com'; // for all other URLs, we use this proxy\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Shows how to completely disable browser fingerprint functionality in PlaywrightCrawler when fingerprinting is not needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue in Crawlee\nDESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to crawl. This shows the basic setup for managing crawl requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: Example of how to customize browser fingerprints in PuppeteerCrawler by specifying browser type, version, and operating system to avoid getting blocked during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Choose from chrome, firefox or safari\n        browserType: 'chrome',\n        // Fingerprints are enabled by default\n        useFingerprints: true,\n        // If fingerprints are enabled, you can provide options for the fingerprinting\n        fingerprintOptions: {\n            // Select browsers from Chrome, Firefox and Safari\n            browsers: [{ name: 'chrome', minVersion: 88 }],\n            // Select operating systems\n            operatingSystems: ['windows'],\n            // Set up language, timezone and screen resolution\n            locales: ['cs-CZ', 'sk-SK', 'de-DE'],\n            // And many more, see @crawlee/fingerprint-generator\n        },\n    },\n    async requestHandler({ page }) {\n        // ... page handling\n    },\n});\n\nawait crawler.run(['https://www.example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Request Handler Function for Google Maps Scraping in Python\nDESCRIPTION: A request handler function that establishes a connection to Google Maps and processes the page. This function is called for each URL the crawler visits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def scrape_google_maps(context):\n    \"\"\"\n    Establishes connection to Google Maps and handles the initial page load\n    \"\"\"\n    page = context.page\n    await page.goto(context.request.url)\n    context.log.info(f\"Processing: {context.request.url}\")\n```\n\n----------------------------------------\n\nTITLE: Using Custom cookieJar with sendRequest\nDESCRIPTION: Example demonstrating how to use a custom cookieJar with sendRequest. This uses the tough-cookie package to manage cookies across requests, which is useful for maintaining sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrapy for breadth-first crawling\nDESCRIPTION: This Python code snippet shows how to configure Scrapy's settings to use breadth-first crawling instead of the default depth-first order. It modifies the queue and priority settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleFifoDiskQueue\"\nSCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Selector with enqueueLinks\nDESCRIPTION: Overriding the default link selection in enqueueLinks by specifying a custom CSS selector to find elements containing links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a request queue in Crawlee, including adding requests, handling them, and checking queue status.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\n// Add requests to queue\nawait requestQueue.addRequest({ url: 'https://example.com/1' });\nawait requestQueue.addRequest({ url: 'https://example.com/2' });\nawait requestQueue.addRequest({ url: 'https://example.com/3' });\n\n// Get next request from queue\nconst request1 = await requestQueue.fetchNextRequest();\nconsole.log(request1.url); // https://example.com/1\n\n// Mark request as handled\nawait requestQueue.markRequestHandled(request1);\n\n// Check if queue is empty\nconst isEmpty = await requestQueue.isEmpty();\nconsole.log(isEmpty); // false\n\n// Get queue info\nconst info = await requestQueue.getInfo();\nconsole.log(info);\n// {\n//   totalRequestCount: 3,\n//   handledRequestCount: 1,\n//   pendingRequestCount: 2,\n//   ...\n// }\n```\n\n----------------------------------------\n\nTITLE: Handling Compressed Responses with Headers in Python using httpx\nDESCRIPTION: This code demonstrates how to make an HTTP request to Wayfair with proper headers, showing the problem with compressed responses when lacking the Brotli library. The snippet includes browser-like headers and prints the first 10 bytes of the response to illustrate encoding issues.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nurl = 'https://www.wayfair.com/'\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n    \"Connection\": \"keep-alive\",\n}\n\nresponse = httpx.get(url, headers=headers)\n\nprint(response.content[:10])\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium Launch Options for AWS Lambda\nDESCRIPTION: JavaScript code demonstrating how to configure PlaywrightCrawler with AWS-specific Chromium settings and launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Structure for Dataset Examples in Crawlee\nDESCRIPTION: A JSON array representing a sample dataset that contains information about scraped web pages, including the URL and the count of heading elements (h1-h3) on each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project\nDESCRIPTION: Command to initialize the project configuration for Apify Platform deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawlee for Web Crawling in TypeScript\nDESCRIPTION: Implementation of Crawlee to crawl Amazon product pages. It creates a CheerioCrawler with a request handler that processes each page, extracts product details, and saves them as structured data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, CheerioCrawlingContext, log } from 'crawlee';\nimport { extractProductDetails } from './scraper.js';\n\n/**\n * Performs the logic of the crawler. It is called for each URL to crawl.\n * - Passed to the crawler using the `requestHandler` option.\n */\nconst requestHandler = async (context: CheerioCrawlingContext) => {\n    const { $, request } = context;\n    const { url } = request;\n\n    log.info(`Scraping product page`, { url });\n    const extractedProduct = extractProductDetails($);\n\n    log.info(`Scraped product details for \"${extractedProduct.title}\", saving...`, { url });\n    crawler.pushData(extractedProduct);\n};\n\n/**\n * The crawler instance. Crawlee provides a few different crawlers, but we'll use CheerioCrawler, as it's very fast and simple to use.\n * - Alternatively, we could use a full browser crawler like `PlaywrightCrawler` to imitate a real browser.\n */\nconst crawler = new CheerioCrawler({ requestHandler });\n\nawait crawler.run(['https://www.amazon.com/dp/B0BV7XQ9V9']);\n```\n\n----------------------------------------\n\nTITLE: BrowserPool Methods Replacing PuppeteerPool Methods\nDESCRIPTION: Comparison of how to perform common operations like recycling pages and retiring browsers with the new BrowserPool API versus the old PuppeteerPool API.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\n----------------------------------------\n\nTITLE: Creating a New Crawlee Project\nDESCRIPTION: Command to create a new Crawlee project using the Crawlee CLI through npx. This will set up a new crawler project with all necessary dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Crawlee\nDESCRIPTION: Complete implementation of the AWS Lambda handler function that runs a Crawlee crawler. This code handles initialization, execution, and returns the crawling results with a proper status code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support, which is required since SDK v1 no longer bundles browser automation libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Updating Event Handling for Crawlee\nDESCRIPTION: Example showing how to migrate from Apify.events to the new Actor.on pattern for event subscription in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration with Custom URLs in Crawlee\nDESCRIPTION: Creates a new ProxyConfiguration instance with custom proxy URLs and demonstrates how to get a new proxy URL. This basic setup allows users to start using their own proxies immediately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Running the Bluesky API Crawler with Specified Queries\nDESCRIPTION: This method executes the crawler with a list of search queries. It constructs search URLs for each query and initiates the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl(self, queries: list[str]) -> None:\n    \"\"\"Crawl the given URL.\"\"\"\n    if not self._crawler:\n        raise ValueError('Crawler not initialized.')\n\n    search_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.feed.searchPosts')\n\n    await self._crawler.run([str(search_url.with_query(q=query)) for query in queries])\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main wrapper function in TypeScript\nDESCRIPTION: Alternative approach to the explicit init/exit pattern, using the Actor.main() function that automatically handles initialization and cleanup. This is functionally equivalent to the explicit approach but with less boilerplate.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links with Cheerio\nDESCRIPTION: Example demonstrating how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer-Chrome Docker Image\nDESCRIPTION: Dockerfile configuration for using Apify's Puppeteer-Chrome image. This includes Puppeteer and Chrome browser, supporting both CheerioCrawler and PuppeteerCrawler with XVFB for headless/headful modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Job Listing Handler for Extracting Job Details\nDESCRIPTION: Handler function that extracts job information (title, company name, posting time, and URL) from individual LinkedIn job posting pages, cleans the data, and saves it to storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('job_listing')\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Handler for job listings.\"\"\"\n\n    await context.page.wait_for_load_state('load')\n\n    job_title = await context.page.locator('div.top-card-layout__entity-info h1.top-card-layout__title').text_content()\n\n    company_name  = await context.page.locator('span.topcard__flavor a').text_content()\n\n    time_of_posting= await context.page.locator('div.topcard__flavor-row span.posted-time-ago__text').text_content()\n\n\n    await context.push_data(\n        {\n            # we are making use of regex to remove special characters for the extracted texts\n\n            'title': re.sub(r'[\\s\\n]+', '', job_title),\n            'Company name': re.sub(r'[\\s\\n]+', '', company_name),\n            'Time of posting': re.sub(r'[\\s\\n]+', '', time_of_posting),\n            'url': context.request.loaded_url,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom Link Selector with enqueueLinks\nDESCRIPTION: Example of using enqueueLinks with a custom selector to find links in specific div elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Camoufox with PlaywrightCrawler\nDESCRIPTION: Example of using Camoufox, a stealthy Firefox build, with PlaywrightCrawler for handling Cloudflare challenges.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, handleCloudflareChallenge } from 'crawlee';\nimport { CamoufoxLauncher } from '@apify/camoufox-js';\n\nconst camoufox = new CamoufoxLauncher();\nconst crawler = new PlaywrightCrawler({\n    preNavigationHooks: [handleCloudflareChallenge],\n    browserPoolOptions: {\n        launchOptions: {\n            launcher: camoufox,\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Crawler Concurrency Limits\nDESCRIPTION: Demonstrates how to configure minimum and maximum concurrency settings for parallel request handling in a CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    // Start with 3 parallel requests\n    minConcurrency: 3,\n    // Allow up to 10 parallel requests\n    maxConcurrency: 10,\n    async requestHandler({ $ }) {\n        // Process the data here...\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of PuppeteerCrawler Without Waiting\nDESCRIPTION: This snippet demonstrates an incorrect usage of PuppeteerCrawler where the code doesn't wait for elements to render. This approach will likely fail to extract the desired content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be rendered yet\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns in Crawlee\nDESCRIPTION: Uses glob patterns to filter which URLs should be enqueued, limiting the crawler to only follow links matching the specified pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: This snippet shows how to perform basic operations with key-value stores in Crawlee, including getting input, setting output, opening a named store, and manipulating values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Map Method Result in Crawlee\nDESCRIPTION: The expected result of the map operation, showing an array of heading counts greater than 5.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Using Reduce Method with Crawlee Dataset\nDESCRIPTION: Example of using the Dataset.reduce() method to aggregate values across all dataset items. This code calculates the total number of headings across all scraped pages by accumulating the headingCount values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler with SendRequest Helper\nDESCRIPTION: Demonstrates using the context.sendRequest() helper with got-scraping in a BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for Element Selection and Data Extraction\nDESCRIPTION: This code snippet demonstrates the difference between plain browser JavaScript and Cheerio syntax for extracting text content from elements and creating arrays of link URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Using Request List in Crawlee\nDESCRIPTION: Shows how to create and use a Request List with a Crawler in Crawlee. This is useful for processing a predefined list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options in Apify SDK for JavaScript\nDESCRIPTION: Shows the transition from using launchPuppeteerOptions to the new launchContext object in Apify's PuppeteerCrawler. This change separates Apify-specific options from Puppeteer launch options, reducing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects with TypeScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Using launchPuppeteerOptions (Old Pattern)\nDESCRIPTION: Example of the deprecated launchPuppeteerOptions configuration that mixed Apify-specific options with Puppeteer launch options in a single object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result Example\nDESCRIPTION: Example output showing the result of reduce operation that sums all header counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in CheerioCrawler\nDESCRIPTION: Shows how to access and inspect the current proxy information within the CheerioCrawler's request handler. This allows retrieving the proxy URL used for the request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request, $ }) {\n        console.log(`Used proxy URL: ${proxyInfo.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in HttpCrawler\nDESCRIPTION: Shows how to inspect the current proxy information during the request handler in HttpCrawler. This is useful for debugging and logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, proxyInfo, log }) {\n        // Log information about the currently used proxy\n        if (proxyInfo) {\n            log.info(`Currently using proxy: ${proxyInfo.url}`);\n        }\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue and Adding Requests in Crawlee\nDESCRIPTION: This code demonstrates how to initialize a RequestQueue and add a request to it. The RequestQueue is used to store URLs that the crawler will visit.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests per Crawl Limit\nDESCRIPTION: Example of setting a maximum limit of pages to crawl using the maxRequestsPerCrawl option in CheerioCrawler to prevent excessive crawling during testing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up an Actor with Actor.main()\nDESCRIPTION: Example of setting up a Cheerio crawler that integrates with the Apify platform using Actor.main(). This approach handles initialization and proper exit automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.main(async () => {\n    const crawler = new CheerioCrawler({\n        async requestHandler({ $, request, enqueueLinks }) {\n            const title = $('title').text();\n            const h1 = $('h1').text();\n            await Dataset.pushData({\n                url: request.url,\n                title,\n                h1,\n            });\n\n            // Only follow links on the same domain\n            await enqueueLinks({\n                // Only limit the behavior\n                // of the globs on our domain.\n                // The crawler follows links to\n                // other domains normally and will\n                // not limit those with globs.\n                baseUrl: request.loadedUrl,\n                // Only follow links from talks and speakers\n                globs: ['**/talks/**', '**/speakers/**'],\n            });\n        },\n    });\n\n    // Either specify the list of URLs to crawl...\n    await crawler.run(['https://crawlee.dev']);\n});\n\n```\n\n----------------------------------------\n\nTITLE: Using Proxies for Google Maps Scraping\nDESCRIPTION: Implementation of proxy configuration for the Google Maps scraper using Crawlee's proxy management. This allows avoiding IP blocks, bypassing rate limits, and accessing location-specific data by rotating through multiple proxy IPs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom crawlee.proxy_configuration import ProxyConfiguration\n\n# Configure your proxy settings\nproxy_configuration = ProxyConfiguration(\n    proxy_urls=[\n        \"http://username:password@proxy.provider.com:12345\",\n        # Add more proxy URLs as needed\n    ]\n)\n\n# Initialize crawler with proxy support\ncrawler = PlaywrightCrawler(\n    headless=True,\n    request_handler_timeout=timedelta(minutes=5),\n    proxy_configuration=proxy_configuration,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Express Server for Crawlee in GCP Cloud Run\nDESCRIPTION: Complete implementation of a stateless Express HTTP server that wraps a Crawlee crawler for deployment to GCP Cloud Run. The server listens on the port specified by GCP's environment variables and returns the crawler's data as the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Importing API Link Component in JSX\nDESCRIPTION: Import statement for a custom API link component used in the documentation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Disabled Storage Persistence for GCP\nDESCRIPTION: Configures CheerioCrawler with a separate Configuration instance where persistStorage is set to false. This is necessary for cloud functions as they have ephemeral file systems.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Implementing User Profile Handler for Bluesky API\nDESCRIPTION: Implements the user handler method to process user profile requests, extract user data, and store it in the dataset. This method is called when the scraper is operating in 'users' mode.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nasync def _user_handler(self, context: HttpCrawlingContext) -> None:\n    \"\"\"Handle user profile requests.\"\"\"\n    context.log.info(f'Processing user {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    user_item = {\n        'did': data['did'],\n        'created': data['createdAt'],\n        'avatar': data.get('avatar'),\n        'description': data.get('description'),\n        'display_name': data.get('displayName'),\n        'handle': data['handle'],\n        'indexed': data.get('indexedAt'),\n        'posts_count': data['postsCount'],\n        'followers_count': data['followersCount'],\n        'follows_count': data['followsCount'],\n    }\n\n    await context.push_data(user_item)\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler. It sends a request and logs the response body.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Search Form with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to interact with GitHub's search form. It fills in search terms, repository owner, start date, and language, then submits the form and extracts the search results into a dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        if (request.label === 'START') {\n            log.info('Looking for repositories...');\n            // Fill form fields\n            await page.type('#search_form_input', 'document.querySelector');\n            // Select search by repository owner\n            await page.click('#search_form_repo_owner');\n            await page.type('#search_form_repo_owner', 'apify');\n            // Set start date to February 2022\n            await page.click('#search_form_start_date');\n            for (let i = 0; i < 10; i++) {\n                await page.keyboard.press('Backspace');\n            }\n            await page.type('#search_form_start_date', '2022-02-01');\n            // Select language as JavaScript\n            await page.select('#search_form_language', 'JavaScript');\n            // Submit search form\n            await Promise.all([\n                page.waitForNavigation(),\n                page.click('#search_form_submit'),\n            ]);\n\n            // Extract search results\n            const searchResults = await page.$$eval('.search-results .repo-item', (items) => {\n                return items.map((item) => ({\n                    title: item.querySelector('.repo-title').textContent.trim(),\n                    description: item.querySelector('.repo-description')?.textContent.trim() || null,\n                    stars: parseInt(item.querySelector('.repo-stars')?.textContent.trim() || '0', 10),\n                    lastUpdate: item.querySelector('.repo-last-update')?.textContent.trim() || null,\n                }));\n            });\n\n            // Save results to dataset\n            await Dataset.pushData({\n                query: 'document.querySelector',\n                owner: 'apify',\n                startDate: '2022-02-01',\n                language: 'JavaScript',\n                results: searchResults,\n            });\n\n            log.info(`Found ${searchResults.length} repositories`);\n        }\n    },\n    // Uncomment this to make it run headful\n    // headless: false,\n});\n\n// Start the crawler with the initial request\nawait crawler.run(['https://github.com/search/advanced']);\n```\n\n----------------------------------------\n\nTITLE: Failed Attempt to Scrape with PuppeteerCrawler Without Waiting\nDESCRIPTION: This snippet demonstrates a failed attempt to scrape content using PuppeteerCrawler without waiting for elements to render, resulting in an error.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Try to extract text content of an actor card without waiting\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript as Development Dependency\nDESCRIPTION: Command to install TypeScript locally as a development dependency in a Crawlee project, avoiding pollution of the production environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev typescript\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File using Apify API in JavaScript\nDESCRIPTION: This code snippet demonstrates how to export an entire dataset to a single CSV file using Apify's Dataset and KeyValueStore APIs. It utilizes the 'exportToValue' function to export the default dataset and then stores the resulting CSV file in a key-value store named 'my-data'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset, KeyValueStore } from '@crawlee/memory';\n\nawait Actor.init();\n\n// Get input\nconst input = await Actor.getInput();\n\n// Create a dataset\nconst dataset = await Dataset.open();\n\n// Push some items to the dataset\nawait dataset.pushData({ foo: 'bar1' });\nawait dataset.pushData({ foo: 'bar2' });\nawait dataset.pushData({ foo: 'bar3' });\n\n// Export entire dataset to a single CSV file\nconst csv = await dataset.exportToValue('CSV');\n\n// Save the CSV file to the key-value store\nawait KeyValueStore.setValue('DATASET.csv', csv, { contentType: 'text/csv' });\n\n// Print some information\nconsole.log('CSV file size:', csv.length);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP deployment\nDESCRIPTION: Sets the main entry point for the GCP Function in package.json by updating the 'main' field to point to src/main.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, which enables automation of Firefox and WebKit (Safari) browsers in addition to Chromium.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced AutoscaledPool Options in Crawlee\nDESCRIPTION: This snippet illustrates how to set advanced autoscaledPoolOptions in a CheerioCrawler for fine-tuning the autoscaling behavior of the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.8,\n        scaleUpStepRatio: 0.1,\n        scaleDownStepRatio: 0.1,\n        maybeRunIntervalSecs: 1,\n        loggingIntervalSecs: 120,\n        autoscaleIntervalSecs: 15,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using curl_cffi for Web Scraping with TLS Fingerprinting Bypass in Python\nDESCRIPTION: This code demonstrates how to use the curl_cffi library to make a GET request to a website while bypassing TLS fingerprinting. It uses custom headers and impersonates a specific browser version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom curl_cffi import requests\n\nurl = 'https://www.g2.com/'\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n    \"Connection\": \"keep-alive\",\n}\n\nresponse = requests.get(url, headers=headers, impersonate=\"chrome124\")\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy your local actor code to the Apify platform. The code will be uploaded, built, and made available to run on the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor with Playwright Chrome\nDESCRIPTION: This Dockerfile sets up an environment for running Crawlee actors with Playwright and Chrome. It uses a specialized base image, implements efficient dependency caching, installs production dependencies only, and configures proper permissions and startup command.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration with Custom URLs in JavaScript\nDESCRIPTION: Demonstrates how to create a basic ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler and Globs\nDESCRIPTION: Demonstrates how to use CheerioCrawler to crawl specific links on a website using glob patterns. The code shows how to set up a crawler that only follows links matching certain URL patterns and processes them using cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Create the crawler and add URLs\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ enqueueLinks, $ }) {\n        // Add new URLs to the queue according to our glob pattern\n        await enqueueLinks({\n            globs: ['**/actor-page/**'],\n        });\n\n        // Save some results from the page\n        // console.log(`The title is: ${$('title').text()}`);\n    },\n    // Limit the number of connections\n    maxRequestsPerCrawl: 10,\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using launchPuppeteerFunction (Old Pattern)\nDESCRIPTION: Example of the deprecated launchPuppeteerFunction that was used to customize the browser launch process based on conditions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Handling JSON Responses with sendRequest in TypeScript\nDESCRIPTION: Example demonstrating how to handle JSON responses by setting the responseType option to 'json'. This tells the parser to interpret the response as JSON instead of plain text.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in CheerioCrawler\nDESCRIPTION: Demonstrates how to access and inspect proxy information during CheerioCrawler request processing. Useful for monitoring which proxy is being used for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, proxyInfo, log }) {\n        // Log information about the currently used proxy\n        if (proxyInfo) {\n            log.info(`Currently using proxy: ${proxyInfo.url}`);\n        }\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Disabling fingerprints in Crawlee browser pool\nDESCRIPTION: Example of how to disable dynamic fingerprints in the Crawlee browser pool options when creating a PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Up Crawlee Actor Docker Image\nDESCRIPTION: This Dockerfile configures a Docker image for a Crawlee actor. It starts with a Node.js base image, installs NPM dependencies, copies the project files, and sets up the run command. The build process is optimized for caching and minimal image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Processing TikTok API Responses with Pagination Handling\nDESCRIPTION: Request handler function that processes API responses, extracts data items, and handles pagination. It enforces result limits and manages the crawling state between requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-30-jsdom-based-scraping/index.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nasync requestHandler(context) {\n    const { log, request, json } = context;\n    const { userData } = request;\n    const { itemsCounter = 0, resultsLimit = 0 } = userData;\n    if (!json.data) {\n        throw new Error('BLOCKED');\n    }\n    const { data } = json;\n    const items = data.list;\n    const counter = itemsCounter + items.length;\n    const dataItems = items.slice(\n        0,\n        resultsLimit && counter > resultsLimit\n            ? resultsLimit - itemsCounter\n            : undefined,\n    );\n    await context.pushData(dataItems);\n    const {\n        pagination: { page, total },\n    } = data;\n    log.info(\n        `Scraped ${dataItems.length} results out of ${total} from search page ${page}`,\n    );\n    const isResultsLimitNotReached =\n        counter < Math.min(total, resultsLimit);\n    if (isResultsLimitNotReached && data.pagination.has_more) {\n        const nextUrl = new URL(request.url);\n        nextUrl.searchParams.set('page', page + 1);\n        await crawler.addRequests([\n            {\n                url: nextUrl.toString(),\n                headers: request.headers,\n                userData: {\n                    ...request.userData,\n                    itemsCounter: itemsCounter + dataItems.length,\n                },\n            },\n        ]);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom CookieJar with sendRequest in TypeScript\nDESCRIPTION: This example shows how to use a custom CookieJar with sendRequest. It creates a new CookieJar instance and passes it to the sendRequest function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a request queue with a Crawler in Crawlee. This approach allows for more control over the queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Installing PuppeteerCrawler Manually\nDESCRIPTION: Command to manually install Crawlee and Puppeteer with npm, as Puppeteer is not bundled with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure Example in JSON\nDESCRIPTION: Sample JSON data structure stored in the default dataset, showing URL and heading count data that will be used in the map and reduce examples.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return\nDESCRIPTION: Final version of the AWS Lambda handler that instantiates a new Cheerio Crawler for each Lambda invocation, runs it, and returns the scraped data with a success status code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a website and enqueue links that have the same domain name, including any subdomains. The 'same-domain' strategy allows crawling across subdomains while staying within the same root domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all same-domain links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // You can also use the string form\n            // strategy: 'same-domain',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\n// Start the crawler with initial request\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Express HTTP Server for Crawlee in GCP Cloud Run\nDESCRIPTION: Implements an Express HTTP server to handle requests for the Crawlee crawler in GCP Cloud Run. This setup creates a stateless handler that initializes the crawler for each request and returns the crawled data as the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to create and manage a Request Queue explicitly when using a Crawler class in Crawlee, providing more control over the queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, RequestQueue } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev' },\n]);\n\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(request.url);\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor with explicit init/exit in TypeScript\nDESCRIPTION: Example showing how to explicitly initialize and exit an Actor using the new Apify SDK. This approach uses top-level await with Actor.init() at the beginning and Actor.exit() at the end of the script.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Accessing crawlingContext in preLaunchHooks\nDESCRIPTION: Example of accessing request data in preLaunchHooks through the crawler.crawlingContexts Map to customize browser launch options based on request properties.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Cookie Jar Implementation with BasicCrawler\nDESCRIPTION: Shows how to use a custom cookie jar with BasicCrawler for managing cookies during requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PuppeteerCrawler\nDESCRIPTION: Shows how to access and inspect information about the currently used proxy in PuppeteerCrawler's requestHandler function. This is useful for debugging or logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo }) {\n        console.log(proxyInfo.url); // http://proxy-1.com\n        console.log(proxyInfo.hostname); // proxy-1.com\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Using ProxyConfiguration with CheerioCrawler in TypeScript\nDESCRIPTION: This snippet illustrates how to integrate ProxyConfiguration with CheerioCrawler for web scraping tasks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, session }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Demonstrates the correct way to manage browsers using the new BrowserController class, which provides a unified API for both Puppeteer and Playwright browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl in Cheerio Crawler\nDESCRIPTION: Setting a limit on the number of requests processed by the crawler using maxRequestsPerCrawl option. This is useful for testing or limiting the scope of a crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with HttpCrawler\nDESCRIPTION: Demonstration of SessionPool integration with HttpCrawler for managing HTTP requests with session handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{HttpSource}\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: A JavaScript snippet that selects all anchor elements with href attributes and extracts their URLs into an array using querySelectorAll and map methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: This code snippet uses JSDOMCrawler to find all <a> elements with an href attribute on a page and extracts their href values into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Disabled Storage Persistence for GCP Cloud Run\nDESCRIPTION: Initial setup of a Crawlee PlaywrightCrawler with storage persistence disabled, which is necessary for stateless execution in cloud environments. This configuration prevents the crawler from trying to maintain state between executions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler\nDESCRIPTION: Implementation using CheerioScraper for crawling multiple URLs. The code imports Cheerio crawler package and contains a basic crawler setup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Setting Crawl Strategy to Follow All Links in Crawlee\nDESCRIPTION: Configures enqueueLinks to follow all links regardless of domain using the 'all' strategy, allowing the crawler to wander across the internet.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Operations with Request Queue in Crawlee\nDESCRIPTION: Demonstrates fundamental operations with a RequestQueue including creation, adding requests, and handling them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst queue = await RequestQueue.open('my-queue');\n\n// Add URL to the queue\nawait queue.addRequest({ url: 'http://example.com' });\n\n// Add multiple URLs to the queue\nawait queue.addRequests([\n    { url: 'http://example.com/foo' },\n    { url: 'http://example.com/bar' },\n]);\n\n// Get request from queue\nconst request1 = await queue.fetchNextRequest();\nconst request2 = await queue.fetchNextRequest();\n\n// Process request...\nconsole.log(request1.url);\nconsole.log(request2.url);\n\n// Mark requests as handled\nawait queue.markRequestHandled(request1);\nawait queue.markRequestHandled(request2);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Web Crawler with @crawlee/basic\nDESCRIPTION: This example demonstrates how to create a BasicCrawler instance that fetches HTML content from specified URLs and stores it in a dataset. The crawler processes two example URLs and uses the sendRequest utility to handle HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/basic-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create a crawler instance\nconst crawler = new BasicCrawler({\n    async requestHandler({ request, sendRequest }) {\n        // 'request' contains an instance of the Request class\n        // Here we simply fetch the HTML of the page and store it to a dataset\n        const { body } = await sendRequest({\n            url: request.url,\n            method: request.method,\n            body: request.payload,\n            headers: request.headers,\n        });\n\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        })\n    },\n});\n\n// Enqueue the initial requests and run the crawler\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer Using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot of the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { launchPuppeteer, utils } from 'crawlee';\n\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for TypeScript Projects\nDESCRIPTION: Dockerfile using multi-stage build to optimize the final image size. It builds the TypeScript code in the first stage and only includes production dependencies in the final image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Implementing CheerioCrawler for HTML Parsing\nDESCRIPTION: Example shows how to set up and use CheerioCrawler to crawl URLs from an external file. The crawler makes HTTP requests to each URL, uses Cheerio to parse the HTML content, and extracts the page title and h1 tags from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    minConcurrency: 10,\n    maxConcurrency: 50,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // Limit to 10 requests per one crawl.\n    maxRequestsPerCrawl: 10,\n\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with options as:\n    // https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlingContext\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        // Extract data from the page using Cheerio.\n        const title = $('title').text();\n        const h1texts = [];\n        $('h1').each((index, el) => {\n            h1texts.push($(el).text());\n        });\n\n        // Store the results to the dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1texts,\n        });\n\n        // Extract links from the page and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Download the list of URLs from a text file.\nconst urls = await downloadListOfUrls({ url: 'https://example.com/urls.txt' });\n\n// Run the crawler with the provided URLs.\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Old Launch Options Configuration in JavaScript\nDESCRIPTION: Shows the deprecated way of configuring launch options where Apify and Puppeteer options were mixed together.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing BasicCrawler for Web Scraping in JavaScript\nDESCRIPTION: This code demonstrates how to use Crawlee's BasicCrawler to download web pages and store their raw HTML and URL in a dataset. It uses the sendRequest utility function for HTTP requests and processes a list of start URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that uses plain HTTP requests to download web pages.\nconst crawler = new BasicCrawler({\n    // Let's limit our crawler to only 10 requests. With the concurrency of 10,\n    // this means the crawler will run for about 1 second.\n    maxRequestsPerCrawl: 10,\n    // Crawlers come with a default auto-scaling feature which keeps the concurrency\n    // high initially and then progressively decreases it. If you want to override\n    // this feature and keep the concurrency constant, set autoscaledPoolOptions.\n    // Here we set it to 10.\n    autoscaledPoolOptions: {\n        minConcurrency: 10,\n        maxConcurrency: 10,\n    },\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with options as:\n    // {\n    //    url: string,\n    //    userData: Object,\n    //    $: CheerioAPI\n    // }\n    async requestHandler({ request, sendRequest, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Fetch the page HTML via sendRequest\n        const { body } = await sendRequest();\n\n        // Store the HTML and URL to the default dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\n// Run the crawler with initial request urls\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction/first-crawler',\n    'https://crawlee.dev/docs/guides/apify-platform',\n    'https://crawlee.dev/docs/examples/cheerio-crawler',\n    'https://crawlee.dev/docs/examples/puppeteer-crawler',\n]);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in TypeScript\nDESCRIPTION: Demonstrates how to use Playwright Crawler to crawl a specific list of URLs. This example configures the crawler with Playwright-specific settings and shows how to extract data from web pages using modern browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Define the list of URLs that we want to crawl.\nconst startUrls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n];\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerMinute: 120,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // Options to be passed to the Playwright browser instance.\n    launchContext: {\n        launchOptions: {\n            // Here you can set options that are passed to the playwright .launch() method\n            // For example, you can set headless: false to display a browser UI\n            headless: true,\n        },\n    },\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Puppeteer scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    // The function accepts a single parameter, which is an object with the following fields:\n    // - request: an instance of the Request class with information such as URL and HTTP method\n    // - page: Playwright's Page object (see https://playwright.dev/docs/api/class-page)\n    async requestHandler({ request, page }) {\n        console.log(`Processing ${request.url}...`);\n\n        // Extract data from the page using Playwright.\n        const title = await page.title();\n        const h1 = await page.$eval('h1', (el) => el.textContent);\n\n        // Store the results to the dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(startUrls);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript Compiler Options\nDESCRIPTION: JSON configuration for tsconfig.json, extending @apify/tsconfig and setting up compiler options for ES2022 modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Proxy Integration with JSDOMCrawler\nDESCRIPTION: Example of integrating a ProxyConfiguration instance with JSDOMCrawler. This enables the crawler to use proxies when parsing websites with JSDOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window }) {\n        // Process the data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Router with Multiple Handlers - JavaScript\nDESCRIPTION: Defines route handlers for different page types (detail, category, default) using createPlaywrightRouter. Each handler processes specific page types and manages data extraction and navigation flow.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1);\n    const manufacturer = urlPart[0].split('-')[0];\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL',\n    });\n\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY',\n        });\n    }\n});\n\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Wrapper Function with Crawlee\nDESCRIPTION: Example showing how to use the Actor.main() wrapper function which handles initialization and cleanup automatically. This is equivalent to the explicit init/exit approach but with cleaner syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json\nDESCRIPTION: Example of a crawlee.json configuration file that sets persistence interval and log level settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This example uses JSDOMCrawler to crawl a list of URLs from an external file, parse the HTML using jsdom, and extract the page title and all h1 tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\nconst crawler = new JSDOMCrawler({\n    // Use the requestHandler to process each of the crawled pages\n    async requestHandler({ window, request, log }) {\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent;\n        const h1Texts = Array.from(document.querySelectorAll('h1'))\n            .map((el) => el.textContent);\n\n        log.info(`The title of ${request.url} is: ${title}`);\n        log.info(`The page contains ${h1Texts.length} h1 tags`);\n\n        // Save results to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1Texts,\n        });\n    },\n});\n\nconst urls = (await readFile('urls.txt', 'utf8')).split('\\n');\nawait crawler.run(urls);\n\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Shows a more concise way to implement a CheerioCrawler using the crawler.run() method with direct URL input, eliminating the need for explicit RequestQueue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Finding Links Using enqueueLinks with TypeScript\nDESCRIPTION: Example showing how to use the enqueueLinks function to find and queue new URLs from a page. This method simplifies the process of discovering and adding links to the RequestQueue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        await enqueueLinks();\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Custom Browser Modules with Crawlee Launch Functions\nDESCRIPTION: Examples demonstrating how to use custom Puppeteer or Playwright modules with the launch functions, showing the normalized approach across both browser automation libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport puppeteer from 'puppeteer';\n\nawait Actor.init();\n\nconst url = 'https://example.com';\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto(url);\n\nconst screenshot = await page.screenshot();\nawait Actor.setValue('screenshot', screenshot, { contentType: 'image/png' });\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Cheerio Package\nDESCRIPTION: Command to install only the Cheerio crawler package for lighter installations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Project\nDESCRIPTION: This Dockerfile specifies the setup for a Crawlee actor using Node.js 16 as the base image. It optimizes the build process using Docker layer cache, installs only production dependencies, and configures the execution environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced AutoscaledPool Options in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set advanced autoscaledPoolOptions in CheerioCrawler, including desiredConcurrency, scaleUpStepRatio, and autoscaleIntervalSecs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 5,\n        scaleUpStepRatio: 0.05,\n        autoscaleIntervalSecs: 10,\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Using HTML to Text Conversion in Crawlee for Python\nDESCRIPTION: Example demonstrating the use of the new html_to_text crawling context helper in a ParselCrawler to extract clean text from HTML content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    crawler = ParselCrawler()\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info('Crawling: %s', context.request.url)\n        text = context.html_to_text()\n        # Continue with the processing...\n\n    await crawler.run(['https://crawlee.dev'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing PlaywrightCrawler Manually\nDESCRIPTION: Command to install Crawlee with Playwright, which enables browser automation for Chrome, Firefox, and WebKit browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Cheerio Crawler\nDESCRIPTION: Implementation of a web crawler using Cheerio to navigate and extract all links from a website. Uses enqueueLinks() to automatically add discovered links to the RequestQueue. By default, only processes links under the same subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, downloadListOfUrls } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing new URLs...');\n        // Add new URLs to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl\nconst startUrls = await downloadListOfUrls({ url: 'https://example.com' });\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler in Typescript\nDESCRIPTION: This code demonstrates using the 'SameDomain' strategy with enqueueLinks() in CheerioCrawler to enqueue links with the same domain name, including links from any subdomain, as well as relative links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // The crawler will automatically process the requests in the queue\n    async requestHandler({ request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all links from page to RequestQueue that have the same domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n            // Optional: Provide a link selector to use for calculating the urls\n            // LinkSelectorToBeAdded\n        });\n    },\n});\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler Implementation\nDESCRIPTION: Final implementation showing the complete AWS Lambda handler function that initializes and runs the crawler, returning the results in the expected Lambda response format.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Entire Dataset to CSV in Crawlee\nDESCRIPTION: This code snippet demonstrates how to export an entire default dataset to a single CSV file and save it to the key-value store. It utilizes the exportToValue function from the Dataset class and sets the format to CSV.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\n// Open the default dataset\nconst dataset = await Dataset.open();\n\n// Export the dataset to a single CSV file\nconst csv = await dataset.exportToValue('csv');\n\n// Save the CSV file to the default key-value store\nawait KeyValueStore.setValue('RESULT.csv', csv, { contentType: 'text/csv' });\n\nconsole.log('Dataset was exported to RESULT.csv');\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for Development Execution\nDESCRIPTION: JSON snippet showing how to set up a start:dev script using ts-node-esm for development in package.json.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() in Crawlee\nDESCRIPTION: Example showing how to initialize and terminate a Crawlee actor using the explicit init/exit methods. This approach requires manually calling these asynchronous methods at the beginning and end of the script.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Updated Arguments for Apify.launchPuppeteer Function (JavaScript)\nDESCRIPTION: This code snippet illustrates the changes in the argument structure for the Apify.launchPuppeteer function. The new structure separates Apify-specific options from Puppeteer launch options, improving clarity and reducing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee\nDESCRIPTION: Diff showing how to migrate event handling from Apify SDK's events system to the Actor event methods in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Proxy Inspection in HttpCrawler\nDESCRIPTION: Shows how to access proxy information in HttpCrawler's requestHandler using proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nInspectionHttpSource\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Proxy List in Crawlee\nDESCRIPTION: Shows how to set up a static list of proxy URLs including the option to use no proxy (null). ProxyConfiguration will rotate through these proxies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n        null // null means no proxy is used\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Without Element Waiting\nDESCRIPTION: This code shows what happens when you try to access elements with PuppeteerCrawler without waiting for them to render. It will fail to extract content because the elements are not yet available in the DOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// This will fail because we're trying to access an element\n// that has not been rendered to the DOM yet\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    maxRequestRetries: 3,\n    async requestHandler({ page, request }) {\n        // We need to wait for the selector or else\n        // the element might not be in the DOM yet\n        const actorText = await page.evaluate(() => {\n            return document.querySelector('.ActorStoreItem').textContent;\n        });\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Regardless of Domain\nDESCRIPTION: Configuration of enqueueLinks to follow all links encountered, regardless of their domain, using the 'all' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PuppeteerCrawler\nDESCRIPTION: Demonstrates how to access and inspect the current proxy information within the PuppeteerCrawler's request handler. This allows retrieving the proxy URL used for the browser session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request, page }) {\n        console.log(`Used proxy URL: ${proxyInfo.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to find all anchor elements with href attributes on a page and extract those URLs into an array using Cheerio's selector and map functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium Launch Options for AWS Lambda Environment\nDESCRIPTION: Sets up the PlaywrightCrawler with the correct executable path and arguments for running Chromium in AWS Lambda. Uses the @sparticuz/chromium package to provide the browser binary path and appropriate launch arguments for the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Storage in Scrapy using Feed Exports\nDESCRIPTION: Settings configuration in Scrapy to store scraped data in different formats (CSV or JSON) using Feed Exports. This allows for automatic serialization of scraped items to files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# To store in CSV format\nFEEDS = {\n    'data/crawl_data.csv': {'format': 'csv', 'overwrite': True}\n}\n\n# OR to store in JSON format\n\nFEEDS = {\n    'data/crawl_data.json': {'format': 'json', 'overwrite': True}\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in TypeScript\nDESCRIPTION: Demonstrates how to use Puppeteer Crawler to crawl a specific list of URLs. This example shows setting up the crawler with Puppeteer-specific configurations and extracting data from web pages using browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Define the list of URLs that we want to crawl.\nconst startUrls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n];\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerMinute: 120,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // Options to be passed to the Puppeteer browser instance.\n    launchContext: {\n        launchOptions: {\n            // Here you can set options that are passed to the playwright .launch() method\n            // For example, you can set headless: false to display a browser UI\n            headless: true,\n        },\n    },\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Puppeteer scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    // The function accepts a single parameter, which is an object with the following fields:\n    // - request: an instance of the Request class with information such as URL and HTTP method\n    // - page: Puppeteer's Page object (see https://pptr.dev/#show=api-class-page)\n    async requestHandler({ request, page }) {\n        console.log(`Processing ${request.url}...`);\n\n        // Extract data from the page using Puppeteer.\n        const title = await page.title();\n        const h1 = await page.$eval('h1', (el) => el.textContent);\n\n        // Store the results to the dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(startUrls);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending from @apify/tsconfig with ES2022 settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Custom Configuration\nDESCRIPTION: Setting up CheerioCrawler with persistStorage disabled for cloud function compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for DOM Element Selection\nDESCRIPTION: This example compares how to extract the text content of the title element and collect all href links using both plain browser JavaScript and Cheerio. It demonstrates the similar syntax but different approaches between browser DOM methods and Cheerio's jQuery-like API.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Using Window API with JSDOMCrawler in Browser vs JSDOM Context\nDESCRIPTION: Demonstrates the difference in accessing page title between browser JavaScript and JSDOM environment. In browsers, you can use document.title directly, while in JSDOM you typically access it through window.document.title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Creating Accept Cookies Context Manager\nDESCRIPTION: An asyncio context manager that handles cookie dialogs in parallel to avoid blocking the main task, with proper cancellation handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom playwright.async_api import TimeoutError as PlaywrightTimeoutError\n\n@asynccontextmanager\nasync def accept_cookies(page: Page):\n    task = asyncio.create_task(page.get_by_test_id('dialog-accept-button').click())\n    try:\n        yield\n    finally:\n        if not task.done():\n            task.cancel()\n\n        with suppress(asyncio.CancelledError, PlaywrightTimeoutError):\n            await task\n```\n\n----------------------------------------\n\nTITLE: Scraping and Processing Product Price\nDESCRIPTION: Shows how to extract and process a product's price by filtering elements, cleaning the text, and converting to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced autoscaledPoolOptions in Crawlee Crawler\nDESCRIPTION: Fine-tunes the internal autoscaling mechanism with advanced options controlling scaling behavior, intervals, and performance parameters. These settings provide granular control over crawler performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        // We want to start with 10 requests in parallel\n        desiredConcurrency: 10,\n        // We want to have at least this ratio of the desired concurrency\n        desiredConcurrencyRatio: 0.95,\n        // We want to take big steps up to increase throughput faster\n        scaleUpStepRatio: 0.05,\n        // And take small steps down to avoid being too aggressive\n        scaleDownStepRatio: 0.01,\n        // Always try to start a new request within this many seconds if possible\n        maybeRunIntervalSecs: 0.5,\n        // Log the state of the pool at least once per minute\n        loggingIntervalSecs: 60,\n        // Check for scaling at least once per 10 seconds\n        autoscaleIntervalSecs: 10,\n    },\n    // Rest of the crawler options\n    async requestHandler({ request, $ }) {\n        log.info(`Processing ${request.url}`);\n        // Rest of the handler code\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Parallel Scraper with Child Processes in Crawlee\nDESCRIPTION: A script that spawns multiple worker processes to handle parallel scraping using a shared request queue with locking. The parent process coordinates the workers and collects their results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, LogLevel } from 'crawlee';\nimport { fork } from 'node:child_process';\nimport { fileURLToPath } from 'node:url';\nimport { getOrInitQueue } from './requestQueue.mjs';\nimport { createRouter } from './routes.mjs';\n\nconst WORKER_COUNT = 2;\n\n// If the process has an environment variable IS_WORKER_THREAD, it's a worker process\nif (process.env.IS_WORKER_THREAD) {\n    // Disable the automatic purge on start\n    // This is needed when running locally, as otherwise multiple processes will try to clear the default storage (and that will cause clashes)\n    Configuration.set('purgeOnStart', false);\n\n    // Get the request queue from the parent process\n    const requestQueue = await getOrInitQueue(false);\n\n    // Configure crawlee to store the worker-specific data in a separate directory (needs to be done AFTER the queue is initialized when running locally)\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    const router = createRouter();\n\n    const crawler = new CheerioCrawler({\n        // Get requests from the request queue with locking support\n        requestQueue,\n        // Use the request handler from the router\n        requestHandler: router,\n        // We don't need to add any URLs here, they're already in the queue\n        // If you want to start from a specific URL, you can add it here\n        // startUrls: [],\n\n        // Set a reasonable limit for concurrency and max requests per crawl to avoid overloading the target server\n        maxConcurrency: 10,\n        // maxRequestsPerCrawl: 20,\n\n        // Handle things like blocked requests, timeouts, etc.\n        // Using Cheerio crawler, we don't have to handle much\n        failedRequestHandler: ({ request }) => {\n            console.error(`Request ${request.url} failed too many times`);\n        },\n    });\n\n    await crawler.run();\n\n    // We're done, exit the process\n    process.exit(0);\n} else {\n    // We're in the parent process, so let's create worker processes\n    const currentFilePath = fileURLToPath(import.meta.url);\n\n    // Create promises for each worker to know when they exit\n    const workerPromises = [];\n    const scraperData = [];\n\n    // Start the worker processes\n    for (let i = 0; i < WORKER_COUNT; i++) {\n        const worker = fork(currentFilePath, [], {\n            env: {\n                IS_WORKER_THREAD: 'true',\n                WORKER_INDEX: String(i),\n            },\n        });\n\n        // Create a promise that resolves when this worker exits\n        const workerPromise = new Promise((resolve) => {\n            worker.on('exit', (code) => {\n                console.log(`Worker ${i} exited with code ${code}`);\n                resolve();\n            });\n        });\n\n        // Add event listener to receive data from the worker\n        worker.on('message', (data) => {\n            console.log(`Received data from worker ${i}`);\n            scraperData.push(data);\n        });\n\n        workerPromises.push(workerPromise);\n    }\n\n    // Wait for all workers to finish\n    await Promise.all(workerPromises);\n\n    // All workers have finished, so let's log the results\n    console.log(`Scraped ${scraperData.length} items!`);\n\n    // We could also save the data to a file or do something else with it\n    console.log('Done!');\n}\n```\n\n----------------------------------------\n\nTITLE: Modified Crawlee Script with Apify Integration\nDESCRIPTION: Example of a Crawlee crawler script modified to work with the Apify Platform, including Actor initialization and cleanup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring minConcurrency and maxConcurrency in CheerioCrawler\nDESCRIPTION: This code snippet shows how to set the minConcurrency and maxConcurrency options when creating a CheerioCrawler. These options control the minimum and maximum number of parallel requests that can be run at any time.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    minConcurrency: 1,\n    maxConcurrency: 100,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country in JavaScript\nDESCRIPTION: This snippet demonstrates how to create an Apify Proxy configuration with custom settings. It specifies the use of residential proxies from the United States, allowing for more targeted proxy selection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Direct Puppeteer using page.screenshot()\nDESCRIPTION: This code demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method directly. It launches a browser, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open new page.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Capture the screenshot.\n    const screenshotBuffer = await page.screenshot();\n\n    // Save the screenshot to the default key-value store.\n    await KeyValueStore.setValue('screenshot', screenshotBuffer, { contentType: 'image/png' });\n\n    console.log('Screenshot saved!');\n} finally {\n    // Close Puppeteer.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Response Handling with BasicCrawler\nDESCRIPTION: Demonstrates how to handle JSON responses by setting the appropriate responseType in the sendRequest options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Node.js Docker Image Configuration\nDESCRIPTION: Basic Docker configuration for Node.js based crawlers without browser automation support. Best used with CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links using CheerioCrawler\nDESCRIPTION: Example showing how to crawl links from the same domain and its subdomains using the 'same-domain' strategy. Processes URLs that match the base domain regardless of subdomain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Add all links that share the same domain (including subdomains)\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Sending Email Alerts with Apify SDK in Python\nDESCRIPTION: Python code snippet that checks if a product price is below a threshold and sends an email notification using the apify/send-mail actor. This demonstrates how to use the Apify SDK to trigger another actor with custom input parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Call the \"Send Email\" Actor when the price goes below the threshold            \nif data['price'] < price_threshold:\n    actor_run = await Actor.start(\n        actor_id=\"apify/send-mail\",\n        run_input={\n            \"to\": \"your_email@email.com\",\n            \"subject\": \"Python Price Alert\",\n            \"text\": f\"The price of '{data['product_name']}' has dropped below ${price_threshold} and is now ${data['price']}.\\n\\nCheck it out here: {data['url']}\",\n        },\n    )\n    Actor.log.info(f\"Email sent with run ID: {actor_run.id}\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments in Previous vs v1 Versions\nDESCRIPTION: Demonstrates the difference in handler argument structure between previous SDK versions and v1, introducing the concept of Crawling Context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Extracting Links with JSDOM\nDESCRIPTION: Code snippet that finds all anchor elements with href attributes and extracts their URLs into an array using JSDOM's querySelector functionality\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Using HttpCrawler to Crawl URLs and Save HTML in Typescript\nDESCRIPTION: This example demonstrates a complete implementation of using HttpCrawler to read URLs from a file, process them with HTTP requests, and save the resulting HTML. It includes request filtering, setup of crawler options, and handling of response data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, HttpCrawler, CrawlerExtension, createHttpRouter } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n/**\n * A crawler reads urls from a file and processes each page using a \n * plain HTTP request and saves HTML.\n */\nconst crawler = new HttpCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n    requestHandler: async ({ request, response, crawler, json, body, statusCode, headers, $ }) => {\n        // Save HTML to default dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: body.toString('utf-8'),\n            statusCode,\n        });\n    },\n    // Comment this option to scrape the actual website.\n    // It's better to test your crawlers first with our mock server.\n    useSessionPool: false,\n    // Uncomment this option to see the browser in action.\n    // headless: false,\n});\n\n// Parse and add URLs from the text file to the crawler's request queue\nconst fileUrls = await readFile('./urls.txt', 'utf8');\nconst urls = fileUrls.trim().split('\\n');\nconsole.log(`Added ${urls.length} URLs to the crawler's request queue: ${urls}`);\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Building Crawlee TypeScript Projects\nDESCRIPTION: Multi-stage Dockerfile for building and running Crawlee TypeScript projects, optimizing for production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Configuring maxRequestsPerMinute in Crawlee Crawler\nDESCRIPTION: Sets the maximum number of requests per minute to control the crawler's rate limiting. This prevents request bursts while maintaining a steady throughput.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // This will ensure we don't make more than 60 requests per minute\n    maxRequestsPerMinute: 60,\n    // Rest of the crawler options\n    async requestHandler({ request, $ }) {\n        log.info(`Processing ${request.url}`);\n        // Rest of the handler code\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced autoscaledPoolOptions in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set advanced autoscaledPoolOptions when initializing a CheerioCrawler. These options provide fine-grained control over the autoscaling behavior of the crawler, including desired concurrency, scaling ratios, and various interval settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.95,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 0.5,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 60\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels with enqueueLinks\nDESCRIPTION: Demonstrates how to use the Request.label shortcut for categorizing requests. This example shows conditional link enqueuing based on the current request's label.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: Commands for installing Crawlee and its various packages using npm. Shows different installation options including the meta-package and individual crawler packages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n\nnpm install @crawlee/cheerio\n\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer Dependencies\nDESCRIPTION: Command to install puppeteer-extra and its stealth plugin using npm package manager\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Installing Brotli for HTTP Response Decompression\nDESCRIPTION: These commands show how to install the Brotli library and its integration with popular HTTP client libraries (aiohttp and httpx) to handle compressed responses correctly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install Brotli\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install aiohttp[speedups]\npip install httpx[brotli]\n```\n\n----------------------------------------\n\nTITLE: Basic EnqueueLinks Example in JavaScript\nDESCRIPTION: A simple example of using the enqueueLinks function without parameters to find all <a> elements on a page and add them to the crawler queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: File Storage Path for Dataset Items in Crawlee\nDESCRIPTION: Shows the directory path where dataset items are saved when working with the default dataset in Crawlee. Each item is stored as a separate file within this storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright in JavaScript\nDESCRIPTION: Demonstrates how to use Playwright to locate and extract the product SKU from a specific HTML element.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Using auto-saved crawler state with useState()\nDESCRIPTION: Shows how to use the crawler.useState() method to maintain state across crawler runs. The state is automatically saved when the persistState event occurs, eliminating the need to explicitly save values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Updated Launch Options for Apify.launchPuppeteer() (JavaScript)\nDESCRIPTION: This code demonstrates the updated structure of launch options for 'Apify.launchPuppeteer()'. It separates Apify-specific options from Puppeteer launch options for better clarity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Package Dependencies\nDESCRIPTION: npm commands for installing Crawlee and its dependencies, showing different installation options for full or specific crawler implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n\nnpm install @crawlee/cheerio\n\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler\nDESCRIPTION: Code for crawling category pages using PlaywrightCrawler with the enqueueLinks() function and a specific selector to target only category links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Creating ProxyConfiguration with Custom Groups\nDESCRIPTION: Creates a ProxyConfiguration instance with specific proxy groups for Apify proxy. This configuration specifies which proxy groups to use when rotating proxies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    groups: ['GROUP1', 'GROUP2'],\n});\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Shows a more concise way to create a CheerioCrawler using the crawler.run() method with direct URL input.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/02-first-crawler.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Custom Selector with enqueueLinks in Crawlee\nDESCRIPTION: Customizing the element selector used by enqueueLinks function to find links on a page. This example uses div elements with 'has-link' class instead of the default anchor tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in HttpCrawler with TypeScript\nDESCRIPTION: This snippet shows how to access and log the current proxy information within the HttpCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo }) {\n        if (proxyInfo) {\n            console.log(`Proxy URL: ${proxyInfo.url}`);\n        }\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee\nDESCRIPTION: This example shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'SameHostname' strategy, which is the default for enqueueLinks(), to only enqueue URLs from the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add new links from page to the crawling queue\n        // Only links that have the same hostname as the startUrls will be crawled\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Crawler with AWS Chromium\nDESCRIPTION: JavaScript code to set up a PlaywrightCrawler with AWS-compatible Chromium settings, including the executable path and launch arguments from @sparticuz/chromium.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Using Playwright Firefox Image\nDESCRIPTION: Using the Docker image optimized for Playwright with Firefox browser pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:20\n```\n\n----------------------------------------\n\nTITLE: HTML Link Example\nDESCRIPTION: Example of an HTML anchor tag with href attribute for demonstration purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://crawlee.dev/js/docs/introduction\">This is a link to Crawlee introduction</a>\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Setup with Direct URL Addition in Crawlee\nDESCRIPTION: This snippet shows a more concise way to set up a CheerioCrawler. It uses the crawler's built-in RequestQueue and adds URLs directly through the run() method, simplifying the initialization process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing Browser Pool with Playwright\nDESCRIPTION: Command to install the Browser Pool library and Playwright using npm. Browser Pool doesn't come preinstalled with browser automation libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/browser-pool playwright\n```\n\n----------------------------------------\n\nTITLE: Using Glob Patterns with enqueueLinks in TypeScript\nDESCRIPTION: Shows how to use glob patterns to filter URLs when enqueueing links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom HTTP Client with Fetch API in TypeScript\nDESCRIPTION: A skeleton implementation of the BaseHttpClient interface using the standard fetch API. This implementation handles both GET and POST requests, processes response data, and manages cookies.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BaseHttpClient, InternalHttpRequestOptions, RequestOptions } from '@crawlee/core';\n\nclass FetchHttpClient implements BaseHttpClient {\n    // Initialize any internal options\n    constructor(options: Record<string, unknown> = {}) {\n        console.log('Created FetchHttpClient with options', options);\n    }\n\n    // Cookies may be stored in this property and available for the lifetime of the client\n    private cookies: Record<string, string> = {};\n\n    async request<T>(options: InternalHttpRequestOptions): Promise<T> {\n        // The object can take any input options, like proxies, headers etc\n        // The simplified version just handles GET and POST requests with a single retry\n        const { method = 'GET', url, payload, responseType } = options;\n\n        // Note that other properties would need to be handled for a complete implementation\n        const requestOptions: RequestInit = {\n            method,\n            headers: {\n                // Include cookies in the request\n                Cookie: Object.entries(this.cookies)\n                    .map(([key, value]) => `${key}=${value}`)\n                    .join('; '),\n                // You would need to also handle the other headers from options.headers\n            },\n        };\n\n        if (method === 'POST' && payload) {\n            requestOptions.body = payload as BodyInit;\n        }\n\n        const response = await fetch(url, requestOptions);\n\n        // Handle cookies from the response\n        const setCookieHeader = response.headers.get('set-cookie');\n        if (setCookieHeader) {\n            const cookieStrings = setCookieHeader.split(',');\n            for (const cookieString of cookieStrings) {\n                const [keyValue] = cookieString.split(';');\n                const [key, value] = keyValue.split('=');\n                this.cookies[key.trim()] = value.trim();\n            }\n        }\n\n        // Handle different response types\n        let data: unknown;\n        if (responseType === 'buffer') {\n            data = await response.arrayBuffer();\n        } else if (responseType === 'json') {\n            data = await response.json();\n        } else {\n            data = await response.text();\n        }\n\n        return data as T;\n    }\n\n    // This method is used by sendRequest\n    async sendRequest<T>(options: RequestOptions): Promise<T> {\n        return this.request<T>(options as InternalHttpRequestOptions);\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright\nDESCRIPTION: Demonstrates checking if a product is in stock by locating and filtering elements containing 'In stock' text and checking if any such elements exist.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects with TypeScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy an actor project to the Apify platform using the CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform via CLI\nDESCRIPTION: Command to authenticate with the Apify Platform using the CLI tool. Requires a personal access token from the Apify account.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration in JavaScript\nDESCRIPTION: This snippet shows how to create a proxy configuration using Apify Proxy. It initializes the proxy configuration using Actor.createProxyConfiguration() and obtains a new proxy URL that can be used for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Session Headers Injection\nDESCRIPTION: Setup for CheerioCrawler that utilizes session pool and pre-navigation hooks to inject authentication headers into requests. This enables making authenticated API calls without browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-30-jsdom-based-scraping/index.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    sessionPoolOptions: {\n        maxPoolSize: 1,\n        createSessionFunction: async (sessionPool) =>\n            createSessionFunction(sessionPool, proxyConfiguration),\n    },\n    preNavigationHooks: [\n        (crawlingContext) => {\n            const { request, session } = crawlingContext;\n            request.headers = {\n                ...request.headers,\n                ...session.userData?.headers,\n            };\n        },\n    ],\n    proxyConfiguration,\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nconst utils = { puppeteer: { saveSnapshot: () => {} } };\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with WebKit Docker Image\nDESCRIPTION: Example of using a Docker image with pre-installed Playwright and WebKit browser.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Installing TSConfig Dependencies\nDESCRIPTION: Command to install Apify TSConfig package\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL with JavaScript String Manipulation\nDESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the URL string into parts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json Configuration\nDESCRIPTION: Example demonstrating how to use a CheerioCrawler with configuration from crawlee.json without any explicit configuration in the code. Shows how the crawler uses the configured persistStateIntervalMillis value.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Context IDs\nDESCRIPTION: Example showing how to use the new context ID property to maintain references between different crawling contexts, useful for manipulating data across multiple page handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with HttpCrawler in TypeScript\nDESCRIPTION: This code shows how to use ProxyConfiguration with HttpCrawler, including proxy setup and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, session }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Integrating AWS Chromium with Crawlee\nDESCRIPTION: Implementation showing how to configure PlaywrightCrawler with AWS-specific Chromium settings including executable path and launch arguments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for TypeScript project with multi-stage build\nDESCRIPTION: Dockerfile configuration using multi-stage build to first compile TypeScript code with development dependencies and then create a lean production image containing only the compiled code and production dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Setting minConcurrency and maxConcurrency in Crawlee Crawler\nDESCRIPTION: Controls the minimum and maximum number of parallel requests a crawler can make. The crawler automatically scales between these values based on available system resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Start with at least 10 requests in parallel\n    minConcurrency: 10,\n    // Allow up to 100 requests in parallel\n    maxConcurrency: 100,\n    // Rest of the crawler options\n    async requestHandler({ request, $ }) {\n        log.info(`Processing ${request.url}`);\n        // Rest of the handler code\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests to Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the new addRequests method, which handles large numbers of requests efficiently.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, puppeteer is no longer bundled with the SDK and must be installed separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Basic enqueueLinks Usage in Crawlee\nDESCRIPTION: Basic example showing the simplest form of the enqueueLinks() function that finds all links on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in TypeScript Crawler\nDESCRIPTION: Demonstrates how to enqueue links using globs pattern matching in a PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Managing Proxy Sessions with HttpCrawler in TypeScript\nDESCRIPTION: This snippet illustrates how to use proxy sessions with HttpCrawler for consistent IP addresses across multiple requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ session }) {\n        const proxyUrl = await proxyConfiguration.newUrl(session.id);\n        console.log(proxyUrl);\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Path in Crawlee\nDESCRIPTION: This snippet shows the file path where dataset items are stored in the Crawlee project structure. Each item is saved to its own file in the default dataset directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production environment\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee\nDESCRIPTION: Shows the difference in event handling between Apify SDK v2 and Crawlee. Events are now managed by EventManager class instead of the previous EventEmitter instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_15\n\nLANGUAGE: diff\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Functions for Puppeteer and Playwright in JavaScript\nDESCRIPTION: This snippet shows how to use the updated launch functions for both Puppeteer and Playwright. It demonstrates the new structure of launch options and how to specify custom modules for launching browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Locating Saved Dataset Files (Bash)\nDESCRIPTION: This bash snippet shows the directory path where each item in the default dataset will be saved as individual files. The path is relative to the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Scraping Implementation\nDESCRIPTION: Complete code showing how to scrape all product information including URL, manufacturer, title, SKU, price and stock status.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels in Crawler (TypeScript)\nDESCRIPTION: Demonstrates the use of Request.label shortcut for labeling requests and using it in enqueueLinks options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing Link Selection in enqueueLinks\nDESCRIPTION: Example of overriding the default link selector in the enqueueLinks function to target specific elements containing links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with HttpCrawler\nDESCRIPTION: Demonstrates how to integrate a proxy configuration with HttpCrawler to route all crawler requests through proxies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json }) {\n        // Process the JSON data extracted from the site\n        // ...\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Proxy Usage with ProxyConfiguration in Crawlee\nDESCRIPTION: Demonstrates how to create a new ProxyConfiguration instance with custom proxy URLs and how to obtain a proxy URL for use in requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Installing and Compressing Chromium Dependencies for AWS Lambda\nDESCRIPTION: This bash script installs the @sparticuz/chromium package and compresses the node_modules folder into a zip file that can be uploaded as a Lambda Layer. The package provides Chromium binaries compatible with Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright-Extra Dependencies\nDESCRIPTION: Command to install playwright-extra and puppeteer stealth plugin using npm package manager\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Handling Company Data with Crawlee for Python\nDESCRIPTION: This snippet defines a company_handler function that processes company data from Crunchbase. It extracts information such as company name, description, website, and location using JMESPath queries on JSON data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('company')\nasync def company_handler(context: ParselCrawlingContext) -> None:\n    \"\"\"Company request handler.\"\"\"\n    context.log.info(f'company_handler processing {context.request.url} ...')\n\n    json_selector = context.selector.xpath('//*[@id=\"ng-state\"]/text()')\n\n    await context.push_data(\n        {\n            'Company Name': json_selector.jmespath('HttpState.*.data[].properties.identifier.value').get(),\n            'Short Description': json_selector.jmespath('HttpState.*.data[].properties.short_description').get(),\n            'Website': json_selector.jmespath('HttpState.*.data[].cards.company_about_fields2.website.value').get(),\n            'Location': '; '.join(\n                json_selector.jmespath(\n                    'HttpState.*.data[].cards.company_about_fields2.location_identifiers[].value'\n                ).getall()\n            ),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tiered Proxies in Crawlee\nDESCRIPTION: Shows how to configure tiered proxy URLs that automatically switch based on blocking behavior, starting with no proxy and escalating to more powerful proxies as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    tieredProxyUrls: [\n        [null], // At first, we try to connect without a proxy\n        ['http://okay-proxy.com'],\n        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],\n        ['http://very-good-and-expensive-proxy.com'],\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue in Crawlee\nDESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL request to it. Shows the basic setup for managing crawling requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Using Crawler's Logging in Request Handler\nDESCRIPTION: Demonstrates how to use the scoped log instance provided in the crawling context. This logger prefixes messages with the crawler name and is the preferred approach for logging inside request handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda Layer deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Adding Production Script to package.json\nDESCRIPTION: Script configuration for running the compiled JavaScript code in production using Node.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Request Labeling in Crawler\nDESCRIPTION: Example of using Request.label shortcut for labeling requests in the enqueueLinks helper.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests in PlaywrightCrawler\nDESCRIPTION: This example demonstrates how to skip browser navigation for image requests while still fetching them using sendRequest. The code sets up a PlaywrightCrawler that processes regular pages normally but handles image URLs differently by setting skipNavigation to true and using the sendRequest method to fetch and save them directly to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\nawait PlaywrightCrawler.run({\n    startUrls: [\n        { url: 'https://crawlee.dev' },\n        // We want to save this image,\n        // but we don't need to load it in the browser.\n        {\n            url: 'https://cdn.crawlee.dev/icon.png',\n            skipNavigation: true,\n        },\n    ],\n    async requestHandler({ request, sendRequest, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // If we're supposed to skip navigation, we'll use our own handling\n        if (request.skipNavigation) {\n            log.info('Skipping navigation, will handle manually');\n\n            // Is this an image we want to save?\n            if (request.url.endsWith('.png')) {\n                const response = await sendRequest({\n                    url: request.url,\n                    responseType: 'buffer',\n                });\n\n                // Save the image to the default key-value store\n                await KeyValueStore.getDefaultInstance().setValue(\n                    `image-${Math.random().toString().slice(2, 8)}`,\n                    response.body,\n                    { contentType: 'image/png' },\n                );\n            }\n\n            return;\n        }\n\n        // Otherwise, we'll just log the HTML title\n        const title = await request.page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n    },\n});\n\nlog.info('Crawler finished.');\n\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options in Apify SDK for JavaScript\nDESCRIPTION: Shows the transition from launchPuppeteerOptions to launchContext in Apify's PuppeteerCrawler. This change separates Apify-specific options from Puppeteer options, improving clarity and reducing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with WebKit Docker Image\nDESCRIPTION: Docker configuration for using Playwright with WebKit browser, supporting CheerioCrawler and PlaywrightCrawler with WebKit-specific capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:20\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Groups and Country Code\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection for better performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links Strategy Implementation\nDESCRIPTION: Example showing how to crawl links from the same domain including subdomains using CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add links from same domain (including subdomains) to queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using ProxyConfiguration in a CheerioCrawler\nDESCRIPTION: Shows how to integrate a ProxyConfiguration instance with a CheerioCrawler. The crawler will use the provided proxy configuration when making requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* options */ });\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Example of specifying Node.js version 20 when using the base Apify actor-node Docker image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 20\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler for Basic HTML Scraping\nDESCRIPTION: This snippet demonstrates attempting to use CheerioCrawler to extract content from a JavaScript-rendered page, which will fail because CheerioCrawler doesn't execute JavaScript when processing the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Example of Compressed Response Output\nDESCRIPTION: This bash snippet shows the garbled output when a compressed response is received without the proper decompression library installed. It displays the first 10 bytes of a Brotli-compressed response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nb'\\x83\\x0c\\x00\\x00\\xc4\\r\\x8e4\\x82\\x8a'\n```\n\n----------------------------------------\n\nTITLE: Using Browser Lifecycle Hooks with BrowserPool\nDESCRIPTION: Example demonstrating how to use BrowserPool's lifecycle hooks through browserPoolOptions to customize browser behavior, such as switching between headless and headful mode based on request data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Basic example of using the context-aware sendRequest() function with a BasicCrawler to make HTTP requests. This function uses got-scraping under the hood to mimic browser requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Patterns using PlaywrightCrawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs. This method allows filtering URLs based on specified patterns without needing to pass requestQueue or page arguments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-stage Build Configuration\nDESCRIPTION: Dockerfile setup for building and running TypeScript projects in production using multi-stage builds.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Autoscaling Pool Options in Crawlee\nDESCRIPTION: Example of setting detailed autoscaling pool parameters for fine-tuned crawler performance\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.95,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 0.5,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 120\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Advanced Autoscaling Pool Configuration\nDESCRIPTION: Shows how to configure advanced autoscaling pool options for fine-tuned control over crawler scaling behavior and performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 5,\n        desiredConcurrencyRatio: 0.95,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 0.5,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 120,\n    },\n    async requestHandler({ $ }) {\n        // Process the data here...\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest Helper with BasicCrawler\nDESCRIPTION: Shows how to use the context.sendRequest() helper function which provides a convenient way to process requests through got-scraping. This replaces the deprecated requestAsBrowser method with a more direct approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests to Crawler (TypeScript)\nDESCRIPTION: Shows how to add multiple requests to a crawler using the addRequests method, which handles batching and allows for immediate crawling start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, which enables Firefox and Webkit browser automation in addition to Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'all' enqueue strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // the properties below are just examples,\n            // and are not necessary.\n            transformRequestFunction: (req) => {\n                req.userData.anyValue = 123;\n                return req;\n            },\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Node with Puppeteer and Chrome\nDESCRIPTION: Example of using the Docker image with Puppeteer and Chrome pre-installed, suitable for CheerioCrawler and PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests in Batches in TypeScript\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method. This technique efficiently handles large numbers of requests by adding them in smaller batches to avoid API rate limits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Parsing HTTP Responses in CustomCrawler\nDESCRIPTION: Method to parse HTTP responses in the CustomCrawler class. Handles HTML responses to extract build ID for Next.js or processes JSON responses using orjson. Also implements custom enqueue_links function for URL generation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    async def _parse_http_response(self, context: HttpCrawlingContext) -> AsyncGenerator[CustomContext, None]:\n\n        page_data = None\n\n        if context.http_response.headers['content-type'] == 'text/html; charset=utf-8':\n            # Get Build ID for Next js from the start page of the site, form a link to next.js endpoints\n            build_id = search(rb'\"buildId\":\"(.{21})\"', context.http_response.read()).group(1)\n            self._build_id = build_id.decode('UTF-8')\n            self._base_url = self._base_url.format(build_id=self._build_id)\n        else:\n            # Convert json to python dictionary\n            page_data = context.http_response.read()\n            page_data = page_data.decode('ISO-8859-1').encode('utf-8')\n            page_data = loads(page_data)\n\n        async def enqueue_links(\n            *, path_template: str, items: list[str], user_data: dict[str, Any] | None = None, label: str | None = None\n        ) -> None:\n\n            requests = list[Request]()\n            user_data = user_data if user_data else {}\n\n            for item in items:\n                link_user_data = user_data.copy()\n\n                if label is not None:\n                    link_user_data.setdefault('label', label)\n\n                if link_user_data.get('label') == 'SEARCH':\n                    link_user_data['location'] = item\n\n                url = self._base_url + path_template.format(item=item, **user_data)\n                requests.append(Request.from_url(url, user_data=link_user_data))\n\n            await context.add_requests(requests)\n\n        yield CustomContext(\n            request=context.request,\n            session=context.session,\n            proxy_info=context.proxy_info,\n            enqueue_links=enqueue_links,\n            add_requests=context.add_requests,\n            send_request=context.send_request,\n            push_data=context.push_data,\n            log=context.log,\n            http_response=context.http_response,\n            page_data=page_data,\n        )\n```\n\n----------------------------------------\n\nTITLE: Map Method Result Format in Crawlee Dataset\nDESCRIPTION: The expected result of the map method demonstration, showing an array of heading counts greater than 5. This result would be saved to the key-value store after processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript for Crawlee Project\nDESCRIPTION: Set up the tsconfig.json file for a Crawlee project, extending @apify/tsconfig and configuring for ES2022 modules and top-level await support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using saveSnapshot() utility\nDESCRIPTION: This example uses PuppeteerCrawler with the context-aware saveSnapshot() utility to capture screenshots. The crawler handles multiple URLs and automatically saves screenshots to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Create the crawler\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, crawler }) {\n        // Navigate to the URL.\n        await page.goto(request.url);\n\n        // Get a unique key for the screenshot from the URL.\n        const key = new URL(request.url).hostname;\n\n        // Capture and save the screenshot using the utility.\n        await crawler.utils.saveSnapshot(page, { key });\n\n        console.log(`Screenshot of ${request.url} saved!`);\n    },\n});\n\n// Add requests to the queue\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://apify.com' },\n]);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: New Browser Pool Hooks in JavaScript\nDESCRIPTION: Demonstrates the new pattern using browser-pool lifecycle hooks for customizing browser launch behavior with improved flexibility and access to context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Sitemap Handler for Gzipped Files\nDESCRIPTION: Handler that processes gzip-compressed sitemap files, decompresses them, and extracts company URLs for further processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# routes.py\n\nfrom gzip import decompress\nfrom parsel import Selector\n\n\n@router.handler('sitemap')\nasync def sitemap_handler(context: ParselCrawlingContext) -> None:\n    \"\"\"Sitemap gzip request handler.\"\"\"\n    context.log.info(f'sitemap_handler processing {context.request.url} ...')\n\n    data = context.http_response.read()\n    data = decompress(data)\n\n    selector = Selector(data.decode())\n\n    requests = [Request.from_url(url, label='company') for url in selector.xpath('//loc/text()').getall()]\n\n    await context.add_requests(requests)\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Status\nDESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of specific elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction\nDESCRIPTION: Combined implementation showing all the scraping logic together to extract product information including URL, manufacturer, title, SKU, price and stock status.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1);\nconst manufacturer = urlPart.split('-')[0];\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use a proxy server with BasicCrawler by passing the proxyUrl option to sendRequest. This is useful for hiding the real IP address during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using 'All' Strategy with enqueueLinks for Unrestricted Crawling\nDESCRIPTION: Setting the enqueueLinks strategy to 'all' to follow all links regardless of domain. This configuration allows the crawler to follow links to any domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with JSDOMCrawler\nDESCRIPTION: Shows how to set up proxy configuration with JSDOMCrawler. The crawler automatically uses the configured proxies for all connections and rotates them as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window}) {\n        console.log(`Fetched ${request.url} with title: ${window.document.title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler. This snippet shows the basic setup for sending a request and logging the response body.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Data Return\nDESCRIPTION: The complete Lambda handler implementation that initializes a CheerioCrawler with in-memory storage, runs the crawler, and returns the scraped data in the Lambda response. This ensures statelessness and proper data handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee Project to Apify Platform\nDESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, which archives the project, uploads it, and initiates a Docker build on the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Using Request List with PuppeteerCrawler in Crawlee\nDESCRIPTION: Shows how to create and use a request list with a PuppeteerCrawler in Crawlee. This is useful for crawling a predefined list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-puppeteer-chrome Docker Image\nDESCRIPTION: Shows how to use the Apify Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Required Dependencies\nDESCRIPTION: Command to add Crawlee with parsel and curl-impersonate dependencies to the Poetry project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry add crawlee[parsel,curl-impersonate]\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Proxy List with Null Option in JavaScript\nDESCRIPTION: This example shows how to create a proxy configuration with a mix of proxy URLs and null values. Null values indicate that no proxy should be used for some requests, implementing a rotation strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n        null // null means no proxy is used\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to set up and execute a recursive website crawl using PuppeteerCrawler from the Crawlee library. It processes the initial URL, extracts links from the page, and follows them recursively while collecting data from each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // A function to be evaluated by Puppeteer within the browser context.\n        const data = await page.$$eval('.athing', ($posts) => {\n            const scrapedData = [];\n            // We're getting the title, rank and URL of each post on Hacker News.\n            for (const $post of $posts) {\n                const title = $post.querySelector('.title a').innerText;\n                const rank = $post.querySelector('.rank').innerText;\n                const itemId = $post.id;\n\n                scrapedData.push({\n                    title,\n                    rank,\n                    href: `https://news.ycombinator.com/item?id=${itemId}`,\n                });\n            }\n            return scrapedData;\n        });\n\n        // Store the results to the default dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData(data);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // Let's limit our crawls to make the example quick and safe.\n    maxRequestsPerCrawl: 10,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Node.js Version Specific Configuration\nDESCRIPTION: Example of specifying Node.js version in Docker configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 16\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with BasicCrawler and sendRequest\nDESCRIPTION: Demonstrates how to use a proxy server with the sendRequest function in BasicCrawler. The example shows how to pass the proxyUrl option to hide the real IP address when scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Apify Proxy Configuration in Crawlee\nDESCRIPTION: This code demonstrates how to initialize and use Apify Proxy within Crawlee. It creates a proxy configuration using Actor.createProxyConfiguration() and generates a new proxy URL that can be used for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Using Platform Storage in Local Actor\nDESCRIPTION: JavaScript code demonstrating how to use Apify platform storage when developing and running an actor locally, including creating a public URL for a stored item.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Configuration for AWS Lambda\nDESCRIPTION: Initial code modification to make a Cheerio Crawler work on AWS Lambda, using a unique Configuration instance with in-memory storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in JSDOMCrawler with TypeScript\nDESCRIPTION: This snippet illustrates how to access and log the current proxy information within the JSDOMCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo }) {\n        if (proxyInfo) {\n            console.log(`Proxy URL: ${proxyInfo.url}`);\n        }\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Node.js Docker Image for Crawlee\nDESCRIPTION: Dockerfile configuration for using Apify's Node.js Docker image. This is the smallest image based on Alpine Linux, ideal for CheerioCrawler but does not support browser-based crawlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Basic URL Manipulation Example\nDESCRIPTION: Example of pagination URL format for navigating product pages in the warehouse store\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://warehouse-theme-metal.myshopify.com/collections/headphones?page=2\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly clean up the default storage directories using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Configuring headerGeneratorOptions with sendRequest\nDESCRIPTION: Example of using headerGeneratorOptions to customize the browser fingerprint generation. This allows control over which device types, locales, operating systems, and browsers are mimicked in requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in SDK v1 Crawlers\nDESCRIPTION: Example showing how to access the BrowserPool instance that replaced PuppeteerPool in SDK v1, available through the crawler object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee\nDESCRIPTION: Shows how to migrate from Apify.events (an EventEmitter instance) to the new Actor.on method for event handling in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-stage Build Configuration\nDESCRIPTION: Dockerfile configuration using multi-stage build for optimal production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Name and Price\nDESCRIPTION: Python function to handle requests, extract product name and price from the webpage, and store the data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n@crawler.router.default_handler\nasync def request_handler(context: BeautifulSoupCrawlingContext) -> None:\n    url = context.request.url\n    Actor.log.info(f'Scraping {url}...')\n\n    # Select the product name and price elements.\n    product_name_element = context.soup.find('div', class_='productname')\n    product_price_element = context.soup.find('span', id='product-price-395001')\n\n    # Extract the desired data.\n    data = {\n        'url': context.request.url,       \n        'product_name': product_name_element.text.strip() if product_name_element else None,\n        'price': float(product_price_element['data-price-amount']) if product_price_element else None,\n    }\n\n    # Store the extracted data to the default dataset.\n    await context.push_data(data)\n```\n\n----------------------------------------\n\nTITLE: SendRequest API Implementation in Crawlee\nDESCRIPTION: Shows the internal implementation of the sendRequest function which uses got-scraping under the hood. The function accepts override options and combines them with default values derived from the request context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.puppeteer.saveSnapshot() utility. It creates a PuppeteerCrawler instance, defines a handler function to save snapshots, and starts the crawler with a list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, log, puppeteerUtils }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await puppeteerUtils.saveSnapshot({\n            key: `screenshot-${request.id}`,\n            saveHtml: false,\n        });\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with Lifecycle Hooks in JavaScript\nDESCRIPTION: Illustrates the transition from using a custom launchPuppeteerFunction to utilizing browser-pool's lifecycle hooks, specifically preLaunchHooks. This change provides more flexibility and consistency across Puppeteer and Playwright implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Playwright Crawler in Crawlee\nDESCRIPTION: This code illustrates how to use Playwright Crawler to extract and crawl all links from a website. It creates a RequestQueue, initializes a PlaywrightCrawler, and uses enqueueLinks() to add new links to the queue as it browses the site. Note that this requires the apify/actor-node-playwright-chrome image on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function for Cheerio Crawler\nDESCRIPTION: Wraps the Cheerio crawler initialization and execution in an AWS Lambda handler function to make it executable in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route for Parallel Scraping in JavaScript\nDESCRIPTION: This code modifies the DETAIL route handler to send scraped data back to the parent process instead of using the crawler's built-in data storage. This adaptation is necessary for parallel scraping with child processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nDETAIL: async ({ $, request, log }) => {\n    const { url } = request;\n    log.info(`Product page opened: ${url}`);\n    const title = $('h1').text().trim();\n    const price = $('p.price_color').text().trim();\n    const inStock = $('p.instock').length > 0;\n    const data = {\n        url,\n        title,\n        price,\n        inStock,\n    };\n    if (process.send) {\n        process.send(data);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Map Method Result Example\nDESCRIPTION: Example output showing the result of mapping operation that filters header counts greater than 5.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Command to install ts-node for running TypeScript files directly during development.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue Only for Batch Operations in Crawlee\nDESCRIPTION: Example of using the Request Queue's addRequests method to add multiple requests at once, eliminating the need to combine RequestQueue with RequestList for batch operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, RequestQueue } from 'crawlee';\n\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Initialize the request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add initial requests to the queue in a batch\nawait requestQueue.addRequests(sources);\n\nconst crawler = new PlaywrightCrawler({\n    requestQueue,\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract links from the page and add them to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: BrowserPool Methods Compared to PuppeteerPool\nDESCRIPTION: Compares methods between the old PuppeteerPool and new BrowserPool, showing how functionality has been updated or replaced in the new implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry for Python Project Management\nDESCRIPTION: Command to install Poetry using pipx for managing the Python project dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install poetry\n```\n\n----------------------------------------\n\nTITLE: Setting Min and Max Concurrency in CheerioCrawler\nDESCRIPTION: Configures a CheerioCrawler with minimum and maximum concurrency limits, ensuring the crawler starts with at least 5 parallel requests and scales up to a maximum of 100 concurrent requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Start with 5 parallel requests\n    minConcurrency: 5,\n    // Scale up to a maximum of 100 parallel requests\n    maxConcurrency: 100,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using the New Navigation Hooks Pattern in JavaScript\nDESCRIPTION: Example of the improved approach using preNavigationHooks and postNavigationHooks arrays. This simplifies the code by separating concerns and removing the need to manually call gotoExtended or return responses.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tiered Proxy Configuration in JavaScript\nDESCRIPTION: This code shows how to configure tiered proxies that automatically switch between different proxy tiers based on blocking behavior. It starts with no proxy and escalates to more powerful proxies if needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    tieredProxyUrls: [\n        [null], // At first, we try to connect without a proxy\n        ['http://okay-proxy.com'],\n        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],\n        ['http://very-good-and-expensive-proxy.com'],\n    ]\n});\n```\n\n----------------------------------------\n\nTITLE: Explicitly Configuring Request Queue with a Crawler\nDESCRIPTION: Shows how to explicitly create and configure a request queue to use with a crawler instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Open a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Add more requests to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add the initial requests and start the crawl\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n]);\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File in Crawlee\nDESCRIPTION: This code demonstrates how to use the exportToValue function to export the entire default dataset to a single CSV file and store it in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nawait Dataset.exportToValue({\n    format: 'csv',\n    toValue: async (data) => {\n        await KeyValueStore.setValue('OUTPUT.csv', data);\n    },\n    transformation: {\n        fields: [\n            { name: 'title', value: (item) => item.title, },\n            { name: 'url', value: (item) => item.url, },\n        ],\n    },\n});\n\n```\n\n----------------------------------------\n\nTITLE: Accessing ProxyInfo in PuppeteerCrawler\nDESCRIPTION: Example showing how to access proxy information in PuppeteerCrawler's requestHandler using the proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionPuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image Tags\nDESCRIPTION: Examples showing how to use pre-release (beta) versions of Apify Docker images, both with and without specific library versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:20-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:20-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Crawlee Execution Log Output\nDESCRIPTION: Example log output showing crawler execution and page titles being scraped\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in TypeScript\nDESCRIPTION: Demonstrates how to use Cheerio Crawler to crawl a specific list of URLs. This example shows setting up the crawler, handling the crawling process, and adding results to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Define the list of URLs that we want to crawl.\nconst startUrls = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n];\n\n// Create an instance of the CheerioCrawler class - a crawler\n// that automatically loads the URLs and parses their HTML using the cheerio library.\nconst crawler = new CheerioCrawler({\n    // The crawler downloads and processes the web pages in parallel, with a concurrency\n    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).\n    // Here we define some hard limits for the concurrency.\n    maxRequestsPerMinute: 120,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Puppeteer scripts you are familiar with,\n    // with the exception that browsers and pages are automatically managed by the Apify SDK.\n    // The function accepts a single parameter, which is an object with the following fields:\n    // - request: an instance of the Request class with information such as URL and HTTP method\n    // - $: the cheerio object containing parsed HTML\n    async requestHandler({ request, $ }) {\n        console.log(`Processing ${request.url}...`);\n\n        // Extract data from the page using cheerio.\n        const title = $('title').text();\n        const h1 = $('h1').text();\n\n        // Store the results to the dataset. In local configuration,\n        // the data will be stored as JSON files in ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n});\n\n// Run the crawler and wait for it to finish.\nawait crawler.run(startUrls);\n\nconsole.log('Crawler finished.');\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Handler for Bluesky API Crawling in Python\nDESCRIPTION: This method handles search responses from the Bluesky API. It processes post data, creates requests for user profiles, and handles pagination. The handler saves posts to a dataset and adds requests for user data and next pages to the crawler queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def _search_handler(self, context: HttpCrawlingContext) -> None:\n    context.log.info(f'Processing search {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    if 'posts' not in data:\n        context.log.warning(f'No posts found in response: {context.request.url}')\n        return\n\n    user_requests = {}\n    posts = []\n\n    profile_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.actor.getProfile')\n\n    for post in data['posts']:\n        # Add user request if not already added in current context\n        if post['author']['did'] not in user_requests:\n            user_requests[post['author']['did']] = Request.from_url(\n                url=str(profile_url.with_query(actor=post['author']['did'])),\n                user_data={'label': 'user'},\n            )\n\n        posts.append(\n            {\n                'uri': post['uri'],\n                'cid': post['cid'],\n                'author_did': post['author']['did'],\n                'created': post['record']['createdAt'],\n                'indexed': post['indexedAt'],\n                'reply_count': post['replyCount'],\n                'repost_count': post['repostCount'],\n                'like_count': post['likeCount'],\n                'quote_count': post['quoteCount'],\n                'text': post['record']['text'],\n                'langs': '; '.join(post['record'].get('langs', [])),\n                'reply_parent': post['record'].get('reply', {}).get('parent', {}).get('uri'),\n                'reply_root': post['record'].get('reply', {}).get('root', {}).get('uri'),\n            }\n        )\n\n    await self._posts.push_data(posts)  # Push a batch of posts to the dataset\n    await context.add_requests(list(user_requests.values()))\n\n    if cursor := data.get('cursor'):\n        next_url = URL(context.request.url).update_query({'cursor': cursor})  # Use yarl for update the query string\n\n        await context.add_requests([str(next_url)])\n```\n\n----------------------------------------\n\nTITLE: Importing Crawlers in Crawlee for Python v0.5\nDESCRIPTION: Example showing the new import structure for crawler classes in Crawlee for Python v0.5, demonstrating the consolidated package structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n```\n\n----------------------------------------\n\nTITLE: Using cURL to Test Website Protection\nDESCRIPTION: This bash command uses cURL to test if a website is protected by Cloudflare, showing how even command-line tools get blocked with a 403 response when standard headers are used.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XGET -H 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\"' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8' -H 'Accept-Language: en-US,en;q=0.5' -H 'Connection: keep-alive' 'https://www.g2.com/' -s -o /dev/null -w \"%{http_code}\\n\"\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { launchPuppeteer } from 'crawlee';\n\nawait Actor.init();\n\nconst url = 'https://example.com';\nconst browser = await launchPuppeteer();\nconst page = await browser.newPage();\nawait page.goto(url);\n\nconst { utils } = Actor;\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Example for Pagination\nDESCRIPTION: Example URL structure showing how pagination is implemented on the target website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://warehouse-theme-metal.myshopify.com/collections/headphones?page=2\n```\n\n----------------------------------------\n\nTITLE: Using Node.js Base Image\nDESCRIPTION: Using the minimal Alpine Linux-based Node.js image without browsers, best suited for CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Accessing ProxyInfo in JSDOMCrawler\nDESCRIPTION: Example showing how to access proxy information in JSDOMCrawler's requestHandler using the proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionJSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Using PlaywrightCrawler to scrape Hacker News\nDESCRIPTION: This example demonstrates how to use PlaywrightCrawler in combination with RequestQueue to recursively crawl the Hacker News website. The crawler starts with the homepage URL, identifies and follows pagination links, and extracts data from each page into the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, enqueueLinks } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API.\n        const results = await page.$$eval('.athing', ($posts) => {\n            return $posts.map($post => {\n                const titleElement = $post.querySelector('.title a');\n                const rank = $post.querySelector('.rank')?.textContent;\n                const score = $post.nextElementSibling?.querySelector('.score')?.textContent;\n\n                return {\n                    title: titleElement?.textContent ?? null,\n                    rank: rank ? parseInt(rank.replace('.', ''), 10) : null,\n                    href: titleElement?.getAttribute('href') ?? null,\n                    score: score ? parseInt(score, 10) : null,\n                };\n            });\n        });\n\n        // Save the data to dataset.\n        await Dataset.pushData(results);\n\n        // Find a link to the next page and enqueue it if it exists.\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://news.ycombinator.com/']);\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with BasicCrawler\nDESCRIPTION: Shows how to use a proxy server with BasicCrawler by passing the proxyUrl option to sendRequest.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Browser DOM Manipulation with Cheerio in TypeScript\nDESCRIPTION: This code snippet demonstrates the equivalent syntax between plain JavaScript DOM methods and Cheerio methods for extracting text content from title elements and collecting href attributes from a page. It shows how Cheerio's jQuery-like syntax works in Node.js environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Actor Locally with Apify CLI\nDESCRIPTION: Commands to create a new actor project using the Apify CLI and run it locally. It creates a \"Hello world\" boilerplate project and executes it.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler\nDESCRIPTION: Example showing how to crawl any URLs found on a website using the 'All' strategy, which matches any URLs regardless of their domain. Uses CheerioCrawler to process HTML content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add all links from page to the queue, regardless of their domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox browser in Typescript\nDESCRIPTION: This example shows how to set up a PlaywrightCrawler that uses Firefox browser instead of the default Chromium. It configures the crawler to visit example.com and extract the page title using Firefox-specific browser settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { firefox } from 'playwright';\n\n// This example demonstrates how to use PlaywrightCrawler with Firefox browser.\nconst crawler = new PlaywrightCrawler({\n    // Use the firefox browser\n    browserType: firefox,\n    // Instead of using the browser: firefox option, it's enough to just set the browserType option since it will default to headless\n    // You can use browser to customize the Firefox browser instance\n    // By default, the browser instances are automatically launched (and managed) by the crawler\n    // If you prefer to manage your own browser instance, use the launchContext.launcher option\n\n    // Let's limit our crawls to make the run quick\n    maxRequestsPerCrawl: 10,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://example.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform Using CLI\nDESCRIPTION: Command for installing and logging into the Apify platform using the Apify CLI, which allows automatic credential usage in your scrapers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Getting Page ID from BrowserPool\nDESCRIPTION: Shows how to obtain a page ID from the BrowserPool, which corresponds to the crawling context ID and provides access to the full context in hooks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst pageId = browserPool.getPageId\n```\n\n----------------------------------------\n\nTITLE: Sanity Check Using PlaywrightCrawler\nDESCRIPTION: Initial setup code for verifying the scraper's ability to access and parse category data from the target website using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Navigation Hooks in JavaScript\nDESCRIPTION: This snippet demonstrates how to replace the deprecated gotoFunction with preNavigationHooks and postNavigationHooks in Apify SDK v3. It simplifies page navigation customization and improves code readability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Updating Crawler Logic to Store Responses in SuperScraper with TypeScript\nDESCRIPTION: This code snippet shows how SuperScraper updates its crawler logic to store response objects. It fetches or creates a crawler for the given proxy configuration, stores the response object, and adds the request to the crawler's queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst key = JSON.stringify(crawlerOptions);\nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\n\naddResponse(request.uniqueKey!, res);\n\nawait crawler.requestQueue!.addRequest(request);\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub CI Workflow for Automated Testing\nDESCRIPTION: This workflow runs tests on Crawlee across multiple Node.js versions. It sets up the testing environment, installs dependencies, runs tests including coverage, and uploads coverage reports to Codecov.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: CI\n\non:\n    push:\n        branches: [master, renovate/**]\n    pull_request:\n\njobs:\n    test:\n        runs-on: ${{ matrix.os }}\n\n        strategy:\n            fail-fast: false\n            matrix:\n                os: [ubuntu-latest, windows-latest]\n                node-version: [16, 18, 20]\n\n        steps:\n            - uses: actions/checkout@v3\n            - name: Use Node.js ${{ matrix.node-version }}\n              uses: actions/setup-node@v3\n              with:\n                  node-version: ${{ matrix.node-version }}\n                  cache: 'npm'\n            - run: npm ci\n            - run: npm run build\n            - run: npm test\n\n    coverage:\n        runs-on: ubuntu-latest\n        steps:\n            - uses: actions/checkout@v3\n            - name: Use Node.js\n              uses: actions/setup-node@v3\n              with:\n                  node-version: 16\n                  cache: 'npm'\n            - run: npm ci\n            - run: npm run test:coverage\n            - uses: codecov/codecov-action@v3\n```\n\n----------------------------------------\n\nTITLE: Using tls-client for Web Scraping with TLS Fingerprinting Bypass in Python\nDESCRIPTION: This code demonstrates how to use the tls-client library to make a GET request to a website while bypassing TLS fingerprinting. It sets up a session with a specific client identifier and custom headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tls_client import Session\n\nclient = Session(client_identifier=\"firefox_120\")\n\nurl = 'https://www.g2.com/'\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n    \"Connection\": \"keep-alive\",\n}\n\nresponse = client.get(url, headers=headers)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Using transformRequestFunction to Control URL Enqueuing in Crawlee\nDESCRIPTION: Demonstrates how to use the transformRequestFunction to have fine-grained control over which URLs are enqueued, in this case ignoring PDF files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Storage for Crawlee\nDESCRIPTION: Code example showing how to use a custom storage implementation (@apify/storage-local) with Actor initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with Lifecycle Hooks in JavaScript\nDESCRIPTION: Illustrates the transition from using a custom launchPuppeteerFunction to using browser-pool lifecycle hooks in Apify SDK. This change provides more flexibility and consistency across Puppeteer and Playwright implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Playwright Crawler in TypeScript\nDESCRIPTION: A code snippet demonstrating a Playwright-based crawler implementation for crawling all links on a website. This example requires the apify/actor-node-playwright-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Configuration in GCP Cloud Run\nDESCRIPTION: Basic setup for a Crawlee crawler with a Configuration instance where persistent storage is disabled to accommodate the serverless environment of Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route Handler for Inter-Process Communication in JavaScript\nDESCRIPTION: This code updates the DETAIL route handler to use process.send for sending scraped data back to the parent process instead of using context.pushData. This is necessary for communication between child processes and the parent process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nDETAIL: async ({ $, request }) => {\n    const title = $('h1[itemprop=\"name\"]').text().trim();\n    const sku = $('span[itemprop=\"sku\"]').text().trim();\n    const price = $('span[itemprop=\"price\"]').attr('content');\n\n    const result = {\n        url: request.loadedUrl,\n        title,\n        sku,\n        price: parseFloat(price),\n    };\n\n    if (process && process.send) {\n        process.send(result);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Routing Logic for Crawlee Web Scraper (JavaScript)\nDESCRIPTION: Demonstrates how to use Crawlee's router to structure page handling logic. This implementation includes routes for detail pages, category pages, and a default handler, replacing complex if-else statements with a more modular approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-stage Docker Build for Crawlee Actor\nDESCRIPTION: Complete Dockerfile that sets up a multi-stage build process for a Crawlee-based actor. It handles dependency management, builds the TypeScript code, and configures the production environment with minimal dependencies to keep the image small and efficient.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue for Batch Processing URLs in Crawlee\nDESCRIPTION: Demonstrates how to use Request Queue with its addRequests method to efficiently add multiple URLs in a single batch operation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Prepare a batch of URLs\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open a request queue and add all the requests at once\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequests(sources);\n\n// The crawler will automatically process requests from the queue\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page (extract data, take page screenshot, etc).\n\n        // We can also add more links to the queue as we find them\n        await enqueueLinks();\n    },\n});\n\n// Start the crawl\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, enabling automation of Firefox and Webkit browsers in addition to Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() Explicitly in TypeScript\nDESCRIPTION: Demonstrates the explicit initialization and exit pattern using Actor from the apify package. This approach requires manually calling init() at the beginning and exit() at the end of your code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Error Output from CheerioCrawler\nDESCRIPTION: The console output from running the CheerioCrawler code, showing that it fails to extract any content from the actor cards due to JavaScript rendering.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Launching Puppeteer and Playwright in Apify SDK v3 JavaScript\nDESCRIPTION: This snippet shows how to launch Puppeteer and Playwright browsers using the updated launch functions in Apify SDK v3. It demonstrates the new structure of launch options and how to use custom modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n\n// Using custom modules\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring JSDOMCrawler with ProxyConfiguration in TypeScript\nDESCRIPTION: This code demonstrates how to set up JSDOMCrawler with ProxyConfiguration for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, session }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Shows how to manually clean up the default request storages in a Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generator Options in TypeScript\nDESCRIPTION: This snippet demonstrates how to configure header generator options for sendRequest. It specifies devices, locales, operating systems, and browsers to use for generating browser-like headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Worker Processes\nDESCRIPTION: Sets up a parallel scraping system using Node.js child processes, managing worker threads and data collection from multiple scraper instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { fork } from 'child_process';\nimport { Configuration, CheerioCrawler } from 'crawlee';\nimport { getOrInitQueue } from './requestQueue.mjs';\nimport { router } from './routes.mjs';\n\nif (process.env.IS_WORKER_THREAD) {\n    Configuration.set('purgeOnStart', false);\n    const requestQueue = await getOrInitQueue(false);\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    const crawler = new CheerioCrawler({\n        requestQueue,\n        requestHandler: router,\n    });\n\n    await crawler.run();\n    process.exit(0);\n} else {\n    const workers = [];\n    const workerPromises = [];\n\n    for (let i = 0; i < 2; i++) {\n        const worker = fork(process.argv[1], [], {\n            env: {\n                ...process.env,\n                IS_WORKER_THREAD: '1',\n                WORKER_INDEX: String(i),\n            },\n        });\n\n        workers.push(worker);\n        workerPromises.push(\n            new Promise((resolve) => {\n                worker.on('message', (message) => {\n                    console.log('Received:', message);\n                });\n                worker.on('exit', resolve);\n            })\n        );\n    }\n\n    await Promise.all(workerPromises);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor\nDESCRIPTION: Dockerfile that sets up a container environment for running a Crawlee actor. It uses a base image with Node.js and Playwright, installs production dependencies, and copies the application code. The build process is optimized using Docker layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Actor Usage with Main Function in TypeScript\nDESCRIPTION: Demonstrates using Actor.main() as a wrapper function that handles initialization and cleanup automatically. This is the recommended way to handle Actor lifecycle.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Implementing PlaywrightCrawler for Headless Browser Scraping\nDESCRIPTION: Demonstrates how to replace CheerioCrawler with PlaywrightCrawler for more advanced scraping capabilities. Using a headless browser enables handling of JavaScript-heavy pages and better avoids blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({ requestHandler, proxyConfiguration });\n\n...\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-Stage Build for TypeScript Crawlee Projects\nDESCRIPTION: Dockerfile using multi-stage build to compile TypeScript code and create a minimal production image for Crawlee projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Element Wait Failure Log\nDESCRIPTION: Shows the error message when attempting to access elements without proper waiting mechanisms in headless browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Browser Pool Hooks Usage in JavaScript\nDESCRIPTION: Shows advanced usage of browser-pool hooks including composing multiple hooks and accessing crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with sendRequest in TypeScript\nDESCRIPTION: Example showing how to use a proxy server with sendRequest. The proxyUrl option allows specifying the proxy server in the format protocol://username:password@hostname:port.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Stage Dockerfile for Crawlee Actor with Node.js\nDESCRIPTION: This Dockerfile creates a multi-stage build for a Crawlee-based Actor. It first builds the application in a builder stage, then creates a minimal production image with only the necessary dependencies and compiled code. The approach optimizes build time through intelligent layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Crawlee Node.js Application\nDESCRIPTION: A Dockerfile that implements a multi-stage build process for a Crawlee application. It first builds the TypeScript/JavaScript project in a builder stage, then creates a production image with only the necessary files and dependencies to minimize image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Actor Card Content Output\nDESCRIPTION: Shows the successful output when using a headless browser to scrape JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Custom Storage Settings\nDESCRIPTION: Initial code setup showing how to initialize PlaywrightCrawler with a custom Configuration instance to prevent storage interference between Lambda instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK dependency for Node.js project\nDESCRIPTION: Command to install the Apify SDK package, which enables integration with the Apify Platform cloud services.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler with RequestQueue\nDESCRIPTION: This code shows how to create a CheerioCrawler that processes a website using a RequestQueue. It demonstrates setting up the crawler with a requestHandler that extracts the page title using Cheerio, a jQuery-like HTML parser.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright\nDESCRIPTION: This code checks if a product is in stock by looking for a specific element with the 'product-form__inventory' class that contains the text 'In stock'. It returns a boolean value based on whether the element exists.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Running an Actor Locally with Apify CLI\nDESCRIPTION: Commands to navigate to the actor directory and run it locally using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Category Links with Crawlee\nDESCRIPTION: This snippet shows how to use PlaywrightCrawler to crawl the main categories page of an online store. It uses enqueueLinks with a specific selector to add category links to the crawl queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management with ProxyConfiguration\nDESCRIPTION: Demonstrates how to manually manage sessions with ProxyConfiguration outside of a crawler to maintain consistent proxy-session pairings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration, SessionPool } from 'crawlee';\nimport got from 'got';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst sessionPool = new SessionPool({\n    // options go here\n});\n\nasync function crawl() {\n    // Get an available session\n    const session = await sessionPool.getSession();\n    // Get a proxy URL with the session ID\n    const proxyUrl = await proxyConfiguration.newUrl(session.id);\n\n    try {\n        // Make the HTTP request\n        const response = await got('https://crawlee.dev', { \n            proxy: proxyUrl,\n            headers: session.getCookieString('https://crawlee.dev'),\n        });\n        \n        // Store an identifier of the proxy used\n        session.userData.proxyUrl = proxyUrl;\n        \n        // Process the response\n        console.log(response.body);\n        \n        // Artificially reduce the session's life when it gets blocked\n        if (response.statusCode === 403) {\n            session.markBad();\n        }\n    } catch (e) {\n        // Handle got errors\n        session.markBad();\n        throw e;\n    } finally {\n        // Return the session to the pool\n        await sessionPool.returnSession(session);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: This snippet demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's chained methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options in Apify SDK v3 JavaScript\nDESCRIPTION: This code shows how to update the launch options in Apify SDK v3. It replaces the deprecated launchPuppeteerOptions with a new launchContext object, which clearly separates Apify-specific options from Puppeteer options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: New Pattern with Pre and Post Navigation Hooks in PuppeteerCrawler\nDESCRIPTION: Updated pattern using preNavigationHooks and postNavigationHooks which simplifies page navigation by separating pre-processing and post-processing into distinct hook arrays.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It configures logging and starts the crawler with a specific URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: Shows how to use KeyValueStore to get input, save output, create named stores, and manage data records. The code demonstrates reading and writing data, with automatic JSON conversion for JavaScript objects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Extracting and Formatting Product Price with Playwright\nDESCRIPTION: Shows how to locate, extract, and format the current price of a product by filtering elements with a '$' character and converting the string to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile for building and running Crawlee projects, optimizing for production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\nFROM apify/actor-node:16\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Old Puppeteer Launch Options Configuration\nDESCRIPTION: Example of the deprecated launchPuppeteerOptions pattern where Apify-specific and Puppeteer-specific options were mixed together in a single object, causing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns in enqueueLinks\nDESCRIPTION: Example showing how to use glob patterns to filter which URLs should be enqueued, targeting only specific URL patterns on the apify.com domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Cloud Functions\nDESCRIPTION: Setting the main entry point in package.json to direct GCP to the correct source file. This configuration is essential for proper cloud function execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Event Handling Migration in TypeScript\nDESCRIPTION: Shows the difference between old and new event handling syntax, migrating from Apify.events to Actor.on method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Failed Dynamic Content Scraping with PuppeteerCrawler (No Wait)\nDESCRIPTION: This snippet demonstrates an attempt to scrape dynamic content without waiting for elements using PuppeteerCrawler, which fails as elements are not rendered in time.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Try to extract text content of the first actor card without waiting\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor with Node.js\nDESCRIPTION: This Dockerfile sets up a Node.js environment for Crawlee actors. It uses a specialized base image, optimizes dependency installation by copying package files first, installs only production dependencies, and configures the container to start the application with npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using Global Configuration with Crawler\nDESCRIPTION: Example demonstrating how to use the Configuration class to set global configuration options for a Crawler instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: PlaywrightCrawler Without Proper Element Waiting\nDESCRIPTION: An incorrect implementation of PlaywrightCrawler that fails to scrape JavaScript-rendered content because it doesn't properly wait for elements to appear before attempting to extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be in the DOM yet\n        const element = await page.locator('.ActorStoreItem').first();\n        const actorText = await element.textContent();\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Functions in Apify SDK for JavaScript\nDESCRIPTION: Demonstrates the updated launch options for Apify.launchPuppeteer() and introduces Apify.launchPlaywright(). This change separates Apify-specific options from browser-specific launch options and adds support for Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PuppeteerCrawler with TypeScript\nDESCRIPTION: This snippet demonstrates how to access and log the current proxy information within the PuppeteerCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo }) {\n        if (proxyInfo) {\n            console.log(`Proxy URL: ${proxyInfo.url}`);\n        }\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration in Crawlee\nDESCRIPTION: Demonstrates how to create a new ProxyConfiguration instance with custom proxy URLs and request a new proxy URL. This shows the foundational setup for using proxies with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with URL pattern matching in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks helper with URL pattern matching via globs. This allows filtering links on a page based on specific URL patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: Simple example of enqueuing links using the default behavior without any specific selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Stage Docker Build for Crawlee Actor with Playwright Chrome\nDESCRIPTION: This Dockerfile defines a complete build process for a Crawlee actor that uses Playwright with Chrome. It employs a multi-stage build pattern to keep the final image size small by building in one stage and copying only the necessary files to the runtime image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Adding Timeouts to Responses During Migration in SuperScraper with TypeScript\nDESCRIPTION: This function adds timeouts to all pending responses during Actor migration. It ensures clean termination by sending error responses after a specified timeout period, preventing lingering requests during server transitions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport const addTimeoutToAllResponses = (timeoutInSeconds: number = 60) => {\n    const migrationErrorMessage = {\n        errorMessage: 'Actor had to migrate to another server. Please, retry your request.',\n    };\n\n    const responseKeys = Object.keys(responses);\n\n    for (const key of responseKeys) {\n        setTimeout(() => {\n            sendErrorResponseById(key, JSON.stringify(migrationErrorMessage));\n        }, timeoutInSeconds * 1000);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Using Playwright Crawler in TypeScript\nDESCRIPTION: This example demonstrates how to crawl all links on a website using Playwright Crawler. It configures a crawler with Playwright browser automation, processes pages by enqueueing links, and captures screenshots. Playwright provides modern browser automation capabilities across multiple browser engines.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Here you can set options that are passed to the launchPlaywright() function.\n    launchContext: {\n        launchOptions: {\n            headless: true,\n            // Other Playwright options\n        },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee configuration and handlers\n    // that will be used for each of the requests.\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}...`);\n\n        // A function to be evaluated by Playwright within the browser context.\n        const pageTitle = await page.title();\n        log.info(`Title of ${request.url}: ${pageTitle}`);\n\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n\n        // Save a screenshot of the current page to a file\n        await page.screenshot({\n            path: `${request.url.replace(/[^a-zA-Z0-9]/g, '')}.jpg`,\n            fullPage: true,\n        });\n\n        // Save the HTML content of the page to dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: await page.content()\n        });\n    },\n});\n\n// Add first URL to a RequestQueue\nconst url = 'https://apify.com';\nawait crawler.run([url]);\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Actor Docker Image\nDESCRIPTION: This Dockerfile configures an Apify actor environment. It starts with an Apify base image, installs Node.js dependencies, copies project files, and sets up the run command. The build process is optimized for speed and minimal image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Updated Launch Functions in JavaScript\nDESCRIPTION: Demonstrates the new launch function patterns for both Puppeteer and Playwright with explicit launch options and custom module support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing CheerioCrawler for HTML Parsing in TypeScript\nDESCRIPTION: This code demonstrates how to create a CheerioCrawler instance to crawl URLs from an external file, parse HTML with Cheerio, and extract page titles and h1 tags from each page. It includes the complete setup for request handling and data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'fs/promises';\n\n// Reads a list of URLs from a file\nconst readUrls = async () => {\n    const content = await readFile('./urls.txt', { encoding: 'utf8' });\n    return content.trim().split('\\n');\n};\n\n// Create an instance of the CheerioCrawler class\nconst crawler = new CheerioCrawler({\n    // Let's limit the crawling to only 10 requests per minute\n    maxRequestsPerMinute: 10,\n    // This function is called for each URL to crawl.\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        // The $ argument is the Cheerio object which contains parsed HTML\n        // of the website. It is very similar to jQuery.\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // We can use the familiar syntax like:\n        const h1Texts = [];\n        $('h1').each((index, el) => {\n            h1Texts.push($(el).text());\n        });\n\n        // Save the results to Dataset.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1Texts,\n        });\n    },\n});\n\n// Start the crawler with the initial list of URLs\nreadUrls()\n    .then((urls) => crawler.run(urls))\n    .catch((err) => console.error(err));\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests per Crawl in Cheerio Crawler\nDESCRIPTION: Code showing how to limit the number of pages a crawler will process using the maxRequestsPerCrawl option to prevent the crawler from running indefinitely.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler Using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.puppeteer.saveSnapshot() utility. It creates a PuppeteerCrawler instance, defines a handler function that saves snapshots for each page, and starts the crawler with a list of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, crawler }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        await crawler.puppeteerUtils.saveSnapshot(page);\n    },\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Examples of installing Crawlee and its dependencies using npm, showing different installation options for various use cases.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Modifying the Detail Route for Parallel Scraping in Crawlee\nDESCRIPTION: Updates the detail route handler to send scraped data back to the parent process using IPC (Inter-Process Communication) instead of storing it directly, enabling proper data collection in a parallel scraping setup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst { DETAIL } = Labels;\nroutes[DETAIL] = async ({ request, $, log }) => {\n    const { userData } = request;\n    const { id } = userData;\n\n    log.info(`Extracting data from product with ID: ${id}`, { url: request.url });\n\n    // Extract the data from the page using Cheerio selectors\n    const title = $('h3.product-title').text().trim();\n    const price = $('span.price').text().trim();\n    const discountBadge = $('span.discount-badge').text().trim();\n\n    // Process the extracted data\n    const priceNumber = Number(price.replace('$', ''));\n    const discountMatch = discountBadge.match(/-(\\d+)%/);\n    const discountPercentage = discountMatch ? Number(discountMatch[1]) : 0;\n\n    const results = {\n        id,\n        url: request.url,\n        title,\n        price: priceNumber,\n        discountPercentage,\n    };\n\n    // Check if we're in a worker process (from parallel-scraper.mjs)\n    if (process.env.IS_WORKER_THREAD) {\n        // We're in a worker process, so send the data to the parent process\n        process.send(results);\n    } else {\n        // We're not in a worker process, probably running the scraper directly\n        log.info(`Saving data: ${JSON.stringify(results)}`);\n        // Push the data to the dataset\n        await crawler.pushData(results);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Creating ProxyConfiguration with Country Selection\nDESCRIPTION: Creates a ProxyConfiguration that uses proxies from a specific country (United States). This is useful when you need to access websites with geo-restrictions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    countryCode: 'US',\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler with Crawlee in TypeScript\nDESCRIPTION: This code demonstrates how to create a basic web crawler using Crawlee's BasicCrawler class. It downloads several web pages using HTTP requests, processes them, and stores their HTML content and URLs in the default dataset. The example shows the crawler configuration, request handling, and dataset storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that just downloads and processes URLs.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, sendRequest, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n\n        // Download the page content as text\n        const { body } = await sendRequest();\n\n        log.info(`The title of \"${url}\" is: ${body.match(/<title>([^<]+)<\\/title>/i)?.[1]}`);\n\n        // Store the data in the default dataset\n        // (will be exported to storage/datasets/default directory on the local disk).\n        await Dataset.pushData({\n            url,\n            html: body,\n        });\n    },\n});\n\n// Add URLs to the queue and start the crawl.\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction',\n    'https://crawlee.dev/docs/guides/apify-platform',\n    'https://crawlee.dev/docs/guides/getting-started',\n    'https://crawlee.dev/docs/guides/request-storage',\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers for Web Scraping\nDESCRIPTION: Defines route handlers for different page types (detail, category, default) using Crawlee's router. Includes logic for extracting product details, handling pagination, and enqueueing links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1);\n    const manufacturer = urlPart[0].split('-')[0];\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL',\n    });\n\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY',\n        });\n    }\n});\n\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Project for Apify Platform\nDESCRIPTION: Modified main script showing integration with Apify Platform through Actor.init() and Actor.exit() calls\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TypeScript Configuration\nDESCRIPTION: Installs the Apify TypeScript configuration package as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Integrating Proxy Configuration with HttpCrawler\nDESCRIPTION: Shows how to set up proxy configuration with HttpCrawler. The crawler automatically uses the configured proxies for all connections and rotates them as needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, json }) {\n        console.log(`Fetched ${request.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing and Authenticating with Apify CLI\nDESCRIPTION: Commands to install the Apify CLI tool and authenticate with your API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Using LLM Prompt for Framework Analysis in Web Scraping\nDESCRIPTION: A prompt example for querying an LLM like ChatGPT or Claude about Next.js framework specifics when doing web scraping analysis. This approach helps to understand routing and link formation in the target website's framework.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/11-10-web-scraping-tips/index.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nI am in the process of optimizing my website using Next.js. Are there any files passed to the browser that describe all internal routing and how links are formed?\n\nRestrictions:\n- Accompany your answers with code samples\n- Use this message as the main message for all subsequent responses\n- Reference only those elements that are available on the client side, without access to the project code base\n\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json Configuration\nDESCRIPTION: Example of using CheerioCrawler with configuration loaded from crawlee.json file. The crawler processes two URLs sequentially with waiting periods, demonstrating the persistence interval configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee to Crawl All Links\nDESCRIPTION: Setting up Crawlee's enqueueLinks function to follow all links, regardless of domain, using the 'all' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Downloading Files with Streams using FileDownload Crawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the FileDownload crawler class to download files using Node.js streams. It logs the download progress and stores the downloaded data in the key-value store. The example includes handling of different file types and custom naming of stored files.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/file_download_stream.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FileDownloadCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new FileDownloadCrawler({\n    async requestHandler({ enqueueLinks, downloadFile, log }) {\n        // Add more file links to the queue\n        await enqueueLinks({\n            globs: ['**/*.{pdf,docx,xlsx,png,jpg}'],\n        });\n\n        // Download the file using streams\n        const { body, contentType, fileName } = await downloadFile({\n            // This will ensure that the download stream is piped\n            // properly, and the data will be saved to the key-value store\n            streamDownload: true,\n            // Optional: log the download progress\n            onProgress: ({ downloadedBytes, totalBytes }) => {\n                log.info(\n                    `Downloaded ${downloadedBytes} bytes out of ${totalBytes} total bytes`,\n                );\n            },\n        });\n\n        // The body now contains a ReadableStream\n        // We can pipe it to a file or do other operations\n        await KeyValueStore.setValue(fileName, body, { contentType });\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run(['https://apify.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Launching Multiple Browser Types with Browser Pool\nDESCRIPTION: Example showing how to configure Browser Pool with multiple browser engines (Chromium, Firefox, WebKit) and open pages in a round-robin fashion.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BrowserPool, PlaywrightPlugin } from '@crawlee/browser-pool';\nimport playwright from 'playwright';\n\nconst browserPool = new BrowserPool({\n    browserPlugins: [\n        new PlaywrightPlugin(playwright.chromium),\n        new PlaywrightPlugin(playwright.firefox),\n        new PlaywrightPlugin(playwright.webkit),\n    ],\n});\n\n// Open 4 pages in 3 browsers. The browsers are launched\n// in a round-robin fashion based on the plugin order.\nconst chromiumPage = await browserPool.newPage();\nconst firefoxPage = await browserPool.newPage();\nconst webkitPage = await browserPool.newPage();\nconst chromiumPage2 = await browserPool.newPage();\n\n// Don't forget to close pages / destroy pool when you're done.\n```\n\n----------------------------------------\n\nTITLE: Configuring enqueueLinks for Same-Domain Crawling in TypeScript\nDESCRIPTION: Shows how to set up enqueueLinks to include subdomains in the crawl using the 'same-domain' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests Before Enqueuing\nDESCRIPTION: Example of using transformRequestFunction with enqueueLinks to filter or modify requests before they are added to the queue, in this case ignoring PDF files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending from @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool from Crawler\nDESCRIPTION: Example showing how to access the BrowserPool instance from both the crawler object and within the handle page function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This example shows how to use JSDOMCrawler to crawl a list of URLs from an external file, parse the HTML using jsdom, and extract page titles and h1 tags. It demonstrates handling both successful requests and failures.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\n// Create an instance of the JSDOMCrawler class\nconst crawler = new JSDOMCrawler({\n    // Let's limit the number of concurrency to prevent memory overloads\n    maxConcurrency: 10,\n    // This function is called for each URL\n    async requestHandler({ window, request, log }) {\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent;\n        const h1texts = Array.from(document.querySelectorAll('h1'))\n            .map((element) => element.textContent);\n\n        // Store the results to the dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1texts,\n        });\n\n        log.info(`Crawler extracted data from ${request.url}`, { url: request.url });\n    },\n    // This function is called if the request failed\n    failedRequestHandler({ request, log }) {\n        log.error(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Read the list of URLs from a file\nconst urlsText = await readFile('./urls.txt', { encoding: 'utf8' });\nconst urls = urlsText.split('\\n').filter((url) => url.length > 0);\n\n// Run the crawler\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management\nDESCRIPTION: Demonstration of manual session management without a crawler, showing how to directly use SessionPool for custom implementation scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{StandaloneSource}\n```\n\n----------------------------------------\n\nTITLE: Enabling System Information V2 Experiment in CheerioCrawler\nDESCRIPTION: This code snippet demonstrates how to enable the System Information V2 experimental feature in Crawlee. It shows setting the configuration option and creating a CheerioCrawler instance that will use the improved metric collection system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/experiments/systemInfoV2.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\n\nConfiguration.set('systemInfoV2', true);\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Failed Dynamic Content Scraping with PlaywrightCrawler (No Wait)\nDESCRIPTION: This snippet shows an attempt to scrape dynamic content without waiting for elements, which fails due to elements not being rendered in time.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Try to extract text content of the first actor card without waiting\n        const actorText = await page.textContent('.ActorStoreItem', { timeout: 1 });\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda Layer deployment\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Setting Apify Token in Configuration\nDESCRIPTION: Example of setting the Apify API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State in Cheerio Crawler (TypeScript)\nDESCRIPTION: Shows how to use the new useState() method to manage and automatically save crawler state in a CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Updated Access to AutoscaledPool in SDK v1\nDESCRIPTION: Example showing the correct way to access autoscaledPool in SDK v1, which has moved under the crawler property in the crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 along with Puppeteer for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation with Implicit RequestQueue\nDESCRIPTION: This snippet shows a more concise way to create a CheerioCrawler by using the crawler's implicit RequestQueue. Instead of manually creating a RequestQueue, it uses the crawler.run() method with an array of URLs, which simplifies the code while providing the same functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Fetching HTML from a Single URL using got-scraping in JavaScript/TypeScript\nDESCRIPTION: This code demonstrates how to use the got-scraping package to fetch the HTML content of a web page. It makes a GET request to a specified URL and logs the received HTML to the console, showing a basic example of web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\n/**\n * This is a simple example showing the very basics of using Scraper tools.\n * It simply grabs the HTML of apify.com using the gotScraping library,\n * which is a modification of the very popular got library, which aims to\n * prevent bot detection by using headers that are common in normal web browsers.\n */\nconst run = async () => {\n    const { body } = await gotScraping({\n        url: 'https://apify.com',\n        headerGeneratorOptions: {\n            browsers: [\n                {\n                    name: 'chrome',\n                    minVersion: 87,\n                    maxVersion: 89,\n                },\n            ],\n            devices: ['desktop'],\n            locales: ['en-US'],\n            operatingSystems: ['windows', 'macos'],\n        },\n    });\n\n    console.log(body);\n};\n\nrun();\n\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content with Cheerio in CheerioCrawler\nDESCRIPTION: A simple example showing how to extract the text content of the first h2 element on a page using Cheerio's selector and text() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Operations in Crawlee\nDESCRIPTION: This snippet illustrates how to perform basic operations with datasets in Crawlee, including writing single and multiple rows to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler in TypeScript\nDESCRIPTION: This snippet shows how to create a basic CheerioCrawler that crawls a single URL and prints the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Basic Crawlee Crawler Setup in TypeScript\nDESCRIPTION: Initial setup of a Crawlee crawler that scrapes a single page's title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Country and Group Options\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection. Example shows setting up residential proxies from the US.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction using Playwright in JavaScript\nDESCRIPTION: This snippet combines all the previous extraction methods to scrape comprehensive product information from a web page using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Automating GitHub Repository Search Forms with PuppeteerCrawler\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler to fill and submit a GitHub search form. It configures search parameters including search term, repository owner, start date, and language, then extracts and processes the search results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/forms.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page }) {\n        // Navigate to the GitHub search page\n        await page.goto('https://github.com/search/advanced');\n\n        // Fill in the search form\n        await page.type('#adv-search-qualifier-value-repository.name', 'crawler');\n        await page.type('#adv-search-qualifier-value-owner', 'apify');\n        await page.type('#adv-search-date-from', '2019-01-01');\n\n        // Select \"JavaScript\" from the language select box\n        await page.select('#search_language', 'JavaScript');\n\n        // Submit the form\n        await Promise.all([\n            page.waitForNavigation(),\n            page.click('.js-advanced-search-submit'),\n        ]);\n\n        // Extract the data from the page\n        const repos = await page.$$eval('.repo-list-item', ($repos) => {\n            return $repos.map(($repo) => {\n                const title = $repo.querySelector('.f4.text-normal a').innerText.trim();\n                const description = $repo.querySelector('.mb-1')?.innerText.trim() || null;\n                const url = $repo.querySelector('.f4.text-normal a').href;\n\n                return {\n                    title,\n                    description,\n                    url,\n                };\n            });\n        });\n\n        // Save the data to the default dataset\n        await Dataset.pushData({\n            searchResults: repos,\n        });\n\n        console.log(`Found ${repos.length} repositories`);\n    },\n});\n\nawait crawler.run(['https://github.com/search/advanced']);\n```\n\n----------------------------------------\n\nTITLE: Using local storage with Crawlee Actor\nDESCRIPTION: Example of how to use @apify/storage-local with Crawlee Actor for local development and testing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Pre and Post Navigation Hooks in JavaScript\nDESCRIPTION: This snippet demonstrates how to migrate from using a gotoFunction to using preNavigationHooks and postNavigationHooks in an Apify PuppeteerCrawler. It simplifies navigation customization and improves code readability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue in JavaScript for Crawlee v2.3\nDESCRIPTION: This code snippet shows how to create a RequestQueue instance in JavaScript using the older Crawlee v2.3 API. It demonstrates both named and default queue initialization patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/motivation.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst { RequestQueue } = require('crawlee');\n\n// Opens a request queue. If it doesn't exist, it will be created.\nconst queue = await RequestQueue.open('my-queue');\n\n// You can also use the default queue.\nconst defaultQueue = await RequestQueue.open();\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Chromium for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and compress the node_modules folder for AWS Lambda deployment. This creates a dependencies.zip file that will be uploaded to S3 and used as a Lambda Layer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in TypeScript\nDESCRIPTION: A complete example showing how to set up and run a recursive website crawler using PuppeteerCrawler. The code demonstrates how to initialize the crawler, handle page requests, extract data, and follow links recursively while managing state and datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PuppeteerCrawler, log } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // The crawler will automatically process the 'https://crawlee.dev' URL first\n    // and then recursively all the links it finds on the page that satisfy the 'enqueueLinks' options.\n    // The same options would be used by the 'crawler.addRequests()' function.\n    requestHandler: async ({ request, page, enqueueLinks }) => {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.loadedUrl,\n            title,\n        });\n\n        // Extract links from the current page and add them to the crawling queue.\n        await enqueueLinks({\n            // Consider only links that match the glob pattern\n            globs: ['https://crawlee.dev/**'],\n            // Exclude links to other domains or with hashes\n            exclude: ['**/courses/**'],\n        });\n    },\n    // Comment this option to scrape the full website.\n    maxRequestsPerCrawl: 20,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.13.2\nDESCRIPTION: Version bump only for @crawlee/playwright package.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/playwright-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.2](https://github.com/apify/crawlee/compare/v3.13.1...v3.13.2) (2025-04-08)\n\n**Note:** Version bump only for package @crawlee/playwright\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Puppeteer Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Puppeteer Crawler in Crawlee. It uses the Sitemap utility to fetch and parse the sitemap from cnn.com, then processes each URL with the Puppeteer Crawler to extract and log headlines from articles.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, EnqueueStrategy } from '@crawlee/puppeteer';\nimport { Sitemap } from '@crawlee/utils';\n\n// We'll use CNN as our sitemap provider\nconst sitemapUrl = 'https://www.cnn.com/sitemaps/cnn/index.xml';\n\n// Create and initialize the crawler\nconst crawler = new PuppeteerCrawler({\n    // We want to keep the crawler going even if there's an error in our code\n    ignoreSslErrors: true,\n    requestHandler: async ({ page, request, enqueueLinks, log }) => {\n        // If this is a regular document, we have to parse it to find all the links\n        log.info(`Processing ${request.url}...`);\n\n        // If we're crawling an article, extract some data from it.\n        if (/\\/[\\w-]+\\/\\d{4}\\/\\d{2}\\/\\d{2}\\/.+/i.test(request.url)) {\n            const title = await page.title();\n            log.info(`Found article with title: ${title}`);\n        }\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests.\n});\n\n// Initialize the Sitemap class from @crawlee/utils with the supplied URL\nconst sitemap = await Sitemap.load({ url: sitemapUrl });\n\n// Add the sitemap URLs to the crawler's request queue\nawait crawler.addRequests(\n    sitemap.urls.map((url) => ({ url, label: 'SITEMAP' })),\n    { strategy: EnqueueStrategy.SameHostname },\n);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in Handler Functions\nDESCRIPTION: Shows how to access crawler properties like requestQueue and autoscaledPool from within handler functions using the new crawler object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request List with PuppeteerCrawler in Crawlee\nDESCRIPTION: Shows how to create a Request List with predefined URLs and use it with a PuppeteerCrawler to process each request in the list.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: This code demonstrates how to use SessionPool independently without a crawler. It shows how to manually create, manage, and rotate sessions with proper error handling and proxy configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\n{StandaloneSource}\n```\n\n----------------------------------------\n\nTITLE: Getting Public URL for a Key-Value Store Item\nDESCRIPTION: Example demonstrating how to obtain a public URL for an item stored in a Key-Value Store on the Apify Platform, which can be used for sharing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Extracting and Processing Product Price using Playwright in JavaScript\nDESCRIPTION: This snippet shows how to extract the product price from a web page using Playwright, filter the correct element, and process the price string into a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Using Playwright-Firefox Docker Image\nDESCRIPTION: Dockerfile configuration for using Apify's Playwright-Firefox image. This image comes with Firefox pre-installed for use with PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Standalone Session Management with ProxyConfiguration\nDESCRIPTION: Demonstrates how to manually create and manage proxy sessions without using a crawler. This is useful for custom implementations or when using proxies outside of the crawler ecosystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\n// Getting a proxy URL for the first time with a specific session ID\nconst firstProxyUrl = await proxyConfiguration.newUrl('my-session');\n\n// Getting a proxy URL for the second time with the same session ID\nconst secondProxyUrl = await proxyConfiguration.newUrl('my-session');\n\n// Would output true - the same proxy URL is used for the same session\nconsole.log(firstProxyUrl === secondProxyUrl); \n\n// Getting a proxy URL for a different session\nconst differentProxyUrl = await proxyConfiguration.newUrl('different-session');\n\n// Can be true or false based on available proxies and their rotation\nconsole.log(firstProxyUrl === differentProxyUrl);\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain with enqueueLinks\nDESCRIPTION: Implementation of a crawler that filters links to stay on the same domain using the default behavior of the enqueueLinks function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // find all links and add them to the crawling queue\n        // by default, the enqueueLinks function only enqueues links that point to the same hostname\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations\nDESCRIPTION: Demonstrates the basic operations of a request queue including opening, adding requests, and processing them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open the default request queue\nconst requestQueue = await RequestQueue.open();\n\n// Add requests to the queue\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Get the queue info\nconst info = await requestQueue.getInfo();\nconsole.log(info);\n\n// Get the next request from the queue\nconst request = await requestQueue.fetchNextRequest();\nconsole.log(request);\n\n// Mark the request as handled\nawait requestQueue.markRequestHandled(request);\n```\n\n----------------------------------------\n\nTITLE: Updated Autoscaled Pool Access in SDK v1\nDESCRIPTION: Example showing how the location of autoscaledPool has changed in SDK v1, moving from being directly on the context to being under the crawler property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Demonstrates how to initialize a BasicCrawler and use the sendRequest function to make HTTP requests. This function uses got-scraping under the hood to mimic browser requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns in enqueueLinks\nDESCRIPTION: Using glob patterns to filter URLs in enqueueLinks. This example only enqueues URLs that match the specified pattern for apify.com domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up Docker Environment for Crawlee Actor on Node.js 16\nDESCRIPTION: This Dockerfile configures a container for running a Crawlee actor using the apify/actor-node:16 base image. It implements best practices for Docker layer caching by copying package files first, installing only production dependencies, and then copying the remaining source code. The container runs the actor using 'npm start' when launched.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function Handler for CheerioCrawler\nDESCRIPTION: Implements a handler function that wraps the crawler execution and returns the crawled data. This function serves as the entry point for the GCP Cloud Function, accepting request and response objects to interact with the HTTP endpoint.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components Through Context\nDESCRIPTION: Example of how to access crawler components like requestQueue and autoscaledPool through the crawler property in the crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic HTTP Crawler Implementation in TypeScript\nDESCRIPTION: Demonstrates how to use HttpCrawler to crawl URLs from an external file, make HTTP requests, and save HTML content. The crawler uses the Crawlee framework and processes URLs in batches.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, Dataset } from 'crawlee';\nimport { readFileSync } from 'fs';\n\n// Create an instance of the HttpCrawler class - a crawler\n// that automatically loads the URLs using plain HTTP requests\nconst crawler = new HttpCrawler({\n    async requestHandler({ $, request, body }) {\n        // Save results as HTML\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\n// Load URLs from a text file\nconst sources = readFileSync('urls.txt', 'utf8')\n    .split('\\n')\n    .map((url) => url.trim())\n    .filter((url) => url.length > 0);\n\n// Execute the crawler with the text file URLs\nawait crawler.run(sources);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Crawlee's Utils in Puppeteer\nDESCRIPTION: This example shows how to use Crawlee's utility function saveSnapshot() to capture a screenshot. The utility automatically handles key generation based on timestamps and provides additional debugging capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launchPuppeteer } from 'crawlee/puppeteer';\nimport { puppeteerUtils } from 'crawlee/puppeteer';\n\nconst url = 'https://apify.com';\n\n// Launch a browser\nconst browser = await launchPuppeteer();\n\n// Open a new tab\nconst page = await browser.newPage();\n\n// Navigate to the URL\nawait page.goto(url);\n\n// Capture and save the screenshot using the utility function\n// It automatically generates the key, which is returned\nconst key = await puppeteerUtils.saveSnapshot(page, { key: 'my-screenshot' });\n\n// Close browser\nawait browser.close();\n\nconsole.log(`Screenshot saved as ${key}!`);\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support\nDESCRIPTION: This snippet shows how to create a request queue that supports locking by importing RequestQueueV2 instead of the standard RequestQueue. It then demonstrates adding request URLs to the queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() Methods in Crawlee\nDESCRIPTION: Demonstrates the explicit initialization and exit approach in Crawlee, where Actor.init() sets up storage and Actor.exit() handles teardown and process termination.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with PlaywrightCrawler\nDESCRIPTION: Full implementation of an AWS Lambda handler function that instantiates a PlaywrightCrawler with proper Chromium configuration, runs the crawler, and returns the crawled data in the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code as Apify Actor using Actor.main()\nDESCRIPTION: Example of wrapping Crawlee code in Actor.main() function to run as an Apify actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.main(async () => {\n    const crawler = new CheerioCrawler({\n        async requestHandler({ $, request, enqueueLinks, log }) {\n            const title = $('title').text();\n            log.info(`Title of ${request.url} is '${title}'`);\n\n            await Dataset.pushData({\n                url: request.url,\n                title,\n            });\n\n            await enqueueLinks();\n        },\n        maxRequestsPerCrawl: 20,\n    });\n\n    await crawler.run(['https://crawlee.dev']);\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing useState with CheerioCrawler\nDESCRIPTION: Demonstrates how to use the crawler's useState() method to maintain state during crawling. The state is automatically saved when a persistState event occurs, and the reference is cached for consistent access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in JavaScript (SDK v1)\nDESCRIPTION: Example demonstrating the new Crawling Context object in SDK v1, which provides consistent access to properties across different handler functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright in Bash\nDESCRIPTION: Command to install Apify SDK v1 along with Playwright using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation Using Implicit RequestQueue\nDESCRIPTION: This code shows a more concise way to create a crawler by using the crawler's implicit RequestQueue and the run() method's parameter to add starting URLs, eliminating the need for explicit queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue in JavaScript for Crawlee v3.0\nDESCRIPTION: This code snippet shows how to create a RequestQueue instance in JavaScript using Crawlee v3.0. It demonstrates both named and default queue initialization patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { RequestQueue } = require('crawlee');\n\n// Opens a request queue. If it doesn't exist, it will be created.\nconst queue = await RequestQueue.open('my-queue');\n\n// You can also use the default queue.\nconst defaultQueue = await RequestQueue.open();\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with BasicCrawler\nDESCRIPTION: Example of configuring a proxy server with BasicCrawler's sendRequest function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generator Options in sendRequest\nDESCRIPTION: Example showing how to customize the browser fingerprint generation by specifying devices, locales, operating systems, and browsers to mimic. This helps in making requests appear more like they come from regular browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Element Selection for enqueueLinks\nDESCRIPTION: Example of overriding the default selector used by enqueueLinks to find links on a page by specifying a custom CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Response\nDESCRIPTION: Final version of the Lambda handler that includes crawler initialization, execution, and returns the scraped data with proper HTTP status code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Functions in Apify SDK for JavaScript\nDESCRIPTION: Demonstrates the changes in launch function arguments and the introduction of Playwright support in Apify SDK. This update standardizes the launch options across different browser automation tools.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform via CLI\nDESCRIPTION: Commands to install Apify CLI and log in using an API token for accessing Apify platform features from Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to use Playwright Crawler to crawl multiple specified URLs. It extracts the title from each page and stores the results using the Dataset API. Requires the apify/actor-node-playwright-chrome image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of Crawlee\nDESCRIPTION: npm commands to manually install Crawlee and its dependencies for different crawler types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Successful Actor Card Extraction\nDESCRIPTION: Shows the successful but unformatted output when properly waiting for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Handlers for Crawlee in JavaScript\nDESCRIPTION: Defines route handlers for different page types including product details, category pages, and default handling. Includes data extraction logic and link enqueuing for pagination and product details.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1);\n    const manufacturer = urlPart[0].split('-')[0];\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL',\n    });\n\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY',\n        });\n    }\n});\n\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee (TypeScript)\nDESCRIPTION: Imports the PlaywrightCrawler and Dataset modules from Crawlee. This is necessary to use the Dataset.pushData() function for saving scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to combine a Request List for initial URLs with a Request Queue for dynamically discovered URLs during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, RequestQueue, RequestList } from 'crawlee';\n\n// First, we define our request list containing the initial requests\nconst requestList = await RequestList.open('my-list', [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n    // ... we can have millions of URLs here\n]);\n\n// Then we create a request queue that will handle new requests\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// And now we can use both\nconst crawler = new CheerioCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n\n        // We can add new requests at runtime to the queue\n        // This DOES NOT ADD to the list, the list is immutable.\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Using Playwright Chrome Docker Image\nDESCRIPTION: Example of using the Docker image with pre-installed Playwright and Chrome browser. Supports CheerioCrawler and PlaywrightCrawler with both headless and headful modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content of an Element with Cheerio\nDESCRIPTION: This snippet shows how to find the first h2 element on a page and extract its text content using Cheerio's text() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Custom Configuration\nDESCRIPTION: Sets up a basic Cheerio crawler with a custom Configuration instance to prevent storage persistence on Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Images\nDESCRIPTION: Examples of how to use pre-release versions of Apify Docker images for testing, with or without specific automation library versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:16-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Creating Session with Authentication Headers for TikTok API\nDESCRIPTION: Function that initializes a session with proxy and fetches the required authentication headers from TikTok's creative center. It calls a helper function to extract verification tokens from the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-30-jsdom-based-scraping/index.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nexport const createSessionFunction = async (\n    sessionPool,\n    proxyConfiguration,\n) => {\n    const proxyUrl = await proxyConfiguration.newUrl(Math.random().toString());\n    const url =\n        'https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pad/en';\n    // need url with data to generate token\n    const response = await gotScraping({ url, proxyUrl });\n    const headers = await getApiUrlWithVerificationToken(\n        response.body.toString(),\n        url,\n    );\n    if (!headers) {\n        throw new Error(`Token generation blocked`);\n    }\n    log.info(`Generated API verification headers`, Object.values(headers));\n    return new Session({\n        userData: {\n            headers,\n        },\n        sessionPool,\n    });\n};\n```\n\n----------------------------------------\n\nTITLE: Using the 'all' Strategy with enqueueLinks\nDESCRIPTION: Configures enqueueLinks to follow all links regardless of domain, allowing the crawler to explore beyond the initial website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup Implementation\nDESCRIPTION: Shows how to purge default storage directories in Crawlee using the purgeDefaultStorages helper function\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor with Playwright Chrome\nDESCRIPTION: Dockerfile that builds a containerized environment for running a Crawlee actor. It uses a pre-configured image with Node.js, Playwright, and Chrome, installs dependencies while skipping development packages, and sets up the application code with the proper file ownership.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using a Custom HTTP Client with Crawlee's BasicCrawler in TypeScript\nDESCRIPTION: Example showing how to instantiate a custom HTTP client and pass it to a BasicCrawler constructor. This demonstrates the integration of a custom HTTP client with Crawlee's crawling framework.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from '@crawlee/core';\nimport { FetchHttpClient } from './implementation';\n\nconst crawler = new BasicCrawler({\n    // ...\n    httpClient: new FetchHttpClient(),\n    async requestHandler({ request, sendRequest }) {\n        const response = await sendRequest({\n            url: request.url,\n            method: 'GET',\n        });\n        \n        // process the response\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Request Limits with maxRequestsPerCrawl\nDESCRIPTION: Example showing how to limit the number of requests a crawler will make using the maxRequestsPerCrawl option, which is useful for testing or preventing runaway crawls.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain without enqueueLinks\nDESCRIPTION: Shows how to manually filter links to ensure the crawler only visits pages within the same domain without using the enqueueLinks helper. It extracts all links and filters them by hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Set a limit for the number of pages we'll crawl\n    maxRequestsPerCrawl: 20,\n    \n    async requestHandler({ $, crawler, request }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // Extract the title of the page\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // Extract all links from the page\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n        \n        // Resolve relative URLs\n        const absoluteUrls = links.map((link) => new URL(link, request.url).href);\n        \n        // Get the hostname of the URL we're currently processing\n        // For https://crawlee.dev/api/cheerio-crawler/class/CheerioCrawler\n        // the hostname is crawlee.dev\n        const currentHostname = new URL(request.url).hostname;\n        \n        // Filter links from the same hostname (domain)\n        const sameHostnameLinks = absoluteUrls.filter((link) => {\n            try {\n                return new URL(link).hostname === currentHostname;\n            } catch (error) {\n                return false; // Skip invalid URLs\n            }\n        });\n        \n        // Add filtered URLs to the request queue\n        await crawler.addRequests(sameHostnameLinks);\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler Implementation\nDESCRIPTION: Final implementation including AWS Lambda handler function wrapping the crawler execution and returning results\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request.label for Enqueuing Links\nDESCRIPTION: Demonstrates the Request.label shortcut for labeling requests during crawling. This feature provides an easier way to categorize requests compared to the previous approach using Request.userData.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox for Web Scraping in TypeScript\nDESCRIPTION: This code demonstrates how to initialize and configure PlaywrightCrawler to use Firefox browser. It creates a dataset storage, sets up the crawler with Firefox browser instance, and processes a list of URLs to extract and save website titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Firefox browser.\nconst crawler = new PlaywrightCrawler({\n    // Use Firefox browser\n    browserType: firefox,\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in Crawlee Crawler\nDESCRIPTION: Demonstrates how to enable the request locking experiment in a CheerioCrawler instance. This configuration allows the crawler to use the new request locking API for parallel crawling operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Deprecating gotoFunction Pattern in JavaScript\nDESCRIPTION: Shows the older pattern of using gotoFunction for navigation customization that is being deprecated due to complexity and implementation challenges.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() in TypeScript\nDESCRIPTION: Example of directly using Actor.init() and Actor.exit() methods for managing the actor lifecycle. This approach uses top-level await and explicitly handles initialization and termination.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Results Output\nDESCRIPTION: Example of the JSON output generated by Crawlee crawlers and stored in the datasets directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor to Apify Platform\nDESCRIPTION: Command to deploy the actor code to the Apify platform using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, allowing use of multiple browser types including Firefox and Webkit.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController in JavaScript\nDESCRIPTION: Example demonstrating the correct usage of BrowserController for managing browser actions in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content of an Element with Cheerio\nDESCRIPTION: A simple example showing how to find the first h2 element on a page and extract its text content using Cheerio's selector and text method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Cookie Jar with sendRequest in TypeScript\nDESCRIPTION: Shows how to use a custom cookie jar with sendRequest. The cookieJar option accepts a tough-cookie CookieJar instance to manage cookies for HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using JSON responseType with BasicCrawler in TypeScript\nDESCRIPTION: Demonstrates how to set responseType to 'json' when fetching JSON APIs instead of HTML content. This tells Got how to parse the response appropriately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using 'all' Strategy to Follow All Links\nDESCRIPTION: Example of configuring enqueueLinks to follow every link regardless of domain by using the 'all' strategy, which allows the crawler to wander the internet.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Apify Actor\nDESCRIPTION: Commands to create a new actor project from a template and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Example Output JSON of Scraped Product Data\nDESCRIPTION: Shows the expected JSON output format of the scraped product data, including URL, manufacturer, title, SKU, price, and stock availability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-str-za810es-7-2-channel-hi-res-wi-fi-network-av-receiver\",\n    \"manufacturer\": \"sony\",\n    \"title\": \"Sony STR-ZA810ES 7.2-Ch Hi-Res Wi-Fi Network A/V Receiver\",\n    \"sku\": \"SON-692802-STR-DE\",\n    \"currentPrice\": 698,\n    \"availableInStock\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating CAPTCHA Detection with Crawlee Request Handler\nDESCRIPTION: Integration of CAPTCHA detection into the Crawlee request handler. This code shows how to detect and handle CAPTCHAs in the request flow before proceeding with data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { handleCaptchaBlocking } from './blocking-detection.js';\n\nconst requestHandler = async (context: CheerioCrawlingContext) => {\n    const { request, $ } = context;\n    const { url } = request;\n\n    handleCaptchaBlocking($); // Alternatively, we can put this into the crawler's `postNavigationHooks`\n\n    log.info(`Scraping product page`, { url });\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Initializing Crawlee with Configuration in AWS Lambda\nDESCRIPTION: This code snippet shows how to create a PlaywrightCrawler instance with a new Configuration to ensure each crawler has its own storage in the Lambda environment. This prevents interference between concurrent Lambda executions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Information via BrowserController\nDESCRIPTION: Example showing how to access metadata about the current browser session, such as proxy information and session details, using the BrowserController's launchContext.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Options in JavaScript\nDESCRIPTION: Example showing how to configure BrowserPool options, including lifecycle hooks, in a PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler for Hacker News Scraping\nDESCRIPTION: Sets up a PlaywrightCrawler instance to scrape Hacker News website recursively. The crawler processes pages, extracts data about posts, and follows pagination links. Results are stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/playwright_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, PlaywrightCrawler, RequestQueue } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Extract data from the page using Playwright API.\n        const posts = await page.$$eval('.athing', $posts => {\n            return $posts.map($post => {\n                const id = $post.getAttribute('id');\n                const rank = $post.querySelector('.rank')?.textContent;\n                const title = $post.querySelector('.title a')?.textContent;\n                const url = $post.querySelector('.title a')?.getAttribute('href');\n\n                return {\n                    id,\n                    rank,\n                    title,\n                    url,\n                };\n            });\n        });\n\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            posts,\n        });\n\n        // Enqueue all links from the page.\n        await enqueueLinks({\n            globs: ['https://news.ycombinator.com/*'],\n            label: 'list',\n        });\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run([\n    'https://news.ycombinator.com/',\n]);\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Router in JavaScript\nDESCRIPTION: Sets up the main crawler file that initializes PlaywrightCrawler with a router instance and configures logging levels.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with PlaywrightCrawler in TypeScript\nDESCRIPTION: Sets up a PlaywrightCrawler to crawl category pages of an e-commerce site. It uses enqueueLinks() with a specific selector to target category links and labels them for later processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Initializing Request List in Crawlee\nDESCRIPTION: Shows how to create and use a request list with a PuppeteerCrawler in Crawlee. The request list is used for crawling a predefined set of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler for AWS Lambda\nDESCRIPTION: This code snippet demonstrates how to configure a PlaywrightCrawler instance with a new Configuration to ensure each crawler has its own storage in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Demonstrates proper browser management using the new BrowserController API.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending from @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Proxy Integration with PuppeteerCrawler\nDESCRIPTION: Example of integrating a ProxyConfiguration instance with PuppeteerCrawler. This sets up the crawler to use proxies when navigating websites with Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page }) {\n        // Process the data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl\nDESCRIPTION: Configuration example showing how to limit the maximum number of requests processed in a crawl session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Examples of installing Crawlee packages using npm, showing both full package and individual crawler installation options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer Using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport puppeteer from 'puppeteer';\nimport { utils } from 'crawlee';\n\nawait Actor.init();\n\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://example.com');\n\nawait utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n\nawait browser.close();\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Defining CustomContext Class for Crawlee\nDESCRIPTION: Implementation of a CustomContext class that extends HttpCrawlingResult and BasicCrawlingContext for use with the custom crawler. It includes page_data for JSON responses and a modified enqueue_links callable.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# custom_context.py\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\n\nfrom crawlee.basic_crawler import BasicCrawlingContext\nfrom crawlee.http_crawler import HttpCrawlingResult\n\nif TYPE_CHECKING:\n\n    from collections.abc import Callable\n\n\n@dataclass(frozen=True)\nclass CustomContext(HttpCrawlingResult, BasicCrawlingContext):\n    \"\"\"Crawling context used by CustomCrawler.\"\"\"\n\n    page_data: dict | None\n    # not `EnqueueLinksFunction`` because we are breaking protocol since we are not working with HTML\n    # and we are not using selectors\n    enqueue_links: Callable\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue and Adding Requests in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it. It's a basic setup for crawling, though the tutorial later introduces more efficient methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Initializing a PlaywrightCrawler with Crawlee Configuration for GCP\nDESCRIPTION: Sets up a basic PlaywrightCrawler with a custom Configuration that disables persistent storage, which is necessary for cloud environments like GCP Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler\nDESCRIPTION: Shows how to use the sendRequest helper method to process requests through got-scraping in a BasicCrawler, replacing the deprecated requestAsBrowser utility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Cheerio Crawler for AWS Lambda in JavaScript\nDESCRIPTION: This snippet shows how to initialize a CheerioCrawler with a unique Configuration instance and in-memory storage for AWS Lambda compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Creating RequestQueueV2 Instance\nDESCRIPTION: Shows how to create and initialize a request queue that supports locking using the new RequestQueueV2 class. The queue is populated with multiple URLs to crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Setting up a basic HTTP server in Node.js\nDESCRIPTION: Creates a simple HTTP server using Node.js built-in 'http' module. The server listens on port 3000 and responds with a 'Hello World' message.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createServer } from 'http';\nimport { log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    // We will return the page title here later instead\n    res.end('Hello World\\n');\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Old vs New Handler Arguments Pattern\nDESCRIPTION: Demonstration of how handler arguments have changed from separate objects to a unified Crawling Context object, allowing for better tracking of values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Using Window API in JSDOMCrawler vs Browsers\nDESCRIPTION: Demonstrates the difference between accessing page title in browsers versus when using JSDOM in Crawlee. In browsers, you can access the title directly from document, while in JSDOM you need to use the window object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows how to create a basic proxy configuration using Apify Proxy service.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Proxy Inspection in PuppeteerCrawler\nDESCRIPTION: Demonstrates how to access proxy information in PuppeteerCrawler's requestHandler using proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nInspectionPuppeteerSource\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Crawling Context IDs\nDESCRIPTION: Example showing how to access data across different crawling contexts using the new context ID system, which allows manipulation of master page data from another handler function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Browser Lifecycle Hooks Implementation (JavaScript)\nDESCRIPTION: Shows the migration from launchPuppeteerFunction to browser-pool lifecycle hooks for better browser management and configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links from Page using JSDOM\nDESCRIPTION: Finds all anchor elements with href attributes on a page and extracts their URLs into an array using querySelector and Array methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Updated Launch Function Arguments for Puppeteer and Playwright\nDESCRIPTION: Examples showing the updated argument structure for browser launch functions, including the transition to the more explicit options object structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform via CLI\nDESCRIPTION: Command to log into the Apify Platform using the Apify CLI, which requires a personal access token from the Apify account.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Handlers for Crawlee Crawler\nDESCRIPTION: Shows how to create a router with handlers for different page types (detail pages, category pages, and a default handler). Each handler has specific logic for processing and enqueueing links with appropriate labels for routing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee to Crawl Subdomains\nDESCRIPTION: Setting up Crawlee's enqueueLinks function to include subdomains in the crawl using the 'same-domain' strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Getting Public URL for Key-Value Store Item\nDESCRIPTION: Code example demonstrating how to get a shareable public URL for an item stored in a Key-Value Store on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Using the Deprecated gotoFunction Pattern in JavaScript\nDESCRIPTION: Example of the old gotoFunction pattern which required manual handling of preprocessing, navigation with gotoExtended, and postprocessing steps. This approach was complex and required users to remember implementation details.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Comparing Document Title Access between Browsers and JSDOM\nDESCRIPTION: Demonstrates the difference in accessing the page title between browser JavaScript and JSDOM. While browsers can use document.title directly, JSDOM requires window.document.title.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation\nDESCRIPTION: The internal implementation of the sendRequest function showing how it configures the got-scraping library with default options. It uses request parameters, session management, and other context options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from an Element with Cheerio\nDESCRIPTION: This snippet demonstrates how to find the first h2 element on a page and extract its text content using Cheerio's selector syntax and text() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy in CheerioCrawler\nDESCRIPTION: Variable reference InspectionCheerioSource showing proxy inspection in CheerioCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionCheerioSource}\n```\n\n----------------------------------------\n\nTITLE: New launchContext Pattern for Browser Initialization\nDESCRIPTION: Updated configuration using launchContext with explicit separation between Apify options and Puppeteer launchOptions for better clarity and consistency with browser-pool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Crawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use HttpCrawler to crawl a list of URLs from an external file. It loads each URL using a plain HTTP request and saves the HTML content. The crawler is configured with various options including maximum requests, session pool, and proxy configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/http_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\nimport { readFile, writeFile } from 'fs/promises';\n\nconst crawler = new HttpCrawler({\n    // Use the specified maxRequestsPerCrawl or fall back to 20 as a default\n    maxRequestsPerCrawl: +(process.env.MAX_REQUESTS_PER_CRAWL ?? 20),\n    // You can uncomment the following line to set a custom session pool\n    // sessionPoolOptions: { maxPoolSize: 100 },\n    // If you want to use proxies, you can configure them here:\n    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    async requestHandler({ request, body, $ }) {\n        const title = $('title').text();\n        console.log(`Title of ${request.url}: ${title}`);\n        await writeFile(`output/${Math.random().toString(36).slice(2)}.html`, body.toString(), 'utf-8');\n    },\n});\n\nconst urls = (await readFile('urls.txt', 'utf-8')).split('\\n').filter(Boolean);\n\nawait crawler.run(urls);\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData()\nDESCRIPTION: This code demonstrates how to save extracted data to Crawlee's default dataset. The pushData() method takes the results object and stores it as a new row in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File in TypeScript using Crawlee\nDESCRIPTION: This code snippet demonstrates how to export an entire dataset to a single CSV file using Crawlee's Dataset API. It uses the `exportToValue` function to export the default dataset and stores the result in a key-value store named 'my-data' with the key 'OUTPUT'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\nawait Dataset.exportToValue('OUTPUT', {\n    contentType: 'text/csv',\n    keyValueStoreName: 'my-data',\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Directory Structure for Request Queue in Crawlee\nDESCRIPTION: Example showing the directory structure for request queue storage. The queue is stored in a directory specified by CRAWLEE_STORAGE_DIR, with request data saved in entries.json.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Scraping in JavaScript\nDESCRIPTION: This code modifies the CATEGORY route handler to enqueue product URLs to a shared request queue instead of directly scraping them. It uses the getOrInitQueue function to access the shared queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nCATEGORY: async ({ $, request, log, crawler }) => {\n    const products = $('li.col-xs-6');\n    log.info(`Number of products found: ${products.length}`);\n\n    const requestQueue = await getOrInitQueue();\n\n    for (const product of products) {\n        const url = `https://warehouse-theme-metal.myshopify.com${$(product).find('a').attr('href')}`;\n        await requestQueue.addRequest({ url, label: 'DETAIL' });\n    }\n\n    const nextPage = $('li.pagination-next a').attr('href');\n    if (nextPage) {\n        await crawler.addRequests([{\n            url: `https://warehouse-theme-metal.myshopify.com${nextPage}`,\n            label: 'CATEGORY',\n        }]);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a request queue in Crawlee, including adding requests, handling them, and checking queue status.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst queue = await RequestQueue.open('my-queue');\n\n// Add requests to the queue\nawait queue.addRequest({ url: 'https://example.com' });\n\n// Get a request from the queue and process it\nconst request = await queue.fetchNextRequest();\nif (request) {\n    // Process the request...\n    console.log(`Processing ${request.url}...`);\n    // Mark the request as handled\n    await queue.markRequestHandled(request);\n}\n\n// Check if queue is empty\nconst isFinished = await queue.isFinished();\nconsole.log(`Is queue finished? ${isFinished}`);\n\n// Get queue info\nconst info = await queue.getInfo();\nconsole.log('Queue info:', info);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Separate Configuration Instance\nDESCRIPTION: Updates the Crawlee crawler initialization to use a new Configuration instance, which ensures each crawler has its own storage and won't interfere with other instances in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js type declarations\nDESCRIPTION: Command to install TypeScript type declarations for Node.js, enabling type-checking for Node.js features in the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Browser Retirement Method Changes\nDESCRIPTION: Example showing the updated method for retiring a browser in BrowserPool compared to the old PuppeteerPool approach.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing in Crawlee\nDESCRIPTION: A simple example of using the enqueueLinks() function without parameters to find all <a> elements on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Attempting to Scrape JavaScript-Rendered Content with CheerioCrawler\nDESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content from Apify Store using CheerioCrawler. It fails to extract the content because CheerioCrawler cannot execute client-side JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Playwright Context Options with LaunchContext\nDESCRIPTION: Demonstrates how to set context options when keeping the default useIncognitoPages value (false), which creates a shared context across all pages launched by one browser. The context options are passed via launchOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n    browserPlugins: [\n        new PlaywrightPlugin(\n            playwright.chromium,\n            {\n                launchOptions: {\n                    deviceScaleFactor: 2,\n                },\n            },\n        ),\n    ],\n});\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot using PuppeteerCrawler with saveSnapshot() Utility\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() utility with Puppeteer directly. It launches a browser, navigates to a URL, and saves a snapshot, which includes both a screenshot and HTML.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { utils } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open a new tab.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Capture the screenshot using the utility function.\n    // This will save both a screenshot and HTML to the default key-value store.\n    await utils.puppeteer.saveSnapshot(page, { key: 'my-snapshot' });\n\n    console.log('Screenshot captured successfully!');\n} finally {\n    // Close Puppeteer.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Data from Worker to Parent Process in Node.js\nDESCRIPTION: This snippet demonstrates how to send scraped data from a worker process back to the parent process in a parallel scraping setup. It's used instead of context.pushData to ensure centralized data storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.send()\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Configuration Class\nDESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee programmatically. This approach allows for dynamic configuration changes in the code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example of disabling browser fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler for Recursive Web Scraping in JavaScript\nDESCRIPTION: This code demonstrates how to use PuppeteerCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and stores results in a dataset. The crawler continues until no more desired links are available.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';\n\nconst startUrls = ['https://news.ycombinator.com'];\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue: await RequestQueue.open(),\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results to default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Enqueue links to next pages\n        await enqueueLinks({\n            selector: '.titleline > a',\n            label: 'detail', // <= assign labels to distinguish between multiple enqueuing\n        });\n\n        // Enqueue pagination\n        await enqueueLinks({\n            selector: '.morelink',\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.addRequests(startUrls);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json configuration\nDESCRIPTION: JavaScript example demonstrating how Crawlee automatically uses configuration from crawlee.json without explicitly importing or passing Configuration to the crawler. This example creates a CheerioCrawler that crawls two URLs with specific sleep times.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Successful Crawler Output\nDESCRIPTION: Shows the successful output when using a headless browser crawler with proper element waiting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in Handler Functions\nDESCRIPTION: Example showing how the new Crawling Context provides a consistent shared object across different handler functions, enabling better state tracking during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Importing ApiLink Component in Markdown\nDESCRIPTION: This snippet imports a custom React component called ApiLink, likely used for creating API reference links in the documentation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK with API Token\nDESCRIPTION: Example showing how to initialize the Apify SDK with an API token using Configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Proxy Usage in Crawlee\nDESCRIPTION: A simple example of how to configure and use proxy URLs in Crawlee by creating a ProxyConfiguration instance and retrieving a new proxy URL for use.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Updated Methods for Page and Browser Management\nDESCRIPTION: Examples showing the changes in browser and page management methods between PuppeteerPool in previous versions and BrowserPool in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Using Browser Lifecycle Hooks with preLaunchHook in JavaScript\nDESCRIPTION: Example of the improved approach using browser-pool lifecycle hooks. This pattern is more consistent across browser types and allows for modular, composable browser configuration functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Extracting All Page Links Using JSDOM\nDESCRIPTION: Shows how to use querySelector with JSDOM to find all anchor tags with href attributes and extract their URLs into an array using modern JavaScript array methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Browser Dependencies\nDESCRIPTION: Commands showing how to install Crawlee with either Playwright or Puppeteer dependencies for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Importing Crawlee Dataset Components\nDESCRIPTION: Shows how to import the necessary Crawlee components for data saving functionality including PlaywrightCrawler and Dataset\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers in Crawlee\nDESCRIPTION: Defines route handlers for different page types (category pages, product detail pages) using Crawlee's Router. Each handler extracts specific data or follows links based on the page type.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Custom Link Selector Configuration\nDESCRIPTION: Example of using a custom CSS selector with enqueueLinks to find specific elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging in with Apify CLI\nDESCRIPTION: Commands to install Apify CLI globally and log in with an API token\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Customizing Link Selection with enqueueLinks in TypeScript\nDESCRIPTION: Demonstrates how to override the default link selection in enqueueLinks by specifying a custom selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Parameters with crawlee.json\nDESCRIPTION: Example of a crawlee.json configuration file that sets the state persistence interval to 10 seconds and log level to DEBUG. This file should be placed in the root of your project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using launchContext with Explicit Options (New Pattern)\nDESCRIPTION: Implementation of the new launchContext pattern that clearly separates Apify-specific options from browser-specific launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Checking Node.js Version for Crawlee Prerequisites\nDESCRIPTION: Confirms if the installed Node.js version meets Crawlee's requirement of v16.0 or higher.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies with Pip\nDESCRIPTION: Commands to install Crawlee and its required dependencies using pipx\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install crawlee[beautifulsoup,curl-impersonate]\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Playwright Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Playwright Crawler in Crawlee. It uses the Sitemap utility class to download and process URLs from a sitemap, then leverages Playwright's browser automation to crawl those URLs and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from '@crawlee/playwright';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs and renders their JavaScript using a headless Chrome browser.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Choose a new link from the page but only from the same hostname\n        await enqueueLinks({\n            transformRequestFunction: (req) => {\n                // Ignore urls that go to the homepage\n                if (req.url === 'https://apify.com/') return false;\n\n                return req;\n            },\n        });\n\n        // Save results as JSON to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Download the sitemap and add the URLs to the queue\nconst sitemap = await Sitemap.load('https://apify.com/sitemap.xml');\nfor (const url of sitemap.urls) {\n    await crawler.addRequests([{ url }]);\n}\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: A simple example showing how to use the enqueueLinks() function without parameters to find all <a> elements on a page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Advanced Autoscaled Pool Configuration in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to configure advanced autoscaled pool options in CheerioCrawler. It includes settings for desired concurrency, scaling ratios, and various intervals.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // ... other options\n    autoscaledPoolOptions: {\n        desiredConcurrency: 10,\n        desiredConcurrencyRatio: 0.9,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 0.5,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 120,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Storage Directory Structure Example\nDESCRIPTION: Shows the file structure for request queue storage on disk, demonstrating how entries are organized in the CRAWLEE_STORAGE_DIR.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result Example\nDESCRIPTION: Expected output after using the reduce method to sum all heading counts. The result is a single value stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler for GCP Cloud Functions\nDESCRIPTION: Updates to the main.js file to configure CheerioCrawler with persistStorage set to false for GCP compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Setting Up GitHub Release Workflow for Version Publication\nDESCRIPTION: This workflow automates the release process for Crawlee. It checks out code, sets up Node.js, installs dependencies, builds the project, and uses semantic-release to determine versioning and publish releases based on commit conventions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: Release\n\non:\n    push:\n        branches: [master]\n\njobs:\n    release:\n        name: Release\n        runs-on: ubuntu-latest\n        steps:\n            - name: Checkout\n              uses: actions/checkout@v3\n              with:\n                  fetch-depth: 0\n            - name: Setup Node.js\n              uses: actions/setup-node@v3\n              with:\n                  node-version: 16\n                  cache: 'npm'\n            - name: Install dependencies\n              run: npm ci\n            - name: Build\n              run: npm run build\n            - name: Release\n              env:\n                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n                  NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n              run: npx semantic-release\n```\n\n----------------------------------------\n\nTITLE: Specifying Dataset Storage Location in Crawlee\nDESCRIPTION: This bash snippet shows the directory structure where individual dataset items are stored in the Crawlee project. Each item is saved as a separate file in the default dataset directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Using Request List in Crawlee\nDESCRIPTION: Shows how to create and use a Request List for crawling a predefined set of URLs without adding more during the run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Manually\nDESCRIPTION: Commands for manual installation of Crawlee and its dependencies for different crawler types\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool Options in Crawler\nDESCRIPTION: Example of configuring BrowserPool options in a PuppeteerCrawler, including setting up lifecycle hooks for browser management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example of how to customize browser fingerprints in PlaywrightCrawler by specifying browser type, version, and operating system to avoid getting blocked during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Choose from chrome, firefox or safari\n        browserType: 'chrome',\n        // Fingerprints are enabled by default\n        useFingerprints: true,\n        // If fingerprints are enabled, you can provide options for the fingerprinting\n        fingerprintOptions: {\n            // Select browsers from Chrome, Firefox and Safari\n            browsers: [{ name: 'chrome', minVersion: 88 }],\n            // Select operating systems\n            operatingSystems: ['windows'],\n            // Set up language, timezone and screen resolution\n            locales: ['cs-CZ', 'sk-SK', 'de-DE'],\n            // And many more, see @crawlee/fingerprint-generator\n        },\n    },\n    async requestHandler({ page }) {\n        // ... page handling\n    },\n});\n\nawait crawler.run(['https://www.example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using Apify Token and CRAWLEE_STORAGE_DIR environment variables. Shows the usage of forceCloud option for accessing platform storages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result Output in Crawlee\nDESCRIPTION: The expected output from the reduce method example, showing the total count of all headers across all pages in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation\nDESCRIPTION: Simple crawler that downloads a single page and extracts its title using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json Configuration\nDESCRIPTION: JavaScript example showing a CheerioCrawler implementation that uses the configuration from crawlee.json without explicitly importing or passing the Configuration to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Category Page Crawling with Playwright\nDESCRIPTION: Implementation of a PlaywrightCrawler that specifically targets category links using selectors and request labeling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/05-crawling.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n\n        // Only run this logic on the main category listing, not on sub-pages.\n        if (request.label !== 'CATEGORY') {\n\n          // Wait for the category cards to render,\n          // otherwise enqueueLinks wouldn't enqueue anything.\n          await page.waitForSelector('.collection-block-item');\n\n          // Add links to the queue, but only from\n          // elements matching the provided selector.\n          await enqueueLinks({\n              selector: '.collection-block-item',\n              label: 'CATEGORY',\n          });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Using Global Configuration\nDESCRIPTION: JavaScript example demonstrating how to access and modify the global configuration object to set persistStateIntervalMillis to 10 seconds using the Configuration class.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function Handler for CheerioCrawler\nDESCRIPTION: Wraps the CheerioCrawler execution in an async handler function that can be exported and used as the entry point for a GCP Cloud Function. It takes request and response objects as parameters and sends the crawler data as the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    return res.send(await crawler.getData())\n}\n```\n\n----------------------------------------\n\nTITLE: Using preLaunchHooks with crawlingContext in Apify PuppeteerCrawler (JavaScript)\nDESCRIPTION: This snippet shows how to use 'preLaunchHooks' with access to the 'crawlingContext' in Apify's PuppeteerCrawler. It demonstrates how to dynamically adjust launch options based on the request's userData, providing greater flexibility in browser configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting Listing Data from Google Maps with Python\nDESCRIPTION: This function extracts structured data from a single Google Maps listing element, including name, rating, reviews, price, link, address, category, and amenities. It handles missing data gracefully and keeps track of processed items.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def _extract_listing_data(self, listing: ElementHandle) -> Optional[Dict]:\n    \"\"\"Extract structured data from a single listing element.\"\"\"\n    try:\n        name_el = await listing.query_selector(\".qBF1Pd\")\n        if not name_el:\n            return None\n        name = await name_el.inner_text()\n        if name in self.processed_names:\n            return None\n\n        elements = {\n            \"rating\": await listing.query_selector(\".MW4etd\"),\n            \"reviews\": await listing.query_selector(\".UY7F9\"),\n            \"price\": await listing.query_selector(\".wcldff\"),\n            \"link\": await listing.query_selector(\"a.hfpxzc\"),\n            \"address\": await listing.query_selector(\".W4Efsd:nth-child(2)\"),\n            \"category\": await listing.query_selector(\".W4Efsd:nth-child(1)\"),\n        }\n\n        amenities = []\n        amenities_els = await listing.query_selector_all(\".dc6iWb\")\n        for amenity in amenities_els:\n            amenity_text = await amenity.get_attribute(\"aria-label\")\n            if amenity_text:\n                amenities.append(amenity_text)\n\n        place_data = {\n            \"name\": name,\n            \"rating\": await elements[\"rating\"].inner_text() if elements[\"rating\"] else None,\n            \"reviews\": (await elements[\"reviews\"].inner_text()).strip(\"()\") if elements[\"reviews\"] else None,\n            \"price\": await elements[\"price\"].inner_text() if elements[\"price\"] else None,\n            \"address\": await elements[\"address\"].inner_text() if elements[\"address\"] else None,\n            \"category\": await elements[\"category\"].inner_text() if elements[\"category\"] else None,\n            \"amenities\": amenities if amenities else None,\n            \"link\": await elements[\"link\"].get_attribute(\"href\") if elements[\"link\"] else None,\n        }\n\n        self.processed_names.add(name)\n        return place_data\n    except Exception as e:\n        context.log.exception(\"Error extracting listing data\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK and CLI for Crawlee Deployment\nDESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deploying Crawlee projects to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Proxy Selection Function in Crawlee\nDESCRIPTION: Demonstrates how to use a custom function to select a proxy URL based on the request URL, allowing for conditional proxy usage depending on the target site.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = new ProxyConfiguration({\n    newUrlFunction: (sessionId, { request }) => {\n        if (request?.url.includes('crawlee.dev')) {\n            return null; // for crawlee.dev, we don't use a proxy\n        }\n\n        return 'http://proxy-1.com'; // for all other URLs, we use this proxy\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Injecting Global Variable using IIFE in JavaScript\nDESCRIPTION: Self-executing function that injects a variable 'injectedVariable' with value 42 into the global window object. Uses an Immediately Invoked Function Expression (IIFE) pattern for encapsulation.\nSOURCE: https://github.com/apify/crawlee/blob/master/test/shared/data/inject_file.txt#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n(function inject(global) {\n    global.injectedVariable = 42;\n}(window));\n```\n\n----------------------------------------\n\nTITLE: Using Browser Lifecycle Hooks in Apify SDK v3 JavaScript\nDESCRIPTION: This example illustrates how to use browser lifecycle hooks in Apify SDK v3. It replaces the deprecated launchPuppeteerFunction with preLaunchHooks, providing more flexibility and consistency across Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Title with Browser JavaScript and JSDOM\nDESCRIPTION: This snippet demonstrates how to retrieve the page title using both browser JavaScript and JSDOM syntax. It highlights the similarity between the two approaches.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Legacy launchPuppeteerOptions Pattern\nDESCRIPTION: Example of the deprecated launchPuppeteerOptions object that mixed Apify-specific options with Puppeteer options in a confusing way.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request List with PuppeteerCrawler in Crawlee\nDESCRIPTION: Code demonstrating how to initialize a RequestList with source URLs and use it with a PuppeteerCrawler. The list is immutable and all URLs must be known in advance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Node Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using the smallest Apify Docker image based on Alpine Linux without any browsers, suitable for CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Apify Platform\nDESCRIPTION: Command to log into the Apify Platform using the CLI tool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Custom Configuration with CheerioCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to create a custom configuration with a shorter state persistence interval and pass it to a CheerioCrawler. The crawler processes two URLs with deliberate delays to demonstrate state persistence behavior, showing how proper configuration ensures statistics are saved before the process terminates.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for TypeScript Crawlee Projects\nDESCRIPTION: Dockerfile using multi-stage build to separately handle build-time dependencies (TypeScript, dev tools) and runtime dependencies, resulting in a smaller production image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Complete E-commerce Crawling with Request Labels in Crawlee\nDESCRIPTION: A complete example that demonstrates crawling both category and product detail pages. It uses request labels to differentiate between page types and handle each appropriately, including pagination through category pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Map Method in Crawlee\nDESCRIPTION: This JavaScript snippet demonstrates how to use the Dataset map method in Crawlee. It filters pages with more than 5 header elements and returns an array of their heading counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst moreThan5headers = await dataset.map((item) => {\n    if (item.headingCount > 5) {\n        return item.headingCount;\n    }\n});\n\n// Save the result to the default key-value store\nconst kvStore = await KeyValueStore.open();\nawait kvStore.setValue('pages-with-more-than-5-headers', moreThan5headers);\n```\n\n----------------------------------------\n\nTITLE: Installing puppeteer-extra and stealth plugin with npm\nDESCRIPTION: Command to install the puppeteer-extra and puppeteer-extra-plugin-stealth packages via npm, which are required for the Puppeteer implementation example.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor with Custom Storage in TypeScript\nDESCRIPTION: Demonstrates how to initialize an Actor with custom storage using @apify/storage-local in a TypeScript environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Converting launchPuppeteerOptions to launchContext in Crawlee\nDESCRIPTION: Shows how to migrate from the old launchPuppeteerOptions object to the new launchContext structure, which clearly separates Apify-specific options from browser-specific launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler\nDESCRIPTION: Puppeteer-based crawler implementation for handling multiple URLs. Requires 'apify/actor-node-puppeteer-chrome' Docker image when running on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data Structure for Dataset Operations in Crawlee\nDESCRIPTION: A sample JSON array structure that represents data scraped from web pages, storing URLs and their respective heading counts. This data is stored in the default dataset location and can be populated using dataset.pushData().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with HttpCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with HttpCrawler which provides built-in session management. The crawler automatically handles session rotation and proxy usage for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new HttpCrawler({\n    // Enable automatic proxy IP address rotation\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    proxyConfiguration,\n    // Limits connection for avoiding proxy ban\n    maxConcurrency: 50,\n    // If you plan to use proxy do not forget session pool options\n    sessionPoolOptions: {\n        // Overrides default Session pool configuration\n        sessionOptions: {\n            // All other session options work normally\n            maxUsageCount: 5,\n        },\n\n        // Let's have 100 sessions in our pool\n        maxPoolSize: 100,\n    },\n    async requestHandler({ request, response, session }) {\n        console.log(`Processing ${request.url}...`);\n\n        if (response.statusCode !== 200) {\n            session.markBad();\n            throw new Error(`Request ${request.url} failed with status code ${response.statusCode}`);\n        }\n\n        console.log(`URL: ${request.url}, Status code: ${response.statusCode}`);\n\n        session.markGood();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Playwright Multi-Browser Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with support for multiple browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, SDK v1 doesn't bundle puppeteer package, so it needs to be installed separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Crawlee\nDESCRIPTION: Shows how to perform basic operations with datasets including writing single and multiple rows of data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for the documentation page, defining the ID, title, and description.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\nid: introduction\ntitle: Introduction\ndescription: Your first steps into the world of scraping with Crawlee\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Manually for CheerioCrawler\nDESCRIPTION: Command to manually install Crawlee for use with CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Image for Apify Actor with Node.js\nDESCRIPTION: This Dockerfile sets up an environment for an Apify actor using a Node.js base image. It installs npm packages, copies source code, and configures the run command. The build process is optimized for speed using Docker layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Using Main Method in TypeScript\nDESCRIPTION: Shows how to initialize and exit an Actor using the wrapper method Actor.main() which handles initialization and cleanup automatically\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage for Crawlee with Apify Platform\nDESCRIPTION: Example of how to use @apify/storage-local with Crawlee when running on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Operations in Crawlee\nDESCRIPTION: This snippet illustrates how to perform basic operations with datasets in Crawlee, including writing single and multiple rows to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Basic Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates core operations with key-value stores including reading input, writing output, opening named stores, and managing values with automatic JSON conversion\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Map Method Result in JavaScript\nDESCRIPTION: The expected output of the map method example, showing an array of heading counts that are greater than 5.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending from @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Product Scraping Logic with Playwright in JavaScript\nDESCRIPTION: Combines all the scraping techniques to extract comprehensive product information including URL, manufacturer, title, SKU, price, and stock availability.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output from Crawlee Scraping\nDESCRIPTION: Example of JSON output stored by Crawlee in the storage/datasets directory after scraping the Crawlee website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Requests in Batches to Request Queue in Crawlee\nDESCRIPTION: Demonstrates how to add multiple requests to a Request Queue in a single batch operation using the addRequests() function in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst queue = await RequestQueue.open();\n\n// Prepare a list of URLs\nconst urls = [\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n    'http://www.example.com/page-3',\n];\n\n// Add multiple requests to the queue in a single call\nawait queue.addRequests(urls);\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue: queue,\n    async requestHandler({ page, request }) {\n        // Process the page\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Migrating from gotoFunction to Navigation Hooks in Crawlee\nDESCRIPTION: Example showing how to replace the complex `gotoFunction` pattern with simpler `preNavigationHooks` and `postNavigationHooks` arrays, which provide better separation of concerns for page navigation customization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.reduce() to Calculate Totals\nDESCRIPTION: Example showing how to use Dataset.reduce() to aggregate data from all dataset items. This code calculates the total number of headings across all pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Advanced Apify Proxy Configuration with Groups and Location\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection. This example shows how to use only Residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration for TypeScript Projects\nDESCRIPTION: Comprehensive package.json configuration that includes type module setting, main entry point, dependencies, development dependencies, and scripts for development and production builds.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript for Crawlee Projects\nDESCRIPTION: Demonstrates the recommended TypeScript configuration for Crawlee projects, including extending from @apify/tsconfig and setting appropriate compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() in Crawlee\nDESCRIPTION: Demonstrates how to explicitly initialize and exit the Actor environment using async/await. The init() method sets up storage and event handling, while exit() performs teardown operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example demonstrating the proper use of BrowserController for browser operations like closing and setting cookies in a browser-agnostic way.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Custom Configuration\nDESCRIPTION: Sets up a basic Cheerio crawler with custom configuration to use in-memory storage for AWS Lambda compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration for Crawlee\nDESCRIPTION: Creates a basic ProxyConfiguration instance using a custom URL. The ProxyConfiguration allows the crawler to use proxies when making requests to websites.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: ['http://proxy.example.com:8000'],\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize Apify project configuration and create necessary files\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawling with Crawlee in JavaScript\nDESCRIPTION: This code demonstrates how to use Crawlee's BasicCrawler to download web pages and store their content. It uses the sendRequest utility function to make HTTP requests and saves the raw HTML and URL to a default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\n// Create an instance of the BasicCrawler class - a crawler\n// that just downloads and processes URLs.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    // It accepts a single parameter, which is an object with options as:\n    // {\n    //    request: Request,\n    //    sendRequest: Function,\n    //    autoscaledPool: AutoscaledPool,\n    //    crawler: BasicCrawler,\n    //    session: Session,\n    // }\n    async requestHandler({ request, sendRequest, crawler }) {\n        // sendRequest() is not available on the request object\n        // because that would make no sense. We use sendRequest()\n        // from context instead.\n        const response = await sendRequest(request);\n\n        // Store the HTML content and URL to the default dataset.\n        await crawler.pushData({\n            url: request.url,\n            html: response.body,\n        });\n    },\n});\n\n// Run the crawler with initial request urls\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://example.com',\n    'https://apify.com',\n]);\n\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Command to install TypeScript compiler as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee JSON Output\nDESCRIPTION: Example of the JSON data structure stored in the output dataset by Crawlee when extracting information from a website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration for TypeScript Crawlee Project\nDESCRIPTION: Complete package.json configuration for a TypeScript Crawlee project with all necessary dependencies and scripts for development, building, and production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in TypeScript\nDESCRIPTION: Import statement to include the Dataset module from Crawlee for data storage functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry for v3.13.2\nDESCRIPTION: Changelog entry documenting the addition of an onSkippedRequest option feature and related references.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/linkedom-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.2](https://github.com/apify/crawlee/compare/v3.13.1...v3.13.2) (2025-04-08)\n\n### Features\n\n* add `onSkippedRequest` option ([#2916](https://github.com/apify/crawlee/issues/2916)) ([764f992](https://github.com/apify/crawlee/commit/764f99203627b6a44d2ee90d623b8b0e6ecbffb5)), closes [#2910](https://github.com/apify/crawlee/issues/2910)\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer Crawler in TypeScript\nDESCRIPTION: This snippet shows how to download and crawl URLs from a sitemap using Puppeteer Crawler. It uses the downloadListOfUrls utility from @crawlee/utils to extract URLs from a sitemap and processes them using Puppeteer for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { downloadListOfUrls } from '@crawlee/utils';\nimport { PuppeteerCrawler, sleep } from 'crawlee';\n\n// Create the crawler and provide the required configuration\nconst crawler = new PuppeteerCrawler({\n    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    maxRequestsPerCrawl: 20,\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, enqueueLinks, page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Add any links from the page to the queue\n        await enqueueLinks();\n\n        // Wait for a random amount of time to avoid overloading the site\n        await sleep(Math.random() * 2000);\n    },\n    // This function is called if the page processing failed more than maxRequestRetries+1 times.\n    failedRequestHandler({ request, log }) {\n        log.info(`Request ${request.url} failed too many times`);\n    },\n});\n\n// Get the list of URLs from the sitemap\nconst urls = await downloadListOfUrls({ url: 'https://apify.com/sitemap.xml' });\n\n// Add the URLs to the crawler queue\nawait crawler.addRequests(urls);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: This example shows how to use SessionPool as a standalone component without a crawler. It demonstrates creating and configuring a session pool, managing sessions manually, and making HTTP requests with sessions through a proxy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool, ProxyConfiguration, gotScraping } from 'crawlee';\n\n// Create your session pool\nconst sessionPool = new SessionPool({\n    // You can use the proxy configuration with multiple proxies\n    proxyConfiguration: new ProxyConfiguration({\n        proxyUrls: [\n            'http://user:password@proxy-1.com:8000',\n            'http://user:password@proxy-2.com:8000',\n        ],\n    }),\n\n    // Options as listed in BasicCrawler example\n    maxPoolSize: 100,\n});\n\nawait sessionPool.initialize();\n\n// Now we can get the session and use it in our requests\nconst session = await sessionPool.getSession();\n\n// We can use the session with various HTTP clients\n// or just get the proxy URL\nconst proxyUrl = session.proxyUrl;\n\n// Example with the gotScraping package\nconst response = await gotScraping({\n    url: 'https://api.example.com/v2/user/1',\n    proxyUrl,\n    // If you want to use cookies from session\n    headers: {\n        Cookie: session.getCookieString('https://api.example.com'),\n    },\n});\n\nif (response.statusCode !== 200) {\n    // If the response is not successful,\n    // we mark the session as bad, so it will be retried later\n    session.markBad();\n} else {\n    // Otherwise mark it good\n    session.markGood();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in Handler Functions\nDESCRIPTION: Demonstrates how to access crawler properties like requestQueue and autoscaledPool through the new crawler object in the Crawling Context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with CheerioCrawler\nDESCRIPTION: Demonstrates how to set up CheerioCrawler with proxy configuration for HTML parsing, showing how to extract and process data from web pages through proxies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ $, request }) {\n        console.log(`Processing: ${request.url}`);\n        const title = $('title').text();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with CheerioCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with CheerioCrawler for handling proxy rotation and maintaining sessions. The crawler processes HTML content using Cheerio library while managing sessions automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new CheerioCrawler({\n    // Enable automatic proxy IP address rotation\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    proxyConfiguration,\n    // Limits connection for avoiding proxy ban\n    maxConcurrency: 50,\n    // Called for each URL\n    async requestHandler({ request, response, $, session }) {\n        console.log(`Processing ${request.url}...`);\n\n        if (response.statusCode !== 200) {\n            session.markBad();\n            throw new Error(`Request ${request.url} failed with status code ${response.statusCode}`);\n        }\n\n        const title = $('title').text();\n        console.log(`URL: ${request.url}, Title: ${title}`);\n\n        session.markGood();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function for Cheerio Crawler\nDESCRIPTION: This code defines an AWS Lambda handler function that initializes and runs a CheerioCrawler. It ensures a new crawler instance is created for each Lambda invocation to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Limit in Cheerio Crawler\nDESCRIPTION: Demonstrates how to limit the number of requests processed by the crawler using the maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Context ID in SDK v1\nDESCRIPTION: Example showing how to use the new context ID property to reference and manipulate data across different crawling contexts, enabling more complex automation workflows.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue Explicitly with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to create and configure a Request Queue explicitly and then use it with a Crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Open the default request queue\nconst requestQueue = await RequestQueue.open();\n\n// Add URLs to the queue\nawait requestQueue.addRequest({ url: 'https://example.com' });\n\n// Create a crawler that will use our request queue\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n        \n        // Add discovered links to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration\nDESCRIPTION: Shows how to create a proxy configuration using Apify Proxy. This code demonstrates the basic setup for using Apify's proxy services for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: CheerioCrawler Log Output\nDESCRIPTION: Log entries from a CheerioCrawler instance showing the crawling of multiple pages on crawlee.dev, including the home page and documentation sections. Each entry displays the page URL and its title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Session Management with PuppeteerCrawler\nDESCRIPTION: Shows how to use proxy session management with PuppeteerCrawler. This maintains proxy consistency for requests within the same session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, page, session }) {\n        // The session ensures that the same proxy is used for certain requests\n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local datasets and platform-stored datasets using the Actor class when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables are configured.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build and main entry point\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Output JSON\nDESCRIPTION: Example of the JSON output format produced by Crawlee, showing the structure of scraped data stored in the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Storage with Actor\nDESCRIPTION: Example showing how to use @apify/storage-local instead of the default memory storage with the Actor class, with optional configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler with Implicit RequestQueue\nDESCRIPTION: Demonstrates a more concise approach to creating a crawler by using the implicit RequestQueue and passing URLs directly to the run method. This simplifies the code while achieving the same functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: PlaywrightCrawler Without Element Waiting\nDESCRIPTION: This code demonstrates what happens when you try to access elements with PlaywrightCrawler without waiting for them to render. It will fail to extract content because the elements are not yet available in the DOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// This will fail because we're trying to access an element\n// that has not been rendered to the DOM yet\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    maxRequestRetries: 3,\n    async requestHandler({ page, request }) {\n        // This will fail, because we're not waiting for the element\n        // to be rendered into the DOM\n        const actorText = await page.$('.ActorStoreItem');\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Context Information\nDESCRIPTION: Example showing how to access information about the browser context, such as proxy configuration and session data, through the BrowserController.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Sitemaps from String Reference (Markdown)\nDESCRIPTION: Reference to a feature that enables loading sitemaps from a string, including GitHub issue and commit references.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n* Loading sitemaps from string ([#2496](https://github.com/apify/crawlee/issues/2496)) ([38ed0d6](https://github.com/apify/crawlee/commit/38ed0d6ad90a868df9c02632334fec8db9ef29a0)), closes [#2460](https://github.com/apify/crawlee/issues/2460)\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee Project to Apify Platform\nDESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This code snippet demonstrates how to disable the use of browser fingerprints in PlaywrightCrawler. It sets the 'useFingerprints' option to false in the browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Complete package.json configuration for TypeScript project\nDESCRIPTION: Full package.json configuration including module type setting for ES Modules, all dependencies, and scripts for development, production, and building the TypeScript project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example of disabling browser fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Event Handling Migration Example\nDESCRIPTION: Demonstrates how to migrate from the older Apify.events pattern to the new Actor.on pattern for event handling in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK and CLI for Crawlee Deployment\nDESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for authentication and deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV using Apify\nDESCRIPTION: Shows how to export an entire dataset to a single CSV file stored in the default key-value store using the Dataset.exportToValue function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Get the dataset\nconst dataset = await Actor.openDataset();\n\n// Export the entire dataset to a single CSV file\nawait dataset.exportToValue('OUTPUT.csv', {\n    format: 'csv',\n});\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Apify Proxy in Crawlee\nDESCRIPTION: Shows how to initialize and use Apify Proxy in Crawlee. This code creates a proxy configuration and generates a new proxy URL that can be used for web scraping with automatic IP rotation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support. This option allows using Firefox and Webkit (Safari) browsers in addition to Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with npm\nDESCRIPTION: Command to install the Crawlee package using npm. This is the initial step for setting up a project with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser in TypeScript\nDESCRIPTION: This code demonstrates how to set up PlaywrightCrawler to use Firefox browser instead of the default Chromium. It shows the configuration for launching Firefox with the playwright-firefox browser name and includes proper request handling and data extraction logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Firefox browser.\nconst crawler = new PlaywrightCrawler({\n    // Use the Firefox browser\n    browserType: 'firefox',\n    \n    // Here you can set options that are passed to the playwright .launch() function.\n    launchContext: {\n        launchOptions: {\n            headless: true,\n        },\n    },\n\n    // Stop crawling after several pages\n    maxRequestsPerCrawl: 50,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Interacting with a React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This example demonstrates how to use JSDOMCrawler to interact with a React calculator app by clicking buttons and extracting results. It navigates to the calculator, clicks on buttons '1', '+', '1', '=', and then extracts the calculation result.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, parseWithCheerio }) {\n        const { document } = window;\n\n        // Click the buttons that form the equation\n        document.querySelector('.calculator [value=\"1\"]')?.dispatchEvent(new window.MouseEvent('click'));\n        document.querySelector('.calculator [value=\"+\"]')?.dispatchEvent(new window.MouseEvent('click'));\n        document.querySelector('.calculator [value=\"1\"]')?.dispatchEvent(new window.MouseEvent('click'));\n        document.querySelector('.calculator [value=\"=\"]')?.dispatchEvent(new window.MouseEvent('click'));\n\n        // Extract the result\n        const displayElement = document.querySelector('.calculator .display');\n        const result = displayElement ? displayElement.textContent?.trim() : null;\n\n        // Store the result\n        await Dataset.pushData({\n            result,\n        });\n\n        // Use parseWithCheerio to parse the HTML using Cheerio\n        const $ = parseWithCheerio();\n        console.log('Page title:', $('title').text());\n    }\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production runtime\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Crawlee Dataset Results\nDESCRIPTION: Sample JSON output from a Crawlee crawl showing the structure of saved results, which are stored as JSON files in the storage/datasets directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK Authentication\nDESCRIPTION: JavaScript code showing how to initialize the Apify SDK with an API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Custom Configuration with CheerioCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to create a custom Configuration object with a shorter state persistence interval and pass it to a CheerioCrawler. The code includes a simple crawler that processes two URLs sequentially with deliberate delays to demonstrate state persistence behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in Typescript\nDESCRIPTION: This code demonstrates using the 'SameHostname' strategy with enqueueLinks() in CheerioCrawler to enqueue only links that point to the same hostname as the current URL, including relative links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // The crawler will automatically process the requests in the queue\n    async requestHandler({ request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Add all links from page to RequestQueue that have the same hostname\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n            // Optional: Provide a link selector to use for calculating the urls\n            // LinkSelectorToBeAdded\n        });\n    },\n});\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Current Proxy in PlaywrightCrawler with TypeScript\nDESCRIPTION: This code shows how to inspect and log the current proxy details within the PlaywrightCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo }) {\n        if (proxyInfo) {\n            console.log(`Proxy URL: ${proxyInfo.url}`);\n        }\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using preNavigationHooks with CheerioCrawler\nDESCRIPTION: Example of using preNavigationHooks to adjust gotOptions before making HTTP requests in CheerioCrawler. This allows customization of request parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/cheerio-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\npreNavigationHooks: [\n    (crawlingContext, gotOptions) => {\n        // ...\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns\nDESCRIPTION: Example of using glob patterns with enqueueLinks to filter which URLs should be enqueued, in this case limiting to URLs on apify.com.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Using the Unified Crawling Context in SDK v1\nDESCRIPTION: Example demonstrating the new unified Crawling Context in SDK v1 where all handler functions share the same context object, making it easier to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.12.0\nDESCRIPTION: Bug fix to ignore errors from iframe content extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/playwright-crawler/CHANGELOG.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n# [3.12.0](https://github.com/apify/crawlee/compare/v3.11.5...v3.12.0) (2024-11-04)\n\n\n### Bug Fixes\n\n* ignore errors from iframe content extraction ([#2714](https://github.com/apify/crawlee/issues/2714)) ([627e5c2](https://github.com/apify/crawlee/commit/627e5c2fbadce63c7e631217cd0e735597c0ce08)), closes [#2708](https://github.com/apify/crawlee/issues/2708)\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Commands to install Apify CLI and log in with an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Custom Input Validation with Try-Catch\nDESCRIPTION: This snippet shows how to implement custom input validation using try-catch. It allows for more complex validation logic and custom error handling when the input doesn't meet requirements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/motivation.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor, log } from 'apify';\n\nawait Actor.main(async () => {\n    let input;\n    try {\n        // Get input and perform custom validation\n        input = await Actor.getInput<{\n            startUrls: string[];\n            maxItems: number;\n        }>();\n\n        if (!input) throw new Error('Input is missing!');\n        if (!input.startUrls || !input.startUrls.length) throw new Error('startUrls must be provided!');\n        if (input.maxItems !== undefined && (typeof input.maxItems !== 'number' || input.maxItems <= 0)) {\n            throw new Error('maxItems must be a positive number!');\n        }\n\n        // Set default values\n        input.maxItems = input.maxItems || 10;\n    } catch (error) {\n        log.error(`Input validation failed: ${error.message}`);\n        await Actor.fail();\n        return;\n    }\n\n    // ... rest of the actor code with validated input\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy in PuppeteerCrawler\nDESCRIPTION: Variable reference InspectionPuppeteerSource showing proxy inspection in PuppeteerCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionPuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Map Method Result Example\nDESCRIPTION: Expected output after using the map method to filter heading counts greater than 5. The result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output Format from Google Maps Scraper in Python\nDESCRIPTION: Sample JSON structure showing the expected output format from the Google Maps scraper, including hotel name, rating, reviews, price, amenities, and link.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"Vividus Hotels, Bangalore\",\n    \"rating\": \"4.3\",\n    \"reviews\": \"633\",\n    \"price\": \"3,667\",\n    \"amenities\": [\n        \"Pool available\",\n        \"Free breakfast available\",\n        \"Free Wi-Fi available\",\n        \"Free parking available\"\n    ],\n    \"link\": \"https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/...\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for building Crawlee TypeScript projects\nDESCRIPTION: Multi-stage Dockerfile for building and running Crawlee TypeScript projects, optimizing for production use.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Browser Pool Configuration with Lifecycle Hooks\nDESCRIPTION: Example showing how to configure BrowserPool with lifecycle hooks and custom options in a PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Crawlee Actor Implementation using Actor.main()\nDESCRIPTION: Example of how to implement a Cheerio crawler as an Apify actor using the Actor.main() method, which handles initialization and cleanup automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nAwait Actor.main(async () => {\n    const startUrls = [\n        'https://crawlee.dev',\n    ];\n\n    const crawler = new CheerioCrawler({\n        async requestHandler({ request, $, enqueueLinks, log }) {\n            const title = $('title').text();\n            log.info(`Title of ${request.url} is '${title}'`);\n\n            // Save results to dataset\n            await Dataset.pushData({\n                url: request.url,\n                title,\n            });\n\n            // Enqueue all links from the page\n            await enqueueLinks();\n        },\n    });\n\n    await crawler.run(startUrls);\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Context in Pre-Launch Hooks\nDESCRIPTION: Example showing how pre-launch hooks can access the crawling context of the request that triggered the browser launch via crawler.crawlingContexts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Context in Browser Launch Hooks\nDESCRIPTION: Demonstrates how to access the crawling context of a request that triggered a browser launch using the crawler.crawlingContexts Map.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to disable the use of browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all scripts and dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Build Configuration\nDESCRIPTION: Multi-stage Dockerfile setup for building and running TypeScript Crawlee projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Using Auto-saved Crawler State\nDESCRIPTION: Demonstrates the useState() method which provides an automatically saved state object. Changes to this state are persisted automatically when the persistState event occurs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Basic Node.js Docker Image\nDESCRIPTION: Docker configuration for the lightweight actor-node image based on Alpine Linux, ideal for CheerioCrawler without browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Shows how to clean up default storage directories using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Shows a streamlined approach to creating a crawler using the crawler.run() method with direct URL input, eliminating the need for manual request queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output of Amazon Product Data\nDESCRIPTION: Sample JSON output showing structured product data extracted from an Amazon product page including title, price, ratings, images, and product attributes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"ASUS ROG Strix G16 (2023) Gaming Laptop, 16\\\" 16:10 FHD 165Hz, GeForce RTX 4070, Intel Core i9-13980HX, 16GB DDR5, 1TB PCIe SSD, Wi-Fi 6E, Windows 11, G614JI-AS94, Eclipse Gray\",\n    \"price\": 1799.99,\n    \"listPrice\": 1999.99,\n    \"reviewRating\": 4.3,\n    \"reviewCount\": 372,\n    \"imageUrls\": [\n        \"https://m.media-amazon.com/images/I/41EWnXeuMzL._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/51gAOHZbtUL._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/51WLw+9ItgL._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/41D-FN8qjLL._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/41X+oNPvdkL._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/41X6TCWz69L._AC_US40_.jpg\",\n        \"https://m.media-amazon.com/images/I/31rphsiD0lL.SS40_BG85,85,85_BR-120_PKdp-play-icon-overlay__.jpg\"\n    ],\n    \"attributes\": [\n        {\n            \"label\": \"Brand\",\n            \"value\": \"ASUS\"\n        },\n        {\n            \"label\": \"Model Name\",\n            \"value\": \"ROG Strix G16\"\n        },\n        {\n            \"label\": \"Screen Size\",\n            \"value\": \"16 Inches\"\n        },\n        {\n            \"label\": \"Color\",\n            \"value\": \"Eclipse Gray\"\n        },\n        {\n            \"label\": \"Hard Disk Size\",\n            \"value\": \"1 TB\"\n        },\n        {\n            \"label\": \"CPU Model\",\n            \"value\": \"Intel Core i9\"\n        },\n        {\n            \"label\": \"Ram Memory Installed Size\",\n            \"value\": \"16 GB\"\n        },\n        {\n            \"label\": \"Operating System\",\n            \"value\": \"Windows 11 Home\"\n        },\n        {\n            \"label\": \"Special Feature\",\n            \"value\": \"Anti Glare Coating\"\n        },\n        {\n            \"label\": \"Graphics Card Description\",\n            \"value\": \"Dedicated\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Base Package\nDESCRIPTION: Command to install the main Crawlee package which contains most @crawlee/* packages and crawler classes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler and Cheerio\nDESCRIPTION: An alternative setup using PlaywrightCrawler with Cheerio for HTML parsing. This approach demonstrates how to combine Playwright for navigation with Cheerio for efficient HTML manipulation and data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { load } from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, log }) {\n        const html = await page.content();\n        const $ = load(html);\n\n        $('.collection-block-item').each((_, el) => {\n            const categoryText = $(el).text();\n            log.info(`Category: ${categoryText}`);\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n```\n\n----------------------------------------\n\nTITLE: Declaring Missing Dependency in JavaScript\nDESCRIPTION: Fixes a bug by declaring a missing dependency on 'tslib'. This addresses issue #1747.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/jsdom-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n* declare missing dependency on `tslib` ([27e96c8](https://github.com/apify/crawlee/commit/27e96c80c26e7fc31809a4b518d699573cb8c662)), closes [#1747](https://github.com/apify/crawlee/issues/1747)\n```\n\n----------------------------------------\n\nTITLE: Using actor-node Docker Image\nDESCRIPTION: Example of using the smallest Apify Docker image based on Alpine Linux. This image is best used with CheerioCrawler and does not include any browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Complete Crawling Implementation with PlaywrightCrawler in Crawlee\nDESCRIPTION: Demonstrates a full implementation of crawling category and product detail pages, handling pagination, and using request labels for different page types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/05-crawling.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            // We're not doing anything with the details yet.\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in TypeScript\nDESCRIPTION: Implementation for crawling multiple specified URLs using the Playwright Crawler in Crawlee. This version uses Playwright for browser automation, extracting the page title from each site in the list.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, PlaywrightCrawlingContext } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the web pages in a headless browser.\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerMinute: 100,\n\n    // On error, retry each page at most once.\n    maxRequestRetries: 1,\n\n    // Increase the timeout for processing of each page.\n    requestHandlerTimeoutSecs: 30,\n\n    // This function will be called for each URL to crawl.\n    // The argument context contains information about the crawler,\n    // the URL, and other utilities.\n    async requestHandler({ page, request, log }: PlaywrightCrawlingContext) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n    },\n});\n\n// These links are from the Crawlee documentation.\nconst usingSiteNavigation = [\n    'https://crawlee.dev/',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/guides/request-storage',\n    'https://crawlee.dev/docs/guides/result-storage',\n];\n\n// Run the crawler with the defined requests.\nawait crawler.run(usingSiteNavigation);\n```\n\n----------------------------------------\n\nTITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in Crawlee\nDESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save the images directly to a key-value store. It utilizes the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/skip-navigation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, sendRequest, log }) {\n        // Check if the URL is an image\n        if (request.url.endsWith('.jpg') || request.url.endsWith('.png')) {\n            log.info(`Downloading image ${request.url}`);\n            // Skip navigation for image URLs\n            request.skipNavigation = true;\n            \n            // Use sendRequest to fetch the image data\n            const imageBuffer = await sendRequest();\n            \n            // Save the image to the default key-value store\n            await KeyValueStore.getDefaultInstance().setValue(request.url, imageBuffer, { contentType: 'image/jpeg' });\n        } else {\n            // Handle other URLs as normal\n            log.info(`Processing ${request.url}`);\n            // ... rest of your crawling logic\n        }\n    }\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Setting Concurrency Limits in Crawlee\nDESCRIPTION: Demonstrates how to configure minimum and maximum concurrent requests in a CheerioCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    minConcurrency: 1,\n    maxConcurrency: 100,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Using State Management in Crawlee for Python\nDESCRIPTION: Example showing how to use the new use_state crawling context helper to create and manage persistent state values within a crawler, allowing data to be maintained across different runs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee import Request\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    # Create a crawler with purge_on_start disabled to retain state across runs.\n    crawler = ParselCrawler(\n        configuration=Configuration(purge_on_start=False),\n    )\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info(f'Crawling {context.request.url}')\n\n        # Retrieve or initialize the state with a default value.\n        state = await context.use_state('state', default_value={'runs': 0})\n\n        # Increment the run count.\n        state['runs'] += 1\n\n    # Create a request with always_enqueue enabled to bypass deduplication and ensure it is processed.\n    request = Request.from_url('https://crawlee.dev/', always_enqueue=True)\n\n    # Run the crawler with the start request.\n    await crawler.run([request])\n\n    # Fetch the persisted state from the key-value store.\n    kvs = await crawler.get_key_value_store()\n    state = await kvs.get_auto_saved_value('state')\n    crawler.log.info(f'Final state after run: {state}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Custom CSS Selector for enqueueLinks in Crawlee\nDESCRIPTION: Shows how to override the default selector in enqueueLinks to find links within specific div elements with the 'has-link' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Getting Public URL for a Key-Value Store Item\nDESCRIPTION: Example showing how to create a publicly accessible URL for an item stored in an Apify Key-Value Store, useful for sharing files stored on the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Cheerio Crawler in TypeScript\nDESCRIPTION: This snippet shows a basic setup of a CheerioCrawler that crawls a single URL and logs the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing tsconfig Dependencies\nDESCRIPTION: Command to install Apify's TypeScript configuration package.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Managing Storage Path for Crawlee Request Storage\nDESCRIPTION: Explains how Crawlee stores request data on local disk, using the CRAWLEE_STORAGE_DIR environment variable or defaulting to ./storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Using Apify Storage Public URLs\nDESCRIPTION: Example showing how to get a public URL for an item stored in Apify Key-Value Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with WebKit browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Using Custom Cookie Jar with SendRequest\nDESCRIPTION: Example showing how to use a custom cookie jar from the tough-cookie package with sendRequest to manage cookies during web scraping sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing ProxyConfiguration with Custom Proxy URLs in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ]\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure Example\nDESCRIPTION: Shows the directory structure pattern used by datasets in Crawlee for storing data files locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom configuration for Crawlee, set up a CheerioCrawler with this configuration, and define URL-specific handlers. It showcases the use of sleep function for delays and request queuing. The example is designed to illustrate the impact of custom persistStateIntervalMillis setting on crawler statistics.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Querying Elements by CSS Selector in JavaScript Console\nDESCRIPTION: This JavaScript/TypeScript code demonstrates how to use document.querySelectorAll() to select all elements with a specific CSS class. It's used to verify that the selector '.collection-block-item' correctly targets only the desired collection cards (31 in total) on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: ts\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Using Full Playwright Docker Image\nDESCRIPTION: Dockerfile configuration for using Apify's full Playwright image. This large image includes all Playwright browsers (Chromium, Chrome, Firefox, WebKit), suitable for multi-browser development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Retry Settings in Scrapy\nDESCRIPTION: Settings configuration in Scrapy to control the retry behavior for failed requests, including enabling retries, setting the number of retry attempts, and specifying which HTTP status codes should trigger retries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nRETRY_ENABLED = True\nRETRY_TIMES = 2  # Number of retry attempts\nRETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524]  # HTTP error codes to retry\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Wrapper Function in TypeScript\nDESCRIPTION: Example of using the Actor.main() wrapper function which handles initialization and termination automatically. This approach encapsulates the user code in a callback function and simplifies error handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Modifying Context Options with prePageCreateHooks\nDESCRIPTION: Demonstrates how to use prePageCreateHooks to dynamically set context options before page creation. This approach allows customizing context options for each page without specifying them directly in the newPage call.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n    browserPlugins: [\n        new PlaywrightPlugin(\n            playwright.chromium,\n            {\n                useIncognitoPages: true,\n                launchOptions: {\n                // launch options\n                    headless: false,\n                    devtools: true,\n                },\n            },\n        ),\n    ],\n    prePageCreateHooks: [\n        (pageId, browserController, pageOptions) => {\n            pageOptions.deviceScaleFactor = 2;\n            pageOptions.colorScheme = 'dark';\n            pageOptions.locale = 'de-DE';\n\n            // You must modify the 'pageOptions' object, not assign to the variable.\n            // pageOptions = {deviceScaleFactor: 2, ...etc} => This will not work!\n        },\n    ],\n});\n\n// Launches Chromium with Playwright and returns a Playwright Page.\nconst page = await browserPool.newPage();\n```\n\n----------------------------------------\n\nTITLE: New Launch Context Configuration in JavaScript\nDESCRIPTION: Demonstrates the new explicit launch context configuration pattern that clearly separates Apify and Puppeteer options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing PlaywrightCrawler with Configuration for GCP Cloud Run\nDESCRIPTION: Sets up a PlaywrightCrawler instance with a custom Configuration to disable persistent storage, which is necessary for running in a serverless environment like GCP Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Custom Storage Configuration\nDESCRIPTION: Basic setup of PlaywrightCrawler with custom Configuration instance to prevent storage interference between Lambda instances\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset in Crawlee\nDESCRIPTION: This snippet shows how to import the Dataset class from Crawlee along with PlaywrightCrawler to enable data storage functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Using Auto-saved Crawler State\nDESCRIPTION: Example demonstrating how to use the crawler's useState method to maintain state that is automatically persisted. This allows for simple state management without manual saving.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI Globally\nDESCRIPTION: Command to install the Apify CLI tool globally for managing Apify projects and deployments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Default Storages in Crawlee\nDESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Using Platform Storage in Local Actor\nDESCRIPTION: Example of getting a public URL for an item stored in a Key-Value Store on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Updated Access to AutoscaledPool in SDK v1\nDESCRIPTION: Shows the correct way to access autoscaledPool in SDK v1, where it has been moved under the crawler property rather than being directly available on the context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Commands for installing Crawlee and its dependencies, showing different installation options for various use cases.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n\nnpm install @crawlee/cheerio\n\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Container for Crawlee Actor with Node.js and Playwright\nDESCRIPTION: This Dockerfile sets up a container environment for a Crawlee actor based on Node.js with Playwright and Chrome. It optimizes the build process by using Docker layer caching, installs only production dependencies to keep the image small, and configures the container to run the actor on startup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Scraping\nDESCRIPTION: Modified route handler that enqueues product URLs to a request queue with locking support instead of directly processing them. This allows multiple worker processes to consume and process these URLs in parallel.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nroutes: [\n    {\n        label: 'CATEGORY',\n        async handler({ log, crawler, $ }) {\n            // We'll use our queue with locking support instead of the default crawler queue\n            const requestQueue = await getOrInitQueue();\n\n            const products = $('a.product-item');\n            log.info(`Found ${products.length} products on the page.`);\n\n            for (const product of products) {\n                const productUrl = $(product).attr('href');\n                log.debug(`Enqueueing product detail: ${productUrl}`);\n                \n                // Add the product URL to our queue with locking support\n                await requestQueue.addRequest({\n                    url: productUrl,\n                    label: 'DETAIL',\n                });\n            }\n\n            const nextPage = $('a.pagination__next');\n            if (nextPage.length > 0) {\n                const nextPageUrl = $(nextPage).attr('href');\n                log.info(`Pagination found, enqueueing next page: ${nextPageUrl}`);\n                await crawler.addRequests([{\n                    url: nextPageUrl,\n                    label: 'CATEGORY',\n                }]);\n            }\n        },\n    },\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU Using Playwright\nDESCRIPTION: Demonstrates how to extract the SKU (Stock Keeping Unit) of a product using Playwright. The selector targets a span element with the specific class 'product-meta__sku-number'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI globally\nDESCRIPTION: Command to install the Apify CLI tool globally, which helps with authentication and deployment to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Running PlaywrightCrawler in Headful Mode\nDESCRIPTION: Example showing how to configure PlaywrightCrawler to run in headful mode (with a visible browser window) which is useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Let's open browsers in headful mode so that we can see what's going on.\n    headless: false,\n    // Let's slow it down a bit to make it more visible.\n    preNavigationHooks: [() => new Promise(resolve => setTimeout(resolve, 300))],\n\n    // Your logic here\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom HTTP Client with fetch in TypeScript\nDESCRIPTION: A skeleton implementation of the BaseHttpClient interface using the standard fetch API. This demonstrates the required structure for a custom HTTP client that can be used with Crawlee's BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { type BaseHttpClient, type Request, type RequestOptions } from '@crawlee/core';\n\nexport class FetchHttpClient implements BaseHttpClient {\n    // Used for `sendRequest`\n    async call(options: RequestOptions): Promise<any> {\n        const abortController = new AbortController();\n\n        const fetchOptions = {\n            method: options.method,\n            headers: options.headers,\n            body: options.payload,\n            signal: abortController.signal,\n        };\n\n        if (options.timeoutSecs) {\n            setTimeout(() => abortController.abort(), options.timeoutSecs * 1000);\n        }\n\n        const response = await fetch(options.url, fetchOptions);\n\n        return {\n            statusCode: response.status,\n            headers: Object.fromEntries(response.headers.entries()),\n            body: await response.text(),\n        };\n    }\n\n    // Used for plain HTTP crawling\n    async requestAsBrowser(originalRequest: Request): Promise<any> {\n        const result = await fetch(originalRequest.url, {\n            method: originalRequest.method,\n            headers: originalRequest.headers,\n            body: originalRequest.payload,\n        });\n\n        return {\n            request: {\n                url: originalRequest.url,\n            },\n            headers: Object.fromEntries(result.headers.entries()),\n            statusCode: result.status,\n            body: await result.text(),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using proxyUrl with sendRequest in BasicCrawler\nDESCRIPTION: Example of manually passing a proxy URL to the sendRequest function in BasicCrawler. This allows requests to be routed through a proxy server to hide the real IP address.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands for installing @sparticuz/chromium and creating a zip file of the node_modules directory to be uploaded as a Lambda Layer.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with 'All' Strategy in TypeScript\nDESCRIPTION: This snippet shows how to use enqueueLinks with the 'all' strategy to follow every link, regardless of its domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Declarations\nDESCRIPTION: Command to install type declarations for Node.js which enables proper type-checking for Node.js features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Using Platform Storage in Local Actor\nDESCRIPTION: JavaScript code demonstrating how to use Apify platform storage when developing locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Running Headful Browsers with PuppeteerCrawler\nDESCRIPTION: JavaScript code snippet showing how to set up PuppeteerCrawler to run in headful mode for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    headless: false,\n    requestHandler: async ({ page, request, enqueueLinks, log }) => {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n        await Dataset.pushData({ title, url: request.loadedUrl });\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for TypeScript Build\nDESCRIPTION: Adds a build script to compile TypeScript and specifies the main entry point for the compiled code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to combine Request Queue with Request List for scenarios where you have both initial URLs and need to dynamically add more during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Prepare a batch of initial URLs\nconst initialSources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Combine request list and request queue\nconst requestList = await RequestList.open('my-list', initialSources);\nconst requestQueue = await RequestQueue.open();\n\n// The crawler processes requests from the list via the queue\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Process the page (extract data, take page screenshot, etc).\n\n        // We can add new discovered links to the queue\n        await enqueueLinks();\n    },\n});\n\n// Start the crawl\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Finding Saved Data Location in Bash\nDESCRIPTION: File path showing where Crawlee saves dataset files by default. The data is stored in numbered JSON files in the storage/datasets/default directory within the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: This example demonstrates how to create a custom Configuration instance with a modified persistStateIntervalMillis setting of 10 seconds and pass it to a CheerioCrawler. The crawler processes two example URLs with timed delays to demonstrate state persistence behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU using Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to use Playwright to extract the product SKU from a web page using a CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Using Playwright-Chrome Docker Image\nDESCRIPTION: Dockerfile configuration for using Apify's Playwright-Chrome image. Similar to the Puppeteer-Chrome image but for Playwright, supporting CheerioCrawler and PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue with Request List in Crawlee\nDESCRIPTION: Shows how to use both Request Queue and Request List together, where initial URLs from the list are enqueued into the queue before processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\n// Define the starting URLs\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Initialize Request List\nconst requestList = await RequestList.open('my-list', sources);\n\n// Initialize Request Queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Create a crawler that will use both requestList and requestQueue\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    // optionally (depending on the use-case) we could also use the `startUrls` option here\n    \n    async requestHandler({ request, page, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // extract links and add them to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Deprecated launchPuppeteerFunction Pattern in JavaScript\nDESCRIPTION: Example of the old launchPuppeteerFunction pattern for customizing browser launch behavior. This approach required users to manually call Apify.launchPuppeteer with modified options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command for installing Apify SDK v1 with Puppeteer support. Unlike previous versions, Puppeteer is no longer bundled with the SDK and must be installed separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Creating a Cloud Function Handler with CheerioCrawler\nDESCRIPTION: Implements a handler function that uses CheerioCrawler to scrape data and return the results through the GCP Cloud Function response object. This function serves as the entry point for the cloud function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Per-Instance Storage in AWS Lambda\nDESCRIPTION: Updates the Crawlee initialization code to use a separate Configuration instance for each crawler. This prevents interference between multiple crawler instances running in the same Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Map Method Usage with Dataset in Crawlee\nDESCRIPTION: Demonstrates using the map method on a Dataset to transform data by filtering for pages with more than 5 headers. The result is stored in the key-value store for further use.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Running CheerioCrawler as an Actor Using init() and exit()\nDESCRIPTION: Example of a CheerioCrawler implementation using Actor.init() and Actor.exit() functions to properly initialize and terminate the actor on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, createCheerioRouter } from 'crawlee';\n\n// Initialize the Actor\nawait Actor.init();\n\n// Create a request list\nconst startUrls = ['https://crawlee.dev'];\n\n// Create a router\nconst router = createCheerioRouter();\n\n// Add a route for the start URL\nrouter.addDefaultHandler(async ({ $, request, enqueueLinks }) => {\n    const title = $('title').text();\n    console.log(`The title of ${request.url} is '${title}'`);\n\n    // Save results\n    await Actor.pushData({\n        title,\n        url: request.url,\n    });\n\n    // Enqueue links to more pages\n    await enqueueLinks();\n});\n\n// Create a crawler\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\n// Run the crawler\nawait crawler.run(startUrls);\n\n// Exit the Actor\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify CLI\nDESCRIPTION: Commands to install Apify CLI and log in with an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of Crawlee for PuppeteerCrawler\nDESCRIPTION: Command to manually install Crawlee and Puppeteer for use with PuppeteerCrawler using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain without enqueueLinks\nDESCRIPTION: Manual implementation of filtering links to stay on the same domain without using the enqueueLinks helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, crawler }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // find all links and add them to the crawling queue\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n\n        // Get the hostname from the URL\n        const { hostname } = new URL(request.url);\n\n        for (const url of links) {\n            try {\n                // Skip invalid URLs\n                const parsedUrl = new URL(url, request.loadedUrl);\n                // Skip other domains\n                if (hostname !== parsedUrl.hostname) continue;\n                // Add URL to the queue\n                await crawler.addRequests([{ url: parsedUrl.href }]);\n            } catch (err) {\n                // Skip invalid URLs\n            }\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Demonstrates the correct usage of BrowserController for managing browser instances and performing browser-specific operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration Example\nDESCRIPTION: Example of configuring custom storage using @apify/storage-local with Actor initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Actor Docker Environment\nDESCRIPTION: This Dockerfile sets up a containerized environment for a Crawlee actor. It uses the apify/actor-node-playwright-chrome image as base, optimizes the build process by separating package installation and code copying, and configures the container to run the actor with npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Implementing Crawlee Playwright Crawler with Cheerio Parsing\nDESCRIPTION: A variation of the PlaywrightCrawler that uses Cheerio for HTML parsing. This demonstrates how to combine the capabilities of Playwright for browser automation with Cheerio for efficient HTML parsing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        if (request.url.includes('/collections/all')) {\n            // We are now on a collection page.\n            log.info('I am on a collection page!');\n            \n            // Get the content of the page\n            const html = await page.content();\n            \n            // Use cheerio to parse the HTML, just like with CheerioCrawler\n            const $ = cheerio.load(html);\n            \n            // Now we can use the familiar cheerio syntax to query the DOM\n            $('.collection-block-item').each((i, el) => {\n                log.info(`CATEGORY TEXT: ${$(el).text()}`);\n            });\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: Demonstrates how to get a product's SKU using a specific class selector with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Starting Crawlee Project\nDESCRIPTION: Commands to navigate to project directory and start the crawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route Handler for Inter-Process Communication in Crawlee\nDESCRIPTION: Modified detail route handler that sends scraped data back to the parent process instead of using the standard Crawlee data storage. This enables data collection from multiple parallel scraper instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Original router.addHandler for DETAIL label\nrouter.addHandler(Labels.DETAIL, async ({ request, page, log }) => {\n    const title = await page.title();\n    log.info(`${title}`, { url: request.loadedUrl });\n\n    // Get the detail data\n    const h1Text = await page.$eval('h1', (el) => el.textContent);\n    const price = await page.$eval('.detail-price', (el) => el.textContent);\n\n    // Instead of saving to the dataset, we want to send the data to the parent process\n    // Otherwise each worker will have its own dataset, and we want a single combined dataset\n    const data = {\n        title: h1Text,\n        price,\n        url: request.loadedUrl,\n    };\n\n    // If this is a worker thread, send the data to the parent process\n    if (process.env.IS_WORKER_THREAD) {\n        process.send(data);\n    } else {\n        // Otherwise, we're running in the initial scraper, so we can just push the data\n        await context.pushData(data);\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Playwright Crawler in TypeScript\nDESCRIPTION: This example demonstrates using PlaywrightScraper to crawl URLs from a sitemap. It uses the Sitemap utility class to extract URLs from a sitemap, then processes each page with Playwright's browser automation for modern web scraping with JavaScript support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightScraper } from '@crawlee/playwright';\nimport { Sitemap } from '@crawlee/utils';\n\nconst scraper = new PlaywrightScraper({\n    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests\n});\n\nscraper.router.addDefaultHandler(async ({ page, request, log, pushData }) => {\n    const title = await page.title();\n    log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n    // Save results as JSON to ./storage/datasets/default\n    await pushData({ title, url: request.loadedUrl });\n});\n\n// Create an instance of the Sitemap class\n// and download the sitemap\nconst sitemap = await Sitemap.load('https://crawlee.dev/sitemap.xml');\n\n// Add URLs from the sitemap as requests to the queue\nawait scraper.addRequests(sitemap.urls);\n\nawait scraper.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee with Node.js 16\nDESCRIPTION: This Dockerfile creates an environment for running Crawlee actors. It uses the apify/actor-node:16 base image, installs production dependencies efficiently using layer caching, copies the application code, and configures the container to start with 'npm start'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Map Method in Crawlee\nDESCRIPTION: Example of using the Dataset map method to transform data by filtering for pages with more than 5 headers. The result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprint Generation with sendRequest in TypeScript\nDESCRIPTION: Example demonstrating how to customize browser fingerprint generation options when using sendRequest. This allows specification of devices, locales, operating systems, and browsers for generating realistic HTTP headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route Handler for Inter-Process Communication\nDESCRIPTION: Updated detail route handler that sends scraped data back to the parent process using process.send instead of pushing it to the default storage. This enables data collection across multiple worker processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    label: 'DETAIL',\n    async handler({ log, $, request }) {\n        const title = $('h1.heading-lg').text().trim();\n        log.info(`Scraping product: ${title}`, { url: request.url });\n\n        const price = $('.price').text().trim();\n        const description = $('.description').text().trim();\n        const inStockElement = $('.shipping-time');\n        const inStock = inStockElement.length > 0 && inStockElement.text().trim() !== 'Out of stock';\n\n        // Get poster image URL\n        const imageElement = $('.product-image');\n        const imageUrl = imageElement.attr('src');\n\n        const result = {\n            url: request.url,\n            title,\n            description,\n            price,\n            inStock,\n            imageUrl\n        };\n\n        // Instead of pushing to dataset, send data back to parent process if we're a worker\n        if (process.env.IS_WORKER_THREAD) {\n            process.send(result);\n        } else {\n            // Otherwise, push to dataset as usual\n            await context.pushData(result);\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: JavaScript snippet that finds all anchor elements with href attributes on a page and extracts their URLs into an array. This demonstrates a common scraping pattern using the JSDOM API.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install and run a new Crawlee project using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue File Directory Structure\nDESCRIPTION: Shows the directory structure and file location where Request Queue data is stored in the local filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Apify Platform\nDESCRIPTION: Command to log in to Apify Platform using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Crawlee\nDESCRIPTION: Full JavaScript implementation of the AWS Lambda handler function that runs a Crawlee crawler. This code wraps the crawler initialization and execution in an async handler function that Lambda will execute, and returns a proper response object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Using PlaywrightCrawler in Crawlee for Python\nDESCRIPTION: Example demonstrating how to use Crawlee's PlaywrightCrawler to crawl a website, extract its title and content. The code shows the basic setup with an asynchronous handler function and crawler initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/07-05-launching-crawlee-python/index.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext\n\n\nasync def main() -> None:\n    # Create a crawler instance\n    crawler = PlaywrightCrawler(\n        # headless=False,\n        # browser_type='firefox',\n    )\n\n    @crawler.router.default_handler\n    async def request_handler(context: PlaywrightCrawlingContext) -> None:\n        data = {\n            'request_url': context.request.url,\n            'page_url': context.page.url,\n            'page_title': await context.page.title(),\n            'page_content': (await context.page.content())[:10000],\n        }\n        await context.push_data(data)\n\n    await crawler.run(['https://crawlee.dev'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with a Crawler\nDESCRIPTION: Demonstrates how to use a request queue implicitly with a crawler by adding requests directly through the crawler's methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // The crawler automatically uses the default RequestQueue\n        // We don't need to create it explicitly\n\n        // Add more requests to the queue\n        await enqueueLinks();\n    },\n});\n\n// Add the initial requests and start the crawl\nawait crawler.addRequests([\n    { url: 'https://crawlee.dev' },\n]);\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Auto-saved Crawler State in TypeScript\nDESCRIPTION: Shows how to use the crawler.useState() method to maintain state that is automatically saved when the persistState event occurs. This simplifies state management in crawlers by handling caching and persistence automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Shows how to explicitly purge default storages including the request queue and request list data before starting a crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/request_storage.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL with JavaScript String Manipulation\nDESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string into parts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler Using utils.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.saveSnapshot() method. It creates a crawler that visits multiple URLs and saves snapshots of each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, utils }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const key = new URL(request.url).hostname;\n        await utils.saveSnapshot(page, { key });\n    },\n});\n\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Extracting and Processing Product Price with Playwright\nDESCRIPTION: This snippet shows how to extract a product price from a webpage, filter for the element containing the dollar sign, and process the string into a numeric value by removing formatting characters like commas and dollar signs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with preLaunchHook in Apify PuppeteerCrawler (JavaScript)\nDESCRIPTION: This example demonstrates how to replace the 'launchPuppeteerFunction' with a 'preLaunchHook' in the browserPoolOptions of Apify's PuppeteerCrawler. This change allows for more flexibility and consistency across both Puppeteer and Playwright implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Rate Limits in Crawlee\nDESCRIPTION: Example showing how to set maximum requests per minute in a CheerioCrawler to limit request rates. This helps prevent overwhelming target websites with too many requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    // Only allow 120 requests to be done per minute\n    maxRequestsPerMinute: 120,\n    async requestHandler({ $ }) {\n        // Process the data here...\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Modifying CheerioCrawler for GCP with persistStorage disabled\nDESCRIPTION: This code shows how to initialize a CheerioCrawler with a separate Configuration instance that disables storage persistence for GCP compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Custom Storage Settings\nDESCRIPTION: Basic Crawlee configuration setup with custom storage settings to prevent interference between crawler instances in Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Sample Dataset Structure in JSON\nDESCRIPTION: Example JSON structure showing scraped data stored in the default dataset, containing URLs and their corresponding header counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Min and Max Concurrency in CheerioCrawler\nDESCRIPTION: This example shows how to set the minConcurrency and maxConcurrency options in CheerioCrawler. It sets a minimum of 5 concurrent requests and a maximum of 10.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // ... other options\n    minConcurrency: 5,\n    maxConcurrency: 10,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json File\nDESCRIPTION: A basic crawlee.json configuration file that sets the state persistence interval to 10 seconds and changes the log level to DEBUG. This file should be placed at the root of your project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee and Playwright for PlaywrightCrawler\nDESCRIPTION: Command to manually install Crawlee and Playwright for use with PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Apify Actor Locally\nDESCRIPTION: Commands to create a new Apify actor project and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context Map for Cross-Context Access\nDESCRIPTION: Demonstrates how to use the new crawling context map to access data across different contexts, useful for complex scraping scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Browser Dependencies\nDESCRIPTION: Commands for installing Crawlee with Playwright or browser-specific dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright in JavaScript\nDESCRIPTION: This snippet combines all the previous extraction techniques into a complete solution for scraping product information. It extracts the URL, manufacturer, title, SKU, price, and stock availability information from a product page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Extraction with Playwright in JavaScript\nDESCRIPTION: This comprehensive snippet combines all the extraction techniques to scrape complete product information including URL, manufacturer, title, SKU, price, and stock availability. It demonstrates the full scraping process for a product detail page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling Using CheerioCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl specific links on a website by using the globs pattern matching feature. The crawler processes the crawlee.dev website and only enqueues links that match the documentation pages pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, createCheerioRouter } from 'crawlee';\n\nconst router = createCheerioRouter();\n\nrouter.addDefaultHandler(async ({ log, $ }) => {\n    log.info('Title', { title: $('title').text() });\n});\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    requestHandler: router,\n    // This will make the crawler only crawl links from crawlee.dev domain\n    requestHandlerTimeoutSecs: 10,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\nconst title = await router.use(async ({ $, log }) => {\n    const pageTitle = $('title').text();\n    log.info('Title', { title: pageTitle });\n});\n\nawait crawler.addRequests([\n    {\n        url: 'https://crawlee.dev',\n        // Only enqueue links to documentation pages\n        userData: {\n            globs: ['https://crawlee.dev/docs/**']\n        }\n    }\n]);\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-chrome Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with Chrome. This image can be used with CheerioCrawler and PlaywrightCrawler, but not PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json\nDESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. This file should be placed in the root of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Playwright Crawler in TypeScript\nDESCRIPTION: This snippet demonstrates using Playwright Crawler to crawl all links on a website. It configures a PlaywrightCrawler, uses enqueueLinks() to add new links to the queue, and processes each page by extracting the title and URL. It also shows how to add custom labels to requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            transformRequestFunction(req) {\n                // Add a custom label to the request\n                req.label = 'my-label';\n                return req;\n            },\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit the number of requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a list of URLs using Playwright Crawler. It sets up a RequestQueue with multiple URLs, configures the crawler to process each page by extracting the title using browser automation with Playwright, and stores the results using Dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create a request queue\nconst requestQueue = await RequestQueue.open();\n\n// Add multiple URLs to the queue\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://apify.com' },\n    { url: 'https://sdk.apify.com' },\n]);\n\n// Create a crawler\nconst crawler = new PlaywrightCrawler({\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        // Extract data from the page\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the data to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor\nDESCRIPTION: This Dockerfile sets up a containerized environment for running a Crawlee-based web scraping actor. It uses a multi-stage build process to leverage Docker layer caching, installing only production dependencies to keep the image size small.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Puppeteer Crawler in Crawlee\nDESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl multiple specified URLs. It sets up the crawler, processes each page to extract the title, and saves the results to the default dataset. It requires the 'apify/actor-node-puppeteer-chrome' image for the Dockerfile when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Map Method Result Output in Crawlee\nDESCRIPTION: The expected output from the map method example, showing an array of heading counts that are greater than 5.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: SendRequest API Implementation in Crawlee\nDESCRIPTION: The internal implementation of the sendRequest function, showing how it uses gotScraping with various configuration options and how it integrates with session management and proxy handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueueV2 for Standalone Usage\nDESCRIPTION: Shows how to create and use the new RequestQueueV2 class outside of crawlers. This implementation supports the request locking feature for parallel processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for Development and Production\nDESCRIPTION: Adds scripts for running the project in development and production modes, and sets the module type to ES modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"start:prod\": \"node dist/main.js\"\n    },\n    \"type\": \"module\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing CheerioCrawler Manually\nDESCRIPTION: Command to manually install Crawlee for using CheerioCrawler in an existing Node.js project.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform via CLI\nDESCRIPTION: Command to log in to the Apify Platform using the Apify CLI, which requires a personal access token from the Apify console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This snippet shows how to disable browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // ...\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee Projects\nDESCRIPTION: TypeScript configuration file showing recommended settings for Crawlee projects, including module settings and compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Crawler in Crawlee\nDESCRIPTION: Demonstrates how to use a Request Queue implicitly with a Crawler in Crawlee. The crawler automatically manages the queue, adding new requests discovered during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, enqueueLinks }) {\n        // Extract data from the page\n        // ...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Basic Example (TypeScript)\nDESCRIPTION: Shows the basic way to initialize and exit an Actor using direct init/exit calls. Demonstrates async/await pattern with top level await support in Node 16.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with URL Patterns in Playwright Crawler\nDESCRIPTION: Example showing how to use the enqueueLinks method with glob patterns to filter URLs in a PlaywrightCrawler. This demonstrates the pattern matching capability for targeting specific URL formats.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Crawlee Actor with Node.js and Playwright\nDESCRIPTION: This Dockerfile sets up a container environment for running Crawlee actors with Playwright and Chrome. It implements best practices like layer caching with selective copying of package files first, installing only production dependencies, and setting appropriate permissions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-firefox Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with Firefox pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It sets up a crawler to visit multiple URLs, take screenshots, and save them to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        const screenshot = await page.screenshot();\n        const key = request.url.replace(/[:/]/g, '_');\n        await Actor.setValue(key, screenshot, { contentType: 'image/png' });\n    },\n});\n\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Extracting and Formatting Current Price using Playwright in JavaScript\nDESCRIPTION: This code block shows how to extract the current price, filter the correct element, and convert the price string to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This code shows how to turn off browser fingerprinting functionality in PuppeteerCrawler by setting useFingerprints to false, which may be useful in scenarios where fingerprinting is not needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // Other crawler options...\n});\n\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using Configuration Class\nDESCRIPTION: Example showing how to use the Configuration class to programmatically set configuration options. This approach accesses the global configuration singleton and sets the persistStateIntervalMillis parameter.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue and Adding Requests in Crawlee\nDESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a request to it. It's used to set up the initial URL for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium for AWS Lambda Environment in Crawlee\nDESCRIPTION: Updates the Crawlee code to use the Chromium path from @sparticuz/chromium and sets necessary browser arguments for AWS Lambda environment. This addresses hardware limitations in Lambda for browser execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Map Method Result Example in JavaScript\nDESCRIPTION: The expected output of the map method example, showing an array containing heading counts for pages with more than 5 headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Autoscaled Pool Options in CheerioCrawler\nDESCRIPTION: This example demonstrates how to set advanced autoscaledPoolOptions for a CheerioCrawler, including desiredConcurrency, scaleUpStepRatio, and other fine-tuning parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/scaling_crawlers.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    autoscaledPoolOptions: {\n        desiredConcurrency: 5,\n        desiredConcurrencyRatio: 0.9,\n        scaleUpStepRatio: 0.05,\n        scaleDownStepRatio: 0.05,\n        maybeRunIntervalSecs: 0.5,\n        loggingIntervalSecs: 60,\n        autoscaleIntervalSecs: 10,\n        maxTasksPerMinute: 200,\n    },\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Error Log from Headless Browser Without Waiting\nDESCRIPTION: This log shows the error that occurs when attempting to access elements before they've been rendered in the DOM, demonstrating the importance of waiting for elements in dynamic pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/javascript-rendering.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Express Server for Crawlee in GCP Cloud Run\nDESCRIPTION: Wraps a Crawlee crawler with an Express HTTP server to handle client requests in GCP Cloud Run. The server listens on the port specified by the GCP environment and returns crawling results as the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Shorthand String Configuration for ProxyConfiguration\nDESCRIPTION: Demonstrates using a shorthand string notation to create a ProxyConfiguration with specific proxy groups. This is a more concise way to specify proxy groups.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Shorthand notation using just a string\nconst proxyConfiguration = new ProxyConfiguration('groups=GROUP1,GROUP2');\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration for Crawlee\nDESCRIPTION: TypeScript code example demonstrating how to configure custom storage for Crawlee using ApifyStorageLocal.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Building Multi-stage Dockerfile for Crawlee Actor with Node.js and Playwright\nDESCRIPTION: Defines a multi-stage Docker build process for a Crawlee actor. The first stage builds the application with development dependencies, while the second stage creates a production-optimized image with minimal dependencies. Includes configuration for headful browser support via XVFB.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY --chown=myuser . ./\n\nRUN npm run build\n\nFROM apify/actor-node-playwright-chrome:16\n\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Complete Product Scraping Implementation with Playwright\nDESCRIPTION: This combined snippet demonstrates the full implementation of scraping all the required product information from a product page, including URL, manufacturer, title, SKU, current price, and stock availability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Request Objects in Crawlee\nDESCRIPTION: Demonstrates how to create a custom Request object with additional user data and labels in Crawlee. This allows passing custom information between crawler operations and implementing specialized handling based on request type.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/motivation.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Request, CheerioCrawler } from 'crawlee';\n\n// Create a custom request with userData and label\nconst request = new Request({\n    url: 'https://example.com/product/1234',\n    userData: {\n        productId: '1234',\n        category: 'electronics',\n        referrer: 'homepage'\n    },\n    label: 'PRODUCT_DETAIL',  // Use label to differentiate request types\n});\n\n// Use the request with a crawler\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, $ }) {\n        // You can access the userData in the handler\n        const { productId, category } = request.userData;\n        \n        // Handle different request types based on label\n        if (request.label === 'PRODUCT_DETAIL') {\n            // Extract product details\n            const productName = $('.product-name').text();\n            const price = $('.price').text();\n            \n            // Process the data...\n        }\n    },\n});\n\nawait crawler.run([request]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This code demonstrates how to capture a screenshot using the saveSnapshot() utility function from Crawlee. This function automatically handles generating keys and saving to the key-value store based on the current page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\nimport { puppeteerUtils } from '@crawlee/puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open new page.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Save a screenshot of the page to the default key-value store.\n    // The key will be automatically generated based on the URL and content type.\n    const savedScreenshot = await puppeteerUtils.saveSnapshot(page, {\n        key: 'my-screenshot',\n        saveHtml: false,\n        // Puppeteer screenshot options\n        fullPage: true,\n        // ... other puppeteer screenshot options\n    });\n    console.log(`Screenshot saved to ${savedScreenshot.keyValueStoreName} with key: ${savedScreenshot.key}`);\n} finally {\n    // Close browser.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to set up a recursive web crawler using PuppeteerCrawler from the Crawlee library. It handles URL discovery, request queueing, page processing, and data extraction from a website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.loadedUrl,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Adding Apify SDK as a Dependency\nDESCRIPTION: Command to add the Apify SDK for Python as a project dependency using UV package manager. This SDK provides integration with the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nuv add apify\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer-Extra Dependencies\nDESCRIPTION: Command to install puppeteer-extra and its stealth plugin using npm package manager\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData()\nDESCRIPTION: Code that shows how to save extracted results to a dataset using the pushData() method, which creates a new row in the dataset table.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TSConfig\nDESCRIPTION: Command to install Apify's TypeScript configuration package.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Creating TypeScript Configuration File\nDESCRIPTION: TypeScript configuration file (tsconfig.json) that extends Apify's base configuration and enables ES2022 features including top-level await. It specifies the output directory and includes all files in the src directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing PuppeteerCrawler Manually\nDESCRIPTION: Command to manually install Crawlee with Puppeteer for using PuppeteerCrawler in an existing Node.js project.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Crawlee\nDESCRIPTION: Demonstrates basic dataset operations including writing single and multiple rows to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify project configuration\nDESCRIPTION: Command to initialize the project for Apify Platform deployment. Creates an .actor directory with configuration files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Adding multiple requests in batches with crawler.addRequests()\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method. This function handles large volumes of requests efficiently by processing them in smaller batches to avoid API rate limits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages in TypeScript\nDESCRIPTION: Examples of how to install Crawlee packages using npm, including the full meta-package and individual crawler modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List in Crawlee\nDESCRIPTION: Shows how to use both a request queue and a request list together in a Crawlee crawler, demonstrating their combined functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\nconst requestList = await RequestList.open('my-list', [\n    { url: 'https://example.com/1' },\n    { url: 'https://example.com/2' },\n    { url: 'https://example.com/3' },\n]);\n\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ request, page, enqueueLinks }) {\n        // ...\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Context Information\nDESCRIPTION: Example showing how to access information about the browser context, including proxy and session details, using the browserController's launchContext property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Code snippet showing how to disable the dynamic browser fingerprints feature in PlaywrightCrawler by setting useFingerprints to false in browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Up Docker Environment for Crawlee Actor with Node.js 16\nDESCRIPTION: This Dockerfile configures a container environment for Crawlee actors. It uses the apify/actor-node:16 base image, installs production dependencies while skipping development and optional ones, copies source code, and specifies the start command.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-firefox Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with Firefox pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()\nDESCRIPTION: This snippet demonstrates how to capture screenshots when using PuppeteerCrawler. It defines a crawler that visits multiple URLs, takes a screenshot of each page, and saves them to a key-value store with URL-based keys.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore, PuppeteerCrawler } from 'crawlee';\n\n// Get the default Key-value store\nconst store = await KeyValueStore.open();\n\n// Create the crawler\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page }) {\n        // Wait for the page to fully load\n        await page.waitForSelector('body');\n\n        // Get the title of the page\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Create a key from the URL\n        const key = `screenshot-${request.id}.png`;\n\n        // Capture the screenshot\n        const screenshotBuffer = await page.screenshot({ fullPage: true });\n        \n        // Save the screenshot to the default key-value store\n        await store.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n    },\n});\n\n// Add requests and run the crawler\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: Extracts the manufacturer name from a product URL by splitting the URL string. The code handles URLs that follow the pattern '/products/<manufacturer>-product-name'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context IDs for Cross-Context Access\nDESCRIPTION: Example showing how to track and access different crawling contexts using their IDs, enabling cross-context communication during crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json File\nDESCRIPTION: Example of a crawlee.json configuration file that sets persistStateIntervalMillis to 10000 milliseconds and logLevel to DEBUG. This file should be placed in the root of your project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue and Adding URLs in Crawlee\nDESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to it for crawling. This is the first step in setting up a crawler to visit web pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with AWS-compatible Chromium Setup\nDESCRIPTION: This JavaScript snippet shows how to integrate @sparticuz/chromium with PlaywrightCrawler, supplying the Lambda-compatible Chromium executable path and the necessary browser arguments for AWS Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Result of Dataset.map() operation\nDESCRIPTION: The expected output when using the map method to filter header counts greater than 5. This result would be saved to the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using Configuration Class\nDESCRIPTION: Example demonstrating how to use the Configuration class to set global configuration options programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Environment for Crawlee Actor with Playwright-Chrome\nDESCRIPTION: Configures a Docker container for running Crawlee actors with Playwright and Chrome. The setup process is optimized by using layer caching, installing only production dependencies, and handling file permissions. The base image includes Node.js 20 with Playwright and Chrome pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Implementing a Crawler with crawlee.json Configuration\nDESCRIPTION: Example showing how to use a CheerioCrawler without explicit configuration code, relying instead on the crawlee.json file for configuration settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue in Crawlee\nDESCRIPTION: Shows how to create a RequestQueue instance and add a URL to crawl using Crawlee's RequestQueue class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee Projects\nDESCRIPTION: Multi-stage Dockerfile for building and running Crawlee projects, optimizing for production deployment with TypeScript compilation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Interacting with React Calculator using JSDOMCrawler in TypeScript\nDESCRIPTION: This code demonstrates how to use JSDOMCrawler to interact with a React calculator app by clicking buttons and extracting results. It programmatically clicks 1, +, 1, = buttons and extracts the calculation result from the DOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, request, log }) {\n        const { document } = window;\n        log.info(`Processing ${request.url}...`);\n\n        // Click calculator buttons programmatically\n        document.querySelector('.calculator button[data-key=\"1\"]')?.click();\n        document.querySelector('.calculator button[data-key=\"+\"]')?.click();\n        document.querySelector('.calculator button[data-key=\"1\"]')?.click();\n        document.querySelector('.calculator button[data-key=\"=\"]')?.click();\n\n        // Extract the result\n        const result = document.querySelector('.calculator-display')?.textContent;\n\n        log.info(`Result: ${result}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            result,\n        });\n    },\n});\n\nawait crawler.run(['https://ahfarmer.github.io/calculator/']);\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a predefined list of URLs using Cheerio Crawler. It sets up a RequestQueue with multiple URLs, configures the crawler to process each page by extracting the title, and stores the results using Dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset, RequestQueue } from 'crawlee';\n\n// Create a request queue\nconst requestQueue = await RequestQueue.open();\n\n// Add multiple URLs to the queue\nawait requestQueue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://apify.com' },\n    { url: 'https://sdk.apify.com' },\n]);\n\n// Create a crawler\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        // Extract data from the page\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the data to dataset\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n});\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TypeScript Configuration\nDESCRIPTION: Command to install Apify's base TypeScript configuration which contains recommended compiler options and settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Using Crawler's Stop Method in Crawlee for Python\nDESCRIPTION: Example demonstrating the use of the new stop method in BasicCrawler and its subclasses, allowing the crawling process to be halted when a specific condition is met.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    crawler = ParselCrawler()\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info('Crawling: %s', context.request.url)\n\n        # Extract and enqueue links from the page.\n        await context.enqueue_links()\n\n        title = context.selector.css('title::text').get()\n\n        # Condition when you want to stop the crawler, e.g. you\n        # have found what you were looking for.\n        if 'Crawlee for Python' in title:\n            context.log.info('Condition met, stopping the crawler.')\n            await crawler.stop()\n\n    await crawler.run(['https://crawlee.dev'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Scraping Single URL with got-scraping in Crawlee\nDESCRIPTION: Demonstrates how to use got-scraping to fetch and process HTML content from a single URL. Shows integration with Crawlee's basic functionality for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\nconst response = await gotScraping('https://crawlee.dev');\nconsole.log(response.body);\n```\n\n----------------------------------------\n\nTITLE: Using Map Method with Crawlee Dataset\nDESCRIPTION: Demonstrates how to use the Dataset.map() method to transform dataset items by filtering pages that have more than 5 header elements, returning an array of header counts that meet the condition.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nawait Dataset.open();\nconst moreThan5headers = await Dataset.map((item) => {\n    if (item.headingCount > 5) {\n        return item.headingCount;\n    }\n    return undefined;\n});\n\nconsole.log('Pages with more than 5 headers:', moreThan5headers);\n\nconst store = await KeyValueStore.open();\nawait store.setValue('pages-with-more-than-5-headers', moreThan5headers);\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-webkit Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with WebKit pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all dependencies and scripts for both development and production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Web Crawling with PuppeteerCrawler\nDESCRIPTION: Example showing how to perform recursive crawling of the Crawlee website using PuppeteerCrawler, which uses a headless browser for JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Let's limit our crawls to just a few pages to prevent\n    // overwhelming the website and our dataset.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can perform the extraction logic.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save the results to the dataset where we can access them later.\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            // Consider only links that contain 'docs'.\n            globs: ['https://crawlee.dev/**'],\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Deploying Bluesky Scraper to Apify Platform Using CLI\nDESCRIPTION: Instructions for deploying the Bluesky API scraper to the Apify Platform using the Apify CLI. Includes steps for authentication, pushing the project, and running the Actor on the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool with Lifecycle Hooks\nDESCRIPTION: Example demonstrating how to configure the BrowserPool with lifecycle hooks, allowing customization of browser behavior such as switching between headless and headful mode based on request properties.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with Configuration from crawlee.json\nDESCRIPTION: JavaScript example showing a Cheerio crawler using configuration from crawlee.json without explicitly importing or setting configuration in the code. Demonstrates how crawler behavior is affected by configuration options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration for CheerioCrawler in JavaScript\nDESCRIPTION: Example demonstrating how to create a custom Configuration with a 10-second persistStateIntervalMillis setting and pass it to a CheerioCrawler. The crawler processes two URLs sequentially with sleep intervals to demonstrate state persistence.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/configuration.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Create new configuration\nconst config = new Configuration({\n    // Set the 'persistStateIntervalMillis' option to 10 seconds\n    persistStateIntervalMillis: 10_000,\n});\n\n// Now we need to pass the configuration to the crawler\nconst crawler = new CheerioCrawler({}, config);\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in Playwright Crawler\nDESCRIPTION: Code example showing how to disable the dynamic browser fingerprints feature in PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic GraphQL Query with Python Requests\nDESCRIPTION: Demonstrates how to make a basic GraphQL query to fetch restaurant posts with sorting parameters. The request includes basic post metadata like titles, summaries, and coordinates.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/11-10-web-scraping-tips/index.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"https://restoran.ua/graphql\"\n\ndata = {\n    \"operationName\": \"Posts_PostsForView\",\n    \"variables\": {\"sort\": {\"sortBy\": [\"startAt_DESC\"]}},\n    \"query\": \"\"\"query Posts_PostsForView(\n    $where: PostForViewWhereInput,\n    $sort: PostForViewSortInput,\n    $pagination: PaginationInput,\n    $search: String,\n    $token: String,\n    $coordinates_slice: SliceInput)\n    {\n        PostsForView(\n                where: $where\n                sort: $sort\n                pagination: $pagination\n                search: $search\n                token: $token\n                ) {\n                        id\n                        title: ukTitle\n                        summary: ukSummary\n                        slug\n                        startAt\n                        endAt\n                        newsFeed\n                        events\n                        journal\n                        toProfessionals\n                        photoHeader {\n                            address: mobile\n                            __typename\n                            }\n                        coordinates(slice: $coordinates_slice) {\n                            lng\n                            lat\n                            __typename\n                            }\n                        __typename\n                    }\n    }\"\"\"\n}\n\nresponse = requests.post(url, json=data)\n\nprint(response.json())\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Example of specifying Node.js version 20 in a Dockerfile using the apify/actor-node image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 20\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Using PuppeteerCrawler with Context-Aware saveSnapshot() Utility\nDESCRIPTION: This code shows how to capture screenshots with PuppeteerCrawler using the context-aware saveSnapshot() utility. This approach simplifies screenshot capture by automatically handling naming and storage based on the crawler context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\n// Create the crawler\nconst crawler = new PuppeteerCrawler({\n    // Function called for each URL\n    async requestHandler({ request, page, puppeteerUtils }) {\n        // Wait for the page to fully load\n        await page.waitForSelector('body');\n\n        // Get the title of the page\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Use the context-aware saveSnapshot() utility\n        await puppeteerUtils.saveSnapshot();\n    },\n    // Add some custom options\n    puppeteerPoolOptions: {\n        useFingerprints: true,\n        // Don't forget to include a UA that identifies your project\n        userAgent: 'Mozilla/5.0 (Macintosh; Intel Mac OS X) MyBot/1.0.0',\n    },\n});\n\n// Add requests and run the crawler\nawait crawler.run(['https://example.com', 'https://google.com', 'https://apify.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Reduce Method in Crawlee\nDESCRIPTION: Example of using the Dataset reduce method to compute an aggregate value (total header count) from all items in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst pagesHeadingCount = await dataset.reduce((memo, item) => {\n    return memo + item.headingCount;\n}, 0);\n\n// Save the result to the default key-value store\nconst store = await KeyValueStore.open();\nawait store.setValue('pages-heading-count', pagesHeadingCount);\n```\n\n----------------------------------------\n\nTITLE: Using transformRequestFunction with enqueueLinks for Advanced Filtering\nDESCRIPTION: Implementing a transformRequestFunction to have fine-grained control over which URLs are enqueued. This example filters out PDF files while keeping the glob pattern restriction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This snippet shows how to turn off browser fingerprinting in PuppeteerCrawler by setting the useFingerprints option to false, which prevents the crawler from applying any fingerprinting techniques during scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // The default value is true\n        useFingerprints: false,\n    },\n    // ... other crawler options\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler in JavaScript\nDESCRIPTION: This code snippet demonstrates a basic setup for a PlaywrightCrawler that visits a start URL and prints the text content of all category elements on the page. It serves as a sanity check to ensure the crawler is set up correctly before implementing full scraping logic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const categoryElements = await page.$$('.collection-block-item');\n\n        for (const categoryElement of categoryElements) {\n            const categoryText = await categoryElement.textContent();\n            console.log(categoryText);\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium for AWS Lambda in Crawlee\nDESCRIPTION: This snippet shows how to configure Crawlee to use the AWS-optimized Chromium browser. It imports @sparticuz/chromium to get the executable path and required arguments for running Chrome in a Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Accessing Key-value Store in Crawlee\nDESCRIPTION: Demonstrates basic operations of key-value stores in Crawlee, including retrieving input, writing output, creating named stores, and manipulating stored values.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using Configuration class\nDESCRIPTION: Example showing how to configure Crawlee using the Configuration class directly in code. This approach accesses the global configuration instance and sets persistStateIntervalMillis to 10000 ms.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development runtime using ts-node-esm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Example of specifying Node.js version 20 when using the apify/actor-node base image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 20\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK Dependency\nDESCRIPTION: Command to install the Apify SDK as a project dependency for working with the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Using Scoped Logging in Request Handlers\nDESCRIPTION: Shows how to use the context-scoped log instance provided in the crawling context. This logger prefixes messages with the crawler name and is the preferred method for logging inside request handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler and Cheerio in Crawlee\nDESCRIPTION: This code snippet shows how to use PlaywrightCrawler with Cheerio in Crawlee for initial website exploration. It visits the start URL, extracts category information using Cheerio, and demonstrates basic link enqueuing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { load } from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${page.url()} is '${title}'`);\n\n        const html = await page.content();\n        const $ = load(html);\n\n        $('.collection-block-item').each((_, el) => {\n            log.info('Category:', $(el).text());\n        });\n\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Complete Product Data Scraping with Playwright in JavaScript\nDESCRIPTION: This comprehensive code snippet combines all the previous snippets to scrape complete product information including URL, manufacturer, title, SKU, price, and stock availability using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Header Generator Configuration\nDESCRIPTION: Shows how to configure browser fingerprint generation options for requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json for TypeScript Crawlee Project\nDESCRIPTION: Complete package.json configuration for a TypeScript Crawlee project, including ES modules settings, dependencies, and all the necessary scripts for development and production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler with Crawlee\nDESCRIPTION: A basic example showing how to use Crawlee's BasicCrawler to download web pages and store their HTML content. The crawler uses sendRequest utility for HTTP requests and saves data to the default dataset storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\n// Create a BasicCrawler instance\nconst crawler = new BasicCrawler({\n    // Let's limit our requests to only 10 per minute\n    maxRequestsPerMinute: 10,\n    // Function called for each URL\n    async requestHandler({ request, sendRequest }) {\n        const response = await sendRequest();\n\n        // Save the HTML content and URL to the default dataset\n        await Dataset.pushData({\n            url: request.url,\n            html: response.body,\n        });\n    },\n});\n\n// Add URLs to crawler\nawait crawler.addRequests([\n    'http://example.com/page-1',\n    'http://example.com/page-2',\n    'http://example.com/page-3',\n]);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Processing Crunchbase API Search Results with Crawlee for Python\nDESCRIPTION: This snippet implements the default_handler function to process search results from the Crunchbase API. It parses the JSON response and creates new requests for individual company data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom crawlee.crawlers import HttpCrawler\nfrom crawlee.router import Router\nfrom crawlee import Request\n\nrouter = Router[HttpCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n    context.log.info(f'default_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    requests = []\n\n    for entity in data['entities']:\n        permalink = entity['identifier']['permalink']\n        requests.append(\n            Request.from_url(\n                url=f'https://api.crunchbase.com/api/v4/entities/organizations/{permalink}?field_ids=short_description%2Clocation_identifiers%2Cwebsite_url',\n                label='company',\n            )\n        )\n\n    await context.add_requests(requests)\n```\n\n----------------------------------------\n\nTITLE: CheerioCrawler Log Output for Crawlee Website Crawl\nDESCRIPTION: Log entries showing a CheerioCrawler crawling the Crawlee.dev website. The output displays the crawler starting and the titles of four different pages that were successfully crawled.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Reduce Method in Crawlee\nDESCRIPTION: Example of using the Dataset reduce method to aggregate data by calculating the total number of headers across all pages. The result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Failed Extraction Output\nDESCRIPTION: Shows the empty output when attempting to scrape JavaScript-rendered content with CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Running PlaywrightCrawler in Headful Mode\nDESCRIPTION: Example showing how to configure PlaywrightCrawler to run in headful mode, displaying the browser UI during development for easier debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headful Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Let the crawler run headful so that you can see the browser.\n    headless: false,\n    // Slow down the crawler to make it easier to see what's happening in the browser.\n    navigationTimeoutSecs: 60,\n    requestHandlerTimeoutSecs: 60,\n    preNavigationHooks: [\n        async ({ page }) => {\n            // Wait for random amount of milliseconds to slow down the crawler.\n            // This is just to make the headful mode more visible for the tutorial.\n            await new Promise((resolve) => setTimeout(resolve, Math.random() * 3000));\n        },\n    ],\n\n    // This function will be called for each URL to crawl.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Enqueue links but don't save results\n        // because they would be the same as with CheerioCrawler.\n        await enqueueLinks({\n            // Consider only links that contain 'docs'.\n            globs: ['https://crawlee.dev/**'],\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure Example\nDESCRIPTION: Shows the file system structure used by Crawlee's dataset storage system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Finalizing Crawler Setup in SuperScraper with TypeScript\nDESCRIPTION: This code snippet completes the crawler setup by starting it, logging its status, and adding it to the crawlers map. It also includes error handling for when the crawler ends unexpectedly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\ncrawler.run().then(\n    () => log.warning(`Crawler ended`, crawlerOptions),\n    () => { }\n);\n\ncrawlers.set(JSON.stringify(crawlerOptions), crawler);\n\nlog.info('Crawler ready ', crawlerOptions);\n\nreturn crawler;\n```\n\n----------------------------------------\n\nTITLE: SendRequest API Implementation\nDESCRIPTION: Shows the internal implementation of the sendRequest function with its default configuration and options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring production execution script in package.json\nDESCRIPTION: Package.json script configuration for running the compiled JavaScript code in production environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to limit the number of requests processed by the crawler using the maxRequestsPerCrawl option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer with Chrome Docker Image\nDESCRIPTION: Example of using a Docker image with pre-installed Puppeteer and Chrome browser, supporting both headless and headful browser modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-puppeteer-chrome Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Puppeteer and Chrome. Suitable for CheerioCrawler and PuppeteerCrawler, but not PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Session Management with HttpCrawler and Proxies\nDESCRIPTION: Demonstrates how to combine session management with proxy configuration in HttpCrawler. This helps maintain consistent identities by pairing session IDs with specific proxy URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true, // This is the default\n    async requestHandler({ session, request, json }) {\n        console.log(`Using session ${session.id}`);\n        console.log(`Fetched ${request.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scrapers with Child Processes in JavaScript\nDESCRIPTION: This code creates multiple parallel scrapers using Node.js child processes. It initializes a shared request queue, spawns worker processes, and handles communication between parent and child processes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { fork } from 'child_process';\nimport { CheerioCrawler, Configuration, log } from 'crawlee';\nimport { router } from './routes.mjs';\nimport { getOrInitQueue } from './requestQueue.mjs';\n\nif (process.env.IS_WORKER_THREAD) {\n    // Disable the automatic purge on start\n    Configuration.set('purgeOnStart', false);\n\n    // Get the request queue from the parent process\n    const requestQueue = await getOrInitQueue(false);\n\n    // Configure crawlee to store the worker-specific data in a separate directory\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    const crawler = new CheerioCrawler({\n        requestQueue,\n        requestHandler: router,\n        minConcurrency: 1,\n        maxConcurrency: 10,\n    }, config);\n\n    await crawler.run();\n    process.exit(0);\n} else {\n    // Parent process\n    const numberOfWorkers = 2;\n    const workers = [];\n\n    for (let i = 0; i < numberOfWorkers; i++) {\n        const worker = fork('./src/parallel-scraper.mjs', [], {\n            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },\n        });\n\n        const workerPromise = new Promise((resolve) => {\n            worker.on('exit', (code) => {\n                log.info(`Worker ${i} exited with code ${code}`);\n                resolve();\n            });\n\n            worker.on('message', (message) => {\n                log.info('Received data from worker:', message);\n            });\n        });\n\n        workers.push(workerPromise);\n    }\n\n    await Promise.all(workers);\n    log.info('All workers finished');\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for Docker Images\nDESCRIPTION: Shows the recommended way to specify dependencies in package.json when using Apify Docker images. This ensures compatibility with pre-installed browser versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Puppeteer's page.screenshot() Method\nDESCRIPTION: This code demonstrates how to capture a screenshot of a web page using Puppeteer's native page.screenshot() method and save it to a key-value store. It launches a browser, navigates to a URL, takes a screenshot, and saves it with a key derived from the URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open new page.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Capture the screenshot.\n    const screenshotBuffer = await page.screenshot();\n\n    // Save the screenshot to the default key-value store.\n    const url = new URL('https://crawlee.dev');\n    const key = `screenshot-${url.hostname}`;\n\n    await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n    console.log(`Screenshot of ${url.href} saved to key-value store with key: ${key}`);\n} finally {\n    // Close browser.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Dataset Operations in Crawlee\nDESCRIPTION: Shows how to perform basic dataset operations including writing single and multiple rows to both default and named datasets\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels with enqueueLinks\nDESCRIPTION: Shows how to use the Request.label shortcut for labeling requests and checking labels within the request handler. This example demonstrates using labels in combination with enqueueLinks for handling different URL types.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Example demonstrating how to completely disable browser fingerprinting in PlaywrightCrawler when it's not needed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // Disable fingerprints completely\n        useFingerprints: false\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)\nDESCRIPTION: Uses the Dataset.pushData() function to save the scraped results. This replaces the console.log() call and stores the data in Crawlee's default Dataset storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Custom Configuration for AWS Lambda\nDESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a custom Configuration object for use in AWS Lambda. It sets persistStorage to false to use in-memory storage due to Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Working with Datasets in Crawlee\nDESCRIPTION: Demonstrates basic operations with datasets including writing single rows, opening named datasets, and writing multiple rows. Shows how to store structured data where each object has the same attributes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Shows how to explicitly clean up default storage directories except for the INPUT key in the default key-value store. This function ensures storage is purged only once per execution context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Actor Docker Environment\nDESCRIPTION: Defines a complete Dockerfile configuration for a Crawlee actor using the actor-node-playwright-chrome base image. The setup optimizes build time through layer caching, installs only production dependencies, and maintains proper file permissions using the myuser account.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json File\nDESCRIPTION: Example crawlee.json file showing how to set configuration options like persistStateIntervalMillis and logLevel that will be picked up automatically by Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: New Browser Lifecycle Hooks with preLaunchHook\nDESCRIPTION: Updated pattern using browser-pool lifecycle hooks with preLaunchHooks for customizing browser launch behavior in a more modular and consistent way across browser types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue with Batch Request Addition\nDESCRIPTION: Shows how to efficiently add multiple requests to a request queue in batch operation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // ...\n    },\n});\n\n// This API enables adding multiple requests in a batch in an efficient way\nawait crawler.addRequests([\n    'https://example.com/page-1',\n    'https://example.com/page-2',\n    'https://example.com/page-3',\n]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Auto-saved Crawler State\nDESCRIPTION: Shows how to use the crawler.useState() method to maintain state that is automatically persisted. The state object is cached and automatically saved when the persistState event occurs, eliminating the need for manual state management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler with RequestQueue in Crawlee\nDESCRIPTION: This example shows how to create a CheerioCrawler with a RequestQueue and a requestHandler that extracts the page title. It demonstrates the complete flow from creating a queue to running the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawling with Crawlee BasicCrawler\nDESCRIPTION: Demonstrates using BasicCrawler to download web pages via HTTP requests and store their HTML content. The crawler processes a list of URLs, sends HTTP requests using got-scraping, and saves the results to a default dataset storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/basic_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler, Dataset } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const startUrls = [\n            'https://crawlee.dev',\n            'https://apify.com',\n            'https://google.com',\n        ];\n\n        for (const url of startUrls) {\n            try {\n                const response = await sendRequest({ url });\n                await Dataset.pushData({\n                    url: response.url,\n                    html: response.body,\n                });\n                log.info(`Crawled ${response.url}...`);\n            } catch (e) {\n                log.error(`Failed to crawl ${url}: ${e.message}`);\n            }\n        }\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Managing Sessions with PlaywrightCrawler in Crawlee\nDESCRIPTION: Example of using SessionPool with PlaywrightCrawler for handling proxy rotation and maintaining sessions. The crawler uses Playwright to render pages in a real browser while managing sessions automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/session_management.mdx#2025-04-11_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\n// First, we initialize the proxy configuration\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Then we initialize the crawler\nconst crawler = new PlaywrightCrawler({\n    // Enable automatic proxy IP address rotation\n    useSessionPool: true,\n    persistCookiesPerSession: true,\n    proxyConfiguration,\n    // Limits connection for avoiding proxy ban\n    maxConcurrency: 50,\n    // Called for each URL\n    async requestHandler({ request, page, session }) {\n        console.log(`Processing ${request.url}...`);\n\n        try {\n            const title = await page.title();\n            console.log(`URL: ${request.url}, Title: ${title}`);\n\n            session.markGood();\n        } catch (error) {\n            session.markBad();\n            throw error;\n        }\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Building a Complete CheerioCrawler with RequestQueue\nDESCRIPTION: This code shows how to create a CheerioCrawler with a manually initialized RequestQueue. The crawler visits the specified URL, extracts the page title using Cheerio, and logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Crawling a Single URL with got-scraping in TypeScript\nDESCRIPTION: This code demonstrates how to use the got-scraping package to fetch the HTML content of a specific URL. It logs both the status code of the response and the HTML title extracted from the page content using a regular expression.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\nconst url = 'https://crawlee.dev';\n\n// Define a function that will be called to crawl the URL\nasync function crawlPage(url: string) {\n    // Use got-scraping to make the HTTP request\n    const response = await gotScraping(url);\n    \n    // Log the status code\n    console.log(`Status code: ${response.statusCode}`);\n    \n    // Extract the title using regex\n    const titleMatch = response.body.match(/<title>([^<]*)<\\/title>/);\n    const title = titleMatch ? titleMatch[1] : 'No title found';\n    \n    // Log the title\n    console.log(`Title: ${title}`);\n}\n\n// Call the function\nawait crawlPage(url);\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Shows how to disable dynamic browser fingerprints in PlaywrightCrawler using the browserPoolOptions configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Default Router Handler for Sitemap Navigation\nDESCRIPTION: Router setup with a default handler that extracts organization sitemap URLs from the sitemap index and processes one of them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# routes.py\n\nfrom crawlee.crawlers import ParselCrawlingContext\nfrom crawlee.router import Router\nfrom crawlee import Request\n\nrouter = Router[ParselCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: ParselCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n    context.log.info(f'default_handler processing {context.request} ...')\n\n    requests = [\n        Request.from_url(url, label='sitemap')\n        for url in context.selector.xpath('//loc[contains(., \"sitemap-organizations\")]/text()').getall()\n    ]\n\n    # Since this is a tutorial, I don't want to upload more than one sitemap link\n    await context.add_requests(requests, limit=1)\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to completely disable browser fingerprints in PlaywrightCrawler using browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result in Crawlee\nDESCRIPTION: The expected result from the reduce method operation, showing the total count of heading elements across all pages. This result would be saved to the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Status\nDESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of specific elements and text.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Using Basic Node.js Docker Image\nDESCRIPTION: Example of using the lightweight Alpine Linux-based Node.js image without browsers, ideal for CheerioCrawler applications.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler for JavaScript-rendered Content (TypeScript)\nDESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Running the Bluesky API Crawler with UV\nDESCRIPTION: Command to run the Bluesky crawler using the UV package manager. This uses the entrypoint defined in pyproject.toml to execute the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nuv run bluesky-crawlee\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Map Method in Crawlee\nDESCRIPTION: Example of using the Dataset.map() method to transform data by filtering for pages that have more than 5 heading elements. The result is a new array containing only the heading counts that meet the condition.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Full Playwright Docker Configuration\nDESCRIPTION: Dockerfile configuration for Node.js with all Playwright browsers (Chromium, Chrome, Firefox, WebKit)\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Custom Selector for enqueueLinks\nDESCRIPTION: Shows how to customize the selector used by enqueueLinks to find links, targeting div elements with a specific class rather than the default <a> elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript with tsconfig.json\nDESCRIPTION: TypeScript configuration file that extends Apify's base configuration, sets up ES2022 module system for top-level await support, and defines project structure with source and output directories.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Type-Safe Input Schema with Zod Library\nDESCRIPTION: This approach uses Zod for type-safe schema validation. It provides runtime validation while maintaining TypeScript type safety, combining the benefits of both approaches.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/motivation.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { z } from 'zod';\n\nconst inputSchema = z.object({\n    startUrls: z.array(z.string()).min(1),\n    maxItems: z.number().int().positive().default(10),\n});\n\ntype Input = z.infer<typeof inputSchema>;\n\nawait Actor.main(async () => {\n    // Parse and validate the input\n    const input = inputSchema.parse(await Actor.getInput());\n\n    // ... rest of the actor code that can use the validated and typed input\n});\n```\n\n----------------------------------------\n\nTITLE: Package.json Browser Dependencies\nDESCRIPTION: Example package.json configuration showing how to specify browser automation dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Unique Configuration for AWS Lambda\nDESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for in-memory storage on AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with preLaunchHook in Apify SDK (JavaScript)\nDESCRIPTION: This example demonstrates how to replace the 'launchPuppeteerFunction' with a 'preLaunchHook' in Apify's PuppeteerCrawler. It allows for more flexible and consistent browser configuration across Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to inspect proxy details during request handling in PlaywrightCrawler. Essential for debugging and monitoring proxy rotation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo, log }) {\n        // Log information about the currently used proxy\n        if (proxyInfo) {\n            log.info(`Currently using proxy: ${proxyInfo.url}`);\n        }\n        // Process the browser page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using PlaywrightCrawler with browserforge in Python\nDESCRIPTION: Example of using PlaywrightCrawler with browserforge integration for improved fingerprinting and header management. It demonstrates crawling a URL and logging response headers and user agent information.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-06/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext\n\n\nasync def main() -> None:\n    # The browserforge fingerprints and headers are used by default.\n    crawler = PlaywrightCrawler()\n\n    @crawler.router.default_handler\n    async def handler(context: PlaywrightCrawlingContext) -> None:\n        url = context.request.url\n        context.log.info(f'Crawling URL: {url}')\n\n        # Decode and log the response body, which contains the headers we sent.\n        headers = (await context.response.body()).decode()\n        context.log.info(f'Response headers: {headers}')\n\n        # Extract and log the User-Agent and UA data used in the browser context.\n        ua = await context.page.evaluate('() => window.navigator.userAgent')\n        ua_data = await context.page.evaluate('() => window.navigator.userAgentData')\n        context.log.info(f'Navigator user-agent: {ua}')\n        context.log.info(f'Navigator user-agent data: {ua_data}')\n\n    # The endpoint httpbin.org/headers returns the request headers in the response body.\n    await crawler.run(['https://www.httpbin.org/headers'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Puppeteer Crawler in TypeScript\nDESCRIPTION: This example shows how to crawl a sitemap using Puppeteer Crawler. It uses the Sitemap utility to parse the sitemap XML, enqueues all discovered URLs, and processes each page in a browser environment to extract its title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore, PuppeteerCrawler, PuppeteerConfiguration, PuppeteerCrawlingContext, createPuppeteerRouter } from 'crawlee';\nimport { Sitemap } from '@crawlee/utils';\n\n// Create a router\nconst router = createPuppeteerRouter();\n\n// Add a route for the sitemap page\nrouter.addHandler('sitemap', async ({ enqueueLinks, log, request }) => {\n    const sitemapUrl = request.url;\n\n    log.info(`Processing sitemap at ${sitemapUrl}`);\n\n    // Initialize the Sitemap object with the URL\n    const sitemap = await Sitemap.load({ url: sitemapUrl });\n\n    // Get an array of URLs from the sitemap\n    const urls = sitemap.urls;\n\n    log.info(`Found ${urls.length} URLs in the sitemap`);\n\n    // Add the URLs to the crawler's request queue\n    await enqueueLinks({\n        urls: urls.map((u) => u.url),\n        label: 'detail',\n    });\n});\n\n// Add a route for the detail pages\nrouter.addHandler('detail', async ({ page, log, request }) => {\n    const title = await page.title();\n    log.info(`Processing detail page: ${title}`, { url: request.url });\n\n    // Save the results to the default dataset\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\n// Define crawler configuration\nconst configuration: PuppeteerConfiguration = {\n    // Use the router to handle requests\n    requestHandler: router,\n\n    // Start with the sitemap URL\n    startUrls: [\n        {\n            url: 'https://crawlee.dev/sitemap.xml',\n            userData: {\n                label: 'sitemap',\n            },\n        },\n    ],\n};\n\n// Set up the crawler\nconst crawler = new PuppeteerCrawler(configuration);\n\n// Run the crawler\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Storage Directory Structure Example\nDESCRIPTION: Demonstrates the file system structure used by Crawlee's key-value store for storing data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example for Crawlee Storage\nDESCRIPTION: Shows the file path pattern used for storing key-value data in Crawlee's storage system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Apify Client in Python\nDESCRIPTION: Sets up logging configuration for Apify Client by creating a StreamHandler with a custom ActorLogFormatter and configuring loggers for 'apify_client' and 'apify'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom apify.log import ActorLogFormatter\n\nhandler = logging.StreamHandler()\nhandler.setFormatter(ActorLogFormatter())\n\napify_client_logger = logging.getLogger('apify_client')\napify_client_logger.setLevel(logging.INFO)\napify_client_logger.addHandler(handler)\n\napify_logger = logging.getLogger('apify')\napify_logger.setLevel(logging.DEBUG)\napify_logger.addHandler(handler)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in HttpCrawler\nDESCRIPTION: Demonstrates how to access and inspect the current proxy information within the HttpCrawler's request handler. This allows retrieving the proxy URL used for the request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new HttpCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, request, json }) {\n        console.log(`Used proxy URL: ${proxyInfo.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Example showing how to properly close browsers and set cookies using the new BrowserController, which provides a unified API for both Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Key-Value Store Operations in Crawlee\nDESCRIPTION: This snippet shows how to perform basic operations with key-value stores in Crawlee, including reading input, writing output, and working with named stores.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() in Crawlee (TypeScript)\nDESCRIPTION: Shows an alternative way to run an Actor using the Actor.main() method, which is a syntax sugar for init() and exit(). It wraps the user's code in a try/catch block.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Creating Entry Point for Crunchbase Crawler\nDESCRIPTION: This snippet creates an entry point for the Crunchbase crawler using asyncio. It defines the __main__.py file to run the main function asynchronously.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom .main import main\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating an Express Server Wrapper for Crawlee on GCP Cloud Run\nDESCRIPTION: Complete implementation of a Crawlee crawler wrapped in an Express HTTP server for GCP Cloud Run. The server listens on the port specified by GCP's environment variable and returns crawling results in the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with PuppeteerCrawler using saveSnapshot() Utility\nDESCRIPTION: This example shows how to use the context-aware saveSnapshot() utility with PuppeteerCrawler. This approach simplifies screenshot capture during crawling by handling the screenshot and storage operations automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst urls = ['https://apify.com', 'https://crawlee.dev'];\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, crawler }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        // Save a screenshot using the utility function\n        // The key is automatically generated from the URL\n        await crawler.utils.saveSnapshot(page);\n\n        // You can also specify a custom key:\n        // await crawler.utils.saveSnapshot(page, { key: 'my-screenshot' });\n\n        console.log(`Screenshot of ${request.url} saved`);\n    },\n});\n\nawait crawler.run(urls);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Extraction Handler\nDESCRIPTION: Default request handler for extracting search result data using BeautifulSoup\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@crawler.router.default_handler\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n    context.log.info(f'Processing {context.request} ...')\n\n    for item in context.soup.select(\"div#search div#rso div[data-hveid][lang]\"):\n        data = {\n            'title': item.select_one(\"h3\").get_text(),\n            \"url\": item.select_one(\"a\").get(\"href\"),\n            \"text_widget\": item.select_one(\"div[style*='line']\").get_text(),\n        }\n        await context.push_data(data)\n```\n\n----------------------------------------\n\nTITLE: Running the Crawlee Example Project\nDESCRIPTION: Commands to navigate into the created project directory and start the crawler example.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Map Method Result in Crawlee\nDESCRIPTION: The expected result from the map method operation, showing an array of heading counts that are greater than 5. This result would be saved to the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Declarations\nDESCRIPTION: Installs type declarations for Node.js to enable type-checking for Node.js features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Using Basic Node.js Docker Image for Cheerio Crawler\nDESCRIPTION: Example of using the lightweight actor-node image based on Alpine Linux. This is the smallest image suitable for CheerioCrawler but doesn't support browser-based crawlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example showing how to disable dynamic fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Session Management with PlaywrightCrawler and Proxies\nDESCRIPTION: Demonstrates implementing session management with PlaywrightCrawler and proxies to maintain consistent browser identities across requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    useSessionPool: true, // This is the default\n    persistCookiesPerSession: true, // This is the default\n    proxyConfiguration,\n    async requestHandler({ request, page, session }) {\n        // Process the response\n        // ...\n\n        // You can mark session as bad when it gets blocked\n        if (someCondition) {\n            session.markBad();\n        }\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Parsing Numeric Values from Strings in TypeScript\nDESCRIPTION: This snippet defines utility functions for parsing numeric values from strings, specifically designed for handling price and review count data from Amazon product pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n/**\n * Parses a number from a string by removing all non-numeric characters.\n * - Keeps the decimal point.\n */\nconst parseNumberValue = (rawString: string): number => {\n    return Number(rawString.replace(/[^\\d.]+/g, ''));\n};\n\n/**\n * Parses a number value from the first element matching the given selector.\n */\nexport const parseNumberFromSelector = ($: CheerioAPI, selector: string): number => {\n    const rawValue = $(selector).first().text();\n    return parseNumberValue(rawValue);\n};\n```\n\n----------------------------------------\n\nTITLE: Using Request.label in RequestHandler\nDESCRIPTION: Demonstrates the use of the new Request.label shortcut for labeling requests and using it with enqueueLinks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating API Start URLs for TikTok Ads Creative Radar\nDESCRIPTION: Function that generates the initial API endpoint URL with query parameters based on user input. Parameters include time period, country, results limit, industry, and filter options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-30-jsdom-based-scraping/index.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nexport const createStartUrls = (input) => {\n    const {\n        days = '7',\n        country = '',\n        resultsLimit = 100,\n        industry = '',\n        isNewToTop100,\n    } = input;\n\n    const filterBy = isNewToTop100 ? 'new_on_board' : '';\n    return [\n        {\n            url: `https://ads.tiktok.com/creative_radar_api/v1/popular_trend/hashtag/list?page=1&limit=50&period=${days}&country_code=${country}&filter_by=${filterBy}&sort_by=popular&industry_id=${industry}`,\n            headers: {\n                // required headers\n            },\n            userData: { resultsLimit },\n        },\n    ];\n};\n```\n\n----------------------------------------\n\nTITLE: Finding Links with enqueueLinks\nDESCRIPTION: Demonstrates how to find and enqueue links on a page using the enqueueLinks function, which automatically finds <a> elements with href attributes and adds them to the request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Set a limit for the number of pages we'll crawl\n    maxRequestsPerCrawl: 20,\n    \n    async requestHandler({ $, request, enqueueLinks }) {\n        console.log(`Processing ${request.url}...`);\n        \n        // Extract the title of the page\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // Find all links and add them to the crawling queue\n        await enqueueLinks();\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Docker Environment for Crawlee Actor with Playwright and Chrome\nDESCRIPTION: Dockerfile that configures a container for running a Crawlee actor with Playwright and Chrome support. It optimizes the build process by separating dependency installation from code copying to leverage Docker layer caching, and installs only production dependencies to keep the image size small.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Title with JSDOM in JavaScript\nDESCRIPTION: Demonstrates how to retrieve the page title using JSDOM, comparing it with browser JavaScript. This snippet shows the equivalence between document.title in browsers and window.document.title in JSDOM.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for the documentation page, defining ID, title, and description\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\nid: introduction\ntitle: Introduction\ndescription: Your first steps into the world of scraping with Crawlee\n---\n```\n\n----------------------------------------\n\nTITLE: Adding Development Script to package.json\nDESCRIPTION: Script configuration for running the project in development mode using ts-node-esm with transpile-only mode for faster development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with CheerioCrawler and Data Return\nDESCRIPTION: Complete implementation of an AWS Lambda handler using CheerioCrawler. The code initializes the crawler with in-memory storage, runs it on the specified URLs, and returns the scraped data with a 200 status code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Managing Dataset Operations in Crawlee\nDESCRIPTION: Shows how to work with datasets for storing structured data, including pushing single and multiple rows of data to both default and named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Setting Response Type to JSON in SendRequest\nDESCRIPTION: Shows how to configure the sendRequest function to parse the response as JSON instead of the default text format. This is useful when working with JSON APIs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Single URL Crawling with Got-Scraping\nDESCRIPTION: React/JSX code block for displaying a runnable code example that demonstrates URL crawling. Uses RunnableCodeBlock component to render the example with syntax highlighting.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<RunnableCodeBlock className=\"language-js\" type=\"cheerio\">\n\t{CrawlSource}\n</RunnableCodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue with Locking Support\nDESCRIPTION: This code shows how to create a RequestQueue that supports locking by importing RequestQueueV2 instead of the standard RequestQueue. It demonstrates opening the queue and adding requests to it.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee, which can be useful for cleaning up before starting a new crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Storing Dataset Files in Crawlee Project Directory\nDESCRIPTION: Shows the directory path where individual dataset items are saved within the project folder structure. Each item in the dataset is stored as a separate file in the default dataset directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Implementing Pagination Handler\nDESCRIPTION: Code for handling pagination in Google search results\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    await context.enqueue_links(selector=\"div[role='navigation'] td[role='heading']:last-of-type > a\")\n```\n\n----------------------------------------\n\nTITLE: Using Window API with JSDOMCrawler\nDESCRIPTION: Demonstrates how to access the page title using document and window objects, comparing browser JavaScript syntax with JSDOM syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components Through the Crawler Object\nDESCRIPTION: Example showing how to access components like requestQueue and autoscaledPool through the crawler object in the Crawling Context, replacing previous direct access methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with crawlee.json configuration\nDESCRIPTION: JavaScript example demonstrating how to use Crawlee with configuration set in crawlee.json. This code doesn't explicitly import or pass Configuration to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Using Request.label with enqueueLinks for Request Categorization\nDESCRIPTION: Shows how to use the Request.label shortcut to categorize different types of requests. This example conditionally enqueues links with a 'DETAIL' label.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: sendRequest API Implementation in TypeScript\nDESCRIPTION: This code snippet shows the implementation of the sendRequest function, which uses got-scraping to send HTTP requests. It includes various options like URL, method, headers, and proxy settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Express Server for Crawlee in GCP Cloud Run\nDESCRIPTION: This snippet demonstrates how to wrap a Crawlee crawler with an Express HTTP server to make it compatible with GCP Cloud Run. It creates an endpoint that runs the crawler and returns the data when accessed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Custom Configuration for AWS Lambda\nDESCRIPTION: Code snippet showing how to instantiate a CheerioCrawler with a unique Configuration instance that uses in-memory storage to avoid filesystem issues on Lambda. The persistStorage option is set to false to work with Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to customize browser fingerprints when using PlaywrightCrawler to avoid getting blocked. It configures fingerprints with specific browser and operating system settings via the browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // The default browser is Chrome, but we can explicitly define it like this\n        browserPlugins: [{\n            id: 'playwright',\n            createBrowserFunction: async () => {\n                const { chromium } = await import('playwright');\n                return chromium.launch({ headless: false });\n            },\n        }],\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'chrome', minVersion: 88 }, // Only Chrome 88 and up will be used\n                ],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n    // Other crawler options...\n});\n\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: HTTP Crawler Bug Fix - Response Type Update\nDESCRIPTION: Bug fix replacing IncomingMessage with PlainResponse type for the context's response object in HTTP crawler\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/http-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**http-crawler:** replace `IncomingMessage` with `PlainResponse` for context's `response`\n```\n\n----------------------------------------\n\nTITLE: Using Crawlee with Configuration from crawlee.json\nDESCRIPTION: Example code showing how Crawlee uses configuration from crawlee.json without explicitly importing or passing Configuration to the crawler. The example demonstrates state persistence based on the configured interval.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Sanity Check with PlaywrightCrawler and Cheerio in TypeScript\nDESCRIPTION: This code snippet shows an alternative approach to the sanity check, using PlaywrightCrawler with Cheerio for HTML parsing. It crawls the start URL and extracts category information using Cheerio selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport * as cheerio from 'cheerio';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, log }) {\n        const html = await page.content();\n        const $ = cheerio.load(html);\n\n        const categoryElements = $('.collection-block-item');\n\n        log.info(`Number of categories found: ${categoryElements.length}`);\n\n        categoryElements.each((_, el) => {\n            console.log($(el).text());\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Setting Context Options with browserPool.newPage Options\nDESCRIPTION: Shows how to configure context options when using incognito pages (useIncognitoPages: true), which creates a new context for each page. In this approach, context options are passed via pageOptions when calling newPage().\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n     browserPlugins: [\n        new PlaywrightPlugin(\n            playwright.chromium,\n            {\n                useIncognitoPages: true, // You must turn on incognito pages.\n                launchOptions: {\n                    // launch options\n                    headless: false,\n                    devtools: true,\n                },\n            },\n        ),\n    ],\n});\n\n// Launches Chromium with Playwright and returns a Playwright Page.\nconst page = await browserPool.newPage({\n    pageOptions: {\n        // context options\n        deviceScaleFactor: 2,\n        colorScheme: 'light',\n        locale: 'de-DE',\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Navigation Hooks in Apify PuppeteerCrawler (JavaScript)\nDESCRIPTION: This snippet demonstrates the transition from using a custom 'gotoFunction' to using 'preNavigationHooks' and 'postNavigationHooks' in Apify's PuppeteerCrawler. It simplifies the process of adding pre and post-navigation behavior without needing to remember the intricacies of the gotoExtended function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-rendered website using Scrapy with Playwright plugin\nDESCRIPTION: This Python code demonstrates how to use Scrapy with the Playwright plugin to scrape a JavaScript-rendered website (Apify Store). It includes asynchronous handling and element selection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nclass ActorSpider(scrapy.Spider):\n    name = 'actor_spider'\n    start_urls = ['https://apify.com/store']\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(\n                url,\n                meta={\"playwright\": True, \"playwright_include_page\": True},\n                callback=self.parse_playwright\n            )\n\n    async def parse_playwright(self, response):\n        page = response.meta['playwright_page']\n        await page.wait_for_selector('.ActorStoreItem-title-wrapper')\n        actor_card = await page.query_selector('.ActorStoreItem-title-wrapper')\n\n        if actor_card:\n            actor_text = await actor_card.text_content()\n            yield {\n                'actor': actor_text.strip() if actor_text else 'N/A'\n            }\n\n        await page.close()\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content from HTML Elements with Cheerio\nDESCRIPTION: Shows how to find and extract the text content from the first h2 element on a page using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Example Dataset JSON Structure for Map and Reduce Operations\nDESCRIPTION: Sample JSON dataset containing URLs and their associated heading counts. This structure is stored in the default dataset directory and can be created using the dataset.pushData() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Building a Complete CheerioCrawler with RequestQueue\nDESCRIPTION: Shows how to create a CheerioCrawler with a RequestQueue and a requestHandler to process web pages. The crawler visits crawlee.dev and extracts the page title using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies in package.json\nDESCRIPTION: Example of how to specify dependencies in package.json when using Apify Docker images. Using an asterisk for the automation library version ensures compatibility with the pre-installed version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing CrawlingContext Interface Inference in Crawlee\nDESCRIPTION: Ensures that the CrawlingContext interface is correctly inferred in route handlers. This improves type checking and autocompletion in IDEs when working with route handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_11\n\nLANGUAGE: TypeScript\nCODE:\n```\nensure CrawlingContext interface is inferred correctly in route handlers\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Crawlee with curl-impersonate\nDESCRIPTION: Commands to install Crawlee with curl-impersonate support and orjson for high-performance JSON parsing in asynchronous applications.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install crawlee[curl-impersonate]==0.3.5\npip install orjson>=3.10.7,<4.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Legacy Usage of gotoFunction in PuppeteerCrawler\nDESCRIPTION: Example of the old pattern using gotoFunction for pre and post navigation processing in PuppeteerCrawler, which required manually calling gotoExtended and returning the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Apify Platform Storage in Local Development\nDESCRIPTION: Demonstrates how to use Apify platform storage when developing and running an actor locally. This code opens a Key-Value Store and gets a public URL for an item in the store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Filtering Links to Same Domain Without enqueueLinks\nDESCRIPTION: Manually implements domain filtering by checking URLs against the current hostname before adding them to the queue, showing the underlying logic that enqueueLinks handles automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, crawler }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // Create a RequestQueue instance\n        const requestQueue = await crawler.getRequestQueue();\n\n        // Find all links\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n\n        // Create URL objects\n        const urlObjects = links.map((link) => new URL(link, request.url));\n\n        // Get current hostname from request\n        const { hostname } = new URL(request.url);\n\n        // Filter for URLs from the same hostname\n        const sameHostnameUrls = urlObjects\n            .filter((url) => url.hostname === hostname)\n            .map((url) => ({ url: url.href }));\n\n        // Add filtered URLs to RequestQueue\n        for (const request of sameHostnameUrls) {\n            await requestQueue.addRequest(request);\n        }\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image Tags\nDESCRIPTION: Demonstrates how to use pre-release versions of Docker images for testing purposes. Examples are provided for both with and without library version specifications.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:16-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Using Custom Browser Modules with Launcher Option\nDESCRIPTION: Examples of specifying custom browser modules for both Puppeteer and Playwright using the launcher option in launch functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for Crawlee TypeScript Project\nDESCRIPTION: Set up a multi-stage Dockerfile for building and running a Crawlee TypeScript project, optimizing for production deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:20 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:20\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build script and main entry point.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Function handler\nDESCRIPTION: Creates a complete handler function that initializes the crawler, runs it, and returns the crawled data as a response. This function is exported as a named export to be used as the Cloud Function entry point.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Accepting and Logging User Input in Crawlee\nDESCRIPTION: This TypeScript code snippet demonstrates how to import and use the getInput function from Crawlee to retrieve and log user input. It also shows how to handle cases where no input is provided.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of your actor\nconst input = await Actor.getInput();\nconsole.log('My input:');\nconsole.dir(input);\n\n// If input is missing, fall back to default values\nif (!input) {\n    console.log('No input specified!');\n}\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests in Batches with Crawler\nDESCRIPTION: Shows how to use the crawler.addRequests() method to add multiple requests in batches. This method enqueues the first 1000 requests immediately while processing the rest in the background, allowing crawling to start almost instantly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generator Options in SendRequest\nDESCRIPTION: Demonstrates how to configure browser fingerprinting through header generator options. These options allow controlling which devices, locales, operating systems, and browsers to mimic when generating headers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Map Method in Crawlee\nDESCRIPTION: Demonstrates how to use the Dataset map method to filter pages with more than 5 headers. The result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst moreThan5headers = await dataset.map((item) => {\n    if (item.headingCount > 5) {\n        return item.headingCount;\n    }\n});\n\nconst kvStore = await KeyValueStore.open();\nawait kvStore.setValue('more-than-5-headers', moreThan5headers);\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support in JavaScript\nDESCRIPTION: This code snippet defines a function to create or retrieve a request queue that supports locking. It uses the Apify SDK to manage the queue and ensures proper initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { RequestQueue } from '@crawlee/memory';\n\nconst QUEUE_NAME = 'PRODUCT_URLS';\n\nexport async function getOrInitQueue(forceEmpty = false) {\n    const client = await Actor.apifyClient();\n    const queueClient = await client.requestQueue(QUEUE_NAME);\n\n    if (forceEmpty) {\n        await queueClient.drop();\n    }\n\n    return new RequestQueue({\n        clientKey: queueClient.clientKey,\n        name: QUEUE_NAME,\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables. Shows the difference between accessing local and remote storage using the forceCloud option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Example Dataset Structure in JSON\nDESCRIPTION: A sample JSON dataset structure representing scraped pages with URLs and heading counts. This data is stored in the default dataset directory and can be created using dataset.pushData().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Firing Local SystemInfo Events in JavaScript\nDESCRIPTION: Update to fire local SystemInfo events every second in the core module.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Fire local SystemInfo events every second\n```\n\n----------------------------------------\n\nTITLE: Verifying NPM Installation\nDESCRIPTION: Command to check installed NPM version\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Retiring Browsers in BrowserPool\nDESCRIPTION: Shows how to retire browsers with the new BrowserPool API, replacing the previous PuppeteerPool.retire() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.map() method in Crawlee\nDESCRIPTION: Example demonstrating how to use the Dataset map method to transform dataset items. This code filters URLs that have more than 5 headers and extracts just the headingCount values into a new array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support. This enables using Playwright for browser automation with Firefox, Chrome, and Webkit.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Enqueueing All Links with CheerioCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to crawl all links on a website using the 'All' strategy with CheerioCrawler. This will enqueue all links found during crawling, regardless of their domain.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, enqueueLinks }) {\n        console.log(`Processing: ${request.url}`);\n\n        // Add all links from the page to the crawling queue regardless of domain\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            // We can also use the string representation of the strategy\n            // strategy: 'all',\n        });\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Selective Link Crawling with CheerioCrawler and Globs Pattern\nDESCRIPTION: Shows how to use CheerioCrawler to selectively crawl links on a website by using the globs property in enqueueLinks() method. The crawler will only process links that match the specified glob pattern, allowing for targeted crawling of specific URL patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawl_some_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing new links to crawl.');\n        await enqueueLinks({\n            globs: ['**/cases/**'],\n        });\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Data Return for Cheerio Crawler\nDESCRIPTION: Final implementation of AWS Lambda handler for Cheerio Crawler that creates a new crawler instance, runs it, and returns the scraped data as the Lambda response. This ensures proper data handling and stateless execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Creating a Persistent Cheerio Crawler for Web Scraping\nDESCRIPTION: Configures a CheerioCrawler with keepAlive option set to true to ensure the crawler remains active even when there are no pending requests. The crawler extracts page titles and will later be integrated with HTTP response handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        // We will send the response here later\n        log.info(`Page title: ${title} on ${request.url}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete package.json Configuration\nDESCRIPTION: Complete package.json configuration that includes all necessary dependencies, scripts for development and production, and module type setting for ES modules support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler. It includes configuration for the crawler, request handling, and data extraction from the crawled pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/puppeteer_recursive_crawl.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks }) {\n        const title = await page.title();\n        console.log(`Title of ${request.url}: ${title}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        await enqueueLinks({\n            globs: ['https://apify.com/**'],\n            label: 'detail',\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://apify.com/']);\n\n```\n\n----------------------------------------\n\nTITLE: Defining Input Schema in Actor Main.ts\nDESCRIPTION: This snippet shows how to define a basic input schema directly in the main.ts file of an Actor. The schema validates that the input contains required properties with appropriate types.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // Define the input schema directly in the code\n    const input = await Actor.getInput<{\n        startUrls: string[];\n        maxItems: number;\n    }>({{\n        // Input schema definition\n        properties: {\n            startUrls: {\n                type: 'array',\n                editor: 'stringList',\n                description: 'URLs to start with',\n                prefill: [\n                    'https://crawlee.dev',\n                ],\n            },\n            maxItems: {\n                type: 'integer',\n                description: 'Maximum number of items',\n                default: 10,\n            },\n        },\n        required: ['startUrls'],\n    }});\n\n    // ... rest of the actor code\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Commands to install Crawlee and its dependencies from the 'next' distribution tag.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee@next\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio@next\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee@next playwright\n# or npm install @crawlee/playwright@next playwright\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Shows a more concise way to initialize and run a CheerioCrawler using the crawler.run() method with direct URL input.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using the Configuration Class\nDESCRIPTION: Example demonstrating how to access and modify the global Configuration instance to set parameters like persistStateIntervalMillis programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Main Scraping Function for Google Maps Listings\nDESCRIPTION: Primary function that coordinates the scraping process by locating listings, extracting data from each one, and handling pagination through scrolling. It includes progress reporting and total count tracking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def _scrape_listings(self, context) -> None:\n    \"\"\"Main scraping function to process all listings\"\"\"\n    try:\n        page = context.page\n        print(f\"\\nProcessing URL: {context.request.url}\\n\")\n\n        await page.wait_for_selector(\".Nv2PK\", timeout=30000)\n        await page.wait_for_timeout(2000)\n\n        while True:\n            listings = await page.query_selector_all(\".Nv2PK\")\n            new_items = 0\n\n            for listing in listings:\n                place_data = await self._extract_listing_data(listing)\n                if place_data:\n                    await context.push_data(place_data)\n                    new_items += 1\n                    print(f\"Processed: {place_data['name']}\")\n\n            if new_items == 0 and not await self._load_more_items(page):\n                break\n            if new_items > 0:\n                await self._load_more_items(page)\n\n        print(f\"\\nFinished processing! Total items: {len(self.processed_names)}\")\n    except Exception as e:\n        print(f\"Error in scraping: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in JavaScript\nDESCRIPTION: This snippet demonstrates how to use Actor to access both local and cloud-based datasets. It shows how to open a local dataset by default and use the forceCloud option to access a remote dataset on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Configuring PreNavigation Hooks in HTTP Crawler\nDESCRIPTION: Example showing how to configure pre-navigation hooks to modify gotOptions before making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/http-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\npreNavigationHooks: [\n    (crawlingContext, gotOptions) => {\n        // ...\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Creating a Docker Container for Node.js Playwright Actors\nDESCRIPTION: This Dockerfile sets up a containerized environment for running web scraping actors with Node.js and Playwright. It uses a multi-stage build to optimize image size, first building the application and then creating a production image with minimal dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Helper function to explicitly clean up the default request storage directory and request list stored in the default key-value store before a crawler run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to enable the request locking experiment in a CheerioCrawler configuration. The experiment is enabled by setting requestLocking to true in the experiments object.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: HTTP Crawler Feature - Cheerio Parser Addition\nDESCRIPTION: Addition of parseWithCheerio helper method to HttpCrawler for HTML parsing capabilities\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/http-crawler/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n**HttpCrawler:** add `parseWithCheerio` helper to `HttpCrawler`\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV with Crawlee\nDESCRIPTION: Demonstrates using Dataset.exportToValue() to export the entire default dataset to a single CSV file in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nawait Dataset.exportToValue('OUTPUT.csv', {\n    contentType: 'text/csv',\n    keyValueStoreName: 'default',\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Crawler Map for SuperScraper in TypeScript\nDESCRIPTION: This code snippet initializes a Map to store multiple instances of PlaywrightCrawler, each associated with a unique proxy configuration. This allows SuperScraper to handle different proxy settings efficiently.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst crawlers = new Map<string, PlaywrightCrawler>();\n```\n\n----------------------------------------\n\nTITLE: Updated Browser Launch API with launchOptions\nDESCRIPTION: Comparison of old and new browser launch function patterns, showing the separation of browser-specific options into the launchOptions object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TSConfig\nDESCRIPTION: Installing @apify/tsconfig as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json\nDESCRIPTION: Example crawlee.json file showing how to set configuration options like persistStateIntervalMillis and logLevel that will be used as global configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueueV2 Instance for Request Locking\nDESCRIPTION: This snippet shows how to create a request queue that supports locking by using the RequestQueueV2 class instead of the standard RequestQueue. It demonstrates opening the queue and adding requests to it.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 for Request Locking\nDESCRIPTION: Demonstrates how to use the new RequestQueueV2 class that supports request locking. Shows queue initialization and adding requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Saving Data to Crawlee Dataset\nDESCRIPTION: Demonstrates how to save scraped results to a Crawlee Dataset using the pushData method, which creates a new row in the dataset storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup Operations\nDESCRIPTION: Shows how to purge default storage directories in Crawlee using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Shows how to purge default storage directories using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Dataset.reduce() Result Example\nDESCRIPTION: The expected output from the reduce() method example, showing the total count of all headings across all pages in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Integrating HTTP server with Crawlee for web scraping\nDESCRIPTION: Combines the HTTP server and CheerioCrawler to handle incoming requests, scrape web pages, and return the scraped data. It uses a RequestQueue to manage scraping tasks and maps HTTP responses to Crawlee Requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createServer } from 'http';\nimport { CheerioCrawler, RequestQueue, log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    const urlToScrape = new URL(req.url, `http://${req.headers.host}`).searchParams.get('url');\n    if (!urlToScrape) {\n        res.writeHead(400, { 'Content-Type': 'text/plain' });\n        res.end('Missing url parameter\\n');\n        return;\n    }\n\n    try {\n        const title = await scrapeTitle(urlToScrape);\n        res.writeHead(200, { 'Content-Type': 'application/json' });\n        res.end(JSON.stringify({ title }));\n    } catch (error) {\n        res.writeHead(500, { 'Content-Type': 'text/plain' });\n        res.end(`Error: ${error.message}\\n`);\n    }\n});\n\nconst requestQueue = await RequestQueue.open();\n\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        request.userData.title = title;\n    },\n});\n\nasync function scrapeTitle(url) {\n    const request = await requestQueue.addRequest({ url, userData: {} });\n    await crawler.run();\n    return request.userData.title;\n}\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n\ncrawler.run();\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize the Apify project configuration, creating a .actor directory and actor.json file with settings for deployment to the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments in Previous SDK Versions\nDESCRIPTION: Demonstrates how handler arguments were structured in previous SDK versions, where separate objects were created for different handlers, making it difficult to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Starting a Crawlee Project in Bash\nDESCRIPTION: Commands to navigate into the newly created crawler directory and start the crawler using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Demonstrates how to clean up default storage directories using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler Implementation with crawlee.json Configuration\nDESCRIPTION: Example showing how to implement a basic crawler that uses the crawlee.json configuration file without explicit configuration in code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/configuration.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, sleep } from 'crawlee';\n// We are not importing nor passing\n// the Configuration to the crawler.\n// We are not assigning any env vars either.\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // for the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // for the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in SDK v1\nDESCRIPTION: Example showing how SDK v1 uses a single Crawling Context object shared across all handler functions, making it easier to track values consistently across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Extracting and Processing Product Price with Playwright in JavaScript\nDESCRIPTION: This snippet shows a complex price extraction process using Playwright. It selects elements with the 'price' class that contain a '$' character, extracts the text, splits by the dollar sign, removes commas, and converts the string to a number.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots with Direct Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This example shows how to use Crawlee's utility function saveSnapshot() to capture a screenshot with Puppeteer. It launches a browser, navigates to a URL, and uses the utility to automatically save the screenshot.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { puppeteerUtils } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open new page.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Capture and save the screenshot.\n    await puppeteerUtils.saveSnapshot(page, { key: 'screenshot' });\n\n    console.log('Screenshot saved!');\n} finally {\n    // Close Puppeteer.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support in Crawlee\nDESCRIPTION: A utility function that initializes or retrieves a request queue with locking support. It can optionally purge the queue before initialization, enabling a clean slate for new scraping operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { RequestQueue } from '@crawlee/core';\n\n/**\n * Gets or initializes a request queue for parallel scraping.\n * @param {boolean} purgeExisting Whether to purge the existing queue before initialization. Defaults to false.\n * @returns {Promise<RequestQueue>} The initialized request queue.\n */\nexport async function getOrInitQueue(purgeExisting = false) {\n    await Actor.init({ requestQueueOptions: { lockingEnabled: true } });\n\n    const requestQueue = await Actor.openRequestQueue();\n    \n    if (purgeExisting) {\n        // Empty the queue by dropping its queued requests\n        await requestQueue.drop();\n    }\n    \n    return requestQueue;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Express HTTP Server for Crawlee in GCP Cloud Run\nDESCRIPTION: Complete implementation of a Crawlee crawler wrapped in an Express HTTP server for GCP Cloud Run. The script sets up an endpoint that executes the crawler and returns data in response to HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: This example shows how to customize browser fingerprints in PuppeteerCrawler by specifying browser type, operating system, and language preferences to avoid detection when scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Browser fingerprints are generated automatically by default.\n    // Here we customize them to match browsers used in the US.\n    browserPoolOptions: {\n        fingerprintOptions: {\n            // Use desktop browser fingerprints (the default)\n            mobile: false,\n            // Restrict to just Chrome browser fingerprints\n            browsers: ['chrome'],\n            // Use only fingerprints from the latest Chrome version\n            // Other options are 'random' (default) and 'randomized'\n            browserVersion: 'latest',\n            // Only generate fingerprints for Windows\n            operatingSystems: ['windows'],\n            // Use locales typical for US users\n            locales: ['en-US', 'en'],\n        },\n    },\n    // Other crawler options...\n});\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Session Management with PlaywrightCrawler and Proxies\nDESCRIPTION: Shows how to use session management with PlaywrightCrawler to maintain consistent proxy usage across browser sessions, enhancing the ability to avoid detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Let's create our crawler\nconst crawler = new PlaywrightCrawler({\n    // useSessionPool enables automatic session management\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ page, session, request }) {\n        // each request is automatically being assigned a random session\n        // and the proxy attached in proxyConfiguration is selected based\n        // on the session id\n        console.log(`Processing: ${request.url}`);\n        console.log(`Using session: ${session.id}`);\n        const title = await page.title();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with playwright-extra and stealth plugin\nDESCRIPTION: This snippet demonstrates how to set up a PlaywrightCrawler using playwright-extra and the stealth plugin. It shows the configuration of the crawler, including the use of a custom launcher function to integrate playwright-extra.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport { chromium } from 'playwright-extra';\nimport StealthPlugin from 'puppeteer-extra-plugin-stealth';\n\nchromium.use(StealthPlugin());\n\nconst crawler = new PlaywrightCrawler({\n    // Instead of the default launcher, we use our own\n    // that uses playwright-extra under the hood\n    launchContext: {\n        launcher: chromium,\n    },\n    async requestHandler({ page, request, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Unique Configuration for AWS Lambda\nDESCRIPTION: Basic setup of a Cheerio Crawler with a unique Configuration instance using in-memory storage (persistStorage: false) to avoid Lambda stateful behavior issues. This is the first step in making Crawlee compatible with AWS Lambda's environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Sanity Check Crawler with Playwright\nDESCRIPTION: This code snippet creates a PlaywrightCrawler to visit the start URL and print the text content of all category elements on the page. It demonstrates how to set up a basic crawler and perform initial checks on the target website.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const categories = await page.$$('.collection-block-item');\n\n        for (const category of categories) {\n            const categoryName = await category.textContent();\n            console.log(categoryName);\n        }\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler in Crawlee\nDESCRIPTION: This code shows how to customize browser fingerprints with PuppeteerCrawler to avoid blocking during scraping. It sets specific fingerprint options like browser type, version, and operating system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'chrome', minVersion: 88 }, // Only Chrome 88 and up will be used\n                ],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n    // Other crawler options...\n});\n\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Initializing RequestQueue with Crawlee\nDESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to crawl using Crawlee's RequestQueue class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets in Crawlee\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets when using Apify Token and local storage directory. Shows the usage of forceCloud option for accessing platform storages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State Management\nDESCRIPTION: Shows how to use the useState method for maintaining crawler state that is automatically saved during persistence events\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup Implementation\nDESCRIPTION: Shows how to purge default storages in Crawlee using the purgeDefaultStorages helper function\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Playwright Crawler in Crawlee\nDESCRIPTION: This example demonstrates using Crawlee's Playwright Crawler to download and crawl URLs from a sitemap. It uses the downloadListOfUrls utility to fetch sitemap URLs and processes them with Playwright Crawler, which provides modern browser automation capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from '@crawlee/playwright';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\n// Configuration of the sitemap's URL\nconst startUrls = ['https://crawlee.dev/sitemap.xml'];\n\n// Create an instance of the PlaywrightCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Playwright.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n        const title = await page.title();\n        log.info(`Title of ${url}: ${title}`);\n        // Here you process each URL using Playwright - extract data, take screenshots, etc.\n    },\n    // If you need to use proxy, you can use the proxy configuration\n    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n});\n\n// This logic downloads the sitemap from the specified URL\nasync function crawlSitemap() {\n    // Download the sitemap and parse its URLs\n    const urls = await downloadListOfUrls({ url: startUrls[0] });\n\n    // Add all the URLs to the crawler's queue\n    await crawler.addRequests(urls.map((url) => ({ url })));\n\n    // Run the crawler\n    await crawler.run();\n}\n\n// Execute the crawlSitemap function\nawait crawlSitemap();\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers with PlaywrightRouter in Crawlee\nDESCRIPTION: A router implementation that defines handlers for different types of pages: detail product pages, category pages, and a default handler. Each handler extracts specific data or enqueues new links based on the page context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Docker Best Practice Configuration with Package.json\nDESCRIPTION: Example of using proper image tagging in Dockerfile and configuring package.json to use pre-installed versions of browser automation libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Wrapping Crawler Logic in AWS Lambda Handler Function\nDESCRIPTION: Structuring the crawler code as an AWS Lambda handler function, which is the entry point that AWS will execute. This pattern ensures proper initialization for each Lambda invocation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Querying Elements by CSS Selector in JavaScript Console\nDESCRIPTION: This code snippet demonstrates how to use document.querySelectorAll() to find all elements matching a specific CSS class selector. It's used to verify that the selector '.collection-block-item' correctly targets only the 31 collection cards on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json with Build Script for TypeScript\nDESCRIPTION: Basic package.json configuration that adds a TypeScript build script and specifies the main entry point to the compiled JavaScript file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler for Crawlee Crawler\nDESCRIPTION: This code demonstrates how to wrap the Crawlee crawler code in an exported handler function for AWS Lambda execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability\nDESCRIPTION: Demonstrates how to check if a product is in stock by looking for specific text in an element.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: Demonstrates how to get the product SKU using a specific class selector with Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Complete Crawlee Scraping Example (JavaScript)\nDESCRIPTION: This is the final version of the scraping code that includes data saving functionality. It uses PlaywrightCrawler to scrape a website and Dataset.pushData() to save the extracted data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Session Management with PuppeteerCrawler\nDESCRIPTION: Example of implementing session management with PuppeteerCrawler for browser automation. Shows configuration of sessions with Puppeteer's browser automation capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/session_management.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: This snippet shows how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Using Node with Playwright WebKit\nDESCRIPTION: Example of using the Docker image with Node.js and Playwright WebKit pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:20\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue in Crawlee\nDESCRIPTION: Creates a request queue instance and adds a URL to crawl. This demonstrates the basic setup for managing crawling requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/02-first-crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// First you create the request queue instance.\nconst requestQueue = await RequestQueue.open();\n// And then you add one or more requests to it.\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI Globally\nDESCRIPTION: Command to install the Apify CLI globally, which provides tools for authenticating with and deploying to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context Maps for Cross-Context Access\nDESCRIPTION: Example showing how to use the new crawling context ID system to maintain state across different page contexts.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a New Actor Project with Apify CLI\nDESCRIPTION: CLI command to generate a new actor project using the Apify CLI tool, which creates a directory with the necessary boilerplate files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Apify Actor\nDESCRIPTION: Bash commands to create a new Apify actor project and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Installing and Compressing Browser Dependencies for AWS Lambda\nDESCRIPTION: This snippet shows how to install the @sparticuz/chromium package and create a zip archive of node_modules for use as a Lambda Layer. This is necessary because the browser binaries are too large for direct Lambda deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Setting Apify API Token via Configuration\nDESCRIPTION: Code example showing how to set your Apify API token using the Configuration instance when initializing the Actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments in JavaScript (Pre-v1)\nDESCRIPTION: Example showing how handler arguments were separate objects in versions prior to SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests to Crawler with Batch Processing\nDESCRIPTION: Example of adding a large number of requests to a crawler using the addRequests method, which processes requests in batches of 1000 to avoid API rate limits. Includes options for waiting for all requests to be added.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Platform Datasets with Actor in Crawlee\nDESCRIPTION: This snippet demonstrates how to work with both local and cloud-based datasets in Crawlee. It shows how to open a local dataset by default and a remote dataset using the forceCloud option when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables are set.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Using SessionID with ProxyConfiguration in Standalone Mode\nDESCRIPTION: Demonstrates how to use sessionId parameter with ProxyConfiguration to maintain consistent proxy URLs for the same session IDs. This helps create a more realistic browsing pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// We'll always get the same proxy URL for the same session ID\nconst firstProxy = await proxyConfiguration.newUrl('my_session');\nconst secondProxy = await proxyConfiguration.newUrl('my_session');\nconst thirdProxy = await proxyConfiguration.newUrl('another_session');\n\nconsole.log(firstProxy); // http://proxy-1.com\nconsole.log(secondProxy); // http://proxy-1.com\nconsole.log(thirdProxy); // http://proxy-2.com\n```\n\n----------------------------------------\n\nTITLE: Crawling Single URL with got-scraping\nDESCRIPTION: Demonstrates how to fetch HTML content from a specified URL using the got-scraping package. The example shows the basic setup for making an HTTP request and handling the response. It references that the URL can be made dynamic through user input instead of being hardcoded.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\n// Fetch the HTML content of a web page\nconst response = await gotScraping('https://example.com');\nconsole.log(response.body);\n```\n\n----------------------------------------\n\nTITLE: Creating a Handler Function for GCP Cloud Functions\nDESCRIPTION: Wraps the crawler execution in an async handler function that takes request and response objects, returning the crawled data in the response. This function is exported for GCP to use as the entry point.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Command to install TypeScript compiler as a development dependency\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Using RequestQueueV2 with CheerioCrawler\nDESCRIPTION: This example demonstrates how to use a custom RequestQueueV2 instance with a CheerioCrawler. It shows that you must enable the requestLocking experiment in the crawler when using a RequestQueueV2 instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Documenting Crawlee Release 2.0.3 in Markdown\nDESCRIPTION: This snippet details the changes in Crawlee version 2.0.3, including the removal of 'forceUrlEncoding', limitations on 'handleRequestTimeoutMillis', and updates to dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_27\n\nLANGUAGE: markdown\nCODE:\n```\n## [2.0.3](https://github.com/apify/crawlee/compare/v2.0.2...v2.0.3) (2021-08-20)\n\n* **BREAKING IN EDGE CASES** * We removed `forceUrlEncoding` in `requestAsBrowser` because we found out that recent versions of the underlying HTTP client `got` already encode URLs\n  and `forceUrlEncoding` could lead to weird behavior. We think of this as fixing a bug, so we're not bumping the major version.\n* Limit `handleRequestTimeoutMillis` to max valid value to prevent Node.js fallback to `1`.\n* Use `got-scraping@^3.0.1`\n* Disable SSL validation on MITM proxie\n* Limit `handleRequestTimeoutMillis` to max valid value\n```\n\n----------------------------------------\n\nTITLE: Crawling Category Pages with Playwright and Crawlee\nDESCRIPTION: This snippet demonstrates how to use PlaywrightCrawler to crawl category pages of an e-commerce site. It uses enqueueLinks with a specific selector to add category links to the crawler's queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        // Wait for the category cards to render,\n        // otherwise enqueueLinks wouldn't enqueue anything.\n        await page.waitForSelector('.collection-block-item');\n\n        // Add links to the queue, but only from\n        // elements matching the provided selector.\n        await enqueueLinks({\n            selector: '.collection-block-item',\n            label: 'CATEGORY',\n        });\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests in Batches with Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method. This method handles queueing efficiently by adding the first 1000 requests immediately and processing the rest in the background to avoid API rate limits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio Crawler in Crawlee\nDESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl multiple specified URLs. It sets up the crawler, processes each page to extract the title, and saves the results to the default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run([\n    'https://crawlee.dev',\n    'https://apify.com',\n]);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: Example showing how to completely disable the browser fingerprinting feature in PlaywrightCrawler by setting useFingerprints to false in the browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\nconst crawler = new PlaywrightCrawler({\n    // We need to specify the playwright browser just like before\n    browserType: firefox,\n    // Here comes the important part - we disable fingerprints completely\n    browserPoolOptions: {\n        // This will tell the crawler to not use fingerprints at all\n        useFingerprints: false,\n    },\n    // Process the scraped data\n    async requestHandler({ request, page }) {\n        const data = {\n            url: request.url,\n            title: await page.title(),\n        };\n\n        // Save the data to the default dataset\n        await Dataset.pushData(data);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://playwright.dev']);\n```\n\n----------------------------------------\n\nTITLE: Building a Basic CheerioCrawler\nDESCRIPTION: Shows how to create a CheerioCrawler instance with a RequestQueue and a request handler to extract page titles using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Scraper with Child Processes in JavaScript\nDESCRIPTION: This code creates a parallel scraper using Node.js child processes. It forks multiple instances of itself, each running as a worker process to scrape URLs from the shared request queue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { getOrInitQueue } from './requestQueue.mjs';\nimport { fork } from 'child_process';\nimport { router } from './routes.mjs';\n\nif (process.env.IS_WORKER_THREAD) {\n    Configuration.set('purgeOnStart', false);\n    const requestQueue = await getOrInitQueue(false);\n    const config = new Configuration({\n        storageClientOptions: {\n            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,\n        },\n    });\n\n    const crawler = new CheerioCrawler({\n        requestQueue,\n        requestHandler: router,\n    }, config);\n\n    await crawler.run();\n    process.exit(0);\n} else {\n    const workerCount = 2;\n    const workers = [];\n\n    for (let i = 0; i < workerCount; i++) {\n        const worker = fork('./src/parallel-scraper.mjs', [], {\n            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },\n        });\n\n        workers.push(new Promise((resolve) => {\n            worker.on('exit', resolve);\n        }));\n\n        worker.on('message', (message) => {\n            console.log('Received:', message);\n        });\n    }\n\n    await Promise.all(workers);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: Example of disabling browser fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with crawlee.json file\nDESCRIPTION: Example of a crawlee.json configuration file that sets the state persistence interval to 10 seconds and log level to DEBUG. This file should be placed in the root of your project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running and Deploying an Actor\nDESCRIPTION: Commands to run an actor locally and deploy it to the Apify platform\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify run\napify push\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency Limits in CheerioCrawler\nDESCRIPTION: This snippet shows how to set the minimum and maximum number of parallel requests for a CheerioCrawler. These options control the crawler's scaling behavior based on system resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Start with at least 10 parallel requests\n    minConcurrency: 10,\n    // Scale up to a maximum of 100 parallel requests\n    maxConcurrency: 100,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from an Element with Cheerio\nDESCRIPTION: A simple example showing how to find the first h2 element on a page and extract its text content using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Demonstrates how to purge default storage directories using the purgeDefaultStorages helper function\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot using Puppeteer page.screenshot() Method\nDESCRIPTION: This snippet demonstrates how to use Puppeteer directly to capture a screenshot of a web page. It launches a browser, navigates to a URL, takes a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launch } from 'puppeteer';\n\n// Launch the browser.\nconst browser = await launch();\n\ntry {\n    // Open a new tab.\n    const page = await browser.newPage();\n\n    // Navigate to the URL.\n    await page.goto('https://crawlee.dev');\n\n    // Capture the screenshot.\n    const screenshotBuffer = await page.screenshot();\n\n    // Save the screenshot to the default key-value store.\n    await KeyValueStore.setValue('my-screenshot', screenshotBuffer, {\n        contentType: 'image/png',\n    });\n\n    console.log('Screenshot captured successfully!');\n} finally {\n    // Close Puppeteer.\n    await browser.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Using Playwright-WebKit Docker Image\nDESCRIPTION: Dockerfile configuration for using Apify's Playwright-WebKit image. This image comes with WebKit pre-installed for use with PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler with Unique Configuration for AWS Lambda\nDESCRIPTION: This snippet shows how to properly instantiate a CheerioCrawler with a unique Configuration instance and persistStorage set to false to ensure stateless operation on AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Failed Scraping Attempt with PlaywrightCrawler (No Wait)\nDESCRIPTION: This snippet demonstrates a failed attempt to scrape JavaScript-rendered content using PlaywrightCrawler without waiting for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of an actor card\n        const actorText = await page.locator('.ActorStoreItem').first().innerText();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer. Unlike previous versions that bundled Puppeteer, SDK v1 requires explicit installation of the browser automation library you want to use.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Setting up Express Server with Crawlee for GCP Cloud Run\nDESCRIPTION: A complete setup for running Crawlee in GCP Cloud Run with an Express HTTP server. The code creates an endpoint that initializes a crawler, runs it against specified URLs, and returns the scraped data in the HTTP response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/gcp-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-start\nimport express from 'express';\nconst app = express();\n// highlight-end\n\nconst startUrls = ['https://crawlee.dev'];\n\n\n// highlight-next-line\napp.get('/', async (req, res) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n    \n    await crawler.run(startUrls);    \n\n    // highlight-next-line\n    return res.send(await crawler.getData());\n// highlight-next-line\n});\n\n// highlight-next-line\napp.listen(parseInt(process.env.PORT) || 3000);\n```\n\n----------------------------------------\n\nTITLE: Sending Successful Response in SuperScraper with TypeScript\nDESCRIPTION: This function sends a successful response back to the user after scraping is complete. It retrieves the corresponding response object using the unique key, sends the scraped data, and then removes the response object from the map.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport const sendSuccResponseById = (responseId: string, result: unknown, contentType: string) => {\n    const res = responses.get(responseId);\n    if (!res) {\n        log.info(`Response for request ${responseId} not found`);\n        return;\n    }\n\n    res.writeHead(200, { 'Content-Type': contentType });\n    res.end(result);\n    responses.delete(responseId);\n};\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl\nDESCRIPTION: Limits the number of requests processed by the crawler. This is useful for testing or when crawling large websites to prevent processing too many pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with CSS Selector in Browser Console - TypeScript\nDESCRIPTION: Demonstrates how to use querySelector to find all elements with a specific CSS class. This code is run in the browser console to verify element selection and ensure only the desired elements (collection cards) are being targeted.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/04-real-world-project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration\nDESCRIPTION: Shows how to initialize Apify Proxy configuration and obtain a proxy URL. This enables IP address rotation for web scraping using Apify's proxy infrastructure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee\nDESCRIPTION: Demonstrates how to save scraped data into Crawlee's Dataset storage using the pushData() method instead of logging to console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI Globally\nDESCRIPTION: Command to install the Apify CLI tool globally for managing Apify projects and deployments\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in SDK v1\nDESCRIPTION: Example showing how to access crawler properties like requestQueue and autoscaledPool through the crawler property in the crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Hello World Actor Locally\nDESCRIPTION: Bash commands to create a boilerplate actor project using Apify CLI and run it locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Combining Request Queue and Request List\nDESCRIPTION: Demonstrates how to use both Request Queue and Request List together in a crawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestList = await RequestList.open('my-list', [\n    'https://example.com/page-1',\n    'https://example.com/page-2',\n    'https://example.com/page-3',\n]);\n\nconst requestQueue = await RequestQueue.open();\n\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    requestQueue,\n    async requestHandler({ page, request, enqueueLinks }) {\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Setting API Token with Configuration in Crawlee\nDESCRIPTION: Example of setting up the Apify API token programmatically using the Configuration instance, which enables access to Apify platform features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer in Bash\nDESCRIPTION: Command to install Apify SDK v1 along with Puppeteer using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Extracting and Formatting Product Price Using Playwright\nDESCRIPTION: Shows how to extract a product price, filter for the correct element using text content, and convert the string price into a numeric value. This includes handling currency symbols and thousand separators.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with Crawlee's key-value store including reading input, writing output, and managing named stores.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n\n// Get the INPUT from the default key-value store\nconst input = await KeyValueStore.getInput();\n\n// Write the OUTPUT to the default key-value store\nawait KeyValueStore.setValue('OUTPUT', { myResult: 123 });\n\n// Open a named key-value store\nconst store = await KeyValueStore.open('some-name');\n\n// Write a record to the named key-value store.\n// JavaScript object is automatically converted to JSON,\n// strings and binary buffers are stored as they are\nawait store.setValue('some-key', { foo: 'bar' });\n\n// Read a record from the named key-value store.\n// Note that JSON is automatically parsed to a JavaScript object,\n// text data is returned as a string, and other data is returned as binary buffer\nconst value = await store.getValue('some-key');\n\n// Delete a record from the named key-value store\nawait store.setValue('some-key', null);\n```\n\n----------------------------------------\n\nTITLE: Migrating from gotoFunction to Navigation Hooks in Crawlee\nDESCRIPTION: Demonstrates how to replace the old gotoFunction pattern with the more straightforward preNavigationHooks and postNavigationHooks approach, which simplifies page handling before and after navigation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Verifying Node.js Installation in Bash\nDESCRIPTION: Command to check the installed version of Node.js. Crawlee requires Node.js version 16.0 or higher.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL with String Manipulation in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the URL string and accessing specific parts. It assumes that all product URLs follow the pattern '/products/<manufacturer>'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Checking Stock Availability with Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to check if a product is in stock by finding a specific element on the page. It uses Playwright's filter method to find elements containing the text 'In stock' and the count method to determine existence.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP handler function for CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to wrap the CheerioCrawler in a handler function that can be exported and used as a GCP Cloud Function entry point, including returning the crawler data as the API response.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    return res.send(await crawler.getData())\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Request List Implementation\nDESCRIPTION: Demonstrates how to create and use a RequestList with a PuppeteerCrawler to process a predefined list of URLs\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestList, PuppeteerCrawler } from 'crawlee';\n\n// Prepare the sources array with URLs to visit\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Open the request list.\n// List name is used to persist the sources and the list state in the key-value store\nconst requestList = await RequestList.open('my-list', sources);\n\n// The crawler will automatically process requests from the list\n// It's used the same way for Cheerio /Playwright crawlers.\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page, request }) {\n        // Process the page (extract data, take page screenshot, etc).\n        // No more requests could be added to the request list here\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Session Management with CheerioCrawler and Proxies\nDESCRIPTION: Shows how to implement session management with CheerioCrawler to maintain session-specific proxy assignments, improving scraping resilience against blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\n// Let's create our crawler\nconst crawler = new CheerioCrawler({\n    // useSessionPool enables automatic session management\n    useSessionPool: true,\n    proxyConfiguration,\n    async requestHandler({ $, session, request }) {\n        // each request is automatically being assigned a random session\n        // and the proxy attached in proxyConfiguration is selected based\n        // on the session id\n        console.log(`Processing: ${request.url}`);\n        console.log(`Using session: ${session.id}`);\n        const title = $('title').text();\n        console.log(`Title: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Dockerfile\nDESCRIPTION: Shows how to specify the Node.js version when using an Apify Docker image. This ensures compatibility and stability in production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 16\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for Element Selection\nDESCRIPTION: This code snippet compares how to extract text content from a title element and collect all href links on a page using both plain browser JavaScript and Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Filtering\nDESCRIPTION: Example of filtering URLs using glob patterns in enqueueLinks configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy the local actor code to the Apify platform using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Minute in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set a limit on the number of requests per minute in a CheerioCrawler to avoid overwhelming the target website while maintaining high throughput.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Limit the crawler to 120 requests per minute:\n    maxRequestsPerMinute: 120,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Using Dataset.map() to Filter Heading Counts\nDESCRIPTION: Example demonstrating how to use Dataset.map() to transform dataset items. This code filters pages that have more than 5 headers and returns an array of their heading counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/map_and_reduce.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{MapSource}\n```\n\n----------------------------------------\n\nTITLE: Migrating from PuppeteerPool Methods to BrowserPool\nDESCRIPTION: Examples showing how to update code that used PuppeteerPool methods to the new BrowserPool equivalents in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Replacing launchPuppeteerFunction with Browser Pool Lifecycle Hooks\nDESCRIPTION: Example showing how to migrate from the custom `launchPuppeteerFunction` to the more flexible browser pool lifecycle hooks system, which allows for better composition of browser launch behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-webkit Docker Image\nDESCRIPTION: Shows how to use the Apify Docker image that includes Playwright and WebKit.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_9\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with disabled storage persistence\nDESCRIPTION: Creates a CheerioCrawler instance with a separate Configuration object where persistStorage is disabled, which is necessary for stateless cloud function environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON responseType with sendRequest\nDESCRIPTION: Example showing how to set responseType to 'json' when using sendRequest. This setting defines how the response should be parsed, allowing direct handling of JSON API responses.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Upgrading Crawlee using pip\nDESCRIPTION: Command to upgrade Crawlee to the latest version from PyPI using pip.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-06/index.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install --upgrade crawlee\n```\n\n----------------------------------------\n\nTITLE: Implementing GCP Cloud Function Handler for CheerioCrawler\nDESCRIPTION: Wrapping the CheerioCrawler execution in an async handler function that accepts request and response objects from GCP, and returns crawler data in the response. This setup enables the crawler to be triggered via HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController in SDK v1\nDESCRIPTION: Example showing how to properly use the BrowserController to manage browser instances and perform cross-browser compatible operations like setting cookies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: All-Domain Crawling Strategy\nDESCRIPTION: Configuration to follow all links regardless of domain using the all strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee (TypeScript)\nDESCRIPTION: This code snippet shows how to migrate event handling from Apify SDK to Crawlee. It demonstrates the change from using Apify.events to Actor.on for subscribing to events.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Proxy Configuration for Web Scraping\nDESCRIPTION: This snippet shows how to create a basic proxy configuration using the Apify Proxy service. It demonstrates how to initialize the proxy configuration and obtain a new proxy URL for web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in JavaScript\nDESCRIPTION: Using Dataset.pushData() function to save the extracted results to the default dataset. This method creates a new row in the dataset table with the provided data.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Adjusting Crawlee Code for Apify Platform Deployment\nDESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform, including initializing and exiting the Apify Actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Updated Path to Autoscaled Pool\nDESCRIPTION: Example showing the change in how to access the autoscaledPool property, which is now under the crawler object rather than directly on the context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets in Crawlee\nDESCRIPTION: Demonstrates how to open local and cloud-based datasets when using both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables. The example shows how to use the forceCloud option to access platform storages explicitly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler for HTTP Requests\nDESCRIPTION: Demonstrates how to use the sendRequest helper method with BasicCrawler to process requests through got-scraping. This example shows how to override request options and handle JSON responses.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Non-Persistent Storage\nDESCRIPTION: Configures the CheerioCrawler with a custom Configuration instance that disables persistent storage, which is necessary for serverless environments like GCP Cloud Functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: LiveView Feature Changes in BrowserPool\nDESCRIPTION: Explains that LiveView functionality is not available in BrowserPool, unlike the previous PuppeteerPool implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string. This method assumes that product URLs follow a pattern where the manufacturer name appears at the beginning of the product slug.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Independent Storage in AWS Lambda\nDESCRIPTION: This JavaScript code shows how to initialize a PlaywrightCrawler with a new Configuration instance that has persistStorage disabled. This prevents storage conflicts between multiple crawler instances running in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support\nDESCRIPTION: This example shows how to create a request queue that supports locking by using the RequestQueueV2 class instead of the standard RequestQueue. The queue is initialized and populated with multiple URL requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/experiments/request_locking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/js/docs' },\n    { url: 'https://crawlee.dev/js/api' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()\nDESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It sets up a PuppeteerCrawler, navigates to a URL, and uses the saveSnapshot() utility to capture and save the screenshot.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, crawler: { browserPool }, log }) {\n        const { utils } = browserPool;\n\n        log.info(`Saving snapshot of ${request.url}...`);\n        await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Handler Function for Cheerio Crawler\nDESCRIPTION: Wrapping Crawlee crawler logic in an AWS Lambda handler function to make it compatible with AWS Lambda execution. This ensures that each Lambda invocation creates a fresh crawler instance to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Creating a Lambda Handler Function for CheerioCrawler\nDESCRIPTION: This code snippet demonstrates how to wrap a CheerioCrawler in an AWS Lambda handler function to make it executable in the Lambda environment. The crawler is instantiated inside the handler to maintain statelessness.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n// highlight-next-line\n};\n```\n\n----------------------------------------\n\nTITLE: Using a Request Queue with Locking in Crawlers\nDESCRIPTION: Demonstrates how to use a custom request queue that supports locking with a crawler by enabling the requestLocking experiment and passing the queue to the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Request Queue for Batch Request Addition in Crawlee\nDESCRIPTION: Demonstrates how to use the Request Queue to add multiple requests in a batch, which is more efficient than adding them one by one.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\n\n// Define an array of request-like objects\nconst sources = [\n    { url: 'http://www.example.com/page-1' },\n    { url: 'http://www.example.com/page-2' },\n    { url: 'http://www.example.com/page-3' },\n];\n\n// Add all the requests to the queue in one call\nawait requestQueue.addRequests(sources);\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ request }) {\n        console.log(request.url);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with CheerioCrawler\nDESCRIPTION: Example of using SessionPool with CheerioCrawler for handling sessions in HTML parsing scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Comparing DOM Selection in Cheerio vs Browser JavaScript\nDESCRIPTION: Demonstrates the difference between using Cheerio's jQuery-like syntax and plain JavaScript for DOM manipulation, specifically for selecting title elements and href attributes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Using Main Function in TypeScript\nDESCRIPTION: Shows the alternative approach using Actor.main() wrapper function which handles initialization and cleanup automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Setting Apify API Token via Configuration Instance\nDESCRIPTION: Code snippet showing how to provide Apify credentials programmatically using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Setting Min and Max Concurrency in Crawlee\nDESCRIPTION: Controls the minimum and maximum number of parallel requests that can run at any time. The crawler starts with minConcurrency (10) and can scale up to maxConcurrency (50) based on system resources.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // At the minimum, we want to use 10 parallel requests\n    minConcurrency: 10,\n    // And at most, we want to use 50 parallel requests\n    maxConcurrency: 50,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio with Browser JavaScript for DOM Manipulation\nDESCRIPTION: This snippet demonstrates how to use Cheerio's jQuery-like syntax compared to plain browser JavaScript for common DOM operations like selecting elements and extracting content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Dataset.map() Result Example\nDESCRIPTION: The expected output from the map() method example, showing an array of heading counts that meet the filtering criteria (greater than 5).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n[11, 8]\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Apify Actor Locally\nDESCRIPTION: Bash commands to create a new Apify actor project and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns\nDESCRIPTION: Shows how to use glob patterns with enqueueLinks to filter URLs, following only links that match the specified pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Node.js Docker Configuration\nDESCRIPTION: Basic Docker configuration for Node.js-based crawlers without browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Updating package.json for GCP deployment\nDESCRIPTION: Modification to the package.json file to set the main entry point for the GCP Cloud Function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Data from Child Process to Parent in Node.js\nDESCRIPTION: Demonstrates how to send scraped data from a child process back to the parent process instead of using context.pushData. This is necessary when using child processes to ensure data is centralized in the parent's dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.send(data);\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Disabled Storage Persistence\nDESCRIPTION: Update the CheerioCrawler initialization to use a separate Configuration instance with persistStorage set to false. This is necessary for cloud functions as they have ephemeral file systems.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image\nDESCRIPTION: Examples of using pre-release versions of Apify Docker images for testing purposes, with and without library version specification.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:16-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Configuring enqueueLinks to Include Subdomains\nDESCRIPTION: Demonstrates how to use the 'same-domain' strategy with enqueueLinks to include subdomain links in crawling, rather than the default behavior which only stays on the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/03-adding-urls.mdx#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL with JavaScript String Manipulation\nDESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string. It obtains the last part of the URL and then extracts the first segment as the manufacturer name.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Demonstrates how to purge default storage directories in Crawlee using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/result_storage.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Getting a Public URL for a Stored Item on Apify Platform\nDESCRIPTION: Code example demonstrating how to store an item in Key-Value Store and get a shareable public URL for it on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Retiring Browsers with BrowserPool\nDESCRIPTION: Shows how to retire a browser using the new BrowserPool API compared to the previous PuppeteerPool method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Script for Crawlee TypeScript Project\nDESCRIPTION: Add a start:prod script to package.json for running the compiled JavaScript code of a Crawlee TypeScript project in production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging into Apify CLI\nDESCRIPTION: Commands to install the Apify CLI globally and log in with an API token\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Creating an HTTP Handler Function for GCP Cloud Functions\nDESCRIPTION: Exports an asynchronous handler function that initializes and runs the crawler, then sends the collected data as a response. This function serves as the entry point for the GCP Cloud Function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Demonstrates proper usage of BrowserController for browser management tasks like closing browsers and setting cookies, which ensures compatibility with both Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with CSS Selectors in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use document.querySelectorAll() with a CSS class selector to find all elements with the 'collection-block-item' class. This technique is used to verify that the selector targets only the desired elements (31 collection cards) and nothing else.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Crawlee Actor with Node.js\nDESCRIPTION: This Dockerfile sets up an environment for running a Crawlee actor. It uses the apify/actor-node:20 base image, installs NPM packages, copies the source code, and specifies the command to start the actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Crawlee with Data Return\nDESCRIPTION: This is the final version of the main.js script for running Crawlee on AWS Lambda. It includes the handler function, crawler initialization, and returns the scraped data as the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This example shows how to integrate SessionPool with HttpCrawler in Crawlee. It configures the HttpCrawler with SessionPool options and demonstrates session usage in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { HttpCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new HttpCrawler({\n    async requestHandler({ session, sendRequest }) {\n        // Use session\n        const { body } = await sendRequest({\n            useExtendedUniqueKey: true,\n            session,\n        });\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in CheerioCrawler\nDESCRIPTION: Shows how to access and inspect information about the currently used proxy in CheerioCrawler's requestHandler function. This is useful for debugging or logging proxy usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, $, proxyInfo }) {\n        console.log(proxyInfo.url); // http://proxy-1.com\n        console.log(proxyInfo.hostname); // proxy-1.com\n    },\n});\n\nawait crawler.run(['https://example.com/']);\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with Pattern Matching in Playwright Crawler\nDESCRIPTION: Shows how to use the enqueueLinks method with glob patterns to filter URLs to be crawled. This example demonstrates filtering links using the globs parameter to match specific URL patterns.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Handling User Requests in SuperScraper with TypeScript\nDESCRIPTION: This code snippet demonstrates how SuperScraper handles incoming user requests. It checks for an existing crawler with the specified proxy configuration or creates a new one if needed, then adds the request to the crawler's queue for processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst key = JSON.stringify(crawlerOptions);\nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\n\nawait crawler.addRequests([request]);\n```\n\n----------------------------------------\n\nTITLE: Configuring Header Generation with BasicCrawler in TypeScript\nDESCRIPTION: Shows how to configure header generation options when using sendRequest with BasicCrawler. This allows for customizing the browser fingerprint used in requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Without Explicit Element Waiting\nDESCRIPTION: An incorrect implementation of PuppeteerCrawler that fails to scrape JavaScript-rendered content because it doesn't wait for elements to appear before attempting to extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be in the DOM yet\n        const element = await page.$('.ActorStoreItem');\n        const actorText = await page.evaluate(el => el.textContent, element);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Setting up a PlaywrightCrawler with Router in Crawlee\nDESCRIPTION: Main entry file that initializes a PlaywrightCrawler with a router for handling requests. It configures logging and starts the crawler with a seed URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/08-refactoring.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Basic CheerioCrawler Attempt (Failing to Scrape JavaScript-Rendered Content)\nDESCRIPTION: This snippet demonstrates an attempt to scrape Apify Store using CheerioCrawler. It fails because CheerioCrawler cannot execute client-side JavaScript, resulting in empty actor card content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Playwright\nDESCRIPTION: Implementation of multi-URL crawling using Playwright Crawler in Crawlee. Demonstrates the setup of a Playwright crawler for processing multiple URLs and extracting data using modern browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, createPlaywrightRouter, PlaywrightCrawler } from 'crawlee';\n\nconst sources = [\n    'https://crawlee.dev',\n    'https://apify.com',\n];\n\nconst router = createPlaywrightRouter();\n\nrouter.addDefaultHandler(async ({ page, request, log }) => {\n    const title = await page.title();\n    log.info(`Title of ${request.url} is '${title}'`);\n\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(sources);\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local and cloud-based datasets using the Actor class when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR are configured. Shows the difference between default local storage and forced cloud storage access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation\nDESCRIPTION: Demonstrates a simple CheerioCrawler setup that fails to extract JavaScript-rendered content from Apify Store. Shows why static HTML crawling isn't sufficient for client-side rendered pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform with CLI\nDESCRIPTION: Commands to install Apify CLI and log in using your API token to enable Apify platform features in your Crawlee projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform via Configuration\nDESCRIPTION: JavaScript code snippet showing how to log into the Apify platform using the Configuration instance and setting the API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links Regardless of Domain\nDESCRIPTION: Shows how to configure enqueueLinks to follow all links from a page regardless of domain, which allows unrestricted crawling of the web.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PuppeteerCrawler\nDESCRIPTION: Example of how to customize browser fingerprints in PuppeteerCrawler by specifying browser type, version, and operating system. This customization helps avoid detection as a scraper.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/avoid_blocking.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    // Limit the number of concurrent requests to avoid overloading the target website\n    maxConcurrency: 5,\n    // Here comes the important part - you can override predefined fingerprints or their parts\n    browserPoolOptions: {\n        // This defines the fingerprints we want to use\n        fingerprintOptions: {\n            // We want to use the newest Chrome\n            browser: 'chrome',\n            // We want to pretend to be a visitor from Germany\n            locales: [\n                'de',\n            ],\n            // We want to pretend to be on Windows 11 (Win10 = Win11)\n            os: 'win10',\n        },\n    },\n    // Let's slow down each request a bit\n    requestHandlerTimeoutSecs: 30,\n    navigationTimeoutSecs: 30,\n    // Process the scraped data\n    async requestHandler({ request, page }) {\n        const data = {\n            url: request.url,\n            title: await page.title(),\n        };\n\n        // Save the data to the default dataset\n        await Dataset.pushData(data);\n    },\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://puppeteer.dev']);\n```\n\n----------------------------------------\n\nTITLE: Error Log from Failed Element Selection\nDESCRIPTION: The console error output when attempting to select elements that haven't been rendered yet in the DOM, demonstrating why proper waiting strategies are necessary when scraping JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_7\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Browser Search URL Example for Accommodation Website\nDESCRIPTION: The URL that appears in the browser address bar when performing a search on the website, containing parameters for location, filters, and coordinates.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.accommodationforstudents.com/search-results?location=London&beds=0&occupancy=min&minPrice=0&maxPrice=500&latitude=51.509865&longitude=-0.118092&geo=false&page=1\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform via CLI\nDESCRIPTION: Commands to install Apify CLI and log in to your Apify account using an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Configuring Cheerio Crawler with Unique Configuration for AWS Lambda\nDESCRIPTION: Initial code modification showing how to instantiate a CheerioCrawler with a unique Configuration instance and persistStorage set to false to handle Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using the Configuration class\nDESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Local Actor\nDESCRIPTION: Commands to create a new actor project and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json file\nDESCRIPTION: Example of a crawlee.json configuration file placed in the project root to set global Crawlee options. This example sets the persistStateIntervalMillis to 10000 ms and logLevel to DEBUG.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Crawled Data with Crawlee for Python\nDESCRIPTION: This snippet shows how to export the collected data to a JSON file using Crawlee's export_data_json method after the crawler finishes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nawait crawler.export_data_json('crunchbase_data.json')\n```\n\n----------------------------------------\n\nTITLE: Inspecting Current Proxy in JSDOMCrawler\nDESCRIPTION: Shows how to access proxy information in JSDOMCrawler request handlers. Provides visibility into which proxy is being used for specific requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/proxy_management.mdx#2025-04-11_snippet_14\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window, proxyInfo, log }) {\n        // Log information about the currently used proxy\n        if (proxyInfo) {\n            log.info(`Currently using proxy: ${proxyInfo.url}`);\n        }\n        const { document } = window;\n        // Process the downloaded page...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Storage Directory Structure for Key-Value Store\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing key-value data on disk.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Accessing Public URL of a Key-Value Store Item\nDESCRIPTION: Example of how to get a shareable public URL for an item stored in a Key-Value Store on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest Helper in BasicCrawler\nDESCRIPTION: Shows how to use the new sendRequest helper in a BasicCrawler to process requests through got-scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Provides a complete package.json configuration including all necessary scripts, dependencies, and project settings for a TypeScript Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging into Apify CLI\nDESCRIPTION: Commands to install the Apify CLI globally and log in with an API token\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright\nDESCRIPTION: This code checks if a product is in stock by looking for a specific element on the page. It uses Playwright's filter method to find elements containing the text 'In stock' and determines availability based on whether such elements exist.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 along with Playwright for additional browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs with Cheerio\nDESCRIPTION: Implementation of multi-URL crawling using Cheerio Crawler in Crawlee. Demonstrates how to set up a basic Cheerio crawler to process multiple URLs and extract data from them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, createCheerioRouter, CheerioCrawler } from 'crawlee';\n\nconst sources = [\n    'https://crawlee.dev',\n    'https://apify.com',\n];\n\nconst router = createCheerioRouter();\n\nrouter.addDefaultHandler(async ({ enqueueLinks, log, $, request }) => {\n    const title = $('title').text();\n    log.info(`Title of ${request.url} is '${title}'`);\n\n    await Dataset.pushData({\n        url: request.url,\n        title,\n    });\n});\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(sources);\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Requests in Batches with crawler.addRequests()\nDESCRIPTION: Demonstrates how to add many requests to a crawler in batches. The method resolves after the first batch is added while continuing to add remaining requests in the background.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue in TypeScript for Crawlee v3.0\nDESCRIPTION: This code snippet demonstrates how to create a RequestQueue instance in TypeScript using Crawlee v3.0. It shows both the named and default RequestQueue initialization methods, including how to specify a name parameter.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Opens a request queue. If it doesn't exist, it will be created.\nconst queue = await RequestQueue.open('my-queue');\n\n// You can also use the default queue.\nconst defaultQueue = await RequestQueue.open();\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler Implementation\nDESCRIPTION: Full implementation of the AWS Lambda handler function incorporating Crawlee crawler with AWS Chromium integration and proper response handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to use both local and cloud-based datasets in Crawlee by using the Actor class. This snippet shows how to open a local dataset by default and how to access a remote dataset using the forceCloud option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Retrieving User Input in a Crawlee Actor\nDESCRIPTION: This code retrieves actor input from the default key-value store using the Actor SDK, validates its structure, and logs it to the console. It demonstrates the standard pattern for accessing user-provided configuration in Apify actors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of the actor\nconst input = await Actor.getInput();\nconsole.log('Input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with JSDOMCrawler in Crawlee\nDESCRIPTION: This example demonstrates how to implement SessionPool with JSDOMCrawler in Crawlee. It configures the JSDOMCrawler with SessionPool options and shows session usage in the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/session_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, session }) {\n        // Use session\n        // ...\n    },\n    sessionPoolOptions: {\n        maxPoolSize: 100,\n        createSessionFunction: async (sessionPool) => {\n            const session = await sessionPool.getSession();\n            session.userData.foo = 'bar';\n            return session;\n        },\n    },\n    useSessionPool: true,\n    proxyConfiguration,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context IDs for Cross-Context Access\nDESCRIPTION: Shows how to use the new id property of crawlingContext to maintain references across different contexts, useful for complex scraping scenarios.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest Helper with BasicCrawler\nDESCRIPTION: Shows how to use the context.sendRequest() helper function to process requests through got-scraping. This replaces the deprecated requestAsBrowser method and allows processing context-bound Request objects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Request.label for request classification in enqueueLinks\nDESCRIPTION: Demonstrates how to use the Request.label shortcut for classifying requests. This example shows checking the current request's label and enqueuing new links with a specific label.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright in JavaScript\nDESCRIPTION: This code checks whether a product is in stock by filtering elements with the class 'product-form__inventory' that contain the text 'In stock', then checking if any matching elements exist using the count method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all necessary scripts and dependencies for both development and production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using preLaunchHooks with crawlingContext in Apify SDK (JavaScript)\nDESCRIPTION: This snippet shows how to use 'preLaunchHooks' with access to the 'crawlingContext' in Apify's PuppeteerCrawler. It allows for dynamic configuration based on the current request being processed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserPool with Lifecycle Hooks\nDESCRIPTION: Example showing how to configure BrowserPool options and hooks, such as setting browser retirement policy and using pre-launch hooks to dynamically modify launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Handling Page Function with Crawling Context in SDK v1\nDESCRIPTION: Example demonstrating the new crawling context approach in SDK v1, where all handler functions receive the same context object, making it easier to share state between functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Using a Custom HTTP Client with BasicCrawler in TypeScript\nDESCRIPTION: Example demonstrating how to instantiate a custom HTTP client implementation and pass it to a BasicCrawler constructor. This shows the complete setup for integrating a custom client with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/custom-http-client/custom-http-client.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from '@crawlee/core';\n\n// Create your implementation\nconst httpClient = new FetchHttpClient({\n    // Some custom options\n    timeout: 30000,\n});\n\n// Pass it to the crawler\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest }) {\n        // sendRequest will use your implementation\n        const data = await sendRequest({\n            url: 'https://example.com',\n        });\n\n        console.log(data);\n    },\n    httpClient,\n});\n\nawait crawler.run(['https://example.com']);\n\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration Example\nDESCRIPTION: Example demonstrating how to configure local storage with ApifyStorageLocal.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Browser JavaScript Comparison for Page Title Access in JSDOM vs Browsers\nDESCRIPTION: This snippet demonstrates the difference between accessing the page title in a browser environment versus in JSDOM. In browsers, you can use document.title directly, while in JSDOM you typically use window.document.title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Command to install Apify CLI and log in to your Apify account using an API token, which allows you to run scrapers with your credentials.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Accessing User Input in Crawlee Actors\nDESCRIPTION: Code example showing how to access and log user input in a Crawlee actor using the Actor.getInput() method. This is the primary way to retrieve input data provided to your actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Retrieve the input of your actor\nconst input = await Actor.getInput();\nconsole.log('Input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Using All Strategy with enqueueLinks\nDESCRIPTION: Example showing how to configure enqueueLinks to follow all links regardless of domain, effectively allowing the crawler to explore the entire internet.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in SDK v1\nDESCRIPTION: Example showing how the new Crawling Context works in SDK v1. All handler functions now receive the same context object, making it easier to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Using Logging in Crawler Context\nDESCRIPTION: Example of how to use the scoped log instance provided in the crawling context, which prefixes log messages with the crawler name for better organization of log output.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Scraping with CheerioCrawler (JavaScript)\nDESCRIPTION: Example of using CheerioCrawler to attempt scraping JavaScript-rendered content from Apify Store. This approach fails because Cheerio cannot execute JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: This example demonstrates how to enable the request locking experiment in a CheerioCrawler instance. The experiment is enabled through the experiments configuration object with the requestLocking property set to true.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform with CLI\nDESCRIPTION: Command to deploy your local actor code to the Apify platform using the Apify CLI, allowing it to be run in the cloud environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Actor Locally\nDESCRIPTION: Commands to create a boilerplate actor project using Apify CLI and then run it locally. The CLI prompts you to select a template and creates the necessary project files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Failed Extraction Output\nDESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating why it fails with JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform\nDESCRIPTION: Command to log in to the Apify Platform using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Example of specifying Node.js version 16 in an Apify Docker image to ensure compatibility and stability.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 16\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets in Crawlee\nDESCRIPTION: Demonstrates how to access both local and remote datasets using the Actor class when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables are set.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Running an Apify Actor Locally\nDESCRIPTION: Demonstrates how to run an Apify actor project locally using the Apify CLI. This allows you to test and debug your actor before deploying it to the platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Location\nDESCRIPTION: Demonstrates how to configure Apify Proxy with specific proxy groups and country selection. Sets up residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee using crawlee.json\nDESCRIPTION: Example of basic Crawlee configuration using a JSON file that specifies persistStateIntervalMillis and logLevel options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/configuration.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistStateIntervalMillis\": 10000,\n  \"logLevel\": \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Apify Proxy Configuration in Crawlee\nDESCRIPTION: Shows how to create a proxy configuration using Apify Proxy in Crawlee. This snippet demonstrates the basic usage of Actor.createProxyConfiguration() to obtain a proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Using gotoFunction in PuppeteerCrawler (Old Pattern)\nDESCRIPTION: Example of the deprecated gotoFunction pattern which requires manual handling of pre-processing, navigation with gotoExtended, post-processing, and response return.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Complete package.json configuration including all dependencies and scripts for development and production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI Globally\nDESCRIPTION: Command to install the Apify CLI globally for authentication and deployment tasks.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Crawlee Result JSON Example\nDESCRIPTION: Sample JSON output stored by Crawlee after crawling, showing the extracted data structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 along with Puppeteer, which is no longer bundled with the SDK.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Functions in Apify SDK for JavaScript\nDESCRIPTION: Demonstrates the changes in launch function arguments for both Puppeteer and Playwright in Apify SDK. It shows how to use the new launchContext structure and how to specify custom modules for launching browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Session Management with JSDOMCrawler and Proxies\nDESCRIPTION: Demonstrates how to combine session management with proxy configuration in JSDOMCrawler. Each session maintains consistent identity with the same proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    persistCookiesPerSession: true, // This is the default\n    async requestHandler({ session, request, window }) {\n        console.log(`Using session ${session.id}`);\n        console.log(`Fetched ${request.url} with title: ${window.document.title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Persistent Storage Disabled for GCP Cloud Run\nDESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a Configuration instance that disables persistent storage, which is necessary for stateless execution in Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Exporting an Entire Dataset to CSV in Crawlee\nDESCRIPTION: This example demonstrates how to export the entire default dataset to a single CSV file and store it in a key-value store named 'my-data'. It uses the exportToValue function with CSV format configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\n/**\n * Example usage of the Dataset.exportToValue() function to export the entire\n * default dataset to a single CSV file in the default key-value store.\n */\nawait Dataset.exportToValue({\n    format: 'csv',\n    store: await KeyValueStore.open('my-data'),\n    key: 'DATA.csv',\n});\n\n```\n\n----------------------------------------\n\nTITLE: Setting up HTTP Server with Node.js for Crawlee\nDESCRIPTION: Creates a basic HTTP server using Node.js built-in 'http' module. The server listens on port 3000 and logs incoming requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createServer } from 'http';\nimport { log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    // We will return the page title here later instead\n    res.end('Hello World\\n');\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Storage Persistence Disabled\nDESCRIPTION: Creating a CheerioCrawler instance with a separate Configuration object that disables storage persistence, which is necessary for stateless cloud function execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Operations in Crawlee\nDESCRIPTION: This snippet illustrates how to perform basic operations with datasets in Crawlee, including pushing data to the default dataset and working with named datasets.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/result_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Comparing Cheerio and Plain JavaScript DOM Manipulation\nDESCRIPTION: This snippet demonstrates the syntax differences between browser JavaScript and Cheerio for common DOM operations like selecting elements and extracting content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/cheerio_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the text content of the <title> element.\ndocument.querySelector('title').textContent; // plain JS\n$('title').text(); // Cheerio\n\n// Return an array of all 'href' links on the page.\nArray.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS\n$('[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get(); // Cheerio\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific browser and operating system parameters for fingerprint generation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\n                    { name: 'firefox', minVersion: 88 },\n                    { name: 'chrome', minVersion: 93 },\n                ],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using crawler-scoped logging in request handler\nDESCRIPTION: Demonstrates how to use the scoped log instance provided in the crawling context. This logger prefixes messages with the crawler name and is recommended for logging inside request handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Updated Browser Launch Functions in JavaScript\nDESCRIPTION: Demonstrates the new structure for launching browsers with both Puppeteer and Playwright, including support for custom modules and standardized launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteer = require('puppeteer');\nconst playwright = require('playwright');\n\nawait Apify.launchPuppeteer();\n// Is the same as:\nawait Apify.launchPuppeteer({\n    launcher: puppeteer\n})\n\nawait Apify.launchPlaywright();\n// Is the same as:\nawait Apify.launchPlaywright({\n    launcher: playwright.chromium\n})\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV File using Apify Crawlee in TypeScript\nDESCRIPTION: This code exports the entire default dataset to a single CSV file and stores it in the default key-value store. It uses the 'exportToValue' function from the Dataset class and sets the CSV format options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nawait Dataset.exportToValue('OUTPUT', {\n    contentType: 'text/csv',\n    csvOptions: {\n        columns: ['title', 'url'],\n    },\n});\n\nconst store = await KeyValueStore.open();\nconst value = await store.getValue('OUTPUT');\nconsole.log(value);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Google Maps Scraper with Python\nDESCRIPTION: Commands to create a virtual environment, activate it, and install required dependencies including Crawlee and Playwright for the Google Maps scraper.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create and activate a virtual environment\npython -m venv google-maps-scraper\n\n# Windows:\n.\\google-maps-scraper\\Scripts\\activate\n\n# Mac/Linux:\nsource google-maps-scraper/bin/activate\n\n# We plan to use Playwright with Crawlee, so we need to install both:\npip install crawlee \"crawlee[playwright]\"\nplaywright install\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: A snippet that demonstrates how to extract all href values from link elements on a page. It uses querySelectorAll to find all <a> elements with href attributes and maps them to an array of URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This code example demonstrates how to completely disable the browser fingerprinting feature in PlaywrightCrawler by setting the useFingerprints option to false in the browserPoolOptions configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        // The default value is true\n        useFingerprints: false,\n    },\n    // ... other crawler options\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to enable the request locking experiment in a CheerioCrawler instance. The experiment is enabled by setting the 'requestLocking' option to true in the 'experiments' object of the crawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Comparing Browser and JSDOM Page Title Access\nDESCRIPTION: Demonstrates how to access page title in both browser JavaScript and JSDOM environments. Shows the slight syntax difference between the two contexts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Updated Arguments for Puppeteer Launch Function\nDESCRIPTION: Shows the change in argument structure for the Apify.launchPuppeteer() function, separating Apify-specific options from Puppeteer's native launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    headless: true,\n})\n\n// NEW\nawait Apify.launchPuppeteer({\n    useChrome: true,\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing CheerioCrawler Manually\nDESCRIPTION: Command to manually install Crawlee with npm for using CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Globs in enqueueLinks\nDESCRIPTION: This snippet demonstrates how to use globs to filter URLs when enqueueing links in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Using Direct Methods in TypeScript\nDESCRIPTION: Demonstrates how to initialize and exit an Actor using the direct methods Actor.init() and Actor.exit()\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple JSDOM Crawler\nDESCRIPTION: Example of creating a JSDOM crawler that uses JSDOM library to parse HTML and extract data from web pages in a Node.js environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler } from 'crawlee';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ request, window, log }) {\n        const { document } = window;\n        const title = document.querySelector('title')?.textContent ?? 'Unknown title';\n        log.info(`Title of ${request.url} is: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy your local actor code to the Apify platform, where it will be built and can be run on Apify's infrastructure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify SDK with API Token\nDESCRIPTION: JavaScript code showing how to initialize the Apify SDK with an API token using Configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Console Output from Failed CheerioCrawler Attempt\nDESCRIPTION: This log shows the empty output from the CheerioCrawler attempt, demonstrating that it can't access JavaScript-rendered content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/javascript-rendering.mdx#2025-04-11_snippet_1\n\nLANGUAGE: log\nCODE:\n```\nACTOR:\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code as an Actor Using Actor.main()\nDESCRIPTION: Example showing how to use Actor.main() to wrap Crawlee crawler code to run as an actor on the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nawait Actor.main(async () => {\n    // Initialize the crawler\n    const crawler = new CheerioCrawler({\n        // Function called for each URL\n        async requestHandler({ request, $ }) {\n            const title = $('title').text();\n            const h1 = $('h1').text();\n            await Dataset.pushData({\n                url: request.url,\n                title,\n                h1,\n            });\n        },\n        // Let's limit our crawls to make our tests shorter and safer\n        maxRequestsPerCrawl: 20,\n    });\n\n    // Add first URL to the queue and start the crawl\n    await crawler.run(['https://crawlee.dev']);\n});\n\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Configuration\nDESCRIPTION: Docker configuration specific to Playwright with Chrome browser support. Optimized for size by limiting to single browser.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Configuring maxRequestsPerMinute in Crawlee\nDESCRIPTION: Sets up a CheerioCrawler with a limit of 100 requests per minute to prevent overloading the target website while maintaining crawler efficiency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // other configuration options\n    maxRequestsPerMinute: 100,\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy in JSDOMCrawler\nDESCRIPTION: Variable reference InspectionJSDOMSource showing proxy inspection in JSDOMCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionJSDOMSource}\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using Configuration\nDESCRIPTION: Demonstrates how to log into the Apify platform programmatically by setting the API token in the Actor configuration. This allows your code to access Apify platform features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Handling JSON Responses with ResponseType Option\nDESCRIPTION: Example demonstrating how to configure sendRequest to handle JSON API responses by setting the responseType option to 'json' instead of the default 'text'.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Pre-release Beta Configuration\nDESCRIPTION: Examples of using pre-release beta versions of Docker images\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:16-beta\n\n# With library version.\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TSConfig file setup with ES2022 module system and compiler options\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Services Directly via Crawler Constructor\nDESCRIPTION: This snippet shows how to pass services directly to the ParselCrawler constructor. The crawler will automatically register these services with the ServiceLocator, simplifying the initialization process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.events import LocalEventManager\nfrom crawlee.storage_clients import MemoryStorageClient\n\n\nasync def main() -> None:\n    crawler = ParselCrawler(\n        configuration=Configuration(),\n        storage_client=MemoryStorageClient(),\n        event_manager=LocalEventManager(),\n    )\n\n    # ...\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Log Output\nDESCRIPTION: Example log output showing Crawlee crawling the Crawlee website, displaying page titles as it visits different URLs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple preLaunchHooks\nDESCRIPTION: Example of combining multiple preLaunchHooks functions to create a custom browser launch pipeline with predefined behaviors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local and remote datasets using the Actor class. This code shows how to open a local dataset by default and a remote dataset by using the forceCloud option.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with URL Pattern Matching in TypeScript\nDESCRIPTION: Demonstrates how to use enqueueLinks method with URL pattern matching via globs to filter URLs for crawling. This allows targeting specific URL patterns when discovering links on a webpage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Reduce Method with Crawlee Dataset\nDESCRIPTION: Shows how to use the Dataset.reduce() method to aggregate data across all dataset items, calculating the total number of heading elements across all scraped pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nawait Dataset.open();\nconst pagesHeadingCount = await Dataset.reduce((memo, item) => {\n    return memo + item.headingCount;\n}, 0);\n\nconsole.log('Total number of headings:', pagesHeadingCount);\n\nconst store = await KeyValueStore.open();\nawait store.setValue('total-heading-count', pagesHeadingCount);\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Playwright Crawler\nDESCRIPTION: Implementation of a Playwright-based crawler that follows and processes all links within a website's subdomain. Uses the enqueueLinks() method with Playwright for modern browser automation-based crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, enqueueLinks } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks, page, log }) {\n        log.info('enqueueing new URLs');\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Playwright URL Crawler Implementation\nDESCRIPTION: Web scraping implementation using Playwright Crawler for modern browser automation. Requires apify/actor-node-playwright-chrome image for execution on Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_multiple_urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Dataset, KeyValueStore, createPlaywrightRouter, PlaywrightCrawler } from 'crawlee';\n\n// Example data structure\ninterface Article {\n    url: string;\n    title: string;\n    description: string;\n}\n\n// Add URLs to the list here\nconst START_URLS = [\n    'https://crawlee.dev',\n    'https://crawlee.dev/docs/quick-start',\n    'https://crawlee.dev/docs/introduction',\n];\n```\n\n----------------------------------------\n\nTITLE: Creating CheerioCrawler for Web Scraping\nDESCRIPTION: Sets up a CheerioCrawler with keepAlive option to continuously wait for new requests. The crawler extracts the page title from each scraped page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        // We will send the response here later\n        log.info(`Page title: ${title} on ${request.url}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Requests Per Minute in Crawlee\nDESCRIPTION: This code demonstrates how to limit the rate of requests by setting the maxRequestsPerMinute option in a CheerioCrawler. It limits the crawler to 120 requests per minute, preventing it from overwhelming the target website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: js\nCODE:\n```\n// Set the crawler to do maximum 120 requests per minute (or 2 requests per second)\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120,\n    // other options...\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox Browser\nDESCRIPTION: Demonstrates importing and setting up a PlaywrightCrawler instance using Firefox browser. Shows configuration for headless mode and provides browser setup for Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\nimport { firefox } from 'playwright';\n\nawait new PlaywrightCrawler({\n    browserPoolOptions: {\n        browserPlugin: firefox,\n    },\n    async requestHandler({ page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${page.url()} is '${title}'`);\n    },\n}).run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Managing Crawling Contexts with IDs in JavaScript\nDESCRIPTION: Example showing how to use the new 'id' property of crawlingContext to keep track of and access different contexts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping Single URL using got-scraping\nDESCRIPTION: Example code demonstrating how to fetch and parse HTML content from a single URL using got-scraping package. The code imports required dependencies and makes a GET request to retrieve webpage content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\nconst url = 'http://example.com';\n\nconst response = await gotScraping(url);\nconsole.log(response.body);\n```\n\n----------------------------------------\n\nTITLE: Sample Dataset JSON Structure in Crawlee\nDESCRIPTION: Example of the JSON structure stored in the default dataset under {PROJECT_FOLDER}/storage/datasets/default/. It contains scraped data including URL and heading count for each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Domain Links with CheerioCrawler\nDESCRIPTION: Example showing how to crawl URLs from the same domain using the 'SameDomain' strategy. This matches URLs from the same domain including any subdomains.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/crawl_relative_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Add links from the same domain (including subdomains) to the queue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameDomain,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Reduce Method in Crawlee\nDESCRIPTION: This JavaScript snippet shows how to use the Dataset reduce method in Crawlee. It calculates the total number of headers across all scraped pages in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/map_and_reduce.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset, KeyValueStore } from 'crawlee';\n\nconst dataset = await Dataset.open();\nconst pagesHeadingCount = await dataset.reduce((memo, value) => {\n    return memo + value.headingCount;\n}, 0);\n\n// Save the result to the default key-value store\nconst kvStore = await KeyValueStore.open();\nawait kvStore.setValue('total-heading-count', pagesHeadingCount);\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: A JavaScript snippet that uses querySelectorAll to find all anchor elements with href attributes on a page, then maps the results to extract just the href values into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Browser Fingerprint Configuration\nDESCRIPTION: Example of disabling browser fingerprints in PlaywrightCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install and start a new Crawlee project using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Crawl\nDESCRIPTION: Configuration example showing how to limit the number of requests a crawler will process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Map of Crawling Contexts with Context IDs\nDESCRIPTION: Example showing how to track and access different crawling contexts using their unique IDs, enabling cross-context operations like controlling a master page from other handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels for Categorization in TypeScript\nDESCRIPTION: Demonstrates the use of Request.label shortcut for categorizing requests. This example shows how to check for request labels and enqueue new links with specific labels for easier request type management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: New Navigation Hooks Pattern in JavaScript\nDESCRIPTION: Demonstrates the new pattern using preNavigationHooks and postNavigationHooks for cleaner and more maintainable navigation customization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Pre and Post Navigation Hooks in JavaScript\nDESCRIPTION: Demonstrates the transition from using a custom gotoFunction to utilizing preNavigationHooks and postNavigationHooks in Apify's PuppeteerCrawler. This change simplifies navigation customization and improves code clarity.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Running Crawlee Code with Actor.init() and Actor.exit()\nDESCRIPTION: Alternative approach to run Crawlee crawler code as an actor on the Apify platform using the Actor.init() and Actor.exit() methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Initialize the actor\nawait Actor.init();\n\n// Initialize the crawler\nconst crawler = new CheerioCrawler({\n    // Function called for each URL\n    async requestHandler({ request, $ }) {\n        const title = $('title').text();\n        const h1 = $('h1').text();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n            h1,\n        });\n    },\n    // Let's limit our crawls to make our tests shorter and safer\n    maxRequestsPerCrawl: 20,\n});\n\n// Add first URL to the queue and start the crawl\nawait crawler.run(['https://crawlee.dev']);\n\n// Exit successfully\nawait Actor.exit();\n\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee Project to Apify Platform\nDESCRIPTION: Command to push the project to the Apify Platform, creating an Actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration for Crawlee\nDESCRIPTION: Example of configuring storage for Crawlee using @apify/storage-local.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows how to create and initialize a basic proxy configuration using Apify Proxy service. Creates a new proxy URL using the ProxyConfiguration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Configuration\nDESCRIPTION: Docker configuration for running Puppeteer with Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Requests Per Minute in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in CheerioCrawler to limit the rate of requests. It sets the limit to 120 requests per minute.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // ... other options\n    maxRequestsPerMinute: 120,\n});\n```\n\n----------------------------------------\n\nTITLE: Using BrowserPool Lifecycle Hooks in SDK v1\nDESCRIPTION: Example demonstrating how to configure browserPoolOptions with lifecycle hooks to customize browser behavior, such as conditionally enabling headful mode based on request metadata.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging in with Apify CLI\nDESCRIPTION: Commands to install Apify CLI globally and log in with an API token for authentication.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Implementation of sendRequest in Crawlee\nDESCRIPTION: Shows the internal implementation of the sendRequest function with its default configuration and options. This function leverages got-scraping with session management and proxy support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nasync sendRequest(overrideOptions?: GotOptionsInit) => {\n    return gotScraping({\n        url: request.url,\n        method: request.method,\n        body: request.payload,\n        headers: request.headers,\n        proxyUrl: crawlingContext.proxyInfo?.url,\n        sessionToken: session,\n        responseType: 'text',\n        ...overrideOptions,\n        retry: {\n            limit: 0,\n            ...overrideOptions?.retry,\n        },\n        cookieJar: {\n            getCookieString: (url: string) => session!.getCookieString(url),\n            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),\n            ...overrideOptions?.cookieJar,\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes all Playwright browsers. Suitable for development or testing with multiple browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Integrating AWS Chromium with Crawlee Configuration\nDESCRIPTION: Setting up PlaywrightCrawler with AWS-specific Chromium configuration including executable path and launch arguments\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee Projects\nDESCRIPTION: Recommended TypeScript configuration for Crawlee projects that extends the Apify TypeScript configuration with appropriate module settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for TypeScript Build\nDESCRIPTION: JSON snippet showing how to set up the build script and main entry point for a TypeScript project in package.json.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Terminating a Crawlee Crawl\nDESCRIPTION: Shows how to stop a running crawler using keyboard shortcut in the terminal.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nCTRL+C\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Information via BrowserController\nDESCRIPTION: Example demonstrating how to access browser information like proxy settings and session data through the BrowserController's launchContext property.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling a Sitemap with Cheerio Crawler in Crawlee\nDESCRIPTION: This code demonstrates how to download URLs from a sitemap and crawl them using the Cheerio Crawler in Crawlee. It uses the downloadListOfUrls utility method to extract URLs from the sitemap, then processes each page with Cheerio Crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/crawl_sitemap.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n{CheerioSource}\n```\n\n----------------------------------------\n\nTITLE: Running PlaywrightCrawler in Headful Mode\nDESCRIPTION: Example of running PlaywrightCrawler in headful mode (with visible browser window) for development and debugging purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    headless: false,\n    requestHandlerTimeoutSecs: 30,\n    async requestHandler({ page, request, enqueueLinks }) {\n        await page.waitForSelector('title');\n        const title = await page.title();\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n        await enqueueLinks();\n        await new Promise((r) => setTimeout(r, 1000));\n    },\n    maxRequestsPerCrawl: 20,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Exiting Actor in Crawlee (TypeScript)\nDESCRIPTION: This snippet demonstrates how to initialize and exit an Actor in Crawlee using the new syntax. It shows two equivalent ways of handling Actor lifecycle: using separate init and exit calls, and using the Actor.main() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Returning Scraped Data from AWS Lambda Crawlee Function\nDESCRIPTION: This snippet shows the complete AWS Lambda handler function for a Crawlee project, including returning the scraped data as the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Enabling System Information V2 in CheerioCrawler\nDESCRIPTION: Example showing how to enable the experimental System Information V2 feature in a CheerioCrawler instance. This configures Crawlee to use the improved metric collection system that is cgroup aware.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/experiments/systemInfoV2.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\n\nConfiguration.set('systemInfoV2', true);\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Sample JSON dataset structure in Crawlee\nDESCRIPTION: A sample JSON dataset containing URLs and heading counts from crawled pages. This data structure is stored in the default dataset under the project's storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Passing Configuration to CheerioCrawler\nDESCRIPTION: Updates the main.js file to use a separate Configuration instance with persistStorage option set to false when initializing the CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Legacy launchPuppeteerFunction Pattern\nDESCRIPTION: Example of the deprecated launchPuppeteerFunction that was used for customizing browser launch behavior by manually modifying options and calling Apify.launchPuppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Crawler State Management\nDESCRIPTION: Demonstrates how to utilize the crawler's useState method to maintain state across requests. The state is automatically saved when the persistState event occurs, simplifying state management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing and Compressing Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package, which provides brotli-compressed chromium binaries for AWS Lambda, and to zip the node_modules folder for uploading as a Lambda Layer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a Request Queue in Crawlee, including creating a queue, adding requests, and processing them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add multiple requests to the queue\nawait requestQueue.addRequests([\n    { url: 'https://example.com/products' },\n    { url: 'https://example.com/about' },\n]);\n\nlet request;\nwhile (request = await requestQueue.fetchNextRequest()) {\n    try {\n        // ... process the request ...\n        await requestQueue.markRequestHandled(request);\n    } catch (e) {\n        await requestQueue.reclaimRequest(request);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Crawlee Execution Log\nDESCRIPTION: Sample log output showing crawler execution and page titles being scraped\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Title with Window API in JSDOM\nDESCRIPTION: Demonstrates how to access the page title using both browser JavaScript and JSDOM syntax. This shows the syntax difference between browsers and JSDOM implementation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: Demonstrates how to enable the request locking experiment in a CheerioCrawler instance. The crawler processes URLs and extracts page titles using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init() and Actor.exit() with Crawlee\nDESCRIPTION: Example showing how to explicitly initialize and exit the Actor using the async methods. This approach requires manual initialization and cleanup with top-level await.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country Code\nDESCRIPTION: This code demonstrates how to create an Apify Proxy configuration with specific proxy groups and country selection. It configures the proxy to use only Residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Playwright in Docker\nDESCRIPTION: Example of package.json configuration that uses the asterisk wildcard to ensure the pre-installed version of Playwright is used in the Docker container.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Apify Platform\nDESCRIPTION: Command to log in to the Apify Platform using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Example Dataset JSON Structure in Crawlee\nDESCRIPTION: This JSON snippet shows the structure of a sample dataset containing scraped data from web pages. It includes the URL and the count of heading elements for each page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration\nDESCRIPTION: Example of configuring local storage with ApifyStorageLocal in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in CheerioCrawler\nDESCRIPTION: Demonstrates how to access proxy information within CheerioCrawler's request handler. This helps track which proxy is being used for each request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, $ }) {\n        // Prints information about the proxy used for the request\n        if (proxyInfo) {\n            console.log(proxyInfo.url);\n        }\n        \n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using KeyValueStore Public URLs\nDESCRIPTION: Example demonstrating how to get a public URL for items stored in a Key-Value Store on Apify Platform\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country in JavaScript\nDESCRIPTION: Illustrates how to create an Apify Proxy configuration with specific proxy groups and country selection using the Actor.createProxyConfiguration() method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Batch Request Addition with Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the new addRequests method, with support for both immediate and background processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Locking in CheerioCrawler\nDESCRIPTION: Code example showing how to enable the request locking experiment in a CheerioCrawler instance. This allows the crawler to use the request locking API for parallel processing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/experiments/request_locking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration in Crawlee\nDESCRIPTION: Shows how to create a proxy configuration using Apify Proxy and obtain a proxy URL. This is the minimal setup needed to start using Apify Proxy with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataset Storage Path (Bash)\nDESCRIPTION: This bash snippet shows the directory path where each item in the default dataset will be saved as a separate file. It uses a placeholder for the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Scraping JavaScript-rendered website using Crawlee with Playwright\nDESCRIPTION: This JavaScript code shows how to use Crawlee with Playwright to scrape a JavaScript-rendered website (Apify Store). It demonstrates Crawlee's built-in support for headless browsing and element selection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page }) {\n        const actorCard = page.locator('.ActorStoreItem-title-wrapper').first();\n        const actorText = await actorCard.textContent();\n        await crawler.pushData({\n            'actor': actorText\n        });\n    },\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Navigating Dataset Directory Structure in Crawlee\nDESCRIPTION: Shows the directory structure used by Crawlee's datasets for storing structured data. Each dataset item is stored as a separate JSON file with a zero-based index in the path.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Countries\nDESCRIPTION: Demonstrates how to create an Apify Proxy configuration with specific proxy groups and country settings. This example configures the proxy to use only residential proxies from the United States.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Platform Datasets using Actor module\nDESCRIPTION: Demonstrates how to open datasets either locally or on the Apify platform by using the Actor module and the forceCloud option. This shows the difference between accessing local storage and cloud storage when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR are configured.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Using Actor Node Base Image\nDESCRIPTION: Example of using the smallest Alpine Linux-based image for CheerioCrawler without browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript for Crawlee Projects\nDESCRIPTION: TypeScript configuration file (tsconfig.json) setup for Crawlee projects, extending from @apify/tsconfig with specific compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of Crawlee for CheerioCrawler\nDESCRIPTION: Command to manually install Crawlee for use with CheerioCrawler using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with Firefox Docker Image\nDESCRIPTION: Example of using a Docker image with pre-installed Playwright and Firefox browser.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components via Crawling Context\nDESCRIPTION: Example showing how to access crawler components like requestQueue and autoscaledPool through the crawler property in the crawling context, which replaces previous direct access to these components.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Zipping Chromium Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and create a zip file of node_modules for uploading as a Lambda Layer. This prepares the browser binaries needed for Crawlee to run in AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title Using Playwright\nDESCRIPTION: Shows how to use Playwright's locator API to extract a product title from an HTML page. The selector targets an h1 element that is a child of an element with the 'product-meta' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with Puppeteer Crawler in TypeScript\nDESCRIPTION: A code snippet showing how to implement a Puppeteer-based crawler that navigates and follows links on a website. The example requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler with Response\nDESCRIPTION: Final implementation of the Lambda handler that includes crawler execution and returns scraped data in the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Crawlee Execution Log Output\nDESCRIPTION: Example log output showing crawler execution and page titles being scraped\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Adjusting Crawlee Code for Apify Platform Deployment\nDESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform, including initializing and exiting the Apify Actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Transforming Requests with a Custom Function\nDESCRIPTION: Advanced example of using a transformRequestFunction with enqueueLinks to filter out PDF links and customize requests before they are enqueued.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.pdf`\n        if (req.url.endsWith('.pdf')) return false;\n        return req;\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Fetching JSON responses with sendRequest in TypeScript\nDESCRIPTION: Demonstrates how to modify the responseType option to handle JSON responses. By default, sendRequest is configured for plaintext/HTML, but can be adjusted for JSON APIs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Demonstrates how to use the context-aware sendRequest function with BasicCrawler to send HTTP requests. This basic example shows the default usage pattern.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple HTTP Crawler\nDESCRIPTION: Basic example showing how to create and run an HTTP crawler in TypeScript that processes URLs and logs their titles and requests made.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HttpCrawler } from 'crawlee';\n\nconst crawler = new HttpCrawler({\n    async requestHandler({ request, body, log }) {\n        const title = body.match(/<title>([^<]*)<\\/title>/)?.[1] || 'Unknown title';\n        log.info(`Title of ${request.url} is: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Session Management with PlaywrightCrawler\nDESCRIPTION: Example of implementing proxy session management with PlaywrightCrawler. This ensures consistent proxy usage for related requests in a session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ request, page, session }) {\n        // The session ensures that the same proxy is used for certain requests\n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Batch Request Adding in Crawler\nDESCRIPTION: Shows how to add multiple requests in batches using the crawler.addRequests() method with support for waiting for all requests to be added.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with CSS Selector in TypeScript\nDESCRIPTION: Demonstrates how to use document.querySelectorAll() to find all elements with a specific CSS class. This code returns all collection card elements on the page that have the 'collection-block-item' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Failed Scraping Attempt with PuppeteerCrawler (No Wait)\nDESCRIPTION: This snippet demonstrates a failed attempt to scrape JavaScript-rendered content using PuppeteerCrawler without waiting for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Extract text content of an actor card\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options in Apify SDK for JavaScript\nDESCRIPTION: Shows the transition from using launchPuppeteerOptions to the new launchContext object in Apify's PuppeteerCrawler. This change separates Apify-specific options from Puppeteer options, reducing confusion.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using Apify's Docker image with Playwright and Firefox pre-installed, suitable for PlaywrightCrawler with Firefox.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:20\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest in Basic Crawler (TypeScript)\nDESCRIPTION: Demonstrates how to use the new context.sendRequest() helper to process requests through got-scraping in a BasicCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawlee Output Data Format\nDESCRIPTION: Example of the JSON output format produced by Crawlee crawlers\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example URL Structure for Pagination\nDESCRIPTION: Demonstrates the URL pattern used for paginated results in the warehouse store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://warehouse-theme-metal.myshopify.com/collections/headphones?page=2\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawling Context in Handler Functions\nDESCRIPTION: Demonstrates how to access the new Crawling Context object in handler functions, which provides consistent access to properties across different handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript for Crawlee projects\nDESCRIPTION: TypeScript configuration file (tsconfig.json) setup for Crawlee projects, extending from @apify/tsconfig and setting appropriate compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation\nDESCRIPTION: Demonstrates a basic CheerioCrawler setup that fails to extract JavaScript-rendered content from Apify Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Adding Response to Map in SuperScraper with TypeScript\nDESCRIPTION: This function adds a response object to the responses map. It's used to store the response object associated with each request, allowing SuperScraper to send back scraped data when processing is complete.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport function addResponse(responseId: string, response: ServerResponse) {\n    responses.set(responseId, response);\n}\n```\n\n----------------------------------------\n\nTITLE: Adjusting Crawlee Code for Apify Platform Deployment\nDESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform. Includes importing Actor from Apify SDK and adding Actor.init() and Actor.exit() calls.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\nawait Actor.init();\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Using Auto-saved Crawler State in Crawlee v3\nDESCRIPTION: Demonstrates how to use the new useState() method to create and automatically save crawler state.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Method for Simplified Initialization\nDESCRIPTION: Alternative approach showing how to use the Actor.main() wrapper function which handles initialization and cleanup automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Proxy with Specific Groups and Country Codes\nDESCRIPTION: Demonstrates how to configure Apify Proxy to use specific proxy groups (e.g., RESIDENTIAL) and target specific countries for IP addresses, providing better control over proxy selection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File Path for Apify Actor\nDESCRIPTION: Demonstrates the file path structure for providing input to an Apify actor through the default key-value store. The INPUT.json file should be placed in the project's storage directory under the default key-value store location.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Browser JavaScript Example with Document Title in JSDOMCrawler\nDESCRIPTION: This example demonstrates the difference between accessing the page title in browsers versus JSDOM. In browsers, you can use document.title directly, while in JSDOM you need to access it through window.document.title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Storage Directory Structure Example\nDESCRIPTION: Shows the directory structure pattern used by key-value stores in Crawlee for storing data files locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets in JavaScript\nDESCRIPTION: Demonstrates how to open both local and remote datasets using the Actor class when both APIFY_TOKEN and CRAWLEE_STORAGE_DIR environment variables are set.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Logging in with Apify CLI\nDESCRIPTION: Commands to install the Apify CLI tool and authenticate with your API token to access Apify platform features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Configuration\nDESCRIPTION: Dockerfile configuration for Node.js with Puppeteer and Chrome browser support\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Cheerio Crawler\nDESCRIPTION: Implementation of a Cheerio-based crawler that traverses and collects all links from a website. It uses enqueueLinks() to add new URLs to the RequestQueue and has a maximum request limit. The code demonstrates handling of requests, data collection, and link enqueueing.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_all_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // Let's limit our crawling to only 10 requests\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ request, enqueueLinks, log }) {\n        log.info(`Processing ${request.url}`);\n\n        // Save the HTML elements containing the text 'Example Domain'\n        await Dataset.pushData({\n            url: request.url,\n            title: await $('title').text(),\n        });\n\n        // Extract all links from the page and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js as a development dependency, enabling type-checking for Node.js APIs.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Installing TypeScript as a development dependency in the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Manually\nDESCRIPTION: Commands to manually install Crawlee and its dependencies for different crawler types using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: NPM commands for installing Crawlee and its optional browser automation dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Updated Access to AutoscaledPool in SDK v1\nDESCRIPTION: Example showing that direct access to autoscaledPool is removed in SDK v1 and must now be accessed through the crawler property instead.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext) => {\n    crawlingContext.autoscaledPool // does NOT exist anymore\n    crawlingContext.crawler.autoscaledPool // <= this is correct usage\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Request Queue Operations in Crawlee\nDESCRIPTION: Demonstrates basic operations with a Request Queue in Crawlee, including opening a queue, adding requests, and processing them.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Open a named request queue\nconst requestQueue = await RequestQueue.open('my-queue');\n\n// Add requests to the queue\nawait requestQueue.addRequest({ url: 'https://example.com/foo' });\nawait requestQueue.addRequest({ url: 'https://example.com/bar' });\n\n// Get request from queue and process it\nlet request;\nwhile (request = await requestQueue.fetchNextRequest()) {\n    // Process request...\n    console.log(request.url);\n    await requestQueue.markRequestHandled(request);\n}\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Crawlee\nDESCRIPTION: Shows how to perform basic operations with Crawlee's dataset including writing single and multiple rows of data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/result_storage.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Dataset } from 'crawlee';\n\n// Write a single row to the default dataset\nawait Dataset.pushData({ col1: 123, col2: 'val2' });\n\n// Open a named dataset\nconst dataset = await Dataset.open('some-name');\n\n// Write a single row\nawait dataset.pushData({ foo: 'bar' });\n\n// Write multiple rows\nawait dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);\n```\n\n----------------------------------------\n\nTITLE: Basic Apify Proxy Configuration\nDESCRIPTION: Shows how to initialize and get a proxy URL from Apify Proxy service using the Actor.createProxyConfiguration() method. This is the basic setup for using Apify Proxy in your scraping projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Combining RequestQueueV2 with CheerioCrawler\nDESCRIPTION: Shows how to use a custom RequestQueueV2 instance with CheerioCrawler while enabling the request locking experiment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/experiments/request_locking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    experiments: {\n        requestLocking: true,\n    },\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using BrowserController for Browser Management\nDESCRIPTION: Demonstrates the correct usage of BrowserController for managing browser instances and performing browser-specific operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    await page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    await browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    await page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    await browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Detail Route Handler for Worker Process Communication\nDESCRIPTION: Updated detail route handler that sends scraped data back to the parent process instead of pushing directly to storage, enabling proper data collection in a parallel processing environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Modified DETAIL route handler using process.send instead of context.pushData\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Custom Storage Settings\nDESCRIPTION: JavaScript code showing how to initialize a PlaywrightCrawler with custom Configuration to prevent storage interference between Lambda instances.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components via Crawling Context\nDESCRIPTION: Demonstrates how to access crawler components like requestQueue and autoscaledPool through the crawler property of the crawling context, replacing the previous direct access.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation in TypeScript\nDESCRIPTION: A simple TypeScript implementation of CheerioCrawler that downloads a single page, extracts its title, and prints it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PlaywrightCrawler\nDESCRIPTION: This code snippet shows how to completely disable browser fingerprinting in PlaywrightCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // Here we disable the automatic generation of fingerprints\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // Other crawler options...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Playwright WebKit Image\nDESCRIPTION: Using the Docker image optimized for Playwright with WebKit browser pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:20\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright in JavaScript\nDESCRIPTION: This code demonstrates how to extract a product SKU from a webpage using Playwright's locator API. It targets a span element with the specific class 'product-meta__sku-number'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Viewing Dataset Storage Path in Bash\nDESCRIPTION: Shows the file path where each dataset item will be saved when using Crawlee's dataset functionality. Dataset items are stored in individual files within the project's storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: DevTools Element Selection Query\nDESCRIPTION: JavaScript query selector command for testing element selection in browser DevTools.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler without Waiting (JavaScript)\nDESCRIPTION: Example demonstrating the need for waiting in PuppeteerCrawler. This code will fail because it doesn't wait for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/javascript-rendering.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be in the DOM yet\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Fetching JSON Response with BasicCrawler in TypeScript\nDESCRIPTION: Shows how to set the responseType option to 'json' when using sendRequest with BasicCrawler. This is useful when expecting a JSON response from the server.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox for Web Scraping in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up PlaywrightCrawler using Firefox browser for web scraping. It includes configuration for the crawler, browser launch options, and a basic page processing function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\nconst crawler = new PlaywrightCrawler({\n    // Use Firefox browser\n    browserType: firefox,\n    launchOptions: {\n        headless: true,\n    },\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Automatically enqueue links from the page\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. This is similar to previous versions but requires explicit installation of Puppeteer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.13.1\nDESCRIPTION: Changelog entry detailing bug fixes and features including robots.txt handling and HTTP status code treatment\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.1](https://github.com/apify/crawlee/compare/v3.13.0...v3.13.1) (2025-04-07)\n\n### Bug Fixes\n\n* don't double increment session usage count in `BrowserCrawler` ([#2908](https://github.com/apify/crawlee/issues/2908)) ([3107e55](https://github.com/apify/crawlee/commit/3107e5511142a3579adc2348fcb6a9dcadd5c0b9)), closes [#2851](https://github.com/apify/crawlee/issues/2851)\n* rename `RobotsFile` to `RobotsTxtFile` ([#2913](https://github.com/apify/crawlee/issues/2913)) ([3160f71](https://github.com/apify/crawlee/commit/3160f717e865326476d78089d778cbc7d35aa58d)), closes [#2910](https://github.com/apify/crawlee/issues/2910)\n* treat `406` as other `4xx` status codes in `HttpCrawler` ([#2907](https://github.com/apify/crawlee/issues/2907)) ([b0e6f6d](https://github.com/apify/crawlee/commit/b0e6f6d3fc4455de467baf666e0f67f8738cc57f)), closes [#2892](https://github.com/apify/crawlee/issues/2892)\n\n### Features\n\n* add `respectRobotsTxtFile` crawler option ([#2910](https://github.com/apify/crawlee/issues/2910)) ([0eabed1](https://github.com/apify/crawlee/commit/0eabed1f13070d902c2c67b340621830a7f64464))\n```\n\n----------------------------------------\n\nTITLE: Crawling Context in SDK v1\nDESCRIPTION: Example demonstrating the new Crawling Context object in SDK v1, which provides a single shared context object across all handler functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Accepting User Input in Crawlee Actor\nDESCRIPTION: This code demonstrates how to access user input in a Crawlee actor. It imports the Actor class from '@apify/crawlee', retrieves the input data, and logs it to show how to use input values within an actor.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'crawlee';\n\nawait Actor.init();\n\n// Get input\nconst input = await Actor.getInput();\nconsole.log('Input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Enabling Headful Browser Mode in TypeScript\nDESCRIPTION: Code snippet showing how to enable headful (visible browser window) mode in Crawlee by uncommenting the headless option in the crawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Location in File System\nDESCRIPTION: Shows the directory path where each dataset item will be saved as a separate file when using the default dataset in a Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Cleaning Storage in Crawlee\nDESCRIPTION: Demonstrates how to purge default storage directories using the purgeDefaultStorages helper function, which cleans up storage while preserving input data.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Shows how to deploy an actor project to the Apify platform using the Apify CLI. This uploads and builds your code on the platform, making it ready to run as a cloud service.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging in with Apify CLI\nDESCRIPTION: Commands to install the Apify CLI globally and log in to your Apify account using your API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Building Apify Crawlee Docker Image with Node.js and Playwright\nDESCRIPTION: This Dockerfile creates a Docker image for an Apify Crawlee project. It uses a multi-stage build process to optimize the final image size and includes Node.js, Playwright, and Chrome. The build process installs dependencies, builds the project, and sets up the runtime environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY --chown=myuser . ./\n\nRUN npm run build\n\nFROM apify/actor-node-playwright-chrome:16\n\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Using KeyValueStore Public URLs\nDESCRIPTION: Example of getting a public URL for a stored item in Apify Platform's Key-Value Store\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Command to install ts-node as a development dependency to run TypeScript code directly without compilation during development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Crawlee Storage Output Example\nDESCRIPTION: Example of JSON output format stored by Crawlee in the storage directory\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Terminating Crawlee Process\nDESCRIPTION: Command to stop the running crawler process\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nCTRL+C\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in SDK v1\nDESCRIPTION: Example showing how to access crawler properties like requestQueue and autoscaledPool from within the handlePageFunction in SDK v1, where they've been moved under the crawler object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Setting minConcurrency and maxConcurrency in CheerioCrawler\nDESCRIPTION: This code shows how to configure the minimum and maximum concurrency for a CheerioCrawler, setting the range of parallel requests from 5 to 10.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    minConcurrency: 5,\n    maxConcurrency: 10,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to check if a product is in stock by looking for a specific element on the page using Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Using Main Method (TypeScript)\nDESCRIPTION: Alternative approach using Actor.main() wrapper function which handles initialization and cleanup automatically. Functionally equivalent to the basic example.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler with Utils Snapshot\nDESCRIPTION: Shows how to use the context-aware saveSnapshot() utility with PuppeteerCrawler for automatic screenshot capture and storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from '@crawlee/puppeteer';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks, log }) {\n        // Capture and save the screenshot\n        await utils.puppeteer.saveSnapshot(page);\n\n        // Optionally, enqueue more links\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links from a Page using Cheerio\nDESCRIPTION: This snippet shows how to use Cheerio to find all <a> elements with an href attribute on a page and extract their href values into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Locating Stored Dataset Files in Crawlee Project\nDESCRIPTION: This bash snippet shows the directory structure where individual dataset items are stored in a Crawlee project. Each item is saved as a separate file in the default dataset directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Playwright Crawler\nDESCRIPTION: Example of creating a Playwright crawler that uses the Playwright library to interact with a browser, navigate to URLs and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments Before and After v1\nDESCRIPTION: Demonstrates the change from separate argument objects to a unified Crawling Context in handler functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer\nDESCRIPTION: Shows implementation of sitemap crawling using Puppeteer Crawler. Includes configuration for running on Apify Platform with specific Docker image requirements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerSitemapCrawler } from '@crawlee/puppeteer';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new PuppeteerSitemapCrawler({\n    // ...\n});\n\nconst sitemapUrls = await downloadListOfUrls({ url: 'https://example.com/sitemap.xml' });\n\nawait crawler.run(sitemapUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Playwright Chrome Image\nDESCRIPTION: Using the Docker image optimized for Playwright with Chrome browser, compatible with CheerioCrawler and PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Using Node with Playwright Firefox\nDESCRIPTION: Example of using the Docker image with Node.js and Playwright Firefox pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:20\n```\n\n----------------------------------------\n\nTITLE: Configuring ParselCrawler with CurlImpersonateHttpClient\nDESCRIPTION: Python code for configuring a ParselCrawler with CurlImpersonateHttpClient to bypass Cloudflare protection by impersonating Safari browser. Sets concurrency settings to prevent blocking.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# main.py\n\nfrom crawlee import ConcurrencySettings, HttpHeaders\nfrom crawlee.crawlers import ParselCrawler\nfrom crawlee.http_clients import CurlImpersonateHttpClient\n\nfrom .routes import router\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n    concurrency_settings = ConcurrencySettings(max_concurrency=1, max_tasks_per_minute=50)\n\n    http_client = CurlImpersonateHttpClient(\n        impersonate='safari17_0',\n        headers=HttpHeaders(\n            {\n                'accept-language': 'en',\n                'accept-encoding': 'gzip, deflate, br, zstd',\n            }\n        ),\n    )\n    crawler = ParselCrawler(\n        request_handler=router,\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=30,\n    )\n\n    await crawler.run(['https://www.crunchbase.com/www-sitemaps/sitemap-index.xml'])\n\n    await crawler.export_data_json('crunchbase_data.json')\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler Implementation\nDESCRIPTION: Shows a streamlined approach to creating a CheerioCrawler using the crawler.run() method with direct URL input, eliminating the need for explicit RequestQueue initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Cross-Context Access Using Context IDs\nDESCRIPTION: Example demonstrating how to access one crawling context from another using the new context ID system, which is useful for coordinating between different page handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Screenshot Capture with PuppeteerCrawler\nDESCRIPTION: Demonstrates how to capture screenshots of multiple pages using PuppeteerCrawler with page.screenshot(). Screenshots are saved with URL-based keys.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from '@crawlee/puppeteer';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request, enqueueLinks }) {\n        // Create a key from the URL\n        const key = request.url.replace(/[:/]/g, '_');\n\n        // Capture the screenshot\n        const screenshot = await page.screenshot();\n\n        // Save the screenshot to the default key-value store\n        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });\n\n        // Optionally, enqueue more links\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Context Information via BrowserController\nDESCRIPTION: Shows how to access important browser information through the BrowserController, such as proxy details and session data, which was difficult to obtain in previous SDK versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Input File in Bash for Crawlee Projects\nDESCRIPTION: This snippet shows the path where to create an INPUT.json file to provide input to a Crawlee actor. The file should be placed in the default key-value store of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Comparison of Handler Arguments Before v1\nDESCRIPTION: Example showing how handler arguments worked before SDK v1, where each handler function received its own separate arguments object, making it difficult to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Router Enhancement - Inline Definition\nDESCRIPTION: Feature update allowing inline router definition in the HTTP crawler\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/http-crawler/CHANGELOG.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n**router:** allow inline router definition\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Logging User Input in Crawlee Actor\nDESCRIPTION: This code snippet demonstrates how to retrieve user input from the Apify platform and log it. It uses the Actor.getInput() method to fetch the input and console.log() to display it.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of your actor\nconst input = await Actor.getInput();\nconsole.log('My input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Creating an INPUT.json File for Actor Input\nDESCRIPTION: This bash example shows the file path where you should create the INPUT.json file to provide input to your actor in a local development environment. The file should be placed in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP Cloud Functions\nDESCRIPTION: Sets the main entry point for the GCP Cloud Function in the package.json file, pointing to src/main.js.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, Puppeteer is no longer bundled with the SDK and must be installed separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset from Crawlee in TypeScript\nDESCRIPTION: Code snippet showing how to import the Dataset module along with PlaywrightCrawler from Crawlee to enable data saving functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Crawlee Actor Implementation using init() and exit()\nDESCRIPTION: Alternative implementation of a Cheerio crawler using explicit Actor.init() and Actor.exit() calls instead of the Actor.main() wrapper. This approach gives more control over the actor lifecycle.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// Initialize the actor\nawait Actor.init();\n\ntry {\n    const startUrls = [\n        'https://crawlee.dev',\n    ];\n\n    const crawler = new CheerioCrawler({\n        async requestHandler({ request, $, enqueueLinks, log }) {\n            const title = $('title').text();\n            log.info(`Title of ${request.url} is '${title}'`);\n\n            // Save results to dataset\n            await Dataset.pushData({\n                url: request.url,\n                title,\n            });\n\n            // Enqueue all links from the page\n            await enqueueLinks();\n        },\n    });\n\n    await crawler.run(startUrls);\n} catch (error) {\n    console.log(error);\n} finally {\n    // Exit the actor\n    await Actor.exit();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Dependencies\nDESCRIPTION: Commands for installing the Apify SDK as a project dependency and Apify CLI as a global tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee with Disabled Storage Persistence for GCP Cloud Run\nDESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a Configuration object that disables storage persistence, which is necessary for serverless environments like GCP Cloud Run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/gcp-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Full Playwright Docker Image\nDESCRIPTION: Example of using the comprehensive Playwright image with all browsers (Chromium, Chrome, Firefox, WebKit) pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Browser Fingerprint Configuration\nDESCRIPTION: TypeScript code example showing how to disable browser fingerprints in PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Crawlee Crawling Example Output\nDESCRIPTION: Sample output from running a Crawlee crawler, showing the progress of the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title(https://crawlee.dev): Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\nINFO  PlaywrightCrawler: Title(https://crawlee.dev/docs/introduction): Introduction | Crawlee\nINFO  PlaywrightCrawler: Title(https://crawlee.dev/docs/quick-start): Quick Start | Crawlee\nINFO  PlaywrightCrawler: Title(https://crawlee.dev/docs/examples): Examples | Crawlee\nINFO  PlaywrightCrawler: Title(https://crawlee.dev/api/core): @crawlee/core | API Reference | Crawlee\nINFO  PlaywrightCrawler: Crawl finished, processed 5 pages\n```\n\n----------------------------------------\n\nTITLE: Using BrowserPool Lifecycle Hooks\nDESCRIPTION: Example showing how to configure BrowserPool lifecycle hooks via browserPoolOptions, including pre-launch customization based on request data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        retireBrowserAfterPageCount: 10,\n        preLaunchHooks: [\n            async (pageId, launchContext) => {\n                const { request } = crawler.crawlingContexts.get(pageId);\n                if (request.userData.useHeadful === true) {\n                    launchContext.launchOptions.headless = false;\n                }\n            }\n        ]\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using preLaunchHooks in browserPoolOptions (New Pattern)\nDESCRIPTION: Implementation of the new browserPoolOptions.preLaunchHooks pattern that replaces launchPuppeteerFunction with lifecycle hooks for better control over browser launching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Integrating ProxyConfiguration with JSDOMCrawler\nDESCRIPTION: Illustrates integration of proxy configuration with JSDOMCrawler for handling HTTP requests through proxy servers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, window }) {\n        // Process data using JSDOM API\n        // ...\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP Cloud Functions in Crawlee Project\nDESCRIPTION: Sets the main entry point in package.json to src/main.js, which is required for GCP Cloud Functions to correctly identify the entry point of the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing BrowserPool Methods with PuppeteerPool\nDESCRIPTION: Examples comparing how to perform common operations like recycling pages and retiring browsers between the old PuppeteerPool and the new BrowserPool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.recyclePage(page);\n\n// NEW\nawait page.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.retire(page.browser());\n\n// NEW\nbrowserPool.retireBrowserByPage(page);\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum and Maximum Concurrency in Crawlee\nDESCRIPTION: This example shows how to configure the minConcurrency and maxConcurrency options in a CheerioCrawler to control the range of parallel requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/scaling_crawlers.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    minConcurrency: 5,\n    maxConcurrency: 20,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, SDK v1 doesn't bundle Puppeteer or Playwright, so users need to install their preferred browser automation library separately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks method with glob patterns to filter URLs in a PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows how to initialize and create a basic proxy configuration using Apify Proxy service. Requires being logged into Apify account for local usage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Using Playwright Firefox Docker Image\nDESCRIPTION: Example of using the Docker image with pre-installed Playwright and Firefox browser. Similar to the Chrome variant but with Firefox pre-installed instead.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:20\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using Apify's Docker image with Playwright and Chrome pre-installed, suitable for PlaywrightCrawler with Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Handling CAPTCHA Detection in Web Scraping with TypeScript\nDESCRIPTION: Implementation of CAPTCHA detection when scraping Amazon pages. This function checks for the presence of a CAPTCHA form and throws an error if one is detected, allowing Crawlee to automatically retry the request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioAPI } from 'cheerio';\n\nconst CAPTCHA_SELECTOR = '[action=\"/errors/validateCaptcha\"]';\n\n/**\n * Handles the captcha blocking. Throws an error if a captcha is displayed.\n * - Crawlee automatically retries any requests that throw an error.\n * - Status code blocking (e.g. Amazon's `503`) is handled automatically by Crawlee.\n */\nexport const handleCaptchaBlocking = ($: CheerioAPI) => {\n    const isCaptchaDisplayed = $(CAPTCHA_SELECTOR).length > 0;\n    if (isCaptchaDisplayed) throw new Error('Captcha is displayed! Retrying...');\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Deployment\nDESCRIPTION: Sets the main entry point in package.json to point to src/main.js, which is required for proper deployment to GCP Cloud Functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Accepting User Input in Crawlee Typescript Example\nDESCRIPTION: This code demonstrates how to accept and log user input in a Crawlee project. It imports the Actor interface from Apify, uses getInput() to retrieve user input and logs the input to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Get input\nconst input = await Actor.getInput() as Record<string, unknown>;\n\n// Print the input\nconsole.log('Input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Header Generator Options Configuration\nDESCRIPTION: Demonstrates how to configure header generator options for browser fingerprinting in requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Firefox in Crawlee\nDESCRIPTION: This code snippet demonstrates how to set up and use PlaywrightCrawler with a headless Firefox browser in Crawlee. It includes configuration for the crawler, browser launch options, and a basic crawling function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/playwright_crawler_firefox.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\nimport { firefox } from 'playwright';\n\nconst crawler = new PlaywrightCrawler({\n    launchContext: {\n        launchOptions: {\n            headless: true,\n            // Use firefox browser\n            browserName: 'firefox' as const,\n        },\n    },\n    async requestHandler({ page, parseWithCheerio, request, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            title,\n            url: request.url,\n        });\n    },\n    maxRequestsPerCrawl: 20, // Limit to 20 requests per crawler run\n});\n\nawait crawler.run(['https://crawlee.dev']);\n\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in Crawlers\nDESCRIPTION: Example showing how to access the new BrowserPool instance from both the crawler instance and within the handlePageFunction context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Playwright\nDESCRIPTION: Commands to install Crawlee with Playwright support, showing both full and specific package installation options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Using ProxyInfo with ProxyConfiguration\nDESCRIPTION: Demonstrates how to use ProxyConfiguration to get proxy URLs and utilize ProxyInfo in a custom crawler setup. This example shows accessing proxy details programmatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/motivation.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ProxyConfiguration, ProxyInfo } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* options */ });\n\nconst proxyUrl = await proxyConfiguration.newUrl();\n// http://bob:password123@proxy.apify.com:8000\n\nconst proxyInfo: ProxyInfo = await proxyConfiguration.newProxyInfo();\n// {\n//   url: 'http://bob:password123@proxy.apify.com:8000',\n//   hostname: 'proxy.apify.com',\n//   port: 8000,\n//   username: 'bob',\n//   password: 'password123',\n//   groups: ['GROUP1', 'GROUP2']\n// }\n```\n\n----------------------------------------\n\nTITLE: Setting up INPUT.json Path for Apify Actor\nDESCRIPTION: Demonstrates the file path structure for providing input to an Apify actor through the INPUT.json file in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Hello World Actor Locally\nDESCRIPTION: Bash commands to create a new actor project and run it locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments Before v1\nDESCRIPTION: Example showing how handler arguments were separate objects in previous SDK versions, making value tracking difficult across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee packages in TypeScript\nDESCRIPTION: Examples of installing Crawlee packages using npm, including the full meta-package and individual crawler packages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Adapting Crawlee Project for Apify Platform Deployment\nDESCRIPTION: Code modifications needed to integrate a Crawlee crawler with the Apify Platform. This includes adding Actor.init() at the start and Actor.exit() at the end to handle platform storage connections and proper shutdown.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// highlight-next-line\nawait Actor.init();\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n// highlight-next-line\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Example for Pagination\nDESCRIPTION: Demonstrates the URL structure for paginated results in the example warehouse store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://warehouse-theme-metal.myshopify.com/collections/headphones?page=2\n```\n\n----------------------------------------\n\nTITLE: Complete Lambda Handler with Data Return\nDESCRIPTION: Final implementation of the AWS Lambda handler function that runs the crawler and returns the scraped data with proper HTTP status code when the crawler finishes execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/aws-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    }\n    // highlight-end\n};\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Configuration\nDESCRIPTION: Dockerfile configuration for Node.js with Playwright and Chrome browser support\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Crawler with State Management\nDESCRIPTION: Shows how to use the useState() method for maintaining crawler state that is automatically saved.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Manually for CheerioCrawler\nDESCRIPTION: Install Crawlee package for projects using CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Deploying Project to Apify Platform\nDESCRIPTION: Command to deploy the project to Apify Platform, creating a new Actor\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Crawlee Output Example\nDESCRIPTION: Example of stored crawling results in JSON format\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Playwright Crawler\nDESCRIPTION: Demonstrates how to use the enqueueLinks helper with glob patterns to filter URLs for crawling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Docker Images\nDESCRIPTION: Example package.json configuration showing the recommended approach of using asterisk (*) for automation library versions to prevent reinstallation of pre-installed libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Usage with Dataset in Crawlee\nDESCRIPTION: Demonstrates using the reduce method on a Dataset to aggregate data by summing all heading counts across pages. The accumulated result is stored in the key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/map_and_reduce.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{ReduceSource}\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components through Context\nDESCRIPTION: Example demonstrating how to access crawler components like requestQueue and autoscaledPool through the crawler property in the crawling context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in Crawler Instances\nDESCRIPTION: Example demonstrating how to access the BrowserPool instance from both inside and outside the handlePageFunction in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Requests Per Minute in Crawlee\nDESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in a CheerioCrawler to limit the rate of requests to 120 per minute.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Browser Lifecycle Hooks in Crawlee\nDESCRIPTION: Shows how to combine multiple preLaunchHooks to create reusable browser configurations, demonstrating how hooks can be composed for better code organization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright Docker Image\nDESCRIPTION: Demonstrates how to use the Apify Docker image that includes all Playwright browsers, suitable for development and testing with multiple browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Old Launch Function Pattern in JavaScript\nDESCRIPTION: Shows the deprecated launchPuppeteerFunction pattern for customizing browser launch behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for TypeScript Build in Crawlee Project\nDESCRIPTION: Set up the package.json file with a build script using TypeScript compiler and specify the main entry point for the built code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Playwright with Cheerio Store Scraper\nDESCRIPTION: Alternative scraping setup using PlaywrightCrawler with Cheerio for HTML parsing\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ $ }) {\n        $('.collection-block-item').each((_, el) => {\n            console.log($(el).text());\n        });\n    }\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee with Playwright Chrome\nDESCRIPTION: This Dockerfile sets up a container for running Crawlee actors based on Node.js 16 with Playwright and Chrome. It optimizes the build process by leveraging Docker layer caching, installing only production dependencies, and configuring the container to run the actor on startup.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for Crawlee with Browser Support\nDESCRIPTION: Full implementation of an AWS Lambda handler function for running a Crawlee crawler with browser support. It sets up the crawler with proper configuration, executes it, and returns the crawled data in the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Services Using Service Locator Explicitly\nDESCRIPTION: This snippet demonstrates how to explicitly set up Crawlee's core services (Configuration, MemoryStorageClient, and LocalEventManager) using the service_locator. This approach manually registers each service with the ServiceLocator.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee import service_locator\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.events import LocalEventManager\nfrom crawlee.storage_clients import MemoryStorageClient\n\n\nasync def main() -> None:\n    service_locator.set_configuration(Configuration())\n    service_locator.set_storage_client(MemoryStorageClient())\n    service_locator.set_event_manager(LocalEventManager())\n\n    crawler = ParselCrawler()\n\n    # ...\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Extra Dependencies\nDESCRIPTION: Command to install required npm packages playwright-extra and puppeteer-extra-plugin-stealth for Playwright implementation\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool from Crawler Instance\nDESCRIPTION: Example showing how to access the BrowserPool instance from both inside and outside the crawler's handlePageFunction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Using Proxy with sendRequest in BasicCrawler\nDESCRIPTION: Example showing how to include a proxy URL when making HTTP requests with sendRequest. This helps to hide the real IP address during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a New Crawlee Project with pipx\nDESCRIPTION: Command to initialize a new Crawlee for Python project using pipx. This creates the necessary boilerplate code for a LinkedIn job scraper using PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx run crawlee create linkedin-scraper\n```\n\n----------------------------------------\n\nTITLE: Attempting to Scrape JavaScript-Rendered Content with CheerioCrawler\nDESCRIPTION: This snippet demonstrates an attempt to scrape content from Apify Store using CheerioCrawler, which fails because the content is rendered client-side with JavaScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/javascript-rendering.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        // Extract text content of an actor card\n        const actorText = $('.ActorStoreItem').text();\n        console.log(`ACTOR: ${actorText}`);\n    }\n})\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with PlaywrightCrawler\nDESCRIPTION: Command for manually installing Crawlee with Playwright, which needs to be installed explicitly as it's not bundled with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Accessing Browser Context Information\nDESCRIPTION: Example showing how to access proxy and session information from the browser controller's launch context in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ browserController }) => {\n    // Information about the proxy used by the browser\n    browserController.launchContext.proxyInfo\n\n    // Session used by the browser\n    browserController.launchContext.session\n}\n```\n\n----------------------------------------\n\nTITLE: Creating INPUT.json File for Actor Input\nDESCRIPTION: This bash snippet shows the file path where an INPUT.json file should be created to provide input to the actor. The file should be placed in the default key-value store of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Running an Actor Locally with Apify CLI\nDESCRIPTION: Command to run your Crawlee-based actor locally using Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify run\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Components in SDK v1\nDESCRIPTION: Demonstrates how to access components like requestQueue and autoscaledPool through the crawler property in the crawling context, replacing previous direct access methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Using KeyValueStore Public URLs\nDESCRIPTION: Example of storing data and getting a public URL for a stored item in KeyValueStore\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: npm commands for installing Crawlee and its optional browser automation dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with Firefox browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Using Scoped Logging in RequestHandler\nDESCRIPTION: Shows how to use the scoped log instance provided in the crawling context. This logger prefixes messages with the crawler name for better log organization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Command to install TypeScript compiler as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor Boilerplate with Apify CLI\nDESCRIPTION: Commands to create a new actor boilerplate project and run it locally using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\n----------------------------------------\n\nTITLE: Path to Saved Dataset Files\nDESCRIPTION: The bash path showing where Crawlee saves the dataset files by default in the project directory structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/07-saving-data.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns\nDESCRIPTION: Example of using glob patterns with enqueueLinks to filter URLs that match specific patterns, in this case targeting links on apify.com.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/03-adding-urls.mdx#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in SDK v1 Crawlers\nDESCRIPTION: Example showing how to access the new BrowserPool from both the crawler instance and within the handlePageFunction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Docker and Package.json Best Practice for Browser Automation\nDESCRIPTION: Best practice example showing how to properly configure both the Dockerfile and package.json to use pre-installed browser automation libraries, preventing version conflicts.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Puppeteer\nDESCRIPTION: Implementation of sitemap crawling using Puppeteer Crawler, which provides full browser automation capabilities. Requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_sitemap.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring maxRequestsPerMinute in CheerioCrawler\nDESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in a CheerioCrawler to limit the number of requests per minute to 120.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120,\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Request Queue with Locking Support\nDESCRIPTION: Creates and initializes a request queue that supports locking for parallel scraping. Includes functionality to optionally clear the queue on initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { getOrInitQueue } from './requestQueue.mjs';\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Functions\nDESCRIPTION: Package.json configuration showing the main entry point setting required for GCP Cloud Functions deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in JavaScript\nDESCRIPTION: Code snippet demonstrating how to save scraped data to Crawlee's default dataset using the pushData() method. This replaces a console.log statement with data persistence.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Adding AWS Lambda Handler Function\nDESCRIPTION: Wrapping the crawler logic in an AWS Lambda handler function for execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nexport const handler = async (event, context) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n};\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee, which can be useful for cleaning up before starting a new crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Multi-stage Dockerfile for Crawlee Actor with Node.js\nDESCRIPTION: This Dockerfile uses a multi-stage build approach to create an optimized Docker image for a Crawlee-based web scraping project. The first stage builds the TypeScript project, while the second creates a production image with minimal dependencies for better performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)\nDESCRIPTION: This code demonstrates how to use Dataset.pushData() to save scraped data. It replaces a console.log() call with the Dataset.pushData() function, which stores the results in Crawlee's default dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Optimizing Adding Large Amount of Requests in JavaScript\nDESCRIPTION: Performance improvement to optimize adding a large amount of requests via crawler.addRequests().\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\ncrawler.addRequests()\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-puppeteer-chrome Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Puppeteer (Chromium) and the Chrome browser. This image can be used with CheerioCrawler and PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Using Custom CookieJar with BasicCrawler in TypeScript\nDESCRIPTION: Shows how to use a custom CookieJar from the tough-cookie package to manage cookies for requests. This allows for fine-grained control over cookie handling during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Output Log\nDESCRIPTION: Example of the log output when running Crawlee, showing how it handles requests and extracts data from the Crawlee website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_9\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/guides/apify-platform is 'Running on the Apify Platform | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/introduction is 'Introduction | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/guides/request-storage is 'Request Storage | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/guides/result-storage is 'Result Storage | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/guides/configuration is 'Configuration | Crawlee'\nINFO  CheerioCrawler: Crawl finished\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development environment using ts-node\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of PlaywrightCrawler Without Waiting\nDESCRIPTION: This snippet shows an incorrect usage of PlaywrightCrawler where the code doesn't wait for elements to render. This approach will likely fail to extract the desired content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ page, request }) {\n        // This will fail because the element might not be rendered yet\n        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Updated Product Detail Extraction with Numeric Parsing in TypeScript\nDESCRIPTION: This snippet shows an improved version of the product detail extraction function, now using the numeric parsing utilities to convert string values to numbers for price and review fields.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioAPI } from 'cheerio';\nimport { parseNumberFromSelector } from './utils.js';\n\ntype ProductDetails = {\n    title: string;\n    price: number;        //\n    listPrice: number;    // updated to numbers\n    reviewRating: number; //\n    reviewCount: number;  //\n};\n\n...\n\n/**\n * Scrapes the product details from the given Cheerio object.\n */\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\n    const title = $(SELECTORS.TITLE).text().trim();\n\n    const price = parseNumberFromSelector($, SELECTORS.PRICE);\n    const listPrice = parseNumberFromSelector($, SELECTORS.LIST_PRICE);\n    const reviewRating = parseNumberFromSelector($, SELECTORS.REVIEW_RATING);\n    const reviewCount = parseNumberFromSelector($, SELECTORS.REVIEW_COUNT);\n\n    return { title, price, listPrice, reviewRating, reviewCount };\n};\n```\n\n----------------------------------------\n\nTITLE: Fetching JSON Response with sendRequest in TypeScript\nDESCRIPTION: This snippet demonstrates how to use sendRequest to fetch a JSON response. It sets the responseType option to 'json' to automatically parse the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee using CLI\nDESCRIPTION: Commands to install and run Crawlee using the Crawlee CLI, which sets up a new project with all necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Crawlee Module\nDESCRIPTION: Command for installing individual Crawlee modules, specifically the Cheerio module.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Setting API Token with Configuration Instance\nDESCRIPTION: Example of setting up the Apify SDK with an API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler for GCP Cloud Functions\nDESCRIPTION: Updates the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for running in a serverless environment like GCP Cloud Functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using Node with All Playwright Browsers\nDESCRIPTION: Example of using the comprehensive Playwright image that includes all supported browsers (Chromium, Chrome, Firefox, WebKit).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:20\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Basic HTTP Server with Node.js for Crawlee\nDESCRIPTION: Creates a simple HTTP server using Node.js built-in 'http' module. This server listens on port 3000 and returns a placeholder response that will later be replaced with scraped page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/running-in-web-server/running-in-web-server.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createServer } from 'http';\nimport { log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    // We will return the page title here later instead\n    res.end('Hello World\\n');\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment for Crawlee Actor\nDESCRIPTION: This Dockerfile sets up an environment for running Crawlee actors based on the apify/actor-node:16 image. It uses a staged approach to copy package files first, install dependencies efficiently while omitting development packages, and then copies the application code. The container is configured to run the actor on startup using npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Accessing BrowserPool in SDK v1 Crawlers\nDESCRIPTION: Demonstrates how to access the new BrowserPool instance in PuppeteerCrawler and PlaywrightCrawler, which replaced the previous PuppeteerPool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PlaywrightCrawler({\n    handlePageFunction: async ({ page, crawler }) => {\n        crawler.browserPool // <-----\n    }\n});\n\ncrawler.browserPool // <-----\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for TypeScript Project\nDESCRIPTION: Dockerfile for building and running a TypeScript project with multi-stage build process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Replacing gotoFunction with Pre and Post Navigation Hooks in JavaScript\nDESCRIPTION: Demonstrates the transition from using a custom gotoFunction to using preNavigationHooks and postNavigationHooks in Apify's PuppeteerCrawler. This change simplifies navigation customization and improves code clarity.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = await gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData in JavaScript\nDESCRIPTION: Uses Dataset.pushData() to save the extracted results to the default Dataset storage. This creates a new row in the dataset for each call.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Accessing Dataset Storage Directory\nDESCRIPTION: Shows the default storage location for dataset items in a Crawlee project. Each dataset item is saved as a separate file in this directory structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from an Element with Cheerio\nDESCRIPTION: A simple Cheerio example that finds the first h2 element on a page and returns its text content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Custom Selector in TypeScript\nDESCRIPTION: This snippet shows how to use the enqueueLinks function with a custom selector to find and enqueue specific links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title with Playwright\nDESCRIPTION: Shows how to extract the product title using Playwright's locator to find an h1 element within product-meta class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Using preNavigationHooks with JSDOMCrawler\nDESCRIPTION: Example showing how to use preNavigationHooks to modify gotOptions before navigation occurs in JSDOMCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/jsdom-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\npreNavigationHooks: [\n    (crawlingContext, gotOptions) => {\n        // ...\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Using actor-node Docker Image\nDESCRIPTION: Example of using the smallest Apify Docker image based on Alpine Linux. This image is suitable for CheerioCrawler but does not include browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright\nDESCRIPTION: Implementation of sitemap crawling using Playwright Crawler, offering modern browser automation features. Requires the apify/actor-node-playwright-chrome image when running on the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{PlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title\nDESCRIPTION: Shows how to extract the product title using Playwright's locator to find the h1 element within product-meta class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title with Playwright in JavaScript\nDESCRIPTION: This code snippet shows how to use Playwright to scrape the product title from a webpage using a CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee\nDESCRIPTION: Pushes extracted results to the default Dataset storage instead of logging them to console, which saves the data as JSON files in the local storage directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Setting Apify API Token through Configuration Object\nDESCRIPTION: Example showing how to set your API token directly in code using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Failed Crawler Error Output\nDESCRIPTION: Shows the error message when attempting to access elements without proper waiting in headless browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: log\nCODE:\n```\nERROR [...] Error: failed to find element matching selector \".ActorStoreItem\"\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Docker Image\nDESCRIPTION: Shows the recommended package.json configuration when using Apify Docker images, using an asterisk for the automation library version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Proxy Configuration\nDESCRIPTION: Shows basic setup of Apify Proxy configuration for web scraping. Creates a proxy configuration instance and generates a new proxy URL.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Purging Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee, which cleans up the request storage directory and request list in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/request_storage.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-stage Build Configuration\nDESCRIPTION: Dockerfile implementing a multi-stage build process for TypeScript-based Crawlee projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_18\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData\nDESCRIPTION: Demonstrates how to save scraped results to the default Dataset storage using the pushData method.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Adding ow Dependency to Crawlee Utils\nDESCRIPTION: Adds a missing dependency on the 'ow' package to the Crawlee utils module. This resolves issues related to missing argument validation functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\nadd missing dependency on `ow`\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Configuration\nDESCRIPTION: Dockerfile configuration for Node.js with Playwright and WebKit browser support\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Using Context IDs for Cross-Context Access\nDESCRIPTION: Example showing how to use context IDs to maintain references between different crawling contexts, enabling cross-context access to pages and requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet masterContextId;\nconst handlePageFunction = async ({ id, page, request, crawler }) => {\n    if (request.userData.masterPage) {\n        masterContextId = id;\n        // Prepare the master page.\n    } else {\n        const masterContext = crawler.crawlingContexts.get(masterContextId);\n        const masterPage = masterContext.page;\n        const masterRequest = masterContext.request;\n        // Now we can manipulate the master data from another handlePageFunction.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom Cookie Jar with SendRequest\nDESCRIPTION: Shows how to use a custom cookie jar with the sendRequest function to manage cookies during requests. The example uses the tough-cookie package to create a cookie jar.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using Proxy Sessions with JSDOMCrawler in TypeScript\nDESCRIPTION: This snippet demonstrates how to use proxy sessions with JSDOMCrawler to maintain consistent IP addresses across requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/proxy_management.mdx#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({ /* ... */ });\n\nconst crawler = new JSDOMCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    async requestHandler({ session }) {\n        const proxyUrl = await proxyConfiguration.newUrl(session.id);\n        console.log(proxyUrl);\n        // ...\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with Cheerio\nDESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's chained method syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/cheerio_crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]')\n    .map((i, el) => $(el).attr('href'))\n    .get();\n```\n\n----------------------------------------\n\nTITLE: Streamlit Web Application for LinkedIn Job Scraper\nDESCRIPTION: Creates a Streamlit web interface with a form for users to input job title, location, and output filename. On submission, it runs the LinkedIn scraper using subprocess and displays the results.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nimport subprocess\n\n# Streamlit form for inputs\nst.title(\"LinkedIn Job Scraper\")\n\nwith st.form(\"scraper_form\"):\n    title = st.text_input(\"Job Title\", value=\"backend developer\")\n    location = st.text_input(\"Job Location\", value=\"newyork\")\n    data_name = st.text_input(\"Output File Name\", value=\"backend_jobs\")\n\n    submit_button = st.form_submit_button(\"Run Scraper\")\n\nif submit_button:\n\n    # Run the scraping script with the form inputs\n    command = f\"\"\"poetry run python -m linkedin-scraper --title \"{title}\"  --location \"{location}\" --data_name \"{data_name}\" \"\"\"\n\n    with st.spinner(\"Crawling in progress...\"):\n         # Execute the command and display the results\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n        st.write(\"Script Output:\")\n        st.text(result.stdout)\n\n        if result.returncode == 0:\n            st.success(f\"Data successfully saved in {data_name}.csv\")\n        else:\n            st.error(f\"Error: {result.stderr}\")\n```\n\n----------------------------------------\n\nTITLE: Crawling Website Links with Puppeteer Crawler\nDESCRIPTION: Implementation using PuppeteerCrawler for browser-based link crawling. Handles JavaScript-rendered content while discovering and processing links. Requires apify/actor-node-puppeteer-chrome Docker image for platform execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawl_all_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerSource}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON output from Crawlee\nDESCRIPTION: Example of JSON output stored by Crawlee after crawling a website.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Crawlee Dataset Components\nDESCRIPTION: Imports the necessary components from Crawlee including PlaywrightCrawler and Dataset for data storage functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer Extra and Stealth Plugin packages\nDESCRIPTION: Commands to install the necessary npm packages for using Puppeteer with the stealth plugin. This installs puppeteer-extra and puppeteer-extra-plugin-stealth which help in avoiding bot detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for GCP Cloud Functions\nDESCRIPTION: Update the package.json file to specify the main entry point for the GCP Cloud Function. This change ensures GCP knows which file to execute when the function is triggered.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting All Links from a Page using JSDOMCrawler\nDESCRIPTION: This snippet demonstrates how to use JSDOMCrawler to find all <a> elements with an href attribute on a page and extract their URLs into an array.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Running a Local Actor with Apify CLI\nDESCRIPTION: Commands to navigate to your actor project directory and run it locally using the Apify CLI, which simulates the Apify platform environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Using Crawling Context in SDK v1\nDESCRIPTION: Shows the new Crawling Context approach in SDK v1, where a single context object is shared across all handler functions, making it easier to maintain state.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (crawlingContext1) => {\n    crawlingContext1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (crawlingContext2) => {\n    crawlingContext2.hasOwnProperty('proxyInfo') // true\n}\n\n// All contexts are the same object.\ncrawlingContext1 === crawlingContext2 // true\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Stage Docker Build for Crawlee Actor with Playwright and Chrome\nDESCRIPTION: A complete Dockerfile for building and running a Crawlee actor with Playwright and Chrome browser. It uses a multi-stage build approach to keep the final image size smaller by separating the build process from the runtime environment. The first stage builds the source code, while the second stage only includes production dependencies and built files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Handling Page Function with Previous API\nDESCRIPTION: Example showing how handler arguments worked in previous SDK versions, where each function received separate argument objects making it difficult to track values across function invocations.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Importing Crawlee Dataset Components\nDESCRIPTION: Shows how to import the necessary Crawlee components for data storage, including PlaywrightCrawler and Dataset classes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content with Cheerio\nDESCRIPTION: Simple example showing how to extract text content from an h2 element using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee project to Apify Platform\nDESCRIPTION: Command to deploy the prepared Crawlee project to the Apify Platform. Creates an archive, uploads it to Apify, and initiates the Docker build process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects with recommended settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Full Playwright Docker Configuration\nDESCRIPTION: Docker configuration for running all Playwright browsers (Chromium, Chrome, Firefox, WebKit). Complete but larger image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: LiveView Snapshot Method Removal\nDESCRIPTION: Example showing the removal of LiveView functionality from BrowserPool, which was previously available in PuppeteerPool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Docker and Package.json Best Practice Configuration\nDESCRIPTION: Recommended setup for Docker image and package.json to ensure compatibility between pre-installed browser versions and automation libraries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Using Direct Init/Exit in TypeScript\nDESCRIPTION: Demonstrates the explicit initialization and exit of an Actor using the new Actor.init() and Actor.exit() methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Same-Domain Strategy in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the enqueueLinks function with a strategy to include subdomains in the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer with Chrome Docker Image\nDESCRIPTION: Docker configuration for using Puppeteer with Chrome browser, supporting both CheerioCrawler and PuppeteerCrawler with headless and headful modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling from Apify SDK to Crawlee\nDESCRIPTION: Shows how to update event handling code when migrating from Apify SDK to Crawlee, replacing Apify.events.on with Actor.on.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_11\n\nLANGUAGE: diff\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium for AWS Lambda Environment in Crawlee\nDESCRIPTION: JavaScript code showing how to supply the Chromium executable path from the @sparticuz/chromium package and pass necessary arguments to disable GPU acceleration for the AWS Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript definitions for Node.js\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.13.0\nDESCRIPTION: Added a new feature to handle Cloudflare challenges in Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/playwright-crawler/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# [3.13.0](https://github.com/apify/crawlee/compare/v3.12.2...v3.13.0) (2025-03-04)\n\n\n### Features\n\n* **playwright:** add `handleCloudflareChallenge` helper ([#2865](https://github.com/apify/crawlee/issues/2865)) ([9a1725f](https://github.com/apify/crawlee/commit/9a1725f7b87fb70194fc31858500cb35639fb964))\n```\n\n----------------------------------------\n\nTITLE: Using Request Labels for Categorization in Request Handler\nDESCRIPTION: Demonstrates how to use the Request.label shortcut for categorizing requests. This feature allows easy labeling of requests and subsequent filtering based on these labels during the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nasync requestHandler({ request, enqueueLinks }) {\n    if (request.label !== 'DETAIL') {\n        await enqueueLinks({\n            globs: ['...'],\n            label: 'DETAIL',\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using custom cookie jar with sendRequest in TypeScript\nDESCRIPTION: Shows how to use a custom cookie jar with sendRequest using the tough-cookie package. This allows for more control over cookie management when making HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/got_scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\nimport { CookieJar } from 'tough-cookie';\n\nconst cookieJar = new CookieJar();\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({ cookieJar });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee Projects\nDESCRIPTION: Recommended TypeScript configuration for Crawlee projects, extending the @apify/tsconfig preset with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Implementation with Explicit Element Waiting\nDESCRIPTION: An implementation using PuppeteerCrawler that successfully scrapes JavaScript-rendered content by explicitly waiting for elements to appear on the page before extracting data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ page, request }) {\n        // Wait for the actor cards to render\n        // With Puppeteer, we need to explicitly wait for the element\n        await page.waitForSelector('.ActorStoreItem');\n        const element = await page.$('.ActorStoreItem');\n        const actorText = await page.evaluate(el => el.textContent, element);\n        console.log(`ACTOR: ${actorText}`);\n    }\n});\n\nawait crawler.run(['https://apify.com/store']);\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestQueue in TypeScript for Crawlee v2.3\nDESCRIPTION: This code snippet demonstrates how to create a RequestQueue instance in TypeScript using the older Crawlee v2.3 API. It shows both the named and default RequestQueue initialization methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/motivation.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n// Opens a request queue. If it doesn't exist, it will be created.\nconst queue = await RequestQueue.open('my-queue');\n\n// You can also use the default queue.\nconst defaultQueue = await RequestQueue.open();\n```\n\n----------------------------------------\n\nTITLE: Creating a new Crawlee project with CLI extras using uvx\nDESCRIPTION: Command to create a new Crawlee project from a template using uvx with CLI extras installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-06/index.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nuvx 'crawlee[cli]' create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP Cloud Function Handler for CheerioCrawler\nDESCRIPTION: Wrap the crawler execution in a handler function that accepts request and response objects from GCP. The function runs the crawler and returns the collected data via the response object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/gcp-cheerio.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (req, res) => {\n    const crawler = new CheerioCrawler({\n        requestHandler: router,\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n    \n    // highlight-next-line\n    return res.send(await crawler.getData())\n// highlight-next-line\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Headful Mode in Playwright Crawler\nDESCRIPTION: Code snippet showing how to configure a Playwright crawler to run in headful mode (with visible browser window) by uncommenting the headless option, which is useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Dependencies\nDESCRIPTION: Command to install playwright-extra and puppeteer stealth plugin using npm package manager\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Creating a New Actor with Apify CLI\nDESCRIPTION: Command to create a new actor project using the Apify CLI tool with the Hello World template.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration to CheerioCrawler\nDESCRIPTION: Updates the CheerioCrawler initialization to include a separate Configuration instance with persistStorage disabled, which is necessary for cloud function environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript Compiler Options\nDESCRIPTION: Sets up the TypeScript configuration file with compiler options for ES2022 modules and specifies input/output directories.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Title with Browser JavaScript and JSDOM\nDESCRIPTION: This snippet demonstrates how to retrieve the page title using browser JavaScript and JSDOM. It shows the difference in syntax between browser and JSDOM environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawlee Project to Apify Platform\nDESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright. This enables support for multiple browsers including Firefox and Webkit (Safari).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure in Crawlee\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing dataset items on disk.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Modifying Crawlee code for Apify Platform integration\nDESCRIPTION: Code changes required to make a Crawlee project work with the Apify Platform. Adds Actor.init() and Actor.exit() to integrate with Apify's cloud storage and handle graceful shutdowns.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// highlight-next-line\nimport { Actor } from 'apify';\nimport { PlaywrightCrawler, log } from 'crawlee';\nimport { router } from './routes.mjs';\n\n// highlight-next-line\nawait Actor.init();\n\n// This is better set with CRAWLEE_LOG_LEVEL env var\n// or a configuration option. This is just for show \nlog.setLevel(log.LEVELS.DEBUG);\n\nlog.debug('Setting up crawler.');\nconst crawler = new PlaywrightCrawler({\n    // Instead of the long requestHandler with\n    // if clauses we provide a router instance.\n    requestHandler: router,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n\n// highlight-next-line\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Example Dataset JSON Structure in Crawlee\nDESCRIPTION: A JSON example of scraped data stored in the default dataset, containing URLs and their corresponding header element counts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Logging in Crawlee v3\nDESCRIPTION: Illustrates the use of the scoped log instance provided in the crawling context for prefixed logging messages.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of Crawlee for PlaywrightCrawler\nDESCRIPTION: Command to manually install Crawlee and Playwright for use with PlaywrightCrawler using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Adapting Category Route Handler for Parallel Processing\nDESCRIPTION: Modifies the category route handler to enqueue product URLs to a lockable queue instead of processing them immediately.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait getOrInitQueue(true);\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in JavaScript\nDESCRIPTION: Example demonstrating how to access crawler properties like requestQueue and autoscaledPool in SDK v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/upgrading/upgrading_v1.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Shows how to completely disable browser fingerprint generation in PuppeteerCrawler by setting useFingerprints to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nnew PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform via CLI\nDESCRIPTION: Command to log in to the Apify Platform using the Apify CLI. Requires a personal access token from the Apify console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/09-deployment.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with Chrome Docker Image\nDESCRIPTION: Docker configuration for using Playwright with Chrome browser, supporting CheerioCrawler and PlaywrightCrawler with headless and headful modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Using actor-node Docker Image\nDESCRIPTION: Demonstrates how to use the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Creating a New Crawlee Project\nDESCRIPTION: Uses npx to download and run the Crawlee CLI to create a new crawler project with templates.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee\nDESCRIPTION: Shows how to import both PlaywrightCrawler and Dataset modules from Crawlee for data scraping and storage purposes.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Direct TypeScript Execution\nDESCRIPTION: Command to install ts-node as a development dependency, allowing direct execution of TypeScript files during development without compilation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image with Specific Library Version\nDESCRIPTION: Example of using a beta pre-release version of Docker image with Node.js 20 and Playwright 1.10.0.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:20-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Implementing SessionPool with BasicCrawler\nDESCRIPTION: Example showing how to configure and use SessionPool with BasicCrawler to manage proxy rotations and session state.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/session_management.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{BasicSource}\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for uploading as a Lambda Layer. This is necessary because Lambda requires browser binaries to be included in the deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Setting Up Proxy with Browser Pool\nDESCRIPTION: Example showing how to configure a proxy with authentication credentials for a Puppeteer plugin.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst puppeteerPlugin = new PuppeteerPlugin(puppeteer, {\n    proxyUrl: 'http://<username>:<password>@proxy.com:8000'\n});\n```\n\n----------------------------------------\n\nTITLE: URL Regular Expression Adjustment (Markdown)\nDESCRIPTION: Reference to a bug fix that adjusts a regular expression to allow single character hostnames, with GitHub issue and commit links.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n* adjust `URL_NO_COMMAS_REGEX` regexp to allow single character hostnames ([#2492](https://github.com/apify/crawlee/issues/2492)) ([ec802e8](https://github.com/apify/crawlee/commit/ec802e85f54022616e5bdcc1a6fd1bd43e1b3ace)), closes [#2487](https://github.com/apify/crawlee/issues/2487)\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using CLI\nDESCRIPTION: Commands to install Apify CLI and log in with an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/apify_platform.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\napify login -t YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Lifecycle Hooks in JavaScript\nDESCRIPTION: This example illustrates how to replace the old launchPuppeteerFunction with the new preLaunchHook in the browserPoolOptions. It allows for more flexible and consistent browser configuration across Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Finding All Links on a Page with JSDOMCrawler\nDESCRIPTION: A snippet that uses the Window API to find all anchor elements with href attributes on a page and extract their URLs into an array using querySelectorAll and map methods.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nArray.from(document.querySelectorAll('a[href]')).map((a) => a.href);\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Default Storages in Crawlee\nDESCRIPTION: Demonstrates how to explicitly purge default storage directories in Crawlee, including the request queue and request list. This is useful for cleaning up storage before starting a new crawler run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/request_storage.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemap with Playwright\nDESCRIPTION: Illustrates sitemap crawling implementation using Playwright Crawler. Includes specific Docker image requirements for Apify Platform deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightSitemapCrawler } from '@crawlee/playwright';\nimport { downloadListOfUrls } from '@crawlee/utils';\n\nconst crawler = new PlaywrightSitemapCrawler({\n    // ...\n});\n\nconst sitemapUrls = await downloadListOfUrls({ url: 'https://example.com/sitemap.xml' });\n\nawait crawler.run(sitemapUrls);\n```\n\n----------------------------------------\n\nTITLE: Crawler Output Example\nDESCRIPTION: Example output from running the CheerioCrawler showing the extracted page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: log\nCODE:\n```\nThe title of \"https://crawlee.dev\" is: Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee.\n```\n\n----------------------------------------\n\nTITLE: Checking Node.js Version\nDESCRIPTION: Command to verify the installed Node.js version, which should be 16.0 or higher for Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with Non-Persistent Storage\nDESCRIPTION: Updates the CheerioCrawler initialization to use a separate Configuration instance with persistStorage set to false, which is required for serverless environments where file persistence isn't reliable.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/gcp-cheerio.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Saving Data to Default Dataset Directory Path\nDESCRIPTION: Shows the directory path structure where dataset items are stored as individual files within the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Installing PuppeteerCrawler Manually\nDESCRIPTION: Command to install Crawlee with Puppeteer, a library for controlling Chromium or Chrome browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Package Name Declaration in Markdown\nDESCRIPTION: Package name declaration for the Crawlee browser module that provides browser-based web crawling functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# `@crawlee/browser`\n```\n\n----------------------------------------\n\nTITLE: Installing playwright-extra and stealth plugin\nDESCRIPTION: Command for installing playwright-extra and puppeteer-extra-plugin-stealth packages using npm package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project\nDESCRIPTION: Command to initialize the project for Apify, creating necessary configuration files.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Command to install ts-node as a development dependency, which allows running TypeScript code directly without compiling it first.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: HTML Link Example\nDESCRIPTION: Example of an HTML anchor tag with href attribute that the crawler would process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://crawlee.dev/js/docs/introduction\">This is a link to Crawlee introduction</a>\n```\n\n----------------------------------------\n\nTITLE: Deploying to Apify Platform\nDESCRIPTION: Command to deploy the project to the Apify Platform\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result in Crawlee\nDESCRIPTION: The expected result of the reduce operation, showing the total count of all headers from all pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies Configuration\nDESCRIPTION: Example package.json configuration showing proper versioning for Crawlee and Playwright dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Puppeteer Chrome Docker Image\nDESCRIPTION: Example of using the Docker image with pre-installed Puppeteer and Chrome browser. This image supports CheerioCrawler and PuppeteerCrawler with both headless and headful modes.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset Content with JSX Component\nDESCRIPTION: React/JSX component implementation for displaying a runnable code block with dataset export functionality. Uses raw-loader and roa-loader for processing the TypeScript source file.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/export_entire_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';\nimport ApiLink from '@site/src/components/ApiLink';\nimport CrawlSource from '!!raw-loader!roa-loader!./export_entire_dataset.ts';\n\n<RunnableCodeBlock className=\"language-js\" type=\"cheerio\">\n\t{CrawlSource}\n</RunnableCodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize the Apify project configuration, creating necessary files and folders for deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/09-deployment.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Retrieving Page Title with Browser vs JSDOM\nDESCRIPTION: Demonstrates the difference between accessing page title in browser JavaScript versus JSDOM environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Commands for installing Crawlee and its dependencies using npm, showing different installation options for various use cases.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Using Playwright with Firefox Docker Image\nDESCRIPTION: Docker configuration for using Playwright with Firefox browser, supporting CheerioCrawler and PlaywrightCrawler with Firefox-specific capabilities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:20\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory and Starting the Crawler\nDESCRIPTION: Commands to navigate to the newly created project directory and start the example crawler that was created by the CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Path Structure in Crawlee\nDESCRIPTION: Shows the file path pattern used for storing dataset items in Crawlee's storage system.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/result_storage.mdx#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Launch Options with Plugins\nDESCRIPTION: Example showing how to set browser launch options directly in the plugin configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst playwrightPlugin = new PlaywrightPlugin(playwright.chromium, {\n    launchOptions: {\n        headless: true,\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-webkit Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with WebKit pre-installed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Domain Strategy Configuration\nDESCRIPTION: Configuration for including subdomains in the crawl using the same-domain strategy.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration\nDESCRIPTION: Example of configuring local storage for Crawlee using @apify/storage-local.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Storage Directory Structure for Key-Value Store\nDESCRIPTION: Shows the directory structure pattern used by Crawlee for storing key-value data on disk.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/result_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}\n```\n\n----------------------------------------\n\nTITLE: Installing and Zipping Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for uploading as a Lambda Layer.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Using Node with Playwright Chrome\nDESCRIPTION: Example of using the Docker image with Node.js and Playwright Chrome pre-installed, suitable for CheerioCrawler and PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20\n```\n\n----------------------------------------\n\nTITLE: Migrating from gotoFunction to Navigation Hooks in JavaScript\nDESCRIPTION: Shows the transition from using a gotoFunction to pre and post navigation hooks for handling page navigation in Crawlee. The new approach simplifies the implementation and makes the code more maintainable.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    // pre-processing\n    await makePageStealthy(page);\n\n    // Have to remember how to do this:\n    const response = gotoExtended(page, request, {/* have to remember the defaults */});\n\n    // post-processing\n    await page.evaluate(() => {\n        window.foo = 'bar';\n    });\n\n    // Must not forget!\n    return response;\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    gotoFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preNavigationHooks = [\n    async ({ page }) => makePageStealthy(page)\n];\n\nconst postNavigationHooks = [\n    async ({ page }) => page.evaluate(() => {\n        window.foo = 'bar'\n    })\n]\n\nconst crawler = new Apify.PuppeteerCrawler({\n    preNavigationHooks,\n    postNavigationHooks,\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Standalone SessionPool Usage in Crawlee\nDESCRIPTION: This snippet demonstrates how to use SessionPool standalone in Crawlee without a crawler. It shows manual session management, including creating, using, and retiring sessions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/session_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nimport { SessionPool } from 'crawlee';\n\nconst sessionPool = new SessionPool({\n    maxPoolSize: 100,\n});\n\n// Create a new session\nconst session = await sessionPool.getSession();\n\n// Use the session in your HTTP requests\nconst response = await fetch('https://example.com', {\n    headers: {\n        'User-Agent': session.userData.userAgent,\n        'Cookie': session.getCookieString('https://example.com'),\n    },\n    // Use the proxy if it's set\n    agent: session.proxyUrl ? new HttpsProxyAgent(session.proxyUrl) : undefined,\n});\n\n// Mark the session as used\nsession.markGood();\n\n// Rotate the session when you're done with it\nsession.retire();\n```\n\n----------------------------------------\n\nTITLE: Finding Links Without enqueueLinks in TypeScript\nDESCRIPTION: Alternative implementation that shows how to find and queue links manually without using the enqueueLinks helper. This provides more insight into the underlying process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 10,\n    async requestHandler({ $, request, crawler }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n        \n        // Create a RequestQueue instance (an internal one from the crawler - also used by enqueueLinks)\n        const requestQueue = await crawler.getRequestQueue();\n\n        // Find all links\n        const links = $('a[href]')\n            .map((_, el) => $(el).attr('href'))\n            .get();\n\n        // Add all links to the queue\n        for (const url of links) {\n            await requestQueue.addRequest({ url });\n        }\n    }\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from Crawlee\nDESCRIPTION: Sample JSON output stored by Crawlee after scraping, showing the structure of the extracted data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with Poetry\nDESCRIPTION: Command to install all required dependencies for the project using Poetry package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build setup with build script and main entry point.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for TypeScript Build\nDESCRIPTION: Basic package.json configuration with a build script that invokes the TypeScript compiler and specifies the main entry point for the built code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Configuration\nDESCRIPTION: Docker configuration for running Puppeteer with Chrome browser support. Includes XVFB for both headless and headful browser operation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: Example showing how to customize browser fingerprints in PlaywrightCrawler by specifying browser version, operating system, and other parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            // Only use Chrome version 104\n            browsers: [\n                { name: 'chrome', minVersion: 104 },\n            ],\n            // Only generate fingerprints for MacOS\n            operatingSystems: ['macos'],\n            // Use only certain locales\n            locales: ['en-US', 'en-GB'],\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium for AWS Lambda Environment\nDESCRIPTION: This snippet shows how to supply the Chromium path from @sparticuz/chromium and set up launch options for the AWS Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Command to install ts-node for development runtime.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Using enqueueLinks with 'all' Strategy\nDESCRIPTION: Configuring enqueueLinks to follow all links regardless of domain, allowing the crawler to explore the entire internet.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/introduction/03-adding-urls.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'all', // wander the internet\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK Dependency\nDESCRIPTION: Command to install the Apify SDK package which enables integration with Apify Platform storage and services\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Crawlee Crawler Execution Log\nDESCRIPTION: Log output showing the CheerioCrawler crawling the Crawlee documentation site, visiting the main page and documentation sections while extracting page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Getting Text Content with Cheerio\nDESCRIPTION: Shows how to extract text content from an h2 element using Cheerio selectors.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/cheerio_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n$('h2').text()\n```\n\n----------------------------------------\n\nTITLE: Full Playwright Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with all browser engines (Chrome, Firefox, WebKit).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Script for Compiled JavaScript\nDESCRIPTION: Package.json configuration for the production script that runs the compiled JavaScript code from the dist directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for running TypeScript code in development mode using ts-node.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Storage Cleanup in Crawlee\nDESCRIPTION: Shows how to purge default storages in Crawlee using the purgeDefaultStorages helper function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { purgeDefaultStorages } from 'crawlee';\n\nawait purgeDefaultStorages();\n```\n\n----------------------------------------\n\nTITLE: Running Actor on Apify Platform with Apify CLI\nDESCRIPTION: Command to deploy and run your actor code on the Apify platform using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: HTTP SessionPool Example with HttpCrawler\nDESCRIPTION: Example demonstrating SessionPool integration with HttpCrawler for managing HTTP requests and proxy sessions\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{HttpSource}\n```\n\n----------------------------------------\n\nTITLE: Using Node.js 20 Docker Image for Crawlee\nDESCRIPTION: Basic Dockerfile configuration that specifies using Node.js 20 for a Crawlee project without browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 20\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Declarations for Crawlee TypeScript Project\nDESCRIPTION: Install type declarations for Node.js to enable type-checking for Node.js features in a Crawlee TypeScript project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Basic File Storage Path Structure\nDESCRIPTION: Shows the directory structure where request queue data is stored locally\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/request_storage.mdx#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Headful Browser Mode\nDESCRIPTION: TypeScript configuration option to enable visible browser window during crawling\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Setting Apify API Token in Configuration\nDESCRIPTION: JavaScript code to set the Apify API token using the Configuration instance.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/deployment/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Actor with Apify CLI\nDESCRIPTION: CLI commands to create a boilerplate actor project and run it locally.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Command to install TypeScript compiler as a development dependency\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Crawler with Separate Storage\nDESCRIPTION: JavaScript code to initialize a PlaywrightCrawler with a new Configuration instance, ensuring separate storage for each crawler instance in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n}, new Configuration({\n    persistStorage: false,\n}));\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Using SessionPool with HttpCrawler in Crawlee\nDESCRIPTION: This code demonstrates how to implement session management with HttpCrawler. It shows how to configure proxy rotation and session management for HTTP requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/session_management.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\n{HttpSource}\n```\n\n----------------------------------------\n\nTITLE: Event Handling Migration from Apify to Actor\nDESCRIPTION: A diff showing how to migrate from the old Apify.events event handling pattern to the new Actor.on pattern in Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/upgrading/upgrading_v3.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings for top-level await support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CheerioCrawler with In-Memory Storage for AWS Lambda\nDESCRIPTION: Code snippet showing how to initialize a CheerioCrawler with a unique Configuration instance using in-memory storage, which is necessary for AWS Lambda's read-only filesystem.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee manually for PuppeteerCrawler\nDESCRIPTION: Command to manually install Crawlee and Puppeteer for use with PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI with NPM\nDESCRIPTION: Command to install the Apify CLI globally using NPM (Node Package Manager).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nnpm -g install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets using Actor in Crawlee\nDESCRIPTION: Demonstrates how to open datasets stored locally and on the Apify platform. Shows the use of the forceCloud option to specify where data should be stored when both local and platform storage are configured.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Optimizing RequestList Memory Footprint in JavaScript\nDESCRIPTION: Performance improvement to optimize RequestList memory footprint.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nRequestList\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Configuration\nDESCRIPTION: Docker configuration for running Puppeteer with Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Specifying Dataset Storage Path in Crawlee\nDESCRIPTION: Shows the default storage directory path structure where dataset items are saved. Each item in the dataset will be stored as a separate file in this location.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/examples/add_data_to_dataset.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Initializing Cheerio Crawler with Unique Configuration Instance\nDESCRIPTION: Creating a CheerioCrawler with a unique Configuration instance that uses in-memory storage to prevent statefulness in Lambda environments. This addresses the read-only filesystem limitation of AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/aws-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json Build Script and Main Entry Point\nDESCRIPTION: JSON configuration for package.json that defines the build script using TypeScript compiler and specifies the main entry point for the compiled code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with Glob Patterns in Crawlee\nDESCRIPTION: Using glob patterns to filter URLs in Crawlee's enqueueLinks function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Crawlee\nDESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects with TypeScript.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Importing API Link Component in JSX\nDESCRIPTION: React/JSX import statement for an API documentation link component\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Packages\nDESCRIPTION: Shows various npm installation commands for Crawlee and its dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes all Playwright browsers: Chromium, Chrome, Firefox, WebKit. This is a large image suitable for development or testing with multiple browsers.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:16\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production runtime.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install and run Crawlee using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TypeScript Configuration\nDESCRIPTION: Command to install @apify/tsconfig as a development dependency, providing recommended TypeScript settings for Crawlee projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Creating a CheerioCrawler with RequestQueue in Crawlee\nDESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract and log the page title. It demonstrates the basic structure of a Crawlee crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/02-first-crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Add import of CheerioCrawler\nimport { RequestQueue, CheerioCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\n// Create the crawler and add the queue with our URL\n// and a request handler to process the page.\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    // The `$` argument is the Cheerio object\n    // which contains parsed HTML of the website.\n    async requestHandler({ $, request }) {\n        // Extract <title> text with Cheerio.\n        // See Cheerio documentation for API docs.\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler and wait for it to finish\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Extracting URL and Manufacturer in JavaScript\nDESCRIPTION: Demonstrates how to extract the product URL and manufacturer name from the request URL by splitting the string at appropriate points.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Crawler State Management\nDESCRIPTION: Shows how to use the new useState method for managing and auto-saving crawler state.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ crawler }) {\n        const state = await crawler.useState({ foo: [] as number[] });\n        // just change the value, no need to care about saving it\n        state.foo.push(123);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for development execution\nDESCRIPTION: Command to install ts-node as a development dependency, allowing direct execution of TypeScript code during development without prior compilation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder to prepare for uploading as a Lambda Layer. This creates a dependencies.zip file containing the browser binaries that will be used in AWS Lambda.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Playwright Crawler Link Enqueuing\nDESCRIPTION: TypeScript implementation using Playwright Crawler to crawl all links on a website. Demonstrates automated link discovery and crawling with Playwright-specific page handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_all_links.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // The crawler will automatically process all enqueued links\n    maxRequestsPerCrawl: 100,\n    async requestHandler({ enqueueLinks, log, page }) {\n        log.info('Processing...');\n        // Add all links from page to RequestQueue\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Creating a New Crawlee Project with CLI\nDESCRIPTION: Command to create a new Crawlee project using the Crawlee CLI via npx. This initializes a new crawler project with templates.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI with Homebrew\nDESCRIPTION: Command to install the Apify CLI using Homebrew package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nbrew install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Accessing Public URL for a Key-Value Store Item in Apify Platform\nDESCRIPTION: Code demonstrating how to create and get a public URL for an item stored in an Apify Key-Value Store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');\n// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Strategy Implementation\nDESCRIPTION: Implementation of a CheerioCrawler that only follows links from the same hostname. This is the default strategy that matches relative URLs and URLs pointing to the same hostname.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        // Only enqueue links that share the same hostname\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n        });\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Puppeteer\nDESCRIPTION: Command to install Apify SDK v1 with Puppeteer support. Unlike previous versions, SDK v1 doesn't bundle browser automation libraries, so you need to explicitly install them.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify puppeteer\n```\n\n----------------------------------------\n\nTITLE: Playwright Chrome Docker Configuration\nDESCRIPTION: Docker configuration specific to Playwright with Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Running an Actor Locally with Apify CLI\nDESCRIPTION: Command to run an actor project locally using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Adding Development Script with ts-node\nDESCRIPTION: Package.json script configuration for running TypeScript code directly during development using ts-node-esm with transpile-only mode for faster execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler as Development Dependency\nDESCRIPTION: Command to install the TypeScript compiler as a development dependency for your Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration for TypeScript Project\nDESCRIPTION: Full package.json configuration including dependencies, scripts, and module type for a TypeScript project using Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer Extra Dependencies\nDESCRIPTION: Commands for installing the required packages puppeteer-extra and puppeteer-extra-plugin-stealth via npm\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Extra and Stealth Plugin packages\nDESCRIPTION: Commands to install the necessary npm packages for using Playwright with the stealth plugin. This installs playwright-extra and puppeteer-extra-plugin-stealth which help in avoiding bot detection.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform via CLI\nDESCRIPTION: Command to authenticate with the Apify Platform using the Apify CLI, which requires a personal access token from the account integrations page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript as a Development Dependency\nDESCRIPTION: Command to install the TypeScript compiler as a development dependency for the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Proxy Inspection in JSDOMCrawler\nDESCRIPTION: Shows how to access proxy information in JSDOMCrawler's requestHandler using proxyInfo object\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/proxy_management.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nInspectionJSDOMSource\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 along with Playwright for multi-browser automation support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Basic enqueueLinks Usage in Crawlee\nDESCRIPTION: Demonstrates the basic usage of enqueueLinks() function in Crawlee for crawling links.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Installing @apify/tsconfig for Crawlee TypeScript Project\nDESCRIPTION: Install the @apify/tsconfig package as a development dependency to use Apify's recommended TypeScript configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)\nDESCRIPTION: This code demonstrates how to save extracted data using the Dataset.pushData() function. It replaces a console.log statement with the actual data saving operation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/07-saving-data.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait Dataset.pushData(results);\n```\n\n----------------------------------------\n\nTITLE: Crawling URLs and Extracting Data with JSDOMCrawler in TypeScript\nDESCRIPTION: This code snippet shows how to use JSDOMCrawler to crawl a list of URLs from an external file, parse HTML using jsdom, and extract page titles and h1 tags. It demonstrates URL enqueuing and data storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/jsdom_crawler.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSDOMCrawler, Dataset } from 'crawlee';\nimport { readFile } from 'node:fs/promises';\n\nconst crawler = new JSDOMCrawler({\n    async requestHandler({ window, enqueueLinks, log }) {\n        const { document } = window;\n        log.info(`Processing ${document.title}`);\n\n        // Extract data from the page using DOM manipulation\n        const title = document.querySelector('title')?.textContent;\n        const h1 = Array.from(document.querySelectorAll('h1')).map((el) => el.textContent);\n\n        // Save results to dataset\n        await Dataset.pushData({\n            title,\n            h1,\n        });\n\n        // Enqueue links to other pages\n        await enqueueLinks();\n    },\n    // Limit crawled pages to protect against infinite crawling\n    maxRequestsPerCrawl: 20,\n});\n\n// Add URLs to a RequestList\nconst sources = await readFile('sources.txt', 'utf8');\nconst requests = sources.split('\\n').map((url) => ({ url: url.trim() }));\nawait crawler.addRequests(requests);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects, extending @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler in Node.js Project\nDESCRIPTION: Command to install TypeScript as a development dependency in a Node.js project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Adapting Crawler Routes for Parallel Scraping in JavaScript\nDESCRIPTION: This code adapts the CATEGORY route handler to enqueue product URLs to a queue that supports locking. It demonstrates how to modify existing scraper logic for parallelization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nCATEGORY: async ({ $, log, crawler }) => {\n    log.info('Category page opened!');\n\n    const products = $('a.product_pod');\n    log.info(`Number of products found: ${products.length}`);\n\n    const rq = await getOrInitQueue();\n\n    for (const product of products) {\n        const $product = $(product);\n        const url = new URL($product.attr('href'), BASE_URL).href;\n\n        await rq.addRequest({\n            url,\n            userData: {\n                label: 'DETAIL',\n            },\n        });\n    }\n\n    const nextPageHref = $('li.next a').attr('href');\n    if (nextPageHref) {\n        await crawler.addRequests([{\n            url: new URL(nextPageHref, BASE_URL).href,\n            label: 'CATEGORY',\n        }]);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation\nDESCRIPTION: Simple crawler that downloads a single page's HTML and extracts its title using Cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Creating INPUT.json for Actor Input in Crawlee\nDESCRIPTION: This bash snippet shows the file path where an INPUT.json file should be created to provide input to a Crawlee actor. The file is placed in the default key-value store of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Cheerio Crawler\nDESCRIPTION: Example of creating a Cheerio crawler that uses cheerio library for HTML parsing and extracts titles from web pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ request, $, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.url} is: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Using Local and Remote Datasets with Actor in Apify/Crawlee\nDESCRIPTION: Demonstrates how to work with both local and cloud-based datasets using Actor class. Shows the usage of forceCloud option to explicitly access platform storages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/deployment/apify_platform.mdx#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Crawling Same Hostname Links with CheerioCrawler in TypeScript\nDESCRIPTION: This snippet shows how to use CheerioCrawler to crawl only links that point to the same hostname as the starting URL. It uses the 'SameHostname' strategy which is also the default behavior of enqueueLinks().\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/crawl_relative_links.mdx#2025-04-11_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log, request }) {\n        log.info(`Processing ${request.url}...`);\n\n        // Add all same-hostname links from page to RequestQueue\n        await enqueueLinks({\n            strategy: EnqueueStrategy.SameHostname,\n            // Alternatively, you can use the string 'same-hostname'\n            // strategy: 'same-hostname',\n        });\n\n        // Or simply\n        // await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command to install Apify SDK v1 with Playwright support, enabling use of Firefox and Webkit browsers in addition to Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-chrome Docker Image\nDESCRIPTION: Example of using the Apify Docker image that includes Playwright with Chrome. Suitable for CheerioCrawler and PlaywrightCrawler, but not PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Chrome Docker Configuration\nDESCRIPTION: Docker configuration for Node.js with Puppeteer and Chrome browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-puppeteer-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee and Puppeteer for PuppeteerCrawler\nDESCRIPTION: Command to manually install Crawlee and Puppeteer for use with PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Command to install ts-node for development runtime.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production runtime.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Configuration with Stealth Plugin\nDESCRIPTION: Sample code showing Puppeteer crawler setup with puppeteer-extra and stealth plugin integration in a TypeScript environment. Located in src/crawler.ts\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/crawler-plugins/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{PuppeteerExtraSource}\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all dependencies and scripts for development and production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: LiveView Changes in BrowserPool\nDESCRIPTION: Notes that LiveView functionality is no longer available in BrowserPool, as it was in PuppeteerPool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/upgrading/upgrading_v1.md#2025-04-11_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\n// OLD\nawait puppeteerPool.serveLiveViewSnapshot();\n\n// NEW\n// There's no LiveView in BrowserPool\n```\n\n----------------------------------------\n\nTITLE: Starting Crawlee Project\nDESCRIPTION: Commands to navigate to project directory and start the crawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with Firefox browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Command to install ts-node for development runtime\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Configuration\nDESCRIPTION: Docker configuration for Playwright with Firefox browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Checking NPM Installation for Crawlee Prerequisites\nDESCRIPTION: Verifies that NPM is installed on the system, which is required for managing Crawlee dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Setting up Multi-stage Docker Build for Crawlee Actor\nDESCRIPTION: Complete Dockerfile configuration that implements a two-stage build process for a Crawlee actor. The first stage builds the application, while the second stage creates a minimal production image with only the necessary dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY . ./\n\nRUN npm run build\n\nFROM apify/actor-node:16\n\nCOPY --from=builder /usr/src/app/dist ./dist\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Fetching HTML with got-scraping in TypeScript\nDESCRIPTION: Shows how to import and use got-scraping package to fetch HTML content from a webpage. The example demonstrates making an HTTP request and handling the response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/crawl_single_url.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\n\nawait gotScraping('https://example.com');\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Instructions for installing the TypeScript compiler as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Configuring Headful Browser Mode\nDESCRIPTION: TypeScript configuration option to enable visible browser window during crawling\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Crawlee Terminal Output Example\nDESCRIPTION: Sample terminal output showing the progress and logs generated by a Crawlee crawler during execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_10\n\nLANGUAGE: log\nCODE:\n```\n[INFO] CheerioCrawler: Starting the crawl\n[INFO] CheerioCrawler: Title of https://crawlee.dev/: Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/quick-start: Quick Start | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/introduction: Introduction | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/examples: Examples | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides: Guides | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/api/core/class/BasicCrawler: BasicCrawler | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/configuration: Configuration | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/request-storage: Request Storage | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/result-storage: Result Storage | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/api/core/enum/LogLevel: LogLevel | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/proxy-management: Working with Proxies | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/session-management: Session Management | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/scaling-crawlers: Scaling Crawlers | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/avoid-blocking: Avoid getting blocked | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/integration: Integration with other tools | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/upgrading/upgrading-to-v3: Upgrading to v3 | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/cheerio-crawler-guide: CheerioCrawler guide | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/got-scraping: HTTP requests with got-scraping | Crawlee\n[INFO] CheerioCrawler: Title of https://crawlee.dev/docs/guides/jsdom-crawler-guide: JSDOMCrawler guide | Crawlee\n[INFO] CheerioCrawler: Crawl finished. Final request statistics: { pendingRequests: 0, requestsFinished: 20, requestsFailed: 0, retryHistogram: [], handledRequestsCount: 20, unhandledRequestsCount: 0, requestsFinishedPerMinute: 1050, requestsFailedPerMinute: 0, }\n\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title with Playwright\nDESCRIPTION: Shows how to extract a product title using Playwright's locator API to find and get text content from an h1 element.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Logging CheerioCrawler Output for Crawlee Website\nDESCRIPTION: Log output showing the CheerioCrawler crawling the Crawlee documentation website. It captures the start of the crawl and the titles of four pages: the homepage, examples, quick start, and guides.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Apify Proxy Configuration in Crawlee\nDESCRIPTION: Shows how to create a basic proxy configuration using Apify Proxy services. This example demonstrates creating a proxy configuration and generating a new proxy URL for use in web scraping tasks.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n```\n\n----------------------------------------\n\nTITLE: Configuring headerGeneratorOptions in sendRequest\nDESCRIPTION: Demonstrates how to customize browser fingerprint generation using headerGeneratorOptions. This allows for fine-tuning the browser signatures used in requests to better mimic real user traffic.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/got_scraping.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            headerGeneratorOptions: {\n                devices: ['mobile', 'desktop'],\n                locales: ['en-US'],\n                operatingSystems: ['windows', 'macos', 'android', 'ios'],\n                browsers: ['chrome', 'edge', 'firefox', 'safari'],\n            },\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments in Pre-v1 vs v1\nDESCRIPTION: Example showing how handler arguments were separate objects in pre-v1, making it difficult to track values across function invocations, versus the new unified Crawling Context in v1.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Comparing Handler Arguments Before and After v1\nDESCRIPTION: Comparison of how handler function arguments worked in previous versions versus the new Crawling Context approach in v1. It demonstrates how separate argument objects are now consolidated into a single context.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/upgrading/upgrading_v1.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Using Full Playwright Docker Image\nDESCRIPTION: Docker configuration for using the complete Playwright image that includes all browser engines (Chromium, Chrome, Firefox, WebKit), suitable for multi-browser testing.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:20\n```\n\n----------------------------------------\n\nTITLE: Using Playwright WebKit Docker Image\nDESCRIPTION: Example of using the Docker image with pre-installed Playwright and WebKit browser. Similar to the Chrome variant but with WebKit pre-installed instead.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:20\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: Package installation commands for different crawler types including optional dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Configuring Development Script for Crawlee TypeScript Project\nDESCRIPTION: Add a start:dev script to package.json for running the Crawlee TypeScript project directly using ts-node-esm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee CLI and Project Setup\nDESCRIPTION: Commands to install Crawlee using the CLI tool and initialize a new crawler project\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Firefox Tab Container Feature Addition\nDESCRIPTION: Enhancement that enables tab-as-a-container functionality for Firefox browsers. This was implemented in version 3.0.4.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Features\n\n* enable tab-as-a-container for Firefox ([#1456](https://github.com/apify/crawlee/issues/1456)) ([ae5ba4f](https://github.com/apify/crawlee/commit/ae5ba4f15fd6d14f444486234753ce1781c74cc8))\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK v1 with Playwright\nDESCRIPTION: Command for installing Apify SDK v1 with Playwright support, enabling Firefox and Webkit (Safari) browser automation in addition to Chrome.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/upgrading/upgrading_v1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify playwright\n```\n\n----------------------------------------\n\nTITLE: Updating Launch Options Configuration in JavaScript\nDESCRIPTION: Demonstrates the transition from launchPuppeteerOptions to the new launchContext object structure, which provides clearer separation between Apify and Puppeteer options.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler\nDESCRIPTION: Installs the TypeScript compiler as a development dependency in the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Installing puppeteer-extra with stealth plugin using npm\nDESCRIPTION: Command to install puppeteer-extra and puppeteer-extra-plugin-stealth packages via npm package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU using Playwright in JavaScript\nDESCRIPTION: This snippet demonstrates how to use Playwright to find and extract the product SKU from a specific span element.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: This snippet shows how to turn off browser fingerprints in PuppeteerCrawler. It configures the 'useFingerprints' option to false in the browserPoolOptions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Using Scoped Logging in CheerioCrawler in TypeScript\nDESCRIPTION: Demonstrates how to use the scoped log instance provided in the crawling context. This logger prefixes messages with the crawler name and should be preferred for logging inside the request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Result of Dataset.reduce() operation\nDESCRIPTION: The expected output when using the reduce method to sum all heading counts. The result is a single value representing the total number of headers across all pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging AWS Chrome Dependencies\nDESCRIPTION: Commands to install the @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda Layer deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Class in Crawlee\nDESCRIPTION: Import statement for including the Dataset class from Crawlee, which is required for saving scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Sample URLs with International Characters\nDESCRIPTION: Examples of URLs containing special characters and non-ASCII text from multiple languages, demonstrating URL encoding scenarios with German, Korean, Greek, and Japanese characters.\nSOURCE: https://github.com/apify/crawlee/blob/master/test/shared/data/unicode_url_list.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://www.example.com/dsseldorf?neighbourhood=Lrick\nhttp://ko.wikipedia.org/wiki/:\nhttp://en.wikipedia.org/wiki/\nhttp://stackoverflow.com/questions/2742852/\n```\n\n----------------------------------------\n\nTITLE: Setting up an Actor with init() and exit()\nDESCRIPTION: Example of setting up a Cheerio crawler using explicit Actor.init() and Actor.exit() functions to integrate with the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\n// This is needed to initialize the SDK and make sure everything\n// works correctly on the Apify platform\nawait Actor.init();\n\n// Exit gracefully in case of error\ntry {\n    const crawler = new CheerioCrawler({\n        async requestHandler({ $, request, enqueueLinks }) {\n            const title = $('title').text();\n            const h1 = $('h1').text();\n            await Dataset.pushData({\n                url: request.url,\n                title,\n                h1,\n            });\n\n            // Only follow links on the same domain\n            await enqueueLinks({\n                // Only limit the behavior\n                // of the globs on our domain.\n                // The crawler follows links to\n                // other domains normally and will\n                // not limit those with globs.\n                baseUrl: request.loadedUrl,\n                // Only follow links from talks and speakers\n                globs: ['**/talks/**', '**/speakers/**'],\n            });\n        },\n    });\n\n    // Either specify the list of URLs to crawl...\n    await crawler.run(['https://crawlee.dev']);\n} catch (error) {\n    console.log('Something failed', error);\n    // This will register the error within the Actor run\n    // and fail the Actor run\n    throw error;\n} finally {\n    // This is needed to properly finish the Actor run\n    // and release all resources\n    await Actor.exit();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee for Bluesky Data Collection\nDESCRIPTION: Method to initialize Crawlee crawler with appropriate concurrency settings, HTTP client configuration, and request handlers for different types of data collection (search and user profiles).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def init_crawler(self) -> None:\n    \"\"\"Initialize the crawler.\"\"\"\n    if not self._user_did:\n        raise ValueError('Session not created.')\n\n    # Initialize the datasets purge the data if it is not empty\n    self._users = await Dataset.open(name='users', configuration=Configuration(purge_on_start=True))\n    self._posts = await Dataset.open(name='posts', configuration=Configuration(purge_on_start=True))\n\n    # Initialize the crawler\n    self._crawler = HttpCrawler(\n        max_requests_per_crawl=100,\n        http_client=HttpxHttpClient(\n            # Set headers for API requests\n            headers={\n                'Content-Type': 'application/json',\n                'Authorization': f'Bearer {self._access_token}',\n                'Connection': 'Keep-Alive',\n                'accept-encoding': 'gzip, deflate, br, zstd',\n            }\n        ),\n        # Configuring concurrency of crawling requests\n        concurrency_settings=ConcurrencySettings(\n            min_concurrency=10,\n            desired_concurrency=10,\n            max_concurrency=30,\n            max_tasks_per_minute=200,\n        ),\n    )\n\n    self._crawler.router.default_handler(self._search_handler)  # Handler for search requests\n    self._crawler.router.handler(label='user')(self._user_handler)  # Handler for user requests\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for production environment execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Crawlee TypeScript Projects\nDESCRIPTION: Multi-stage Dockerfile example for building and running TypeScript Crawlee projects, with separate build stage for TypeScript compilation and production stage with only runtime dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image Tags\nDESCRIPTION: Demonstrates how to use pre-release versions of Apify Docker images for testing purposes. Examples are given for images with and without specific library versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:16-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:16-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: CheerioCrawler Log Output\nDESCRIPTION: Log output showing a CheerioCrawler instance crawling multiple pages from crawlee.dev, including the main page and documentation sections. Each log entry shows the crawler successfully retrieving page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TypeScript configuration file (tsconfig.json) with module settings and compiler options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Crawlee Log Output Example\nDESCRIPTION: Example log output when running a Crawlee crawler, showing the crawler scraping the Crawlee website and extracting page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_4\n\nLANGUAGE: log\nCODE:\n```\nINFO  PlaywrightCrawler: Starting the crawl\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'\nINFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Handling Page Arguments in Pre-v1 Implementation\nDESCRIPTION: Example showing how handler arguments worked before SDK v1, where separate objects were created for different handlers.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async (args1) => {\n    args1.hasOwnProperty('proxyInfo') // true\n}\n\nconst handleFailedRequestFunction = async (args2) => {\n    args2.hasOwnProperty('proxyInfo') // false\n}\n\nargs1 === args2 // false\n```\n\n----------------------------------------\n\nTITLE: Crawling Sitemaps with Playwright Crawler in TypeScript\nDESCRIPTION: This code demonstrates how to crawl a sitemap using Playwright Crawler in Crawlee. It uses the Sitemap utility to fetch and parse the sitemap from cnn.com, then processes each URL with the Playwright Crawler to extract and log headlines from articles.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawl_sitemap.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, EnqueueStrategy } from '@crawlee/playwright';\nimport { Sitemap } from '@crawlee/utils';\n\n// We'll use CNN as our sitemap provider\nconst sitemapUrl = 'https://www.cnn.com/sitemaps/cnn/index.xml';\n\n// Create and initialize the crawler\nconst crawler = new PlaywrightCrawler({\n    // We want to keep the crawler going even if there's an error in our code\n    ignoreSslErrors: true,\n    requestHandler: async ({ page, request, enqueueLinks, log }) => {\n        // If this is a regular document, we have to parse it to find all the links\n        log.info(`Processing ${request.url}...`);\n\n        // If we're crawling an article, extract some data from it.\n        if (/\\/[\\w-]+\\/\\d{4}\\/\\d{2}\\/\\d{2}\\/.+/i.test(request.url)) {\n            const title = await page.title();\n            log.info(`Found article with title: ${title}`);\n        }\n    },\n    maxRequestsPerCrawl: 10, // Limitation for only 10 requests.\n});\n\n// Initialize the Sitemap class from @crawlee/utils with the supplied URL\nconst sitemap = await Sitemap.load({ url: sitemapUrl });\n\n// Add the sitemap URLs to the crawler's request queue\nawait crawler.addRequests(\n    sitemap.urls.map((url) => ({ url, label: 'SITEMAP' })),\n    { strategy: EnqueueStrategy.SameHostname },\n);\n\n// Run the crawler\nawait crawler.run();\n\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main Wrapper in TypeScript\nDESCRIPTION: Example showing how to use the Actor.main wrapper function which handles initialization, execution, and cleanup. This is equivalent to calling init and exit manually.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node for Development\nDESCRIPTION: Command to install ts-node as a development dependency for running TypeScript directly.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor to Apify Platform\nDESCRIPTION: Command to deploy an actor to the Apify platform\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Handling Blocked Requests in CustomCrawler\nDESCRIPTION: Method to handle blocked requests by checking status codes and raising SessionError if needed. Part of the context pipeline in the CustomCrawler class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    async def _handle_blocked_request(self, crawling_context: CustomContext) -> AsyncGenerator[CustomContext, None]:\n        if self._retry_on_blocked:\n            status_code = crawling_context.http_response.status_code\n\n            if crawling_context.session and crawling_context.session.is_blocked_status_code(status_code=status_code):\n                raise SessionError(f'Assuming the session is blocked based on HTTP status code {status_code}')\n\n        yield crawling_context\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Selector in enqueueLinks Function\nDESCRIPTION: Example showing how to customize the element selector used by enqueueLinks to find links in a page, targeting specific div elements with links instead of all anchor tags.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    selector: 'div.has-link'\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to disable browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // ...\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing and Packaging Browser Dependencies for AWS Lambda\nDESCRIPTION: Commands to install the @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda Layer deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/deployment/aws-browsers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\nnpm i -S @sparticuz/chromium\n\n# Zip the dependencies\nzip -r dependencies.zip ./node_modules\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Locking Support in Crawlee\nDESCRIPTION: A utility function to create or retrieve a request queue that supports request locking, which is essential for parallel scraping. The function can optionally purge the queue before initializing it.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/parallel-scraping/parallel-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue } from 'crawlee';\n\n/**\n * Gets or initializes the request queue with locking support.\n *\n * @param {boolean} [purgeBeforeUse=false] If true, the queue will be purged before use\n * @returns {Promise<RequestQueue>} The request queue\n */\nexport const getOrInitQueue = async (purgeBeforeUse = false) => {\n    // As of v3.5.0, shared is experimental, we will use the default queue\n    // Once Crawlee is updated to a stable version with shared locked queues, we will update this file\n    // And you can use the \"shared\" option to create a shared queue that supports locking\n    const requestQueue = await RequestQueue.open();\n\n    if (purgeBeforeUse) {\n        await requestQueue.drop();\n    }\n\n    return requestQueue;\n};\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. This approach allows for more control over queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, RequestQueue } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\nconst crawler = new CheerioCrawler({\n    requestQueue,\n    async requestHandler({ $, request, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}`);\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Path Structure\nDESCRIPTION: Shows the directory structure where dataset items are stored in the project folder.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/examples/add_data_to_dataset.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/datasets/default/\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development environment using ts-node-esm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration in Package.json\nDESCRIPTION: JSON configuration for the package.json to add a production script that runs the compiled JavaScript code from the dist directory.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Actor Locally with Apify CLI\nDESCRIPTION: Commands to navigate to the actor directory and run it locally using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Search Request URL Example for Accommodation Website\nDESCRIPTION: Example URL pattern used for search requests on the accommodationforstudents.com website, showing query parameters for filtering accommodation options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.accommodationforstudents.com/search?limit=22&skip=0&random=false&mode=text&numberOfBedrooms=0&occupancy=min&countryCode=gb&location=London&sortBy=price&order=asc\n```\n\n----------------------------------------\n\nTITLE: Checking NPM Version\nDESCRIPTION: Command to verify the installed version of NPM package manager\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Creating Two-Stage Docker Build for Crawlee Project\nDESCRIPTION: This Dockerfile implements a two-stage build process for a Crawlee project. The first stage (builder) compiles the TypeScript code, while the second stage creates a production-ready image with only the necessary dependencies. It optimizes Docker layer caching by copying package files first, then installing dependencies, and finally adding source code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_node_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node:16 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node:16\n\n# Copy only built JS files from builder image\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n\n# Run the image.\nCMD npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with CSS Selector in JavaScript Console\nDESCRIPTION: This snippet demonstrates how to use document.querySelectorAll() in the browser console to select all elements with a specific CSS class. It's used to verify that the selector '.collection-block-item' correctly targets only the desired collection card elements.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Handling Infinite Scrolling in Google Maps with Python\nDESCRIPTION: This function manages infinite scrolling in Google Maps by detecting the feed element, scrolling down, and checking if new content has loaded. It includes error handling and timeouts to ensure smooth scrolling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nasync def _load_more_items(self, page: Page) -> bool:\n        \"\"\"Scroll down to load more items.\"\"\"\n        try:\n            feed = await page.query_selector('div[role=\"feed\"]')\n            if not feed:\n                return False\n            prev_scroll = await feed.evaluate(\"(element) => element.scrollTop\")\n            await feed.evaluate(\"(element) => element.scrollTop += 800\")\n            await page.wait_for_timeout(2000)\n\n            new_scroll = await feed.evaluate(\"(element) => element.scrollTop\")\n            if new_scroll <= prev_scroll:\n                return False\n            await page.wait_for_timeout(1000)\n            return True\n        except Exception as e:\n            context.log.exception(\"Error during scroll\")\n            return False\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TypeScript configuration file setup with ES2022 module and target settings, extending @apify/tsconfig.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying NPM Installation\nDESCRIPTION: Command to check the installed version of NPM\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Using Actor.init and Actor.exit in TypeScript\nDESCRIPTION: Example showing how to initialize and exit an Actor using explicit initialization and exit calls. This is equivalent to using the Actor.main wrapper.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Example of selecting a specific Node.js version (20) for the Docker image. This ensures compatibility and stability for production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 20\nFROM apify/actor-node:20\n```\n\n----------------------------------------\n\nTITLE: Version History Documentation in Markdown\nDESCRIPTION: Structured change log entries documenting version changes, bug fixes, and improvements in the Crawlee project. Includes details about breaking changes, dependency updates, and new feature implementations.\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n### Full list of changes\n\n* fix `RequestError: URI malformed` in cheerio crawler (#1205)\n* only provide Cookie header if cookies are present (#1218)\n* handle extra cases for `diffCookie` (#1217)\n* add timeout for task function (#1234)\n* implement proxy per page in browser crawlers (#1228)\n* add fingerprinting support (#1243)\n* implement abortable timeouts (#1245)\n* add timeouts with retries to `runTaskFunction()` (#1250)\n* automatically convert google spreadsheet URLs to CSV exports (#1255)\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript compiler as development dependency\nDESCRIPTION: Command to install TypeScript compiler as a development dependency for the project, ensuring it doesn't pollute the production environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Handlers for LinkedIn Job Scraping\nDESCRIPTION: Creates a router with two handlers: a default handler to extract job listing links from the search results page, and a job_listing handler to extract detailed information from each job posting page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrouter = Router[PlaywrightCrawlingContext]()\n\n@router.default_handler\nasync def default_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n\n    #select all the links for the job posting on the page\n    hrefs = await context.page.locator('ul.jobs-search__results-list a').evaluate_all(\"links => links.map(link => link.href)\")\n\n    #add all the links to the job listing route\n    await context.add_requests(\n            [Request.from_url(rec, label='job_listing') for rec in hrefs]\n        )\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Title with JSDOM in Browser JavaScript\nDESCRIPTION: Shows the difference between accessing a page title in browsers versus JSDOM. While browsers can use document.title directly, JSDOM requires window.document.title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/jsdom_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Return the page title\ndocument.title; // browsers\nwindow.document.title; // JSDOM\n```\n\n----------------------------------------\n\nTITLE: Basic Web Scraping with PlaywrightCrawler\nDESCRIPTION: A simple example showing how to use PlaywrightCrawler to crawl a website, extract page titles, save the data to a dataset, and follow links to other pages automatically.\nSOURCE: https://github.com/apify/crawlee/blob/master/README.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// PlaywrightCrawler crawls the web using a headless\n// browser controlled by the Playwright library.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({ title, url: request.loadedUrl });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest Helper in BasicCrawler\nDESCRIPTION: Example showing how to use the sendRequest helper method in a BasicCrawler to process requests through got-scraping. Demonstrates how to specify response type options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/upgrading/upgrading_v3.md#2025-04-11_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Chromium for AWS Lambda Environment\nDESCRIPTION: Example code showing how to supply the Chromium path from the @sparticuz/chromium package and configure Chrome with appropriate arguments for running in the AWS Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/aws-browsers.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n// highlight-next-line\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n    // highlight-start\n    launchContext: {\n        launchOptions: {\n             executablePath: await aws_chromium.executablePath(),\n             args: aws_chromium.args,\n             headless: true\n        }\n    }\n    // highlight-end\n}, new Configuration({\n    persistStorage: false,\n}));\n```\n\n----------------------------------------\n\nTITLE: Migrating from launchPuppeteerFunction to Browser Lifecycle Hooks\nDESCRIPTION: Shows how to replace the launchPuppeteerFunction with browser-pool lifecycle hooks for more flexible and consistent browser management across Puppeteer and Playwright.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerFunction = async (launchPuppeteerOptions) => {\n    if (someVariable === 'chrome') {\n        launchPuppeteerOptions.useChrome = true;\n    }\n    return Apify.launchPuppeteer(launchPuppeteerOptions);\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction,\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst maybeLaunchChrome = (pageId, launchContext) => {\n    if (someVariable === 'chrome') {\n        launchContext.useChrome = true;\n    }\n}\n\nconst crawler = new Apify.PuppeteerCrawler({\n    browserPoolOptions: {\n        preLaunchHooks: [maybeLaunchChrome]\n    },\n    // ...\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    maybeLaunchChrome,\n    useHeadfulIfNeeded,\n    injectNewFingerprint,\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst preLaunchHooks = [\n    async function maybeLaunchChrome(pageId, launchContext) {\n        const { request } = crawler.crawlingContexts.get(pageId);\n        if (request.userData.useHeadful === true) {\n            launchContext.launchOptions.headless = false;\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Next.js Search API URL Pattern\nDESCRIPTION: The URL pattern for accessing search results data directly from Next.js API, which returns JSON data instead of HTML. Includes placeholders for build ID, location, and page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.accommodationforstudents.com/_next/data/[build_id]/search-results.json?location=[location]&page=[page]\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json build script and main entry point\nDESCRIPTION: Package.json configuration that defines a build script using TypeScript compiler and correctly specifies the main entry point for the built code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-stage Build Configuration\nDESCRIPTION: Dockerfile setup for TypeScript project using multi-stage build to optimize final image size.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16 AS builder\n\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\nFROM apify/actor-node:16\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Docker Multi-stage Build Configuration\nDESCRIPTION: Dockerfile configuration for TypeScript project using multi-stage build process\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Pool Page Limits\nDESCRIPTION: Example showing how to configure maximum number of open pages per browser and page count before browser retirement.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n    maxOpenPagesPerBrowser: 20,\n    retireBrowserAfterPageCount: 100,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Product Detail Extraction\nDESCRIPTION: Handlers for product listings and details that extract shoe URL, title, price, and description from product pages.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@router.handler('listing')\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Handler for shoe listings.\"\"\"\n\n    await context.enqueue_links(selector='a.product-card__link-overlay', label='detail')\n\n\n@router.handler('detail')\nasync def detail_handler(context: PlaywrightCrawlingContext) -> None:\n    \"\"\"Handler for shoe details.\"\"\"\n\n    title = await context.page.get_by_test_id(\n        'product_title',\n    ).text_content()\n\n    price = await context.page.get_by_test_id(\n        'currentPrice-container',\n    ).first.text_content()\n\n    description = await context.page.get_by_test_id(\n        'product-description',\n    ).text_content()\n\n    await context.push_data(\n        {\n            'url': context.request.loaded_url,\n            'title': title,\n            'price': price,\n            'description': description,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Actor Docker Image with Node.js and Crawlee\nDESCRIPTION: This Dockerfile sets up an Apify actor environment using a Node.js base image. It installs npm dependencies, copies the source code, and specifies the command to run the actor. The build process is optimized for quick rebuilds by leveraging Docker layer caching.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_node_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Starting the Crawlee Project\nDESCRIPTION: Navigate to the project directory and start the crawler using npm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Scraping Current Price with Playwright in JavaScript\nDESCRIPTION: Shows how to extract and process the current price of a product using Playwright, including filtering and string manipulation.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n```\n\n----------------------------------------\n\nTITLE: Running and Executing the Google Maps Scraper\nDESCRIPTION: Functions for executing the scraper with a search query, including setup, running the crawler, and exporting data. Contains both the run method and the main entry point for the script.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nasync def run(self, search_query: str) -> None:\n    \"\"\"Execute the scraper with a search query\"\"\"\n    try:\n        await self.setup_crawler()\n        start_url = f\"https://www.google.com/maps/search/{search_query.replace(' ', '+')}\"\n        await self.crawler.run([start_url])\n        await self.crawler.export_data_json('gmap_data.json')\n    except Exception as e:\n        print(f\"Error running scraper: {str(e)}\")\n\nasync def main():\n    \"\"\"Entry point of the script\"\"\"\n    scraper = GoogleMapsScraper(headless=True)\n    search_query = \"hotels in bengaluru\"\n    await scraper.run(search_query)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file extending @apify/tsconfig with ES2022 module and target settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Opening Pages with Each Browser Plugin\nDESCRIPTION: Example using the newPageWithEachPlugin function to consistently run tasks in multiple browser environments simultaneously.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BrowserPool, PlaywrightPlugin, PuppeteerPlugin } from '@crawlee/browser-pool';\nimport playwright from 'playwright';\nimport puppeteer from 'puppeteer';\n\nconst browserPool = new BrowserPool({\n    browserPlugins: [\n        new PlaywrightPlugin(playwright.chromium),\n        new PuppeteerPlugin(puppeteer),\n    ],\n});\n\nconst pages = await browserPool.newPageWithEachPlugin();\nconst promises = pages.map(async page => {\n    // Run some task with each page\n    // pages are in order of plugins:\n    // [playwrightPage, puppeteerPage]\n    await page.close();\n});\nawait Promise.all(promises);\n\n// Continue with some more work.\n```\n\n----------------------------------------\n\nTITLE: Upgrading Crawlee for Python via pip\nDESCRIPTION: Command to upgrade Crawlee for Python to the latest version using pip package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install --upgrade crawlee\n```\n\n----------------------------------------\n\nTITLE: Configuring Headful Browser Mode\nDESCRIPTION: TypeScript configuration option to enable visible browser window during crawling\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Compiler for Crawlee Project\nDESCRIPTION: Install the TypeScript compiler as a development dependency for a Crawlee project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev typescript\n```\n\n----------------------------------------\n\nTITLE: Setting Up PlaywrightCrawler for Google Maps in Python\nDESCRIPTION: Initializing a PlaywrightCrawler with configuration for headless mode and timeout settings to handle Google Maps scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom datetime import timedelta\n\n# Initialize crawler with browser visibility and timeout settings\ncrawler = PlaywrightCrawler(\n    headless=False,  # Shows the browser window while scraping\n    request_handler_timeout=timedelta(\n        minutes=5\n    ),  # Allows plenty of time for page loading\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Crawler Start URL\nDESCRIPTION: Python code snippet to set the start URL for the web crawler to the target product page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nawait crawler.run(['https://www.centralcomputer.com/raspberry-pi-5-8gb-ram-board.html'])\n```\n\n----------------------------------------\n\nTITLE: Deploying the Price Tracking Actor to Apify Platform\nDESCRIPTION: Command to deploy the price tracking actor to the Apify platform. This uploads the actor code and configuration to make it available for cloud execution and scheduling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Defining Price Threshold\nDESCRIPTION: Python code snippet to set a price threshold for triggering alerts.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Define a price threshold\nprice_threshold = 80\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler with Separate Configuration Instance\nDESCRIPTION: JavaScript code showing how to initialize a PlaywrightCrawler with a new Configuration instance. This ensures each crawler has its own storage to prevent interference between multiple crawler instances in the Lambda environment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/deployment/aws-browsers.md#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// For more information, see https://crawlee.dev/\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\n\nconst startUrls = ['https://crawlee.dev'];\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: router,\n// highlight-start\n}, new Configuration({\n    persistStorage: false,\n}));\n// highlight-end\n\nawait crawler.run(startUrls);\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with BasicCrawler in Crawlee\nDESCRIPTION: Basic example demonstrating how to use the sendRequest function with BasicCrawler to make HTTP requests and retrieve response bodies.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest();\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler in main.py\nDESCRIPTION: Main entry point for the crawler that configures PlaywrightCrawler with options for headless mode and maximum requests.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.playwright_crawler import PlaywrightCrawler\n\nfrom .routes import router\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n    crawler = PlaywrightCrawler(\n        headless=False,\n        request_handler=router,\n        max_requests_per_crawl=100,\n    )\n\n    await crawler.run(\n        [\n            'https://nike.com/,\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TSConfig file setup extending @apify/tsconfig with ES2022 module support\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build setup and main entry point specification.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Configuration in Crawlee\nDESCRIPTION: JavaScript implementation of proxy configuration in Crawlee using the ProxyConfiguration class with custom proxy URLs. This setup allows for proxy rotation during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy1.example.com',\n        'http://proxy2.example.com',\n    ]\n});\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing TSConfig Dependencies\nDESCRIPTION: Command to install Apify TSConfig package\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install and initialize a new Crawlee project using the CLI tool\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee and Puppeteer for PuppeteerCrawler\nDESCRIPTION: Install both Crawlee and Puppeteer packages for projects using PuppeteerCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Extracting Manufacturer from URL in JavaScript\nDESCRIPTION: Code to extract the manufacturer name from a product URL by splitting the string. This approach parses the URL to get the manufacturer name from the product path segment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/06-scraping.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from Exported Google Maps Data\nDESCRIPTION: Sample of the exported JSON file containing hotel information scraped from Google Maps, including name, rating, reviews, price, amenities, and link.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"Vividus Hotels, Bangalore\",\n    \"rating\": \"4.3\",\n    \"reviews\": \"633\",\n    \"price\": \"3,667\",\n    \"amenities\": [\n      \"Pool available\",\n      \"Free breakfast available\",\n      \"Free Wi-Fi available\",\n      \"Free parking available\"\n    ],\n    \"link\": \"https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/...\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Adding hideInternalConsole Feature to JSDOMCrawler\nDESCRIPTION: Implements a new feature 'hideInternalConsole' in the JSDOMCrawler. This enhancement is tracked in issue #1707.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/jsdom-crawler/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n* hideInternalConsole in JSDOMCrawler ([#1707](https://github.com/apify/crawlee/issues/1707)) ([8975f90](https://github.com/apify/crawlee/commit/8975f9088cf4dd38629c21e21061616fc1e7b003))\n```\n\n----------------------------------------\n\nTITLE: Crawlee Crawler Log Output\nDESCRIPTION: Console log output showing CheerioCrawler crawling multiple pages from crawlee.dev domain and extracting page titles.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Running the Google Maps Scraper CLI Command\nDESCRIPTION: Command to execute the Google Maps scraper script from the command line.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 gmap_scraper.py\n```\n\n----------------------------------------\n\nTITLE: Verifying Node.js Installation\nDESCRIPTION: Command to check installed Node.js version\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TypeScript Configuration\nDESCRIPTION: Command to install @apify/tsconfig as a development dependency for TypeScript configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Exporting QueueOperationInfo in Crawlee Core Package\nDESCRIPTION: Adds the QueueOperationInfo export to the Crawlee core package. This allows users to access the QueueOperationInfo type from the core package.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nadd `QueueOperationInfo` export to the core package\n```\n\n----------------------------------------\n\nTITLE: Adapting Request Handler for Playwright with Cheerio Support\nDESCRIPTION: Shows how to update a Cheerio-dependent request handler to work with Playwright. This approach leverages the parseWithCheerio helper function to maintain familiar Cheerio syntax while using Playwright's browser engine.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawlingContext } from 'crawlee';\n\nconst requestHandler = async (context: PlaywrightCrawlingContext) => {\n    const { request, parseWithCheerio } = context;\n    const { url } = request;\n\n    const $ = await parseWithCheerio(); // Get the Cheerio object for the page.\n\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Memory Storage Bug Fix Entry\nDESCRIPTION: Changelog entry documenting a fix for handling EXDEV errors during storage purging\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/memory-storage/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.3.3](https://github.com/apify/crawlee/compare/v3.3.2...v3.3.3) (2023-05-31)\n\n### Bug Fixes\n\n* **MemoryStorage:** handle EXDEV errors when purging storages ([#1932](https://github.com/apify/crawlee/issues/1932)) ([e656050](https://github.com/apify/crawlee/commit/e6560507243f5e2d0b126160616573f13e5998e1))\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.11.0\nDESCRIPTION: Added iframe expansion to parseWithCheerio in browsers and an option to ignore iframes.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/playwright-crawler/CHANGELOG.md#2025-04-11_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n# [3.11.0](https://github.com/apify/crawlee/compare/v3.10.5...v3.11.0) (2024-07-09)\n\n\n### Features\n\n* add `iframe` expansion to `parseWithCheerio` in browsers ([#2542](https://github.com/apify/crawlee/issues/2542)) ([328d085](https://github.com/apify/crawlee/commit/328d08598807782b3712bd543e394fe9a000a85d)), closes [#2507](https://github.com/apify/crawlee/issues/2507)\n* add `ignoreIframes` opt-out from the Cheerio iframe expansion ([#2562](https://github.com/apify/crawlee/issues/2562)) ([474a8dc](https://github.com/apify/crawlee/commit/474a8dc06a567cde0651d385fdac9c350ddf4508))\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Properties in Handler Functions\nDESCRIPTION: Shows how to access crawler properties and methods through the new crawler object in the context.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v1.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, crawler }) => {\n    await crawler.requestQueue.addRequest({ url: 'https://example.com' });\n    await crawler.autoscaledPool.pause();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Apify Actor with Playwright and Chrome\nDESCRIPTION: A complete Dockerfile that sets up an Apify actor environment with Node.js 16, Playwright, and Chrome. It optimizes the build process by leveraging Docker layer caching, installs only production dependencies, and configures the container to run the actor via npm start.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/docker_browser_js.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image.\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Using Actor.main() Wrapper Function in TypeScript\nDESCRIPTION: Shows the alternative approach using Actor.main() which handles initialization and exit automatically. This is equivalent to the explicit init/exit pattern but with less boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a New Apify Actor\nDESCRIPTION: Commands to create and run a new actor project using Apify CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Command to install TypeScript type definitions for Node.js as a development dependency to enable type-checking for Node.js features.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript as a Development Dependency\nDESCRIPTION: Command to install TypeScript compiler as a development dependency in a Node.js project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --dev typescript\n```\n\n----------------------------------------\n\nTITLE: Importing Storage Clients in Crawlee for Python v0.5\nDESCRIPTION: Example showing the new import structure for storage client classes in Crawlee for Python v0.5, demonstrating the consolidated package structure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.storage_clients import MemoryStorageClient\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Defining Google Maps Scraper Class in Python\nDESCRIPTION: Importing necessary libraries and defining the GoogleMapsScraper class structure with initialization parameters for headless mode and timeout settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom datetime import timedelta\nfrom typing import Dict, Optional, Set\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom playwright.async_api import Page, ElementHandle\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Links with Globs in Playwright Crawler\nDESCRIPTION: Demonstrates how to enqueue links matching specific URL patterns using globs in a PlaywrightCrawler instance\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://apify.com/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Apify's TypeScript configuration\nDESCRIPTION: Command to install Apify's TypeScript configuration as a development dependency, providing a predefined set of TypeScript rules for Apify projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Instructions for installing Crawlee using the Crawlee CLI, which sets up all dependencies and boilerplate code for a new crawler project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Accepting and Logging User Input in TypeScript with Crawlee\nDESCRIPTION: This code snippet demonstrates how to accept user input in a Crawlee project using TypeScript. It imports necessary modules, retrieves input from the actor, and logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of your actor\nconst input = await Actor.getInput();\nconsole.log('Input:');\nconsole.log(JSON.stringify(input, null, 2));\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawler with CurlImpersonate\nDESCRIPTION: Setting up BeautifulSoupCrawler with CurlImpersonateHttpClient and concurrency settings\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawler\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\nfrom crawlee import ConcurrencySettings, HttpHeaders\n\nasync def main() -> None:\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\n\n    http_client = CurlImpersonateHttpClient(impersonate=\"chrome124\",\n                                            headers=HttpHeaders({\"referer\": \"https://www.google.com/\",\n                                                     \"accept-language\": \"en\",\n                                                     \"accept-encoding\": \"gzip, deflate, br, zstd\",\n                                                     \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n                                            }))\n\n    crawler = BeautifulSoupCrawler(\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=10,\n        max_crawl_depth=5\n    )\n\n    await crawler.run(['https://www.google.com/search?q=Apify'])\n```\n\n----------------------------------------\n\nTITLE: Importing ApiLink Component in Markdown\nDESCRIPTION: This code snippet imports the ApiLink component, likely used for creating API documentation links within the markdown content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: Sanity Check Script with PlaywrightCrawler\nDESCRIPTION: Initial crawler setup to verify the scraping environment and test category page access. Uses PlaywrightCrawler to visit the start URL and print category content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/04-real-world-project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page }) => {\n        const texts = await page.$$eval('.collection-block-item', (elements) => {\n            return elements.map((el) => el.textContent);\n        });\n        console.log('Categories found:', texts);\n    },\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output Using Proxies for NYC Hotels\nDESCRIPTION: Example of hotel data scraped from New York City using proxies, showing the structure of extracted information including name, rating, reviews, price, and amenities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"The Manhattan at Times Square Hotel\",\n  \"rating\": \"3.1\",\n  \"reviews\": \"8,591\",\n  \"price\": \"$120\",\n  \"amenities\": [\n    \"Free parking available\",\n    \"Free Wi-Fi available\",\n    \"Air-conditioned available\",\n    \"Breakfast available\"\n  ],\n  \"link\": \"https://www.google.com/maps/place/...\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image\nDESCRIPTION: Examples of using pre-release versions of Docker images with beta tags, both with and without a specific library version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:20-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:20-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Adding ts-node for development environment execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Version Format Examples in Markdown\nDESCRIPTION: Examples of version numbering formats used in the release process, including beta and latest release versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/RELEASE.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n${VERSION}-beta.${COUNTER}\n0.15.1-beta.3\n0.14.15\n0.15.0-beta.0\nv${VERSION}\nv0.15.0\nv0.16.17\n0.15.1\n```\n\n----------------------------------------\n\nTITLE: Migrating Event Handling in TypeScript\nDESCRIPTION: Shows the difference between old and new event handling syntax, migrating from Apify.events to Actor.on\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/upgrading/upgrading_v3.md#2025-04-11_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n-Apify.events.on(...);\n+Actor.on(...);\n```\n\n----------------------------------------\n\nTITLE: Using BasicCrawler with Custom Proxy URL\nDESCRIPTION: Example showing how to provide a custom proxy URL when using sendRequest with BasicCrawler. This helps hide the real IP address during web scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-stage Docker Build for Crawlee Actor\nDESCRIPTION: Docker configuration that creates an optimized build environment for a Crawlee actor using Node.js and Playwright with Chrome. Uses a multi-stage build process to minimize final image size and implements efficient layer caching for dependencies. Includes proper user permissions and production-ready configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:20 AS builder\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm install --include=dev --audit=false\n\nCOPY --chown=myuser . ./\n\nRUN npm run build\n\nFROM apify/actor-node-playwright-chrome:20\n\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\nCOPY --chown=myuser package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\nCOPY --chown=myuser . ./\n\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Cloning Request userData in Crawlee\nDESCRIPTION: Fixes an issue by cloning the request.userData when creating a new request object. This ensures that modifications to the userData of a new request do not affect the original request's userData.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nclone `request.userData` when creating new request object\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform using Configuration\nDESCRIPTION: JavaScript code to initialize the Apify SDK with an API token.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry for v3.13.1\nDESCRIPTION: Changelog entry documenting renaming of RobotsFile to RobotsTxtFile and addition of respectRobotsTxtFile crawler option.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/linkedom-crawler/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.1](https://github.com/apify/crawlee/compare/v3.13.0...v3.13.1) (2025-04-07)\n\n### Bug Fixes\n\n* rename `RobotsFile` to `RobotsTxtFile` ([#2913](https://github.com/apify/crawlee/issues/2913)) ([3160f71](https://github.com/apify/crawlee/commit/3160f717e865326476d78089d778cbc7d35aa58d)), closes [#2910](https://github.com/apify/crawlee/issues/2910)\n\n### Features\n\n* add `respectRobotsTxtFile` crawler option ([#2910](https://github.com/apify/crawlee/issues/2910)) ([0eabed1](https://github.com/apify/crawlee/commit/0eabed1f13070d902c2c67b340621830a7f64464))\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Definitions\nDESCRIPTION: Adding TypeScript definitions for Node.js as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Installing playwright-extra with stealth plugin using npm\nDESCRIPTION: Command to install playwright-extra and puppeteer-extra-plugin-stealth packages via npm package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: HTTP Client with Headers Encountering Cloudflare Protection\nDESCRIPTION: This Python code demonstrates how requests to protected websites (like G2.com) receive 403 Forbidden responses despite using browser-like headers. It uses httpx with HTTP/2 enabled to make the request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import Client\n\nclient = Client(http2=True)\n\nurl = 'https://www.g2.com/'\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n    \"Connection\": \"keep-alive\",\n}\n\nresponse = client.get(url, headers=headers)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry for Version 3.13.2\nDESCRIPTION: Changelog entry documenting feature additions including onSkippedRequest option\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.2](https://github.com/apify/crawlee/compare/v3.13.1...v3.13.2) (2025-04-08)\n\n### Features\n\n* add `onSkippedRequest` option ([#2916](https://github.com/apify/crawlee/issues/2916)) ([764f992](https://github.com/apify/crawlee/commit/764f99203627b6a44d2ee90d623b8b0e6ecbffb5)), closes [#2910](https://github.com/apify/crawlee/issues/2910)\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration File Setup\nDESCRIPTION: TypeScript configuration (tsconfig.json) that extends Apify's base configuration and enables modern JavaScript features like top-level await by targeting ES2022 modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\"\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee CLI and Creating a Project\nDESCRIPTION: Commands for installing Crawlee using the CLI tool, which creates a new crawler project with boilerplate code and necessary dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Filtering\nDESCRIPTION: Example of using glob patterns to filter URLs during the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying INPUT.json File Location for Crawlee Actor\nDESCRIPTION: This bash snippet shows the file path where the INPUT.json file should be placed to provide input to the actor. The file should be created in the default key-value store of the project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/accept_user_input.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Using Request Loaders in Crawlee for Python\nDESCRIPTION: Example showing how to use the new RequestLoader, RequestManager, and RequestManagerTandem classes to manage how Crawlee accesses and stores requests, allowing combination of external data sources with Crawlee's standard RequestQueue.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-10/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.request_loaders import RequestList, RequestManagerTandem\nfrom crawlee.storages import RequestQueue\n\n\nasync def main() -> None:\n    rl = RequestList(\n        [\n            'https://crawlee.dev',\n            'https://apify.com',\n            # Long list of URLs...\n        ],\n    )\n\n    rq = await RequestQueue.open()\n\n    # Combine them into a single request source.\n    tandem = RequestManagerTandem(rl, rq)\n\n    crawler = ParselCrawler(request_manager=tandem)\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info(f'Crawling {context.request.url}')\n        # ...\n\n    await crawler.run()\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: DevTools Element Selection Using JavaScript Console\nDESCRIPTION: A JavaScript query selector example for testing CSS selectors in the browser console. This one-liner helps verify that the selector '.collection-block-item' correctly identifies only the collection cards on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/04-real-world-project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Dockerfile\nDESCRIPTION: Example of how to specify a Node.js version when using an Apify Docker image. This ensures compatibility and stability in production environments.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use Node.js 16\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Configuring Development Script with ts-node-esm\nDESCRIPTION: Package.json configuration for the development script using ts-node-esm with transpile-only mode for faster development execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring preNavigationHooks in LinkeDOMCrawler\nDESCRIPTION: Example showing how to configure preNavigationHooks to adjust gotOptions before navigation occurs\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/linkedom-crawler/README.md#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\npreNavigationHooks: [\n    (crawlingContext, gotOptions) => {\n        // ...\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Basic Node.js Docker Configuration\nDESCRIPTION: Basic Dockerfile configuration for Node.js 16 environment without browsers\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Running the Streamlit Application\nDESCRIPTION: Command to start the Streamlit web application for the LinkedIn job scraper, making the interface accessible through a web browser.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nstreamlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration\nDESCRIPTION: Multi-stage Dockerfile configuration for TypeScript project deployment.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\n# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\n# run compiled code\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Crawling Web Pages with PuppeteerCrawler\nDESCRIPTION: Example code showing how to use PuppeteerCrawler to perform a recursive crawl of the Crawlee website. PuppeteerCrawler uses a headless browser to crawl, controlled by the Puppeteer library.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// Create an instance of the PuppeteerCrawler class - a crawler\n// that automatically loads the URLs in headless Chrome / Puppeteer.\nconst crawler = new PuppeteerCrawler({\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 20,\n\n    // This function will be called for each URL to crawl.\n    // Here you can write the Crawlee logic for extracting data from a page.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({\n            url: request.url,\n            title,\n        });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks({\n            globs: ['https://crawlee.dev/**'],\n            exclude: ['.pdf', '?*']\n        });\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev/']);\n```\n\n----------------------------------------\n\nTITLE: Implementing Route Handlers in routes.mjs\nDESCRIPTION: Creates a PlaywrightRouter with separate handlers for different page types: product detail pages, category pages, and a default handler for the start page. Each handler implements specific logic for data extraction or link discovery.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/08-refactoring.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createPlaywrightRouter, Dataset } from 'crawlee';\n\n// createPlaywrightRouter() is only a helper to get better\n// intellisense and typings. You can use Router.create() too.\nexport const router = createPlaywrightRouter();\n\n// This replaces the request.label === DETAIL branch of the if clause.\nrouter.addHandler('DETAIL', async ({ request, page, log }) => {\n    log.debug(`Extracting data: ${request.url}`);\n    const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n    const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n    const title = await page.locator('.product-meta h1').textContent();\n    const sku = await page\n        .locator('span.product-meta__sku-number')\n        .textContent();\n\n    const priceElement = page\n        .locator('span.price')\n        .filter({\n            hasText: '$',\n        })\n        .first();\n\n    const currentPriceString = await priceElement.textContent();\n    const rawPrice = currentPriceString.split('$')[1];\n    const price = Number(rawPrice.replaceAll(',', ''));\n\n    const inStockElement = page\n        .locator('span.product-form__inventory')\n        .filter({\n            hasText: 'In stock',\n        })\n        .first();\n\n    const inStock = (await inStockElement.count()) > 0;\n\n    const results = {\n        url: request.url,\n        manufacturer,\n        title,\n        sku,\n        currentPrice: price,\n        availableInStock: inStock,\n    };\n\n    log.debug(`Saving data: ${request.url}`);\n    await Dataset.pushData(results);\n});\n\nrouter.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {\n    log.debug(`Enqueueing pagination for: ${request.url}`);\n    // We are now on a category page. We can use this to paginate through and enqueue all products,\n    // as well as any subsequent pages we find\n\n    await page.waitForSelector('.product-item > a');\n    await enqueueLinks({\n        selector: '.product-item > a',\n        label: 'DETAIL', // <= note the different label\n    });\n\n    // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n    const nextButton = await page.$('a.pagination__next');\n    if (nextButton) {\n        await enqueueLinks({\n            selector: 'a.pagination__next',\n            label: 'CATEGORY', // <= note the same label\n        });\n    }\n});\n\n// This is a fallback route which will handle the start URL\n// as well as the LIST labeled URLs.\nrouter.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {\n    log.debug(`Enqueueing categories from page: ${request.url}`);\n    // This means we're on the start page, with no label.\n    // On this page, we just want to enqueue all the category pages.\n\n    await page.waitForSelector('.collection-block-item');\n    await enqueueLinks({\n        selector: '.collection-block-item',\n        label: 'CATEGORY',\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Product Scraper Implementation in TypeScript\nDESCRIPTION: The complete scraper.ts file that extracts all product details from an Amazon product page including title, price, reviews, images, and attributes. It uses Cheerio for DOM traversal and custom utility functions for data extraction.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioAPI } from 'cheerio';\nimport { parseNumberFromSelector } from './utils.js';\n\ntype ProductAttribute = {\n    label: string;\n    value: string;\n};\n\ntype ProductDetails = {\n    title: string;\n    price: number;\n    listPrice: number;\n    reviewRating: number;\n    reviewCount: number;\n    imageUrls: string[];\n    attributes: ProductAttribute[];\n};\n\n/**\n * CSS selectors for the product details. Feel free to figure out different variations of these selectors.\n */\nconst SELECTORS = {\n    TITLE: 'span#productTitle',\n    PRICE: 'span.priceToPay',\n    LIST_PRICE: 'span.basisPrice .a-offscreen',\n    REVIEW_RATING: '#acrPopover a > span',\n    REVIEW_COUNT: '#acrCustomerReviewText',\n    IMAGES: '#altImages .item img',\n\n    PRODUCT_ATTRIBUTE_ROWS: '#productOverview_feature_div tr',\n    ATTRIBUTES_LABEL: 'td:nth-of-type(1) span',\n    ATTRIBUTES_VALUE: 'td:nth-of-type(2) span',\n} as const;\n\n/**\n * Extracts the product image URLs from the given Cheerio object.\n * - We have to iterate over the image elements and extract the `src` attribute.\n */\nconst extractImageUrls = ($: CheerioAPI): string[] => {\n    const imageUrls = $(SELECTORS.IMAGES)\n        .map((_, imageEl) => $(imageEl).attr('src'))\n        .get(); // `get()` - Retrieve all elements matched by the Cheerio object, as an array. Removes `undefined` values.\n\n    return imageUrls;\n};\n\n/**\n * Extracts the product attributes from the given Cheerio object.\n * - We have to iterate over the attribute rows and extract both label and value for each row.\n */\nconst extractProductAttributes = ($: CheerioAPI): ProductAttribute[] => {\n    const attributeRowEls = $(SELECTORS.PRODUCT_ATTRIBUTE_ROWS).get();\n\n    const attributeRows = attributeRowEls.map((rowEl) => {\n        const label = $(rowEl).find(SELECTORS.ATTRIBUTES_LABEL).text();\n        const value = $(rowEl).find(SELECTORS.ATTRIBUTES_VALUE).text();\n\n        return { label, value };\n    });\n\n    return attributeRows;\n};\n\n/**\n * Scrapes the product details from the given Cheerio object.\n */\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\n    const title = $(SELECTORS.TITLE).text().trim();\n\n    const price = parseNumberFromSelector($, SELECTORS.PRICE);\n    const listPrice = parseNumberFromSelector($, SELECTORS.LIST_PRICE);\n    const reviewRating = parseNumberFromSelector($, SELECTORS.REVIEW_RATING);\n    const reviewCount = parseNumberFromSelector($, SELECTORS.REVIEW_COUNT);\n\n    const imageUrls = extractImageUrls($);\n    const attributes = extractProductAttributes($);\n\n    return { title, price, listPrice, reviewRating, reviewCount, imageUrls, attributes };\n};\n```\n\n----------------------------------------\n\nTITLE: Playwright Firefox Docker Configuration\nDESCRIPTION: Dockerfile configuration for Node.js with Playwright and Firefox browser support\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/docker_images.mdx#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-firefox:16\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Extra Dependencies\nDESCRIPTION: Commands for installing the required packages playwright-extra and puppeteer-extra-plugin-stealth via npm\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/crawler-plugins/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install playwright-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Output from Successful Headless Browser Scraping\nDESCRIPTION: The console output from successfully scraping the actor card content using a headless browser crawler that properly waits for elements to render.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/javascript-rendering.mdx#2025-04-11_snippet_3\n\nLANGUAGE: log\nCODE:\n```\nACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]\n```\n\n----------------------------------------\n\nTITLE: Production Script Configuration\nDESCRIPTION: Package.json configuration for running compiled JavaScript in production.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:prod\": \"node dist/main.js\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using EnqueueStrategy.All with Unsupported Protocols in JavaScript\nDESCRIPTION: Fix for EnqueueStrategy.All erroring with links using unsupported protocols.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nEnqueueStrategy.All\n```\n\n----------------------------------------\n\nTITLE: Saving Data from Bluesky API Crawler to JSON Files\nDESCRIPTION: This method saves the collected posts and users data to JSON files. It writes the content of both datasets to respective files with proper formatting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def save_data(self) -> None:\n    \"\"\"Save the data.\"\"\"\n    if not self._users or not self._posts:\n        raise ValueError('Datasets not initialized.')\n\n    with open('users.json', 'w') as f:\n        await self._users.write_to_json(f, indent=4)\n\n    with open('posts.json', 'w') as f:\n        await self._posts.write_to_json(f, indent=4)\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Attributes with Cheerio in TypeScript\nDESCRIPTION: Function that scrapes product attributes from an Amazon product page. It uses Cheerio to iterate over table rows in the product overview section, extracting both the label and value for each attribute.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntype ProductAttribute = {\n    label: string;\n    value: string;\n};\n\nconst SELECTORS = {\n    ...\n    PRODUCT_ATTRIBUTE_ROWS: '#productOverview_feature_div tr',\n    ATTRIBUTES_LABEL: 'td:nth-of-type(1) span',\n    ATTRIBUTES_VALUE: 'td:nth-of-type(2) span',\n} as const;\n\n/**\n * Extracts the product attributes from the given Cheerio object.\n * - We have to iterate over the attribute rows and extract both label and value for each row.\n */\nconst extractProductAttributes = ($: CheerioAPI): ProductAttribute[] => {\n    const attributeRowEls = $(SELECTORS.PRODUCT_ATTRIBUTE_ROWS).get();\n\n    const attributeRows = attributeRowEls.map((rowEl) => {\n        const label = $(rowEl).find(SELECTORS.ATTRIBUTES_LABEL).text();\n        const value = $(rowEl).find(SELECTORS.ATTRIBUTES_VALUE).text();\n\n        return { label, value };\n    });\n\n    return attributeRows;\n};\n```\n\n----------------------------------------\n\nTITLE: Terminating Crawlee Process\nDESCRIPTION: Keyboard shortcut to stop the crawler execution\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nCTRL+C\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-release Versions of Crawlee\nDESCRIPTION: Command to install a beta build of Crawlee for testing new features or bug fixes before they are officially released.\nSOURCE: https://github.com/apify/crawlee/blob/master/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee@3.12.3-beta.13\n```\n\n----------------------------------------\n\nTITLE: Using ImpitHttpClient with CheerioCrawler in Crawlee\nDESCRIPTION: This snippet demonstrates how to integrate the ImpitHttpClient with a CheerioCrawler. It shows configuration of browser type and HTTP options, and includes a basic request handler that extracts and logs the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/impit-client/README.md#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler, Dictionary } from '@crawlee/cheerio';\nimport { ImpitHttpClient, Browser } from '@crawlee/impit-client';\n\nconst crawler = new CheerioCrawler({\n    httpClient: new ImpitHttpClient({\n        browser: Browser.Firefox,\n        http3: true,\n        ignoreTlsErrors: true,\n    }),\n    async requestHandler({ $, request }) {\n        // Extract the title of the page.\n        const title = $('title').text();\n        console.log(`Title of the page ${request.url}: ${title}`);\n    },\n});\n\ncrawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee manually for CheerioCrawler\nDESCRIPTION: Command to manually install Crawlee for use with CheerioCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager\nDESCRIPTION: Command to install UV, a fast and modern package manager written in Rust, which will be used for Python package management in this project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Playwright WebKit Docker Configuration\nDESCRIPTION: Docker configuration for running Playwright with WebKit browser support.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/docker_images.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-webkit:16\n```\n\n----------------------------------------\n\nTITLE: Configuring Crunchbase API Crawler with Crawlee for Python\nDESCRIPTION: This snippet configures the crawler for working with the Crunchbase API. It sets up concurrency settings, HTTP client with custom headers, and initializes the HttpCrawler with specific parameters.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom crawlee.crawlers import HttpCrawler\nfrom crawlee.http_clients import HttpxHttpClient\nfrom crawlee import ConcurrencySettings, HttpHeaders\n\nfrom .routes import router\n\nCRUNCHBASE_TOKEN = os.getenv('CRUNCHBASE_TOKEN', '')\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n\n    concurrency_settings = ConcurrencySettings(max_tasks_per_minute=60)\n\n    http_client = HttpxHttpClient(\n        headers=HttpHeaders({'accept-encoding': 'gzip, deflate, br, zstd', 'X-cb-user-key': CRUNCHBASE_TOKEN})\n    )\n    crawler = HttpCrawler(\n        request_handler=router,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=30,\n    )\n\n    await crawler.run(\n        ['https://api.crunchbase.com/api/v4/autocompletes?query=apify&collection_ids=organizations&limit=25']\n    )\n\n    await crawler.export_data_json('crunchbase_data.json')\n```\n\n----------------------------------------\n\nTITLE: Configuring maxRequestsPerMinute in Crawlee\nDESCRIPTION: Example showing how to limit the rate of requests per minute in a CheerioCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/scaling_crawlers.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestsPerMinute: 120, // 2 requests per second\n    // ... other options\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Using Crawlee CLI to quickly create a new crawler project with all necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Checking Product Stock Availability with Playwright\nDESCRIPTION: Determines if a product is in stock by checking for the presence of a specific element. The code looks for a span with class 'product-form__inventory' containing the text 'In stock'.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Puppeteer Crawler\nDESCRIPTION: Example of creating a Puppeteer crawler that uses the Puppeteer library to control a headless browser, visit URLs and extract data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url} is: ${title}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all necessary scripts and dependencies\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing ts-node\nDESCRIPTION: Command to install ts-node for development environment execution.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev ts-node\n```\n\n----------------------------------------\n\nTITLE: Installing PlaywrightCrawler Manually\nDESCRIPTION: Command to manually install Crawlee and Playwright with npm, as Playwright is not bundled with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n```\n\n----------------------------------------\n\nTITLE: Version Release Reference\nDESCRIPTION: Git comparison reference for tracking changes between versions.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.0.4](https://github.com/apify/crawlee/compare/v3.0.3...v3.0.4) (2022-08-22)\n```\n\n----------------------------------------\n\nTITLE: Implementing Bluesky API Session Management in Python\nDESCRIPTION: Class implementation for Bluesky API interaction, including methods for creating and deleting sessions using authentication credentials stored in environment variables.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nimport os\nimport traceback\n\nimport httpx\nfrom yarl import URL\n\nfrom crawlee import ConcurrencySettings, Request\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import HttpCrawler, HttpCrawlingContext\nfrom crawlee.http_clients import HttpxHttpClient\nfrom crawlee.storages import Dataset\n\n# Environment variables for authentication\n# BLUESKY_APP_PASSWORD: App-specific password generated from Bluesky settings\n# BLUESKY_IDENTIFIER: Your Bluesky handle (e.g., username.bsky.social)\nBLUESKY_APP_PASSWORD = os.getenv('BLUESKY_APP_PASSWORD')\nBLUESKY_IDENTIFIER = os.getenv('BLUESKY_IDENTIFIER')\n\n\nclass BlueskyApiScraper:\n    \"\"\"A scraper class for extracting data from Bluesky social network using their official API.\n\n    This scraper manages authentication, concurrent requests, and data collection for both\n    posts and user profiles. It uses separate datasets for storing post and user information.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._crawler: HttpCrawler | None = None\n\n        self._users: Dataset | None = None\n        self._posts: Dataset | None = None\n\n        # Variables for storing session data\n        self._service_endpoint: str | None = None\n        self._user_did: str | None = None\n        self._access_token: str | None = None\n        self._refresh_token: str | None = None\n        self._handle: str | None = None\n\n    def create_session(self) -> None:\n        \"\"\"Create credentials for the session.\"\"\"\n        url = 'https://bsky.social/xrpc/com.atproto.server.createSession'\n        headers = {\n            'Content-Type': 'application/json',\n        }\n        data = {'identifier': BLUESKY_IDENTIFIER, 'password': BLUESKY_APP_PASSWORD}\n\n        response = httpx.post(url, headers=headers, json=data)\n        response.raise_for_status()\n\n        data = response.json()\n\n        self._service_endpoint = data['didDoc']['service'][0]['serviceEndpoint']\n        self._user_did = data['didDoc']['id']\n        self._access_token = data['accessJwt']\n        self._refresh_token = data['refreshJwt']\n        self._handle = data['handle']\n\n    def delete_session(self) -> None:\n        \"\"\"Delete the current session.\"\"\"\n        url = f'{self._service_endpoint}/xrpc/com.atproto.server.deleteSession'\n        headers = {'Content-Type': 'application/json', 'authorization': f'Bearer {self._refresh_token}'}\n\n        response = httpx.post(url, headers=headers)\n        response.raise_for_status()\n```\n\n----------------------------------------\n\nTITLE: Google Maps Scraper Class Implementation in Python\nDESCRIPTION: Definition of the GoogleMapsScraper class with initialization method and crawler setup, including a set to track processed business names and prevent duplicate entries.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass GoogleMapsScraper:\n    def __init__(self, headless: bool = True, timeout_minutes: int = 5):\n        self.crawler = PlaywrightCrawler(\n            headless=headless,\n            request_handler_timeout=timedelta(minutes=timeout_minutes),\n        )\n        self.processed_names: Set[str] = set()\n\n    async def setup_crawler(self) -> None:\n        self.crawler.router.default_handler(self._scrape_listings)\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration in Package.json\nDESCRIPTION: JSON configuration for the package.json to add a development script that uses ts-node-esm to run TypeScript code directly with transpile-only mode.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser Fingerprints\nDESCRIPTION: TypeScript code example showing how to disable browser fingerprints in PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Explicit Request Queue Usage with Crawler in Crawlee\nDESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. This approach allows more control over queue management.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/request_storage.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RequestQueue, PuppeteerCrawler } from 'crawlee';\n\nconst requestQueue = await RequestQueue.open();\nawait requestQueue.addRequest({ url: 'https://crawlee.dev' });\n\nconst crawler = new PuppeteerCrawler({\n    requestQueue,\n    async requestHandler({ page, enqueueLinks }) {\n        // Extract data from the page\n        // ...\n\n        // Add new requests to the queue\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Updating package.json for GCP Functions\nDESCRIPTION: Sets the main entry point in package.json to point to the src/main.js file, which is required for GCP Functions to locate the handler function.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    // highlight-next-line\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TSConfig Dependencies\nDESCRIPTION: Command to install Apify's TypeScript configuration package.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Using actor-node-playwright-chrome Docker Image\nDESCRIPTION: Shows how to use the Apify Docker image that includes Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright-chrome:16\n```\n\n----------------------------------------\n\nTITLE: Enhanced GraphQL Query with Multilingual Support\nDESCRIPTION: Extended version of the GraphQL query that fetches content in multiple languages and includes additional fields for mixed blocks containing post content, eliminating the need for separate page visits.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/11-10-web-scraping-tips/index.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"https://restoran.ua/graphql\"\n\ndata = {\n    \"operationName\": \"Posts_PostsForView\",\n    \"variables\": {\"sort\": {\"sortBy\": [\"startAt_DESC\"]}},\n    \"query\": \"\"\"query Posts_PostsForView(\n    $where: PostForViewWhereInput,\n    $sort: PostForViewSortInput,\n    $pagination: PaginationInput,\n    $search: String,\n    $token: String,\n    $coordinates_slice: SliceInput)\n    {\n        PostsForView(\n                where: $where\n                sort: $sort\n                pagination: $pagination\n                search: $search\n                token: $token\n                ) {\n                        id\n                        uk_title: ukTitle\n                        en_title: enTitle\n                        summary: ukSummary\n                        slug\n                        startAt\n                        endAt\n                        newsFeed\n                        events\n                        journal\n                        toProfessionals\n                        photoHeader {\n                            address: mobile\n                            __typename\n                            }\n                        mixedBlocks {\n                            index\n                            en_text: enText\n                            uk_text: ukText\n                            __typename\n                            }\n                        coordinates(slice: $coordinates_slice) {\n                            lng\n                            lat\n                            __typename\n                            }\n                        __typename\n                    }\n    }\"\"\"\n}\n\nresponse = requests.post(url, json=data)\nprint(response.json())\n\n```\n\n----------------------------------------\n\nTITLE: Crawling All Links with CheerioCrawler in Crawlee\nDESCRIPTION: This code snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'All' enqueue strategy to process any URLs encountered during the crawl.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/examples/crawl_relative_links.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, EnqueueStrategy } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks, log }) {\n        log.info('Enqueueing all found links.');\n        await enqueueLinks({\n            strategy: EnqueueStrategy.All,\n            selector: 'a[href]',\n        });\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Use the Crawlee CLI to quickly set up a new crawler project with all necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: HTML Link Example\nDESCRIPTION: Example of an HTML anchor tag structure that the crawler would process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/introduction/03-adding-urls.mdx#2025-04-11_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://crawlee.dev/js/docs/introduction\">This is a link to Crawlee introduction</a>\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with Poetry\nDESCRIPTION: Command to install the required dependencies for the LinkedIn scraper project using Poetry package manager after project initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Checking Node.js Version\nDESCRIPTION: Command to verify the installed version of Node.js\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development environment using ts-node-esm.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting Crawlee Release 2.0.2 in Markdown\nDESCRIPTION: This snippet describes the changes in Crawlee version 2.0.2, focusing on fixing serialization issues in CheerioCrawler caused by parser conflicts in recent versions of cheerio.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_28\n\nLANGUAGE: markdown\nCODE:\n```\n## [2.0.2](https://github.com/apify/crawlee/compare/v2.0.1...v2.0.2) (2021-08-12)\n\n* Fix serialization issues in `CheerioCrawler` caused by parser conflicts in recent versions of `cheerio`.\n```\n\n----------------------------------------\n\nTITLE: Main Function to Launch Google Maps Crawler in Python\nDESCRIPTION: Main function that prepares the search URL, configures the crawler with the request handler, and starts the scraping process for Google Maps.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nasync def main():\n    # Prepare the search URL\n    search_query = \"hotels in bengaluru\"\n    start_url = f\"https://www.google.com/maps/search/{search_query.replace(' ', '+')}\"\n\n    # Tell the crawler how to handle each page it visits\n    crawler.router.default_handler(scrape_google_maps)\n\n    # Start the scraping process\n    await crawler.run([start_url])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output from Google Maps Scraper\nDESCRIPTION: Example of the structured JSON output from the Google Maps scraper showing hotel data including name, rating, reviews, price, link, and amenities.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-13-scrape-google-maps-using-python/index.md#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"GRAND KALINGA HOTEL\",\n    \"rating\": \"4.2\",\n    \"reviews\": \"1,171\",\n    \"price\": \"1,760\",\n    \"link\": \"https://www.google.com/maps/place/GRAND+KALINGA+HOTEL/data=!4m10!3m9!1s0x3bae160e0ce07789:0xb15bf736f4238e6a!5m2!4m1!1i2!8m2!3d12.9762259!4d77.5786043!16s%2Fg%2F11sp32pz28!19sChIJiXfgDA4WrjsRao4j9Db3W7E?authuser=0&hl=en&rclk=1\",\n    \"amenities\": [\n        \"Pool available\",\n        \"Free breakfast available\",\n        \"Free Wi-Fi available\",\n        \"Free parking available\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Project File Structure\nDESCRIPTION: Command to create the standard file structure for a Crawlee for Python project.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir crunchbase-crawlee && touch crunchbase-crawlee/{__init__.py,__main__.py,main.py,routes.py}\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json for GCP Cloud Functions\nDESCRIPTION: This snippet shows how to set the \"main\" field in package.json to point to the entry file for the GCP Cloud Function.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Using Pre-release Docker Image\nDESCRIPTION: Examples of how to reference pre-release (beta) versions of Docker images, both with and without a specific library version.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/guides/docker_images.mdx#2025-04-11_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Without library version.\nFROM apify/actor-node:20-beta\n```\n\nLANGUAGE: dockerfile\nCODE:\n```\n# With library version.\nFROM apify/actor-node-playwright-chrome:20-1.10.0-beta\n```\n\n----------------------------------------\n\nTITLE: Sending Error Response in SuperScraper with TypeScript\nDESCRIPTION: This function sends an error response back to the user if an error occurs during scraping. It retrieves the corresponding response object, sends the error message, and removes the response object from the map.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport const sendErrorResponseById = (responseId: string, result: string, statusCode: number = 500) => {\n    const res = responses.get(responseId);\n    if (!res) {\n        log.info(`Response for request ${responseId} not found`);\n        return;\n    }\n\n    res.writeHead(statusCode, { 'Content-Type': 'application/json' });\n    res.end(result);\n    responses.delete(responseId);\n};\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest with Custom Proxy in TypeScript\nDESCRIPTION: This example shows how to use sendRequest with a custom proxy URL. It demonstrates setting the proxyUrl option when making a request.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.9/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Package.json Browser Version Configuration\nDESCRIPTION: Package.json configuration showing how to properly specify browser automation library versions when using Docker images.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\",\n        \"playwright\": \"*\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee for Python using pipx\nDESCRIPTION: Command to install Crawlee for Python and create a new crawler project named nike-crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx run crawlee create nike-crawler\n```\n\n----------------------------------------\n\nTITLE: Creating a New Apify Actor Project\nDESCRIPTION: Shows how to create a new Apify actor project using the Apify CLI. This sets up a boilerplate for a new actor that can be run locally or deployed to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\n----------------------------------------\n\nTITLE: Installing Apify TypeScript Config\nDESCRIPTION: Command to install Apify's shared TypeScript configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @apify/tsconfig\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer Stealth Dependencies\nDESCRIPTION: Command to install required npm packages puppeteer-extra and puppeteer-extra-plugin-stealth for Puppeteer implementation\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/crawler-plugins/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install puppeteer-extra puppeteer-extra-plugin-stealth\n```\n\n----------------------------------------\n\nTITLE: Installing Scrapy Rotating Proxies Package with pip\nDESCRIPTION: Command to install the scrapy-rotating-proxies package which provides proxy rotation functionality for Scrapy projects.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install scrapy-rotating-proxies\n```\n\n----------------------------------------\n\nTITLE: Complete Google Search Scraper Implementation\nDESCRIPTION: Final implementation of the Google Search scraper with multiple query support and ranking tracking\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\nfrom crawlee import Request, ConcurrencySettings, HttpHeaders\n\nfrom .routes import router\n\nQUERIES = [\"Apify\", \"Crawlee\"]\n\nCRAWL_DEPTH = 2\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\n\n    http_client = CurlImpersonateHttpClient(impersonate=\"chrome124\",\n                                            headers=HttpHeaders({\"referer\": \"https://www.google.com/\",\n                                                     \"accept-language\": \"en\",\n                                                     \"accept-encoding\": \"gzip, deflate, br, zstd\",\n                                                     \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n                                            }))\n    crawler = BeautifulSoupCrawler(\n        request_handler=router,\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=100,\n        max_crawl_depth=CRAWL_DEPTH\n    )\n\n    requests_lists = [Request.from_url(f\"https://www.google.com/search?q={query}\", user_data = {\"query\": query}) for query in QUERIES]\n\n    await crawler.run(requests_lists)\n\n    await crawler.export_data_csv(\"google_ranked.csv\")\n```\n\n----------------------------------------\n\nTITLE: Development Script Configuration\nDESCRIPTION: Package.json configuration for development runtime using ts-node.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"start:dev\": \"ts-node-esm -T src/main.ts\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Demonstrates how to turn off browser fingerprint functionality in PuppeteerCrawler configuration.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        useFingerprints: false\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Dynamically Configuring Browser Launch Options with Hooks\nDESCRIPTION: Example demonstrating how to use pre-launch hooks to dynamically modify browser launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/README.md#2025-04-11_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst browserPool = new BrowserPool({\n    preLaunchHooks: [(pageId, launchContext) => {\n        if (pageId === 'headful') {\n            launchContext.launchOptions.headless = false;\n        }\n    }]\n});\n```\n\n----------------------------------------\n\nTITLE: Package.json Build Configuration\nDESCRIPTION: Basic package.json configuration for TypeScript build setup and main entry point specification.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy in PlaywrightCrawler\nDESCRIPTION: Variable reference InspectionPlaywrightSource showing proxy inspection in PlaywrightCrawler\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/proxy_management.mdx#2025-04-11_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{InspectionPlaywrightSource}\n```\n\n----------------------------------------\n\nTITLE: Creating and Initializing PlaywrightCrawler in SuperScraper with TypeScript\nDESCRIPTION: This function creates a new PlaywrightCrawler instance with specified options. It uses an in-memory queue for better performance and isolation. The crawler is configured with proxy settings and other options before being started and added to the crawlers map.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-05-superscraper/index.md#2025-04-11_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport const createAndStartCrawler = async (crawlerOptions: CrawlerOptions = DEFAULT_CRAWLER_OPTIONS) => {\n    const client = new MemoryStorage({ persistStorage: false });\n    const queue = await RequestQueue.open(undefined, { storageClient: client });\n\n    const proxyConfig = await Actor.createProxyConfiguration(crawlerOptions.proxyConfigurationOptions);\n\n    const crawler = new PlaywrightCrawler({\n        keepAlive: true,\n        proxyConfiguration: proxyConfig,\n        maxRequestRetries: 4,\n        requestQueue: queue,\n    });\n};\n```\n\n----------------------------------------\n\nTITLE: Capturing Page Screenshots with Direct Puppeteer Method\nDESCRIPTION: This code demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method directly. It launches a browser, navigates to apify.com, captures a screenshot, and saves it to a key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/examples/puppeteer_capture_screenshot.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\nimport { launchPuppeteer } from 'crawlee/puppeteer';\n\nconst url = 'https://apify.com';\n\n// Launch a browser\nconst browser = await launchPuppeteer();\n\n// Open a new tab\nconst page = await browser.newPage();\n\n// Navigate to the URL\nawait page.goto(url);\n\n// Capture the screenshot\nconst screenshot = await page.screenshot();\n\n// Save the screenshot to the default key-value store\nawait KeyValueStore.setValue('my-screenshot', screenshot, { contentType: 'image/png' });\n\n// Close browser\nawait browser.close();\n\nconsole.log('Screenshot saved!');\n```\n\n----------------------------------------\n\nTITLE: Full Playwright Docker Image Configuration\nDESCRIPTION: Dockerfile configuration for using Apify's Docker image with all Playwright browsers pre-installed (Chromium, Chrome, Firefox, WebKit).\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/docker_images.mdx#2025-04-11_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:20\n```\n\n----------------------------------------\n\nTITLE: Exporting Scraped Data to CSV\nDESCRIPTION: Code to export the collected shoe data to a CSV file after the crawler completes its run.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    await crawler.export_data('shoes.csv')\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK Dependency\nDESCRIPTION: Command to install the Apify SDK package which enables integration with Apify Platform cloud services.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Complete Package.json Configuration\nDESCRIPTION: Full package.json configuration including all necessary scripts and dependencies for TypeScript project setup.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/typescript_project.mdx#2025-04-11_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"type\": \"module\",\n    \"main\": \"dist/main.js\",\n    \"dependencies\": {\n        \"crawlee\": \"3.0.0\"\n    },\n    \"devDependencies\": {\n        \"@apify/tsconfig\": \"^0.1.0\",\n        \"@types/node\": \"^18.14.0\",\n        \"ts-node\": \"^10.8.0\",\n        \"typescript\": \"^4.7.4\"\n    },\n    \"scripts\": {\n        \"start\": \"npm run start:dev\",\n        \"start:prod\": \"node dist/main.js\",\n        \"start:dev\": \"ts-node-esm -T src/main.ts\",\n        \"build\": \"tsc\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Next.js Manifest File Example\nDESCRIPTION: Example of a Next.js build manifest file URL that contains routing information for the website. This file helps identify available routes in the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.accommodationforstudents.com/_next/static/B5yLvSqNOvFysuIu10hQ5/_buildManifest.js\n```\n\n----------------------------------------\n\nTITLE: Creating New Crawlee Project\nDESCRIPTION: Commands to create and setup a new Crawlee project using CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/12-02-scrape-google-search/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npipx run crawlee create crawlee-google-search\ncd crawlee-google-search\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints in PuppeteerCrawler\nDESCRIPTION: Example showing how to completely disable browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/avoid_blocking.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    browserPoolOptions: {\n        // Choose from chrome, firefox or safari\n        browserType: 'chrome',\n        // Disable fingerprints\n        useFingerprints: false,\n    },\n    async requestHandler({ page }) {\n        // ... page handling\n    },\n});\n\nawait crawler.run(['https://www.example.com/']);\n```\n\n----------------------------------------\n\nTITLE: GitHub Issue Reference\nDESCRIPTION: Reference to GitHub issue numbers for tracking feature requests and bug reports.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n([#1553](https://github.com/apify/crawlee/issues/1553))\n```\n\n----------------------------------------\n\nTITLE: Querying Elements in Chrome DevTools Console\nDESCRIPTION: A JavaScript command to select all elements with the 'collection-block-item' class using document.querySelectorAll() in the browser console. This helps verify that the selector only targets the intended collection card elements on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/04-real-world-project.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.collection-block-item');\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Stage Docker Build for Crawlee Actor with Playwright Chrome\nDESCRIPTION: This Dockerfile creates a containerized environment for running a Crawlee-based web scraping actor. It uses a multi-stage build process - first building the application, then creating a production image with minimal dependencies. The image includes Playwright with Chrome support for browser automation.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/guides/docker_browser_ts.txt#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Specify the base Docker image. You can read more about\n# the available images at https://crawlee.dev/js/docs/guides/docker-images\n# You can also use any other image from Docker Hub.\nFROM apify/actor-node-playwright-chrome:20 AS builder\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install all dependencies. Don't audit to speed up the installation.\nRUN npm install --include=dev --audit=false\n\n# Next, copy the source files using the user set\n# in the base image.\nCOPY --chown=myuser . ./\n\n# Install all dependencies and build the project.\n# Don't audit to speed up the installation.\nRUN npm run build\n\n# Create final image\nFROM apify/actor-node-playwright-chrome:20\n\n# Copy only built JS files from builder image\nCOPY --from=builder --chown=myuser /home/myuser/dist ./dist\n\n# Copy just package.json and package-lock.json\n# to speed up the build using Docker layer cache.\nCOPY --chown=myuser package*.json ./\n\n# Install NPM packages, skip optional and development dependencies to\n# keep the image small. Avoid logging too much and print the dependency\n# tree for debugging\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after NPM install, quick build will be really fast\n# for most source file changes.\nCOPY --chown=myuser . ./\n\n\n# Run the image. If you know you won't need headful browsers,\n# you can remove the XVFB start script for a micro perf gain.\nCMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n```\n\n----------------------------------------\n\nTITLE: Migrating from launchPuppeteerOptions to launchContext in Crawlee\nDESCRIPTION: Example demonstrating the transition from the confusing `launchPuppeteerOptions` structure to the more explicit `launchContext` object, which clearly separates Apify options from Puppeteer launch options.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v1.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst launchPuppeteerOptions = {\n    useChrome: true, // Apify option\n    headless: false, // Puppeteer option\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchContext: {\n        useChrome: true, // Apify option\n        launchOptions: {\n            headless: false // Puppeteer option\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Basic Link Enqueuing with Crawlee\nDESCRIPTION: Simple example of using the enqueueLinks() function without parameters to crawl links.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/05-crawling.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueLinks();\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK as a Project Dependency\nDESCRIPTION: Command to install the Apify SDK package, which provides tools for integrating Crawlee with the Apify Platform services and infrastructure.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration\nDESCRIPTION: Multi-stage Dockerfile for building and running TypeScript project in production.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/guides/typescript_project.mdx#2025-04-11_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20 AS builder\n\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\nFROM apify/actor-node:20\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/dist ./dist\n\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional\n\nCMD npm run start:prod\n```\n\n----------------------------------------\n\nTITLE: Exporting Custom Crawler Components\nDESCRIPTION: Initial Python module file that exports the CustomCrawler and CustomContext classes for use in other modules.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# init.py\n\nfrom .custom_crawler import CustomCrawler\nfrom .types import CustomContext\n\n__all__ = ['CustomCrawler', 'CustomContext']\n```\n\n----------------------------------------\n\nTITLE: Creating a New Python Project with Crawlee\nDESCRIPTION: Commands to initialize a new Python project and install Crawlee, a web scraping library for Python.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv init bluesky-crawlee --package\ncd bluesky-crawlee\nuv add crawlee\n```\n\n----------------------------------------\n\nTITLE: Importing ApiLink Component in Markdown\nDESCRIPTION: This code snippet imports a custom React component called ApiLink, likely used for creating API documentation links within the markdown content.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: Terminating Crawlee\nDESCRIPTION: Command to stop the crawler execution\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nCTRL+C\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json Build Script and Main Entry Point\nDESCRIPTION: JSON configuration for the package.json file to define the build script using TypeScript compiler and specify the main entry point pointing to the built code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/typescript_project.mdx#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"build\": \"tsc\"\n    },\n    \"main\": \"dist/main.js\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding tslib Dependency in Crawlee\nDESCRIPTION: Declares a missing dependency on tslib in the Crawlee package. This resolves issues related to missing TypeScript helper functions.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\ndeclare missing dependency on `tslib`\n```\n\n----------------------------------------\n\nTITLE: Accessing Local and Remote Datasets with Actor in Crawlee\nDESCRIPTION: Demonstrates how to access both local datasets and remote datasets on the Apify platform using the Actor class. The example shows how to use the forceCloud option to specifically target remote storage when both local and platform storage are configured.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/deployment/apify_platform.mdx#2025-04-11_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { Dataset } from 'crawlee';\n\n// or Dataset.open('my-local-data')\nconst localDataset = await Actor.openDataset('my-local-data');\n// but here we need the `Actor` class\nconst remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK and CLI for Crawlee Project Deployment\nDESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deploying Crawlee projects to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/09-deployment.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee for Headful Browser Mode\nDESCRIPTION: TypeScript configuration option to run the browser in visible (headful) mode instead of headless, useful for development and debugging.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/introduction/01-setting-up.mdx#2025-04-11_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Uncomment this option to see the browser window.\nheadless: false\n```\n\n----------------------------------------\n\nTITLE: Running the Price Tracking Actor Locally\nDESCRIPTION: Command to run the price tracking actor locally with a clean environment. The --purge flag ensures a fresh run by removing existing storage.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\napify run --purge\n```\n\n----------------------------------------\n\nTITLE: Running the Nike Crawler\nDESCRIPTION: Command to execute the Nike crawler Python module using Poetry.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python -m nike-crawler\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Configuration for Apify Actor\nDESCRIPTION: Docker configuration for the Apify Actor. This Dockerfile uses the official Apify Python image, sets up the environment with UV for dependency management, and configures the entry point for the application.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_14\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-python:3.13\n\nENV PATH='/app/.venv/bin:$PATH'\n\nWORKDIR /app\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\n\nCOPY pyproject.toml uv.lock ./\n\nRUN uv sync --frozen --no-install-project --no-editable -q --no-dev\n\nCOPY . .\n\nRUN uv sync --frozen --no-editable -q --no-dev\n\nCMD [\"bluesky-crawlee\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Python Using UV\nDESCRIPTION: Command to install a standalone version of Python 3.13 using the UV package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv install python 3.13\n```\n\n----------------------------------------\n\nTITLE: Using sendRequest() in Crawlee v3\nDESCRIPTION: Shows how to use the new context.sendRequest() helper to process requests through got-scraping, replacing the removed requestAsBrowser functionality.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        // we can use the options parameter to override gotScraping options\n        const res = await sendRequest({ responseType: 'json' });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Version in Docker Image\nDESCRIPTION: Demonstrates how to use a specific Node.js version (16) in the Apify actor-node Docker image.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/docker_images.mdx#2025-04-11_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Starting Crawlee Project\nDESCRIPTION: Command to navigate to the created crawler directory and start the crawler.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Apify API Token in Terminal\nDESCRIPTION: Command to log in to your Apify account from the terminal using your API token. This authenticates your local development environment to allow API calls to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Installing tls-client for TLS Fingerprinting Bypass in Python\nDESCRIPTION: This snippet shows how to install the tls-client library using pip. tls-client is a Python wrapper around a Golang library that provides an API similar to requests for bypassing TLS fingerprinting.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/08-20-problems-in-web-scraping/index.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install tls-client\n```\n\n----------------------------------------\n\nTITLE: Version Release Headers in Markdown\nDESCRIPTION: Version headers showing release changes from 3.13.2 down to 3.2.0, using markdown formatting for changelog entries.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/cheerio-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.2](https://github.com/apify/crawlee/compare/v3.13.1...v3.13.2) (2025-04-08)\n\n### Features\n\n* add `onSkippedRequest` option ([#2916](https://github.com/apify/crawlee/issues/2916)) ([764f992](https://github.com/apify/crawlee/commit/764f99203627b6a44d2ee90d623b8b0e6ecbffb5)), closes [#2910](https://github.com/apify/crawlee/issues/2910)\n```\n\n----------------------------------------\n\nTITLE: Running Crunchbase Crawler with Poetry\nDESCRIPTION: This bash command demonstrates how to execute the Crunchbase crawler using Poetry, a Python dependency management and packaging tool.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python -m crunchbase-crawlee\n```\n\n----------------------------------------\n\nTITLE: Importing ApiLink Component in JSX\nDESCRIPTION: Import statement for the ApiLink React component from the site's components directory, used for creating API documentation links in the introduction page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport ApiLink from '@site/src/components/ApiLink';\n```\n\n----------------------------------------\n\nTITLE: GitHub Dependency Fix for CSV-Stringify\nDESCRIPTION: Fix for declaring missing dependencies on 'csv-stringify' and 'fs-extra' packages in ESLint configuration\nSOURCE: https://github.com/apify/crawlee/blob/master/CHANGELOG.md#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n/github.com/redabacha/crawlee/blob/2f05ed22b203f688095300400bb0e6d03a03283c/.eslintrc.json#L50\n```\n\n----------------------------------------\n\nTITLE: Creating New Crawlee Project\nDESCRIPTION: Command to create a new Crawlee project using the Crawlee CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Bluesky Authentication\nDESCRIPTION: Commands to set environment variables for storing Bluesky credentials securely, including the app password and user identifier.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport BLUESKY_APP_PASSWORD=your_app_password\nexport BLUESKY_IDENTIFIER=your_identifier\n```\n\n----------------------------------------\n\nTITLE: Next.js Property Details API URL Pattern\nDESCRIPTION: The URL pattern for accessing individual property listing data from Next.js API, returning JSON data instead of HTML. Includes placeholders for build ID and property ID.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/09-12-finding-students-accommodations/index.md#2025-04-11_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.accommodationforstudents.com/_next/data/[build_id]/property/[id].json\n```\n\n----------------------------------------\n\nTITLE: Making RequestQueue v2 the Default Queue in JavaScript\nDESCRIPTION: Update to make RequestQueue v2 the default queue, with more information available on the Apify blog.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nRequestQueue\n```\n\n----------------------------------------\n\nTITLE: Change Log Entry for Crawlee Puppeteer Package\nDESCRIPTION: A conventionally formatted changelog entry showing version bumps and changes for @crawlee/puppeteer package from version 3.4.0 to 3.13.2.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/puppeteer-crawler/CHANGELOG.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [3.13.2](https://github.com/apify/crawlee/compare/v3.13.1...v3.13.2) (2025-04-08)\n\n**Note:** Version bump only for package @crawlee/puppeteer\n\n## [3.13.1](https://github.com/apify/crawlee/compare/v3.13.0...v3.13.1) (2025-04-07)\n\n**Note:** Version bump only for package @crawlee/puppeteer\n```\n\n----------------------------------------\n\nTITLE: Configuring Robots.txt for Crawlee Documentation\nDESCRIPTION: This robots.txt file sets the user-agent to allow all crawlers and specifies two sitemap locations for the Crawlee documentation website. It includes sitemaps for both the main site and the Python-specific documentation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/static/robots.txt#2025-04-11_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nSitemap: https://crawlee.dev/sitemap.xml\nSitemap: https://crawlee.dev/python/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Starting the Crawler Project\nDESCRIPTION: Command to navigate to the crawler directory and start the crawler after installation.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/quick-start/index.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Verifying Node.js Installation\nDESCRIPTION: Command to check the installed version of Node.js\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/01-setting-up.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Importing Dataset Module in Crawlee (TypeScript)\nDESCRIPTION: This snippet shows how to import the Dataset module from Crawlee, which is necessary for saving scraped data.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/introduction/07-saving-data.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Documentation Page\nDESCRIPTION: Imports React components used for rendering the documentation page, including a runnable code block component and API link component.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/puppeteer_crawler.mdx#2025-04-11_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';\nimport ApiLink from '@site/src/components/ApiLink';\nimport CrawlSource from '!!raw-loader!roa-loader!./puppeteer_crawler.ts';\n```\n\n----------------------------------------\n\nTITLE: Sample Crawlee Results\nDESCRIPTION: Example JSON output showing the structure of crawled data stored in the datasets directory\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/quick-start/index.mdx#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://crawlee.dev/\",\n    \"title\": \"Crawlee  The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee via npm\nDESCRIPTION: Command to install the Crawlee library and its dependencies using npm package manager.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/motivation.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Verifying NPM Installation in Bash\nDESCRIPTION: Command to check the installed version of NPM, which is required for package management with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/01-setting-up.mdx#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Installing Cheerio Crawler Package with NPM\nDESCRIPTION: Command to install only the Cheerio crawler package (@crawlee/cheerio) for projects that only require Cheerio support.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @crawlee/cheerio\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawlee Using the Configuration Class\nDESCRIPTION: Example showing how to use the Configuration class to configure Crawlee programmatically. This example sets the persistStateIntervalMillis to 10000 using the global configuration object.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/configuration.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, Configuration, sleep } from 'crawlee';\n\n// Get the global configuration\nconst config = Configuration.getGlobalConfig();\n// Set the 'persistStateIntervalMillis' option\n// of global configuration to 10 seconds\nconfig.set('persistStateIntervalMillis', 10_000);\n\n// Note, that we are not passing the configuration to the crawler\n// as it's using the global configuration\nconst crawler = new CheerioCrawler();\n\ncrawler.router.addDefaultHandler(async ({ request }) => {\n    // For the first request we wait for 5 seconds,\n    // and add the second request to the queue\n    if (request.url === 'https://www.example.com/1') {\n        await sleep(5_000);\n        await crawler.addRequests(['https://www.example.com/2'])\n    }\n    // For the second request we wait for 10 seconds,\n    // and abort the run\n    if (request.url === 'https://www.example.com/2') {\n        await sleep(10_000);\n        process.exit(0);\n    }\n});\n\nawait crawler.run(['https://www.example.com/1']);\n```\n\n----------------------------------------\n\nTITLE: Creating New Crawlee Project\nDESCRIPTION: Command to create a new Crawlee project using the CLI\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/introduction/01-setting-up.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\n----------------------------------------\n\nTITLE: Reduce Method Result Example in JavaScript\nDESCRIPTION: The expected output of the reduce method example, showing the total count of headers across all pages in the dataset.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.13/examples/map_and_reduce.mdx#2025-04-11_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n23\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory\nDESCRIPTION: Command to create and navigate to the project directory for the Crunchbase scraper.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir crunchbase-crawlee && cd crunchbase-crawlee\n```\n\n----------------------------------------\n\nTITLE: Documenting Crawlee Release 2.0.1 in Markdown\nDESCRIPTION: This snippet outlines the changes in Crawlee version 2.0.1, mentioning the use of got-scraping 2.0.1 until full compatibility is achieved.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_29\n\nLANGUAGE: markdown\nCODE:\n```\n## [2.0.1](https://github.com/apify/crawlee/compare/v2.0.0...v2.0.1) (2021-08-06)\n\n* Use `got-scraping` 2.0.1 until fully compatible.\n```\n\n----------------------------------------\n\nTITLE: CheerioCrawler Log Output for Crawlee Website Crawl\nDESCRIPTION: Log output showing a CheerioCrawler crawling the Crawlee documentation website. The log includes the crawler startup message and the titles of four pages that were successfully crawled, including the main page and documentation sections.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/quick_start_cheerio.txt#2025-04-11_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee  Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n----------------------------------------\n\nTITLE: Initializing Poetry Project\nDESCRIPTION: Command to initialize a new Poetry project for managing dependencies.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/01-03-scrape-crunchbase/index.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry init\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Type Declarations\nDESCRIPTION: Command to install type declarations for Node.js as a development dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.3/guides/typescript_project.mdx#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install --save-dev @types/node\n```\n\n----------------------------------------\n\nTITLE: Complete AWS Lambda Handler for PlaywrightCrawler\nDESCRIPTION: This JavaScript code shows the complete AWS Lambda handler function wrapping the PlaywrightCrawler implementation. It configures the crawler with AWS-compatible Chromium settings, runs it with the provided start URLs, and returns the crawled data in the Lambda response.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/deployment/aws-browsers.md#2025-04-11_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Configuration, PlaywrightCrawler } from 'crawlee';\nimport { router } from './routes.js';\nimport aws_chromium from '@sparticuz/chromium';\n\nconst startUrls = ['https://crawlee.dev'];\n\n// highlight-next-line\nexport const handler = async (event, context) => {\n    const crawler = new PlaywrightCrawler({\n        requestHandler: router,\n        launchContext: {\n            launchOptions: {\n                executablePath: await aws_chromium.executablePath(),\n                args: aws_chromium.args,\n                headless: true\n            }\n        }\n    }, new Configuration({\n        persistStorage: false,\n    }));\n\n    await crawler.run(startUrls);\n\n    // highlight-start\n    return {\n        statusCode: 200,\n        body: await crawler.getData(),\n    };\n}\n// highlight-end\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with Playwright Dependencies\nDESCRIPTION: Commands to install Crawlee with Playwright, demonstrating how to include both the Crawlee package and its browser automation dependency.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee playwright\n# or npm install @crawlee/playwright playwright\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Meta-Package with NPM\nDESCRIPTION: Command to install the Crawlee meta-package, which includes most @crawlee/* packages and all crawler classes.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Using Full Playwright Image\nDESCRIPTION: Using the complete Playwright image with all browser engines (Chromium, Chrome, Firefox, WebKit) for multi-browser testing and development.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.12/guides/docker_images.mdx#2025-04-11_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node-playwright:20\n```\n\n----------------------------------------\n\nTITLE: Playwright Dependencies Update\nDESCRIPTION: Bug fix that updates Playwright to version 1.29.2 and makes peer dependencies less strict. This was implemented in version 3.2.0.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/browser-pool/CHANGELOG.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### Bug Fixes\n\n* update playwright to 1.29.2 and make peer dep. less strict ([#1735](https://github.com/apify/crawlee/issues/1735)) ([c654fcd](https://github.com/apify/crawlee/commit/c654fcdea06fb203b7952ed97650190cc0e74394)), closes [#1723](https://github.com/apify/crawlee/issues/1723)\n```\n\n----------------------------------------\n\nTITLE: Sample Dataset Structure in JSON\nDESCRIPTION: Example JSON data representing scraped page information stored in a dataset. Each item contains a URL and a count of heading elements found on the page.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/examples/map_and_reduce.mdx#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"url\": \"https://crawlee.dev/\",\n        \"headingCount\": 11\n    },\n    {\n        \"url\": \"https://crawlee.dev/storage\",\n        \"headingCount\": 8\n    },\n    {\n        \"url\": \"https://crawlee.dev/proxy\",\n        \"headingCount\": 4\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Input File Path for Apify Actor\nDESCRIPTION: Shows the file path structure for providing input to an Apify actor through the INPUT.json file in the default key-value store.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.5/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Retry Settings in Crawlee\nDESCRIPTION: JavaScript configuration for controlling retry behavior in Crawlee using the maxRequestRetries option. This determines how many times a crawler will retry failed requests before marking them as failed.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    maxRequestRetries: 3 // Crawler will retry three times.\n    // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Using Key-Value Store in Crawlee\nDESCRIPTION: JavaScript example showing how to store data in Crawlee's Key-Value Store, which provides a simple storage mechanism for arbitrary data during the crawling process.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/04-23-scrapy-vs-crawlee/index.md#2025-04-11_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { KeyValueStore } from 'crawlee';\n//... Code to crawl the data\nawait KeyValueStore.setValue('key', { foo: 'bar' });\n```\n\n----------------------------------------\n\nTITLE: Command-line Argument Handling for Scraper Execution\nDESCRIPTION: Sets up command-line argument parsing for the LinkedIn scraper using argparse, enabling the script to accept job title, location, and output filename parameters from the command line.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2024/10-14-linkedin-job-scraper-python/index.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport argparse\n\nfrom .main import main\n\ndef get_args():\n    # ArgumentParser object to capture command-line arguments\n    parser = argparse.ArgumentParser(description=\"Crawl LinkedIn job listings\")\n\n\n    # Define the arguments\n    parser.add_argument(\"--title\", type=str, required=True, help=\"Job title\")\n    parser.add_argument(\"--location\", type=str, required=True, help=\"Job location\")\n    parser.add_argument(\"--data_name\", type=str, required=True, help=\"Name for the output CSV file\")\n\n\n    # Parse the arguments\n    return parser.parse_args()\n\nif __name__ == '__main__':\n    args = get_args()\n    # Run the main function with the parsed command-line arguments\n    asyncio.run(main(args.title, args.location, args.data_name))\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title using Playwright in JavaScript\nDESCRIPTION: This snippet shows how to use Playwright to extract the product title from a web page using a CSS selector.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Complete Price Tracker Script\nDESCRIPTION: Full Python script for the price tracker, including imports, crawler setup, and request handling.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/04-08-price-tracking/index.md#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom apify import Actor\nfrom crawler.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n\n\nasync def main() -> None:\n\n    # Enter the context of the Actor.\n    async with Actor:\n        # Create a crawler.\n        crawler = BeautifulSoupCrawler(\n            # Limit the crawl to max requests. Remove or increase it for crawling all links.\n            max_requests_per_crawl=50,\n        )\n\n        # Define a request handler, which will be called for every request.\n        @crawler.router.default_handler\n        async def request_handler(context: BeautifulSoupCrawlingContext) -> None:\n            url = context.request.url\n            Actor.log.info(f'Scraping {url}...')\n            \n            # Select the product name and price elements.\n            product_name_element = context.soup.find('div', class_='productname')\n            product_price_element = context.soup.find('span', id='product-price-395001')\n\n            # Extract the desired data.\n            data = {\n                'url': context.request.url,\n                'product_name': product_name_element.text.strip() if product_name_element else None,\n                'price': float(product_price_element['data-price-amount']) if product_price_element else None,\n            }\n\n            # Store the extracted data to the default dataset.\n            await context.push_data(data)\n\n        # Run the crawler with the starting requests.\n        await crawler.run(['https://www.centralcomputer.com/raspberry-pi-5-8gb-ram-board.html'])\n```\n\n----------------------------------------\n\nTITLE: Accessing and Logging User Input in Crawlee Actor\nDESCRIPTION: This code snippet demonstrates how to access and log user input in a Crawlee actor. It uses the Actor.getInput() method to retrieve the input and then logs it to the console.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/examples/accept_user_input.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Fetch the input of your actor\nconst input = await Actor.getInput();\nconsole.log('Input:');\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: Demonstrates extracting a product SKU from a span element with class 'product-meta__sku-number' using Playwright's locator API.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Scraping Product Title with Playwright in JavaScript\nDESCRIPTION: Shows how to use Playwright to locate and extract the product title from an HTML element.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/06-scraping.mdx#2025-04-11_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst title = await page.locator('.product-meta h1').textContent();\n```\n\n----------------------------------------\n\nTITLE: Extracting Product SKU with Playwright\nDESCRIPTION: Retrieves the product SKU from an HTML page using Playwright's locator API. The code targets a span element with the 'product-meta__sku-number' class.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/introduction/06-scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n```\n\n----------------------------------------\n\nTITLE: Basic Cheerio Crawler Implementation\nDESCRIPTION: Simple implementation of a CheerioCrawler that crawls a single URL and extracts the page title.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/introduction/03-adding-urls.mdx#2025-04-11_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for Crawlee\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects with recommended settings.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/upgrading/upgrading_v3.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using ProxyUrl with BasicCrawler in TypeScript\nDESCRIPTION: Shows how to manually provide a proxy URL when using BasicCrawler. The proxy is specified in the format protocol://username:password@hostname:port to hide the real IP address during scraping.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.8/guides/got_scraping.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BasicCrawler } from 'crawlee';\n\nconst crawler = new BasicCrawler({\n    async requestHandler({ sendRequest, log }) {\n        const res = await sendRequest({\n            proxyUrl: 'http://auto:password@proxy.apify.com:8000',\n        });\n        log.info('received body', res.body);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy the local actor code to the Apify platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/guides/apify_platform.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PlaywrightCrawler\nDESCRIPTION: Shows how to access and inspect proxy information within a PlaywrightCrawler's request handler.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/proxy_management.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com',\n        'http://proxy-2.com',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ request, page, proxyInfo }) {\n        // We can access the proxy URL through the proxyInfo object.\n        console.log(`Using proxy: ${proxyInfo?.url}`);\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Disabling Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This code demonstrates how to disable the use of browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/guides/avoid_blocking.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        useFingerprints: false,\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Browser Fingerprints with PlaywrightCrawler\nDESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler. It sets specific options for the browser, operating system, and viewport, and uses a custom User-Agent.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.10/guides/avoid_blocking.mdx#2025-04-11_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // ...\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [\"chrome\"],\n                devices: [\"desktop\"],\n                operatingSystems: [\"windows\"],\n            },\n            fingerprintInjector: async ({ page, fingerprint }) => {\n                // Set custom viewport\n                await page.setViewportSize(fingerprint.screen);\n                // Set custom user agent\n                await page.setExtraHTTPHeaders({\n                    'user-agent': fingerprint.userAgent,\n                });\n            },\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Deploying to Apify Platform\nDESCRIPTION: Command to deploy the project to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/introduction/09-deployment.mdx#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Creating Actor Directory Structure for Apify Platform\nDESCRIPTION: Command to create the directory structure and files needed for Apify Actor configuration. This sets up the basic file structure for deploying the crawler to the Apify Platform.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/blog/2025/03-20-scrape-bluesky-using-python/index.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmkdir .actor && touch .actor/{actor.json,Dockerfile,input_schema.json}\n```\n\n----------------------------------------\n\nTITLE: Inspecting Proxy Information in PlaywrightCrawler\nDESCRIPTION: Demonstrates how to access and use proxy information within PlaywrightCrawler's request handler. This helps verify which proxy is being used for each browsing session.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.4/guides/proxy_management.mdx#2025-04-11_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: [\n        'http://proxy-1.com:8000',\n        'http://proxy-2.com:8000',\n    ],\n});\n\nconst crawler = new PlaywrightCrawler({\n    proxyConfiguration,\n    async requestHandler({ proxyInfo, page }) {\n        // Prints information about the proxy used for the request\n        if (proxyInfo) {\n            console.log(proxyInfo.url);\n        }\n        \n        // Process data...\n    },\n});\n\nawait crawler.run(['https://example.com']);\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with PuppeteerCrawler\nDESCRIPTION: Command for manually installing Crawlee with Puppeteer, which needs to be installed explicitly as it's not bundled with Crawlee.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/quick-start/index.mdx#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crawlee puppeteer\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee with CLI\nDESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with all necessary dependencies and boilerplate code.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.6/quick-start/index.mdx#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx crawlee create my-crawler\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-crawler && npm start\n```\n\n----------------------------------------\n\nTITLE: Deploying an Actor to Apify Platform\nDESCRIPTION: Command to deploy your local actor code to the Apify platform using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/apify_platform.mdx#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Creating Actor Boilerplate with Apify CLI\nDESCRIPTION: Commands to create a new actor project using the Apify CLI, which sets up a basic \"Hello world\" template.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/guides/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration\nDESCRIPTION: TypeScript configuration file setup for Crawlee projects using @apify/tsconfig.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.11/upgrading/upgrading_v3.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"extends\": \"@apify/tsconfig\",\n    \"compilerOptions\": {\n        \"module\": \"ES2022\",\n        \"target\": \"ES2022\",\n        \"outDir\": \"dist\",\n        \"lib\": [\"DOM\"]\n    },\n    \"include\": [\n        \"./src/**/*\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration with Actor\nDESCRIPTION: TypeScript code example for configuring storage with Actor initialization.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.2/upgrading/upgrading_v3.md#2025-04-11_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n```\n\n----------------------------------------\n\nTITLE: Using createSessionFunction When Loading Session in JavaScript\nDESCRIPTION: Fix to use createSessionFunction when loading Session from persisted state in the core module.\nSOURCE: https://github.com/apify/crawlee/blob/master/packages/core/CHANGELOG.md#2025-04-11_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\ncreateSessionFunction\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Project Configuration\nDESCRIPTION: Command to initialize the Apify project configuration, creating necessary files and folders\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.7/introduction/09-deployment.mdx#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify init\n```\n\n----------------------------------------\n\nTITLE: Updating package.json for GCP Cloud Functions\nDESCRIPTION: Modifies the package.json file to set the main entry point to src/main.js for GCP Cloud Functions compatibility.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.1/deployment/gcp-cheerio.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-crawlee-project\",\n    \"version\": \"1.0.0\",\n    \"main\": \"src/main.js\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Browser Controller Usage Examples\nDESCRIPTION: Examples showing correct usage of BrowserController for managing browser instances and operations.\nSOURCE: https://github.com/apify/crawlee/blob/master/MIGRATIONS.md#2025-04-11_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ page, browserController }) => {\n    // Wrong usage. Could backfire because it bypasses BrowserPool.\n    page.browser().close();\n\n    // Correct usage. Allows graceful shutdown.\n    browserController.close();\n\n    const cookies = [/* some cookie objects */];\n    // Wrong usage. Will only work in Puppeteer and not Playwright.\n    page.setCookies(...cookies);\n\n    // Correct usage. Will work in both.\n    browserController.setCookies(page, cookies);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a New Actor with Apify CLI\nDESCRIPTION: Commands for creating a boilerplate actor project and running it locally using the Apify CLI.\nSOURCE: https://github.com/apify/crawlee/blob/master/website/versioned_docs/version-3.0/deployment/apify_platform.mdx#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create my-hello-world\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-hello-world\napify run\n```\n\n----------------------------------------\n\nTITLE: Simplified CheerioCrawler with Automatic RequestQueue\nDESCRIPTION: This optimized version uses crawler.run() with URLs directly, which automatically manages the RequestQueue. This approach is shorter, more efficient, and handles batches of requests for better performance.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/02-first-crawler.mdx#2025-04-11_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// You don't need to import RequestQueue anymore\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    }\n})\n\n// Start the crawler with the provided URLs\nawait crawler.run(['https://crawlee.dev']);\n```\n\n----------------------------------------\n\nTITLE: Using Same-Domain Strategy with enqueueLinks\nDESCRIPTION: Example showing how to configure enqueueLinks to include subdomains in the crawl by using the 'same-domain' strategy instead of the default same-hostname behavior.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/03-adding-urls.mdx#2025-04-11_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait enqueueLinks({\n    strategy: 'same-domain'\n});\n```\n\n----------------------------------------\n\nTITLE: Terminating a Crawlee Crawler\nDESCRIPTION: Keyboard shortcut instruction for terminating a running crawler in the terminal.\nSOURCE: https://github.com/apify/crawlee/blob/master/docs/introduction/01-setting-up.mdx#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nCTRL+C\n```"
  }
]
[
  {
    "owner": "microsoft",
    "repo": "omniparser",
    "content": "TITLE: Processing and Analyzing Screenshot with Object Detection and OCR in Python\nDESCRIPTION: Processes a screenshot image by performing OCR, detecting UI elements using the SOM model, and generating captions. The code handles image loading, preprocessing, OCR, object detection, and caption generation with performance timing.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# reload utils\nimport importlib\nimport utils\nimportlib.reload(utils)\n# from utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n\nimage_path = 'imgs/google_page.png'\nimage_path = 'imgs/windows_home.png'\n# image_path = 'imgs/windows_multitab.png'\n# image_path = 'imgs/omni3.jpg'\n# image_path = 'imgs/ios.png'\nimage_path = 'imgs/word.png'\n# image_path = 'imgs/excel2.png'\n\nimage = Image.open(image_path)\nimage_rgb = image.convert('RGB')\nprint('image size:', image.size)\n\nbox_overlay_ratio = max(image.size) / 3200\ndraw_bbox_config = {\n    'text_scale': 0.8 * box_overlay_ratio,\n    'text_thickness': max(int(2 * box_overlay_ratio), 1),\n    'text_padding': max(int(3 * box_overlay_ratio), 1),\n    'thickness': max(int(3 * box_overlay_ratio), 1),\n}\nBOX_TRESHOLD = 0.05\n\nimport time\nstart = time.time()\nocr_bbox_rslt, is_goal_filtered = check_ocr_box(image_path, display_img = False, output_bb_format='xyxy', goal_filtering=None, easyocr_args={'paragraph': False, 'text_threshold':0.9}, use_paddleocr=True)\ntext, ocr_bbox = ocr_bbox_rslt\ncur_time_ocr = time.time() \n\ndino_labled_img, label_coordinates, parsed_content_list = get_som_labeled_img(image_path, som_model, BOX_TRESHOLD = BOX_TRESHOLD, output_coord_in_ratio=True, ocr_bbox=ocr_bbox,draw_bbox_config=draw_bbox_config, caption_model_processor=caption_model_processor, ocr_text=text,use_local_semantics=True, iou_threshold=0.7, scale_img=False, batch_size=128)\ncur_time_caption = time.time() \n```\n\n----------------------------------------\n\nTITLE: Loading YOLO-based SOM Model for UI Element Detection in Python\nDESCRIPTION: Initializes a YOLO-based SOM (Screen Object Model) detection model for identifying UI elements. The model is loaded from a specified path and moved to the CUDA device for GPU acceleration.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\nimport torch\nfrom ultralytics import YOLO\nfrom PIL import Image\ndevice = 'cuda'\nmodel_path='weights/icon_detect/model.pt'\n\nsom_model = get_yolo_model(model_path)\n\nsom_model.to(device)\nprint('model to {}'.format(device))\n```\n\n----------------------------------------\n\nTITLE: Loading Caption Model for UI Element Description in Python\nDESCRIPTION: Loads a caption generation model (Florence2) that can describe UI elements. The model is retrieved using a utility function that handles the model and processor initialization.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# two choices for caption model: fine-tuned blip2 or florence2\nimport importlib\n# import util.utils\n# importlib.reload(utils)\nfrom util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\ncaption_model_processor = get_caption_model_processor(model_name=\"florence2\", model_name_or_path=\"weights/icon_caption_florence\", device=device)\n\n\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame from UI Element Detection Results in Python\nDESCRIPTION: Converts the parsed content list with UI element information into a Pandas DataFrame for easier data analysis. Each element is assigned an ID for reference.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.DataFrame(parsed_content_list)\ndf['ID'] = range(len(df))\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Displaying Annotated Screenshot with UI Elements in Python\nDESCRIPTION: Renders an annotated screenshot showing the detected UI elements and their bounding boxes. The image is decoded from base64 and displayed using matplotlib.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# plot dino_labled_img it is in base64\nimport base64\nimport matplotlib.pyplot as plt\nimport io\nplt.figure(figsize=(15,15))\n\nimage = Image.open(io.BytesIO(base64.b64decode(dino_labled_img)))\nplt.axis('off')\n\nplt.imshow(image)\n# print(len(parsed_content_list))\n```\n\n----------------------------------------\n\nTITLE: Viewing Raw UI Element Detection Results in Python\nDESCRIPTION: Displays the raw parsed content list which contains detailed information about the detected UI elements including their bounding boxes, captions, and relationships.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparsed_content_list\n```\n\n----------------------------------------\n\nTITLE: Downloading OmniParser V2 Model Weights\nDESCRIPTION: Shell commands to download the model checkpoints from HuggingFace to the local weights directory. Downloads detection and caption model files and organizes them in the correct folder structure.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfor f in icon_detect/{train_args.yaml,model.pt,model.yaml} icon_caption/{config.json,generation_config.json,model.safetensors}; do huggingface-cli download microsoft/OmniParser-v2.0 \"$f\" --local-dir weights; done\nmv weights/icon_caption weights/icon_caption_florence\n```\n\n----------------------------------------\n\nTITLE: Downloading OmniParser V2 Weights with Huggingface CLI\nDESCRIPTION: Command to download the necessary model weights for OmniParser V2 from Huggingface. This script removes any existing weight folders, downloads icon detection and caption models, and renames the caption folder appropriately.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/omnitool/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf weights/icon_detect weights/icon_caption weights/icon_caption_florence \nfor folder in icon_caption icon_detect; do huggingface-cli download microsoft/OmniParser-v2.0 --local-dir weights --repo-type model --include \"$folder/*\"; done\nmv weights/icon_caption weights/icon_caption_florence\n```\n\n----------------------------------------\n\nTITLE: Installing OmniParser Environment Setup\nDESCRIPTION: Commands for creating and configuring the Python environment required to run OmniParser. Creates a conda environment with Python 3.12 and installs required dependencies.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncd OmniParser\nconda create -n \"omni\" python==3.12\nconda activate omni\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Starting OmniParser Server\nDESCRIPTION: Command to start the OmniParser server after navigating to the server directory. This launches the FastAPI server that runs OmniParser V2 for processing visual information.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/omnitool/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m omniparserserver\n```\n\n----------------------------------------\n\nTITLE: Checking SOM Model Device and Type in Python\nDESCRIPTION: Queries the device (CPU/GPU) the SOM model is running on and its type. This is useful for debugging and confirming model configuration.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsom_model.device, type(som_model) \n```\n\n----------------------------------------\n\nTITLE: Starting Gradio UI Server\nDESCRIPTION: Command to start the Gradio UI server with specific host URLs for Windows VM and OmniParser. This launches the user interface for providing commands and watching reasoning and execution on OmniBox.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/omnitool/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython app.py --windows_host_url localhost:8006 --omniparser_server_url localhost:8000\n```\n\n----------------------------------------\n\nTITLE: Running Gradio Demo\nDESCRIPTION: Command to launch the Gradio web interface demo for OmniParser.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython gradio_demo.py\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: A requirements file listing Flask for web application development and PyAutoGUI for GUI automation and control. These packages would be installed with pip using this file.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/omnitool/omnibox/vm/win11setup/setupscripts/server/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nflask\nPyAutoGUI\n```\n\n----------------------------------------\n\nTITLE: Testing Windows Host Connectivity\nDESCRIPTION: Command to test if the server running in the VM is responding correctly. This helps diagnose connectivity issues between the Gradio UI and the OmniBox VM.\nSOURCE: https://github.com/microsoft/omniparser/blob/master/omnitool/readme.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:5000/probe\n```"
  }
]
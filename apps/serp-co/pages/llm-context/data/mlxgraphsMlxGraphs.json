[
  {
    "owner": "mlx-graphs",
    "repo": "mlx-graphs",
    "content": "TITLE: Implementing Custom GraphSAGE Convolution Layer\nDESCRIPTION: Example implementation of a custom GraphSAGE convolutional layer that handles edge weights. The implementation extends the MessagePassing base class and includes initialization, forward pass, and message computation logic.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_graphs.nn.linear import Linear\nfrom mlx_graphs.nn.message_passing import MessagePassing\n\nclass SAGEConv(MessagePassing):\n    def __init__(\n        self, node_features_dim: int, out_features_dim: int, bias: bool = True, **kwargs\n    ):\n        super(SAGEConv, self).__init__(aggr=\"mean\", **kwargs)\n\n        self.node_features_dim = node_features_dim\n        self.out_features_dim = out_features_dim\n\n        self.neigh_proj = Linear(node_features_dim, out_features_dim, bias=False)\n        self.self_proj = Linear(node_features_dim, out_features_dim, bias=bias)\n\n    def __call__(self, edge_index: mx.array, node_features: mx.array, edge_weights: mx.array) -> mx.array:\n         \"\"\"Forward layer of the custom SAGE layer.\"\"\"\n         neigh_features = self.propagate( # Message passing directly on GPU\n            edge_index=edge_index,\n            node_features=node_features,\n            message_kwargs={\"edge_weights\": edge_weights},\n         )\n         neigh_features = self.neigh_proj(neigh_features)\n\n        out_features = self.self_proj(node_features) + neigh_features\n        return out_features\n\n   def message(self, src_features: mx.array, dst_features: mx.array, **kwargs) -> mx.array:\n         \"\"\"Message function called by propagate(). Computes messages for all edges in the graph.\"\"\"\n        edge_weights = kwargs.get(\"edge_weights\", None)\n\n        return edge_weights.reshape(-1, 1) * src_features\n```\n\n----------------------------------------\n\nTITLE: Implementing Training and Evaluation Functions\nDESCRIPTION: Defines functions for training the model with the training loader and evaluating model performance on datasets. The train function updates model weights while the test function calculates classification accuracy.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef train(train_loader):\n    loss_sum = 0.0\n    for graph in train_loader:\n        \n        (loss, y_hat), grads = loss_and_grad_fn(\n            model=model,\n            graph=graph,\n            labels=graph.graph_labels,\n        )\n        optimizer.update(model, grads)\n        mx.eval(model.parameters(), optimizer.state)\n        loss_sum += loss.item()\n    return loss_sum / len(train_loader.dataset)\n\ndef test(loader):\n    acc = 0.0\n    for graph in loader:\n        y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n        y_hat = y_hat.argmax(axis=1)\n        acc += (y_hat == graph.graph_labels).sum().item()\n    \n    return acc / len(loader.dataset)\n\ndef epoch():\n    loss = train(train_loader)\n    train_acc = test(train_loader)\n    test_acc = test(test_loader)\n    return loss, train_acc, test_acc\n```\n\n----------------------------------------\n\nTITLE: Training the GCN Model for Graph Classification\nDESCRIPTION: Initializes the GCN model with appropriate dimensions, sets up Adam optimizer, and trains the model for 30 epochs. The training loop tracks and reports the best test accuracy achieved.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.optimizers as optim\nmx.random.seed(42)\n\nmodel = GCN(\n    in_dim=dataset.num_node_features,\n    hidden_dim=64,\n    out_dim=dataset.num_graph_classes,\n)\nmx.eval(model.parameters())\n\noptimizer = optim.Adam(learning_rate=0.01)\nloss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n\nepochs = 30\nbest_test_acc = 0.0\nfor e in range(epochs):\n    loss, train_acc, test_acc = epoch()\n    best_test_acc = max(best_test_acc, test_acc)\n\n    print(\n        \" | \".join(\n            [\n                f\"Epoch: {e:3d}\",\n                f\"Train loss: {loss:.3f}\",\n                f\"Train acc: {train_acc:.3f}\",\n                f\"Test acc: {test_acc:.3f}\",\n            ]\n        )\n    )\nprint(f\"\\n==> Best test accuracy: {best_test_acc:.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Data Loaders\nDESCRIPTION: Sets up training and test datasets with corresponding data loaders for batch processing.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.loaders import Dataloader\n\nnum_training_samples = 110_000\n\ntraining_dataset = qm9[:num_training_samples]\ntest_dataset = qm9[num_training_samples:]\n\nbatch_size = 128\n\ntrain_loader = Dataloader(training_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = Dataloader(test_dataset, batch_size=batch_size, shuffle=False)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for MXG - Python\nDESCRIPTION: This snippet implements the training loop for the MXG model, iterating through epochs and processing batches of graphs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef train_mxg(loader, step, state=None, epochs=1):    \n    for _ in range(epochs):\n        for graph in loader:\n            step(graph)\n            mx.eval(state)\n```\n\n----------------------------------------\n\nTITLE: Using GNN Layers with SAGEConv in Python\nDESCRIPTION: This snippet demonstrates how to use a GNN layer, specifically SAGEConv, for graph neural network operations. It creates a graph, initializes a SAGEConv layer with input and output dimensions, and applies the layer to the graph's edge index and node features.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_graphs.data.data import GraphData\nfrom mlx_graphs.nn import SAGEConv\n\ngraph = GraphData(\n    edge_index = mx.array([[0, 1, 2, 3, 4], [0, 0, 1, 1, 3]]),\n    node_features = mx.ones((5, 16)),\n)\n\nconv = SAGEConv(node_features_dim=16, out_features_dim=32)\nh = conv(graph.edge_index, graph.node_features)\n\n>>> h\narray([[1.65429, -0.376169, 1.04172, ..., -0.919106, 1.42576, 0.490938],\n    [1.65429, -0.376169, 1.04172, ..., -0.919106, 1.42576, 0.490938],\n    [1.05823, -0.295776, 0.075439, ..., -0.104383, 0.031947, -0.351897],\n    [1.65429, -0.376169, 1.04172, ..., -0.919106, 1.42576, 0.490938],\n    [1.05823, -0.295776, 0.075439, ..., -0.104383, 0.031947, -0.351897]],\n    dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Loaders for Mini-Batching\nDESCRIPTION: Creates DataLoaders for efficient mini-batching of graph data. Mini-batching allows for parallelized operations across multiple graphs without connecting them, using batch_indices to track graph membership.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.loaders import Dataloader\n\nBATCH_SIZE = 64\n\ntrain_loader = Dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = Dataloader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nfor batch in train_loader:\n    print(f\"\\nGraph batch of size {len(batch)}\")\n    print(batch)\n    print(batch.batch_indices)\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Function and Forward Pass\nDESCRIPTION: Implements the loss function using cross-entropy and creates a forward function for model training. The forward function returns both the loss and model predictions for gradient calculation.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(y_hat, y, parameters=None):\n    return mx.mean(nn.losses.cross_entropy(y_hat, y))\n\ndef forward_fn(model, graph, labels):\n    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n    loss = loss_fn(y_hat, labels, model.parameters())\n    return loss, y_hat\n```\n\n----------------------------------------\n\nTITLE: Defining MXG Model - Python\nDESCRIPTION: This snippet defines a graph neural network model in the MXG framework, specifying layers, activation functions, and dropout, along with a method for forward propagation.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MXG_model(mlx_nn.Module):\n    def __init__(self, layer, in_dim, hidden_dim, out_dim, dropout=0.5):\n        super(MXG_model, self).__init__()\n\n        self.conv1 = layer(in_dim, hidden_dim)\n        self.conv2 = layer(hidden_dim, hidden_dim)\n        self.conv3 = layer(hidden_dim, hidden_dim)\n        self.linear = mxg_nn.Linear(hidden_dim, out_dim)\n\n        self.dropout = mlx_nn.Dropout(p=dropout)\n\n    def __call__(self, edge_index, node_features, batch_indices):\n        h = mlx_nn.relu(self.conv1(edge_index, node_features))\n        h = mlx_nn.relu(self.conv2(edge_index, h))\n        h = self.conv3(edge_index, h)\n        \n        h = mxg_nn.global_mean_pool(h, batch_indices)\n\n        h = self.dropout(h)\n        h = self.linear(h)\n\n        return h\n```\n\n----------------------------------------\n\nTITLE: Implementing a GCN Model for Graph Classification\nDESCRIPTION: Defines a 3-layer Graph Convolutional Network (GCN) for graph classification. The model uses GCNConv layers followed by a global mean pooling operation to create graph-level embeddings for classification.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.nn as nn\nfrom mlx_graphs.nn import GCNConv, global_mean_pool, Linear\nimport time\n\n\nclass GCN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n        super(GCN, self).__init__()\n\n        self.conv1 = GCNConv(in_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n        self.linear = Linear(hidden_dim, out_dim)\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def __call__(self, edge_index, node_features, batch_indices):\n        h = nn.relu(self.conv1(edge_index, node_features))\n        h = nn.relu(self.conv2(edge_index, h))\n        h = self.conv3(edge_index, h)\n        \n        h = global_mean_pool(h, batch_indices)\n\n        h = self.dropout(h)\n        h = self.linear(h)\n        \n        return h\n```\n\n----------------------------------------\n\nTITLE: Loss Function for MXG - Python\nDESCRIPTION: This snippet implements a loss function using cross-entropy for the MXG model, allowing parameter adjustments during training.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(y_hat, y, parameters=None):\n    return mlx_nn.losses.cross_entropy(y_hat, y, reduction=\"mean\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete GNN Model\nDESCRIPTION: Defines the full Graph Neural Network model combining edge and node models with graph pooling.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.nn import global_mean_pool, GraphNetworkBlock\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gnn1 = GraphNetworkBlock(\n            node_model=NodeModel(qm9.num_node_features, 64, 64), \n            edge_model=EdgeModel(qm9.num_edge_features, qm9.num_node_features, 64),\n        )\n        self.gnn2 = GraphNetworkBlock(\n            node_model=NodeModel(64, 64, 64), \n            edge_model=EdgeModel(64, 64, 64),\n        )\n        self.output_linear = Linear(input_dims=64, output_dims=1)\n        \n    def __call__(\n        self,\n        edge_index: mx.array,\n        node_features: mx.array,\n        edge_features: mx.array,\n        batch_indices: mx.array,\n    ):\n        \n        node_features, edge_features, _ = self.gnn1(\n            edge_index=data.edge_index,\n            node_features=data.node_features,\n            edge_features=data.edge_features,\n        )\n        node_features, _, _ = self.gnn2(\n            edge_index=data.edge_index,\n            node_features=node_features,\n            edge_features=edge_features,\n        )\n        out = global_mean_pool(node_features, batch_indices)\n        return self.output_linear(out)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for DGL - Python\nDESCRIPTION: This snippet implements the training loop for the DGL model, processing batches of graph data and labels over multiple epochs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef train_dgl(loader, step, state=None, epochs=1):\n    for _ in range(epochs):\n        for data, labels in loader:\n            step(data,labels)\n```\n\n----------------------------------------\n\nTITLE: Defining DGL Model - Python\nDESCRIPTION: This snippet defines a graph neural network model in the DGL framework, specifying layers and a forward method for processing graph data with potential layer type handling.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass DGL_model(torch_nn.Module):\n    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n        super(DGL_model, self).__init__()\n\n        if \"GATConv\" in str(layer):\n            self.conv1 = layer(in_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n            self.conv2 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n            self.conv3 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n        else:\n            self.conv1 = layer(in_dim, hidden_dim, allow_zero_in_degree=True)\n            self.conv2 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n            self.conv3 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n\n        self.classify = torch_nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, g, h):\n        h = F.relu(self.conv1(g, h))\n        h = F.relu(self.conv2(g, h))\n        h = F.relu(self.conv3(g, h))\n        with g.local_scope():\n            g.ndata['h'] = h\n            hg = dgl.mean_nodes(g, 'h')\n            return self.classify(hg.squeeze())\n```\n\n----------------------------------------\n\nTITLE: Implementing Edge Model\nDESCRIPTION: Defines the EdgeModel class for processing edge features with source and target node information.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass EdgeModel(nn.Module):\n    def __init__(\n        self,\n        edge_features_dim: int,\n        node_features_dim: int,\n        output_dim: int,\n    ):\n        super().__init__()\n        self.linear = Linear(\n            input_dims=2 * node_features_dim + edge_features_dim, # source + target features + edge features\n            output_dims=output_dim,\n        )\n\n    def __call__(\n        self,\n        edge_index: mx.array,\n        node_features: mx.array,\n        edge_features: mx.array,\n        graph_features = None\n    ):\n        source_nodes = edge_index[0]\n        destination_nodes = edge_index[1]\n        model_input = mx.concatenate(\n            [\n                node_features[destination_nodes],\n                node_features[source_nodes],\n                edge_features,\n            ],\n            1,\n        )\n        new_edge_features = self.linear(model_input)\n        new_edge_features = nn.relu(new_edge_features)\n        return new_edge_features\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop for Node2vec Embedding Model in Python\nDESCRIPTION: Create a simple training loop to train the Node2vec embedding model over 10 epochs, using batched data and calculating loss.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in range(10):\n    total_loss = 0\n    dataloader = model.dataloader(batch_size=32)\n    for pos, neg in dataloader:\n        loss, grad = nn.value_and_grad(model, model.loss)(pos, neg)\n        total_loss+=loss.item()\n        optimizer.update(model, grad)\n    print(f\"Epoch : {epoch} batch loss : {total_loss/32:.5f}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Node Model\nDESCRIPTION: Defines the NodeModel class for processing node features with aggregated edge information.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass NodeModel(nn.Module):\n    def __init__(\n        self,\n        node_features_dim: int,\n        edge_features_dim: int,\n        output_dim: int,\n    ):\n        super().__init__()\n        self.linear = Linear(\n            input_dims=node_features_dim + edge_features_dim,\n            output_dims=output_dim,\n        )\n\n    def __call__(\n        self,\n        edge_index: mx.array,\n        node_features: mx.array,\n        edge_features: mx.array,\n        graph_features = None\n    ):\n        target_nodes = edge_index[1]\n        aggregated_edges = scatter(edge_features, target_nodes, out_size=node_features.shape[0], aggr=\"mean\")\n        model_input = mx.concatenate([node_features, aggregated_edges], 1)\n        new_node_features = self.linear(model_input)\n        new_node_features = nn.relu(new_node_features)\n        return new_node_features\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation\nDESCRIPTION: Implements the main training loop with progress tracking and evaluation.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\nmx.set_default_device(mx.gpu)\nregression_task = 4\n    \nlen_training_loader = int(len(training_dataset)/batch_size + 1)\nlen_test_loader = int(len(test_dataset)/batch_size + 1)\nepochs = 20\n\nepoch_training_losses = []\nepoch_test_losses = []\nfor epoch in range(epochs):\n    # training loop\n    with tqdm(train_loader, total=len_training_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch + 1}\")\n        training_loss = 0.0\n        for idx, data in enumerate(tepoch):\n            (loss, y_hat), grads = loss_and_grad_fn(\n                model=model,\n                graph=data,\n                labels=mx.expand_dims(data.graph_labels[:, regression_task], 1),\n            )\n            optimizer.update(model, grads)\n            mx.eval(model.parameters(), optimizer.state)\n            training_loss += loss.item()/data.num_graphs\n            tepoch.set_postfix(training_loss=training_loss / (idx+1), test_loss=0)\n        epoch_training_losses.append(training_loss/(idx+1))\n\n    # testing loop\n    with tqdm(test_loader, total=len_test_loader, unit=\"batch\", leave=False) as test_epoch:\n        test_epoch.set_description(f\"Testing\")\n        test_loss = 0.0\n        for idx, data in enumerate(test_epoch):\n            y_hat = model(data.edge_index, data.node_features, data.edge_features, data.batch_indices)\n            loss = loss_fn(y_hat, mx.expand_dims(data.graph_labels[:, regression_task], 1))\n            test_loss += loss.item()/data.num_graphs\n            test_epoch.set_postfix(test_loss=test_loss / (idx+1))\n        epoch_test_losses.append(test_loss/(idx+1))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training for PyG - Python\nDESCRIPTION: This snippet sets up a data loader and model for the PyG framework, initializes the optimizer and defines a training step function.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef setup_training_pyg(dataset, layer, batch_size, hid_size):\n    loader = pyg_loaders.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = PyG_model(\n        layer=layer,\n        in_dim=dataset.num_node_features,\n        hidden_dim=hid_size,\n        out_dim=dataset.num_classes,\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    criterion = torch_nn.CrossEntropyLoss()\n\n    model.train()\n    \n    def step(data):\n        out = model(data.x, data.edge_index, data.batch)\n        loss = criterion(out, data.y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return loader, step, None\n```\n\n----------------------------------------\n\nTITLE: Batching Multiple Graphs in Python\nDESCRIPTION: This snippet demonstrates how to batch multiple GraphData objects into a single GraphDataBatch for efficient processing. It creates three separate graphs and combines them using the batch function, which allows for indexing individual graphs within the batch.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.data.batch import batch\n\ngraphs = [\n    GraphData(\n        edge_index=mx.array([[0, 0, 0], [1, 1, 1]]),\n        node_features=mx.zeros((3, 1)),\n    ),\n    GraphData(\n        edge_index=mx.array([[1, 1, 1], [2, 2, 2]]),\n        node_features=mx.ones((3, 1)),\n    ),\n    GraphData(\n        edge_index=mx.array([[3, 3, 3], [4, 4, 4]]),\n        node_features=mx.ones((3, 1)) * 2,\n    )\n]\ngraphs_batch = batch(graphs)\n>>> GraphDataBatch(\n    edge_index(shape=[2, 9], int32)\n    node_features(shape=[9, 1], float32))\n\ngraphs_batch.num_graphs\n>>> 3\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training for DGL - Python\nDESCRIPTION: This snippet configures a data loader and model for the DGL framework, initializing the optimizer and defining a step function for training.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef setup_training_dgl(dataset, layer, batch_size, hid_size):\n    loader = dgl_loaders.GraphDataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n\n    model = DGL_model(\n        layer=layer,\n        in_dim=dataset[0][0].ndata[\"x\"].shape[1],\n        hidden_dim=hid_size,\n        out_dim=dataset.num_classes,\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    criterion = torch_nn.CrossEntropyLoss()\n\n    model.train()\n    \n    def step(data, labels):\n        out = model(data, data.ndata['x'])\n        loss = criterion(out, labels.squeeze())\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return loader, step, None\n```\n\n----------------------------------------\n\nTITLE: Setting Up Optimizer and Dataloader for Node2vec Training in Python\nDESCRIPTION: Import and initialize the Adam optimizer, and create a dataloader for the Node2vec model.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.optimizers as optim\n\noptimizer = optim.Adam(learning_rate=0.001)\n\ndataloader = model.dataloader(batch_size=64)\n\nimport mlx.nn as nn\n```\n\n----------------------------------------\n\nTITLE: MLX Random Walk Implementation\nDESCRIPTION: Random walk generation using MLX with compilation optimization for enhanced performance\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/random_walk_benchmark/Random-walk_benchmarks.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@mx.compile\ndef random_walk_mlx(row_ptr:mx.array, col: mx.array, start:mx.array, walk_length: int, rand_data):\n    num_walks = len(start)\n    num_nodes = row_ptr.shape[0]-1\n    n_out = mx.zeros((num_walks, walk_length + 1), dtype=col.dtype)\n    e_out = mx.zeros((num_walks, walk_length), dtype=col.dtype)\n\n    n_out[:, 0] = start\n\n    for l in range(walk_length):\n        n_cur = n_out[:, l]\n\n        row_start = row_ptr[n_cur]\n        row_end = row_ptr[n_cur + 1]\n\n        mask = (row_end - row_start) > 0\n        num_neighbors = row_end - row_start\n\n        rand_idx = (rand_data[l::walk_length] * num_neighbors)\n        rand_idx = rand_idx.astype(mx.int64)\n        e_cur = row_start + rand_idx\n        n_cur = col[e_cur]\n        n_cur = mx.where(~mask, n_out[:, l], n_cur)\n        e_cur = mx.where(~mask, -1, e_cur)\n\n        n_out[:, l + 1] = n_cur\n        e_out[:, l] = e_cur\n\n    return n_out, e_out\n```\n\n----------------------------------------\n\nTITLE: Setting Node2vec Hyperparameters in Python\nDESCRIPTION: Define hyperparameters for the Node2vec algorithm including embedding dimension, walk length, context size, and exploration parameters p and q.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nembedding_dim = 128\nwalk_length = 20\ncontext_size = 10\nwalks_per_node = 10\nnum_negative_samples = 1\np = 1.0\nq = 3.0\n```\n\n----------------------------------------\n\nTITLE: NumPy Optimized Random Walk Algorithm\nDESCRIPTION: Custom random walk implementation in NumPy with vectorized operations for improved performance\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/random_walk_benchmark/Random-walk_benchmarks.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef random_walk_numpy_optimized(rowptr, col, start, walk_length, rand_data):\n    num_walks = len(start)\n    num_nodes = len(rowptr) - 1\n    \n    n_out = np.zeros((num_walks, walk_length + 1), dtype=np.int64) \n    e_out = np.zeros((num_walks, walk_length), dtype=np.int64)\n    \n    n_out[:, 0] = start\n    \n    for l in range(walk_length):\n        n_cur = n_out[:, l]\n        row_start = rowptr[n_cur]\n        row_end = rowptr[n_cur + 1]\n        \n        mask = (row_end - row_start) > 0\n        num_neighbors = row_end - row_start\n        \n        rand_idx = (rand_data[l::walk_length] * num_neighbors).astype(np.int64)\n        e_cur = row_start + rand_idx\n        n_cur = col[e_cur]\n        \n        n_cur[~mask] = n_out[~mask, l]\n        e_cur[~mask] = -1\n        \n        n_out[:, l + 1] = n_cur\n        e_out[:, l] = e_cur\n    \n    return n_out, e_out\n```\n\n----------------------------------------\n\nTITLE: Using a Data Loader for Batching in Python\nDESCRIPTION: This snippet shows how to use the Dataloader class to automatically batch graphs for training. It creates a loader for the QM7bDataset with a batch size of 16, which can be iterated over to process batches of graphs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.loaders import Dataloader\n\ndata = QM7bDataset()\nloader = Dataloader(data, batch_size=16)\n\nfor item in loader:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Torch Cluster Random Walk Implementation\nDESCRIPTION: High-performance random walk generation using Torch Cluster's optimized C++ parallel processing algorithm\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/random_walk_benchmark/Random-walk_benchmarks.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrandom_walks = torch.ops.torch_cluster.random_walk(row_ptr, col, start, 1000, 1.0, 1.0)\n```\n\n----------------------------------------\n\nTITLE: Loading and Exploring the MUTAG Dataset\nDESCRIPTION: Loads the MUTAG dataset from TUDatasets collection for graph classification of chemical compounds. This dataset contains 188 samples with atoms as nodes and bonds as edges.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = TUDataset(\"MUTAG\")\ndataset\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Training Speed - Python\nDESCRIPTION: This snippet implements the final benchmark execution to measure the training speed of different model implementations on selected datasets.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfor dataset_name in datasets:\n    print(dataset_name)\n    print(\"=\" * 10)\n\n    for framework in frameworks:\n        dataset = framework_to_datasets[framework](dataset_name)\n\n        for i, layer_name in enumerate(layers):\n            layer = layer_classes[framework][layer_name]\n            loader, step, state = framework_to_setup[framework](dataset, layer, batch_size, hid_size)\n            \n            times = timeit.Timer(\n                lambda: benchmark(framework, loader, step, state)\n            ).repeat(repeat=TIMEIT_REPEAT, number=TIMEIT_NUMBER)\n\n            time = min(times) / TIMEIT_NUMBER\n\n            print(\n                \" | \".join(\n                    [\n                        f\"{framework}\",\n                        f\"{layer_name}\",\n                        f\"{time:.3f}s\",\n                    ]\n                )\n            )\n        print(\"\")\n\n```\n\n----------------------------------------\n\nTITLE: Loading QM9 Dataset with TUDataset\nDESCRIPTION: Imports and loads the QM9 molecular dataset using MLX-Graphs' TUDataset class.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.datasets import TUDataset\nqm9 = TUDataset(name=\"QM9\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Configuration - Python\nDESCRIPTION: This snippet sets up the benchmark configurations, including datasets, model frameworks, layer classes, and timing parameters for benchmarking model training among different frameworks.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\n\ndef dgl_dataset(name):\n    pyg_dataset = pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name)\n    dgl_dataset = dgl_datasets.TUDataset(dataset_name)\n\n    for i, (pyg, dgl) in enumerate(zip(pyg_dataset, dgl_dataset.graph_lists)):\n        dgl_dataset.graph_lists[i].ndata[\"x\"] = pyg.x\n    \n    return dgl_dataset\n\ndef benchmark(framework, loader, step, state):\n    train_fn = framework_to_train[framework]\n    train_fn(loader, step, state)\n\nframework_to_setup = {\n    \"mxg\": setup_training_mxg,\n    \"pyg\": setup_training_pyg,\n    \"dgl\": setup_training_dgl,\n}\n\nframework_to_train = {\n    \"mxg\": train_mxg,\n    \"pyg\": train_pyg,\n    \"dgl\": train_dgl,\n}\n\nframework_to_datasets = {\n    \"mxg\": lambda name: mxg_datasets.TUDataset(name),\n    \"pyg\": lambda name: pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name),\n    \"dgl\": lambda name: dgl_dataset(name)\n}\nlayer_classes = {\n    \"mxg\": {\n        \"GCNConv\": mxg_nn.GCNConv,\n        \"GATConv\": mxg_nn.GATConv,\n    },\n    \"pyg\": {\n        \"GCNConv\": pyg_nn.GCNConv,\n        \"GATConv\": pyg_nn.GATConv,\n    },\n    \"dgl\": {\n        \"GCNConv\": dgl_nn.GraphConv,\n        \"GATConv\": dgl_nn.GATConv,\n    }\n}\n\nframeworks = [\"dgl\", \"pyg\", \"mxg\"]\ndatasets = [\n    # \"BZR_MD\", \n    # \"MUTAG\",\n    \"DD\",\n]\nlayers = [\"GCNConv\", \"GATConv\"]\n\nbatch_size = 64\nhid_size = 128\n\ntimeit_repeat = 5\ntimeit_number = 1\n\ntorch.manual_seed(42)\nmx.random.seed(42)\ndgl.seed(42)\n\nmx.set_default_device(mx.gpu)\n```\n\n----------------------------------------\n\nTITLE: Loading Cora Dataset from PlanetoidDataset in Python\nDESCRIPTION: Load the Cora dataset, which is a citation network, using the PlanetoidDataset class from MLX-Graphs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = PlanetoidDataset(\"Cora\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking GCNConv Performance in MLX\nDESCRIPTION: This snippet illustrates the benchmarking results for Graph Convolutional Network (GCNConv) performance within the MLX framework. Different configurations based on graph edges, nodes, and input/output dimensions are analyzed to provide insights into how these parameters influence performance metrics such as speed and processing time.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/results/mlx_0.4.0/M3Pro_5P_6E_14GPU_18G.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n| benchmark_GCNConv / edg=(2, 1000) nod=(10, 64) in_=64 out=64           |   0.70 |   0.86 |   0.36 |    -48% |\n| benchmark_GCNConv / edg=(2, 10000) nod=(10, 64) in_=64 out=64          |   1.25 |   2.41 |   0.76 |    -39% |\n| benchmark_GCNConv / edg=(2, 100000) nod=(10, 64) in_=64 out=64         |   1.80 |  22.03 |   4.82 |   +168% |\n| benchmark_GCNConv / edg=(2, 1000000) nod=(10, 64) in_=64 out=64        |  10.03 | 218.59 |  63.20 |   +530% |\n| benchmark_GCNConv / edg=(2, 1000) nod=(100, 64) in_=64 out=64          |   0.70 |   0.89 |   0.42 |    -39% |\n| benchmark_GCNConv / edg=(2, 10000) nod=(100, 64) in_=64 out=64         |   0.98 |   5.57 |   0.80 |    -18% |\n| benchmark_GCNConv / edg=(2, 100000) nod=(100, 64) in_=64 out=64        |   1.82 |  22.09 |   5.25 |   +188% |\n| benchmark_GCNConv / edg=(2, 1000000) nod=(100, 64) in_=64 out=64       |   9.81 | 219.16 |  63.81 |   +550% |\n| benchmark_GCNConv / edg=(2, 1000) nod=(1000, 64) in_=64 out=64         |   1.00 |   1.12 |   0.21 |    -78% |\n| benchmark_GCNConv / edg=(2, 10000) nod=(1000, 64) in_=64 out=64        |   1.31 |   2.39 |   1.15 |    -11% |\n| benchmark_GCNConv / edg=(2, 100000) nod=(1000, 64) in_=64 out=64       |   3.75 |  22.19 |   5.49 |    +46% |\n| benchmark_GCNConv / edg=(2, 1000000) nod=(1000, 64) in_=64 out=64      |  10.19 | 220.14 |  64.50 |   +532% |\n| benchmark_GCNConv / edg=(2, 1000) nod=(10000, 64) in_=64 out=64        |   0.74 |   0.65 |   1.21 |    +62% |\n| benchmark_GCNConv / edg=(2, 10000) nod=(10000, 64) in_=64 out=64       |   1.06 |   2.82 |   1.97 |    +86% |\n| benchmark_GCNConv / edg=(2, 100000) nod=(10000, 64) in_=64 out=64      |   1.98 |  25.14 |   6.54 |   +231% |\n| benchmark_GCNConv / edg=(2, 1000000) nod=(10000, 64) in_=64 out=64     |  10.73 | 226.82 |  64.58 |   +501% |\n```\n\n----------------------------------------\n\nTITLE: Creating Train-Test Split Using Dataset Indexing\nDESCRIPTION: Splits the dataset into training and testing sets using slice indexing, which returns new Dataset objects containing the respective graphs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset = dataset[:150]\ntest_dataset = dataset[150:]\n\nprint(f\"Training dataset: {train_dataset}\")\nprint(f\"Testing dataset: {test_dataset}\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dataset Properties and Statistics\nDESCRIPTION: Extracts and displays key properties of the MUTAG dataset including the number of graphs, node features, edge features, and calculates statistics such as mean node degree and average graph size.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Some useful properties\nprint(\"Dataset attributes\")\nprint(\"-\" * 20)\nprint(f\"Number of graphs: {len(dataset)}\")\nprint(f\"Number of node features: {dataset.num_node_features}\")\nprint(f\"Number of edge features: {dataset.num_edge_features}\")\nprint(f\"Number of graph features: {dataset.num_graph_features}\")\nprint(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n\n# Statistics of the dataset\nstats = defaultdict(list)\nfor g in dataset:\n    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n    stats[\"Mean num of nodes\"].append(g.num_nodes)\n    stats[\"Mean num of edges\"].append(g.num_edges)\n\nprint(\"Dataset stats\")\nprint(\"-\" * 20)\nfor k, v in stats.items():\n    mean = mx.mean(mx.array(v)).item()\n    print(f\"{k}: {mean:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Dataset in Python\nDESCRIPTION: This snippet demonstrates how to implement a custom dataset by extending the Dataset base class. It creates a QM7bDataset class that downloads molecular data and processes it into a list of GraphData objects, handling the conversion of adjacency matrices to edge indices.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport mlx.core as mx\nimport scipy as sp\n\nfrom mlx_graphs.data import GraphData\nfrom mlx_graphs.datasets.dataset import Dataset\nfrom mlx_graphs.datasets.utils import download\nfrom mlx_graphs.utils.transformations import to_sparse_adjacency_matrix\n\nclass QM7bDataset(Dataset):\n    def __init__(self):\n        super().__init__(name=\"qm7b\")\n\n    def download(self):\n        file_path = os.path.join(self.raw_path, self.name + \".mat\")\n        download(\n            \"http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/qm7b.mat\",\n            path=file_path,\n        )\n\n    def process(self):\n        mat_path = os.path.join(self.raw_path, self.name + \".mat\")\n        data = scipy.io.loadmat(mat_path)\n        labels = mx.array(data[\"T\"].tolist())\n        features = mx.array(data[\"X\"].to_list())\n        num_graphs = labels.shape[0]\n        graphs = []\n        for i in range(num_graphs):\n            edge_index, edge_features = to_sparse_adjacency_matrix(features[i])\n            graphs.append(\n                GraphData(\n                    edge_index=edge_index,\n                    edge_features=edge_features,\n                    graph_labels=mx.expand_dims(labels[i], 0),\n                )\n            )\n        self.graphs = graphs\n```\n\n----------------------------------------\n\nTITLE: Accessing Individual Graph Data from the Dataset\nDESCRIPTION: Demonstrates how to access individual GraphData objects from the dataset using indexing. Each graph contains node features, edge features, and structural information.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset[0]\n```\n\n----------------------------------------\n\nTITLE: Exploring Dataset Properties and Statistics in Python\nDESCRIPTION: Print various properties of the dataset including number of graphs, node features, edge features, and calculate statistics such as mean node degree and number of nodes/edges.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Some useful properties\nprint(\"Dataset attributes\")\nprint(\"-\" * 20)\nprint(f\"Number of graphs: {len(dataset)}\")\nprint(f\"Number of node features: {dataset.num_node_features}\")\nprint(f\"Number of edge features: {dataset.num_edge_features}\")\nprint(f\"Number of graph features: {dataset.num_graph_features}\")\nprint(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n\n# Statistics of the dataset\nstats = defaultdict(list)\nfor g in dataset:\n    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n    stats[\"Mean num of nodes\"].append(g.num_nodes)\n    stats[\"Mean num of edges\"].append(g.num_edges)\n\nprint(\"Dataset stats\")\nprint(\"-\" * 20)\nfor k, v in stats.items():\n    mean = mx.mean(mx.array(v)).item()\n    print(f\"{k}: {mean:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Indexing Graphs in a Batch\nDESCRIPTION: This snippet shows how to access individual graphs within a GraphDataBatch object. It demonstrates both single-element indexing to retrieve one graph and slice indexing to retrieve multiple graphs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngraphs_batch[1]\n>>> GraphData(\n    edge_index(shape=[2, 3], int32)\n    node_features(shape=[3, 1], float32))\n\ngraphs_batch[1:]\n>>> [\n        GraphData(\n            edge_index(shape=[2, 3], int32)\n            node_features(shape=[3, 1], float32)),\n        GraphData(\n            edge_index(shape=[2, 3], int32)\n            node_features(shape=[3, 1], float32))\n    ]\n```\n\n----------------------------------------\n\nTITLE: Printing Dataset Properties\nDESCRIPTION: Displays key properties of the QM9 dataset including number of graphs, node features, edge features, and regression tasks.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Number of graphs: {len(qm9)}\")\nprint(f\"Number of node features: {qm9.num_node_features}\")\nprint(f\"Number of edge features: {qm9.num_edge_features}\")\nprint(f\"Number of regression tasks: {qm9[0].graph_labels.shape[1]}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training for MXG - Python\nDESCRIPTION: This snippet configures a data loader and model for the MXG framework, setting up the optimizer and training step function.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef setup_training_mxg(dataset, layer, batch_size, hid_size):\n    loader = mxg_loaders.Dataloader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = MXG_model(\n        layer=layer,\n        in_dim=dataset.num_node_features,\n        hidden_dim=hid_size,\n        out_dim=dataset.num_graph_classes,\n    )\n    mx.eval(model.parameters())\n\n    optimizer = mlx_optim.Adam(learning_rate=0.01)\n    loss_and_grad_fn = mlx_nn.value_and_grad(model, forward_fn)\n\n    state = [model.state, optimizer.state, mx.random.state]\n\n    def step(graph):\n        (loss, y_hat), grads = loss_and_grad_fn(model=model, graph=graph) \n        optimizer.update(model, grads)\n\n    return loader, step, state\n```\n\n----------------------------------------\n\nTITLE: Creating Simple Graphs with GraphData in Python\nDESCRIPTION: This snippet demonstrates how to create a simple graph using the GraphData class from mlx-graphs. It initializes a graph with edge_index defining the graph topology and node_features providing feature vectors for each node.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_graphs.data.data import GraphData\n\nedge_index = mx.array([[0, 1, 0, 2, 3],\n                       [2, 3, 1, 0, 2]])\nnode_features = mx.random.normal((4, 8))\n\ngraph = GraphData(edge_index, node_features)\n>>> GraphData(\n        edge_index(shape=(2, 5), int32)\n        node_features(shape=(4, 8), float32))\n\ngraph.edge_index\n>>> array([[0, 1, 0, 2, 3],\n           [2, 3, 1, 0, 2]], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Forward Function for MXG - Python\nDESCRIPTION: This snippet defines the forward function that encapsulates how input data flows through the MXG model and calculates loss.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef forward_fn(model, graph):\n    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n    labels = graph.graph_labels \n    loss = loss_fn(y_hat, labels, model.parameters())\n    return loss, y_hat\n```\n\n----------------------------------------\n\nTITLE: Creating Graphs with Custom Attributes in Python\nDESCRIPTION: This snippet shows how to create a graph with custom attributes using GraphData. Besides standard attributes like edge_index and node_features, it adds a custom attribute 'alpha' using kwargs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/quickstart.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_graphs.data.data import GraphData\n\ngraph = GraphData(\n    edge_index=mx.array([[0, 0, 0], [1, 1, 1]]),\n    node_features=mx.random.normal((4, 8)),\n    alpha=mx.ones((4,)),  # Custom attribute\n)\n>>> GraphData(\n        edge_index(shape=(2, 3), int32)\n        node_features(shape=(4, 8), float32)\n        alpha(shape=(4,), float32))\n\ngraph.alpha\n>>> array([1, 1, 1, 1], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Initializing Node2Vec Model in Python\nDESCRIPTION: Create a Node2Vec model instance using the defined hyperparameters and the loaded dataset's properties.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = Node2Vec(\n    edge_index=dataset[0].edge_index,\n    num_nodes=dataset[0].num_nodes,\n    embedding_dim=embedding_dim,\n    walk_length=walk_length,\n    context_size=context_size,\n    walks_per_node=walks_per_node,\n    num_negative_samples=num_negative_samples,\n    p=p,\n    q=q,\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing Cora Dataset\nDESCRIPTION: Loading the Planetoid Cora dataset for graph analysis and random walk benchmarking\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/random_walk_benchmark/Random-walk_benchmarks.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset = Planetoid(root =\"data/Cora\", name='Cora')\n```\n\n----------------------------------------\n\nTITLE: Importing Node2Vec Algorithm from MLX-Graphs in Python\nDESCRIPTION: Import the Node2Vec class from the MLX-Graphs algorithms module to create a neural network for node embedding.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_graphs.algorithms import Node2Vec\n```\n\n----------------------------------------\n\nTITLE: Computing Number of Hops in Graph Topology\nDESCRIPTION: A function to calculate the number of hops between nodes in a graph, useful for understanding graph connectivity and distance metrics\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/utils/topology.rst#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nget_num_hops(graph)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training Components\nDESCRIPTION: Initializes the model, loss function, and optimizer for training.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.optimizers as optim\n\nmodel = Model()\n\nmx.eval(model.parameters())\n\ndef loss_fn(y_hat, y, parameters=None):\n    return nn.losses.mse_loss(y_hat, y, \"sum\")\n\ndef forward_fn(model, graph, labels):\n    y_hat = model(graph.edge_index, graph.node_features, graph.edge_features, graph.batch_indices)\n    loss = loss_fn(y_hat, labels, model.parameters())\n    return loss, y_hat\n\nloss_and_grad_fn = nn.value_and_grad(model, forward_fn)\noptimizer = optim.AdamW(learning_rate=1e-3)\n```\n\n----------------------------------------\n\nTITLE: GNN Framework Performance Benchmark Table\nDESCRIPTION: Markdown table comparing execution times per epoch for different GNN frameworks (DGL, PyG, MXG) across datasets and layer types. Includes compiled MXG version showing improved performance.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/training_loop/results/mlx_0.3.0/M1Max_8P_2E_32GPU_32G.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Dataset | Framework | Layer | Time/epoch |\n| --- | --- | --- | --- |\n| BZR_MD | dgl | GCNConv | 0.062s |\n| BZR_MD | dgl | GATConv | 0.164s |\n| BZR_MD | pyg | GCNConv | 0.109s |\n| BZR_MD | pyg | GATConv | 0.162s |\n| BZR_MD | mxg | GCNConv | 0.051s |\n| BZR_MD | mxg | GATConv | 0.062s |\n| BZR_MD | mxg(compiled) | GCNConv | 0.039s |\n| BZR_MD | mxg(compiled) | GATConv | 0.051s |\n| MUTAG | dgl | GCNConv | 0.016s |\n| MUTAG | dgl | GATConv | 0.029s |\n| MUTAG | pyg | GCNConv | 0.023s |\n| MUTAG | pyg | GATConv | 0.032s |\n| MUTAG | mxg | GCNConv | 0.017s |\n| MUTAG | mxg | GATConv | 0.021s |\n| MUTAG | mxg(compiled) | GCNConv | 0.010s |\n| MUTAG | mxg(compiled) | GATConv | 0.013s |\n| DD | dgl | GCNConv | 1.090s |\n| DD | dgl | GATConv | 2.375s |\n| DD | pyg | GCNConv | 2.038s |\n| DD | pyg | GATConv | 2.637s |\n| DD | mxg | GCNConv | 1.276s |\n| DD | mxg | GATConv | 1.366s |\n| DD | mxg(compiled) | GCNConv | 0.605s |\n| DD | mxg(compiled) | GATConv | 0.722s |\n```\n\n----------------------------------------\n\nTITLE: Defining PyG Model - Python\nDESCRIPTION: This snippet defines a graph neural network model in the PyG framework, specifying layers, activation functions, and a forward method for model execution.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass PyG_model(torch.nn.Module):\n    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n        super(PyG_model, self).__init__()\n        \n        self.conv1 = layer(in_dim, hidden_dim)\n        self.conv2 = layer(hidden_dim, hidden_dim)\n        self.conv3 = layer(hidden_dim, hidden_dim)\n        self.lin = torch_nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.conv3(x, edge_index)\n\n        x = pyg_nn.global_mean_pool(x, batch)\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.lin(x)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Training Loop for PyG - Python\nDESCRIPTION: This snippet implements the training loop for the PyG model, processing batches of data over specified epochs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef train_pyg(loader, step, state=None, epochs=1):\n    for _ in range(epochs):\n        for data in loader:\n            step(data)\n```\n\n----------------------------------------\n\nTITLE: Loss Visualization\nDESCRIPTION: Creates a plot showing training and test loss evolution over epochs.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplt.figure()\nplt.plot(range(epochs), epoch_training_losses, label=\"Training\")\nplt.plot(range(epochs), epoch_test_losses, label=\"Test\")\nplt.title(\"Loss\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Graph Classification\nDESCRIPTION: Imports the essential libraries for working with graph data in MLX-Graphs, including collections for data manipulation and core MLX functionality.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\nimport mlx.core as mx\nfrom mlx_graphs.datasets import TUDataset\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Graph Processing\nDESCRIPTION: Initial import of necessary libraries for graph datasets, sparse matrix operations, and machine learning tools\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/random_walk_benchmark/Random-walk_benchmarks.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom torch_geometric.utils.sparse import index2ptr\nfrom torch_geometric.datasets import Planetoid\nfrom scipy.sparse import csr_matrix\nfrom torch_geometric.nn import Node2Vec\nimport torch\nfrom torch_geometric.utils import is_undirected\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Node2vec Implementation in Python\nDESCRIPTION: Import necessary libraries including MLX core, MLX-Graphs datasets, and other Python standard libraries for implementing Node2vec.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/node2vec.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nimport mlx.core as mx\nfrom mlx_graphs.datasets import PlanetoidDataset\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmark Table - Graph Operations\nDESCRIPTION: Tabulated benchmark results showing execution times in milliseconds for gather and scatter operations across different configurations. Includes speedup comparison between MLX GPU and PyG CPU implementations.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/results/mlx==0.4.0/M3Max_12P_4E_40GPU_128G.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Operation                                                                  | mlx_gpu | mlx_cpu | pyg_cpu | mlx_gpu/pyg_cpu speedup |\n|----------------------------------------------------------------------------|-------|-------|-------|-----------------------|\n| benchmark_gather / edg=(2, 1000) nod=(10, 64)                          |   0.22 |   0.03 |   0.20 |     -6% |\n| benchmark_gather / edg=(2, 10000) nod=(10, 64)                         |   0.38 |   0.14 |   0.40 |     +4% |\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Dependencies\nDESCRIPTION: Imports required MLX core, neural network, and graph utility modules.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_graphs.nn import Linear\nfrom mlx_graphs.utils import scatter\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Scatter Batch Performance in MLX\nDESCRIPTION: This snippet provides results of benchmarking the scatter batch operation using the MLX framework. Various configurations are tested with different parameters such as edges, nodes, and batch size to analyze performance metrics like speed and efficiency. The results reveal how scaling parameters affects processing times and performance gains.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/results/mlx_0.4.0/M3Pro_5P_6E_14GPU_18G.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n| benchmark_scatter_batch / edg=(2, 1000) nod=(100, 64) sca=add bat=1024 |   2.22 | 198.46 |  32.52 |  +1361% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(10, 64) sca=max bat=16    |   0.35 |   3.14 |   1.14 |   +230% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(10, 64) sca=max bat=16   |   1.09 |  31.05 |  10.79 |   +890% |\n| benchmark_scatter_batch / edg=(2, 100000) nod=(10, 64) sca=max bat=16  |   3.31 | 310.10 | 108.29 |  +3176% |\n| benchmark_scatter_batch / edg=(2, 1000000) nod=(10, 64) sca=max bat=16 |  29.16 | 3102.74 | 1178.11 |  +3939% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(100, 64) sca=max bat=16   |   0.35 |   3.13 |   1.11 |   +215% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(100, 64) sca=max bat=16  |   1.04 |  30.99 |  10.68 |   +928% |\n| benchmark_scatter_batch / edg=(2, 100000) nod=(100, 64) sca=max bat=16 |   3.28 | 309.60 | 100.54 |  +2962% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(1000, 64) sca=max bat=16  |   0.47 |   3.14 |   1.10 |   +135% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(1000, 64) sca=max bat=16 |   1.14 |  30.93 |  10.48 |   +819% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(10000, 64) sca=max bat=16 |   0.42 |   3.27 |   1.11 |   +161% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(10, 64) sca=max bat=128   |   0.86 |  24.78 |   8.62 |   +901% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(10, 64) sca=max bat=128  |   2.67 | 247.96 |  89.14 |  +3234% |\n| benchmark_scatter_batch / edg=(2, 100000) nod=(10, 64) sca=max bat=128 |  23.29 | 2479.58 | 945.39 |  +3958% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(100, 64) sca=max bat=128  |   0.89 |  24.82 |   8.12 |   +816% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(100, 64) sca=max bat=128 |   2.71 | 247.99 |  84.24 |  +3010% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(1000, 64) sca=max bat=128 |   0.55 |  24.78 |   8.25 |  +1393% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(10, 64) sca=max bat=1024  |   2.08 | 198.48 |  68.95 |  +3207% |\n| benchmark_scatter_batch / edg=(2, 10000) nod=(10, 64) sca=max bat=1024 |  18.55 | 1982.35 | 756.74 |  +3979% |\n| benchmark_scatter_batch / edg=(2, 1000) nod=(100, 64) sca=max bat=1024 |   2.21 | 198.41 |  64.67 |  +2830% |\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Graph Dependencies - Python\nDESCRIPTION: This snippet imports necessary libraries and dependencies for implementing graph neural networks using the MLX framework.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as mlx_nn\nimport mlx.optimizers as mlx_optim\n\nimport mlx_graphs.nn as mxg_nn\nimport mlx_graphs.datasets as mxg_datasets\nimport mlx_graphs.loaders as mxg_loaders\n```\n\n----------------------------------------\n\nTITLE: Benchmarking GATConv Performance in MLX\nDESCRIPTION: This snippet captures the results from benchmarking the Graph Attention Convolution (GATConv) operation within the MLX framework. It analyzes various configurations based on edges, nodes, and input/output dimensions. The performance metrics obtained provide insights into how these parameters influence processing times and the overall efficiency of GAT neural networks.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/results/mlx_0.4.0/M3Pro_5P_6E_14GPU_18G.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n| benchmark_GATConv / edg=(2, 1000) nod=(10, 64) in_=64 out=64           |   1.06 |   1.49 |   0.18 |    -82% |\n| benchmark_GATConv / edg=(2, 10000) nod=(10, 64) in_=64 out=64          |   1.48 |   3.73 |   0.96 |    -35% |\n| benchmark_GATConv / edg=(2, 100000) nod=(10, 64) in_=64 out=64         |   3.72 |  30.77 |   6.42 |    +72% |\n| benchmark_GATConv / edg=(2, 1000000) nod=(10, 64) in_=64 out=64        |  10.89 | 304.33 |  74.45 |   +583% |\n| benchmark_GATConv / edg=(2, 1000) nod=(100, 64) in_=64 out=64          |   0.84 |   1.36 |   0.54 |    -35% |\n| benchmark_GATConv / edg=(2, 10000) nod=(100, 64) in_=64 out=64         |   1.20 |   3.68 |   1.02 |    -15% |\n| benchmark_GATConv / edg=(2, 100000) nod=(100, 64) in_=64 out=64        |   3.87 |  30.98 |   6.57 |    +69% |\n| benchmark_GATConv / edg=(2, 1000000) nod=(100, 64) in_=64 out=64       |  10.75 | 301.28 |  71.31 |   +563% |\n| benchmark_GATConv / edg=(2, 1000) nod=(1000, 64) in_=64 out=64         |   0.88 |   1.92 |   0.38 |    -56% |\n| benchmark_GATConv / edg=(2, 10000) nod=(1000, 64) in_=64 out=64        |   1.45 |   3.86 |   1.27 |    -12% |\n| benchmark_GATConv / edg=(2, 100000) nod=(1000, 64) in_=64 out=64       |   4.25 |  31.02 |   6.59 |    +55% |\n| benchmark_GATConv / edg=(2, 1000000) nod=(1000, 64) in_=64 out=64      |  10.43 | 309.30 |  72.01 |   +590% |\n| benchmark_GATConv / edg=(2, 1000) nod=(10000, 64) in_=64 out=64        |   2.09 |   1.47 |   1.99 |     -4% |\n| benchmark_GATConv / edg=(2, 10000) nod=(10000, 64) in_=64 out=64       |   2.21 |   4.56 |   2.52 |    +13% |\n| benchmark_GATConv / edg=(2, 100000) nod=(10000, 64) in_=64 out=64      |   2.47 |  32.37 |   9.94 |   +302% |\n| benchmark_GATConv / edg=(2, 1000000) nod=(10000, 64) in_=64 out=64     |  10.98 | 306.73 |  70.65 |   +543% |\n```\n\n----------------------------------------\n\nTITLE: Setting MLX Computation Device\nDESCRIPTION: Shows how to configure the computing device for MLX operations, allowing flexibility to use either CPU or GPU. By default, MLX leverages the Mac's integrated GPU.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/graph_classification.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndevice = mx.gpu # or mx.cpu\nmx.set_default_device(device)\n```\n\n----------------------------------------\n\nTITLE: Importing PyG Graph Dependencies - Python\nDESCRIPTION: This snippet imports necessary libraries and dependencies for implementing graph neural networks using the PyTorch Geometric (PyG) framework.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.optim\nimport torch.nn as torch_nn\nimport torch.nn.functional as F\n\nimport torch_geometric.nn as pyg_nn\nimport torch_geometric.datasets as pyg_datasets\nimport torch_geometric.loader as pyg_loaders\n```\n\n----------------------------------------\n\nTITLE: Importing DGL Graph Dependencies - Python\nDESCRIPTION: This snippet imports necessary libraries and dependencies for implementing graph neural networks using the Deep Graph Library (DGL).\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/notebooks/benchmark_pyg_dgl_mxg.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dgl\nimport dgl.nn.pytorch as dgl_nn\nimport dgl.data as dgl_datasets\nimport dgl.dataloading as dgl_loaders\n```\n\n----------------------------------------\n\nTITLE: Installing additional dependencies for mlx-graphs\nDESCRIPTION: Commands to install extra dependencies for testing, development, benchmarking, and documentation building. Each command installs a specific set of dependencies.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/install.rst#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install -e '.[test]'\npip install -e '.[dev]'\npip install -e '.[benchmarks]'\npip install -e '.[docs]'\n```\n\n----------------------------------------\n\nTITLE: Organizing MLX-Graphs Benchmarks Structure\nDESCRIPTION: Documentation outlining the organization of benchmark tests into three main categories: architectural benchmarks for decision making, framework comparison benchmarks for comparing with other solutions, and profiling benchmarks for operation analysis.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Benchmarks\n\nThis folder contains benchmarks for mlx-graphs.\n\nThey're divided in\n- architectural: used to make architectural decisions\n- framework comparison: comparing mlx-graphs with other frameworks\n- profiling: useful to profile operations\n```\n\n----------------------------------------\n\nTITLE: RST Dataset Module Documentation Structure\nDESCRIPTION: ReStructuredText documentation defining the dataset module structure and available dataset classes in MLX-Graphs. Lists both base dataset classes and specific implementations.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/datasets/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _datasets:\n\nDatasets\n========\n\n.. currentmodule:: mlx_graphs.datasets\n\n\n.. autosummary::\n\t:toctree: _autosummary\n\t:template: dataset-class-template.rst\n\n\tDataset\n\tHeteroDataset\n\n\n.. autosummary::\n\t:toctree: _autosummary\n\t:template: datasets-class-template.rst\n\n\tKarateClubDataset\n\tPlanetoidDataset\n\tQM7bDataset\n\tTUDataset\n\tSuperPixelDataset\n\tOGBDataset\n\tEllipticBitcoinDataset\n\tMovieLens100K\n\tIMDB\n\tDBLP\n```\n\n----------------------------------------\n\nTITLE: Installing mlx-graphs from source\nDESCRIPTION: Command to install mlx-graphs from the local source using pip in editable mode. This is useful for development purposes.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/install.rst#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for MLX-Graphs Utils\nDESCRIPTION: ReStructuredText documentation layout defining the structure of utility module documentation including toctree and maxdepth specifications.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/utils/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _utils:\n\n\nUtils\n=====\n\n\n.. toctree::\n\t:maxdepth: 1\n\n\ttransformations\n\ttopology\n\tsorting\n\tvalidators\n\tscatter\n\tarray_ops\n\tconvert\n```\n\n----------------------------------------\n\nTITLE: Setting Up Examples Table of Contents in ReStructuredText\nDESCRIPTION: This snippet configures a table of contents (toctree) directive in ReStructuredText for the MLX-Graphs examples documentation. It includes a caption labeled 'Examples', sets maximum depth to 1, and lists the qm9_tutorial.ipynb notebook as the only example.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n\t:caption: Examples\n\t:maxdepth: 1\n\n\tqm9_tutorial.ipynb\n```\n\n----------------------------------------\n\nTITLE: Sphinx Class Documentation Template with Jinja2 for MLX-Graphs\nDESCRIPTION: A Jinja2 template used with Sphinx autodoc to generate class documentation. The template creates headings, includes inheritance information, adds method documentation, and creates a method summary section. It specifically handles the __call__ method separately and filters out __init__ and inherited methods from the main methods list.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/_templates/custom-class-template.rst#2025-04-21_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\t:members:\n\t:show-inheritance:\n\n\t{% block methods %}\n\t.. automethod:: __call__\n\n\t{% if methods %}\n\t.. rubric:: {{ _('Methods') }}\n\n\t.. autosummary:\n\t{% for item in methods %}\n\t  {%- if item not in inherited_members and item != '__init__' %}\n\t     ~{{ name }}.{{ item }}\n\t  {%- endif %}\n\t{%- endfor %}\n\t{% endif %}\n\t{% endblock %}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Custom SAGEConvolution Layer using MLX-graphs in Python\nDESCRIPTION: This snippet outlines the implementation of a custom GraphSAGE convolutional layer class, which extends the MessagePassing class from the MLX-graphs library. The class includes methods to initialize layer parameters and to compute forward passes with edge weights, efficiently leveraging GPU processing.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_graphs.nn.linear import Linear\nfrom mlx_graphs.nn.message_passing import MessagePassing\n\nclass SAGEConv(MessagePassing):\n    def __init__(self, node_features_dim: int, out_features_dim: int, bias: bool = True, **kwargs):\n        super(SAGEConv, self).__init__(aggr=\"mean\", **kwargs)\n\n        self.node_features_dim = node_features_dim\n        self.out_features_dim = out_features_dim\n\n        self.neigh_proj = Linear(node_features_dim, out_features_dim, bias=False)\n        self.self_proj = Linear(node_features_dim, out_features_dim, bias=bias)\n\n    def __call__(self, edge_index: mx.array, node_features: mx.array, edge_weights: mx.array) -> mx.array:\n        \"\"\"Forward layer of the custom SAGE layer.\"\"\"\n        neigh_features = self.propagate( # Message passing directly on GPU\n            edge_index=edge_index,\n            node_features=node_features,\n            message_kwargs={\"edge_weights\": edge_weights},\n        )\n        neigh_features = self.neigh_proj(neigh_features)\n\n        out_features = self.self_proj(node_features) + neigh_features\n        return out_features\n\n    def message(self, src_features: mx.array, dst_features: mx.array, **kwargs) -> mx.array:\n        \"\"\"Message function called by propagate(). Computes messages for all edges in the graph.\"\"\"\n        edge_weights = kwargs.get(\"edge_weights\", None)\n\n        return edge_weights.reshape(-1, 1) * src_features\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Documentation for Python Class with Jinja2\nDESCRIPTION: This template generates Sphinx documentation for a Python class. It includes the class name, members, special methods, and creates sections for methods and attributes. The template uses Jinja2 syntax for dynamic content generation.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/_templates/data-class-template.rst#2025-04-21_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\t:members:\n\t:special-members: __cat_dim__, __inc__\n\n\t{% block methods %}\n\n\t{% if methods %}\n\t.. rubric:: {{ _('Methods') }}\n\n\t.. autosummary:\n\t{% for item in all_methods %}\n\t\t{%- if not item.startswith('_') or item in ['__cat_dim__','__inc__',] %}\n\t     ~{{ name }}.{{ item }}\n\t  {%- endif %}\n\t{%- endfor %}\n\t{% endif %}\n\t{% endblock %}\n\n\t{% block attributes %}\n\t{% if attributes %}\n\t.. rubric:: {{ ('Attributes') }}\n\n\t.. autosummary:\n\t{% for item in attributes %}\n\t  ~{{ name }}.{{ item }}\n\t{%- endfor %}\n\t{% endif %}\n\t{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Visualization Setup\nDESCRIPTION: Imports matplotlib for plotting results.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/tutorials/examples/qm9_tutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Experiments for mlx-graphs\nDESCRIPTION: This command launches the benchmark experiments using the launcher.py script.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython launcher.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Profiling Results with Snakeviz\nDESCRIPTION: This command uses the snakeviz tool to visualize the profiling data stored in program.prof, providing a graphical representation of performance metrics.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/profiling/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsnakeviz program.prof\n```\n\n----------------------------------------\n\nTITLE: Installing Benchmark Dependencies for mlx-graphs\nDESCRIPTION: This command installs the necessary dependencies for running the benchmarks using pip.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/detailed_benchmarks/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install '.[benchmarks]'\n```\n\n----------------------------------------\n\nTITLE: Running MLX-Graphs Benchmark Script\nDESCRIPTION: Command to execute the benchmark launcher script that compares mlx-graphs against PyG and DGL frameworks.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/training_loop/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython launcher.py\n```\n\n----------------------------------------\n\nTITLE: Installing Benchmark Dependencies for MLX-Graphs\nDESCRIPTION: Command to install necessary dependencies for running benchmarks. The command uses pip to install the current package with the 'benchmarks' extra dependencies.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/training_loop/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install '.[benchmarks]'\n```\n\n----------------------------------------\n\nTITLE: Running Profiling Script in Python\nDESCRIPTION: This command executes a Python profiling script that will generate a program.prof file containing performance metrics.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/profiling/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython profile_training.py\n```\n\n----------------------------------------\n\nTITLE: Running the Edge Index Format Benchmark in MLX-Graphs\nDESCRIPTION: Instructions for executing the benchmark that compares row-format [2, num_edges] vs column-format [num_edges, 2] edge indices. The benchmark is run with a simple Python command and saves results to a markdown file.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/architectural/col_vs_row_edge_index/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo execute, simply run `python launcher.py`. The results are saved in `results.md`.\n```\n\n----------------------------------------\n\nTITLE: Building MLX-Graphs from Source\nDESCRIPTION: Commands to clone the repository and install MLX-Graphs from source code with its dependencies.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:mlx-graphs/mlx-graphs.git && cd mlx-graphs\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing MLX-Graphs via pip\nDESCRIPTION: Basic pip installation command for the MLX-Graphs library.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx-graphs\n```\n\n----------------------------------------\n\nTITLE: Cloning mlx-graphs repository\nDESCRIPTION: Git command to clone the mlx-graphs repository from GitHub and change to the project directory.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/install.rst#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:mlx-graphs/mlx-graphs.git && cd mlx-graphs\n```\n\n----------------------------------------\n\nTITLE: Installing mlx-graphs via pip\nDESCRIPTION: Command to install mlx-graphs using pip from PyPI. This is the simplest method for most users.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/install.rst#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install mlx-graphs\n```\n\n----------------------------------------\n\nTITLE: Benchmark Results Filename Format\nDESCRIPTION: Naming convention for benchmark result files that includes processor model, CPU core counts (performance and efficiency), GPU core count, and RAM size information.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/benchmarks/framework_comparison/training_loop/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<processor_model>_<# performance CPU cores>P_<# efficiency CPU cores>E_<# GPU cores>GPU_<GB or RAM>G.md\n```\n\n----------------------------------------\n\nTITLE: Opening Documentation Locally\nDESCRIPTION: Opens the generated HTML documentation in a web browser. This command assumes the documentation has been built and resides in the `build/html` directory. The `index.html` file is the entry point to the documentation.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nopen build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Markdown Contributing Guidelines Documentation\nDESCRIPTION: Complete markdown documentation outlining the contribution process for the MLX-Graphs project, including issue reporting, pull request submission, and community guidelines.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Contributing guidelines\n\n## Opening an Issue\n\n- Before [creating an issue](https://help.github.com/en/github/managing-your-work-on-github/creating-an-issue), check if you are using the latest version of the project. If you are not up-to-date, see if updating fixes your issue first.\n- Do not to open duplicate issues**; before creating a new issue, go through existing ones to see if it has previously been reported. If your issue exists, comment with any additional information you have.\n- Select the appropriate issue template and fully complete it. If no template is available for your specific issue, please use a blank one and report as much details as needed.\n\n### Reporting bugs\n\nA great way to contribute to the project is to open a detailed (bug) issue when you encounter a problem. We use a custom template for bugs. Make sure you're using the latest version of the project before opening a bug issue.\n\n### Feature Requests\n\nAnother great way to contribute is to open feature requests!\nWhile all requests are valid, we cannot guarantee your request will be accepted. Your idea may be great, but also out-of-scope for the project.\nFor every feature request, you are welcome to submit a pull request to help!\n\n\n## Asking Questions\nPlease use [Discussions](https://github.com/mlx-graphs/mlx-graphs/discussions) to ask questions instead of issues.\nGitHub issues are not the appropriate place to debug your specific project, but should be reserved for filing bugs and feature requests.\nTo chat and share random questions with the community, we recommend to join our [Discord server](https://discord.gg/K3mWFCxxM7).\n\n## Pull Requests\nWhen you submit a pull request, you'll be presented with a template to fill with a description of your contribution and checks on whether you added tests and documentation. Please fill all the parts.\nTo open a PR:\n- Fork the repo and follow the [local installation guidelines](https://github.com/mlx-graphs/mlx-graphs?tab=readme-ov-file#contributing).\n- Add your contribution and submit a pull request to the repo.\n- If you've added code that should be tested, add tests.\n- Update the necessary documentation.\n- If a change is likely to impact efficiency, run some of the benchmarks before\n   and after the change.\n- Run tests locally to make sure you're not introducing breaking changes.\n- Format the code using pre-commit.\n- Every PR should have passing CI tests and at least one review.\n```\n\n----------------------------------------\n\nTITLE: Building Documentation\nDESCRIPTION: Navigates to the documentation directory and executes the `make html` command to build the documentation. This process utilizes a Makefile to generate the documentation in HTML format, placing the output in the `build/html` directory.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake html\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Class Documentation Template in RST\nDESCRIPTION: A reStructuredText template for Sphinx documentation that sets up class documentation. It uses template variables to dynamically generate documentation, showing class inheritance and setting the current module context.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/_templates/datasets-class-template.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\t:show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML Contributors Image in Markdown\nDESCRIPTION: HTML code embedded in the markdown file that displays an image showing all contributors to the mlx-graphs repository using the contrib.rocks service. The image is responsive with columns and styling options.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/ACKNOWLEDGMENTS.md#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://github.com/mlx-graphs/mlx-graphs/graphs/contributors\">\n  <img class=\"dark-light\" src=\"https://contrib.rocks/image?repo=mlx-graphs/mlx-graphs&anon=0&columns=20&max=100&r=true\" />\n</a>\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for MLX Graphs Dataloader\nDESCRIPTION: This RST code sets up Sphinx documentation for the mlx_graphs.loaders module, specifically focusing on the Dataloader class. The code uses autosummary to generate API documentation automatically.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/loaders/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _loaders:\n\nLoaders\n=======\n\n\n.. currentmodule:: mlx_graphs.loaders\n\n\n.. autosummary::\n\t:toctree: _autosummary\n\n\tDataloader\n```\n\n----------------------------------------\n\nTITLE: Array Operations Module Index in RST\nDESCRIPTION: ReStructuredText documentation index defining the array operations available in the MLX Graphs array_ops utility module. Lists key functions like broadcast, expand, one_hot, pairwise_distances, and index_to_mask.\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/utils/array_ops.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: mlx_graphs.utils.array_ops\n\n.. autosummary::\n\t:toctree: _autosummary_array-ops\n\n\tbroadcast\n\texpand\n\tone_hot\n\tpairwise_distances\n\tindex_to_mask\n```\n\n----------------------------------------\n\nTITLE: Checking Graph Directionality in MLX Graphs\nDESCRIPTION: Utility functions to determine whether a graph is directed or undirected, providing essential topology analysis capabilities for graph processing\nSOURCE: https://github.com/mlx-graphs/mlx-graphs/blob/main/docs/source/api/utils/topology.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nis_undirected(graph)\n```\n\nLANGUAGE: Python\nCODE:\n```\nis_directed(graph)\n```"
  }
]
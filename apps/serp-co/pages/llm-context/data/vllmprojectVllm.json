[
  {
    "owner": "vllm-project",
    "repo": "vllm",
    "content": "TITLE: Initializing vLLM Engine with a Model\nDESCRIPTION: Python code to initialize the vLLM engine with the OPT-125M model from Facebook for offline inference. This creates an LLM instance that will handle the generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=\"facebook/opt-125m\")\n```\n\n----------------------------------------\n\nTITLE: Python Client for OpenAI-Compatible Completions\nDESCRIPTION: Python code using the OpenAI client library to query the vLLM server for text completion by configuring the client to point to the local vLLM server instead of OpenAI's API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\ncompletion = client.completions.create(model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n                                      prompt=\"San Francisco is a\")\nprint(\"Completion result:\", completion)\n```\n\n----------------------------------------\n\nTITLE: Implementing Backward Compatible Model Class\nDESCRIPTION: Demonstrates how to create a backward-compatible model class that works with both old and new vLLM versions by implementing a wrapper class that adapts the old constructor signature to the new configuration-based approach.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/arch_overview.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyOldModel(nn.Module):\n    def __init__(\n        self,\n        config,\n        cache_config: Optional[CacheConfig] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n        lora_config: Optional[LoRAConfig] = None,\n        prefix: str = \"\",\n    ) -> None:\n        ...\n\nfrom vllm.config import VllmConfig\nclass MyNewModel(MyOldModel):\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        config = vllm_config.model_config.hf_config\n        cache_config = vllm_config.cache_config\n        quant_config = vllm_config.quant_config\n        lora_config = vllm_config.lora_config\n        super().__init__(config, cache_config, quant_config, lora_config, prefix)\n\nif __version__ >= \"0.6.4\":\n    MyModel = MyNewModel\nelse:\n    MyModel = MyOldModel\n```\n\n----------------------------------------\n\nTITLE: Curl Request for Text Completion with vLLM\nDESCRIPTION: Curl command to query the vLLM server for a text completion using the OpenAI-compatible completions endpoint with specific generation parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_9\n\nLANGUAGE: console\nCODE:\n```\ncurl http://localhost:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 7,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic LLaVA Prompt Updates in Python\nDESCRIPTION: Implementation of _get_prompt_updates method for LLaVA processor that handles image token repetition based on the number of placeholder feature tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _get_prompt_updates(\n    self,\n    mm_items: MultiModalDataItems,\n    hf_processor_mm_kwargs: Mapping[str, object],\n    out_mm_kwargs: MultiModalKwargs,\n) -> Sequence[PromptUpdate]:\n    hf_config = self.info.get_hf_config()\n    image_token_id = hf_config.image_token_index\n\n    def get_replacement(item_idx: int):\n        images = mm_items.get_items(\"image\", ImageProcessorItems)\n\n        image_size = images.get_image_size(item_idx)\n        num_image_tokens = self.info.get_num_image_tokens(\n            image_width=image_size.width,\n            image_height=image_size.height,\n        )\n\n        return [image_token_id] * num_image_tokens\n\n    return [\n        PromptReplacement(\n            modality=\"image\",\n            target=[image_token_id],\n            replacement=get_replacement,\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Curl Request for Chat Completion with vLLM\nDESCRIPTION: Curl command to query the vLLM server for a chat completion using the OpenAI-compatible chat completions endpoint with system and user messages.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_11\n\nLANGUAGE: console\nCODE:\n```\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Importing vLLM Classes for Offline Inference\nDESCRIPTION: Python code to import the main vLLM classes needed for offline batch inference: LLM for running inference and SamplingParams for configuring generation parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n```\n\n----------------------------------------\n\nTITLE: Initializing Multi-GPU Inference with vLLM's LLM Class\nDESCRIPTION: Example of running distributed inference using vLLM's LLM class with tensor parallelism across 4 GPUs. This code demonstrates how to initialize the LLM and generate text using multiple GPUs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/distributed_serving.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nllm = LLM(\"facebook/opt-13b\", tensor_parallel_size=4)\noutput = llm.generate(\"San Francisco is a\")\n```\n\n----------------------------------------\n\nTITLE: Interacting with Deployed vLLM Model using OpenAI SDK\nDESCRIPTION: Python script demonstrating how to interact with the deployed vLLM model using the OpenAI SDK. It sets up the client with the appropriate base URL and API key, then sends a chat completion request.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/dstack.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://gateway.<gateway domain>\",\n    api_key=\"<YOUR-DSTACK-SERVER-ACCESS-TOKEN>\"\n)\n\ncompletion = client.chat.completions.create(\n    model=\"NousResearch/Llama-2-7b-chat-hf\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n        }\n    ]\n)\n\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with vLLM LLM Class\nDESCRIPTION: Demonstrates how to use the LLM.generate method for text generation with the OPT-125M model. It shows basic usage and how to control generation with SamplingParams.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/generative_models.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"facebook/opt-125m\")\noutputs = llm.generate(\"Hello, my name is\")\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model=\"facebook/opt-125m\")\nparams = SamplingParams(temperature=0)\noutputs = llm.generate(\"Hello, my name is\", params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM LLM Instance with Facebook OPT-125M Model\nDESCRIPTION: This snippet demonstrates how to create an LLM instance using vLLM, loading the facebook/opt-125m model from HuggingFace with default configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/offline_inference.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"facebook/opt-125m\")\n```\n\n----------------------------------------\n\nTITLE: Using vLLM's LLM Class for Offline Inference in Python\nDESCRIPTION: This snippet demonstrates how to use the LLM class from vLLM to perform offline inference. It initializes an LLM with a specific model, defines sampling parameters, and generates outputs for a list of input prompts.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/arch_overview.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\n# Define a list of input prompts\nprompts = [\n    \"Hello, my name is\",\n    \"The capital of France is\",\n    \"The largest ocean is\",\n]\n\n# Define sampling parameters\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Initialize the LLM engine with the OPT-125M model\nllm = LLM(model=\"facebook/opt-125m\")\n\n# Generate outputs for the input prompts\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the generated outputs\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Registering External Models with vLLM ModelRegistry in Python\nDESCRIPTION: This code demonstrates how to register an external model with vLLM's ModelRegistry. It imports the ModelRegistry class from vLLM and a custom model class, then registers the model with a specified name.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/registration.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import ModelRegistry\nfrom your_code import YourModelForCausalLM\nModelRegistry.register_model(\"YourModelForCausalLM\", YourModelForCausalLM)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Prompts and Sampling Parameters\nDESCRIPTION: Python code defining a list of input prompts and configuring sampling parameters with a temperature of 0.8 and top_p of 0.95 for text generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM LLM Instance with Limited Context and Batch Size\nDESCRIPTION: This snippet shows how to initialize an LLM instance with reduced context length and maximum batch size to optimize memory usage for resource-constrained environments.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/offline_inference.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"adept/fuyu-8b\",\n          max_model_len=2048,\n          max_num_seqs=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Guided Decoding for Offline Inference in vLLM with Choice Parameter\nDESCRIPTION: This code demonstrates how to set up guided decoding for offline inference using vLLM. It configures the LLM to restrict its output to specific choices (\"Positive\" or \"Negative\") when classifying sentiment, using the GuidedDecodingParams class within SamplingParams.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\nfrom vllm.sampling_params import GuidedDecodingParams\n\nllm = LLM(model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n\nguided_decoding_params = GuidedDecodingParams(choice=[\"Positive\", \"Negative\"])\nsampling_params = SamplingParams(guided_decoding=guided_decoding_params)\noutputs = llm.generate(\n    prompts=\"Classify this sentiment: vLLM is wonderful!\",\n    sampling_params=sampling_params,\n)\nprint(outputs[0].outputs[0].text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Modal Processing Methods in Model Class\nDESCRIPTION: Example implementation of processing methods for image inputs, including _process_image_input and get_multimodal_embeddings which generate embeddings from image inputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass YourModelForImage2Seq(nn.Module):\n    ...\n\n    def _process_image_input(self, image_input: YourModelImageInputs) -> torch.Tensor:\n\n        assert self.vision_encoder is not None\n        image_features = self.vision_encoder(image_input)\n        return self.multi_modal_projector(image_features)\n\n    def get_multimodal_embeddings(\n            self, **kwargs: object) -> Optional[MultiModalEmbeddings]:\n\n        # Validate the multimodal input keyword arguments\n        image_input = self._parse_and_validate_image_input(**kwargs)\n        if image_input is None:\n            return None\n\n        # Run multimodal inputs through encoder and projector\n        vision_embeddings = self._process_image_input(image_input)\n        return vision_embeddings\n```\n\n----------------------------------------\n\nTITLE: Chatting with vLLM LLM Class\nDESCRIPTION: Demonstrates how to use the LLM.chat method for chat functionality with the Meta-Llama-3-8B-Instruct model. It shows how to format a conversation and generate responses.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/generative_models.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I assist you today?\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Write an essay about the importance of higher education.\",\n    },\n]\noutputs = llm.chat(conversation)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Grammar-Guided SQL Query Generation\nDESCRIPTION: Example of using guided_grammar parameter to generate SQL queries following a specific EBNF grammar structure.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsimplified_sql_grammar = \"\"\"\n    ?start: select_statement\n\n    ?select_statement: \"SELECT \" column_list \" FROM \" table_name\n\n    ?column_list: column_name (\",\" column_name)*\n\n    ?table_name: identifier\n\n    ?column_name: identifier\n\n    ?identifier: /[a-zA-Z_][a-zA-Z0-9_]*/\n\"\"\"\n\ncompletion = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-3B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate an SQL query to show the 'username' and 'email' from the 'users' table.\",\n        }\n    ],\n    extra_body={\"guided_grammar\": simplified_sql_grammar},\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Testing Model Support in vLLM\nDESCRIPTION: This code snippet shows how to test if a model is supported in vLLM by attempting to generate text or encode input. It covers both generative and pooling models.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\n# For generative models (task=generate) only\nllm = LLM(model=..., task=\"generate\")  # Name or path of your model\noutput = llm.generate(\"Hello, my name is\")\nprint(output)\n\n# For pooling models (task={embed,classify,reward,score}) only\nllm = LLM(model=..., task=\"embed\")  # Name or path of your model\noutput = llm.encode(\"Hello, my name is\")\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pythonic Tool Calls Example\nDESCRIPTION: An example showing the Pythonic tool call format where models generate a Python list to represent tool calls instead of JSON. This format supports parallel tool calls for weather lookups in multiple cities.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[get_weather(city='San Francisco', metric='celsius'), get_weather(city='Seattle', metric='celsius')]\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM with CUDA Support using pip\nDESCRIPTION: Command to install vLLM with CUDA support using pip. This method uses pre-built wheels for easy installation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install vllm\n```\n\n----------------------------------------\n\nTITLE: Guided Choice Generation with OpenAI API\nDESCRIPTION: Example showing how to use guided_choice parameter to force the model to select from predefined choices for sentiment classification using vLLM's OpenAI-compatible API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"-\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-3B-Instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n    ],\n    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with vLLM and Processing Results\nDESCRIPTION: Python code to generate text outputs using the initialized vLLM engine, then iterating through and printing the results. This demonstrates how to process the RequestOutput objects returned by vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Speculative Decoding with a Draft Model in vLLM\nDESCRIPTION: Code example showing how to configure vLLM in offline mode to use speculative decoding with a smaller draft model (opt-125m) for the main model (opt-6.7b), speculating 5 tokens at a time.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\n    model=\"facebook/opt-6.7b\",\n    tensor_parallel_size=1,\n    speculative_config={\n        \"model\": \"facebook/opt-125m\",\n        \"num_speculative_tokens\": 5,\n    },\n)\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n```\n\n----------------------------------------\n\nTITLE: Checking Model Existence in vLLM\nDESCRIPTION: This code snippet shows how vLLM determines whether a specified model exists by checking for the corresponding config file. It handles local paths, HuggingFace model IDs, and downloading from the HuggingFace model hub.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/huggingface_integration.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_config_path(pretrained_model_name_or_path: str,\n                   revision: Optional[str] = None,\n                   trust_remote_code: bool = False) -> str:\n    # Check if pretrained_model_name_or_path is a local path\n    if os.path.isdir(pretrained_model_name_or_path):\n        config_path = os.path.join(pretrained_model_name_or_path,\n                                   CONFIG_NAME)\n        if not os.path.exists(config_path):\n            raise ValueError(\n                f\"Config file not found in {pretrained_model_name_or_path}\")\n        return config_path\n\n    # Check if pretrained_model_name_or_path is a HuggingFace model ID\n    if is_huggingface_model_id(pretrained_model_name_or_path):\n        try:\n            config_path = cached_file(\n                pretrained_model_name_or_path,\n                CONFIG_NAME,\n                revision=revision,\n                trust_remote_code=trust_remote_code)\n            return config_path\n        except Exception:\n            # If not found in cache, download from HuggingFace model hub\n            return download_config_file(\n                pretrained_model_name_or_path,\n                revision=revision,\n                trust_remote_code=trust_remote_code)\n\n    raise ValueError(\n        f\"Unable to find config file for {pretrained_model_name_or_path}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Input Embeddings Merger for Multi-Modal Model\nDESCRIPTION: Implementation of get_input_embeddings method to merge multimodal embeddings with text embeddings from input_ids using a utility function.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom .utils import merge_multimodal_embeddings\n\nclass YourModelForImage2Seq(nn.Module):\n    ...\n\n    def get_input_embeddings(\n        self,\n        input_ids: torch.Tensor,\n        multimodal_embeddings: Optional[MultiModalEmbeddings] = None,\n    ) -> torch.Tensor:\n\n        # `get_input_embeddings` should already be implemented for the language \n        # model as one of the requirements of basic vLLM model implementation.\n        inputs_embeds = self.language_model.get_input_embeddings(input_ids)\n\n        if multimodal_embeddings is not None:\n            inputs_embeds = merge_multimodal_embeddings(\n                input_ids=input_ids, \n                inputs_embeds=inputs_embeds, \n                multimodal_embeddings=multimodal_embeddings,\n                placeholder_token_id=self.config.image_token_index)\n\n        return inputs_embeds\n```\n\n----------------------------------------\n\nTITLE: Using Custom Chat Template with vLLM LLM Class\nDESCRIPTION: Shows how to load and use a custom chat template with the LLM.chat method for chat functionality.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/generative_models.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm.entrypoints.chat_utils import load_chat_template\n\n# You can find a list of existing chat templates under `examples/`\ncustom_template = load_chat_template(chat_template=\"<path_to_template>\")\nprint(\"Loaded chat template:\", custom_template)\n\noutputs = llm.chat(conversation, chat_template=custom_template)\n```\n\n----------------------------------------\n\nTITLE: Executing vLLM with XPU Backend and Distributed Features\nDESCRIPTION: Demonstrates how to run vLLM with XPU backend, enabling tensor parallel and pipeline parallel features. This command uses Ray as the distributed executor backend.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npython -m vllm.entrypoints.openai.api_server \\\n     --model=facebook/opt-13b \\\n     --dtype=bfloat16 \\\n     --device=xpu \\\n     --max_model_len=1024 \\\n     --distributed-executor-backend=ray \\\n     --pipeline-parallel-size=2 \\\n     -tp=8\n```\n\n----------------------------------------\n\nTITLE: Single Image Inference with LLaVA Model\nDESCRIPTION: Example showing how to perform single image inference using the LLaVA model in vLLM. Demonstrates loading an image and generating text based on it.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\n\n# Refer to the HuggingFace repo for the correct format to use\nprompt = \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\"\n\n# Load the image using PIL.Image\nimage = PIL.Image.open(...)\n\n# Single prompt inference\noutputs = llm.generate({\n    \"prompt\": prompt,\n    \"multi_modal_data\": {\"image\": image},\n})\n\nfor o in outputs:\n    generated_text = o.outputs[0].text\n    print(generated_text)\n```\n\n----------------------------------------\n\nTITLE: Python Client for OpenAI-Compatible Chat Completions\nDESCRIPTION: Python code using the OpenAI client library to query the vLLM server for chat completions by configuring the client to point to the local vLLM server instead of OpenAI's API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n    ]\n)\nprint(\"Chat response:\", chat_response)\n```\n\n----------------------------------------\n\nTITLE: Using LLM.classify for Classification Models in vLLM\nDESCRIPTION: Example demonstrating how to perform text classification with a classification model. The classify method returns probability vectors for each input prompt, which can be used for multi-class classification tasks.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"jason9693/Qwen2.5-1.5B-apeach\", task=\"classify\")\n(output,) = llm.classify(\"Hello, my name is\")\n\nprobs = output.outputs.probs\nprint(f\"Class Probabilities: {probs!r} (size={len(probs)})\")\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized Models in vLLM\nDESCRIPTION: Code to load a quantized INT8 model in vLLM for inference after completing the quantization process.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\")\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM LLM Instance with Tensor Parallelism\nDESCRIPTION: This code demonstrates how to initialize an LLM instance with tensor parallelism, splitting the model across multiple GPUs to reduce memory usage per device.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/offline_inference.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=\"ibm-granite/granite-3.1-8b-instruct\",\n          tensor_parallel_size=2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running a Quantized Model in vLLM\nDESCRIPTION: Code to load a quantized model in vLLM and generate text, showing the basic inference usage after quantization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-FP8-Dynamic\")\nmodel.generate(\"Hello my name is\")\n```\n\n----------------------------------------\n\nTITLE: Lazy-Loading External Models with vLLM ModelRegistry in Python\nDESCRIPTION: This code shows how to lazy-load an external model with vLLM's ModelRegistry to avoid CUDA reinitialization errors in forked subprocesses. It uses string-based importing to delay loading the model until it's needed.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/registration.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import ModelRegistry\n\nModelRegistry.register_model(\"YourModelForCausalLM\", \"your_code:YourModelForCausalLM\")\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM with LoRA Support\nDESCRIPTION: Instantiates the base LLM model with LoRA support enabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\nfrom vllm.lora.request import LoRARequest\n\nllm = LLM(model=\"meta-llama/Llama-2-7b-hf\", enable_lora=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a DeepSeekReasoner for Structured Output in Python\nDESCRIPTION: Implementation of a Reasoner class for DeepSeek R series models that defines special tokens for reasoning and methods to identify reasoning patterns in model output. This is used for structured output generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass DeepSeekReasoner(Reasoner):\n    \"\"\"\n    Reasoner for DeepSeek R series models.\n    \"\"\"\n    start_token_id: int\n    end_token_id: int\n\n    start_token: str = \"<think>\"\n    end_token: str = \"</think>\"\n\n    @classmethod\n    def from_tokenizer(cls, tokenizer: PreTrainedTokenizer) -> Reasoner:\n        return cls(start_token_id=tokenizer.encode(\n            \"<think>\", add_special_tokens=False)[0],\n                   end_token_id=tokenizer.encode(\"</think>\",\n                                                 add_special_tokens=False)[0])\n\n    def is_reasoning_end(self, input_ids: list[int]) -> bool:\n        return self.end_token_id in input_ids\n    ...\n```\n\n----------------------------------------\n\nTITLE: Complex Pydantic Parsing for Math Solutions\nDESCRIPTION: Shows how to use nested Pydantic models with OpenAI's beta parsing feature to structure step-by-step math solutions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n\nclient = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"dummy\")\ncompletion = client.beta.chat.completions.parse(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful expert math tutor.\"},\n        {\"role\": \"user\", \"content\": \"Solve 8x + 31 = 2.\"},\n    ],\n    response_format=MathResponse,\n    extra_body=dict(guided_decoding_backend=\"outlines\"),\n)\n\nmessage = completion.choices[0].message\nprint(message)\nassert message.parsed\nfor i, step in enumerate(message.parsed.steps):\n    print(f\"Step #{i}:\", step)\nprint(\"Answer:\", message.parsed.final_answer)\n```\n\n----------------------------------------\n\nTITLE: Using LLM.embed for Embedding Models in vLLM\nDESCRIPTION: Example showing how to generate embeddings from text using an embedding model. The embed method outputs a vector representation for each input prompt, suitable for semantic search and similarity tasks.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"intfloat/e5-mistral-7b-instruct\", task=\"embed\")\n(output,) = llm.embed(\"Hello, my name is\")\n\nembeds = output.outputs.embedding\nprint(f\"Embeddings: {embeds!r} (size={len(embeds)})\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Quantization Recipe Configuration for vLLM\nDESCRIPTION: Expanded configuration for the INT4 quantization recipe with tunable parameters for improved accuracy, including bit width, strategy, group size, and other quantization parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom compressed_tensors.quantization import (\n    QuantizationArgs,\n    QuantizationScheme,\n    QuantizationStrategy,\n    QuantizationType,\n) \nrecipe = GPTQModifier(\n    targets=\"Linear\",\n    config_groups={\n        \"config_group\": QuantizationScheme(\n            targets=[\"Linear\"],\n            weights=QuantizationArgs(\n                num_bits=4,\n                type=QuantizationType.INT,\n                strategy=QuantizationStrategy.GROUP,\n                group_size=128,\n                symmetric=True,\n                dynamic=False,\n                actorder=\"weight\",\n            ),\n        ),\n    },\n    ignore=[\"lm_head\"],\n    update_size=NUM_CALIBRATION_SAMPLES,\n    dampening_frac=0.01\n)\n```\n\n----------------------------------------\n\nTITLE: EAGLE-based Speculative Decoding in vLLM\nDESCRIPTION: Implementation of speculative decoding using EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) based draft models, which are specialized models designed for efficient token prediction.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    tensor_parallel_size=4,\n    speculative_config={\n        \"model\": \"yuhuili/EAGLE-LLaMA3-Instruct-8B\",\n        \"draft_tensor_parallel_size\": 1,\n    },\n)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n```\n\n----------------------------------------\n\nTITLE: vLLM Serve Command with Compilation Config\nDESCRIPTION: Command to run vLLM server with specific compilation configuration for batch sizes 1, 2, 4, and 8.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_USE_V1=1 vllm serve meta-llama/Llama-3.2-1B --compilation_config \"{'compile_sizes': [1, 2, 4, 8]}\"\n```\n\n----------------------------------------\n\nTITLE: Applying INT8 Quantization to Language Models\nDESCRIPTION: Code that applies INT8 quantization to a language model using SmoothQuant and GPTQ algorithms from the llmcompressor library, then saves the compressed model for use with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n\n# Configure the quantization algorithms\nrecipe = [\n    SmoothQuantModifier(smoothing_strength=0.8),\n    GPTQModifier(targets=\"Linear\", scheme=\"W8A8\", ignore=[\"lm_head\"]),\n]\n\n# Apply quantization\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save the compressed model\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W8A8-Dynamic-Per-Token\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization Parameters\nDESCRIPTION: Defines quantization configuration including FP8 specifications, global config, KV cache settings, and algorithm configuration for AutoSmoothQuant.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quark.torch.quantization import (Config, QuantizationConfig,\n                                     FP8E4M3PerTensorSpec,\n                                     load_quant_algo_config_from_file)\n\n# Define fp8/per-tensor/static spec.\nFP8_PER_TENSOR_SPEC = FP8E4M3PerTensorSpec(observer_method=\"min_max\",\n    is_dynamic=False).to_quantization_spec()\n\n# Define global quantization config, input tensors and weight apply FP8_PER_TENSOR_SPEC.\nglobal_quant_config = QuantizationConfig(input_tensors=FP8_PER_TENSOR_SPEC,\n    weight=FP8_PER_TENSOR_SPEC)\n\n# Define quantization config for kv-cache layers, output tensors apply FP8_PER_TENSOR_SPEC.\nKV_CACHE_SPEC = FP8_PER_TENSOR_SPEC\nkv_cache_layer_names_for_llama = [\"*k_proj\", \"*v_proj\"]\nkv_cache_quant_config = {name :\n    QuantizationConfig(input_tensors=global_quant_config.input_tensors,\n                       weight=global_quant_config.weight,\n                       output_tensors=KV_CACHE_SPEC)\n    for name in kv_cache_layer_names_for_llama}\nlayer_quant_config = kv_cache_quant_config.copy()\n\n# Define algorithm config by config file.\nLLAMA_AUTOSMOOTHQUANT_CONFIG_FILE =\n    'examples/torch/language_modeling/llm_ptq/models/llama/autosmoothquant_config.json'\nalgo_config = load_quant_algo_config_from_file(LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE)\n\nEXCLUDE_LAYERS = [\"lm_head\"]\nquant_config = Config(\n    global_quant_config=global_quant_config,\n    layer_quant_config=layer_quant_config,\n    kv_cache_quant_config=kv_cache_quant_config,\n    exclude=EXCLUDE_LAYERS,\n    algo_config=algo_config)\n```\n\n----------------------------------------\n\nTITLE: Quick Start with Online Dynamic Quantization in Python\nDESCRIPTION: Demonstrates how to dynamically quantize a model to FP8 precision with vLLM without requiring calibration data. This initializes an LLM with the fp8 quantization parameter and generates text.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nmodel = LLM(\"facebook/opt-125m\", quantization=\"fp8\")\n# INFO 06-10 17:55:42 model_runner.py:157] Loading model weights took 0.1550 GB\nresult = model.generate(\"Hello, my name is\")\n```\n\n----------------------------------------\n\nTITLE: Using LLM.score for Sentence Pair Scoring in vLLM\nDESCRIPTION: Example showing how to compute similarity scores between sentence pairs using a reranker model. This is useful for comparing query-document pairs in retrieval-augmented generation (RAG) systems.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"BAAI/bge-reranker-v2-m3\", task=\"score\")\n(output,) = llm.score(\"What is the capital of France?\",\n                      \"The capital of Brazil is Brasilia.\")\n\nscore = output.outputs.score\nprint(f\"Score: {score}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Prefill Trace for Qwen2.5-7B-Instruct Model in vLLM on TPU\nDESCRIPTION: This snippet demonstrates how to profile the prefill time and operations for the Qwen/Qwen2.5-7B-Instruct model with a single request of 1024 input tokens. It sets up environment variables and runs the profiling script with specific parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/profiling_tpu/README.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport XLA_HLO_DEBUG=1\nexport MODEL=Qwen/Qwen2.5-7B-Instruct\nexport VLLM_TPU_PROFILE_DURATION_MS=3000\nexport VLLM_TPU_PROFILE_DELAY_MS=0\n\npython3 profiling.py \\\n    --model $MODEL \\\n    --input-len 1024 --output-len 1 \\\n    --batch-size 1 --enforce-eager \\\n    --max-model-len 2048 \\\n    --tensor-parallel-size 1 \\\n    --profile-result-dir profiles\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM with CUDA 12.4\nDESCRIPTION: Basic installation commands for vLLM using pip or uv package managers with CUDA 12.4 support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# Install vLLM with CUDA 12.4.\npip install vllm # If you are using pip.\nuv pip install vllm # If you are using uv.\n```\n\n----------------------------------------\n\nTITLE: Benchmarking with PyTorch Profiler Enabled\nDESCRIPTION: Command to run the benchmark_serving.py script with profiling enabled. It uses the vLLM backend with a specified model and dataset while limiting to 2 prompts.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/benchmark_serving.py --backend vllm --model meta-llama/Meta-Llama-3-70B --dataset-name sharegpt --dataset-path sharegpt.json --profile --num-prompts 2\n```\n\n----------------------------------------\n\nTITLE: Computing Image Feature Grid Size for Fuyu\nDESCRIPTION: Helper function to calculate the number of columns and rows for image feature tokens based on image dimensions and patch sizes.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef get_image_feature_grid_size(\n    self,\n    *,\n    image_width: int,\n    image_height: int,\n) -> tuple[int, int]:\n    image_processor = self.get_image_processor()\n    target_width = image_processor.size[\"width\"]\n    target_height = image_processor.size[\"height\"]\n    patch_width = image_processor.patch_size[\"width\"]\n    patch_height = image_processor.patch_size[\"height\"]\n\n    if not (image_width <= target_width and image_height <= target_height):\n        height_scale_factor = target_height / image_height\n        width_scale_factor = target_width / image_width\n        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n\n        image_height = int(image_height * optimal_scale_factor)\n        image_width = int(image_width * optimal_scale_factor)\n\n    ncols = math.ceil(image_width / patch_width)\n    nrows = math.ceil(image_height / patch_height)\n    return ncols, nrows\n```\n\n----------------------------------------\n\nTITLE: Performing Beam Search with vLLM LLM Class\nDESCRIPTION: Shows how to use the LLM.beam_search method to perform beam search with the OPT-125M model. It demonstrates setting beam width and maximum tokens for generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/generative_models.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nfrom vllm.sampling_params import BeamSearchParams\n\nllm = LLM(model=\"facebook/opt-125m\")\nparams = BeamSearchParams(beam_width=5, max_tokens=50)\noutputs = llm.beam_search([{\"prompt\": \"Hello, my name is \"}], params)\n\nfor output in outputs:\n    generated_text = output.sequences[0].text\n    print(f\"Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing In-Flight 4-bit Quantization in vLLM\nDESCRIPTION: Python code to perform in-flight 4-bit quantization using BitsAndBytes with vLLM. This requires explicitly specifying the 'quantization' parameter when creating the LLM instance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/bnb.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nimport torch\nmodel_id = \"huggyllama/llama-7b\"\nllm = LLM(model=model_id, dtype=torch.bfloat16, trust_remote_code=True, \\\nquantization=\"bitsandbytes\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Tool Calling with OpenAI Client\nDESCRIPTION: Python implementation showing how to set up tool calling for a weather function using the OpenAI client, including function definition, tool configuration, and API request handling.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport json\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n\ndef get_weather(location: str, unit: str):\n    return f\"Getting the weather for {location} in {unit}...\"\ntool_functions = {\"get_weather\": get_weather}\n\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            },\n            \"required\": [\"location\", \"unit\"]\n        }\n    }\n}]\n\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\ntool_call = response.choices[0].message.tool_calls[0].function\nprint(f\"Function called: {tool_call.name}\")\nprint(f\"Arguments: {tool_call.arguments}\")\nprint(f\"Result: {get_weather(**json.loads(tool_call.arguments))}\")\n```\n\n----------------------------------------\n\nTITLE: Using AWQ models with vLLM's Python API\nDESCRIPTION: This Python script shows how to use vLLM's LLM class to load and generate text with an AWQ-quantized model. It demonstrates setting up sampling parameters, creating an LLM instance, and generating text from multiple prompts.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/auto_awq.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Create an LLM.\nllm = LLM(model=\"TheBloke/Llama-2-7b-Chat-AWQ\", quantization=\"AWQ\")\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Running vLLM in NVIDIA PyTorch Docker Container\nDESCRIPTION: Command to run NVIDIA's PyTorch Docker image with GPU access and increased shared memory allocation. This provides a pre-configured environment for building vLLM, avoiding common installation issues.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n# Use `--ipc=host` to make sure the shared memory is large enough.\ndocker run --gpus all -it --rm --ipc=host nvcr.io/nvidia/pytorch:23.10-py3\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Generation with Pydantic\nDESCRIPTION: Shows how to generate structured JSON output using a Pydantic model to define the schema for car descriptions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom enum import Enum\n\nclass CarType(str, Enum):\n    sedan = \"sedan\"\n    suv = \"SUV\"\n    truck = \"Truck\"\n    coupe = \"Coupe\"\n\n\nclass CarDescription(BaseModel):\n    brand: str\n    model: str\n    car_type: CarType\n\n\njson_schema = CarDescription.model_json_schema()\n\ncompletion = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-3B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n        }\n    ],\n    extra_body={\"guided_json\": json_schema},\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Service for vLLM\nDESCRIPTION: This YAML snippet creates a Kubernetes Service to expose the vLLM deployment. It maps port 80 to the container port 8000 and uses the app label for selector matching.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: mistral-7b\n  namespace: default\nspec:\n  ports:\n  - name: http-mistral-7b\n    port: 80\n    protocol: TCP\n    targetPort: 8000\n  selector:\n    app: mistral-7b\n  sessionAffinity: None\n  type: ClusterIP\n```\n\n----------------------------------------\n\nTITLE: Building vLLM Docker Image for MI200/MI300 Series\nDESCRIPTION: Command to build the vLLM Docker image for AMD MI200 and MI300 series GPUs with ROCm 6.3 support. Uses the pre-built base image by default.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build -f docker/Dockerfile.rocm -t vllm-rocm .\n```\n\n----------------------------------------\n\nTITLE: Creating Model-specific Multi-modal Processing Tests\nDESCRIPTION: Create a dedicated test file for model-specific testing, such as verifying that keyword arguments from the HuggingFace processor are correctly applied in the vLLM implementation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/tests.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example: tests/models/multimodal/processing/test_phi3v.py\n# (No actual code snippet provided in the text)\n```\n\n----------------------------------------\n\nTITLE: Creating PVC and Secret for Hugging Face Model in Kubernetes\nDESCRIPTION: This snippet demonstrates how to create a Kubernetes PersistentVolumeClaim for storing Hugging Face models and a Secret for storing the Hugging Face token.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncat <<EOF |kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vllm-models\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 50Gi\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hf-token-secret\ntype: Opaque\ndata:\n  token: $(HF_TOKEN)\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM for Multi-GPU Inference in LlamaIndex\nDESCRIPTION: Example of initializing the Vllm class from LlamaIndex with distributed inference across multiple GPUs. The code shows how to specify the model, configure tensor parallelism, and set additional parameters for memory management.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/integrations/llamaindex.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.vllm import Vllm\n\nllm = Vllm(\n    model=\"microsoft/Orca-2-7b\",\n    tensor_parallel_size=4,\n    max_new_tokens=100,\n    vllm_kwargs={\"swap_space\": 1, \"gpu_memory_utilization\": 0.5},\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Chat Completion Sampling Parameters in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines the sampling parameters supported by the Chat Completion API in vLLM. These parameters control how the model generates text responses, including temperature, repetition penalties, and beam search settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# begin-chat-completion-sampling-params\n# end-chat-completion-sampling-params\n```\n\n----------------------------------------\n\nTITLE: Building vLLM from Source with CUDA Support\nDESCRIPTION: Commands to clone the vLLM repository, install build dependencies, and build the package from source with CUDA support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running vLLM through LangChain\nDESCRIPTION: Example showing how to initialize the VLLM class from LangChain and run inference with various parameters including model selection, token generation settings, and sampling parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/integrations/langchain.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import VLLM\n\nllm = VLLM(model=\"mosaicml/mpt-7b\",\n           trust_remote_code=True,  # mandatory for hf models\n           max_new_tokens=128,\n           top_k=10,\n           top_p=0.95,\n           temperature=0.8,\n           # tensor_parallel_size=... # for distributed inference\n)\n\nprint(llm(\"What is the capital of France ?\"))\n```\n\n----------------------------------------\n\nTITLE: Building vLLM from Source for Intel Gaudi\nDESCRIPTION: Steps to clone the vLLM repository, install dependencies, and build the package from source for use with Intel Gaudi devices.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -r requirements/hpu.txt\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Model Plugin in Python\nDESCRIPTION: This code snippet demonstrates how to create a plugin for vLLM that registers a custom model. It includes the setup.py configuration for entry points and the actual plugin implementation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/plugin_system.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# inside `setup.py` file\nfrom setuptools import setup\n\nsetup(name='vllm_add_dummy_model',\n      version='0.1',\n      packages=['vllm_add_dummy_model'],\n      entry_points={\n          'vllm.general_plugins':\n          [\"register_dummy_model = vllm_add_dummy_model:register\"]\n      })\n\n# inside `vllm_add_dummy_model.py` file\ndef register():\n    from vllm import ModelRegistry\n\n    if \"MyLlava\" not in ModelRegistry.get_supported_archs():\n        ModelRegistry.register_model(\"MyLlava\",\n                                        \"vllm_add_dummy_model.my_llava:MyLlava\")\n```\n\n----------------------------------------\n\nTITLE: Cloning vLLM Repository\nDESCRIPTION: Command to clone the vLLM GitHub repository and navigate to its directory, providing the source code needed for building vLLM from source.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git && cd vllm\n```\n\n----------------------------------------\n\nTITLE: Applying FP8 Quantization with llm-compressor\nDESCRIPTION: Demonstrates how to apply FP8 dynamic quantization to a model using the llm-compressor library. This quantizes Linear layers with static per-channel quantization for weights and dynamic per-token quantization for activations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\n\n# Configure the simple PTQ quantization\nrecipe = QuantizationModifier(\n  targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n\n# Apply the quantization algorithm.\noneshot(model=model, recipe=recipe)\n\n# Save the model.\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-Dynamic\"\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Prefix Caching in vLLM with Long-form Text Processing\nDESCRIPTION: Example implementation showing how to enable and use Automatic Prefix Caching in vLLM for processing long-form text queries. The code demonstrates performance benefits when querying the same document multiple times with different questions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/automatic_prefix_caching.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom vllm import LLM, SamplingParams\n\n\n# A prompt containing a large markdown table. The table is randomly generated by GPT-4.\nLONG_PROMPT = \"You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\\n# Table\\n\" + \"\"\"\n| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n|-----|---------------|-----|---------------|---------------|------------------------|----------------|-------------------------------|\n| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|\n| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |\n| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |\n| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |\n| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |\n| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |\n| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |\n| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |\n| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |\n| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |\n| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |\n| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |\n| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |\n| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|\n| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |\n| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |\n| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |\n| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |\n| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |\n| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |\n| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |\n\"\"\"\n\n\ndef get_generation_time(llm, sampling_params, prompts):\n    # time the generation\n    start_time = time.time()\n    output = llm.generate(prompts, sampling_params=sampling_params)\n    end_time = time.time()\n    # print the output and generation time\n    print(f\"Output: {output[0].outputs[0].text}\")\n    print(f\"Generation time: {end_time - start_time} seconds.\")\n\n\n# set enable_prefix_caching=True to enable APC\nllm = LLM(\n    model='lmsys/longchat-13b-16k',\n    enable_prefix_caching=True\n)\n\nsampling_params = SamplingParams(temperature=0, max_tokens=100)\n\n# Querying the age of John Doe\nget_generation_time(\n    llm,\n    sampling_params,\n    LONG_PROMPT + \"Question: what is the age of John Doe? Your answer: The age of John Doe is \",\n)\n\n# Querying the age of Zack Blue\n# This query will be faster since vllm avoids computing the KV cache of LONG_PROMPT again.\nget_generation_time(\n    llm,\n    sampling_params,\n    LONG_PROMPT + \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \",\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM with NVIDIA GPU in Kubernetes\nDESCRIPTION: This YAML snippet creates a Kubernetes Deployment for vLLM using NVIDIA GPUs. It deploys the Mistral-7B-Instruct-v0.3 model, mounts necessary volumes, and sets resource limits for CPU, memory, and GPU.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mistral-7b\n  namespace: default\n  labels:\n    app: mistral-7b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mistral-7b\n  template:\n    metadata:\n      labels:\n        app: mistral-7b\n    spec:\n      volumes:\n      - name: cache-volume\n        persistentVolumeClaim:\n          claimName: mistral-7b\n      - name: shm\n        emptyDir:\n          medium: Memory\n          sizeLimit: \"2Gi\"\n      containers:\n      - name: mistral-7b\n        image: vllm/vllm-openai:latest\n        command: [\"/bin/sh\", \"-c\"]\n        args: [\n          \"vllm serve mistralai/Mistral-7B-Instruct-v0.3 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024\"\n        ]\n        env:\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hf-token-secret\n              key: token\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            cpu: \"10\"\n            memory: 20G\n            nvidia.com/gpu: \"1\"\n          requests:\n            cpu: \"2\"\n            memory: 6G\n            nvidia.com/gpu: \"1\"\n        volumeMounts:\n        - mountPath: /root/.cache/huggingface\n          name: cache-volume\n        - name: shm\n          mountPath: /dev/shm\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 5\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM with Multiple Images Per Prompt in Python\nDESCRIPTION: Example of initializing an LLM object with support for multiple images per prompt by setting the limit_mm_per_prompt parameter to allow up to 4 images.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(\n    model=\"Qwen/Qwen2-VL-7B-Instruct\",\n    limit_mm_per_prompt={\"image\": 4},\n)\n```\n\n----------------------------------------\n\nTITLE: Quantizing a Mistral-7B model using AutoAWQ in Python\nDESCRIPTION: This script demonstrates how to quantize the 'mistralai/Mistral-7B-Instruct-v0.2' model using AutoAWQ. It loads the model, sets quantization parameters, performs the quantization, and saves the quantized model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/auto_awq.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-Instruct-v0.2'\nquant_path = 'mistral-instruct-v0.2-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency for Run:ai Streamer\nDESCRIPTION: Command to start vLLM server with custom concurrency settings for tensor loading, controlling the number of OS threads or S3 client instances.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/runai_model_streamer.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nvllm serve /home/meta-llama/Llama-3.2-3B-Instruct --load-format runai_streamer --model-loader-extra-config '{\"concurrency\":16}'\n```\n\n----------------------------------------\n\nTITLE: Finding Image Size with Maximum Features for Multi-Modal Models\nDESCRIPTION: Helper method to determine the maximum possible dimensions of an image before being converted into patches. It retrieves the target size from the image processor configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef get_image_size_with_most_features(self) -> ImageSize:\n    image_processor = self.get_image_processor()\n    return ImageSize(width=image_processor.size[\"width\"],\n                        height=image_processor.size[\"height\"])\n\ndef get_max_image_tokens(self) -> int:\n    target_width, target_height = self.get_image_size_with_most_features()\n\n    return self.get_num_image_tokens(\n        image_width=target_width,\n        image_height=target_height,\n    )\n```\n\n----------------------------------------\n\nTITLE: Building vLLM Docker Image from Source\nDESCRIPTION: Docker commands to build a vLLM image from source, including the build context and Dockerfile location.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\ndocker build -t vllm-img .\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM using Conda\nDESCRIPTION: Commands to create a Python environment using Conda, activate it, and install vLLM, providing an alternative installation method.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nconda create -n myenv python=3.12 -y\nconda activate myenv\npip install vllm\n```\n\n----------------------------------------\n\nTITLE: Expected Model Loading Log Output\nDESCRIPTION: Example log output showing successful model loading across distributed workers, confirming that the tensor-parallel setup is functioning correctly.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nINFO 05-08 03:20:24 model_runner.py:173] Loading model weights took 0.1189 GB\n(RayWorkerWrapper pid=169, ip=10.20.0.197) INFO 05-08 03:20:28 model_runner.py:173] Loading model weights took 0.1189 GB\n```\n\n----------------------------------------\n\nTITLE: Performing Mixed Precision GEMM Operation with Machete in Python\nDESCRIPTION: This snippet demonstrates the core operation performed by Machete, involving quantized weights, scales, and zeropoints. It shows how the GEMM operation is calculated using different data types for optimal performance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/machete/Readme.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nscale_type = w_s.dtype\ncompute_type = a.dtype\nout = (w_q.to(scale_type) * w_s - w_z.to(scale_type)) @ a\n```\n\n----------------------------------------\n\nTITLE: Using ModelScope Models in vLLM\nDESCRIPTION: This snippet demonstrates how to use models from ModelScope in vLLM. It sets an environment variable and initializes an LLM object with the necessary parameters for ModelScope support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=..., revision=..., task=..., trust_remote_code=True)\n\n# For generative models (task=generate) only\noutput = llm.generate(\"Hello, my name is\")\nprint(output)\n\n# For pooling models (task={embed,classify,reward,score}) only\noutput = llm.encode(\"Hello, my name is\")\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: N-gram Based Speculative Decoding in vLLM\nDESCRIPTION: Code example showing how to configure vLLM to use speculative decoding where token proposals are generated by matching n-grams in the prompt, with a maximum of 5 speculative tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\n    model=\"facebook/opt-6.7b\",\n    tensor_parallel_size=1,\n    speculative_config={\n        \"method\": \"ngram\",\n        \"num_speculative_tokens\": 5,\n        \"prompt_lookup_max\": 4,\n    },\n)\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Head Node Container in a Ray Cluster\nDESCRIPTION: Command to start the head node container in a Ray cluster for vLLM multi-node configuration. This establishes the primary node that worker nodes will connect to.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/distributed_serving.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nbash run_cluster.sh \\\n                vllm/vllm-openai \\\n                ip_of_head_node \\\n                --head \\\n                /path/to/the/huggingface/home/in/this/node \\\n                -e VLLM_HOST_IP=ip_of_this_node\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client Integration for Multimodal\nDESCRIPTION: Example showing how to use the OpenAI client to interact with the vLLM server for multimodal inference with single and multiple images.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n# Single-image input inference\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\nchat_response = client.chat.completions.create(\n    model=\"microsoft/Phi-3.5-vision-instruct\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    }],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Transformers-Compatible Model for vLLM\nDESCRIPTION: This code snippet shows how to implement a custom model that is compatible with the Transformers backend in vLLM. It includes a custom attention module and a model class that supports the attention backend.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import PreTrainedModel\nfrom torch import nn\n\nclass MyAttention(nn.Module):\n\n  def forward(self, hidden_states, **kwargs): # <- kwargs are required\n    ...\n    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n    attn_output, attn_weights = attention_interface(\n      self,\n      query_states,\n      key_states,\n      value_states,\n      **kwargs,\n    )\n    ...\n\nclass MyModel(PreTrainedModel):\n  _supports_attention_backend = True\n```\n\n----------------------------------------\n\nTITLE: Defining Extra Parameters for Chat Completion in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines additional parameters supported by the Chat Completion API beyond standard sampling parameters. These extra options provide further control over model response generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# begin-chat-completion-extra-params\n# end-chat-completion-extra-params\n```\n\n----------------------------------------\n\nTITLE: Running vLLM with Debug Logging\nDESCRIPTION: Command to run a Llama model using vLLM's V1 architecture with debug level logging enabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_USE_V1=1 VLLM_LOGGING_LEVEL=DEBUG vllm serve meta-llama/Llama-3.2-1B\n```\n\n----------------------------------------\n\nTITLE: Loading Model and Tokenizer for Quantization in vLLM\nDESCRIPTION: Code to load a model and tokenizer using transformers AutoModel classes as the first step in the quantization process.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Worker Node Container in a Ray Cluster\nDESCRIPTION: Command to start a worker node container that connects to the head node in a vLLM Ray cluster. This adds computational resources to the distributed system.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/distributed_serving.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nbash run_cluster.sh \\\n                vllm/vllm-openai \\\n                ip_of_head_node \\\n                --worker \\\n                /path/to/the/huggingface/home/in/this/node \\\n                -e VLLM_HOST_IP=ip_of_this_node\n```\n\n----------------------------------------\n\nTITLE: Loading Model for INT8 Quantization in vLLM\nDESCRIPTION: Code to load a pre-trained language model and its tokenizer using the Transformers library as the first step in the quantization process.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```\n\n----------------------------------------\n\nTITLE: Sending Completion Request\nDESCRIPTION: Curl command to send a completion request to the OpenAI-compatible API endpoint.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:30080/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"facebook/opt-125m\",\n    \"prompt\": \"Once upon a time,\",\n    \"max_tokens\": 10\n  }'\n```\n\n----------------------------------------\n\nTITLE: Applying INT4 Quantization in vLLM with llm-compressor\nDESCRIPTION: Code to apply INT4 W4A16 quantization to a model using the llm-compressor library, configuring the quantization algorithm, and saving the compressed model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n\n# Configure the quantization algorithms\nrecipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n\n# Apply quantization\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save the compressed model\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W4A16-G128\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n\n----------------------------------------\n\nTITLE: Modifying Forward Method for vLLM Compatibility in Python\nDESCRIPTION: Demonstrates how to rewrite the forward method of the model to remove unnecessary code and modify input parameters. It treats input_ids and positions as flattened tensors with a single batch size dimension.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/basic.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    positions: torch.Tensor,\n) -> torch.Tensor:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Running vLLM with Pure Tensor Parallelism Across Multiple Nodes\nDESCRIPTION: Command to serve a model using vLLM with only tensor parallelism spanning all GPUs across all nodes. This configuration requires efficient inter-node communication like Infiniband for good performance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/distributed_serving.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nvllm serve /path/to/the/model/in/the/container \\\n     --tensor-parallel-size 16\n```\n\n----------------------------------------\n\nTITLE: vLLM Serve Command with CUDA Graph Configuration\nDESCRIPTION: Command to run vLLM server with specific CUDA graph capture sizes configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_USE_V1=1 vllm serve meta-llama/Llama-3.2-1B --compilation_config \"{'cudagraph_capture_sizes': [1, 2, 4, 8]}\"\n```\n\n----------------------------------------\n\nTITLE: Multiple Image Inference with Phi-3.5\nDESCRIPTION: Shows how to handle multiple images in a single prompt using Phi-3.5-vision model. Includes configuration for maximum model length and image limits.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(\n    model=\"microsoft/Phi-3.5-vision-instruct\",\n    trust_remote_code=True,  # Required to load Phi-3.5-vision\n    max_model_len=4096,  # Otherwise, it may not fit in smaller GPUs\n    limit_mm_per_prompt={\"image\": 2},  # The maximum number to accept\n)\n\n# Refer to the HuggingFace repo for the correct format to use\nprompt = \"<|user|>\\n<|image_1|>\\n<|image_2|>\\nWhat is the content of each image?<|end|>\\n<|assistant|>\\n\"\n\n# Load the images using PIL.Image\nimage1 = PIL.Image.open(...)\nimage2 = PIL.Image.open(...)\n\noutputs = llm.generate({\n    \"prompt\": prompt,\n    \"multi_modal_data\": {\n        \"image\": [image1, image2]\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Tensorizer Package for vLLM\nDESCRIPTION: Command to install the tensorizer package as an optional dependency for vLLM using pip.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/tensorizer.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install vllm[tensorizer]\n```\n\n----------------------------------------\n\nTITLE: Calculating Total Number of Image Tokens for Fuyu Model\nDESCRIPTION: Function to compute the total number of tokens needed to represent an image based on its dimensions. It handles image resizing to fit target dimensions and calculates the number of rows and columns of patches required.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_image_tokens(\n    self,\n    *,\n    image_width: int,\n    image_height: int,\n) -> int:\n    image_processor = self.get_image_processor()\n    target_width = image_processor.size[\"width\"]\n    target_height = image_processor.size[\"height\"]\n    patch_width = image_processor.patch_size[\"width\"]\n    patch_height = image_processor.patch_size[\"height\"]\n\n    if not (image_width <= target_width and image_height <= target_height):\n        height_scale_factor = target_height / image_height\n        width_scale_factor = target_width / image_width\n        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n\n        image_height = int(image_height * optimal_scale_factor)\n        image_width = int(image_width * optimal_scale_factor)\n\n    ncols = math.ceil(image_width / patch_width)\n    nrows = math.ceil(image_height / patch_height)\n    return (ncols + 1) * nrows\n```\n\n----------------------------------------\n\nTITLE: Installing AMD Quark Package\nDESCRIPTION: Command to install the AMD Quark quantization toolkit using pip package manager.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install amd-quark\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantized Model with vLLM\nDESCRIPTION: Demonstrates how to load and evaluate the quantized model using vLLM's LLM class for text generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Create an LLM.\nllm = LLM(model=\"Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant\",\n          kv_cache_dtype='fp8',quantization='quark')\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nprint(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt:    {prompt!r}\")\n    print(f\"Output:    {generated_text!r}\")\n    print(\"-\" * 60)\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM with AMD GPU in Kubernetes\nDESCRIPTION: This YAML snippet creates a Kubernetes Deployment for vLLM using AMD ROCm GPUs like MI300X. It deploys the Mistral-7B-v0.3 model, sets up necessary volumes, and configures resource limits for CPU, memory, and AMD GPU.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mistral-7b\n  namespace: default\n  labels:\n    app: mistral-7b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mistral-7b\n  template:\n    metadata:\n      labels:\n        app: mistral-7b\n    spec:\n      volumes:\n      - name: cache-volume\n        persistentVolumeClaim:\n          claimName: mistral-7b\n      - name: shm\n        emptyDir:\n          medium: Memory\n          sizeLimit: \"8Gi\"\n      hostNetwork: true\n      hostIPC: true\n      containers:\n      - name: mistral-7b\n        image: rocm/vllm:rocm6.2_mi300_ubuntu20.04_py3.9_vllm_0.6.4\n        securityContext:\n          seccompProfile:\n            type: Unconfined\n          runAsGroup: 44\n          capabilities:\n            add:\n            - SYS_PTRACE\n        command: [\"/bin/sh\", \"-c\"]\n        args: [\n          \"vllm serve mistralai/Mistral-7B-v0.3 --port 8000 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024\"\n        ]\n        env:\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hf-token-secret\n              key: token\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            cpu: \"10\"\n            memory: 20G\n            amd.com/gpu: \"1\"\n          requests:\n            cpu: \"6\"\n            memory: 6G\n            amd.com/gpu: \"1\"\n        volumeMounts:\n        - name: cache-volume\n          mountPath: /root/.cache/huggingface\n        - name: shm\n          mountPath: /dev/shm\n```\n\n----------------------------------------\n\nTITLE: Defining Score Extra Parameters in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines additional parameters supported by the Score API. These parameters provide extra control over the scoring process for text pair similarity.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# begin-score-extra-params\n# end-score-extra-params\n```\n\n----------------------------------------\n\nTITLE: Running vLLM with Pipeline Parallelism Across Multiple Nodes\nDESCRIPTION: Command to serve a model using vLLM with both tensor parallelism and pipeline parallelism. This example configures 8-way tensor parallelism within each node and 2-way pipeline parallelism across nodes.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/distributed_serving.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n vllm serve /path/to/the/model/in/the/container \\\n     --tensor-parallel-size 8 \\\n     --pipeline-parallel-size 2\n```\n\n----------------------------------------\n\nTITLE: Loading a Model for FP8 Quantization with Transformers\nDESCRIPTION: Code to load a pre-trained model and tokenizer using the Transformers library's AutoModel classes, preparing it for FP8 quantization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```\n\n----------------------------------------\n\nTITLE: Pseudocode for Query-Key Dot Product in vLLM Attention CUDA Kernel\nDESCRIPTION: Illustrates the high-level flow of query-key dot product calculation. It shows how query data is fetched once and then used for multiple iterations over different key tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nq_vecs = ...\nfor ... {\n   k_ptr = ...\n   for ... {\n      k_vecs[i] = ...\n   }\n   ...\n   float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(q_vecs[thread_group_offset], k_vecs);\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantized Model Accuracy with lm_eval\nDESCRIPTION: Command to evaluate the accuracy of a quantized model using lm_eval, with parameters for the model path, task, and other evaluation settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ lm_eval --model vllm \\\n  --model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W4A16-G128\",add_bos_token=true \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --limit 250 \\\n  --batch_size 'auto'\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies Dockerfile\nDESCRIPTION: Dockerfile example for installing additional optional dependencies on top of the base vLLM image.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM vllm/vllm-openai:v0.8.2\n\n# e.g. install the `audio` and `video` optional dependencies\n# NOTE: Make sure the version of vLLM matches the base image!\nRUN uv pip install --system vllm[audio,video]==0.8.2\n```\n\n----------------------------------------\n\nTITLE: Using Pre-computed Image Embeddings with OpenAI Client\nDESCRIPTION: Python code demonstrating how to pass pre-computed image embeddings directly to language models through the OpenAI client API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimage_embedding = torch.load(...)\ngrid_thw = torch.load(...) # Required by Qwen/Qwen2-VL-2B-Instruct\n\nbuffer = io.BytesIO()\ntorch.save(image_embedding, buffer)\nbuffer.seek(0)\nbinary_data = buffer.read()\nbase64_image_embedding = base64.b64encode(binary_data).decode('utf-8')\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n# Basic usage - this is equivalent to the LLaVA example for offline inference\nmodel = \"llava-hf/llava-1.5-7b-hf\"\nembeds =  {\n    \"type\": \"image_embeds\",\n    \"image_embeds\": f\"{base64_image_embedding}\" \n}\n\n# Pass additional parameters (available to Qwen2-VL and MiniCPM-V)\nmodel = \"Qwen/Qwen2-VL-2B-Instruct\"\nembeds =  {\n    \"type\": \"image_embeds\",\n    \"image_embeds\": {\n        \"image_embeds\": f\"{base64_image_embedding}\" , # Required\n        \"image_grid_thw\": f\"{base64_image_grid_thw}\"  # Required by Qwen/Qwen2-VL-2B-Instruct\n    },\n}\nmodel = \"openbmb/MiniCPM-V-2_6\"\nembeds =  {\n    \"type\": \"image_embeds\",\n    \"image_embeds\": {\n        \"image_embeds\": f\"{base64_image_embedding}\" , # Required\n        \"image_sizes\": f\"{base64_image_sizes}\"  # Required by openbmb/MiniCPM-V-2_6\n    },\n}\nchat_completion = client.chat.completions.create(\n    messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"What's in this image?\",\n        },\n        embeds,\n        ],\n    },\n],\n    model=model,\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM Server as Kubernetes Deployment and Service\nDESCRIPTION: This snippet shows how to create a Kubernetes Deployment for the vLLM server and expose it as a Service. It uses the vllm/vllm-openai:latest image and mounts the previously created PVC and Secret.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncat <<EOF |kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vllm\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vllm\n    spec:\n      containers:\n      - name: vllm\n        image: vllm/vllm-openai:latest\n        command: [\"/bin/sh\", \"-c\"]\n        args: [\n          \"vllm serve meta-llama/Llama-3.2-1B-Instruct\"\n        ]\n        env:\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hf-token-secret\n              key: token\n        ports:\n          - containerPort: 8000\n        volumeMounts:\n          - name: llama-storage\n            mountPath: /root/.cache/huggingface\n      volumes:\n      - name: llama-storage\n        persistentVolumeClaim:\n          claimName: vllm-models\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-server\nspec:\n  selector:\n    app.kubernetes.io/name: vllm\n  ports:\n  - protocol: TCP\n    port: 8000\n    targetPort: 8000\n  type: ClusterIP\nEOF\n```\n\n----------------------------------------\n\nTITLE: Using Machete API for Prepacking and GEMM Operation in Python\nDESCRIPTION: This code snippet illustrates how to use the Machete API to prepack the weight matrix and perform the GEMM operation. It demonstrates the two-step process of prepacking and then executing the GEMM, which is optimized for tensor core layouts.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/machete/Readme.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import _custom_ops as ops\n\n...\nW_q_packed = ops.machete_prepack_B(w_q, wtype)\noutput = ops.machete_gemm(\n    a,\n    b_q=W_q_packed,\n    b_type=wtype,\n    b_scales=w_s,\n    b_group_size=group_size\n)\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing Implementation in v1 LLMEngine\nDESCRIPTION: Code references showing how the v1 LLMEngine creates a new process to run the engine core when multiprocessing is enabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/llm_engine.py#L93-L95\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/llm_engine.py#L70-L77\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/core_client.py#L44-L45\n```\n\n----------------------------------------\n\nTITLE: Quantizing LLaMA Model with GPTQModel\nDESCRIPTION: Example of quantizing a LLaMA model using GPTQModel with 4-bit precision. Shows the complete process including dataset loading, configuration, and model quantization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gptqmodel.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom gptqmodel import GPTQModel, QuantizeConfig\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nquant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\"\n\ncalibration_dataset = load_dataset(\n    \"allenai/c4\",\n    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n    split=\"train\"\n  ).select(range(1024))[\"text\"]\n\nquant_config = QuantizeConfig(bits=4, group_size=128)\n\nmodel = GPTQModel.load(model_id, quant_config)\n\n# increase `batch_size` to match gpu/vram specs to speed up quantization\nmodel.quantize(calibration_dataset, batch_size=2)\n\nmodel.save(quant_path)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for vLLM XPU Backend\nDESCRIPTION: Builds a Docker image for the vLLM XPU backend environment. This command should be run from the root of the vLLM project directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ docker build -f docker/Dockerfile.xpu -t vllm-xpu-env --shm-size=4g .\n```\n\n----------------------------------------\n\nTITLE: Using Python cProfile Context Manager for Code Block Profiling\nDESCRIPTION: Example of using vLLM's cProfile context manager utility to profile a specific block of code, with profile results saved to a specified file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport vllm.utils\n\ndef another_function():\n    # more expensive code\n    pass\n\nwith vllm.utils.cprofile_context(\"another_function.prof\"):\n    another_function()\n```\n\n----------------------------------------\n\nTITLE: Building vLLM from Source on macOS\nDESCRIPTION: Commands to clone the vLLM repository and build it from source using pip. This process requires XCode and Command Line Tools to be pre-installed.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/apple.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -r requirements/cpu.txt\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Reasoning Model\nDESCRIPTION: Python code showing how to use structured output with a reasoning model, including setting up the model and parsing the response.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\nclass People(BaseModel):\n    name: str\n    age: int\n\n\njson_schema = People.model_json_schema()\n\nprompt = (\"Generate a JSON with the name and age of one random person.\")\ncompletion = client.chat.completions.create(\n    model=model,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": prompt,\n    }],\n    extra_body={\"guided_json\": json_schema},\n)\nprint(\"reasoning_content: \", completion.choices[0].message.reasoning_content)\nprint(\"content: \", completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for vLLM Multiprocessing Method\nDESCRIPTION: Code reference showing where the VLLM_WORKER_MULTIPROC_METHOD environment variable is defined, with 'fork' as the default method.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/envs.py#L339-L342\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Processor Inputs for Fuyu Model Profiling\nDESCRIPTION: Implementation of dummy input generation for memory profiling in Fuyu model. Unlike LLaVA, it uses an empty prompt text since Fuyu doesn't expect image placeholders in the inputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef get_dummy_processor_inputs(\n    self,\n    seq_len: int,\n    mm_counts: Mapping[str, int],\n) -> ProcessorInputs:\n    target_width, target_height = \\\n        self.info.get_image_size_with_most_features()\n    num_images = mm_counts.get(\"image\", 0)\n\n    mm_data = {\n        \"image\":\n        self._get_dummy_images(width=target_width,\n                                height=target_height,\n                                num_images=num_images)\n    }\n\n    return ProcessorInputs(\n        prompt_text=\"\",\n        mm_data=mm_data,\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling Reasoning Functionality via Command-Line in vLLM\nDESCRIPTION: Bash command demonstrating how to enable reasoning functionality when serving a model with vLLM. It uses the --enable-reasoning flag along with specifying the custom reasoning parser.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve <model_tag> \\\n    --enable-reasoning --reasoning-parser example\n```\n\n----------------------------------------\n\nTITLE: Downloading and Serving GGUF Model via Command Line\nDESCRIPTION: Example showing how to download a TinyLlama GGUF model from Hugging Face and serve it using vLLM's command-line interface, with a recommendation to use the base model's tokenizer.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gguf.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nwget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n# We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion.\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0\n```\n\n----------------------------------------\n\nTITLE: Installing Intel Extension for PyTorch\nDESCRIPTION: Installs the Intel Extension for PyTorch, which is necessary due to a known conflict in dependency versions. This step is temporary and will be fixed in future versions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip install intel-extension-for-pytorch==2.6.10+xpu \\\n    --extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n```\n\n----------------------------------------\n\nTITLE: Verifying Intel Gaudi Software Installation\nDESCRIPTION: Commands to verify the correct installation of Intel Gaudi software components, including checking for required packages and Python modules.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhl-smi # verify that hl-smi is in your PATH and each Gaudi accelerator is visible\napt list --installed | grep habana # verify that habanalabs-firmware-tools, habanalabs-graph, habanalabs-rdma-core, habanalabs-thunk and habanalabs-container-runtime are installed\npip list | grep habana # verify that habana-torch-plugin, habana-torch-dataloader, habana-pyhlml and habana-media-loader are installed\npip list | grep neural # verify that neural_compressor is installed\n```\n\n----------------------------------------\n\nTITLE: Building vLLM for Arm64/GH200\nDESCRIPTION: Example command for building vLLM Docker image specifically for Nvidia Grace-Hopper (Arm64) architecture with optimized build parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npython3 use_existing_torch.py\nDOCKER_BUILDKIT=1 docker build . \\\n  --file docker/Dockerfile \\\n  --target vllm-openai \\\n  --platform \"linux/arm64\" \\\n  -t vllm/vllm-gh200-openai:latest \\\n  --build-arg max_jobs=66 \\\n  --build-arg nvcc_threads=2 \\\n  --build-arg torch_cuda_arch_list=\"9.0+PTX\" \\\n  --build-arg vllm_fa_cmake_gpu_arches=\"90-real\"\n```\n\n----------------------------------------\n\nTITLE: Simple Pydantic Parsing with OpenAI Beta API\nDESCRIPTION: Demonstrates using OpenAI's beta parsing feature with a simple Pydantic model to extract structured information.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Info(BaseModel):\n    name: str\n    age: int\n\n\nclient = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"dummy\")\ncompletion = client.beta.chat.completions.parse(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"My name is Cameron, I'm 28. What's my name and age?\"},\n    ],\n    response_format=Info,\n    extra_body=dict(guided_decoding_backend=\"outlines\"),\n)\n\nmessage = completion.choices[0].message\nprint(message)\nassert message.parsed\nprint(\"Name:\", message.parsed.name)\nprint(\"Age:\", message.parsed.age)\n```\n\n----------------------------------------\n\nTITLE: Normalizing Softmax Results in vLLM Attention CUDA Kernel\nDESCRIPTION: Finalizes the softmax calculation by normalizing the exponential values. This step divides each logit by the sum of exponentials, producing the final attention weights.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nconst float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);\nfor (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\n   logits[i] *= inv_sum;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM LLM Instance with Explicit Model Architecture\nDESCRIPTION: This example shows how to initialize an LLM instance with a specific model while explicitly specifying the model architecture to resolve potential conflicts or missing information.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/offline_inference.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nmodel = LLM(\n    model=\"cerebras/Cerebras-GPT-1.3B\",\n    hf_overrides={\"architectures\": [\"GPT2LMHeadModel\"]},  # GPT-2\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Reasoning Parser in Python\nDESCRIPTION: Code snippet showing the structure for implementing a custom reasoning parser in vLLM, including necessary imports and class structure.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# import the required packages\n\nfrom vllm.entrypoints.openai.reasoning_parsers.abs_reasoning_parsers import (\n    ReasoningParser, ReasoningParserManager)\nfrom vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\n                                              DeltaMessage)\n\n# define a reasoning parser and register it to vllm\n# the name list in register_module can be used\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Service with Larger Model and Multiple GPUs\nDESCRIPTION: Command to launch the 70B parameter Llama-3 model using 8 A100 GPUs, specifying the model and GPU requirements.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nHF_TOKEN=\"your-huggingface-token\" sky launch serving.yaml --gpus A100:8 --env HF_TOKEN --env MODEL_NAME=meta-llama/Meta-Llama-3-70B-Instruct\n```\n\n----------------------------------------\n\nTITLE: SkyPilot GUI Configuration for vLLM Service\nDESCRIPTION: YAML configuration for deploying a GUI frontend that connects to the vLLM service, providing a user-friendly web interface for interacting with the model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nenvs:\n  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\n  ENDPOINT: x.x.x.x:3031 # Address of the API server running vllm.\n\nresources:\n  cpus: 2\n\nsetup: |\n  conda create -n vllm python=3.10 -y\n  conda activate vllm\n\n  # Install Gradio for web UI.\n  pip install gradio openai\n\nrun: |\n  conda activate vllm\n  export PATH=$PATH:/sbin\n\n  echo 'Starting gradio server...'\n  git clone https://github.com/vllm-project/vllm.git || true\n  python vllm/examples/online_serving/gradio_openai_chatbot_webserver.py \\\n    -m $MODEL_NAME \\\n    --port 8811 \\\n    --model-url http://$ENDPOINT/v1 \\\n    --stop-token-ids 128009,128001 | tee ~/gradio.log\n```\n\n----------------------------------------\n\nTITLE: vLLM SkyPilot Configuration for Single Instance Deployment\nDESCRIPTION: YAML configuration file for deploying vLLM on a single instance using SkyPilot, specifying resources, environment variables, setup instructions, and run commands for serving Llama-3 models.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  accelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\n  use_spot: True\n  disk_size: 512  # Ensure model checkpoints can fit.\n  disk_tier: best\n  ports: 8081  # Expose to internet traffic.\n\nenvs:\n  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\n  HF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\n  conda create -n vllm python=3.10 -y\n  conda activate vllm\n\n  pip install vllm==0.4.0.post1\n  # Install Gradio for web UI.\n  pip install gradio openai\n  pip install flash-attn==2.5.7\n\nrun: |\n  conda activate vllm\n  echo 'Starting vllm api server...'\n  python -u -m vllm.entrypoints.openai.api_server \\\n    --port 8081 \\\n    --model $MODEL_NAME \\\n    --trust-remote-code \\\n    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n    2>&1 | tee api_server.log &\n\n  echo 'Waiting for vllm api server to start...'\n  while ! `cat api_server.log | grep -q 'Uvicorn running on'`; do sleep 1; done\n\n  echo 'Starting gradio server...'\n  git clone https://github.com/vllm-project/vllm.git || true\n  python vllm/examples/online_serving/gradio_openai_chatbot_webserver.py \\\n    -m $MODEL_NAME \\\n    --port 8811 \\\n    --model-url http://localhost:8081/v1 \\\n    --stop-token-ids 128009,128001\n```\n\n----------------------------------------\n\nTITLE: Calculating Exp Sum for Softmax in vLLM Attention CUDA Kernel\nDESCRIPTION: Computes the exponential sum required for softmax normalization. It applies the exponential function to each logit and accumulates the sum, while also updating the logits array.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\n    float val = __expf(logits[i] - qk_max);\n    logits[i] = val;\n    exp_sum += val;\n}\n...\nexp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);\n```\n\n----------------------------------------\n\nTITLE: Expanded GEMM Calculation\nDESCRIPTION: Expanded formula for calculating AB using quantized values, scales, and zero-point.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_4\n\nLANGUAGE: math\nCODE:\n```\nA B = s_a ( \\widehat A - J_a z_a ) s_b \\widehat B\n```\n\nLANGUAGE: math\nCODE:\n```\nA B = s_a s_b \\left( \\widehat A \\widehat B - J_a z_a \\widehat B \\right)\n```\n\nLANGUAGE: math\nCODE:\n```\n\\widehat D = \\widehat A \\widehat B - z_a J_a \\widehat B\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completions with Reasoning Model\nDESCRIPTION: Python code demonstrating how to handle streaming chat completions with a reasoning model, including parsing of reasoning content and final output.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\nmessages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n# For granite, add: `extra_body={\"chat_template_kwargs\": {\"thinking\": True}}`\nstream = client.chat.completions.create(model=model,\n                                        messages=messages,\n                                        stream=True)\n\nprint(\"client: Start streaming chat completions...\")\nprinted_reasoning_content = False\nprinted_content = False\n\nfor chunk in stream:\n    reasoning_content = None\n    content = None\n    # Check the content is reasoning_content or content\n    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n        reasoning_content = chunk.choices[0].delta.reasoning_content\n    elif hasattr(chunk.choices[0].delta, \"content\"):\n        content = chunk.choices[0].delta.content\n\n    if reasoning_content is not None:\n        if not printed_reasoning_content:\n            printed_reasoning_content = True\n            print(\"reasoning_content:\", end=\"\", flush=True)\n        print(reasoning_content, end=\"\", flush=True)\n    elif content is not None:\n        if not printed_content:\n            printed_content = True\n            print(\"\\ncontent:\", end=\"\", flush=True)\n        # Extract and print the content\n        print(content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Complete SkyPilot Configuration with Autoscaling for vLLM\nDESCRIPTION: Full YAML configuration for deploying vLLM as a service with autoscaling capabilities, including min/max replicas and target QPS settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  replica_policy:\n    min_replicas: 2\n    max_replicas: 4\n    target_qps_per_replica: 2\n  # An actual request for readiness probe.\n  readiness_probe:\n    path: /v1/chat/completions\n    post_data:\n      model: $MODEL_NAME\n      messages:\n        - role: user\n          content: Hello! What is your name?\n      max_completion_tokens: 1\n\nresources:\n  accelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\n  use_spot: True\n  disk_size: 512  # Ensure model checkpoints can fit.\n  disk_tier: best\n  ports: 8081  # Expose to internet traffic.\n\nenvs:\n  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\n  HF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\n  conda create -n vllm python=3.10 -y\n  conda activate vllm\n\n  pip install vllm==0.4.0.post1\n  # Install Gradio for web UI.\n  pip install gradio openai\n  pip install flash-attn==2.5.7\n\nrun: |\n  conda activate vllm\n  echo 'Starting vllm api server...'\n  python -u -m vllm.entrypoints.openai.api_server \\\n    --port 8081 \\\n    --model $MODEL_NAME \\\n    --trust-remote-code \\\n    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n    2>&1 | tee api_server.log\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Nsight Systems on Ubuntu\nDESCRIPTION: Bash commands to install NVIDIA Nsight Systems profiling tool on Ubuntu. This includes adding NVIDIA's repository, importing GPG keys, and installing the Nsight Systems CLI.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napt update\napt install -y --no-install-recommends gnupg\necho \"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo \\\"$DISTRIB_RELEASE\\\" | tr -d .)/$(dpkg --print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools.list\napt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\napt update\napt install nsight-systems-cli\n```\n\n----------------------------------------\n\nTITLE: Command-Line Usage for Custom Tool Parser Plugin\nDESCRIPTION: Example command for using a custom tool parser plugin with vLLM. Shows how to specify the plugin file path, parser name, and optional chat template.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n    --enable-auto-tool-choice \\\n    --tool-parser-plugin <absolute path of the plugin file>\n    --tool-call-parser example \\\n    --chat-template <your chat template> \\\n```\n\n----------------------------------------\n\nTITLE: Complete SkyPilot Configuration for Multi-Replica vLLM Deployment\nDESCRIPTION: Full YAML configuration for deploying vLLM as a scalable service with multiple replicas, including readiness probes and resource specifications.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  replicas: 2\n  # An actual request for readiness probe.\n  readiness_probe:\n    path: /v1/chat/completions\n    post_data:\n      model: $MODEL_NAME\n      messages:\n        - role: user\n          content: Hello! What is your name?\n      max_completion_tokens: 1\n\nresources:\n  accelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\n  use_spot: True\n  disk_size: 512  # Ensure model checkpoints can fit.\n  disk_tier: best\n  ports: 8081  # Expose to internet traffic.\n\nenvs:\n  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\n  HF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\n  conda create -n vllm python=3.10 -y\n  conda activate vllm\n\n  pip install vllm==0.4.0.post1\n  # Install Gradio for web UI.\n  pip install gradio openai\n  pip install flash-attn==2.5.7\n\nrun: |\n  conda activate vllm\n  echo 'Starting vllm api server...'\n  python -u -m vllm.entrypoints.openai.api_server \\\n    --port 8081 \\\n    --model $MODEL_NAME \\\n    --trust-remote-code \\\n    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n    2>&1 | tee api_server.log\n```\n\n----------------------------------------\n\nTITLE: torch.compile Test Script\nDESCRIPTION: Test script to verify torch.compile functionality and triton compatibility.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef f(x):\n    # a simple function to test torch.compile\n    x = x + 1\n    x = x * 2\n    x = x.sin()\n    return x\n\nx = torch.randn(4, 4).cuda()\nprint(f(x))\n```\n\n----------------------------------------\n\nTITLE: Warp-Level Reduction in Multi-Head Attention (C++)\nDESCRIPTION: Performs reduction for accs within each warp, allowing each thread to accumulate the accs for the assigned head positions of all tokens in one block.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n   float acc = accs[i];\n   for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {\n      acc += VLLM_SHFL_XOR_SYNC(acc, mask);\n   }\n   accs[i] = acc;\n}\n```\n\n----------------------------------------\n\nTITLE: Asserting Greedy Sampling Equality in Python\nDESCRIPTION: This code snippet is used in various tests to confirm that greedy sampling with speculative decoding matches greedy sampling without it, verifying the lossless guarantee of vLLM's speculative decoding framework.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/b67ae00cdbbe1a58ffc8ff170f0c8d79044a684a/tests/spec_decode/e2e/conftest.py#L291\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tool Parser Plugin\nDESCRIPTION: Template code for implementing a custom tool parser plugin in vLLM. Shows how to define a ToolParser class that handles both streaming and non-streaming tool calls with appropriate registration to make it available through the command line.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# import the required packages\n\n# define a tool parser and register it to vllm\n# the name list in register_module can be used\n# in --tool-call-parser. you can define as many\n# tool parsers as you want here.\n@ToolParserManager.register_module([\"example\"])\nclass ExampleToolParser(ToolParser):\n    def __init__(self, tokenizer: AnyTokenizer):\n        super().__init__(tokenizer)\n\n    # adjust request. e.g.: set skip special tokens\n    # to False for tool call output.\n    def adjust_request(\n            self, request: ChatCompletionRequest) -> ChatCompletionRequest:\n        return request\n\n    # implement the tool call parse for stream call\n    def extract_tool_calls_streaming(\n        self,\n        previous_text: str,\n        current_text: str,\n        delta_text: str,\n        previous_token_ids: Sequence[int],\n        current_token_ids: Sequence[int],\n        delta_token_ids: Sequence[int],\n        request: ChatCompletionRequest,\n    ) -> Union[DeltaMessage, None]:\n        return delta\n\n    # implement the tool parse for non-stream call\n    def extract_tool_calls(\n        self,\n        model_output: str,\n        request: ChatCompletionRequest,\n    ) -> ExtractedToolCallInformation:\n        return ExtractedToolCallInformation(tools_called=False,\n                                            tool_calls=[],\n                                            content=text)\n```\n\n----------------------------------------\n\nTITLE: Configuring dstack Service for vLLM Deployment\nDESCRIPTION: YAML configuration for a dstack Service to deploy vLLM with a specified LLM model. It defines the environment, resources, and commands to run.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/dstack.md#2025-04-05_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntype: service\n\npython: \"3.11\"\nenv:\n    - MODEL=NousResearch/Llama-2-7b-chat-hf\nport: 8000\nresources:\n    gpu: 24GB\ncommands:\n    - pip install vllm\n    - vllm serve $MODEL --port 8000\nmodel:\n    format: openai\n    type: chat\n    name: NousResearch/Llama-2-7b-chat-hf\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM Server for Multiple Images Per Prompt\nDESCRIPTION: Command-line example for serving a multimodal model with vLLM while configuring it to accept up to 4 images per text prompt.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve Qwen/Qwen2-VL-7B-Instruct --limit-mm-per-prompt image=4\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Docker Container with TPU Support\nDESCRIPTION: Command to run the vLLM Docker container with the necessary flags for TPU access. The privileged mode, host networking, and increased shared memory are required for proper TPU functionality.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n# Make sure to add `--privileged --net host --shm-size=16G`.\ndocker run --privileged --net host --shm-size=16G -it vllm-tpu\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor and Pipeline Parallelism for Custom Models\nDESCRIPTION: This snippet demonstrates how to configure tensor and pipeline parallelism for a custom model in vLLM. It defines the necessary parallelism plans in the model's configuration class.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import PretrainedConfig\n\nclass MyConfig(PretrainedConfig):\n  base_model_tp_plan = {\n    \"layers.*.self_attn.k_proj\": \"colwise\",\n    \"layers.*.self_attn.v_proj\": \"colwise\",\n    \"layers.*.self_attn.o_proj\": \"rowwise\",\n    \"layers.*.mlp.gate_proj\": \"colwise\",\n    \"layers.*.mlp.up_proj\": \"colwise\",\n    \"layers.*.mlp.down_proj\": \"rowwise\",\n  }\n  base_model_pp_plan = {\n    \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n    \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n    \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n  }\n```\n\n----------------------------------------\n\nTITLE: Installing Cerebrium Client\nDESCRIPTION: Commands to install the Cerebrium client and log in to the platform.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/cerebrium.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install cerebrium\ncerebrium login\n```\n\n----------------------------------------\n\nTITLE: Defining Core VLLM Extension Source Files in CMake\nDESCRIPTION: Sets the core source files for VLLM CUDA extensions, including attention mechanisms, cache kernels, layer normalization, and quantization support across various modules.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(VLLM_EXT_SRC\n  \"csrc/cache_kernels.cu\"\n  \"csrc/attention/paged_attention_v1.cu\"\n  \"csrc/attention/paged_attention_v2.cu\"\n  \"csrc/pos_encoding_kernels.cu\"\n  \"csrc/activation_kernels.cu\"\n  \"csrc/layernorm_kernels.cu\"\n  \"csrc/layernorm_quant_kernels.cu\"\n  \"csrc/cuda_view.cu\"\n  \"csrc/quantization/gptq/q_gemm.cu\"\n  \"csrc/quantization/compressed_tensors/int8_quant_kernels.cu\"\n  \"csrc/quantization/fp8/common.cu\"\n  \"csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu\"\n  \"csrc/quantization/gguf/gguf_kernel.cu\"\n  \"csrc/cuda_utils_kernels.cu\"\n  \"csrc/prepare_inputs/advance_step.cu\"\n  \"csrc/custom_all_reduce.cu\"\n  \"csrc/torch_bindings.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Using GPTQ Model with vLLM Python API\nDESCRIPTION: Example of using a quantized model through vLLM's Python API, demonstrating prompt generation with custom sampling parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gptqmodel.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.6, top_p=0.9)\n\n# Create an LLM.\nllm = LLM(model=\"DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2\")\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Querying Prometheus Metrics Endpoint in vLLM\nDESCRIPTION: Example of querying the Prometheus metrics endpoint in vLLM, showing various metric types including gauges, counters, and histograms. The output includes metrics for running requests, generated tokens, request success counts, and time to first token.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/metrics.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://0.0.0.0:8000/metrics\n# HELP vllm:num_requests_running Number of requests in model execution batches.\n# TYPE vllm:num_requests_running gauge\nvllm:num_requests_running{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 8.0\n...\n# HELP vllm:generation_tokens_total Number of generation tokens processed.\n# TYPE vllm:generation_tokens_total counter\nvllm:generation_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 27453.0\n...\n# HELP vllm:request_success_total Count of successfully processed requests.\n# TYPE vllm:request_success_total counter\nvllm:request_success_total{finished_reason=\"stop\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1.0\nvllm:request_success_total{finished_reason=\"length\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 131.0\nvllm:request_success_total{finished_reason=\"abort\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n...\n# HELP vllm:time_to_first_token_seconds Histogram of time to first token in seconds.\n# TYPE vllm:time_to_first_token_seconds histogram\nvllm:time_to_first_token_seconds_bucket{le=\"0.001\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.005\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.01\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.02\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 13.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.04\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 97.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.06\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 123.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.08\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 138.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.1\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 140.0\nvllm:time_to_first_token_seconds_count{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 140.0\n```\n\n----------------------------------------\n\nTITLE: vLLM Stack Configuration\nDESCRIPTION: Example YAML configuration for the vLLM production stack, specifying model details, resources, and deployment parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nservingEngineSpec:\n  runtimeClassName: \"\"\n  modelSpec:\n  - name: \"opt125m\"\n    repository: \"vllm/vllm-openai\"\n    tag: \"latest\"\n    modelURL: \"facebook/opt-125m\"\n\n    replicaCount: 1\n\n    requestCPU: 6\n    requestMemory: \"16Gi\"\n    requestGPU: 1\n\n    pvcStorage: \"10Gi\"\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard for vLLM TPU Profile Visualization\nDESCRIPTION: This command starts TensorBoard, pointing it to the directory containing the vLLM TPU profiles. It sets up TensorBoard to run on port 6006, allowing users to view the profiles in their web browser.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/profiling_tpu/README.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir profiles/ --port 6006\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Image Size for Maximum Feature Extraction\nDESCRIPTION: Functions to identify the image size that produces the most features and calculate the maximum possible number of image tokens for a model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_image_size_with_most_features(self) -> ImageSize:\n    hf_config = self.get_hf_config()\n    width = height = hf_config.image_size\n    return ImageSize(width=width, height=height)\n\ndef get_max_image_tokens(self) -> int:\n    target_width, target_height = self.get_image_size_with_most_features()\n\n    return self.get_num_image_tokens(\n        image_width=target_width,\n        image_height=target_height,\n    )\n```\n\n----------------------------------------\n\nTITLE: Connecting to TPU via SSH\nDESCRIPTION: Command to connect to a provisioned TPU using SSH through the Google Cloud CLI. This provides terminal access to the TPU virtual machine for setup and configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute tpus tpu-vm ssh TPU_NAME --zone ZONE\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Existing PyTorch Packages\nDESCRIPTION: Command to uninstall any existing PyTorch and PyTorch XLA packages to avoid conflicts with the specific versions needed for TPU support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall torch torch-xla -y\n```\n\n----------------------------------------\n\nTITLE: Documenting TokensPrompt Class in vLLM\nDESCRIPTION: Generates documentation for the TokensPrompt class showing inheritance, members, and source-ordered documentation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/offline_inference/llm_inputs.md#2025-04-05_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.inputs.TokensPrompt\n    :show-inheritance:\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Configuring Marlin Kernel Compilation for Compatible Architectures\nDESCRIPTION: Conditionally adds Marlin kernels based on compatible CUDA architectures. These specialized kernels support different quantization formats including fp8, sparse operations, and GPTQ.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n  # Only build Marlin kernels if we are building for at least some compatible archs.\n  # Keep building Marlin for 9.0 as there are some group sizes and shapes that\n  # are not supported by Machete yet.\n  cuda_archs_loose_intersection(MARLIN_ARCHS \"8.0;8.6;8.7;8.9;9.0;10.0;10.1;12.0\" \"${CUDA_ARCHS}\")\n  if (MARLIN_ARCHS)\n    set(MARLIN_SRCS\n       \"csrc/quantization/fp8/fp8_marlin.cu\"\n       \"csrc/quantization/marlin/dense/marlin_cuda_kernel.cu\"\n       \"csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu\"\n       \"csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu\"\n       \"csrc/quantization/gptq_marlin/gptq_marlin.cu\"\n       \"csrc/quantization/gptq_marlin/gptq_marlin_repack.cu\"\n       \"csrc/quantization/gptq_marlin/awq_marlin_repack.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${MARLIN_SRCS}\"\n      CUDA_ARCHS \"${MARLIN_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${MARLIN_SRCS}\")\n    message(STATUS \"Building Marlin kernels for archs: ${MARLIN_ARCHS}\")\n  else()\n    message(STATUS \"Not building Marlin kernels as no compatible archs found\"\n                   \" in CUDA target architectures\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Adding SupportsMultiModal Interface to Model Class\nDESCRIPTION: Update to add the SupportsMultiModal interface to the model class to indicate it supports multi-modal inputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n+ from vllm.model_executor.models.interfaces import SupportsMultiModal\n\n- class YourModelForImage2Seq(nn.Module):\n+ class YourModelForImage2Seq(nn.Module, SupportsMultiModal):\n```\n\n----------------------------------------\n\nTITLE: Generating SamplingParams Documentation with RestructuredText\nDESCRIPTION: Auto-generates API documentation for the vllm.SamplingParams class using RestructuredText directives.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/inference_params.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.SamplingParams\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Accuracy with lm_eval\nDESCRIPTION: Command to evaluate the quantized model's accuracy using lm_eval on specific tasks.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nlm_eval --model vllm \\\n  --model_args pretrained=Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant,kv_cache_dtype='fp8',quantization='quark' \\\n  --tasks gsm8k\n```\n\n----------------------------------------\n\nTITLE: Structuring Image Placeholder Tokens with Newlines for Fuyu Model\nDESCRIPTION: This code snippet processes image tokens by adding newline tokens after each row of image patches. It reshapes the tensor of image placeholder IDs and concatenates newline tokens to create a specific token layout for variable-sized images.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntensor_of_image_ids = torch.full(\n    [num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n)\npatches = self.patchify_image(image=image.unsqueeze(0)).squeeze(0)\nassert num_patches == patches.shape[0]\n\nif variable_sized:\n    # Now terminate each line with |NEWLINE|.\n    tensor_of_image_ids = tensor_of_image_ids.reshape(-1, image_width // patch_width)\n    newline_ids = torch.full(\n        [tensor_of_image_ids.shape[0], 1],\n        image_newline_id,\n        dtype=torch.int32,\n        device=image_input.device,\n    )\n    tensor_of_image_ids = torch.cat([tensor_of_image_ids, newline_ids], dim=1)\n    tensor_of_image_ids = tensor_of_image_ids.reshape(-1)\n```\n\n----------------------------------------\n\nTITLE: Configuring LeaderWorkerSet for vLLM Deployment on Kubernetes\nDESCRIPTION: YAML configuration for deploying vLLM with LeaderWorkerSet. It defines a distributed setup with leader and worker nodes, each with 8 GPUs, and configures the necessary resources, environment variables, and service definitions for multi-node tensor-parallel inference.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: leaderworkerset.x-k8s.io/v1\nkind: LeaderWorkerSet\nmetadata:\n  name: vllm\nspec:\n  replicas: 2\n  leaderWorkerTemplate:\n    size: 2\n    restartPolicy: RecreateGroupOnPodRestart\n    leaderTemplate:\n      metadata:\n        labels:\n          role: leader\n      spec:\n        containers:\n          - name: vllm-leader\n            image: docker.io/vllm/vllm-openai:latest\n            env:\n              - name: HUGGING_FACE_HUB_TOKEN\n                value: <your-hf-token>\n            command:\n              - sh\n              - -c\n              - \"bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE); \n                 python3 -m vllm.entrypoints.openai.api_server --port 8080 --model meta-llama/Meta-Llama-3.1-405B-Instruct --tensor-parallel-size 8 --pipeline_parallel_size 2\"\n            resources:\n              limits:\n                nvidia.com/gpu: \"8\"\n                memory: 1124Gi\n                ephemeral-storage: 800Gi\n              requests:\n                ephemeral-storage: 800Gi\n                cpu: 125\n            ports:\n              - containerPort: 8080\n            readinessProbe:\n              tcpSocket:\n                port: 8080\n              initialDelaySeconds: 15\n              periodSeconds: 10\n            volumeMounts:\n              - mountPath: /dev/shm\n                name: dshm\n        volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 15Gi\n    workerTemplate:\n      spec:\n        containers:\n          - name: vllm-worker\n            image: docker.io/vllm/vllm-openai:latest\n            command:\n              - sh\n              - -c\n              - \"bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)\"\n            resources:\n              limits:\n                nvidia.com/gpu: \"8\"\n                memory: 1124Gi\n                ephemeral-storage: 800Gi\n              requests:\n                ephemeral-storage: 800Gi\n                cpu: 125\n            env:\n              - name: HUGGING_FACE_HUB_TOKEN\n                value: <your-hf-token>\n            volumeMounts:\n              - mountPath: /dev/shm\n                name: dshm   \n        volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 15Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-leader\nspec:\n  ports:\n    - name: http\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    leaderworkerset.sigs.k8s.io/name: vllm\n    role: leader\n  type: ClusterIP\n```\n\n----------------------------------------\n\nTITLE: Documenting TextPrompt Class in vLLM\nDESCRIPTION: Generates documentation for the TextPrompt class showing inheritance, members, and source-ordered documentation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/offline_inference/llm_inputs.md#2025-04-05_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.inputs.TextPrompt\n    :show-inheritance:\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Configuring 2:4 Sparse Matrix Multiplication for Hopper GPUs\nDESCRIPTION: Sets up specialized sparse matrix multiplication kernels for Hopper GPUs when using CUDA 12.2+. These kernels support the 2:4 sparsity pattern which offers performance benefits for certain workloads.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\n  #\n  # 2:4 Sparse Kernels\n\n  # The 2:4 sparse kernels cutlass_scaled_sparse_mm and cutlass_compressor\n  # require CUDA 12.2 or later (and only work on Hopper).\n  cuda_archs_loose_intersection(SCALED_MM_ARCHS \"9.0a;\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.2 AND SCALED_MM_ARCHS)\n    set(SRCS \"csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SPARSE_SCALED_MM_C3X=1\")\n    message(STATUS \"Building sparse_scaled_mm_c3x for archs: ${SCALED_MM_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.2 AND SCALED_MM_ARCHS)\n      message(STATUS \"Not building sparse_scaled_mm_c3x kernels as CUDA Compiler version is \"\n                     \"not >= 12.2, we recommend upgrading to CUDA 12.2 or later \"\n                     \"if you intend on running FP8 sparse quantized models on Hopper.\")\n    else()\n      message(STATUS \"Not building sparse_scaled_mm_c3x as no compatible archs found \"\n                     \"in CUDA target architectures\")\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Building vLLM Docker Image from Source\nDESCRIPTION: Command to build the vLLM Docker image from source code using BuildKit.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build . --target vllm-openai --tag vllm/vllm-openai --file docker/Dockerfile\n```\n\n----------------------------------------\n\nTITLE: Generating PoolingParams Documentation with RestructuredText\nDESCRIPTION: Auto-generates API documentation for the vllm.PoolingParams class using RestructuredText directives.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/inference_params.md#2025-04-05_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.PoolingParams\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantized Model Accuracy with lm_eval\nDESCRIPTION: Command to evaluate the accuracy of a quantized model using lm-evaluation-harness on the gsm8k task, with few-shot prompting.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ MODEL=$PWD/Meta-Llama-3-8B-Instruct-FP8-Dynamic\n$ lm_eval \\\n  --model vllm \\\n  --model_args pretrained=$MODEL,add_bos_token=True \\\n  --tasks gsm8k  --num_fewshot 5 --batch_size auto --limit 250\n```\n\n----------------------------------------\n\nTITLE: Custom HF Processor Call for Fuyu Model\nDESCRIPTION: Overrides the base processor call method to handle Fuyu's specific output format. It post-processes the image_patches tensor by removing the extra batch dimension to make it compatible with vLLM's batched field configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef _call_hf_processor(\n    self,\n    prompt: str,\n    mm_data: Mapping[str, object],\n    mm_kwargs: Mapping[str, object],\n) -> BatchFeature:\n    processed_outputs = super()._call_hf_processor(\n        prompt=prompt,\n        mm_data=mm_data,\n        mm_kwargs=mm_kwargs,\n    )\n\n    image_patches = processed_outputs.get(\"image_patches\")\n    if image_patches is not None:\n        images = mm_data[\"images\"]\n        assert isinstance(images, list)\n\n        # Original output: (1, num_images, Pn, Px * Py * C)\n        # New output: (num_images, Pn, Px * Py * C)\n        assert (isinstance(image_patches, list)\n                and len(image_patches) == 1)\n        assert (isinstance(image_patches[0], torch.Tensor)\n                and len(image_patches[0]) == len(images))\n\n        processed_outputs[\"image_patches\"] = image_patches[0]\n\n    return processed_outputs\n```\n\n----------------------------------------\n\nTITLE: Setting Bucketing Parameters for vLLM on HPU\nDESCRIPTION: These environment variables configure the ranges of the bucketing mechanism for both prompt and decode phases. They control batch size and sequence length parameters for graph optimization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport VLLM_PROMPT_BS_BUCKET_MIN=1\nexport VLLM_PROMPT_BS_BUCKET_STEP=32\nexport VLLM_PROMPT_BS_BUCKET_MAX=64\nexport VLLM_PROMPT_SEQ_BUCKET_MIN=$block_size\nexport VLLM_PROMPT_SEQ_BUCKET_STEP=$block_size\nexport VLLM_PROMPT_SEQ_BUCKET_MAX=$max_model_len\n\nexport VLLM_DECODE_BS_BUCKET_MIN=1\nexport VLLM_DECODE_BS_BUCKET_STEP=32\nexport VLLM_DECODE_BS_BUCKET_MAX=$max_num_seqs\nexport VLLM_DECODE_BLOCK_BUCKET_MIN=$block_size\nexport VLLM_DECODE_BLOCK_BUCKET_STEP=$block_size\nexport VLLM_DECODE_BLOCK_BUCKET_MAX=128\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS 3.x Scaled Matrix Multiplication for Hopper GPUs\nDESCRIPTION: Sets up specialized CUTLASS 3.x scaled matrix multiplication kernels for Hopper (SM90) GPUs when using CUDA 12.0+. These kernels provide optimized operations for various precisions including FP8 and INT8.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\n  set(SCALED_MM_3X_ARCHS)\n  # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require\n  # CUDA 12.0 or later\n  cuda_archs_loose_intersection(SCALED_MM_ARCHS \"9.0a;\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)\n    set(SRCS\n       \"csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu\"\n       \"csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu\"\n       \"csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu\"\n       \"csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu\"\n       \"csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SCALED_MM_SM90=1\")\n    # Let scaled_mm_c2x know it doesn't need to build these arches\n    list(APPEND SCALED_MM_3X_ARCHS \"${SCALED_MM_ARCHS}\")\n    message(STATUS \"Building scaled_mm_c3x_sm90 for archs: ${SCALED_MM_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_ARCHS)\n      message(STATUS \"Not building scaled_mm_c3x_sm90 as CUDA Compiler version is \"\n                     \"not >= 12.0, we recommend upgrading to CUDA 12.0 or \"\n                     \"later if you intend on running FP8 quantized models on \"\n                     \"Hopper.\")\n    else()\n      message(STATUS \"Not building scaled_mm_c3x_sm90 as no compatible archs found \"\n                     \"in CUDA target architectures\")\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Disabling HPU Graphs for vLLM Server\nDESCRIPTION: This command-line flag disables HPU Graphs for the vLLM server, trading latency and throughput at lower batches for potentially higher throughput on higher batches.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nvllm-server --enforce-eager\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Server Launch Command\nDESCRIPTION: Command to launch the OpenAI-compatible server for multimodal inference with Phi-3.5-vision model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve microsoft/Phi-3.5-vision-instruct --task generate \\\n  --trust-remote-code --max-model-len 4096 --limit-mm-per-prompt image=2\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS MoE Kernels for CUDA\nDESCRIPTION: Sets up the build configuration for CUTLASS MoE kernels, including version checks and architecture-specific flags.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\ncuda_archs_loose_intersection(SCALED_MM_ARCHS \"9.0a;\" \"${CUDA_ARCHS}\")\nif(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER_EQUAL 12.3 AND SCALED_MM_ARCHS)\n  set(SRCS \"csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu\"\n           \"csrc/quantization/cutlass_w8a8/moe/moe_data.cu\")\n  set_gencode_flags_for_srcs(\n    SRCS \"${SRCS}\"\n    CUDA_ARCHS \"${SCALED_MM_ARCHS}\")\n  list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n  list(APPEND VLLM_GPU_FLAGS \"-DENABLE_CUTLASS_MOE_SM90=1\")\n  message(STATUS \"Building grouped_mm_c3x for archs: ${SCALED_MM_ARCHS}\")\nelse()\n  # Error messages for incompatible versions or architectures\nendif()\n```\n\n----------------------------------------\n\nTITLE: Multimodal Chat Completion Example\nDESCRIPTION: Example showing how to use chat completions with typed content format for multimodal inputs\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n  messages=[\n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this sentiment: vLLM is wonderful!\"}]}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Rendering Async Engine Arguments Documentation with reStructuredText\nDESCRIPTION: A reStructuredText directive that automatically generates documentation for vLLM async engine arguments by parsing the async argument parser defined in vllm.engine.arg_utils.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/engine_args.md#2025-04-05_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. argparse::\n    :module: vllm.engine.arg_utils\n    :func: _async_engine_args_parser\n    :prog: vllm serve\n    :nodefaultconst:\n```\n\n----------------------------------------\n\nTITLE: Curl Request for vLLM Re-rank API\nDESCRIPTION: Example curl command demonstrating how to make a POST request to the vLLM re-rank API endpoint. This request uses the BAAI/bge-reranker-base model to evaluate the relevance between a query about France's capital and three sample documents.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/v1/rerank' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"model\": \"BAAI/bge-reranker-base\",\n  \"query\": \"What is the capital of France?\",\n  \"documents\": [\n    \"The capital of Brazil is Brasilia.\",\n    \"The capital of France is Paris.\",\n    \"Horses and cows are both animals\"\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Verifying CUDA Toolkit Installation for vLLM\nDESCRIPTION: Commands to verify that the CUDA Toolkit is correctly installed by checking the NVCC compiler version. These help troubleshoot CUDA-related installation issues with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nnvcc --version # verify that nvcc is in your PATH\n${CUDA_HOME}/bin/nvcc --version # verify that nvcc is in your CUDA_HOME\n```\n\n----------------------------------------\n\nTITLE: Generating Text with LoRA Adapter\nDESCRIPTION: Demonstrates how to use the LoRA adapter for text generation with specific sampling parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsampling_params = SamplingParams(\n    temperature=0,\n    max_tokens=256,\n    stop=[\"[/assistant]\"]\n)\n\nprompts = [\n     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\n     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\n]\n\noutputs = llm.generate(\n    prompts,\n    sampling_params,\n    lora_request=LoRARequest(\"sql_adapter\", 1, sql_lora_path)\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Installing vLLM from Source\nDESCRIPTION: This snippet demonstrates the process of building and installing vLLM from source. It includes modifying the requirements file, installing dependencies, building the wheel, and installing the resulting package. Note that torch is excluded as nightly builds are used.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/s390x.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nsed -i '/^torch/d' requirements-build.txt    # remove torch from requirements-build.txt since we use nightly builds\npip install -v \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cpu \\\n    -r requirements-build.txt \\\n    -r requirements-cpu.txt \\\nVLLM_TARGET_DEVICE=cpu python setup.py bdist_wheel && \\\npip install dist/*.whl\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Exporting Model\nDESCRIPTION: Performs model quantization, freezes the quantized model, and exports it in HuggingFace safetensors format.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom quark.torch import ModelQuantizer, ModelExporter\nfrom quark.torch.export import ExporterConfig, JsonExporterConfig\n\n# Apply quantization.\nquantizer = ModelQuantizer(quant_config)\nquant_model = quantizer.quantize_model(model, calib_dataloader)\n\n# Freeze quantized model to export.\nfreezed_model = quantizer.freeze(model)\n\n# Define export config.\nLLAMA_KV_CACHE_GROUP = [\"*k_proj\", \"*v_proj\"]\nexport_config = ExporterConfig(json_export_config=JsonExporterConfig())\nexport_config.json_export_config.kv_cache_group = LLAMA_KV_CACHE_GROUP\n\nEXPORT_DIR = MODEL_ID.split(\"/\")[1] + \"-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant\"\nexporter = ModelExporter(config=export_config, export_dir=EXPORT_DIR)\nwith torch.no_grad():\n    exporter.export_safetensors_model(freezed_model,\n        quant_config=quant_config, tokenizer=tokenizer)\n```\n\n----------------------------------------\n\nTITLE: CUDA/ROCm Compilation Configuration\nDESCRIPTION: Sets up GPU-specific compilation flags and architecture targets for both CUDA and ROCm builds\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  clear_cuda_arches(CUDA_ARCH_FLAGS)\n  extract_unique_cuda_archs_ascending(CUDA_ARCHS \"${CUDA_ARCH_FLAGS}\")\n  message(STATUS \"CUDA target architectures: ${CUDA_ARCHS}\")\n  cuda_archs_loose_intersection(CUDA_ARCHS\n    \"${CUDA_SUPPORTED_ARCHS}\" \"${CUDA_ARCHS}\")\nelse()\n  override_gpu_arches(VLLM_GPU_ARCHES\n    ${VLLM_GPU_LANG}\n    \"${${VLLM_GPU_LANG}_SUPPORTED_ARCHS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Paged Attention Kernel Function in C++\nDESCRIPTION: This code snippet defines the template and function signature for the paged attention kernel. It specifies the input parameters including pointers to query, key, and value data in global memory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<\ntypename scalar_t,\nint HEAD_SIZE,\nint BLOCK_SIZE,\nint NUM_THREADS,\nint PARTITION_SIZE = 0>\n__device__ void paged_attention_kernel(\n... // Other side args.\nconst scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]\nconst scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]\nconst scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]\nconst scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]\n... // Other side args.\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM Model with New Constructor Pattern\nDESCRIPTION: Shows the new standardized constructor signature for vLLM models that accepts a VllmConfig object and an optional prefix parameter. This pattern ensures uniform model initialization across different model types.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/arch_overview.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\")\n```\n\n----------------------------------------\n\nTITLE: Example Tool Calling Output\nDESCRIPTION: Sample output showing the results of a tool calling request, including the function name, arguments, and execution result.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nFunction called: get_weather\nArguments: {\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}\nResult: Getting the weather for San Francisco, CA in fahrenheit...\n```\n\n----------------------------------------\n\nTITLE: Testing PyTorch NCCL and GLOO Communication\nDESCRIPTION: This script tests PyTorch NCCL and GLOO communication, as well as vLLM NCCL communication with and without CUDA graph. It's used to confirm whether GPU/CPU communication is working correctly.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Test PyTorch NCCL\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend=\"nccl\")\nlocal_rank = dist.get_rank() % torch.cuda.device_count()\ntorch.cuda.set_device(local_rank)\ndata = torch.FloatTensor([1,] * 128).to(\"cuda\")\ndist.all_reduce(data, op=dist.ReduceOp.SUM)\ntorch.cuda.synchronize()\nvalue = data.mean().item()\nworld_size = dist.get_world_size()\nassert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"PyTorch NCCL is successful!\")\n\n# Test PyTorch GLOO\ngloo_group = dist.new_group(ranks=list(range(world_size)), backend=\"gloo\")\ncpu_data = torch.FloatTensor([1,] * 128)\ndist.all_reduce(cpu_data, op=dist.ReduceOp.SUM, group=gloo_group)\nvalue = cpu_data.mean().item()\nassert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"PyTorch GLOO is successful!\")\n\nif world_size <= 1:\n    exit()\n\n# Test vLLM NCCL, with cuda graph\nfrom vllm.distributed.device_communicators.pynccl import PyNcclCommunicator\n\npynccl = PyNcclCommunicator(group=gloo_group, device=local_rank)\n# pynccl is enabled by default for 0.6.5+,\n# but for 0.6.4 and below, we need to enable it manually.\n# keep the code for backward compatibility when because people\n# prefer to read the latest documentation.\npynccl.disabled = False\n\ns = torch.cuda.Stream()\nwith torch.cuda.stream(s):\n    data.fill_(1)\n    out = pynccl.all_reduce(data, stream=s)\n    value = out.mean().item()\n    assert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"vLLM NCCL is successful!\")\n\ng = torch.cuda.CUDAGraph()\nwith torch.cuda.graph(cuda_graph=g, stream=s):\n    out = pynccl.all_reduce(data, stream=torch.cuda.current_stream())\n\ndata.fill_(1)\ng.replay()\ntorch.cuda.current_stream().synchronize()\nvalue = out.mean().item()\nassert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"vLLM NCCL with cuda graph is successful!\")\n\ndist.destroy_process_group(gloo_group)\ndist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Configuring Automodule Documentation for Model Adapters\nDESCRIPTION: RestructuredText directive configuration for auto-generating API documentation from the vllm.model_executor.models.adapters module. Specifies to include all members and order them by source code order.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/model/adapters.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.model_executor.models.adapters\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Building ROCm Base Docker Image\nDESCRIPTION: Command to build the base Docker image with ROCm software stack for vLLM. This step is optional as pre-built images are available on Docker Hub.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build -f docker/Dockerfile.rocm_base -t rocm/vllm-dev:base .\n```\n\n----------------------------------------\n\nTITLE: Verifying Tensor-Parallel Inference Setup\nDESCRIPTION: Command to check if the distributed tensor-parallel inference is properly configured by examining the logs for model loading information.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs vllm-0 |grep -i \"Loading model weights took\" \n```\n\n----------------------------------------\n\nTITLE: JSON Response from vLLM Re-rank API\nDESCRIPTION: Example response from the vLLM re-rank API showing the ranked documents with relevance scores. The documents are sorted by relevance score, with the highest scoring document (about France's capital) appearing first in the results.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n{\n  \"id\": \"rerank-fae51b2b664d4ed38f5969b612edff77\",\n  \"model\": \"BAAI/bge-reranker-base\",\n  \"usage\": {\n    \"total_tokens\": 56\n  },\n  \"results\": [\n    {\n      \"index\": 1,\n      \"document\": {\n        \"text\": \"The capital of France is Paris.\"\n      },\n      \"relevance_score\": 0.99853515625\n    },\n    {\n      \"index\": 0,\n      \"document\": {\n        \"text\": \"The capital of Brazil is Brasilia.\"\n      },\n      \"relevance_score\": 0.0005860328674316406\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Extension Target Configuration\nDESCRIPTION: Defines and configures the cumem_allocator extension with appropriate source files and GPU-specific settings\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(VLLM_CUMEM_EXT_SRC\n  \"csrc/cumem_allocator.cpp\")\n\nset_gencode_flags_for_srcs(\n  SRCS \"${VLLM_CUMEM_EXT_SRC}\"\n  CUDA_ARCHS \"${CUDA_ARCHS}\")\n\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  message(STATUS \"Enabling cumem allocator extension.\")\n  list(APPEND CUMEM_LIBS CUDA::cuda_driver)\n  define_gpu_extension_target(\n    cumem_allocator\n    DESTINATION vllm\n    LANGUAGE CXX\n    SOURCES ${VLLM_CUMEM_EXT_SRC}\n    LIBRARIES ${CUMEM_LIBS}\n    USE_SABI 3.8\n    WITH_SOABI)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Client with Audio Inputs via Base64 Encoding in Python\nDESCRIPTION: Python code showing how to encode audio content from a URL as base64 and send it to a local vLLM server using the OpenAI client API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport requests\nfrom openai import OpenAI\nfrom vllm.assets.audio import AudioAsset\n\ndef encode_base64_content_from_url(content_url: str) -> str:\n    \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\"\n\n    with requests.get(content_url) as response:\n        response.raise_for_status()\n        result = base64.b64encode(response.content).decode('utf-8')\n\n    return result\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n# Any format supported by librosa is supported\naudio_url = AudioAsset(\"winning_call\").url\naudio_base64 = encode_base64_content_from_url(audio_url)\n\nchat_completion_from_base64 = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What's in this audio?\"\n            },\n            {\n                \"type\": \"input_audio\",\n                \"input_audio\": {\n                    \"data\": audio_base64,\n                    \"format\": \"wav\"\n                },\n            },\n        ],\n    }],\n    model=model,\n    max_completion_tokens=64,\n)\n\nresult = chat_completion_from_base64.choices[0].message.content\nprint(\"Chat completion output from input audio:\", result)\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics Endpoint\nDESCRIPTION: Example of querying the metrics endpoint showing histogram data for token iterations. The response includes various metrics buckets with their respective values.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/metrics.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ curl http://0.0.0.0:8000/metrics\n\n# HELP vllm:iteration_tokens_total Histogram of number of tokens per engine_step.\n# TYPE vllm:iteration_tokens_total histogram\nvllm:iteration_tokens_total_sum{model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 0.0\nvllm:iteration_tokens_total_bucket{le=\"1.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"8.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"16.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"32.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"64.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"128.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"256.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"512.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\n...\n```\n\n----------------------------------------\n\nTITLE: Initializing dstack Project\nDESCRIPTION: Commands to create a new directory for the vLLM-dstack project and initialize it with dstack.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/dstack.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nmkdir -p vllm-dstack\ncd vllm-dstack\ndstack init\n```\n\n----------------------------------------\n\nTITLE: Specifying Supported Multi-Modal Input Limits\nDESCRIPTION: Implementation of get_supported_mm_limits method to define the maximum number of inputs for each modality supported by the model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:\n    return {\"image\": None, \"video\": 1}\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Environment Variables\nDESCRIPTION: Setting up environment variables for Jaeger connection and OpenTelemetry configuration for vLLM server.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nexport JAEGER_IP=$(docker inspect   --format '{{ .NetworkSettings.IPAddress }}' jaeger)\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=grpc://$JAEGER_IP:4317\n```\n\nLANGUAGE: console\nCODE:\n```\nexport OTEL_SERVICE_NAME=\"vllm-server\"\nexport OTEL_EXPORTER_OTLP_TRACES_INSECURE=true\nvllm serve facebook/opt-125m --otlp-traces-endpoint=\"$OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"\n```\n\n----------------------------------------\n\nTITLE: Updating vLLM Service with New Configuration\nDESCRIPTION: Command to update an existing vLLM service with new configuration settings while maintaining service availability.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nHF_TOKEN=\"your-huggingface-token\" sky serve update vllm serving.yaml --env HF_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Installing Triton Flash Attention for ROCm\nDESCRIPTION: Step-by-step commands to clone, build, and install Triton Flash Attention from source. This is a requirement for vLLM on ROCm and provides optimized attention implementations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 -m pip install ninja cmake wheel pybind11\npip uninstall -y triton\ngit clone https://github.com/OpenAI/triton.git\ncd triton\ngit checkout e5be006\ncd python\npip3 install .\ncd ../..\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Containers\nDESCRIPTION: Launches two vLLM containers with specific configurations, including GPU device allocation, shared memory size, and volume mounting for the HuggingFace cache.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nmkdir -p ~/.cache/huggingface/hub/\nhf_cache_dir=~/.cache/huggingface/\ndocker run -itd --ipc host --network vllm_nginx --gpus device=0 --shm-size=10.24gb -v $hf_cache_dir:/root/.cache/huggingface/ -p 8081:8000 --name vllm0 vllm --model meta-llama/Llama-2-7b-chat-hf\ndocker run -itd --ipc host --network vllm_nginx --gpus device=1 --shm-size=10.24gb -v $hf_cache_dir:/root/.cache/huggingface/ -p 8082:8000 --name vllm1 vllm --model meta-llama/Llama-2-7b-chat-hf\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Client with Audio URL Inputs in Python\nDESCRIPTION: Python code showing how to use audio_url directly with the OpenAI client API instead of base64 encoding.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nchat_completion_from_url = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What's in this audio?\"\n            },\n            {\n                \"type\": \"audio_url\",\n                \"audio_url\": {\n                    \"url\": audio_url\n                },\n            },\n        ],\n    }],\n    model=model,\n    max_completion_tokens=64,\n)\n\nresult = chat_completion_from_url.choices[0].message.content\nprint(\"Chat completion output from audio url:\", result)\n```\n\n----------------------------------------\n\nTITLE: Testing vLLM Service with curl\nDESCRIPTION: Command to test the deployed vLLM service by sending a chat completion request to the service endpoint using curl.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nENDPOINT=$(sky serve status --endpoint 8081 vllm)\ncurl -L http://$ENDPOINT/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Who are you?\"\n    }\n    ],\n    \"stop_token_ids\": [128009,  128001]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP/Protobuf Protocol\nDESCRIPTION: Configuration for using HTTP/Protobuf as the transport protocol instead of default gRPC.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://$JAEGER_IP:4318/v1/traces\nvllm serve facebook/opt-125m --otlp-traces-endpoint=\"$OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"\n```\n\n----------------------------------------\n\nTITLE: Running Docker Image for Intel Gaudi\nDESCRIPTION: Commands to pull and run the latest Docker image for Intel Gaudi, setting up the appropriate runtime and environment variables.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\n```\n\n----------------------------------------\n\nTITLE: Creating Nginx Dockerfile\nDESCRIPTION: Creates a Dockerfile for building a custom Nginx container. It uses the latest Nginx image, removes the default configuration, exposes port 80, and sets the CMD to run Nginx in the foreground.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM nginx:latest\nRUN rm /etc/nginx/conf.d/default.conf\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Flash Attention for ROCm\nDESCRIPTION: Commands to clone and build the optional CK Flash Attention implementation for ROCm. This provides an alternative attention implementation that supports features like sliding window attention.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/ROCm/flash-attention.git\ncd flash-attention\ngit checkout b7d29fb\ngit submodule update --init\nGPU_ARCHS=\"gfx90a\" python3 setup.py install\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM from Source for AWS Neuron\nDESCRIPTION: This snippet shows how to clone the vLLM repository, install required dependencies, and build vLLM from source for AWS Neuron SDK compatibility.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/neuron.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -U -r requirements/neuron.txt\nVLLM_TARGET_DEVICE=\"neuron\" pip install .\n```\n\n----------------------------------------\n\nTITLE: Documenting MultiModalKwargsItem Class - RST\nDESCRIPTION: Documentation for the MultiModalKwargsItem class that represents individual keyword argument items.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.MultiModalKwargsItem\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Example API Response from vLLM Model\nDESCRIPTION: Sample JSON response from the vLLM API showing a successful completion request. This illustrates the expected format of responses including token counts and generated text.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n{\n  \"id\": \"cmpl-1bb34faba88b43f9862cfbfb2200949d\",\n  \"object\": \"text_completion\",\n  \"created\": 1715138766,\n  \"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" top destination for foodies, with\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 12,\n    \"completion_tokens\": 7\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Hugging Face Token in Kubernetes\nDESCRIPTION: This YAML snippet creates a Kubernetes Secret to store the Hugging Face token, which is required for accessing gated models. The token should be replaced with the actual value.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hf-token-secret\n  namespace: default\ntype: Opaque\nstringData:\n  token: \"REPLACE_WITH_TOKEN\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Converting Config in vLLM\nDESCRIPTION: This snippet demonstrates how vLLM loads the config file and converts it into a dictionary. It uses the json module to load the file contents.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/huggingface_integration.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig_path = get_config_path(model, revision, trust_remote_code)\nwith open(config_path, \"r\") as f:\n    config_json = json.load(f)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable to Disable vLLM V1\nDESCRIPTION: Command to disable vLLM V1 by setting an environment variable. This allows users to revert to V0 behavior if needed.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/v1_user_guide.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_USE_V1=0\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized Model in vLLM\nDESCRIPTION: Code to load a quantized INT4 model in vLLM for inference after the quantization process is complete.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nmodel = LLM(\"./Meta-Llama-3-8B-Instruct-W4A16-G128\")\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Docker Container\nDESCRIPTION: Command to run the vLLM Docker container with appropriate device mounts and permissions for ROCm. Includes volume mounting for model files and other required configurations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\ndocker run -it \\\n   --network=host \\\n   --group-add=video \\\n   --ipc=host \\\n   --cap-add=SYS_PTRACE \\\n   --security-opt seccomp=unconfined \\\n   --device /dev/kfd \\\n   --device /dev/dri \\\n   -v <path/to/model>:/app/model \\\n   vllm-rocm \\\n   bash\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Official Docker Image\nDESCRIPTION: Command to run the official vLLM Docker image from Docker Hub with GPU support and HuggingFace cache mounting. Requires NVIDIA runtime and GPU access.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ndocker run --runtime nvidia --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model mistralai/Mistral-7B-v0.1\n```\n\n----------------------------------------\n\nTITLE: Importing vLLM Multimodal Registry in Python\nDESCRIPTION: This code snippet imports and displays the MULTIMODAL_REGISTRY from the vllm.multimodal module. The registry likely contains information about supported multi-modal models and their configurations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/index.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autodata:: vllm.multimodal.MULTIMODAL_REGISTRY\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for vLLM Service\nDESCRIPTION: Command to establish port forwarding from local port 8080 to the vLLM service in Kubernetes, enabling local access to the deployed model API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Listen on port 8080 locally, forwarding to the targetPort of the service's port 8080 in a pod selected by the service\nkubectl port-forward svc/vllm-leader 8080:8080\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for fastsafetensors in vLLM\nDESCRIPTION: This snippet shows how to enable the fastsafetensors feature in vLLM by setting an environment variable. The USE_FASTSAFETENSOR variable is set to 'true' to activate the feature.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/fastsafetensor.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nUSE_FASTSAFETENSOR=true\n```\n\n----------------------------------------\n\nTITLE: Completion API Response\nDESCRIPTION: Example JSON response from the completion API endpoint.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"completion-id\",\n  \"object\": \"text_completion\",\n  \"created\": 1737428424,\n  \"model\": \"facebook/opt-125m\",\n  \"choices\": [\n    {\n      \"text\": \" there was a brave knight who...\",\n      \"index\": 0,\n      \"finish_reason\": \"length\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch Profiler for OpenAI Server Interface\nDESCRIPTION: Commands to enable PyTorch profiling when running vLLM's OpenAI server interface. This sets the trace output directory and runs a benchmark with profiling enabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_TORCH_PROFILER_DIR=./vllm_profile python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-70B\n```\n\n----------------------------------------\n\nTITLE: Launching OpenAI-Compatible Server for Video Processing with LLaVA-OneVision\nDESCRIPTION: Command to start a vLLM server using the LLaVA-OneVision model for video processing with an extended context length of 8192 tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve llava-hf/llava-onevision-qwen2-0.5b-ov-hf --task generate --max-model-len 8192\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Environment Variables for vLLM Installation\nDESCRIPTION: Configures the CUDA_HOME environment variable and updates PATH to include the CUDA compiler. These settings ensure that vLLM can find the necessary CUDA components during installation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nexport CUDA_HOME=/usr/local/cuda\nexport PATH=\"${CUDA_HOME}/bin:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: Running vLLM XPU Docker Container\nDESCRIPTION: Runs the vLLM XPU Docker container with the necessary device mappings and network configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ docker run -it \\\n             --rm \\\n             --network=host \\\n             --device /dev/dri \\\n             -v /dev/dri/by-path:/dev/dri/by-path \\\n             vllm-xpu-env\n```\n\n----------------------------------------\n\nTITLE: Updating Model Forward Method for Multi-Modal Support\nDESCRIPTION: Adding pixel_values parameter to the forward method of a model to allow processing of image inputs alongside text inputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    positions: torch.Tensor,\n+     pixel_values: torch.Tensor,\n) -> SamplerOutput:\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM with Kubernetes Commands\nDESCRIPTION: Bash commands to deploy the vLLM LeaderWorkerSet configuration and verify the status of the deployed pods. These commands apply the YAML configuration and check if the pods are running correctly.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f lws.yaml\n```\n\n----------------------------------------\n\nTITLE: Using Spawn Method in vLLM Command Execution\nDESCRIPTION: Code reference showing that vLLM uses the 'spawn' method when the process is owned by the vLLM command for maximum compatibility.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/scripts.py#L123-L140\n```\n\n----------------------------------------\n\nTITLE: Building vLLM for ROCm\nDESCRIPTION: Complete process for building vLLM from source for AMD GPUs running ROCm 6.3. Includes installing AMD SMI, dependencies, and compiling vLLM with architecture-specific optimizations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install --upgrade pip\n\n# Build & install AMD SMI\n$ pip install /opt/rocm/share/amd_smi\n\n# Install dependencies\n$ pip install --upgrade numba scipy huggingface-hub[cli,hf_transfer] setuptools_scm\n$ pip install \"numpy<2\"\n$ pip install -r requirements/rocm.txt\n\n# Build vLLM for MI210/MI250/MI300.\n$ export PYTORCH_ROCM_ARCH=\"gfx90a;gfx942\"\n$ python3 setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Environment Variable for v1 Multiprocessing Control\nDESCRIPTION: Code reference showing the environment variable that controls whether multiprocessing is used in the v1 engine core, defaulting to off.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/envs.py#L452-L454\n```\n\n----------------------------------------\n\nTITLE: Downloading LoRA Adapter from Hugging Face\nDESCRIPTION: Downloads a LoRA adapter from Hugging Face Hub and saves it locally for use with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nsql_lora_path = snapshot_download(repo_id=\"yard1/llama-2-7b-sql-lora-test\")\n```\n\n----------------------------------------\n\nTITLE: Pulling Pre-built vLLM Docker Images\nDESCRIPTION: Commands to pull a pre-built vLLM Docker image from AWS ECR, targeting a specific commit. These images are intended for CI and testing purposes, not production use.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nexport VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch\ndocker pull public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:${VLLM_COMMIT}\n```\n\n----------------------------------------\n\nTITLE: Documenting MultiModalFieldConfig Class - RST\nDESCRIPTION: Documentation for the MultiModalFieldConfig class that handles configuration for multimodal fields.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.MultiModalFieldConfig\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Modal Fields for Fuyu Model\nDESCRIPTION: Configuration for Fuyu model's multi-modal tensor outputs. It specifies that image_patches field contains batched image data to be processed by the model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef _get_mm_fields_config(\n    self,\n    hf_inputs: BatchFeature,\n    hf_processor_mm_kwargs: Mapping[str, object],\n) -> Mapping[str, MultiModalFieldConfig]:\n    return dict(image_patches=MultiModalFieldConfig.batched(\"image\"))\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM Helm Chart\nDESCRIPTION: Command to install the vLLM Helm chart with a specified release name and namespace. Includes configuration for S3 credentials and endpoint settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/helm.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm upgrade --install --create-namespace --namespace=ns-vllm test-vllm . -f values.yaml --set secrets.s3endpoint=$ACCESS_POINT --set secrets.s3bucketname=$BUCKET --set secrets.s3accesskeyid=$ACCESS_KEY --set secrets.s3accesskey=$SECRET_KEY\n```\n\n----------------------------------------\n\nTITLE: Calculating and Reporting Queue Time for Finished Sequence Groups\nDESCRIPTION: Code that tracks and reports two different queue time metrics when a sequence group is finished. Shows the duplicated implementation that needs to be addressed by removing one of the metrics.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/metrics.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif seq_group.is_finished():\n    if (seq_group.metrics.first_scheduled_time is not None and\n            seq_group.metrics.first_token_time is not None):\n        time_queue_requests.append(\n            seq_group.metrics.first_scheduled_time -\n            seq_group.metrics.arrival_time)\n    ...\n    if seq_group.metrics.time_in_queue is not None:\n        time_in_queue_requests.append(\n            seq_group.metrics.time_in_queue)\n```\n\n----------------------------------------\n\nTITLE: Setting HPU PyTorch Bridge Environment Variables for vLLM\nDESCRIPTION: These environment variables control the PyTorch backend and enable tensor parallel inference with HPU Graphs for vLLM execution.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport PT_HPU_LAZY_MODE=1\nexport PT_HPU_ENABLE_LAZY_COLLECTIVES=true\n```\n\n----------------------------------------\n\nTITLE: Hash-based Prefix Caching Example (Text Representation)\nDESCRIPTION: Diagram demonstrating how vLLM hashes each KV-cache block based on the tokens in the block and the prefix before it, showing the hierarchical relationship between blocks in sequence.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/prefix_caching.md#2025-04-05_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n                    Block 1                  Block 2                  Block 3\n         [A gentle breeze stirred] [the leaves as children] [laughed in the distance]\nBlock 1: |<--- block tokens ---->|\nBlock 2: |<------- prefix ------>| |<--- block tokens --->|\nBlock 3: |<------------------ prefix -------------------->| |<--- block tokens ---->|\n```\n\n----------------------------------------\n\nTITLE: Displaying Preemption Warning in vLLM\nDESCRIPTION: This snippet shows a warning message that appears when a sequence group is preempted due to insufficient KV cache space. It indicates the preemption mode and suggests ways to mitigate the issue.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/performance/optimization.md#2025-04-05_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nWARNING 05-09 00:49:33 scheduler.py:1057 Sequence group 0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_cumulative_preemption_cnt=1\n```\n\n----------------------------------------\n\nTITLE: Defining Default Embedding Extra Parameters in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines additional parameters supported by the Embeddings API by default. These parameters provide extra control over the embedding generation process.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# begin-embedding-extra-params\n# end-embedding-extra-params\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM using uv Package Manager\nDESCRIPTION: Commands to create a Python virtual environment using uv, activate it, and install vLLM. This is the recommended installation method for better performance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nuv venv myenv --python 3.12 --seed\nsource myenv/bin/activate\nuv pip install vllm\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama Stack for Embedded vLLM Inference\nDESCRIPTION: YAML configuration for using the inline vLLM provider in Llama Stack. Specifies the model, provider type, and tensor parallel size for distributed inference.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/llamastack.md#2025-04-05_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninference\n  - provider_type: vllm\n    config:\n      model: Llama3.1-8B-Instruct\n      tensor_parallel_size: 4\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Processor Inputs for LLaVA Model Profiling\nDESCRIPTION: Implementation of dummy input generation for memory profiling in LLaVA model. It creates inputs with maximum dimensions to estimate worst-case memory usage, using image placeholders in the prompt text.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_dummy_processor_inputs(\n    self,\n    seq_len: int,\n    mm_counts: Mapping[str, int],\n) -> ProcessorInputs:\n    num_images = mm_counts.get(\"image\", 0)\n\n    processor = self.info.get_hf_processor()\n    image_token = processor.image_token\n  \n    hf_config = self.get_hf_config()\n    target_width, target_height = self.info.get_image_size_with_most_features()\n\n    mm_data = {\n        \"image\":\n        self._get_dummy_images(width=target_width,\n                               height=target_height,\n                               num_images=num_images)\n    }\n\n    return ProcessorInputs(\n        prompt_text=image_token * num_images,\n        mm_data=mm_data,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Nginx Configuration File\nDESCRIPTION: Creates a simple Nginx configuration file that sets up load balancing between two vLLM servers using the least connections algorithm.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_3\n\nLANGUAGE: nginx\nCODE:\n```\nupstream backend {\n    least_conn;\n    server vllm0:8000 max_fails=3 fail_timeout=10000s;\n    server vllm1:8000 max_fails=3 fail_timeout=10000s;\n}\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Required Model Registry Entry for Testing\nDESCRIPTION: Add a model entry to the registry.py file to enable unit tests that verify model initialization with dummy weights. Models should be listed in alphabetical order, and can specify minimum transformers version if needed.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/tests.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example entry format in tests/models/registry.py\n# (No actual code snippet provided in the text)\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation Table of Contents\nDESCRIPTION: A Sphinx toctree directive that organizes documentation for different framework integrations with vLLM, specifying a maximum depth of 1 for the navigation structure.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/index.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::{toctree}\n:maxdepth: 1\n\nbentoml\ncerebrium\ndstack\nhelm\nlws\nmodal\nskypilot\ntriton\n:::\n```\n\n----------------------------------------\n\nTITLE: Tool Calling with Reasoning Model\nDESCRIPTION: Python code demonstrating how to use tool calling with a reasoning model, including setting up tools and parsing the response.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            },\n            \"required\": [\"location\", \"unit\"]\n        }\n    }\n}]\n\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\nprint(response)\ntool_call = response.choices[0].message.tool_calls[0].function\n\nprint(f\"reasoning_content: {response.choices[0].message.reasoning_content}\")\nprint(f\"Function called: {tool_call.name}\")\nprint(f\"Arguments: {tool_call.arguments}\")\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM OpenAI-Compatible Server\nDESCRIPTION: Command to start the vLLM server that implements the OpenAI API protocol using the Qwen2.5-1.5B-Instruct model. This enables applications to use vLLM as a drop-in replacement for OpenAI API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nvllm serve Qwen/Qwen2.5-1.5B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for vLLM CPU Backend (Console)\nDESCRIPTION: This snippet installs necessary Python packages for building the vLLM CPU backend, including CMake, wheel, packaging, ninja, setuptools-scm, numpy, and CPU-specific PyTorch dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/build.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip install --upgrade pip\npip install \"cmake>=3.26\" wheel packaging ninja \"setuptools-scm>=8\" numpy\npip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Calculating CLIP Image Patch Count\nDESCRIPTION: Extracting the patch count calculation from CLIP's modeling code, which determines how many tokens an image requires based on image size and patch size.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nself.num_patches = (self.image_size // self.patch_size) ** 2\nself.num_positions = self.num_patches + 1\n```\n\n----------------------------------------\n\nTITLE: SkyPilot Service Configuration for Multi-Replica Deployment\nDESCRIPTION: YAML configuration for scaling vLLM to multiple service replicas with readiness probe for health checking.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  replicas: 2\n  # An actual request for readiness probe.\n  readiness_probe:\n    path: /v1/chat/completions\n    post_data:\n    model: $MODEL_NAME\n    messages:\n      - role: user\n        content: Hello! What is your name?\n  max_completion_tokens: 1\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with Sphinx toctree\nDESCRIPTION: Sphinx documentation tree structure definition using toctree directive to organize model development documentation sections. Includes links to interfaces_base, interfaces, and adapters pages with maxdepth set to 1.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/model/index.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::{toctree}\n:maxdepth: 1\n\ninterfaces_base\ninterfaces\nadapters\n:::\n```\n\n----------------------------------------\n\nTITLE: Value Processing and Dot Product in Multi-Head Attention (C++)\nDESCRIPTION: Retrieves value data and performs dot multiplication with logits. Each thread processes multiple v_vecs from different rows and the same columns, accumulating results in the accs array.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nfloat accs[NUM_ROWS_PER_THREAD];\nfor ... { // Iteration over different blocks.\n    logits_vec = ...\n    for ... { // Iteration over different rows.\n        v_vec = ...\n        ...\n        accs[i] += dot(logits_vec, v_vec);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Key Pointer in CUDA Kernel for vLLM Attention\nDESCRIPTION: Sets up the pointer to key data in the CUDA kernel. It calculates the memory offset based on the block number, head index, and token offset to access the correct key data for processing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nconst scalar_t* k_ptr = k_cache + physical_block_number * kv_block_stride\n                    + kv_head_idx * kv_head_stride\n                    + physical_block_offset * x;\n```\n\n----------------------------------------\n\nTITLE: Output Generation in Multi-Head Attention (C++)\nDESCRIPTION: Writes the calculated results from local register memory to final output global memory. Defines the output pointer and iterates over different assigned head positions to write the accumulated results.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nscalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE\n                + head_idx * max_num_partitions * HEAD_SIZE\n                + partition_idx * HEAD_SIZE;\n\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\nconst int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\nif (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n    from_float(*(out_ptr + row_idx), accs[i]);\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Provisioning vLLM Service with dstack\nDESCRIPTION: Command to run the dstack service using the configuration file. It shows the process of selecting a cloud provider and instance type.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/dstack.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ dstack run . -f serve.dstack.yml\n\n Getting run plan...\n Configuration  serve.dstack.yml\n Project        deep-diver-main\n User           deep-diver\n Min resources  2..xCPU, 8GB.., 1xGPU (24GB)\n Max price      -\n Max duration   -\n Spot policy    auto\n Retry policy   no\n\n #  BACKEND  REGION       INSTANCE       RESOURCES                               SPOT  PRICE\n 1  gcp   us-central1  g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n 2  gcp   us-east1     g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n 3  gcp   us-west1     g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n    ...\n Shown 3 of 193 offers, $5.876 max\n\nContinue? [y/n]: y\n Submitting run...\n Launching spicy-treefrog-1 (pulling)\nspicy-treefrog-1 provisioning completed (running)\nService is published at ...\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing Warning in vLLM\nDESCRIPTION: Console warning indicating CUDA initialization issues requiring spawn method for multiprocessing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nWARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously\n    initialized. We must use the `spawn` multiprocessing start method. Setting\n    VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See\n    https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing\n    for more information.\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client Chat Completion Example\nDESCRIPTION: Python code demonstrating how to use the OpenAI client to interact with the vLLM server for chat completions\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"token-abc123\",\n)\n\ncompletion = client.chat.completions.create(\n  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Creating Cerebrium Project\nDESCRIPTION: Command to initialize a new Cerebrium project for vLLM deployment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/cerebrium.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ncerebrium init vllm-project\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Compatible Server with BitsAndBytes Quantization\nDESCRIPTION: Command line argument to enable 4-bit in-flight quantization with BitsAndBytes when running vLLM's OpenAI-compatible server.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/bnb.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n--quantization bitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Documenting PlaceholderRange Class - RST\nDESCRIPTION: Documentation for the PlaceholderRange class that handles range operations for placeholders in multimodal inputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.PlaceholderRange\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Full Source Build Installation\nDESCRIPTION: Commands for building and installing vLLM from source with full compilation of C++ and CUDA code.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Autodoc Directive for vllm.LLMEngine\nDESCRIPTION: A reStructuredText code block that configures Sphinx's autodoc extension to generate API documentation for the vllm.LLMEngine class. The directive includes all class members and displays inheritance relationships.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/engine/llm_engine.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.LLMEngine\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Running Custom-Built vLLM Docker Image\nDESCRIPTION: Command to run a custom-built vLLM Docker image with GPU support and HuggingFace cache mounting.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ndocker run --runtime nvidia --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8000:8000 \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\n    vllm/vllm-openai <args...>\n```\n\n----------------------------------------\n\nTITLE: Incorrect vLLM Initialization\nDESCRIPTION: Example of incorrect way to initialize vLLM without proper main guard.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport vllm\n\nllm = vllm.LLM(...)\n```\n\n----------------------------------------\n\nTITLE: Launching Multi-Replica vLLM Service\nDESCRIPTION: Command to deploy the vLLM service with multiple replicas using SkyPilot's 'serve' functionality.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nHF_TOKEN=\"your-huggingface-token\" sky serve up -n vllm serving.yaml --env HF_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Pod Status\nDESCRIPTION: Command to monitor the deployment status of vLLM pods.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo kubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Server with LoRA Support\nDESCRIPTION: Command to start the vLLM server with LoRA modules enabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve meta-llama/Llama-2-7b-hf \\\n    --enable-lora \\\n    --lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/\n```\n\n----------------------------------------\n\nTITLE: Generating LLM Class Documentation with Sphinx Autodoc\nDESCRIPTION: A reStructuredText (RST) directive that uses Sphinx's autodoc extension to automatically generate documentation for the LLM class from the vllm library. The directive includes configuration to show all members and inheritance information.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/offline_inference/llm.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.LLM\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Python-only Development Installation\nDESCRIPTION: Commands for installing vLLM in development mode without compilation, using pre-compiled binaries.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nVLLM_USE_PRECOMPILED=1 pip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Neuron Devices in vLLM\nDESCRIPTION: This code snippet specifies the required dependencies for running vLLM on AWS Neuron devices. It includes the torch-neuronx library (version 2.5.0 or later) and the neuronx-cc compiler, along with common dependencies imported from a separate file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/neuron.txt#2025-04-05_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n# Common dependencies\n-r common.txt\n\n# Dependencies for Neuron devices\ntorch-neuronx >= 2.5.0\nneuronx-cc\n```\n\n----------------------------------------\n\nTITLE: Model Inspection Error\nDESCRIPTION: Error message when vLLM fails to inspect model architectures.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n  File \"vllm/model_executor/models/registry.py\", line xxx, in _raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures ['<arch>'] failed to be inspected. Please check the logs for more details.\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Server with Command Line\nDESCRIPTION: Command to start the vLLM server with a specified model, auto dtype detection and API key configuration\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Model and Tokenizer\nDESCRIPTION: Initializes a Hugging Face model and tokenizer for quantization, setting up the model in evaluation mode with appropriate configurations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Llama-2-70b-chat-hf\"\nMAX_SEQ_LEN = 512\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length=MAX_SEQ_LEN)\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n----------------------------------------\n\nTITLE: Uploading Batch File to AWS S3 in Console\nDESCRIPTION: Command to upload a local OpenAI batch file to an AWS S3 bucket using the AWS CLI.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\naws s3 cp offline_inference/openai/openai_example_batch.jsonl s3://MY_BUCKET/MY_INPUT_FILE.jsonl\n```\n\n----------------------------------------\n\nTITLE: Pod Status Output Example\nDESCRIPTION: Example output showing the status of running vLLM pods.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nNAME                                           READY   STATUS    RESTARTS   AGE\nvllm-deployment-router-859d8fb668-2x2b7        1/1     Running   0          2m38s\nvllm-opt125m-deployment-vllm-84dfc9bd7-vb9bs   1/1     Running   0          2m38s\n```\n\n----------------------------------------\n\nTITLE: Running GPTQ Model with vLLM CLI\nDESCRIPTION: Command line example for running a quantized model using vLLM's offline inference script.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gptqmodel.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython examples/offline_inference/llm_engine_example.py --model DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2\n```\n\n----------------------------------------\n\nTITLE: Python RuntimeError for Process Bootstrapping\nDESCRIPTION: Error message when attempting to start a new process before current process bootstrapping completion.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nRuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n\n        To fix this issue, refer to the \"Safe importing of main module\"\n        section in https://docs.python.org/3/library/multiprocessing.html\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages via pip\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/integrations/langchain.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install langchain langchain_community -q\n```\n\n----------------------------------------\n\nTITLE: Feature and Model Support Status Table in Markdown\nDESCRIPTION: Markdown table showing implementation status of various features and model types in vLLM V1, including references to relevant GitHub PRs and RFCs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/v1_user_guide.md#2025-04-05_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature / Model | Status |\n|-----------------|-------------------|\n| **Prefix Caching**                    | <nobr> Optimized</nobr>                                                        |\n| **Chunked Prefill**                    | <nobr> Optimized</nobr>                                                        |\n| **Logprobs Calculation**                    | <nobr> Functional</nobr>                                                        |\n| **LoRA**                                    | <nobr> Functional ([PR #13096](https://github.com/vllm-project/vllm/pull/13096))</nobr>|\n| **Multimodal Models**                       | <nobr> Functional</nobr>                                                        |\n| **FP8 KV Cache**                            | <nobr> Functional on Hopper devices ([PR #15191](https://github.com/vllm-project/vllm/pull/15191))</nobr>|\n| **Spec Decode**                             | <nobr> WIP ([PR #13933](https://github.com/vllm-project/vllm/pull/13933))</nobr>|\n| **Prompt Logprobs with Prefix Caching**     | <nobr> Planned ([RFC #13414](https://github.com/vllm-project/vllm/issues/13414))</nobr>|\n| **Structured Output Alternative Backends**  | <nobr> Planned</nobr>                                                           |\n| **Embedding Models**                        | <nobr> Planned ([RFC #12249](https://github.com/vllm-project/vllm/issues/12249))</nobr> |\n| **Mamba Models**                            | <nobr> Planned</nobr>                                                           |\n| **Encoder-Decoder Models**                  | <nobr> Planned</nobr>                                                           |\n| **Request-level Structured Output Backend** | <nobr> Deprecated</nobr>                                                        |\n| **best_of**                                 | <nobr> Deprecated ([RFC #13361](https://github.com/vllm-project/vllm/issues/13361))</nobr>|\n| **Per-Request Logits Processors**           | <nobr> Deprecated ([RFC #13360](https://github.com/vllm-project/vllm/pull/13360))</nobr> |\n| **GPU <> CPU KV Cache Swapping**            | <nobr> Deprecated</nobr>                                                        |\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Server with Tool Calling\nDESCRIPTION: Command to start vLLM server with tool calling enabled using Meta's Llama 3.1 8B model and llama3 tool calling chat template.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/tool_calling.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve meta-llama/Llama-3.1-8B-Instruct \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser llama3_json \\\n    --chat-template examples/tool_chat_template_llama3.1_json.jinja\n```\n\n----------------------------------------\n\nTITLE: Preparing Calibration DataLoader\nDESCRIPTION: Sets up a PyTorch DataLoader for calibration data using the Pile dataset, with tokenization and batching configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 1\nNUM_CALIBRATION_DATA = 512\n\n# Load the dataset and get calibration data.\ndataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\ntext_data = dataset[\"text\"][:NUM_CALIBRATION_DATA]\n\ntokenized_outputs = tokenizer(text_data, return_tensors=\"pt\",\n    padding=True, truncation=True, max_length=MAX_SEQ_LEN)\ncalib_dataloader = DataLoader(tokenized_outputs['input_ids'],\n    batch_size=BATCH_SIZE, drop_last=True)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling vLLM Stack\nDESCRIPTION: Command to remove the vLLM deployment using Helm.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsudo helm uninstall vllm\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM GUI Frontend\nDESCRIPTION: Command to deploy a web-based GUI frontend that connects to the running vLLM service, providing a chat interface for users.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nsky launch -c gui ./gui.yaml --env ENDPOINT=$(sky serve status --endpoint vllm)\n```\n\n----------------------------------------\n\nTITLE: Specifying pandas as a Dependency\nDESCRIPTION: This line indicates that the pandas library is required for the project. It doesn't specify a version, which means the latest version will be installed when using package managers like pip.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/kernels/requirements.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npandas\n```\n\n----------------------------------------\n\nTITLE: Bucket Generation without Ramp-up Example\nDESCRIPTION: Example showing how buckets are generated without a ramp-up phase. When min is equal to or greater than step, only the stable phase is used to generate evenly spaced buckets up to the max value.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nmin = 128, step = 128, max = 512\n=> ramp_up = ()\n=> stable = (128, 256, 384, 512)\n=> buckets = ramp_up + stable => (128, 256, 384, 512)\n```\n\n----------------------------------------\n\nTITLE: Expected Port Forwarding Output\nDESCRIPTION: Example output from the port-forwarding command showing successful connection setup between local port and Kubernetes service.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nForwarding from 127.0.0.1:8080 -> 8080\nForwarding from [::1]:8080 -> 8080\n```\n\n----------------------------------------\n\nTITLE: Navigating to docs directory in Bash\nDESCRIPTION: Changes the current working directory to the 'docs' folder.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\n```\n\n----------------------------------------\n\nTITLE: Logging Bucket Configuration for Intel Gaudi in vLLM\nDESCRIPTION: Sample log output showing the prompt and decode bucket configurations generated during vLLM startup. The logs display the bucket parameters (min, step, max) and all generated buckets for both prompt and decode phases.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nINFO 08-01 21:37:59 hpu_model_runner.py:493] Prompt bucket config (min, step, max_warmup) bs:[1, 32, 4], seq:[128, 128, 1024]\nINFO 08-01 21:37:59 hpu_model_runner.py:499] Generated 24 prompt buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024)]\nINFO 08-01 21:37:59 hpu_model_runner.py:504] Decode bucket config (min, step, max_warmup) bs:[1, 128, 4], seq:[128, 128, 2048]\nINFO 08-01 21:37:59 hpu_model_runner.py:509] Generated 48 decode buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantized Model Accuracy with lm_eval\nDESCRIPTION: Command to evaluate the accuracy of a quantized INT8 model using the lm_eval framework, which helps verify that the model maintains acceptable performance after quantization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ lm_eval --model vllm \\\n  --model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\",add_bos_token=true \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --limit 250 \\\n  --batch_size 'auto'\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for vLLM Multimodal Parse Module with Sphinx\nDESCRIPTION: This sphinx directive automatically generates documentation from the vllm.multimodal.parse module. It includes all members in the module and orders them according to their source code organization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/parse.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.multimodal.parse\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Verifying vLLM Server Readiness\nDESCRIPTION: Checks the logs of both vLLM containers to verify that they are running and listening on the correct port.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\ndocker logs vllm0 | grep Uvicorn\ndocker logs vllm1 | grep Uvicorn\n```\n\n----------------------------------------\n\nTITLE: Running an AWQ model with vLLM using command-line\nDESCRIPTION: This command demonstrates how to run an AWQ-quantized model (Llama-2-7b-Chat-AWQ) using vLLM's offline inference script.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/auto_awq.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython examples/offline_inference/llm_engine_example.py --model TheBloke/Llama-2-7b-Chat-AWQ --quantization awq\n```\n\n----------------------------------------\n\nTITLE: Declaring Key Vector Array in CUDA Kernel for vLLM Attention\nDESCRIPTION: Declares an array to store key vectors in register memory. This array is used to hold multiple vectors of key data for efficient processing within a single thread.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nK_vec k_vecs[NUM_VECS_PER_THREAD]\n```\n\n----------------------------------------\n\nTITLE: Launching Nginx Container\nDESCRIPTION: Launches the Nginx container, mapping port 8000 to 80, connecting it to the vllm_nginx network, and mounting the Nginx configuration directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\ndocker run -itd -p 8000:80 --network vllm_nginx -v ./nginx_conf/:/etc/nginx/conf.d/ --name nginx-lb nginx-lb:latest\n```\n\n----------------------------------------\n\nTITLE: Building Nginx Container\nDESCRIPTION: Builds the Nginx container using the previously created Dockerfile and tags it as 'nginx-lb'.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ndocker build . -f Dockerfile.nginx --tag nginx-lb\n```\n\n----------------------------------------\n\nTITLE: Speculative Decoding Benchmark\nDESCRIPTION: Commands to benchmark models with speculative decoding capabilities using the InstructCoder dataset.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_USE_V1=1 vllm serve meta-llama/Meta-Llama-3-8B-Instruct \\\n    --speculative-model \"[ngram]\" \\\n    --ngram_prompt_lookup_min 2 \\\n    --ngram-prompt-lookup-max 5 \\\n    --num_speculative_tokens 5\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 benchmarks/benchmark_serving.py \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --dataset-name hf \\\n    --dataset-path likaixin/InstructCoder \\\n    --num-prompts 2048\n```\n\n----------------------------------------\n\nTITLE: Setting vLLM Root Directory in Console\nDESCRIPTION: Sets the vLLM root directory as an environment variable for easy reference in subsequent commands.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nexport vllm_root=`pwd`\n```\n\n----------------------------------------\n\nTITLE: Checking Available Models\nDESCRIPTION: Curl command to query available models through the OpenAI-compatible API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -o- http://localhost:30080/models\n```\n\n----------------------------------------\n\nTITLE: Tensor Parallel Configuration for NUMA\nDESCRIPTION: Command to enable Tensor Parallel processing with NUMA architecture configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nVLLM_CPU_KVCACHE_SPACE=40 VLLM_CPU_OMP_THREADS_BIND=\"0-31|32-63\" vllm serve meta-llama/Llama-2-7b-chat-hf -tp=2 --distributed-executor-backend mp\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Server with Speculative Decoding\nDESCRIPTION: Command to start a vLLM server in online mode with speculative decoding enabled, using opt-125m as the draft model for opt-6.7b with configuration for 5 speculative tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model facebook/opt-6.7b \\\n    --seed 42 -tp 1 --gpu_memory_utilization 0.8 \\\n    --speculative_config '{\"model\": \"facebook/opt-125m\", \"num_speculative_tokens\": 5}'\n\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Network\nDESCRIPTION: Creates a Docker network named 'vllm_nginx' for communication between the vLLM and Nginx containers.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ndocker network create vllm_nginx\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM with CUDA 11.8\nDESCRIPTION: Installation command for vLLM with CUDA 11.8 support using environment variables to specify version requirements.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n# Install vLLM with CUDA 11.8.\nexport VLLM_VERSION=0.6.1.post1\nexport PYTHON_VERSION=310\npip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Batch Inference with Local File in Python\nDESCRIPTION: Command to execute vLLM batch inference using a local input file and specifying an output file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m vllm.entrypoints.openai.run_batch -i offline_inference/openai/openai_example_batch.jsonl -o results.jsonl --model meta-llama/Meta-Llama-3-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Extra Parameters Usage Example\nDESCRIPTION: Example showing how to pass extra parameters to the vLLM server through the OpenAI client\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n  ],\n  extra_body={\n    \"guided_choice\": [\"positive\", \"negative\"]\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Batch File with Embedding Requests in Text\nDESCRIPTION: Example of an OpenAI batch file containing embedding requests for the E5-Mistral-7B-Instruct model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"intfloat/e5-mistral-7b-instruct\", \"input\": \"You are a helpful assistant.\"}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"intfloat/e5-mistral-7b-instruct\", \"input\": \"You are an unhelpful assistant.\"}}\n```\n\n----------------------------------------\n\nTITLE: Serving Reasoning Model with vLLM CLI\nDESCRIPTION: Command to start a vLLM server with a reasoning model, enabling reasoning capabilities and specifying the parser.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n    --enable-reasoning --reasoning-parser deepseek_r1\n```\n\n----------------------------------------\n\nTITLE: SkyPilot Autoscaling Configuration for vLLM\nDESCRIPTION: YAML configuration for enabling autoscaling of vLLM service replicas based on query traffic, specifying minimum, maximum, and target QPS parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  replica_policy:\n    min_replicas: 2\n    max_replicas: 4\n    target_qps_per_replica: 2\n```\n\n----------------------------------------\n\nTITLE: Generating AWS S3 Presigned URLs in Python\nDESCRIPTION: Python script to generate presigned URLs for accessing and uploading files to AWS S3, using the boto3 library.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef generate_presigned_url(s3_client, client_method, method_parameters, expires_in):\n    \"\"\"\n    Generate a presigned Amazon S3 URL that can be used to perform an action.\n\n    :param s3_client: A Boto3 Amazon S3 client.\n    :param client_method: The name of the client method that the URL performs.\n    :param method_parameters: The parameters of the specified client method.\n    :param expires_in: The number of seconds the presigned URL is valid for.\n    :return: The presigned URL.\n    \"\"\"\n    try:\n        url = s3_client.generate_presigned_url(\n            ClientMethod=client_method, Params=method_parameters, ExpiresIn=expires_in\n        )\n    except ClientError:\n        raise\n    return url\n\n\ns3_client = boto3.client(\"s3\")\ninput_url = generate_presigned_url(\n    s3_client, \"get_object\", {\"Bucket\": \"MY_BUCKET\", \"Key\": \"MY_INPUT_FILE.jsonl\"}, 3600\n)\noutput_url = generate_presigned_url(\n    s3_client, \"put_object\", {\"Bucket\": \"MY_BUCKET\", \"Key\": \"MY_OUTPUT_FILE.jsonl\"}, 3600\n)\nprint(f\"{input_url=}\")\nprint(f\"{output_url=}\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Contents of OpenAI Batch File in Console\nDESCRIPTION: Command to display the contents of the downloaded OpenAI batch file using the cat command.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ cat offline_inference/openai/openai_example_batch.jsonl\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_completion_tokens\": 1000}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_completion_tokens\": 1000}}\n```\n\n----------------------------------------\n\nTITLE: Running Disaggregated Prefilling Example in vLLM\nDESCRIPTION: Refers to a bash script that demonstrates how to use disaggregated prefilling in vLLM. The script is located in the examples/online_serving directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/disagg_prefill.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexamples/online_serving/disaggregated_prefill.sh\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Auto-tuning Output\nDESCRIPTION: Performance comparison logs showing different configurations for matrix multiplication operations, including various Triton templates and their execution times.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nAUTOTUNE mm(8x2048, 2048x3072)\n  triton_mm_4 0.0130 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n  triton_mm_8 0.0134 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n```\n\n----------------------------------------\n\nTITLE: Using GGUF Model with Python LLM API for Chat Completion\nDESCRIPTION: Python code example demonstrating how to use a GGUF model with vLLM's Python API for chat completion. The example includes setting up a conversation, configuring sampling parameters, and generating responses.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gguf.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\n# In this script, we demonstrate how to pass input to the chat method:\nconversation = [\n   {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant\"\n   },\n   {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n   },\n   {\n      \"role\": \"assistant\",\n      \"content\": \"Hello! How can I assist you today?\"\n   },\n   {\n      \"role\": \"user\",\n      \"content\": \"Write an essay about the importance of higher education.\",\n   },\n]\n\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Create an LLM.\nllm = LLM(model=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n         tokenizer=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.chat(conversation, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n   prompt = output.prompt\n   generated_text = output.outputs[0].text\n   print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n----------------------------------------\n\nTITLE: Stopping vLLM Service\nDESCRIPTION: Command to stop and tear down the deployed vLLM service, releasing all associated resources.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nsky serve down vllm\n```\n\n----------------------------------------\n\nTITLE: Building vLLM Docker Image for Radeon RX7900 Series\nDESCRIPTION: Command to build the vLLM Docker image specifically for AMD Radeon RX7900 series GPUs (gfx1100) with ROCm 6.3 support. Uses an alternative base image for Navi architecture.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build --build-arg BASE_IMAGE=\"rocm/vllm-dev:navi_base\" -f docker/Dockerfile.rocm -t vllm-rocm .\n```\n\n----------------------------------------\n\nTITLE: Creating a Score Endpoint Batch File in Text Format\nDESCRIPTION: Example of how to format a batch file for the score endpoint with POST requests. The file contains JSON objects with model and text comparison parameters for reranking operations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/score\", \"body\": {\"model\": \"BAAI/bge-reranker-v2-m3\", \"text_1\": \"What is the capital of France?\", \"text_2\": [\"The capital of Brazil is Brasilia.\", \"The capital of France is Paris.\"]}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/score\", \"body\": {\"model\": \"BAAI/bge-reranker-v2-m3\", \"text_1\": \"What is the capital of France?\", \"text_2\": [\"The capital of Brazil is Brasilia.\", \"The capital of France is Paris.\"]}}\n```\n\n----------------------------------------\n\nTITLE: Executing Disaggregated Prefill Script in Bash\nDESCRIPTION: A reference to a bash script that demonstrates the usage of disaggregated prefilling in vLLM. The script is located in the examples/online_serving directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/vllm/distributed/kv_transfer/README.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[this file](../../../examples/online_serving/disaggregated_prefill.sh)\n```\n\n----------------------------------------\n\nTITLE: Request ID Headers Example\nDESCRIPTION: Example demonstrating the usage of request ID headers with both chat and completion endpoints\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n  ],\n  extra_headers={\n    \"x-request-id\": \"sentiment-classification-00001\",\n  }\n)\nprint(completion._request_id)\n\ncompletion = client.completions.create(\n  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n  prompt=\"A robot may not injure a human being\",\n  extra_headers={\n    \"x-request-id\": \"completion-test\",\n  }\n)\nprint(completion._request_id)\n```\n\n----------------------------------------\n\nTITLE: Serving GGUF Model with Tensor Parallelism\nDESCRIPTION: Command to serve a GGUF model with tensor parallelism across multiple GPUs, enabling distributed inference for better performance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gguf.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n# We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion.\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 --tensor-parallel-size 2\n```\n\n----------------------------------------\n\nTITLE: Generating Dockerfile Build Graph with dockerfilegraph CLI\nDESCRIPTION: This command uses the dockerfilegraph CLI tool to generate a PNG visualization of the Dockerfile's build stages and dependencies. It sets various options like output format, DPI, and label length.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/dockerfile/dockerfile.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndockerfilegraph -o png --legend --dpi 200 --max-label-length 50 --filename docker/Dockerfile\n```\n\n----------------------------------------\n\nTITLE: GGUF Model Download\nDESCRIPTION: Downloads a GGUF quantized model from Hugging Face Hub using the huggingface_hub library.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\nrepo_id = \"bartowski/Phi-3-medium-4k-instruct-GGUF\"\nfilename = \"Phi-3-medium-4k-instruct-IQ2_M.gguf\"\nprint(hf_hub_download(repo_id, filename=filename))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for transformers-neuronx\nDESCRIPTION: This snippet demonstrates how to set up a Python virtual environment, install necessary packages, and configure the Neuron repository for installing transformers-neuronx and its dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/neuron.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nsudo apt-get install -y python3.10-venv g++\n\npython3.10 -m venv aws_neuron_venv_pytorch\n\nsource aws_neuron_venv_pytorch/bin/activate\n\npip install ipykernel\npython3.10 -m ipykernel install --user --name aws_neuron_venv_pytorch --display-name \"Python (torch-neuronx)\"\npip install jupyter notebook\npip install environment_kernels\n\npython -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n\npython -m pip install wget\npython -m pip install awscli\n\npython -m pip install --upgrade neuronx-cc==2.* --pre torch-neuronx==2.1.* torchvision transformers-neuronx\n```\n\n----------------------------------------\n\nTITLE: Correct vLLM Initialization\nDESCRIPTION: Proper way to initialize vLLM with main guard to prevent multiprocessing issues.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/troubleshooting.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == '__main__':\n    import vllm\n\n    llm = vllm.LLM(...)\n```\n\n----------------------------------------\n\nTITLE: Serving GGUF Model with Manual HuggingFace Config\nDESCRIPTION: Command to serve a GGUF model with a manually specified HuggingFace config path, useful when the model is not natively supported by HuggingFace.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gguf.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n# If you model is not supported by huggingface you can manually provide a huggingface compatible config path\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 --hf-config-path Tinyllama/TInyLlama-1.1B-Chat-v1.0\n```\n\n----------------------------------------\n\nTITLE: Monitoring vLLM Service Status\nDESCRIPTION: Command to check the status of the deployed vLLM service, including replica count and endpoints.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nwatch -n10 sky serve status vllm\n```\n\n----------------------------------------\n\nTITLE: Text Generation with vLLM\nDESCRIPTION: Handles text generation using vLLM with configurable sampling parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython examples/offline_inference/basic/generate.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Tuning Environment Variables for vLLM on HPU\nDESCRIPTION: These environment variables allow fine-tuning of vLLM performance on HPU. They control warmup behavior, memory allocation for graphs, and strategies for prompt and decode graph capture.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport VLLM_SKIP_WARMUP=true\nexport VLLM_GRAPH_RESERVED_MEM=0.1\nexport VLLM_GRAPH_PROMPT_RATIO=0.3\nexport VLLM_GRAPH_PROMPT_STRATEGY=min_tokens\nexport VLLM_GRAPH_DECODE_STRATEGY=max_bs\n```\n\n----------------------------------------\n\nTITLE: Specifying PyTorch Dependencies for AMD ROCm in vLLM Project\nDESCRIPTION: This snippet defines the PyTorch, TorchVision, and TorchAudio versions compatible with AMD ROCm 6.2.4. It uses an extra index URL to fetch the appropriate wheel files.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/rocm-build.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.2.4\ntorch==2.6.0\ntorchvision==0.21.0\ntorchaudio==2.6.0\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Client with Video Inputs in Python\nDESCRIPTION: Python code showing how to connect to a local vLLM server and send a video URL for processing using the OpenAI client API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nvideo_url = \"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\"\n\n## Use video url in the payload\nchat_completion_from_url = client.chat.completions.create(\n    messages=[{\n        \"role\":\n        \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What's in this video?\"\n            },\n            {\n                \"type\": \"video_url\",\n                \"video_url\": {\n                    \"url\": video_url\n                },\n            },\n        ],\n    }],\n    model=model,\n    max_completion_tokens=64,\n)\n\nresult = chat_completion_from_url.choices[0].message.content\nprint(\"Chat completion output from image url:\", result)\n```\n\n----------------------------------------\n\nTITLE: Installing SkyPilot for vLLM Deployment\nDESCRIPTION: Commands to install the SkyPilot nightly build and verify cloud provider configuration, which are prerequisites for deploying vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install skypilot-nightly\nsky check\n```\n\n----------------------------------------\n\nTITLE: Scoring with vLLM\nDESCRIPTION: Performs scoring operations using vLLM with adjustable settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython examples/offline_inference/basic/score.py\n```\n\n----------------------------------------\n\nTITLE: Logging Output of HPU Graph Capture Process\nDESCRIPTION: Detailed logging output showing the HPU Graph capture process, including bucket configurations, memory allocation, warmup stages, and final capture statistics. Shows memory usage throughout different stages of initialization and graph capture.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nINFO 08-02 17:37:44 hpu_model_runner.py:493] Prompt bucket config (min, step, max_warmup) bs:[1, 32, 4], seq:[128, 128, 1024]\nINFO 08-02 17:37:44 hpu_model_runner.py:499] Generated 24 prompt buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024)]\nINFO 08-02 17:37:44 hpu_model_runner.py:504] Decode bucket config (min, step, max_warmup) bs:[1, 128, 4], seq:[128, 128, 2048]\nINFO 08-02 17:37:44 hpu_model_runner.py:509] Generated 48 decode buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]\nINFO 08-02 17:37:52 hpu_model_runner.py:430] Pre-loading model weights on hpu:0 took 14.97 GiB of device memory (14.97 GiB/94.62 GiB used) and 2.95 GiB of host memory (475.2 GiB/1007 GiB used)\nINFO 08-02 17:37:52 hpu_model_runner.py:438] Wrapping in HPU Graph took 0 B of device memory (14.97 GiB/94.62 GiB used) and -252 KiB of host memory (475.2 GiB/1007 GiB used)\nINFO 08-02 17:37:52 hpu_model_runner.py:442] Loading model weights took in total 14.97 GiB of device memory (14.97 GiB/94.62 GiB used) and 2.95 GiB of host memory (475.2 GiB/1007 GiB used)\nINFO 08-02 17:37:54 hpu_worker.py:134] Model profiling run took 504 MiB of device memory (15.46 GiB/94.62 GiB used) and 180.9 MiB of host memory (475.4 GiB/1007 GiB used)\nINFO 08-02 17:37:54 hpu_worker.py:158] Free device memory: 79.16 GiB, 39.58 GiB usable (gpu_memory_utilization=0.5), 15.83 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.4), 23.75 GiB reserved for KV cache\nINFO 08-02 17:37:54 hpu_executor.py:85] # HPU blocks: 1519, # CPU blocks: 0\nINFO 08-02 17:37:54 hpu_worker.py:190] Initializing cache engine took 23.73 GiB of device memory (39.2 GiB/94.62 GiB used) and -1.238 MiB of host memory (475.4 GiB/1007 GiB used)\nINFO 08-02 17:37:54 hpu_model_runner.py:1066] [Warmup][Prompt][1/24] batch_size:4 seq_len:1024 free_mem:55.43 GiB\n...\nINFO 08-02 17:38:22 hpu_model_runner.py:1066] [Warmup][Decode][48/48] batch_size:1 seq_len:128 free_mem:55.43 GiB\nINFO 08-02 17:38:22 hpu_model_runner.py:1159] Using 15.85 GiB/55.43 GiB of free device memory for HPUGraphs, 7.923 GiB for prompt and 7.923 GiB for decode (VLLM_GRAPH_PROMPT_RATIO=0.3)\nINFO 08-02 17:38:22 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][1/24] batch_size:1 seq_len:128 free_mem:55.43 GiB\n...\nINFO 08-02 17:38:26 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][11/24] batch_size:1 seq_len:896 free_mem:48.77 GiB\nINFO 08-02 17:38:27 hpu_model_runner.py:1066] [Warmup][Graph/Decode][1/48] batch_size:4 seq_len:128 free_mem:47.51 GiB\n...\nINFO 08-02 17:38:41 hpu_model_runner.py:1066] [Warmup][Graph/Decode][48/48] batch_size:1 seq_len:2048 free_mem:47.35 GiB\nINFO 08-02 17:38:41 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][12/24] batch_size:4 seq_len:256 free_mem:47.35 GiB\nINFO 08-02 17:38:42 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][13/24] batch_size:2 seq_len:512 free_mem:45.91 GiB\nINFO 08-02 17:38:42 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][14/24] batch_size:1 seq_len:1024 free_mem:44.48 GiB\nINFO 08-02 17:38:43 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][15/24] batch_size:2 seq_len:640 free_mem:43.03 GiB\nINFO 08-02 17:38:43 hpu_model_runner.py:1128] Graph/Prompt captured:15 (62.5%) used_mem:14.03 GiB buckets:[(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (4, 128), (4, 256)]\nINFO 08-02 17:38:43 hpu_model_runner.py:1128] Graph/Decode captured:48 (100.0%) used_mem:161.9 MiB buckets:[(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]\nINFO 08-02 17:38:43 hpu_model_runner.py:1206] Warmup finished in 49 secs, allocated 14.19 GiB of device memory\nINFO 08-02 17:38:43 hpu_executor.py:91] init_cache_engine took 37.92 GiB of device memory (53.39 GiB/94.62 GiB used) and 57.86 MiB of host memory (475.4 GiB/1007 GiB used)\n```\n\n----------------------------------------\n\nTITLE: Cloning vLLM Project Repository (Console)\nDESCRIPTION: This snippet clones the vLLM project repository from GitHub and changes the current directory to the cloned project folder.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/build.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/vllm-project/vllm.git vllm_source\ncd vllm_source\n```\n\n----------------------------------------\n\nTITLE: Launching OpenAI-Compatible Server for Audio Processing with Ultravox\nDESCRIPTION: Command to start a vLLM server using the Ultravox model for audio processing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve fixie-ai/ultravox-v0_5-llama-3_2-1b\n```\n\n----------------------------------------\n\nTITLE: Verifying vLLM Pod Status in Kubernetes\nDESCRIPTION: Command to check the status of the deployed vLLM pods in Kubernetes. This helps confirm that all leader and worker pods are running properly after deployment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Chat Interface with vLLM\nDESCRIPTION: Implements chat functionality using vLLM with sampling parameters support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython examples/offline_inference/basic/chat.py\n```\n\n----------------------------------------\n\nTITLE: Provisioning a Cloud TPU with Queued Resource API\nDESCRIPTION: Command to create a TPU v5e with 4 TPU chips using Google Cloud's queued resource API. This command requests a specific TPU configuration and adds the request to a queue maintained by the Cloud TPU service.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngcloud alpha compute tpus queued-resources create QUEUED_RESOURCE_ID \\\n--node-id TPU_NAME \\\n--project PROJECT_ID \\\n--zone ZONE \\\n--accelerator-type ACCELERATOR_TYPE \\\n--runtime-version RUNTIME_VERSION \\\n--service-account SERVICE_ACCOUNT\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration Example\nDESCRIPTION: Example YAML configuration file for vLLM server settings\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# config.yaml\n\nmodel: meta-llama/Llama-3.1-8B-Instruct\nhost: \"127.0.0.1\"\nport: 6379\nuvicorn-log-level: \"info\"\n```\n\n----------------------------------------\n\nTITLE: MLP-based Speculative Decoding in vLLM\nDESCRIPTION: Example code for configuring vLLM to use speculative decoding with specialized MLP speculator models that condition draft predictions on both context vectors and sampled tokens.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n    tensor_parallel_size=4,\n    speculative_config={\n        \"model\": \"ibm-ai-platform/llama3-70b-accelerator\",\n        \"draft_tensor_parallel_size\": 1,\n    },\n)\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n```\n\n----------------------------------------\n\nTITLE: Querying the vLLM Model API with curl\nDESCRIPTION: Example curl command to test the deployed model by sending a completion request to the API. This demonstrates how to interact with the vLLM OpenAI-compatible API endpoint.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ncurl http://localhost:8080/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n    \"prompt\": \"San Francisco is a\",\n    \"max_tokens\": 7,\n    \"temperature\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Audio Fetch Timeout Environment Variable\nDESCRIPTION: Command showing how to override the default audio fetch timeout (10 seconds) by setting an environment variable.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nexport VLLM_AUDIO_FETCH_TIMEOUT=<timeout>\n```\n\n----------------------------------------\n\nTITLE: Generation Config Setting\nDESCRIPTION: Sets the default generation configuration for the LLM model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n--generation-config auto\n```\n\n----------------------------------------\n\nTITLE: Running the FP8 Block Dense GEMM Benchmark\nDESCRIPTION: Command to execute the benchmark script that compares DeepGEMM, Triton, and CUTLASS kernel implementations. This will output detailed performance metrics including execution time, TFLOPS, and bandwidth across various matrix dimensions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/kernels/deepgemm/README.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_fp8_block_dense_gemm.py\n```\n\n----------------------------------------\n\nTITLE: Styling Table Cells in CSS\nDESCRIPTION: CSS styling to improve readability of compatibility matrix tables by reducing font size and centering content\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/compatibility_matrix.md#2025-04-05_snippet_0\n\nLANGUAGE: css\nCODE:\n```\ntd {\n    font-size: 0.8rem;\n    text-align: center;\n  }\n\n  th {\n    text-align: center;\n    font-size: 0.8rem;\n  }\n```\n\n----------------------------------------\n\nTITLE: ScaledEpilogueBias Calculation\nDESCRIPTION: Formulas for the ScaledEpilogueBias, which computes symmetric quantization for activations with bias.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_6\n\nLANGUAGE: math\nCODE:\n```\n\\widehat D = \\widehat A \\widehat B\n```\n\nLANGUAGE: math\nCODE:\n```\nD = s_a s_b \\widehat D + C \n```\n\nLANGUAGE: math\nCODE:\n```\nD = s_a s_b \\widehat A \\widehat B + C\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Service with SkyPilot\nDESCRIPTION: Command to launch the vLLM service using SkyPilot with a HuggingFace token for model access.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/skypilot.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nHF_TOKEN=\"your-huggingface-token\" sky launch serving.yaml --env HF_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Installing dstack Client and Starting Server\nDESCRIPTION: Commands to install the dstack client with all dependencies and start the dstack server.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/dstack.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install \"dstack[all]\ndstack server\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM and lm-evaluation-harness\nDESCRIPTION: Console command to install vLLM and lm-evaluation-harness packages for model loading and accuracy evaluation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npip install vllm lm-eval==0.4.4\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Dataset and Benchmarking vLLM Server\nDESCRIPTION: Script to download a ShareGPT dataset and run benchmark requests against the vLLM server to generate metrics for visualization in Grafana.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/prometheus_grafana/README.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\npython3 ../../../benchmarks/benchmark_serving.py \\\n    --model mistralai/Mistral-7B-v0.1 \\\n    --tokenizer mistralai/Mistral-7B-v0.1 \\\n    --endpoint /v1/completions \\\n    --dataset-name sharegpt \\\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --request-rate 3.0\n```\n\n----------------------------------------\n\nTITLE: Calibrating Scales for FP8 KV Cache using LLM Compressor (Python)\nDESCRIPTION: This snippet shows how to use LLM Compressor to generate calibrated scales for FP8 KV Cache. It loads a model and dataset, applies quantization, and saves the quantized model with calibrated scales.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quantized_kvcache.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom llmcompressor.transformers import oneshot\n\nMODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\nDATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\nDATASET_SPLIT = \"train_sft\"\n\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef process_and_tokenize(example):\n    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n    return tokenizer(\n        text,\n        padding=False,\n        max_length=MAX_SEQUENCE_LENGTH,\n        truncation=True,\n        add_special_tokens=False,\n    )\n\nds = ds.map(process_and_tokenize, remove_columns=ds.column_names)\n\nrecipe = \"\"\"\nquant_stage:\n    quant_modifiers:\n        QuantizationModifier:\n            kv_cache_scheme:\n                num_bits: 8\n                type: float\n                strategy: tensor\n                dynamic: false\n                symmetric: true\n\"\"\"\n\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    max_seq_length=MAX_SEQUENCE_LENGTH,\n    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-KV\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment for vLLM\nDESCRIPTION: Commands to create and activate a Python virtual environment for vLLM installation. This ensures a clean, isolated environment for the project.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv vllm-env\nsource vllm-env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Adapter Dynamically\nDESCRIPTION: API request to dynamically load a LoRA adapter at runtime.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/load_lora_adapter \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"lora_name\": \"sql_adapter\",\n    \"lora_path\": \"/path/to/sql-lora-adapter\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing Development Transformers Dockerfile\nDESCRIPTION: Dockerfile example for installing the development version of HuggingFace Transformers from source.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/docker.md#2025-04-05_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM vllm/vllm-openai:latest\n\nRUN uv pip install --system git+https://github.com/huggingface/transformers.git\n```\n\n----------------------------------------\n\nTITLE: Building vLLM Container\nDESCRIPTION: Builds the vLLM container using the Dockerfile provided in the vLLM project and tags it as 'vllm'.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/nginx.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ncd $vllm_root\ndocker build -f docker/Dockerfile . --tag vllm\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Dependencies\nDESCRIPTION: Installation of required OpenTelemetry packages including SDK, API, exporter and semantic conventions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install \\\n  'opentelemetry-sdk>=1.26.0,<1.27.0' \\\n  'opentelemetry-api>=1.26.0,<1.27.0' \\\n  'opentelemetry-exporter-otlp>=1.26.0,<1.27.0' \\\n  'opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0'\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS 3.x Scaled Matrix Multiplication for Blackwell GPUs\nDESCRIPTION: Sets up specialized CUTLASS 3.x kernels for Blackwell (SM100/101) GPUs when using CUDA 12.8+. These kernels support FP8 quantization for the latest NVIDIA architecture.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\n  # The cutlass_scaled_mm kernels for Blackwell (c3x, i.e. CUTLASS 3.x) require\n  # CUDA 12.8 or later\n  cuda_archs_loose_intersection(SCALED_MM_ARCHS \"10.0a;10.1a;12.0a\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND SCALED_MM_ARCHS)\n    set(SRCS\n      \"csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu\"\n      \"csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8.cu\"\n    )\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SCALED_MM_SM100=1\")\n    # Let scaled_mm_c2x know it doesn't need to build these arches\n    list(APPEND SCALED_MM_3X_ARCHS \"${SCALED_MM_ARCHS}\")\n    message(STATUS \"Building scaled_mm_c3x_sm100 for archs: ${SCALED_MM_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND SCALED_MM_ARCHS)\n      message(STATUS \"Not building scaled_mm_c3x_sm100 as CUDA Compiler version is \"\n                     \"not >= 12.8, we recommend upgrading to CUDA 12.8 or \"\n                     \"later if you intend on running FP8 quantized models on \"\n                     \"Blackwell.\")\n    else()\n      message(STATUS \"Not building scaled_mm_c3x_100 as no compatible archs found \"\n                     \"in CUDA target architectures\")\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Video Fetch Timeout Environment Variable\nDESCRIPTION: Command showing how to override the default video fetch timeout (30 seconds) by setting an environment variable.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/multimodal_inputs.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nexport VLLM_VIDEO_FETCH_TIMEOUT=<timeout>\n```\n\n----------------------------------------\n\nTITLE: GEMM Operation with Bias\nDESCRIPTION: Formula for the GEMM operation D = AB + C, where D is the output, A and B are input matrices, and C is the bias.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_2\n\nLANGUAGE: math\nCODE:\n```\nD = A B + C\n```\n\n----------------------------------------\n\nTITLE: Unloading LoRA Adapter Dynamically\nDESCRIPTION: API request to dynamically unload a LoRA adapter at runtime.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/lora.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/unload_lora_adapter \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"lora_name\": \"sql_adapter\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Profiling OpenAI Server with Nsight Systems\nDESCRIPTION: Commands to profile the vLLM OpenAI-compatible server using Nsight Systems with delay and duration parameters, along with a client command to generate load.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# server\nnsys profile -o report.nsys-rep --trace-fork-before-exec=true --cuda-graph-trace=node --delay 30 --duration 60 vllm serve meta-llama/Llama-3.1-8B-Instruct\n\n# client\npython benchmarks/benchmark_serving.py --backend vllm --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1 --dataset-name random --random-input 1024 --random-output 512\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for vLLM on RHEL 9.4\nDESCRIPTION: This snippet shows the command to install necessary packages using the dnf package manager on RHEL 9.4. These packages are prerequisites for building vLLM from source.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/s390x.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ndnf install -y \\\n    which procps findutils tar vim git gcc g++ make patch make cython zlib-devel \\\n    libjpeg-turbo-devel libtiff-devel libpng-devel libwebp-devel freetype-devel harfbuzz-devel \\\n    openssl-devel openblas openblas-devel wget autoconf automake libtool cmake numactl-devel\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch for ROCm\nDESCRIPTION: Commands to install PyTorch with ROCm 6.3 support from pre-built wheels. This removes any existing torch installation and installs the nightly build of PyTorch with ROCm 6.3 support.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# Install PyTorch\n$ pip uninstall torch -y\n$ pip install --no-cache-dir --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.3\n```\n\n----------------------------------------\n\nTITLE: Configuring MoE Extension for vllm\nDESCRIPTION: Sets up the build configuration for the MoE (Mixture of Experts) extension, including CUDA-specific sources and Marlin MOE kernels.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nset(VLLM_MOE_EXT_SRC\n  \"csrc/moe/torch_bindings.cpp\"\n  \"csrc/moe/moe_align_sum_kernels.cu\"\n  \"csrc/moe/topk_softmax_kernels.cu\")\n\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  list(APPEND VLLM_MOE_EXT_SRC \"csrc/moe/moe_wna16.cu\")\n  # Marlin MOE kernel configuration\nendif()\n\ndefine_gpu_extension_target(\n  _moe_C\n  DESTINATION vllm\n  LANGUAGE ${VLLM_GPU_LANG}\n  SOURCES ${VLLM_MOE_EXT_SRC}\n  COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n  ARCHITECTURES ${VLLM_GPU_ARCHES}\n  USE_SABI 3\n  WITH_SOABI)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling vLLM Helm Chart\nDESCRIPTION: Command to uninstall the vLLM deployment and remove all associated Kubernetes components including persistent volumes.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/helm.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm uninstall test-vllm --namespace=ns-vllm\n```\n\n----------------------------------------\n\nTITLE: Implementing Maximum Token Count for Multimodal Models\nDESCRIPTION: Method that overrides a base implementation to return the maximum number of tokens needed for each image in a multimodal input sequence.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_mm_max_tokens_per_item(\n    self,\n    seq_len: int,\n    mm_counts: Mapping[str, int],\n) -> Mapping[str, int]:\n    return {\"image\": self.get_max_image_tokens()}\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies and Running Tests for vLLM\nDESCRIPTION: Commands for setting up the development environment, including installing dependencies, configuring pre-commit hooks for linting and formatting, and running unit tests.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/overview.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements/dev.txt\n\n# Linting, formatting and static type checking\npre-commit install --hook-type pre-commit --hook-type commit-msg\n\n# You can manually run pre-commit with\npre-commit run --all-files\n\n# Unit tests\npytest tests/\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM Stack with Helm\nDESCRIPTION: Commands to add the vLLM Helm repository and install the vLLM stack using Helm.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo helm repo add vllm https://vllm-project.github.io/production-stack\nsudo helm install vllm vllm/vllm-stack -f tutorials/assets/values-01-minimal-example.yaml\n```\n\n----------------------------------------\n\nTITLE: Docker Daemon Configuration for BuildKit\nDESCRIPTION: JSON configuration for enabling BuildKit in Docker daemon, which is required for building the vLLM Docker images. This configuration should be placed in /etc/docker/daemon.json.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/rocm.inc.md#2025-04-05_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"features\": {\n        \"buildkit\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Machete Kernels for CUDA\nDESCRIPTION: Sets up the build configuration for Machete kernels, including source generation and architecture-specific flags.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\ncuda_archs_loose_intersection(MACHETE_ARCHS \"9.0a\" \"${CUDA_ARCHS}\")\nif(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND MACHETE_ARCHS)\n  set(MACHETE_GEN_SCRIPT\n    ${CMAKE_CURRENT_SOURCE_DIR}/csrc/quantization/machete/generate.py)\n  file(MD5 ${MACHETE_GEN_SCRIPT} MACHETE_GEN_SCRIPT_HASH)\n\n  # Source generation logic\n\n  file(GLOB MACHETE_GEN_SOURCES \"csrc/quantization/machete/generated/*.cu\")\n  list(APPEND VLLM_EXT_SRC ${MACHETE_GEN_SOURCES})\n\n  set_gencode_flags_for_srcs(\n    SRCS \"${MACHETE_GEN_SOURCES}\"\n    CUDA_ARCHS \"${MACHETE_ARCHS}\")\n\n  list(APPEND VLLM_EXT_SRC\n    csrc/quantization/machete/machete_pytorch.cu)\n\n  message(STATUS \"Building Machete kernels for archs: ${MACHETE_ARCHS}\")\nelse()\n  # Error messages for incompatible versions or architectures\nendif()\n```\n\n----------------------------------------\n\nTITLE: Additional Spawn Method Usage in vLLM\nDESCRIPTION: Code references showing other places in vLLM where the 'spawn' method is hard-coded.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L135\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/entrypoints/openai/api_server.py#L184\n```\n\n----------------------------------------\n\nTITLE: Configuring NVFP4 Build for CUDA\nDESCRIPTION: Sets up the build configuration for NVFP4 CUDA kernels, including source files and architecture-specific flags.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(FP4_ARCHS)\n  set(SRCS \"csrc/quantization/fp4/nvfp4_quant_kernels.cu\"\n      \"csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu\")\n  set_gencode_flags_for_srcs(\n    SRCS \"${SRCS}\"\n    CUDA_ARCHS \"${FP4_ARCHS}\")\n  list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n  list(APPEND VLLM_GPU_FLAGS \"-DENABLE_NVFP4=1\")\n  message(STATUS \"Building NVFP4 for archs: ${FP4_ARCHS}\")\nelse()\n  message(STATUS \"Not building NVFP4 as no compatible archs were found.\")\n  set(FP4_ARCHS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Determining Image Token Count for Vision-Language Models\nDESCRIPTION: Function that calculates the number of image tokens required, accounting for model-specific configurations like patch size and feature selection strategy.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_image_tokens(\n    self,\n    *,\n    image_width: int,\n    image_height: int,\n) -> int:\n    hf_config = self.get_hf_config()\n    hf_processor = self.get_hf_processor()\n\n    image_size = hf_config.vision_config.image_size\n    patch_size = hf_config.vision_config.patch_size\n\n    num_image_tokens = (image_size // patch_size) ** 2 + 1\n    if hf_processor.vision_feature_select_strategy == \"default\":\n        num_image_tokens -= 1\n\n    return num_image_tokens\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-modal Processing with Common Tests\nDESCRIPTION: Add a model to the common test file to verify that different input combinations (text/tokens with multi-modal data/cached multi-modal data) produce consistent outputs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/tests.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example entry in tests/models/multimodal/processing/test_common.py\n# (No actual code snippet provided in the text)\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents\nDESCRIPTION: ReStructuredText directive that creates a table of contents tree for offline inference documentation, linking to llm and llm_inputs pages.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/offline_inference/index.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:::{toctree}\n:caption: Contents\n:maxdepth: 1\n\nllm\nllm_inputs\n:::\n```\n\n----------------------------------------\n\nTITLE: Limiting Compilation Jobs for vLLM Installation\nDESCRIPTION: Sets the MAX_JOBS environment variable to limit the number of simultaneous compilation jobs when installing vLLM. This helps prevent system overload, especially on machines with limited resources like WSL.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nexport MAX_JOBS=6\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: VLLM Dependencies Configuration\nDESCRIPTION: Comprehensive requirements specification for VLLM project including common dependencies, NVIDIA GPU support, and platform-specific packages. Defines version constraints for Numba based on Python version, PyTorch ecosystem packages, and Ray framework for pipeline parallelism.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/cuda.txt#2025-04-05_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# Common dependencies\n-r common.txt\n\nnumba == 0.60.0; python_version == '3.9' # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding\nnumba == 0.61; python_version > '3.9'\n\n# Dependencies for NVIDIA GPUs\nray[cgraph]>=2.43.0, !=2.44.* # Ray Compiled Graph, required for pipeline parallelism in V1.\ntorch==2.6.0\ntorchaudio==2.6.0\n# These must be updated alongside torch\ntorchvision==0.21.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version\nxformers==0.0.29.post2; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.6.0\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Batch Inference with Remote File in Python\nDESCRIPTION: Command to execute vLLM batch inference using a remote input file from GitHub and specifying a local output file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npython -m vllm.entrypoints.openai.run_batch -i https://raw.githubusercontent.com/vllm-project/vllm/main/examples/offline_inference/openai/openai_example_batch.jsonl -o results.jsonl --model meta-llama/Meta-Llama-3-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Patches for Fuyu Model\nDESCRIPTION: Function to determine the number of image patches required for the Fuyu model, accounting for image resizing and patch dimensions.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_image_patches(\n    self,\n    *,\n    image_width: int,\n    image_height: int,\n) -> int:\n    image_processor = self.get_image_processor()\n    target_width = image_processor.size[\"width\"]\n    target_height = image_processor.size[\"height\"]\n    patch_width = image_processor.patch_size[\"width\"]\n    patch_height = image_processor.patch_size[\"height\"]\n\n    if not (image_width <= target_width and image_height <= target_height):\n        height_scale_factor = target_height / image_height\n        width_scale_factor = target_width / image_width\n        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n\n        image_height = int(image_height * optimal_scale_factor)\n        image_width = int(image_width * optimal_scale_factor)\n\n    ncols = math.ceil(image_width / patch_width)\n    nrows = math.ceil(image_height / patch_height)\n    return ncols * nrows\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Tokens Per Multi-Modal Item\nDESCRIPTION: Method to compute the maximum number of tokens needed for each type of multi-modal item. For images, it returns the maximum possible tokens based on the model's configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef get_mm_max_tokens_per_item(\n    self,\n    seq_len: int,\n    mm_counts: Mapping[str, int],\n) -> Mapping[str, int]:\n    return {\"image\": self.get_max_image_tokens()}\n```\n\n----------------------------------------\n\nTITLE: Viewing AWS S3 Batch Inference Results in Console\nDESCRIPTION: Command to download and display the results file from AWS S3 using the AWS CLI.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\naws s3 cp s3://MY_BUCKET/MY_OUTPUT_FILE.jsonl -\n```\n\n----------------------------------------\n\nTITLE: Installing Specific vLLM Commit Version\nDESCRIPTION: Commands for installing a specific commit version of vLLM using pip or uv with commit hash specification.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nexport VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch\npip install https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring TPU Dependencies with pip Requirements\nDESCRIPTION: Defines pip package requirements for TPU support, including core dependencies like CMake, Ray, and version-specific PyTorch XLA wheels. Includes pre-release configurations and custom package indexes for TPU-specific packages.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/tpu.txt#2025-04-05_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# Common dependencies\n-r common.txt\n\n# Dependencies for TPU\ncmake>=3.26\npackaging\nsetuptools-scm>=8\nwheel\njinja2>=3.1.6\nray[default]\nray[data]\n\n# Install torch_xla\n--pre\n--extra-index-url https://download.pytorch.org/whl/nightly/cpu\n--find-links https://storage.googleapis.com/libtpu-wheels/index.html\n--find-links https://storage.googleapis.com/libtpu-releases/index.html\n--find-links https://storage.googleapis.com/jax-releases/jax_nightly_releases.html\n--find-links https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\ntorch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250403-cp39-cp39-linux_x86_64.whl ; python_version == \"3.9\"\ntorch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250403-cp310-cp310-linux_x86_64.whl ; python_version == \"3.10\"\ntorch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250403-cp311-cp311-linux_x86_64.whl ; python_version == \"3.11\"\ntorch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250403-cp39-cp39-linux_x86_64.whl ; python_version == \"3.9\"\ntorch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250403-cp310-cp310-linux_x86_64.whl ; python_version == \"3.10\"\ntorch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250403-cp311-cp311-linux_x86_64.whl ; python_version == \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Managing Nsight Systems Profiling Sessions\nDESCRIPTION: Commands to list active Nsight Systems profiling sessions and manually stop a specific profiling session to generate the profiling report.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnsys sessions list\n```\n\n----------------------------------------\n\nTITLE: Checking FP4 Support for Blackwell GPUs\nDESCRIPTION: Identifies if the build environment supports FP4 on Blackwell GPUs by checking for CUDA 12.8+ and compatible architectures.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\n  # FP4 Archs and flags\n  cuda_archs_loose_intersection(FP4_ARCHS \"10.0a\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.8 AND FP4_ARCHS)\n```\n\n----------------------------------------\n\nTITLE: Stopping a Nsight Profiling Session\nDESCRIPTION: Command to stop a specific Nsight Systems profiling session using its session ID, which triggers report generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnsys stop --session=profile-XXXXX\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Modal Fields for LLaVA Model\nDESCRIPTION: Configuration for LLaVA model's multi-modal tensor outputs. It specifies that pixel_values field contains batched image data to be processed correctly by the model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef _get_mm_fields_config(\n    self,\n    hf_inputs: BatchFeature,\n    hf_processor_mm_kwargs: Mapping[str, object],\n) -> Mapping[str, MultiModalFieldConfig]:\n    return dict(\n        pixel_values=MultiModalFieldConfig.batched(\"image\"),\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Server\nDESCRIPTION: Command to start the vLLM server using the unsloth/Llama-3.2-1B-Instruct model\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/metrics.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nvllm serve unsloth/Llama-3.2-1B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Installing Latest vLLM Development Version\nDESCRIPTION: Commands for installing the latest development version of vLLM using pip with pre-release flags.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip install vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n----------------------------------------\n\nTITLE: Installing XPU-enabled PyTorch Dependencies\nDESCRIPTION: Specifies dependencies required for XPU acceleration including PyTorch 2.6.0+xpu, associated libraries, and Intel extensions. Includes common dependencies and additional index URLs for Intel packages.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/xpu.txt#2025-04-05_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# Common dependencies\n-r common.txt\n\nray>=2.9\ncmake>=3.26\npackaging\nsetuptools-scm>=8\nsetuptools>=75.8.0\nwheel\njinja2>=3.1.6\ndatasets # for benchmark scripts\n\ntorch==2.6.0+xpu\ntorchaudio\ntorchvision\npytorch-triton-xpu\n--extra-index-url=https://download.pytorch.org/whl/xpu\n\n# Please refer xpu doc, we need manually install intel-extension-for-pytorch 2.6.10+xpu due to there are some conflict dependencies with torch 2.6.0+xpu\n# FIXME: This will be fix in ipex 2.7. just leave this here for awareness.\n# intel-extension-for-pytorch==2.6.10+xpu\noneccl_bind_pt==2.6.0+xpu\n--extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n```\n\n----------------------------------------\n\nTITLE: Viewing Usage Statistics Data in Bash\nDESCRIPTION: Command to view the collected usage statistics data from the local configuration directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/usage_stats.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntail ~/.config/vllm/usage_stats.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Cerebrium Dependencies\nDESCRIPTION: TOML configuration for specifying the Docker base image and required pip packages for the vLLM deployment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/cerebrium.md#2025-04-05_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[cerebrium.deployment]\ndocker_base_image_url = \"nvidia/cuda:12.1.1-runtime-ubuntu22.04\"\n\n[cerebrium.dependencies.pip]\nvllm = \"latest\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Fuyu Prompt Updates\nDESCRIPTION: Full implementation of _get_prompt_updates for Fuyu processor that handles image tokens, newline tokens, and BOS token placement.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef _get_prompt_updates(\n    self,\n    mm_items: MultiModalDataItems,\n    hf_processor_mm_kwargs: Mapping[str, object],\n    out_mm_kwargs: MultiModalKwargs,\n) -> Sequence[PromptUpdate]:\n    hf_config = self.info.get_hf_config()\n    bos_token_id = hf_config.bos_token_id\n    assert isinstance(bos_token_id, int)\n\n    tokenizer = self.info.get_tokenizer()\n    eot_token_id = tokenizer.bos_token_id\n    assert isinstance(eot_token_id, int)\n\n    def get_replacement_fuyu(item_idx: int):\n        images = mm_items.get_items(\"image\", ImageProcessorItems)\n        image_size = images.get_image_size(item_idx)\n\n        ncols, nrows = self.info.get_image_feature_grid_size(\n            image_width=image_size.width,\n            image_height=image_size.height,\n        )\n        image_tokens = ([_IMAGE_TOKEN_ID] * ncols +\n                        [_NEWLINE_TOKEN_ID]) * nrows\n\n        return PromptUpdateDetails(\n            full=image_tokens + [bos_token_id],\n            features=image_tokens,\n        )\n\n    return [\n        PromptReplacement(\n            modality=\"image\",\n            target=[eot_token_id],\n            replacement=get_replacement_fuyu,\n        )\n    ]\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for External Integrations in Markdown\nDESCRIPTION: This code snippet creates a table of contents using Markdown syntax, listing various external integrations for the vLLM project. It uses the toctree directive to organize the content.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/index.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# External Integrations\n\n:::{toctree}\n:maxdepth: 1\n\nkserve\nkubeai\nllamastack\nllmaz\nproduction-stack\n:::\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM on Unsupported Operating Systems\nDESCRIPTION: Instructions for building vLLM on non-Linux systems for development purposes. This disables device-specific binaries, allowing imports but not full functionality.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/cuda.inc.md#2025-04-05_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nexport VLLM_TARGET_DEVICE=empty\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing LLM Compressor Library for vLLM INT8 Quantization\nDESCRIPTION: Command to install the llm-compressor library, which is required for using INT8 quantization with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install llmcompressor\n```\n\n----------------------------------------\n\nTITLE: Warning Message for CUDA Initialization and Spawn Method\nDESCRIPTION: Console output showing the warning message displayed when CUDA was previously initialized and vLLM must use the 'spawn' method.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nWARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously\n    initialized. We must use the `spawn` multiprocessing start method. Setting\n    VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See\n    https://docs.vllm.ai/en/latest/getting_started/debugging.html#python-multiprocessing\n    for more information.\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS 2.x Scaled Matrix Multiplication for Earlier Architectures\nDESCRIPTION: Sets up CUTLASS 2.x kernels for architectures that aren't covered by the 3.x implementation. This ensures backward compatibility with older NVIDIA GPU generations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n  #\n  # For the cutlass_scaled_mm kernels we want to build the c2x (CUTLASS 2.x)\n  # kernels for the remaining archs that are not already built for 3x.\n  cuda_archs_loose_intersection(SCALED_MM_2X_ARCHS\n    \"7.5;8.0;8.6;8.7;8.9;9.0;10.0;10.1;12.0\" \"${CUDA_ARCHS}\")\n  # subtract out the archs that are already built for 3x\n  list(REMOVE_ITEM SCALED_MM_2X_ARCHS ${SCALED_MM_3X_ARCHS})\n  if (SCALED_MM_2X_ARCHS)\n    set(SRCS \"csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_2X_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SCALED_MM_C2X=1\")\n    message(STATUS \"Building scaled_mm_c2x for archs: ${SCALED_MM_2X_ARCHS}\")\n  else()\n    if (SCALED_MM_3X_ARCHS)\n      message(STATUS \"Not building scaled_mm_c2x as all archs are already built\"\n                     \" for and covered by scaled_mm_c3x\")\n    else()\n      message(STATUS \"Not building scaled_mm_c2x as no compatible archs found \"\n                    \"in CUDA target architectures\")\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Deploying to Cerebrium\nDESCRIPTION: Command to deploy the vLLM project to Cerebrium cloud.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/cerebrium.md#2025-04-05_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ncerebrium deploy\n```\n\n----------------------------------------\n\nTITLE: Registering Processor Classes in vLLM\nDESCRIPTION: Example of registering multimodal processor classes with the vLLM registry using decorator syntax.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/multimodal.md#2025-04-05_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@MULTIMODAL_REGISTRY.register_processor(YourMultiModalProcessor,\n                                        info=YourProcessingInfo,\n                                        dummy_inputs=YourDummyInputsBuilder)\nclass YourModelForImage2Seq(nn.Module, SupportsMultiModal):\n```\n\n----------------------------------------\n\nTITLE: Online Benchmark Script Execution\nDESCRIPTION: Script to run online benchmarking against a served model using the ShareGPT dataset.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model NousResearch/Hermes-3-Llama-3.1-8B \\\n  --endpoint /v1/completions \\\n  --dataset-name sharegpt \\\n  --dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --num-prompts 10\n```\n\n----------------------------------------\n\nTITLE: Installing Miniconda on TPU VM\nDESCRIPTION: Commands to download and install Miniconda on the TPU virtual machine. Miniconda provides the environment management needed for vLLM installation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Installing llm-compressor for INT4 Quantization in vLLM\nDESCRIPTION: Command to install the llm-compressor library, which is required for INT4 quantization with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install llmcompressor\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in Sphinx Documentation for vLLM Integrations\nDESCRIPTION: This snippet configures a table of contents (toctree) directive in Sphinx documentation. It sets up navigation links to pages about integrating vLLM with LangChain and LlamaIndex frameworks, with a maximum depth of 1 level.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/integrations/index.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::{toctree}\n:maxdepth: 1\n\nlangchain\nllamaindex\n:::\n```\n\n----------------------------------------\n\nTITLE: Bucket Generation with Ramp-up Example\nDESCRIPTION: Example showing how buckets are generated with a ramp-up phase. The ramp-up phase multiplies the min value by consecutive powers of two until reaching the step value, creating efficient bucket distribution for smaller batch sizes.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nmin = 2, step = 32, max = 64\n=> ramp_up = (2, 4, 8, 16)\n=> stable = (32, 64)\n=> buckets = ramp_up + stable => (2, 4, 8, 16, 32, 64)\n```\n\n----------------------------------------\n\nTITLE: Implementing get_input_embeddings Method in Python\nDESCRIPTION: Shows how to implement the get_input_embeddings method inside the MyModel class to return text embeddings given input_ids. This provides a unified interface for potential use in multimodal models.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/model/basic.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(nn.Module):\n        ...\n\n    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n        ... \n```\n\n----------------------------------------\n\nTITLE: Analyzing Python Profile Results with snakeviz\nDESCRIPTION: Commands to install and use snakeviz, a visualization tool for Python profiling results, to analyze profile data generated by vLLM's profiling utilities.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install snakeviz\nsnakeviz expensive_function.prof\n```\n\n----------------------------------------\n\nTITLE: Building vLLM from Source for TPU\nDESCRIPTION: Command to build vLLM from source with TPU support by setting the VLLM_TARGET_DEVICE environment variable to \"tpu\" during installation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_TARGET_DEVICE=\"tpu\" python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Installing AutoAWQ via pip\nDESCRIPTION: This command installs the AutoAWQ library using pip, which is necessary for model quantization.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/auto_awq.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install autoawq\n```\n\n----------------------------------------\n\nTITLE: Silencing Specific vLLM Logger Configuration\nDESCRIPTION: Configuration example showing how to silence a specific vLLM logger while maintaining root logger configuration. This includes both the root vLLM logger setup and the target logger silencing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/other/logging_configuration.md#2025-04-05_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"formatters\": {\n    \"vllm\": {\n      \"class\": \"vllm.logging_utils.NewLineFormatter\",\n      \"datefmt\": \"%m-%d %H:%M:%S\",\n      \"format\": \"%(levelname)s %(asctime)s %(filename)s:%(lineno)d] %(message)s\"\n    }\n  },\n  \"handlers\": {\n    \"vllm\": {\n      \"class\" : \"logging.StreamHandler\",\n      \"formatter\": \"vllm\",\n      \"level\": \"INFO\",\n      \"stream\": \"ext://sys.stdout\"\n    }\n  },\n  \"loggers\": {\n    \"vllm\": {\n      \"handlers\": [\"vllm\"],\n      \"level\": \"DEBUG\",\n      \"propagate\": false\n    },\n    \"vllm.example_noisy_logger\": {\n      \"propagate\": false\n    }\n  },\n  \"version\": 1\n}\n```\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_LOGGING_CONFIG_PATH=/path/to/logging_config.json \\\n    vllm serve mistralai/Mistral-7B-v0.1 --max-model-len 2048\n```\n\n----------------------------------------\n\nTITLE: Setting Diagnostic and Profiling Environment Variables for vLLM on HPU\nDESCRIPTION: These environment variables control various diagnostic and profiling features in vLLM when running on HPU (Habana Processing Unit). They enable logging of graph compilations, CPU fallbacks, and profiler traces.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport VLLM_PROFILER_ENABLED=true\nexport VLLM_HPU_LOG_STEP_GRAPH_COMPILATION=true\nexport VLLM_HPU_LOG_STEP_GRAPH_COMPILATION_ALL=true\nexport VLLM_HPU_LOG_STEP_CPU_FALLBACKS=true\nexport VLLM_HPU_LOG_STEP_CPU_FALLBACKS_ALL=true\n```\n\n----------------------------------------\n\nTITLE: Documenting PromptType Enumeration in vLLM\nDESCRIPTION: Auto-generates documentation for the PromptType enumeration class from vllm.inputs module.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/offline_inference/llm_inputs.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autodata:: vllm.inputs.PromptType\n```\n\n----------------------------------------\n\nTITLE: Cleaning previous documentation build\nDESCRIPTION: Removes the previous documentation build using the 'make clean' command.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for vLLM with TPU Support\nDESCRIPTION: Command to build a Docker image with TPU support using the provided Dockerfile.tpu. This creates a containerized environment for running vLLM on TPUs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\ndocker build -f docker/Dockerfile.tpu -t vllm-tpu .\n```\n\n----------------------------------------\n\nTITLE: Installing GPTQModel Package\nDESCRIPTION: Command to install the GPTQModel package using pip with specific installation flags for compatibility.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/gptqmodel.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install -U gptqmodel --no-build-isolation -v\n```\n\n----------------------------------------\n\nTITLE: Models API Response\nDESCRIPTION: Example JSON response from the models API endpoint.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"facebook/opt-125m\",\n      \"object\": \"model\",\n      \"created\": 1737428424,\n      \"owned_by\": \"vllm\",\n      \"root\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Auto-generating documentation for vLLM's multimodal profiling module using Sphinx\nDESCRIPTION: This RST code snippet uses Sphinx's automodule directive to automatically generate documentation for all members in the vllm.multimodal.profiling module, ordering them by source order.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/profiling.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.multimodal.profiling\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for vLLM on Intel Gaudi\nDESCRIPTION: Commands to build a Docker image for vLLM configured for Intel Gaudi, and run a container with the appropriate runtime settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ndocker build -f docker/Dockerfile.hpu -t vllm-hpu-env  .\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --rm vllm-hpu-env\n```\n\n----------------------------------------\n\nTITLE: Installing BitsAndBytes Package for vLLM\nDESCRIPTION: Command to install the BitsAndBytes package with version 0.45.3 or higher as a prerequisite for using BitsAndBytes quantization with vLLM.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/bnb.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install bitsandbytes>=0.45.3\n```\n\n----------------------------------------\n\nTITLE: Installing Llama Stack via pip\nDESCRIPTION: Command to install Llama Stack using pip package manager.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/llamastack.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install llama-stack -q\n```\n\n----------------------------------------\n\nTITLE: Generating Module Documentation with Sphinx in RST\nDESCRIPTION: This code snippet uses Sphinx's automodule directive to automatically generate documentation for the vllm.multimodal.processing module. It includes all members of the module and orders them by source.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/processing.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.multimodal.processing\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Serving documentation locally with Python HTTP server\nDESCRIPTION: Starts a local HTTP server using Python's built-in module to serve the generated HTML documentation on the default port 8000.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m http.server -d build/html/\n```\n\n----------------------------------------\n\nTITLE: Installing Neuron Drivers and Tools on Ubuntu\nDESCRIPTION: This snippet shows how to configure the Neuron repository, update OS packages, and install necessary drivers and tools for AWS Neuron SDK on Ubuntu.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/neuron.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n. /etc/os-release\nsudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF\ndeb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\nEOF\nwget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -\n\nsudo apt-get update -y\n\nsudo apt-get install linux-headers-$(uname -r) -y\n\nsudo apt-get install git -y\n\nsudo apt-get install aws-neuronx-dkms=2.* -y\n\nsudo apt-get install aws-neuronx-collectives=2.* -y\nsudo apt-get install aws-neuronx-runtime-lib=2.* -y\n\nsudo apt-get install aws-neuronx-tools=2.* -y\n\nexport PATH=/opt/aws/neuron/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Installing AutoFP8 for Deprecated Quantization Flow\nDESCRIPTION: Command to install the AutoFP8 library which was previously used for FP8 quantization but is now deprecated in favor of llm-compressor.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/neuralmagic/AutoFP8.git\npip install -e AutoFP8\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama Stack for Remote vLLM Inference\nDESCRIPTION: YAML configuration for setting up Llama Stack to use a remote vLLM server for inference. Specifies the provider type and URL of the vLLM server.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/llamastack.md#2025-04-05_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninference:\n  - provider_id: vllm0\n    provider_type: remote::vllm\n    config:\n      url: http://127.0.0.1:8000\n```\n\n----------------------------------------\n\nTITLE: Documenting MultiModalFieldElem Class - RST\nDESCRIPTION: Documentation for the MultiModalFieldElem class that represents individual elements in multimodal fields.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.MultiModalFieldElem\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Common Build Error Example\nDESCRIPTION: Example of compilation errors that may occur when C++ headers cannot be found during the build process. This typically indicates an issue with Command Line Tools installation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/apple.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[...] fatal error: 'map' file not found\n          1 | #include <map>\n            |          ^~~~~\n      1 error generated.\n      [2/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o\n\n[...] fatal error: 'cstddef' file not found\n         10 | #include <cstddef>\n            |          ^~~~~~~~~\n      1 error generated.\n```\n\n----------------------------------------\n\nTITLE: Preparing Calibration Data for INT8 Quantization\nDESCRIPTION: Code that prepares calibration data for INT8 quantization by loading a dataset, preprocessing it, and tokenizing the text. This ensures accurate quantization of activations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int8.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\n# Load and preprocess the dataset\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\nds = ds.map(preprocess)\n\ndef tokenize(sample):\n    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n```\n\n----------------------------------------\n\nTITLE: Sponsor List in Markdown\nDESCRIPTION: Structured markdown listing of project sponsors categorized by contribution type (cash donations, compute resources, and Slack sponsorship). Includes organizations supporting vLLM through financial means and computing infrastructure.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/community/sponsors.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with README.md. -->\n\nCash Donations:\n\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- Intel\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nSlack Sponsor: Anyscale\n```\n\n----------------------------------------\n\nTITLE: Documenting MultiModalKwargs Class - RST\nDESCRIPTION: Documentation for the MultiModalKwargs class that handles keyword arguments for multimodal processing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.MultiModalKwargs\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Building and Installing vLLM CPU Backend (Console)\nDESCRIPTION: This snippet builds and installs the vLLM CPU backend using the setup.py script, with the VLLM_TARGET_DEVICE environment variable set to 'cpu'.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/build.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nVLLM_TARGET_DEVICE=cpu python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Preparing Calibration Data for INT4 Quantization in vLLM\nDESCRIPTION: Script to prepare calibration data from the ultrachat dataset for INT4 quantization, including tokenization and preprocessing steps.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/int4.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\n# Load and preprocess the dataset\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\nds = ds.map(preprocess)\n\ndef tokenize(sample):\n    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n```\n\n----------------------------------------\n\nTITLE: Using Quark Quantization Script\nDESCRIPTION: Command-line example showing how to use Quark's quantization script for a streamlined quantization process.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/quark.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\npython3 quantize_quark.py --model_dir meta-llama/Llama-2-70b-chat-hf \\\n                          --output_dir /path/to/output \\\n                          --quant_scheme w_fp8_a_fp8 \\\n                          --kv_cache_dtype fp8 \\\n                          --quant_algo autosmoothquant \\\n                          --num_calib_data 512 \\\n                          --model_export hf_format \\\n                          --tasks gsm8k\n```\n\n----------------------------------------\n\nTITLE: Documenting MultiModalInputs Class - RST\nDESCRIPTION: Documentation for the main MultiModalInputs class that manages all multimodal input processing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/inputs.md#2025-04-05_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.multimodal.inputs.MultiModalInputs\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for vLLM XPU Backend\nDESCRIPTION: Installs the required Python packages for building the vLLM XPU backend. This step is necessary before building the wheel from source.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install --upgrade pip\npip install -v -r requirements/xpu.txt\n```\n\n----------------------------------------\n\nTITLE: Offline Quantization with Static Activation Scaling using AutoFP8\nDESCRIPTION: Deprecated method showing how to perform FP8 quantization with static activation scaling factors using AutoFP8. This includes loading calibration data, defining quantization configs, and saving the quantized model.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig\n\npretrained_model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nquantized_model_dir = \"Meta-Llama-3-8B-Instruct-FP8\"\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load and tokenize 512 dataset samples for calibration of activation scales\nds = load_dataset(\"mgoin/ultrachat_2k\", split=\"train_sft\").select(range(512))\nexamples = [tokenizer.apply_chat_template(batch[\"messages\"], tokenize=False) for batch in ds]\nexamples = tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n\n# Define quantization config with static activation scales\nquantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"static\")\n\n# Load the model, quantize, and save checkpoint\nmodel = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\nmodel.quantize(examples)\nmodel.save_quantized(quantized_model_dir)\n```\n\n----------------------------------------\n\nTITLE: Regex-Guided Email Generation\nDESCRIPTION: Demonstrates using guided_regex parameter to generate an email address following a specific pattern using vLLM's OpenAI-compatible API.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/structured_outputs.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-3B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate an example email address for Alan Turing, who works in Enigma. End in .com and new line. Example result: alan.turing@enigma.com\\n\",\n        }\n    ],\n    extra_body={\"guided_regex\": \"\\w+@\\w+\\.com\\n\", \"stop\": [\"\\n\"]},\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Importing vLLM Engine Module for Documentation\nDESCRIPTION: This snippet uses the automodule directive to import the vLLM engine module for the purpose of generating documentation. This automatically includes all module members, functions, classes, and documentation strings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/engine/index.md#2025-04-05_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: vllm.engine\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for vLLM\nDESCRIPTION: Commands to create a new conda environment named 'vllm' with Python 3.12 and activate it. Note that conda is recommended only for environment creation, not for package installation due to PyTorch's deprecation of the conda release channel.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/python_env_setup.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# (Recommended) Create a new conda environment.\nconda create -n vllm python=3.12 -y\nconda activate vllm\n```\n\n----------------------------------------\n\nTITLE: Loading a Quantized Model in vLLM (Deprecated Flow)\nDESCRIPTION: Code showing how to load a model quantized with the deprecated AutoFP8 method in vLLM for inference.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/quantization/fp8.md#2025-04-05_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nmodel = LLM(model=\"Meta-Llama-3-8B-Instruct-FP8/\")\n# INFO 06-10 21:15:41 model_runner.py:159] Loading model weights took 8.4596 GB\nresult = model.generate(\"Hello, my name is\")\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Documentation\nDESCRIPTION: Sets vllm.engine as the current module for documentation purposes. This affects the scope of all subsequent documentation directives, ensuring they reference the correct module.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/engine/index.md#2025-04-05_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: vllm.engine\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating UV Environment for vLLM\nDESCRIPTION: Commands to create a new uv environment named 'vllm' with Python 3.12, including pip and setuptools, and activate it. UV is recommended as a fast alternative to conda for Python environment management.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/python_env_setup.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n# (Recommended) Create a new uv environment. Use `--seed` to install `pip` and `setuptools` in the environment.\nuv venv vllm --python 3.12 --seed\nsource vllm/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Example Usage Statistics Data Format in JSON\nDESCRIPTION: Example of the anonymous usage data collected by vLLM v0.4.0, showing system specifications, model configuration, and runtime parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/usage_stats.md#2025-04-05_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"uuid\": \"fbe880e9-084d-4cab-a395-8984c50f1109\",\n  \"provider\": \"GCP\",\n  \"num_cpu\": 24,\n  \"cpu_type\": \"Intel(R) Xeon(R) CPU @ 2.20GHz\",\n  \"cpu_family_model_stepping\": \"6,85,7\",\n  \"total_memory\": 101261135872,\n  \"architecture\": \"x86_64\",\n  \"platform\": \"Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.31\",\n  \"gpu_count\": 2,\n  \"gpu_type\": \"NVIDIA L4\",\n  \"gpu_memory_per_device\": 23580639232,\n  \"model_architecture\": \"OPTForCausalLM\",\n  \"vllm_version\": \"0.3.2+cu123\",\n  \"context\": \"LLM_CLASS\",\n  \"log_time\": 1711663373492490000,\n  \"source\": \"production\",\n  \"dtype\": \"torch.float16\",\n  \"tensor_parallel_size\": 1,\n  \"block_size\": 16,\n  \"gpu_memory_utilization\": 0.9,\n  \"quantization\": null,\n  \"kv_cache_dtype\": \"auto\",\n  \"enable_lora\": false,\n  \"enable_prefix_caching\": false,\n  \"enforce_eager\": false,\n  \"disable_custom_all_reduce\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Engine Documentation\nDESCRIPTION: Creates a table of contents (toctree) for the engine documentation with a caption and maximum depth. It includes links to documentation for both the synchronous LLM engine and the asynchronous LLM engine.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/engine/index.md#2025-04-05_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:caption: Engines\n:maxdepth: 2\n\nllm_engine\nasync_llm_engine\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM Model Without Seed Specification in Python\nDESCRIPTION: This snippet demonstrates how to initialize a vLLM model without specifying a seed, resulting in different random number outputs across runs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/seed_parameter_behavior.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom vllm import LLM\n\n# Initialize a vLLM model without specifying a seed\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\")\n\n# Try generating random numbers\nprint(random.randint(0, 100))  # Outputs different numbers across runs\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Pooling Parameters in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines pooling parameters used in the Embeddings API. These parameters control how the model pools token embeddings to create a single embedding vector for a text input.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# begin-embedding-pooling-params\n# end-embedding-pooling-params\n```\n\n----------------------------------------\n\nTITLE: Python Requirements Dependencies List\nDESCRIPTION: A requirements.txt style file listing package dependencies, their versions, and the packages that depend on them. Each entry shows a package name, version number, and optionally lists which other packages require it as a dependency.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/test.txt#2025-04-05_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nsacrebleu==2.4.3\n    # via lm-eval\nsafetensors==0.4.5\n    # via\n    #   accelerate\n    #   peft\n    #   timm\n    #   transformers\nscikit-learn==1.5.2\n    # via\n    #   librosa\n    #   lm-eval\n    #   sentence-transformers\nscipy==1.13.1\n    # via\n    #   librosa\n    #   scikit-learn\n    #   sentence-transformers\n    #   statsmodels\n    #   vocos\n```\n\n----------------------------------------\n\nTITLE: KV Block Hash Mapping Structure\nDESCRIPTION: Shows the conceptual mapping between token hashes and KV blocks used for cache management. Demonstrates the one-to-one relationship between prefix+block token combinations and their corresponding KV blocks.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/automatic_prefix_caching.md#2025-04-05_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhash(prefix tokens + block tokens) <--> KV Block\n```\n\n----------------------------------------\n\nTITLE: Python Rerank Pooling Parameters\nDESCRIPTION: Python code snippet from vLLM's OpenAI protocol file defining pooling parameters for the rerank API. These parameters control how the model processes and handles the text representations during reranking.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Content referenced from vllm/entrypoints/openai/protocol.py\n```\n\n----------------------------------------\n\nTITLE: Defining Common Build and Runtime Dependencies for vLLM Project\nDESCRIPTION: This snippet lists various build and runtime dependencies required for the vLLM project, including CMake, packaging tools, and the AMD SMI library.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/rocm-build.txt#2025-04-05_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ncmake>=3.26,<4\npackaging\nsetuptools>=61\nsetuptools-scm>=8\nwheel\njinja2>=3.1.6\namdsmi==6.2.4\n```\n\n----------------------------------------\n\nTITLE: Demonstrating KV Block Structure and Prefix Relationships\nDESCRIPTION: Illustrates how KV blocks are structured and how prefixes are related across blocks in the attention mechanism. Shows the relationship between three blocks of text and their corresponding prefix dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/automatic_prefix_caching.md#2025-04-05_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n                    Block 1                  Block 2                  Block 3\n         [A gentle breeze stirred] [the leaves as children] [laughed in the distance]\nBlock 1: |<--- block tokens ---->|\nBlock 2: |<------- prefix ------>| |<--- block tokens --->|\nBlock 3: |<------------------ prefix -------------------->| |<--- block tokens ---->|\n```\n\n----------------------------------------\n\nTITLE: Expected Pod Status Output\nDESCRIPTION: Example output of the kubectl get pods command showing the expected running status of vLLM leader and worker pods in a successful deployment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/frameworks/lws.md#2025-04-05_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nNAME       READY   STATUS    RESTARTS   AGE\nvllm-0     1/1     Running   0          2s\nvllm-0-1   1/1     Running   0          2s\nvllm-1     1/1     Running   0          2s\nvllm-1-1   1/1     Running   0          2s\n```\n\n----------------------------------------\n\nTITLE: Requirements List for vLLM Dependencies\nDESCRIPTION: Specifies required Python packages and their versions for building and running vLLM. Notable requirements include CMake 3.26+, PyTorch 2.6.0, and various build tools.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/build.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Should be mirrored in pyproject.toml\ncmake>=3.26\nninja\npackaging\nsetuptools>=61\nsetuptools-scm>=8\ntorch==2.6.0\nwheel\njinja2>=3.1.6\n```\n\n----------------------------------------\n\nTITLE: KVCacheBlock Class Definition for Prefix Caching\nDESCRIPTION: Python class definition for KVCacheBlock that serves as the basic building block for prefix caching in vLLM v1. The class tracks block IDs, hash values, reference counts, and manages free queue pointers.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/prefix_caching.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass KVCacheBlock:\n    # The block ID (immutable)\n    block_id: int\n    # The block hash (will be assigned when the block is full,\n    # and will be reset when the block is evicted).\n    block_hash: BlockHashType\n    # The number of requests using this block now.\n    ref_cnt: int\n\n    # The pointers to form a doubly linked list for the free queue.\n    prev_free_block: Optional[\"KVCacheBlock\"] = None\n    next_free_block: Optional[\"KVCacheBlock\"] = None\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Results with Console Commands\nDESCRIPTION: Shows how to view the output of a batch processing job by using the cat command to display the JSON Lines format results file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ cat results.jsonl\n{\"id\":\"vllm-db0f71f7dec244e6bce530e0b4ef908b\",\"custom_id\":\"request-1\",\"response\":{\"status_code\":200,\"request_id\":\"vllm-batch-3580bf4d4ae54d52b67eee266a6eab20\",\"body\":{\"id\":\"embd-33ac2efa7996430184461f2e38529746\",\"object\":\"list\",\"created\":444647,\"model\":\"intfloat/e5-mistral-7b-instruct\",\"data\":[{\"index\":0,\"object\":\"embedding\",\"embedding\":[0.016204833984375,0.0092010498046875,0.0018358230590820312,-0.0028228759765625,0.001422882080078125,-0.0031147003173828125,...]}],\"usage\":{\"prompt_tokens\":8,\"total_tokens\":8,\"completion_tokens\":0}}},\"error\":null}\n...\n```\n\n----------------------------------------\n\nTITLE: Starting Jaeger Container\nDESCRIPTION: Docker command to start Jaeger all-in-one container with necessary port mappings for trace collection.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ndocker run --rm --name jaeger \\\n    -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n    -p 6831:6831/udp \\\n    -p 6832:6832/udp \\\n    -p 5778:5778 \\\n    -p 16686:16686 \\\n    -p 4317:4317 \\\n    -p 4318:4318 \\\n    -p 14250:14250 \\\n    -p 14268:14268 \\\n    -p 14269:14269 \\\n    -p 9411:9411 \\\n    jaegertracing/all-in-one:1.57\n```\n\n----------------------------------------\n\nTITLE: Calculating Request Queue Time in vLLM\nDESCRIPTION: Shows how queue time is calculated in vLLM, highlighting duplicated metrics implementation. One metric calculates time in queue when a request is first scheduled, while another calculates it when a sequence group is finished.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/metrics.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    self.metrics.first_scheduled_time = now\n    self.metrics.time_in_queue = now - self.metrics.arrival_time\n```\n\n----------------------------------------\n\nTITLE: Viewing Score Endpoint Results with Console Commands\nDESCRIPTION: Demonstrates how to check the results of a batch score request using the cat command, showing the JSON Lines output with score values for text comparisons.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_12\n\nLANGUAGE: console\nCODE:\n```\n$ cat results.jsonl\n{\"id\":\"vllm-f87c5c4539184f618e555744a2965987\",\"custom_id\":\"request-1\",\"response\":{\"status_code\":200,\"request_id\":\"vllm-batch-806ab64512e44071b37d3f7ccd291413\",\"body\":{\"id\":\"score-4ee45236897b4d29907d49b01298cdb1\",\"object\":\"list\",\"created\":1737847944,\"model\":\"BAAI/bge-reranker-v2-m3\",\"data\":[{\"index\":0,\"object\":\"score\",\"score\":0.0010900497436523438},{\"index\":1,\"object\":\"score\",\"score\":1.0}],\"usage\":{\"prompt_tokens\":37,\"total_tokens\":37,\"completion_tokens\":0,\"prompt_tokens_details\":null}}},\"error\":null}\n{\"id\":\"vllm-41990c51a26d4fac8419077f12871099\",\"custom_id\":\"request-2\",\"response\":{\"status_code\":200,\"request_id\":\"vllm-batch-73ce66379026482699f81974e14e1e99\",\"body\":{\"id\":\"score-13f2ffe6ba40460fbf9f7f00ad667d75\",\"object\":\"list\",\"created\":1737847944,\"model\":\"BAAI/bge-reranker-v2-m3\",\"data\":[{\"index\":0,\"object\":\"score\",\"score\":0.001094818115234375},{\"index\":1,\"object\":\"score\",\"score\":1.0}],\"usage\":{\"prompt_tokens\":37,\"total_tokens\":37,\"completion_tokens\":0,\"prompt_tokens_details\":null}}},\"error\":null}\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Environment\nDESCRIPTION: Setting up environment variables for client-side tracing configuration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nexport JAEGER_IP=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' jaeger)\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=grpc://$JAEGER_IP:4317\nexport OTEL_EXPORTER_OTLP_TRACES_INSECURE=true\nexport OTEL_SERVICE_NAME=\"client-service\"\npython dummy_client.py\n```\n\n----------------------------------------\n\nTITLE: Calculating Intervals Using Monotonic Time in Python\nDESCRIPTION: Example of using time.monotonic() to calculate intervals between events in the same process. This is crucial for accurate timing measurements in vLLM's metric collection.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/metrics.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nstart_time = time.monotonic()\n# ... some operation ...\nend_time = time.monotonic()\n\ninterval = end_time - start_time\n```\n\n----------------------------------------\n\nTITLE: Basic vLLM Inference Example\nDESCRIPTION: Demonstrates the most basic usage of vLLM for offline inference.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/offline_inference/basic/basic.py\n```\n\n----------------------------------------\n\nTITLE: Installing FastAPI Instrumentation\nDESCRIPTION: Installation and configuration of OpenTelemetry instrumentation for FastAPI integration.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/opentelemetry/Otel.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npip install opentelemetry-instrumentation-fastapi\n```\n\nLANGUAGE: console\nCODE:\n```\nopentelemetry-instrument vllm serve facebook/opt-125m\n```\n\n----------------------------------------\n\nTITLE: Logging Python Code Compilation Process\nDESCRIPTION: Debug logs showing the start of compilation for a function and the traced files considered for compilation cache.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nDEBUG 03-07 03:06:52 [decorators.py:203] Start compiling function <code object forward at 0x7f08acf40c90, file \"xxx/vllm/model_executor/models/llama.py\", line 339>\n\nDEBUG 03-07 03:06:54 [backends.py:370] Traced files (to be considered for compilation cache):\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/_dynamo/polyfills/builtins.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/nn/modules/container.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/nn/modules/module.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/attention/layer.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/distributed/communication_op.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/distributed/parallel_state.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/custom_op.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/activation.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/layernorm.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/linear.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/rotary_embedding.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/vocab_parallel_embedding.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/models/llama.py\n```\n\n----------------------------------------\n\nTITLE: Offline Throughput Benchmark\nDESCRIPTION: Command to run offline throughput benchmarking using the sonnet dataset.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 vllm/benchmarks/benchmark_throughput.py \\\n  --model NousResearch/Hermes-3-Llama-3.1-8B \\\n  --dataset-name sonnet \\\n  --dataset-path vllm/benchmarks/sonnet.txt \\\n  --num-prompts 10\n```\n\n----------------------------------------\n\nTITLE: Generating Decode Trace for Llama-3.1-70B-Instruct Model in vLLM on TPU\nDESCRIPTION: This snippet shows how to profile 32 parallel decodes for the meta-llama/Llama-3.1-70B-Instruct model. It uses a batch of 32 requests, each with 1 input token and 128 output tokens, and sets a delay to skip the initial prefill phase.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/profiling_tpu/README.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport XLA_HLO_DEBUG=1\nexport MODEL=meta-llama/Llama-3.1-70B-Instruct\nexport VLLM_TPU_PROFILE_DURATION_MS=2000\nexport VLLM_TPU_PROFILE_DELAY_MS=1000\n\nrm -rf ~/.cache/vllm/xla_cache\npython3 profiling.py \\\n    --model $MODEL \\\n    --input-len 1 \\\n    --output-len 128 \\\n    --batch-size 32 \\\n    --enforce-eager \\\n    --profile-result-dir profiles \\\n    --max-model-len 2048 --tensor-parallel-size 8\n```\n\n----------------------------------------\n\nTITLE: Logging Cache Directory for torch.compile\nDESCRIPTION: Log output showing the cache directory used by vLLM for torch.compile artifacts.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nINFO 03-07 03:06:55 [backends.py:409] Using cache directory: ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0 for vLLM's torch.compile\n```\n\n----------------------------------------\n\nTITLE: Referencing Linting and Testing Requirements in Python\nDESCRIPTION: This snippet specifies the project's dependencies by referencing separate files for linting and testing requirements. It uses the '-r' flag to include the contents of 'lint.txt' and 'test.txt'.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/dev.txt#2025-04-05_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n-r lint.txt\n-r test.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Config Object in vLLM\nDESCRIPTION: This code shows how vLLM inspects the 'model_type' field in the config dictionary to generate the appropriate config object. It handles directly supported model types and falls back to HuggingFace's AutoConfig for unsupported types.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/huggingface_integration.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_type = config_json[\"model_type\"]\nif model_type in _CONFIG_REGISTRY:\n    config_class = _CONFIG_REGISTRY[model_type]\n    config = config_class.from_dict(config_json)\nelse:\n    try:\n        config = AutoConfig.from_pretrained(\n            model,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n        )\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to load config for model {model}: {e}\")\n\n# Apply historical patches\napply_rope_patches(config)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorBoard Dependencies for vLLM TPU Profile Visualization\nDESCRIPTION: This snippet lists the pip commands to install the necessary dependencies for visualizing vLLM TPU profiles using TensorBoard. It includes tensorflow-cpu, tensorboard-plugin-profile, etils, and importlib_resources.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/profiling_tpu/README.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorflow-cpu tensorboard-plugin-profile etils importlib_resources\n```\n\n----------------------------------------\n\nTITLE: Reducing QK Max Across Thread Block in vLLM Attention CUDA Kernel\nDESCRIPTION: Finalizes the reduction of QK max across the entire thread block. This step ensures that all threads have the global maximum QK value for accurate softmax calculation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\n    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n}\nqk_max = VLLM_SHFL_SYNC(qk_max, 0);\n```\n\n----------------------------------------\n\nTITLE: Accessing Query Data Pointer in C++\nDESCRIPTION: This code snippet shows how to calculate the pointer to the assigned query token data in global memory for each thread.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nconst scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;\n```\n\n----------------------------------------\n\nTITLE: Downloading OpenAI Batch Example File in Python\nDESCRIPTION: Command to download an example OpenAI batch file from the vLLM GitHub repository using wget.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/openai/openai_batch.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nwget https://raw.githubusercontent.com/vllm-project/vllm/main/examples/offline_inference/openai/openai_example_batch.jsonl\n```\n\n----------------------------------------\n\nTITLE: Reducing QK Max Across Warp in vLLM Attention CUDA Kernel\nDESCRIPTION: Performs a reduction operation to find the maximum QK value across a warp. This uses CUDA's shuffle operations for efficient inter-thread communication within a warp.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {\n    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n}\n\nif (lane == 0) {\n   red_smem[warp_idx] = qk_max;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for vLLM\nDESCRIPTION: Commands to create and activate a dedicated Conda environment with Python 3.10 for vLLM installation, ensuring a clean and isolated environment for dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n vllm python=3.10 -y\nconda activate vllm\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation with vLLM\nDESCRIPTION: Generates embeddings using vLLM with customizable parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/README.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython examples/offline_inference/basic/embed.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Logits and Updating QK Max in vLLM Attention CUDA Kernel\nDESCRIPTION: Sets initial logits values and updates the maximum QK value. This step is crucial for the subsequent softmax calculation, handling masking for tokens beyond the context length.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nif (thread_group_offset == 0) {\n   const bool mask = token_idx >= context_len;\n   logits[token_idx - start_token_idx] = mask ? 0.f : qk;\n   qk_max = mask ? qk_max : fmaxf(qk_max, qk);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Rust for vLLM Dependencies\nDESCRIPTION: This command installs Rust, which is required for building certain Python packages like 'outlines-core' and 'uvloop'. It uses the official Rust installation script and sets up the environment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/s390x.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ncurl https://sh.rustup.rs -sSf | sh -s -- -y && \\\n    . \"$HOME/.cargo/env\"\n```\n\n----------------------------------------\n\nTITLE: Logging Compilation Artifact Locations\nDESCRIPTION: Debug logs showing the locations where the computation graph and transformed code are saved after compilation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nDEBUG 03-07 03:07:07 [backends.py:462] Computation graph saved to ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py\nDEBUG 03-07 03:07:07 [wrapper.py:105] Dynamo transformed code saved to ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/transformed_code.py\n```\n\n----------------------------------------\n\nTITLE: Cross-Warp Reduction in Multi-Head Attention (C++)\nDESCRIPTION: Performs reduction for accs across all warps, allowing each thread to have the accumulation of accs for the assigned head positions of all context tokens. Uses shared memory for inter-warp communication.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nfloat* out_smem = reinterpret_cast<float*>(shared_mem);\nfor (int i = NUM_WARPS; i > 1; i /= 2) {\n    // Upper warps write to shared memory.\n    ...\n        float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];\n        for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n                ...\n        dst[row_idx] = accs[i];\n    }\n\n    // Lower warps update the output.\n        const float* src = &out_smem[warp_idx * HEAD_SIZE];\n    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n                ...\n        accs[i] += src[row_idx];\n    }\n\n        // Write out the accs.\n}\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Docker Container with NVIDIA GPU Support\nDESCRIPTION: Docker command to run a pre-built vLLM image with NVIDIA GPU support, mounting the current directory and using all available GPUs.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all -it -v $(pwd):/workspace vllm/vllm-openai\n```\n\n----------------------------------------\n\nTITLE: Inductor Graph Compilation Debug Logs\nDESCRIPTION: Debug logs showing how Inductor compiles and caches computation graphs. The logs demonstrate compilation of different graph pieces and their storage locations.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/v1/torch_compile.md#2025-04-05_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nDEBUG 03-07 03:52:37 [backends.py:134] store the 0-th graph for shape None from inductor via handle ('fpegyiq3v3wzjzphd45wkflpabggdbjpylgr7tta4hj6uplstsiw', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/iw/ciwzrk3ittdqatuzwonnajywvno3llvjcs2vfdldzwzozn3zi3iy.py')\nDEBUG 03-07 03:52:39 [backends.py:134] store the 1-th graph for shape None from inductor via handle ('f7fmlodmf3h3by5iiu2c4zarwoxbg4eytwr3ujdd2jphl4pospfd', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/ly/clyfzxldfsj7ehaluis2mca2omqka4r7mgcedlf6xfjh645nw6k2.py')\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM's OpenAI-Compatible API Server Directly\nDESCRIPTION: This bash command shows how to start the vLLM OpenAI-compatible API server directly using Python, bypassing the vllm CLI. It requires specifying the model to be served as a command-line argument.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/arch_overview.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m vllm.entrypoints.openai.api_server --model <model>\n```\n\n----------------------------------------\n\nTITLE: Building and Installing vLLM XPU Backend\nDESCRIPTION: Builds the vLLM XPU backend from source and installs it. This step should be performed after installing the required dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/gpu/xpu.inc.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nVLLM_TARGET_DEVICE=xpu python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Forcing Spawn Method in MultiProcXPUExecutor\nDESCRIPTION: Code reference showing that the multiproc_xpu_executor forces the use of the 'spawn' method.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/executor/multiproc_xpu_executor.py#L14-L18\n```\n\n----------------------------------------\n\nTITLE: Python RuntimeError for Improper Multiprocessing Usage\nDESCRIPTION: Console output showing the Python exception raised when attempting to start a new process without using a __main__ guard in the code.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/multiprocessing.md#2025-04-05_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nRuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n\n        To fix this issue, refer to the \"Safe importing of main module\"\n        section in https://docs.python.org/3/library/multiprocessing.html\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Commands with uv without Environment\nDESCRIPTION: Alternative command to run vLLM utilities directly with uv without creating a dedicated environment, useful for quick testing.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nuv run --with vllm vllm --help\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Memory for Query Vectors in C++\nDESCRIPTION: This code snippet defines a shared memory array to store query vectors for each thread group, facilitating efficient access during computation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/kernel/paged_attention.md#2025-04-05_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];\n```\n\n----------------------------------------\n\nTITLE: Defining vLLM Environment Variables in Python\nDESCRIPTION: Defines environment variables used to configure various aspects of the vLLM system, including distributed settings, ray usage, tensor parallelism, and other system parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/env_vars.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# begin-env-vars-definition\nENV_VARS = {\n    # Use environment variable to configure the logger\n    \"VLLM_LOG_LEVEL\": {\n        \"default\": \"INFO\",\n        \"type\": str,\n        \"description\": \"Log level for the vLLM logger.\",\n    },\n    \"VLLM_LOG_FORMAT\": {\n        \"default\": \"%(levelname)s:%(name)s:%(asctime)s:%(message)s\",\n        \"type\": str,\n        \"description\": \"Format for the vLLM logger.\",\n    },\n    \"VLLM_DISTRIBUTED_INIT_TIMEOUT\": {\n        \"default\": 300,\n        \"type\": int,\n        \"description\": \"Timeout in seconds for distributed initialization.\",\n    },\n    \"VLLM_PORT\": {\n        \"default\": None,\n        \"type\": int,\n        \"description\": \"Port for communication, vLLM internal usage only.\",\n    },\n    \"VLLM_HOST_IP\": {\n        \"default\": None,\n        \"type\": str,\n        \"description\": \"IP address for communication, vLLM internal usage only.\",\n    },\n    \"VLLM_USE_RAY\": {\n        \"default\": True,\n        \"type\": bool,\n        \"description\": \"Whether to use Ray for distributed inference.\",\n    },\n    \"VLLM_WORKER_USE_RAY\": {\n        \"default\": None,\n        \"type\": bool,\n        \"description\": \"Whether to use Ray for worker, this is determined automatically.\",\n    },\n    \"VLLM_DISTRIBUTED_WORKER_PORT\": {\n        \"default\": 2222,\n        \"type\": int,\n        \"description\": \"Port for distributed worker communication.\",\n    },\n    \"VLLM_DISTRIBUTED_TENSOR_PARALLEL_NCCL_PORT\": {\n        \"default\": 9099,\n        \"type\": int,\n        \"description\": \"Port for tensor parallel communication.\",\n    },\n    \"VLLM_DEEPSPEED_OPTIMIZERS_ENABLE\": {\n        \"default\": False,\n        \"type\": bool,\n        \"description\": \"Whether to enable DeepSpeed optimizers.\",\n    },\n    \"VLLM_ENABLE_DISK_CACHE\": {\n        \"default\": False,\n        \"type\": bool,\n        \"description\": \"Whether to enable disk cache for KV in TokenAttention.\",\n    },\n    \"VLLM_UNSHARD_BEST_EFFORT\": {\n        \"default\": False,\n        \"type\": bool,\n        \"description\": \"Whether to unshard model in best effort mode, which uses less GPU device memory.\",\n    },\n    \"VLLM_DISABLE_DISTRIBUTED_WORKER\": {\n        \"default\": False,\n        \"type\": bool,\n        \"description\": \"Whether to disable distributed workers. This should be used when deploying behind a proxy.\",\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Serving documentation on custom port with Python HTTP server\nDESCRIPTION: Starts a local HTTP server using Python's built-in module to serve the generated HTML documentation on a specified port (3000 in this example).\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m http.server 3000 -d build/html/\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM's OpenAI-Compatible API Server via CLI\nDESCRIPTION: This bash command demonstrates how to start the vLLM OpenAI-compatible API server using the vllm CLI command. It requires specifying the model to be served.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/design/arch_overview.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve <model>\n```\n\n----------------------------------------\n\nTITLE: Disabling Usage Statistics Collection in Bash\nDESCRIPTION: Multiple methods to opt-out of vLLM usage statistics collection using environment variables or file creation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/usage_stats.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Any of the following methods can disable usage stats collection\nexport VLLM_NO_USAGE_STATS=1\nexport DO_NOT_TRACK=1\nmkdir -p ~/.config/vllm && touch ~/.config/vllm/do_not_track\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for AsyncLLMEngine with reStructuredText\nDESCRIPTION: This snippet uses reStructuredText directives to automatically generate documentation for the AsyncLLMEngine class in vLLM. The directive includes all members of the class and shows inheritance relationships.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/engine/async_llm_engine.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: vllm.AsyncLLMEngine\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Embedding Extra Parameters in vLLM OpenAI Protocol\nDESCRIPTION: This code snippet defines additional parameters supported by the Embeddings API when using chat-like input (when 'messages' is passed). These parameters provide extra control specific to chat-based embedding generation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/serving/openai_compatible_server.md#2025-04-05_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# begin-chat-embedding-extra-params\n# end-chat-embedding-extra-params\n```\n\n----------------------------------------\n\nTITLE: Vision Model Benchmark Setup\nDESCRIPTION: Commands to benchmark vision-language models using the VisionArena dataset.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve Qwen/Qwen2-VL-7B-Instruct --disable-log-requests\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend openai-chat \\\n  --model Qwen/Qwen2-VL-7B-Instruct \\\n  --endpoint /v1/chat/completions \\\n  --dataset-name hf \\\n  --dataset-path lmarena-ai/VisionArena-Chat \\\n  --hf-split train \\\n  --num-prompts 1000\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client for vLLM Speculative Decoding\nDESCRIPTION: Python client code using the OpenAI SDK to interact with a vLLM server that has speculative decoding enabled, showing how to make completion requests to the server.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n# Completion API\nstream = False\ncompletion = client.completions.create(\n    model=model,\n    prompt=\"The future of AI is\",\n    echo=False,\n    n=1,\n    stream=stream,\n)\n\nprint(\"Completion results:\")\nif stream:\n    for c in completion:\n        print(c)\nelse:\n    print(completion)\n\n```\n\n----------------------------------------\n\nTITLE: LoRA Adapter Benchmark\nDESCRIPTION: Command to benchmark with LoRA adapters using the ShareGPT dataset.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 vllm/benchmarks/benchmark_throughput.py \\\n  --model meta-llama/Llama-2-7b-hf \\\n  --backend vllm \\\n  --dataset_path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --dataset_name sharegpt \\\n  --num-prompts 10 \\\n  --max-loras 2 \\\n  --max-lora-rank 8 \\\n  --enable-lora \\\n  --lora-path yard1/llama-2-7b-sql-lora-test\n```\n\n----------------------------------------\n\nTITLE: Validating Rejection Sampler Convergence in Python\nDESCRIPTION: This code snippet is part of a test suite that ensures samples from vLLM's rejection sampler align with the target distribution, validating the lossless nature of the speculative decoding implementation.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/spec_decode.md#2025-04-05_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# View Test Code\n# https://github.com/vllm-project/vllm/blob/47b65a550866c7ffbd076ecb74106714838ce7da/tests/samplers/test_rejection_sampler.py#L252\n```\n\n----------------------------------------\n\nTITLE: Installing DeepGEMM from Source for Benchmarking\nDESCRIPTION: Commands to clone the DeepGEMM repository and install it for use with vLLM benchmarking. This requires having vLLM already installed through your normal method and then adding DeepGEMM in its own directory.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/kernels/deepgemm/README.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive https://github.com/deepseek-ai/DeepGEMM\ncd DeepGEMM\npython setup.py install\nuv pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing documentation dependencies with pip\nDESCRIPTION: Installs the required packages for building the documentation using pip and a requirements file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r ../requirements/docs.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Limit for Run:ai Streamer\nDESCRIPTION: Command to start vLLM server with a custom CPU memory buffer limit for tensor loading.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/runai_model_streamer.md#2025-04-05_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nvllm serve /home/meta-llama/Llama-3.2-3B-Instruct --load-format runai_streamer --model-loader-extra-config '{\"memory_limit\":5368709120}'\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM via pip\nDESCRIPTION: Command to install vLLM using pip package manager. This is the simplest way to install vLLM for most users.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/README.md#2025-04-05_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install vllm\n```\n\n----------------------------------------\n\nTITLE: Generating HTML documentation with Sphinx\nDESCRIPTION: Builds the HTML documentation using the 'make html' command, which typically invokes Sphinx.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/README.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM with Run:ai Support\nDESCRIPTION: Command to install vLLM with Run:ai Model Streamer support using pip.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/runai_model_streamer.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip3 install vllm[runai]\n```\n\n----------------------------------------\n\nTITLE: ScaledEpilogue Calculation\nDESCRIPTION: Formulas for the ScaledEpilogue, which computes symmetric quantization for activations without bias.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\widehat D = \\widehat A \\widehat B\n```\n\nLANGUAGE: math\nCODE:\n```\nD = s_a s_b \\widehat D\n```\n\nLANGUAGE: math\nCODE:\n```\nD = s_a s_b \\widehat A \\widehat B\n```\n\n----------------------------------------\n\nTITLE: Checking Transformers Backend Usage in vLLM\nDESCRIPTION: This snippet demonstrates how to check if a model is using the Transformers backend in vLLM. It initializes an LLM object and applies a lambda function to print the model type.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/supported_models.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\nllm = LLM(model=..., task=\"generate\")  # Name or path of your model\nllm.apply_model(lambda model: print(type(model)))\n```\n\n----------------------------------------\n\nTITLE: Documenting vLLM Model Interfaces with Sphinx RST\nDESCRIPTION: Sphinx RST directive to automatically generate documentation for the vllm.model_executor.models.interfaces module. The directive includes all members and orders them by source code order.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/model/interfaces.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.model_executor.models.interfaces\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Project Configuration\nDESCRIPTION: Initial CMake setup including project name, minimum version requirement, and build instructions\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.26)\n\nproject(vllm_extensions LANGUAGES CXX)\n\nset(VLLM_TARGET_DEVICE \"cuda\" CACHE STRING \"Target device backend for vLLM\")\n```\n\n----------------------------------------\n\nTITLE: Using LLM.encode for Reward Models in vLLM\nDESCRIPTION: Example of using the encode method with a reward model to extract hidden states directly, which is useful for reward modeling tasks. The code initializes a reward model and processes a sample text input.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#2025-04-05_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom vllm import LLM\n\nllm = LLM(model=\"Qwen/Qwen2.5-Math-RM-72B\", task=\"reward\")\n(output,) = llm.encode(\"Hello, my name is\")\n\ndata = output.outputs.data\nprint(f\"Data: {data!r}\")\n```\n\n----------------------------------------\n\nTITLE: Generating vLLM Multimodal Registry Documentation with RST\nDESCRIPTION: RST directive to automatically generate API documentation for the vllm.multimodal.registry module, including all members ordered by source file appearance.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/api/multimodal/registry.md#2025-04-05_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: vllm.multimodal.registry\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Version and Architecture Configuration\nDESCRIPTION: Defines supported Python versions, CUDA architectures, and AMD GPU architectures along with torch version requirements\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYTHON_SUPPORTED_VERSIONS \"3.9\" \"3.10\" \"3.11\" \"3.12\")\n\nset(CUDA_SUPPORTED_ARCHS \"7.0;7.2;7.5;8.0;8.6;8.7;8.9;9.0;10.0;10.1;12.0\")\n\nset(HIP_SUPPORTED_ARCHS \"gfx906;gfx908;gfx90a;gfx942;gfx950;gfx1030;gfx1100;gfx1101;gfx1200;gfx1201\")\n\nset(TORCH_SUPPORTED_VERSION_CUDA \"2.6.0\")\nset(TORCH_SUPPORTED_VERSION_ROCM \"2.6.0\")\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Built-in Extensions in Markdown\nDESCRIPTION: This code snippet creates a table of contents using the toctree directive in Markdown. It lists three built-in extensions: runai_model_streamer, tensorizer, and fastsafetensor. The maxdepth parameter is set to 1, indicating a single level of depth for the table of contents.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/models/extensions/index.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::{toctree}\n:maxdepth: 1\n\nrunai_model_streamer\ntensorizer\nfastsafetensor\n:::\n```\n\n----------------------------------------\n\nTITLE: Profiling Offline Inference with Nsight Systems\nDESCRIPTION: Command to profile vLLM offline inference using Nsight Systems. This runs a latency benchmark with specified model parameters while capturing profiling data.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnsys profile -o report.nsys-rep --trace-fork-before-exec=true --cuda-graph-trace=node python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --num-iters-warmup 5 --num-iters 1 --batch-size 16 --input-len 512 --output-len 8\n```\n\n----------------------------------------\n\nTITLE: Adding CUDA-Specific Extension Sources\nDESCRIPTION: Adds CUDA-specific source files to the VLLM extensions, including Mamba SSM, causal convolution, and various quantization kernels. This section runs when the GPU language is set to CUDA.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n  list(APPEND VLLM_EXT_SRC\n    \"csrc/mamba/mamba_ssm/selective_scan_fwd.cu\"\n    \"csrc/mamba/causal_conv1d/causal_conv1d.cu\"\n    \"csrc/quantization/aqlm/gemm_kernels.cu\"\n    \"csrc/quantization/awq/gemm_kernels.cu\"\n    \"csrc/permute_cols.cu\"\n    \"csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu\"\n    \"csrc/quantization/fp4/nvfp4_quant_entry.cu\"\n    \"csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu\"\n    \"csrc/sparse/cutlass/sparse_scaled_mm_entry.cu\"\n    \"csrc/cutlass_extensions/common.cpp\")\n\n  set_gencode_flags_for_srcs(\n    SRCS \"${VLLM_EXT_SRC}\"\n    CUDA_ARCHS \"${CUDA_ARCHS}\")\n```\n\n----------------------------------------\n\nTITLE: Querying Reasoning Model with OpenAI Python Client\nDESCRIPTION: Python code snippet demonstrating how to make a request to a reasoning model using the OpenAI Python client, including setup and parsing of the response.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/features/reasoning_outputs.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n# Round 1\nmessages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n# For granite, add: `extra_body={\"chat_template_kwargs\": {\"thinking\": True}}`\nresponse = client.chat.completions.create(model=model, messages=messages)\n\nreasoning_content = response.choices[0].message.reasoning_content\ncontent = response.choices[0].message.content\n\nprint(\"reasoning_content:\", reasoning_content)\nprint(\"content:\", content)\n```\n\n----------------------------------------\n\nTITLE: Using Python cProfile Decorator for Function Profiling\nDESCRIPTION: Example of using vLLM's cProfile decorator utility to profile a Python function. The profile results are saved to a specified file.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/profiling/profiling_index.md#2025-04-05_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport vllm.utils\n\n@vllm.utils.cprofile(\"expensive_function.prof\")\ndef expensive_function():\n    # some expensive code\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring AllSpark Kernel Compilation\nDESCRIPTION: Conditionally adds AllSpark kernels based on compatible CUDA architectures. These kernels support specialized operations for quantization including GPTQ and int8 quantized matrix multiplication.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n  # Only build AllSpark kernels if we are building for at least some compatible archs.\n  cuda_archs_loose_intersection(ALLSPARK_ARCHS \"8.0;8.6;8.7;8.9\" \"${CUDA_ARCHS}\")\n  if (ALLSPARK_ARCHS)\n    set(ALLSPARK_SRCS\n       \"csrc/quantization/gptq_allspark/allspark_repack.cu\"\n       \"csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${ALLSPARK_SRCS}\"\n      CUDA_ARCHS \"${ALLSPARK_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${ALLSPARK_SRCS}\")\n    message(STATUS \"Building AllSpark kernels for archs: ${ALLSPARK_ARCHS}\")\n  else()\n    message(STATUS \"Not building AllSpark kernels as no compatible archs found\"\n                   \" in CUDA target architectures\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Creating PVC for Mistral-7B Model in Kubernetes\nDESCRIPTION: This YAML snippet creates a PersistentVolumeClaim for storing the Mistral-7B model cache in Kubernetes. It requests 50Gi of storage with ReadWriteOnce access mode.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/k8s.md#2025-04-05_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mistral-7b\n  namespace: default\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: default\n  volumeMode: Filesystem\n```\n\n----------------------------------------\n\nTITLE: Generating Dockerfile Build Graph with Docker Container\nDESCRIPTION: This command runs the dockerfilegraph tool inside a Docker container to generate a PNG visualization of the Dockerfile. It mounts the current directory, sets user permissions, and specifies various options for the output.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/contributing/dockerfile/dockerfile.md#2025-04-05_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n   --rm \\\n   --user \"$(id -u):$(id -g)\" \\\n   --workdir /workspace \\\n   --volume \"$(pwd)\":/workspace \\\n   ghcr.io/patrickhoefler/dockerfilegraph:alpine \\\n   --output png \\\n   --dpi 200 \\\n   --max-label-length 50 \\\n   --filename docker/Dockerfile \\\n   --legend\n```\n\n----------------------------------------\n\nTITLE: Defining GPU Extension Target for vllm\nDESCRIPTION: Creates the main GPU extension target for vllm, including all compiled sources and flags.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\ndefine_gpu_extension_target(\n  _C\n  DESTINATION vllm\n  LANGUAGE ${VLLM_GPU_LANG}\n  SOURCES ${VLLM_EXT_SRC}\n  COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n  ARCHITECTURES ${VLLM_GPU_ARCHES}\n  INCLUDE_DIRECTORIES ${CUTLASS_INCLUDE_DIR}\n  INCLUDE_DIRECTORIES ${CUTLASS_TOOLS_UTIL_INCLUDE_DIR}\n  USE_SABI 3\n  WITH_SOABI)\n\ntarget_compile_definitions(_C PRIVATE CUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1)\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for vLLM Router\nDESCRIPTION: Command to forward the vLLM router service port to the host machine.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/deployment/integrations/production-stack.md#2025-04-05_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo kubectl port-forward svc/vllm-router-service 30080:80\n```\n\n----------------------------------------\n\nTITLE: Curl Request to List vLLM Models\nDESCRIPTION: Curl command to query the vLLM server for the list of available models using the OpenAI-compatible API endpoint.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/quickstart.md#2025-04-05_snippet_8\n\nLANGUAGE: console\nCODE:\n```\ncurl http://localhost:8000/v1/models\n```\n\n----------------------------------------\n\nTITLE: Configuring ROCm Extension for vllm\nDESCRIPTION: Sets up the build configuration for the ROCm extension when using HIP as the GPU language.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif(VLLM_GPU_LANG STREQUAL \"HIP\")\n  set(VLLM_ROCM_EXT_SRC\n    \"csrc/rocm/torch_bindings.cpp\"\n    \"csrc/rocm/attention.cu\")\n\n  define_gpu_extension_target(\n    _rocm_C\n    DESTINATION vllm\n    LANGUAGE ${VLLM_GPU_LANG}\n    SOURCES ${VLLM_ROCM_EXT_SRC}\n    COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n    ARCHITECTURES ${VLLM_GPU_ARCHES}\n    USE_SABI 3\n    WITH_SOABI)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Model Server\nDESCRIPTION: Command to start serving a vLLM model for benchmarking with request logging disabled.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/benchmarks/README.md#2025-04-05_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvllm serve NousResearch/Hermes-3-Llama-3.1-8B --disable-log-requests\n```\n\n----------------------------------------\n\nTITLE: Pre-commit Version Specification\nDESCRIPTION: Defines the exact version of pre-commit tool (4.0.1) required for the project's pre-commit hooks.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/lint.txt#2025-04-05_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\npre-commit==4.0.1\n```\n\n----------------------------------------\n\nTITLE: Hardware Support Status Table in Markdown\nDESCRIPTION: Markdown table displaying the current support status for different hardware platforms in vLLM V1, including NVIDIA, AMD, and TPU.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/v1_user_guide.md#2025-04-05_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Hardware | Status                                   |\n|----------|------------------------------------------|\n| **NVIDIA** | <nobr> Natively Supported</nobr>         |\n| **AMD**    | <nobr> WIP</nobr>           |\n| **TPU**    | <nobr> WIP</nobr>           |\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation - vLLM Code of Conduct\nDESCRIPTION: Comprehensive markdown document detailing the code of conduct, including community standards, enforcement procedures, and impact guidelines for the vLLM project community\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md#2025-04-05_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# vLLM Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socioeconomic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n```\n\n----------------------------------------\n\nTITLE: Defining VLLM HPU Dependencies in pip Requirements Format\nDESCRIPTION: This snippet lists the Python packages required for running VLLM on HPU hardware. It includes common dependencies, HPU-specific libraries, and a custom extension from a GitHub repository. The file uses pip's requirements syntax to specify versions and installation sources.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/hpu.txt#2025-04-05_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# Common dependencies\n-r common.txt\n\n# Dependencies for HPU code\nray\ntriton==3.1.0\npandas\ntabulate\nsetuptools>=61\nsetuptools-scm>=8\nvllm-hpu-extension @ git+https://github.com/HabanaAI/vllm-hpu-extension.git@4312768\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM Build Dependencies\nDESCRIPTION: Commands to install required build dependencies for vLLM on TPU, including Python packages from the TPU-specific requirements file and system libraries needed for numerical operations and parallelism.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/tpu.inc.md#2025-04-05_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements/tpu.txt\nsudo apt-get install libopenblas-base libopenmpi-dev libomp-dev\n```\n\n----------------------------------------\n\nTITLE: Citing vLLM Research Paper in BibTeX Format\nDESCRIPTION: This code snippet provides the BibTeX citation for the research paper associated with the vLLM project. It includes details such as the title, authors, conference, and year of publication.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/README.md#2025-04-05_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for vLLM Project with AMD GPU Support\nDESCRIPTION: A requirements file that specifies dependencies for the vLLM project with particular focus on AMD GPU support. It includes common packages, Python version-specific dependencies, and libraries for cloud storage integration, dataset handling, and model deployment.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/rocm.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Common dependencies\n-r common.txt\n\nnumba == 0.60.0; python_version == '3.9' # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding\nnumba == 0.61; python_version > '3.9'\n\n# Dependencies for AMD GPUs\nawscli\nboto3\nbotocore\ndatasets\nray >= 2.10.0\npeft\npytest-asyncio\ntensorizer>=2.9.0\nrunai-model-streamer==0.11.0\nrunai-model-streamer-s3==0.11.0\n```\n\n----------------------------------------\n\nTITLE: Warmup Process Logs for Intel Gaudi in vLLM\nDESCRIPTION: Sample log output showing the warmup process that pre-compiles graph binaries for each bucket before server start. The logs display progress through all prompt and decode buckets, helping to avoid compilation overhead during runtime.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md#2025-04-05_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nINFO 08-01 22:26:47 hpu_model_runner.py:1066] [Warmup][Prompt][1/24] batch_size:4 seq_len:1024 free_mem:79.16 GiB\nINFO 08-01 22:26:47 hpu_model_runner.py:1066] [Warmup][Prompt][2/24] batch_size:4 seq_len:896 free_mem:55.43 GiB\nINFO 08-01 22:26:48 hpu_model_runner.py:1066] [Warmup][Prompt][3/24] batch_size:4 seq_len:768 free_mem:55.43 GiB\n...\nINFO 08-01 22:26:59 hpu_model_runner.py:1066] [Warmup][Prompt][24/24] batch_size:1 seq_len:128 free_mem:55.43 GiB\nINFO 08-01 22:27:00 hpu_model_runner.py:1066] [Warmup][Decode][1/48] batch_size:4 seq_len:2048 free_mem:55.43 GiB\nINFO 08-01 22:27:00 hpu_model_runner.py:1066] [Warmup][Decode][2/48] batch_size:4 seq_len:1920 free_mem:55.43 GiB\nINFO 08-01 22:27:01 hpu_model_runner.py:1066] [Warmup][Decode][3/48] batch_size:4 seq_len:1792 free_mem:55.43 GiB\n...\nINFO 08-01 22:27:16 hpu_model_runner.py:1066] [Warmup][Decode][47/48] batch_size:2 seq_len:128 free_mem:55.43 GiB\nINFO 08-01 22:27:16 hpu_model_runner.py:1066] [Warmup][Decode][48/48] batch_size:1 seq_len:128 free_mem:55.43 GiB\n```\n\n----------------------------------------\n\nTITLE: Listing Model Configurations for vllm Project\nDESCRIPTION: This snippet contains a list of model configurations for the vllm project. Each line represents a specific configuration, including the quantization or compression method, model repository, branch or version, and in some cases, additional parameters.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/tests/weight_loading/models.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngptq_marlin, robertgshaw2/zephyr-7b-beta-channelwise-gptq, main\ngptq_marlin, TheBloke/Llama-2-7B-GPTQ, main\ngptq_marlin, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, main\ngptq_marlin, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, gptq-8bit--1g-actorder_True\ngptq_marlin, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, gptq-8bit-32g-actorder_True\ngptq_marlin, TechxGenus/gemma-1.1-2b-it-GPTQ, main\ngptq, robertgshaw2/zephyr-7b-beta-channelwise-gptq, main\ngptq, TheBloke/Llama-2-7B-GPTQ, main\ngptq, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, main\ngptq, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, gptq-8bit--1g-actorder_True\ngptq, TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ, gptq-8bit-32g-actorder_True\ngptq, TechxGenus/gemma-1.1-2b-it-GPTQ, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w8w8-test-static-shape-change, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w8-channel-a8-tensor, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w8a8-dynamic-token-v2, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w8a8-channel-dynamic-token-v2, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w4a16-group128-v2, main\ncompressed-tensors, nm-testing/tinyllama-oneshot-w8a16-per-channel, main\ncompressed-tensors, nm-testing/Meta-Llama-3-8B-FP8-compressed-tensors-test, main\ncompressed-tensors, nm-testing/Phi-3-mini-128k-instruct-FP8, main\ncompressed-tensors, neuralmagic/Phi-3-medium-128k-instruct-quantized.w4a16, main\ncompressed-tensors, nm-testing/TinyLlama-1.1B-Chat-v1.0-actorder-group, main\n#compressed-tensors, mgoin/DeepSeek-Coder-V2-Lite-Instruct-FP8, main\ncompressed-tensors, nm-testing/SparseLlama-3.1-8B-gsm8k-pruned.2of4-FP8-Dynamic-testing, main, 90\ncompressed-tensors, nm-testing/SparseLlama-3.1-8B-gsm8k-pruned.2of4-W8A8-testing, main, 90\nawq, casperhansen/mixtral-instruct-awq, main\nawq_marlin, casperhansen/mixtral-instruct-awq, main\nfp8, neuralmagic/Meta-Llama-3-8B-Instruct-FP8-KV, main\nmarlin, nm-testing/zephyr-beta-7b-marlin-g128, main\nmarlin, robertgshaw2/zephyr-7b-beta-channelwise-marlin, main\nqqq, HandH1998/QQQ-Llama-3-8b-g128, main\nqqq, HandH1998/QQQ-Llama-3-8b, main\nhqq, nm-testing/Llama-3.2-1B-Instruct-HQQ, main\nNone, mgleize/fairseq2-dummy-Llama-3.2-1B, main\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch CPU Dependencies\nDESCRIPTION: Requirements file that specifies PyTorch and related package versions for different CPU architectures. Includes platform-specific conditions for torch, torchaudio, and torchvision packages along with common dependencies.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/cpu.txt#2025-04-05_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Common dependencies\n-r common.txt\n\n# Dependencies for CPUs\ntorch==2.6.0+cpu; platform_machine == \"x86_64\"\ntorch==2.6.0; platform_system == \"Darwin\"\ntorch==2.6.0; platform_machine == \"ppc64le\" or platform_machine == \"aarch64\"\ntorch==2.7.0.dev20250304; platform_machine == \"s390x\"\n\n# required for the image processor of minicpm-o-2_6, this must be updated alongside torch\ntorchaudio; platform_machine != \"ppc64le\" and platform_machine != \"s390x\"\ntorchaudio==2.6.0; platform_machine == \"ppc64le\"\n\n# required for the image processor of phi3v, this must be updated alongside torch\ntorchvision; platform_machine != \"ppc64le\"  and platform_machine != \"s390x\"\ntorchvision==0.21.0; platform_machine == \"ppc64le\"\ndatasets # for benchmark scripts\n```\n\n----------------------------------------\n\nTITLE: Installing GCC Compiler and Dependencies for vLLM (Console)\nDESCRIPTION: This snippet installs GCC 12, G++ 12, and libnuma-dev on Ubuntu 22.4. It then sets up GCC 12 as the default compiler using update-alternatives.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu/build.inc.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nsudo apt-get update  -y\nsudo apt-get install -y gcc-12 g++-12 libnuma-dev\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS Integration for VLLM\nDESCRIPTION: Sets up the CUTLASS library integration, enabling header-only mode and handling the source directory configuration. Supports both local directory and Git repository sources with version pinning.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt#2025-04-05_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  SET(CUTLASS_ENABLE_HEADERS_ONLY ON CACHE BOOL \"Enable only the header library\")\n\n  # Set CUTLASS_REVISION manually -- its revision detection doesn't work in this case.\n  # Please keep this in sync with FetchContent_Declare line below.\n  set(CUTLASS_REVISION \"v3.8.0\" CACHE STRING \"CUTLASS revision to use\")\n\n  # Use the specified CUTLASS source directory for compilation if VLLM_CUTLASS_SRC_DIR is provided\n  if (DEFINED ENV{VLLM_CUTLASS_SRC_DIR})\n    set(VLLM_CUTLASS_SRC_DIR $ENV{VLLM_CUTLASS_SRC_DIR})\n  endif()\n\n  if(VLLM_CUTLASS_SRC_DIR)\n    if(NOT IS_ABSOLUTE VLLM_CUTLASS_SRC_DIR)\n      get_filename_component(VLLM_CUTLASS_SRC_DIR \"${VLLM_CUTLASS_SRC_DIR}\" ABSOLUTE)\n    endif()\n    message(STATUS \"The VLLM_CUTLASS_SRC_DIR is set, using ${VLLM_CUTLASS_SRC_DIR} for compilation\")\n    FetchContent_Declare(cutlass SOURCE_DIR ${VLLM_CUTLASS_SRC_DIR})\n  else()\n    FetchContent_Declare(\n        cutlass\n        GIT_REPOSITORY https://github.com/nvidia/cutlass.git\n        # Please keep this in sync with CUTLASS_REVISION line above.\n        GIT_TAG v3.8.0\n        GIT_PROGRESS TRUE\n\n        # Speed up CUTLASS download by retrieving only the specified GIT_TAG instead of the history.\n        # Important: If GIT_SHALLOW is enabled then GIT_TAG works only with branch names and tags.\n        # So if the GIT_TAG above is updated to a commit hash, GIT_SHALLOW must be set to FALSE\n        GIT_SHALLOW TRUE\n    )\n  endif()\n  FetchContent_MakeAvailable(cutlass)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for vllm Project\nDESCRIPTION: This snippet enumerates the required Python packages and their version specifications for the vllm project. It includes dependencies for tokenization, API development, machine learning frameworks, and various utility libraries.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/requirements/common.txt#2025-04-05_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncachetools\npsutil\nsentencepiece  # Required for LLaMA tokenizer.\nnumpy\nrequests >= 2.26.0\ntqdm\nblake3\npy-cpuinfo\ntransformers >= 4.50.3\nhuggingface-hub[hf_xet] >= 0.30.0  # Required for Xet downloads.\ntokenizers >= 0.19.1  # Required for Llama 3.\nprotobuf # Required by LlamaTokenizer.\nfastapi[standard] >= 0.115.0 # Required by FastAPI's form models in the OpenAI API server's audio transcriptions endpoint.\naiohttp\nopenai >= 1.52.0 # Ensure modern openai package (ensure types module present and max_completion_tokens field support)\npydantic >= 2.9\nprometheus_client >= 0.18.0\npillow  # Required for image processing\nprometheus-fastapi-instrumentator >= 7.0.0\ntiktoken >= 0.6.0  # Required for DBRX tokenizer\nlm-format-enforcer >= 0.10.11, < 0.11\nllguidance >= 0.7.9, < 0.8.0; platform_machine == \"x86_64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\"\noutlines == 0.1.11\nlark == 1.2.2\nxgrammar == 0.1.17; platform_machine == \"x86_64\" or platform_machine == \"aarch64\"\ntyping_extensions >= 4.10\nfilelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\npartial-json-parser # used for parsing partial JSON outputs\npyzmq\nmsgspec\ngguf == 0.10.0\nimportlib_metadata\nmistral_common[opencv] >= 1.5.4\nopencv-python-headless >= 4.11.0    # required for video IO\npyyaml\nsix>=1.16.0; python_version > '3.11' # transitive dependency of pandas that needs to be the latest version for python 3.12\nsetuptools>=74.1.1; python_version > '3.11' # Setuptools is used by triton, we need to ensure a modern version is installed for 3.12+ so that it does not try to import distutils, which was removed in 3.12\neinops # Required for Qwen2-VL.\ncompressed-tensors == 0.9.2 # required for compressed-tensors\ndepyf==0.18.0 # required for profiling and debugging with compilation config\ncloudpickle # allows pickling lambda functions in model_executor/models/registry.py\nwatchfiles # required for http server to monitor the updates of TLS files\npython-json-logger # Used by logging as per examples/other/logging_configuration.md\nscipy # Required for phi-4-multimodal-instruct\nninja # Required for xgrammar, rocm, tpu, xpu\n```\n\n----------------------------------------\n\nTITLE: Docker Build and Run for CPU Environment\nDESCRIPTION: Commands to build and run vLLM in a Docker container with CPU configuration options including KV cache space and CPU core binding settings.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/source/getting_started/installation/cpu.md#2025-04-05_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ docker build -f docker/Dockerfile.cpu --tag vllm-cpu-env --target vllm-openai .\n\n# Launching OpenAI server \n$ docker run --rm \\\n             --privileged=true \\\n             --shm-size=4g \\\n             -p 8000:8000 \\\n             -e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \\\n             -e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \\\n             vllm-cpu-env \\\n             --model=meta-llama/Llama-3.2-1B-Instruct \\\n             --dtype=bfloat16 \\\n             other vLLM OpenAI server arguments\n```\n\n----------------------------------------\n\nTITLE: Calculating Quantized Activation Matrix\nDESCRIPTION: Formula for calculating the quantized activation matrix A using scale (s_a), quantized values (), all-ones matrix (J_a), and zero-point (z_a).\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nA = s_a (\\widehat A - J_a z_a)\n```\n\n----------------------------------------\n\nTITLE: Scaled GEMM Output\nDESCRIPTION: Formula for the scaled GEMM output D using quantized values (D) and scales (s_a, s_b).\nSOURCE: https://github.com/vllm-project/vllm/blob/main/csrc/quantization/cutlass_w8a8/Epilogues.md#2025-04-05_snippet_3\n\nLANGUAGE: math\nCODE:\n```\nD = s_a s_b \\widehat D + C\n```\n\n----------------------------------------\n\nTITLE: Initializing vLLM Model With Specific Seed in Python\nDESCRIPTION: This snippet shows how to initialize a vLLM model with a specific seed value, resulting in consistent random number outputs across runs for reproducibility.\nSOURCE: https://github.com/vllm-project/vllm/blob/main/docs/seed_parameter_behavior.md#2025-04-05_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom vllm import LLM\n\n# Initialize a vLLM model with a specific seed\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\", seed=42)\n\n# Try generating random numbers\nprint(random.randint(0, 100))  # Outputs the same number across runs\n```"
  }
]
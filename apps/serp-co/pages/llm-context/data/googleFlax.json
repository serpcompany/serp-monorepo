[
  {
    "owner": "google",
    "repo": "flax",
    "content": "TITLE: Implementing MLP in Flax\nDESCRIPTION: Defines a multi-layer perceptron (MLP) class using Flax's nnx module. The MLP includes linear layers, dropout, batch normalization, and GELU activation.\nSOURCE: https://github.com/google/flax/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nnx.Module):\n  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n    self.linear1 = Linear(din, dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.linear2 = Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x: jax.Array):\n    x = nnx.gelu(self.dropout(self.bn(self.linear1(x))))\n    return self.linear2(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Neural Network Model with Flax NNX\nDESCRIPTION: This snippet demonstrates how to create a simple neural network model using Flax NNX. It includes model definition, initialization, optimization, and a training step function. The example showcases Flax NNX's Pythonic syntax and integration with JAX transformations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport optax\n\n\nclass Model(nnx.Module):\n  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x):\n    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n    return self.linear_out(x)\n\nmodel = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\noptimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\n\n@nnx.jit  # automatic state management for JAX transforms\ndef train_step(model, optimizer, x, y):\n  def loss_fn(model):\n    y_pred = model(x)  # call methods directly\n    return ((y_pred - y) ** 2).mean()\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)  # in-place updates\n\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Layer with Running Mean in Flax\nDESCRIPTION: This snippet shows how to create a stateful layer in Flax that maintains a running mean. It demonstrates the use of the variable method to manage state variables separate from model parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass BiasAdderWithRunningMean(nn.Module):\n  decay: float = 0.99\n\n  @nn.compact\n  def __call__(self, x):\n    # easy pattern to detect if we're initializing via empty variable tree\n    is_initialized = self.has_variable('batch_stats', 'mean')\n    ra_mean = self.variable('batch_stats', 'mean',\n                            lambda s: jnp.zeros(s),\n                            x.shape[1:])\n    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n    if is_initialized:\n      ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n\n    return x - ra_mean.value + bias\n\n\nkey1, key2 = random.split(random.key(0), 2)\nx = jnp.ones((10,5))\nmodel = BiasAdderWithRunningMean()\nvariables = model.init(key1, x)\nprint('initialized variables:\\n', variables)\ny, updated_state = model.apply(variables, x, mutable=['batch_stats'])\nprint('updated state:\\n', updated_state)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP with Nested Modules\nDESCRIPTION: Creates a multi-layer perceptron by composing multiple modules including Linear layers, Dropout, and BatchNorm.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nnx.Module):\n  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n    self.linear1 = Linear(din, dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.linear2 = Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x: jax.Array):\n    x = nnx.gelu(self.dropout(self.bn(self.linear1(x))))\n    return self.linear2(x)\n\nmodel = MLP(2, 16, 5, rngs=nnx.Rngs(0))\n\ny = model(x=jnp.ones((3, 2)))\n\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation with BatchNorm\nDESCRIPTION: Demonstrates a complete training step function that handles BatchNorm updates, including gradient computation and state updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/batch_norm.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state: TrainState, batch):\n  def loss_fn(params):\n    logits, updates = state.apply_fn(\n      {'params': params, 'batch_stats': state.batch_stats},\n      x=batch['image'], train=True, mutable=['batch_stats'])\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=batch['label']).mean()\n    return loss, (logits, updates)\n  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n  (loss, (logits, updates)), grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n  state = state.replace(batch_stats=updates['batch_stats'])\n  metrics = {\n    'loss': loss,\n    'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n  }\n  return state, metrics\n```\n\n----------------------------------------\n\nTITLE: Creating and Training a Neural Network Model with NNX\nDESCRIPTION: This code snippet demonstrates how to define a neural network model using NNX's Module system, initialize it, create an optimizer, and implement a training step with automatic state management and inplace updates.\nSOURCE: https://github.com/google/flax/blob/main/flax/nnx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport optax\n\nclass Model(nnx.Module):\n  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x):\n    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n    return self.linear_out(x)\n\n\nmodel = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\noptimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\n\n@nnx.jit  # automatic state management\ndef train_step(model, optimizer, x, y):\n  def loss_fn(model):\n    y_pred = model(x)  # call methods directly\n    return ((y_pred - y) ** 2).mean()\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)  # inplace updates\n\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Restoring Latest Checkpoint with Orbax CheckpointManager\nDESCRIPTION: This snippet shows how to restore the latest checkpoint using Orbax's CheckpointManager. It demonstrates finding the latest step and restoring the checkpoint for that step.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstep = checkpoint_manager.latest_step()  # step = 4\ncheckpoint_manager.restore(step)\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Architecture with Flax Linen - Python\nDESCRIPTION: Implements a simple CNN using Flax's Linen API with convolutional layers, ReLU activation, and average pooling.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import linen as nn\n\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Training an MLP with Automatic State Management using nnx.jit in Python\nDESCRIPTION: Demonstrates how to create and train an MLP with custom Linear layers, Dropout, and BatchNorm using nnx.jit. The example shows automatic state propagation and reference sharing between the optimizer and model.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@nnx.jit  # Automatic state management\ndef train_step(model, optimizer, x, y):\n  def loss_fn(model: MLP):\n    y_pred = model(x)\n    return jnp.mean((y_pred - y) ** 2)\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)  # In place updates.\n\n  return loss\n\nx, y = jnp.ones((5, 2)), jnp.ones((5, 10))\nloss = train_step(model, optimizer, x, y)\n\nprint(f'{loss = }')\nprint(f'{optimizer.step.value = }')\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training a Flax Model with State Variables\nDESCRIPTION: This code demonstrates the initialization and training loop for a Flax model with state variables. It splits the state and parameters, initializes an optimizer, and iteratively updates the model using the update_step function.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = BiasAdderWithRunningMean()\nvariables = model.init(random.key(0), dummy_input)\n# Split state and params (which are updated by optimizer).\nstate, params = flax.core.pop(variables, 'params')\ndel variables  # Delete variables to avoid wasting resources\ntx = optax.sgd(learning_rate=0.02)\nopt_state = tx.init(params)\n\nfor _ in range(num_epochs):\n  opt_state, params, state = update_step(\n      model.apply, dummy_input, opt_state, params, state)\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint with Legacy Flax API\nDESCRIPTION: This snippet shows how to save a checkpoint using the legacy Flax checkpointing utilities. It's provided for comparison with the Orbax method and demonstrates the simpler but less feature-rich legacy approach.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.training import checkpoints\n\ncheckpoints.save_checkpoint(ckpt_dir='/tmp/flax_ckpt/flax-checkpointing',\n                            target=ckpt,\n                            step=0,\n                            overwrite=True,\n                            keep=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Parameters in Flax\nDESCRIPTION: Shows how to initialize parameters for a Flax model using the init method, which requires a PRNG key and dummy input data for shape inference.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkey1, key2 = random.split(random.key(0))\nx = random.normal(key1, (10,)) # Dummy input data\nparams = model.init(key2, x) # Initialization call\njax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes\n```\n\n----------------------------------------\n\nTITLE: Model Training Loop\nDESCRIPTION: Executes training loop with different learning rates and evaluates model performance\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor lr in  (0.03, 0.1, 0.3):\n  config.learning_rate = lr\n  name = f'{config.model}_lr={config.learning_rate}'\n  print(f'\\n\\n{name}')\n  state = train.train_and_evaluate(config, workdir=f'./models/{name}')\n```\n\n----------------------------------------\n\nTITLE: Training Flax Model with Optax Optimizer\nDESCRIPTION: Implements a training loop using Optax's update and apply_updates methods to optimize model parameters based on computed gradients.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(101):\n  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n  updates, opt_state = tx.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n  if i % 10 == 0:\n    print('Loss step {}: '.format(i), loss_val)\n```\n\n----------------------------------------\n\nTITLE: Implementing Flax Module with Parameters, Dropout, and BatchNorm\nDESCRIPTION: This example creates a Block module that combines differentiable parameters, stochastic layers (Dropout), and mutable variables (BatchNorm). It demonstrates how to initialize, apply, and update variables in a Flax module, including handling of RNGs and mutable state.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nn.Module):\n  features: int\n  training: bool\n  @nn.compact\n  def __call__(self, inputs):\n    x = nn.Dense(self.features)(inputs)\n    x = nn.Dropout(rate=0.5)(x, deterministic=not self.training)\n    x = nn.BatchNorm(use_running_average=not self.training)(x)\n    return x\n\nkey1, key2, key3, key4 = random.split(random.key(0), 4)\nx = random.uniform(key1, (3,4,4))\n\nmodel = Block(features=3, training=True)\n\ninit_variables = model.init({'params': key2, 'dropout': key3}, x)\n_, init_params = flax.core.pop(init_variables, 'params')\n\n# When calling `apply` with mutable kinds, returns a pair of output,\n# mutated_variables.\ny, mutated_variables = model.apply(\n    init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n\n# Now we reassemble the full variables from the updates (in a real training\n# loop, with the updated params from an optimizer).\nupdated_variables = flax.core.freeze(dict(params=init_params,\n                                          **mutated_variables))\n\nprint('updated variables:\\n', updated_variables)\nprint('initialized variable shapes:\\n',\n      jax.tree_util.tree_map(jnp.shape, init_variables))\nprint('output:\\n', y)\n\n# Let's run these model variables during \"evaluation\":\neval_model = Block(features=3, training=False)\ny = eval_model.apply(updated_variables, x)  # Nothing mutable; single return value.\nprint('eval output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: JAX Integration - Nested Module Management\nDESCRIPTION: Demonstrates handling of nested modules with state management and JIT compilation in a parent-child relationship.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Parent(nnx.Module):\n    def __init__(self, model: MLP):\n        self.model = model\n\n    def __call__(self, x):\n        params, batch_stats, counts, graphdef = self.model.split(nnx.Param, nnx.BatchStat, Count)\n\n        @jax.jit\n        def forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n            model = graphdef.merge(params, batch_stats, counts)\n            y = model(x)\n            params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)\n            return y, params, batch_stats, counts\n\n        y, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)\n\n        self.model.update(params, batch_stats, counts)\n        return y\n\nparent = Parent(model)\n\ny = parent(jnp.ones((2, 4)))\n\nprint(f'{y.shape = }')\nprint(f'{parent.model.count.value = }')\n```\n\n----------------------------------------\n\nTITLE: Setting Up Optax Optimizer for Flax Training\nDESCRIPTION: Demonstrates how to initialize an Adam optimizer using Optax with the model parameters and a specified learning rate.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport optax\ntx = optax.adam(learning_rate=learning_rate)\nopt_state = tx.init(params)\nloss_grad_fn = jax.value_and_grad(mse)\n```\n\n----------------------------------------\n\nTITLE: Training an MLP with Batch-Dependent State Using vmap\nDESCRIPTION: This training loop initializes and trains an MLP model with batch-dependent state using vmap. It separates parameters from state variables, initializes an optimizer, and iteratively updates the model through batched computation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = MLP(hidden_size=10, out_size=1)\nvariables = model.init(random.key(0), dummy_input)\n# Split state and params (which are updated by optimizer).\nstate, params = flax.core.pop(variables, 'params')\ndel variables  # Delete variables to avoid wasting resources\ntx = optax.sgd(learning_rate=0.02)\nopt_state = tx.init(params)\n\nfor _ in range(num_epochs):\n  opt_state, params, state, loss = update_step(\n      model.apply, X, Y, opt_state, params, state)\n```\n\n----------------------------------------\n\nTITLE: Training Step and Compilation in Haiku and Flax NNX\nDESCRIPTION: Illustrates how to write a training step and compile it using JAX just-in-time compilation in both frameworks. Highlights differences in handling gradients, optimizers, and model updates.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n...\n\n@jax.jit\ndef train_step(key, params, inputs, labels):\n  def loss_fn(params):\n    logits = model.apply(\n      params, key,\n      inputs, training=True # <== inputs\n\n    )\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(params)\n\n\n  params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)\n\n  return params\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel.train() # set deterministic=False\n\n@nnx.jit\ndef train_step(model, inputs, labels):\n  def loss_fn(model):\n    logits = model(\n\n      inputs, # <== inputs\n\n    )\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = nnx.grad(loss_fn)(model)\n  _, params, rest = nnx.split(model, nnx.Param, ...)\n  params = jax.tree.map(lambda p, g: p - 0.1 * g, params, grads)\n  nnx.update(model, nnx.merge_state(params, rest))\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Model with Flax NNX\nDESCRIPTION: Demonstrates core features of Flax NNX including eager initialization, reference sharing, and automatic state management using a simple neural network model with linear layers, batch normalization, and dropout.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport optax\n\nclass Model(nnx.Module):\n  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x):\n    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n    return self.linear_out(x)\n\nmodel = Model(2, 64, 3, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-3))\n\n@nnx.jit\ndef train_step(model, optimizer, x, y):\n  def loss_fn(model):\n    y_pred = model(x)\n    return ((y_pred - y) ** 2).mean()\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)\n\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing MNIST Dataset\nDESCRIPTION: Loads MNIST dataset using TensorFlow Datasets, normalizes the images, and creates shuffled batched datasets for training and testing.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\ntf.random.set_seed(0)\n\ntrain_steps = 1200\neval_every = 200\nbatch_size = 32\n\ntrain_ds: tf.data.Dataset = tfds.load('mnist', split='train')\ntest_ds: tf.data.Dataset = tfds.load('mnist', split='test')\n\ntrain_ds = train_ds.map(\n  lambda sample: {\n    'image': tf.cast(sample['image'], tf.float32) / 255,\n    'label': sample['label'],\n  }\n)\ntest_ds = test_ds.map(\n  lambda sample: {\n    'image': tf.cast(sample['image'], tf.float32) / 255,\n    'label': sample['label'],\n  }\n)\n\ntrain_ds = train_ds.repeat().shuffle(1024)\ntrain_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\ntest_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dropout in Flax Model\nDESCRIPTION: A complete example showing how to properly implement and use dropout in a Flax model. The code demonstrates initializing PRNG keys, defining a model with dropout, and performing forward passes with appropriate randomness handling.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_sharp_bits.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Randomness.\nseed = 0\nroot_key = jax.random.key(seed=seed)\nmain_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)\n\n# A simple network.\nclass MyModel(nn.Module):\n  num_neurons: int\n  training: bool\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(self.num_neurons)(x)\n    # Set the dropout layer with a rate of 50% .\n    # When the `deterministic` flag is `True`, dropout is turned off.\n    x = nn.Dropout(rate=0.5, deterministic=not self.training)(x)\n    return x\n\n# Instantiate `MyModel` (you don't need to set `training=True` to\n# avoid performing the forward pass computation).\nmy_model = MyModel(num_neurons=3, training=False)\n\nx = jax.random.uniform(key=main_key, shape=(3, 4, 4))\n\n# Initialize with `flax.linen.init()`.\n# The `params_key` is equivalent to a dictionary of PRNGs.\n# (Here, you are providing only one PRNG key.) \nvariables = my_model.init(params_key, x)\n\n# Perform the forward pass with `flax.linen.apply()`.\ny = my_model.apply(variables, x, rngs={'dropout': dropout_key})\n```\n\n----------------------------------------\n\nTITLE: Defining Mean Squared Error Loss Function with Flax\nDESCRIPTION: Implements a mean squared error loss function for a Flax model using JAX's vmap for vectorized computation across all samples.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Same as JAX version but using model.apply().\n@jax.jit\ndef mse(params, x_batched, y_batched):\n  # Define the squared loss for a single pair (x,y)\n  def squared_error(x, y):\n    pred = model.apply(params, x)\n    return jnp.inner(y-pred, y-pred) / 2.0\n  # Vectorize the previous to compute the average of the loss on all samples.\n  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training and Evaluation Loop\nDESCRIPTION: Main training loop that performs optimization steps, computes metrics for both training and test sets, and records the results after each epoch.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor step,batch in enumerate(train_ds.as_numpy_iterator()):\n\n  # Run optimization steps over training batches and compute batch metrics\n  state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n\n  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n    for metric,value in state.metrics.compute().items(): # compute metrics\n      metrics_history[f'train_{metric}'].append(value) # record metrics\n    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n    # Compute metrics on the test set after each training epoch\n    test_state = state\n    for test_batch in test_ds.as_numpy_iterator():\n      test_state = compute_metrics(state=test_state, batch=test_batch)\n\n    for metric,value in test_state.metrics.compute().items():\n      metrics_history[f'test_{metric}'].append(value)\n\n    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['train_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Using nn.compact in Flax\nDESCRIPTION: Example of defining a Multi-Layer Perceptron using the compact decorator in Flax. Submodules are defined inline within the forward pass method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/setup_or_nncompact.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(32, name=\"dense1\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(32, name=\"dense2\")(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating a Stateful Linear Module with Custom Variable Type in Python\nDESCRIPTION: Implements a StatefulLinear module that tracks the number of forward passes through a custom Count variable type. The example shows how to define parameters and custom state variables in a module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Count(nnx.Variable): pass\n\nclass StatefulLinear(nnx.Module):\n  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n    self.w = nnx.Param(jax.random.uniform(rngs(), (din, dout)))\n    self.b = nnx.Param(jnp.zeros((dout,)))\n    self.count = Count(jnp.array(0, dtype=jnp.uint32))\n\n  def __call__(self, x: jax.Array):\n    self.count += 1\n    return x @ self.w + self.b\n\nmodel = StatefulLinear(din=3, dout=5, rngs=nnx.Rngs(0))\ny = model(jnp.ones((1, 3)))\n\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Defining Encoder Module with Submodules in Flax/Linen\nDESCRIPTION: Demonstrates how to define modules that use other modules as components. Shows the transition from old-style module instantiation to new Linen approach where modules are constructed and called separately.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass Encoder(nn.Module):\n\n  def apply(self, x):\n    x = nn.Dense(x, 500)\n    x = nn.relu(x)\n    z = nn.Dense(x, 500, name=\"latents\")\n    return z\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass Encoder(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(500)(x)\n    x = nn.relu(x)\n    z = nn.Dense(500, name='latents')(x)\n    return z\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Parameters with StateAxes in Flax NNX\nDESCRIPTION: Demonstrates how to selectively vectorize only the Param variables while broadcasting Count variables using StateAxes. This example defines a Weights module with kernel, bias parameters and a count variable, then uses nnx.vmap with a custom StateAxes configuration.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, kernel: jax.Array, bias: jax.Array, count: jax.Array):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n    self.count = Count(count)\n\nweights = Weights(\n  kernel=random.uniform(random.key(0), (10, 2, 3)),\n  bias=jnp.zeros((10, 3)),\n  count=jnp.array(0),\n)\nx = jax.random.normal(random.key(1), (10, 2))\n\n\ndef stateful_vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  weights.count += 1\n  return x @ weights.kernel + weights.bias\n\nstate_axes = nnx.StateAxes({nnx.Param: 0, Count: None}) # broadcast Count\ny = nnx.vmap(stateful_vector_dot, in_axes=(state_axes, 0), out_axes=1)(weights, x)\n\nweights.count\n```\n\n----------------------------------------\n\nTITLE: Integrating Flax Model with Optax Optimizer\nDESCRIPTION: This snippet shows how to integrate a Flax model with an Optax optimizer, demonstrating a complete training step that updates both model parameters and state variables.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0, 1))\ndef update_step(tx, apply_fn, x, opt_state, params, state):\n\n  def loss(params):\n    y, updated_state = apply_fn({'params': params, **state},\n                                x, mutable=list(state.keys()))\n    l = ((x - y) ** 2).sum()\n    return l, updated_state\n\n  (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n  updates, opt_state = tx.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n  return opt_state, params, state\n\nx = jnp.ones((10,5))\nvariables = model.init(random.key(0), x)\nstate, params = flax.core.pop(variables, 'params')\ndel variables\ntx = optax.sgd(learning_rate=0.02)\nopt_state = tx.init(params)\n\nfor _ in range(3):\n  opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)\n  print('Updated state: ', state)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Step - Python\nDESCRIPTION: Defines JIT-compiled training step function that computes loss, gradients and updates model parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state, batch):\n  \"\"\"Train for a single step.\"\"\"\n  def loss_fn(params):\n    logits = state.apply_fn({'params': params}, batch['image'])\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch['label']).mean()\n    return loss\n  grad_fn = jax.grad(loss_fn)\n  grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n  return state\n```\n\n----------------------------------------\n\nTITLE: Dense Layer and MLP Implementation\nDESCRIPTION: Implements a dense (fully connected) layer and a simple multi-layer perceptron (MLP) using Flax's functional API. Includes parameter initialization and forward pass functionality.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef dense(\n    scope: Scope,\n    inputs: Array,\n    features: int,\n    bias: bool = True,\n    kernel_init=nn.linear.default_kernel_init,\n    bias_init=nn.initializers.zeros_init(),\n):\n  kernel = scope.param('kernel', kernel_init, (inputs.shape[-1], features))\n  y = jnp.dot(inputs, kernel)\n  if bias:\n    y += scope.param('bias', bias_init, (features,))\n  return y\n\n\nmodel_fn = functools.partial(dense, features=3)\n\nx = jnp.ones((1, 2))\ny, params = init(model_fn)(random.key(0), x)\nprint(params)\n\n\ndef mlp(scope: Scope, inputs: Array, features: int):\n  hidden = scope.child(dense, 'hidden')(inputs, features)\n  hidden = nn.relu(hidden)\n  return dense(scope.push('out'), hidden, 1)\n\n\ninit(mlp)(random.key(0), x, features=3)\n```\n\n----------------------------------------\n\nTITLE: Implementing TrainState Class in Python using Flax\nDESCRIPTION: Core implementation of TrainState class that manages model parameters, optimization state and training step count. Includes methods for applying gradients and creating new instances.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(flax.struct.PyTreeNode):\n  step: int\n  apply_fn: Callable = flax.struct.field(pytree_node=False)\n  params: flax.core.FrozenDict[str, Any]\n  tx: optax.GradientTransformation = flax.struct.field(pytree_node=False)\n  opt_state: optax.OptState\n\n  def apply_gradients(self, *, grads, **kwargs):\n    updates, new_opt_state = self.tx.update(\n        grads, self.opt_state, self.params)\n    new_params = optax.apply_updates(self.params, updates)\n    return self.replace(\n        step=self.step + 1,\n        params=new_params,\n        opt_state=new_opt_state,\n        **kwargs,\n    )\n\n  @classmethod\n  def create(cls, *, apply_fn, params, tx, **kwargs):\n    opt_state = tx.init(params)\n    return cls(\n        step=0,\n        apply_fn=apply_fn,\n        params=params,\n        tx=tx,\n        opt_state=opt_state,\n        **kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Layer Perceptron in Flax\nDESCRIPTION: This snippet demonstrates how to create a custom multi-layer perceptron using Flax's nn.Module. It shows the explicit way of defining layers in the setup method and using them in the __call__ method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass ExplicitMLP(nn.Module):\n  features: Sequence[int]\n\n  def setup(self):\n    # we automatically know what to do with lists, dicts of submodules\n    self.layers = [nn.Dense(feat) for feat in self.features]\n    # for single submodules, we would just write:\n    # self.layer1 = nn.Dense(feat1)\n\n  def __call__(self, inputs):\n    x = inputs\n    for i, lyr in enumerate(self.layers):\n      x = lyr(x)\n      if i != len(self.layers) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = ExplicitMLP(features=[3,4,5])\nparams = model.init(key2, x)\ny = model.apply(params, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Creating Parallel Train State with jax.pmap() for Model Ensembling in JAX\nDESCRIPTION: This snippet demonstrates how to modify a create_train_state function for parallel execution using jax.pmap(). It declares learning_rate and momentum as static broadcast arguments while mapping over the RNG to create different model initializations across devices.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/ensembling.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2))\ndef create_train_state(rng, learning_rate, momentum):\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Head Attention using Flax vmap\nDESCRIPTION: Demonstrates how to build a multi-headed attention module from a single-headed implementation using Flax's vmap transformation. The code includes raw attention computation, single-head attention, and multi-head attention implementations with support for batching and dropout.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass RawDotProductAttention(nn.Module):\n  attn_dropout_rate: float = 0.1\n  train: bool = False\n\n  @nn.compact\n  def __call__(self, query, key, value, bias=None, dtype=jnp.float32):\n    assert key.ndim == query.ndim\n    assert key.ndim == value.ndim\n\n    n = query.ndim\n    attn_weights = lax.dot_general(\n        query, key,\n        (((n-1,), (n - 1,)), ((), ())))\n    if bias is not None:\n      attn_weights += bias\n    norm_dims = tuple(range(attn_weights.ndim // 2, attn_weights.ndim))\n    attn_weights = jax.nn.softmax(attn_weights, axis=norm_dims)\n    attn_weights = nn.Dropout(self.attn_dropout_rate)(attn_weights,\n                                                      deterministic=not self.train)\n    attn_weights = attn_weights.astype(dtype)\n\n    contract_dims = (\n        tuple(range(n - 1, attn_weights.ndim)),\n        tuple(range(0, n  - 1)))\n    y = lax.dot_general(\n        attn_weights, value,\n        (contract_dims, ((), ())))\n    return y\n```\n\n----------------------------------------\n\nTITLE: Creating JIT-Compiled Model Inference Function in Flax\nDESCRIPTION: Defines a JIT-compiled prediction function that takes a CNN model and a batch of data as input, returning the predicted class labels. The model is set to evaluation mode before inference to ensure deterministic outputs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval() # Switch to evaluation mode.\n\n@nnx.jit\ndef pred_step(model: CNN, batch):\n  logits = model(batch['image'])\n  return logits.argmax(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Variable Creation in Flax Linen and NNX\nDESCRIPTION: Shows how to instantiate models and initialize parameters in Flax Linen and Flax NNX. Linen uses a separate init call, while NNX automatically initializes parameters during instantiation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(256, 10)\nsample_x = jnp.ones((1, 784))\nvariables = model.init(jax.random.key(0), sample_x, training=False)\nparams = variables[\"params\"]\n\nassert params['Dense_0']['bias'].shape == (10,)\nassert params['Block_0']['Dense_0']['kernel'].shape == (784, 256)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(784, 256, 10, rngs=nnx.Rngs(0))\n\n\n# Parameters were already initialized during model instantiation.\n\nassert model.linear.bias.value.shape == (10,)\nassert model.block.linear.kernel.value.shape == (784, 256)\n```\n\n----------------------------------------\n\nTITLE: Forward Pass with Dropout During Training\nDESCRIPTION: Shows how to perform a forward pass with dropout enabled during training, including proper PRNG key handling and training flag setting.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny = my_model.apply({'params': params}, x, training=True, rngs={'dropout': dropout_key})\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Model with Flax NNX\nDESCRIPTION: Implements a convolutional neural network using Flax NNX Module class with two convolutional layers and two linear layers.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nfrom functools import partial\n\nclass CNN(nnx.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  def __init__(self, *, rngs: nnx.Rngs):\n    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n    self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n    self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n    self.linear1 = nnx.Linear(3136, 256, rngs=rngs)\n    self.linear2 = nnx.Linear(256, 10, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.avg_pool(nnx.relu(self.conv1(x)))\n    x = self.avg_pool(nnx.relu(self.conv2(x)))\n    x = x.reshape(x.shape[0], -1)  # flatten\n    x = nnx.relu(self.linear1(x))\n    x = self.linear2(x)\n    return x\n\nmodel = CNN(rngs=nnx.Rngs(0))\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Predictions\nDESCRIPTION: Creates a grid of test images with their predicted labels using matplotlib for visual inspection of model performance.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(5, 5, figsize=(12, 12))\nfor i, ax in enumerate(axs.flatten()):\n    ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')\n    ax.set_title(f\"label={pred[i]}\")\n    ax.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual Gradient Descent Training in Flax\nDESCRIPTION: Shows a basic gradient descent training loop using JAX's value_and_grad function to compute gradients and update model parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearning_rate = 0.3  # Gradient step size.\nprint('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\nloss_grad_fn = jax.value_and_grad(mse)\n\n@jax.jit\ndef update_params(params, learning_rate, grads):\n  params = jax.tree_util.tree_map(\n      lambda p, g: p - learning_rate * g, params, grads)\n  return params\n\nfor i in range(101):\n  # Perform one gradient update.\n  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n  params = update_params(params, learning_rate, grads)\n  if i % 10 == 0:\n    print(f'Loss step {i}: ', loss_val)\n```\n\n----------------------------------------\n\nTITLE: Basic NNX Model Training with JAX\nDESCRIPTION: Implementation of a basic neural network model using Flax NNX with a training loop. Demonstrates model definition, optimizer setup, metrics tracking and training step function using nnx.jit.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/performance.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nclass Model(nnx.Module):\n  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x):\n    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n    return self.linear_out(x)\n  \nmodel = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\noptimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\nmetrics = nnx.MultiMetric(\n  loss=nnx.metrics.Average('loss'),\n)\n\n@nnx.jit  # <== currently slow\ndef train_step(model, optimizer, metrics, x, y):\n  def loss_fn(model):\n    y_pred = model(x)  # call methods directly\n    return ((y_pred - y) ** 2).mean()\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)  # in-place updates\n  metrics.update(loss=loss)\n\n  return loss\n  \nfor _ in range(10):\n  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n  loss = train_step(model, optimizer, metrics, x, y)\n```\n\n----------------------------------------\n\nTITLE: Creating an Update Step Function for Flax Models\nDESCRIPTION: This function defines a single update step for training a Flax model. It calculates the loss and gradients, applies updates using an optimizer, and returns the updated optimizer state, parameters, and model state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef update_step(apply_fn, x, opt_state, params, state):\n  def loss(params):\n    y, updated_state = apply_fn({'params': params, **state},\n                                x, mutable=list(state.keys()))\n    l = ((x - y) ** 2).sum() # Replace with your loss here.\n    return l, updated_state\n\n  (l, updated_state), grads = jax.value_and_grad(\n      loss, has_aux=True)(params)\n  updates, opt_state = tx.update(grads, opt_state)  # Defined below.\n  params = optax.apply_updates(params, updates)\n  return opt_state, params, updated_state\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Step with Optax in Python\nDESCRIPTION: A complete training step function using Optax for optimization. The function computes gradients with respect to model parameters, applies the optimizer transformations, and updates the model state.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@functools.partial(jax.jit, static_argnums=(4, 5))\ndef train_step(opt_state, variables, inputs, labels, apply_fn, tx_update_fn):\n\n  def loss_fn(params):\n    logits, new_model_state = apply_fn(\n        {**variables, 'params': params}, inputs, mutable=['batch_stats'])\n    loss = xent_loss(logits, labels)\n    return loss, new_model_state\n\n  variables, params = variables.pop('params')\n  (loss, new_model_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n      params)\n  updates, new_opt_state = tx_update_fn(grads, opt_state, params)\n  new_params = optax.apply_updates(params, updates)\n  new_variables = {**variables, **new_model_state, 'params': new_params}\n  return new_opt_state, new_variables, loss\n\n\nopt_state = tx.init(variables['params'])\nfor batch in ds.as_numpy_iterator():\n  opt_state, variables, loss = train_step(\n      opt_state, variables, batch['image'], batch['label'], model.apply,\n      tx.update)\n  print(loss)\n```\n\n----------------------------------------\n\nTITLE: Implementing Core APIs for Neural Network Operations in Python\nDESCRIPTION: This snippet defines the core APIs for neural network operations, including Variable, State, GraphDef, and Module classes. It also implements the Rngs class for managing random number generation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/tiny_nnx.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dataclasses\nimport hashlib\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nA = tp.TypeVar(\"A\")\nM = tp.TypeVar(\"M\", bound=\"Module\")\nSharding = tp.Tuple[tp.Optional[str], ...]\nArray = jax.Array\n\n\nclass Variable(tp.Generic[A]):\n\n  def __init__(\n      self,\n      value: A,\n      *,\n      sharding: tp.Optional[Sharding] = None,\n  ):\n    self.value = value\n    self.sharding = sharding\n\n  def __repr__(self) -> str:\n    return (\n        f\"{type(self).__name__}(value={self.value}, sharding={self.sharding})\"\n    )\n\n  def __init_subclass__(cls):\n    super().__init_subclass__()\n    jax.tree_util.register_pytree_node(\n        cls,\n        lambda x: ((x.value,), (x.sharding,)),\n        lambda metadata, value: cls(value[0], sharding=metadata[0]),\n    )\n\n\nclass State(dict[str, Variable[tp.Any]]):\n\n  def extract(self, variable_type: tp.Type[Variable]) -> \"State\":\n    return State(\n        {\n            path: variable\n            for path, variable in self.items()\n            if isinstance(variable, variable_type)\n        }\n    )\n\n  def __repr__(self) -> str:\n    elems = \",\\n  \".join(\n        f\"'{path}': {variable}\".replace(\"\\n\", \"\\n    \")\n        for path, variable in self.items()\n    )\n    return f\"State({{\\n  {elems}\\n}})\"\n\n\njax.tree_util.register_pytree_node(\n    State,\n    # in reality, values and paths should be sorted by path\n    lambda x: (tuple(x.values()), tuple(x.keys())),\n    lambda paths, values: State(dict(zip(paths, values))),\n)\n\n\n@dataclasses.dataclass\nclass GraphDef(tp.Generic[M]):\n  type: tp.Type[M]\n  index: int\n  submodules: dict[str, tp.Union[\"GraphDef[Module]\", int]]\n  static_fields: dict[str, tp.Any]\n\n  def merge(self, state: State) -> M:\n    module = GraphDef._build_module_recursive(self, {})\n    module.update(state)\n    return module\n\n  @staticmethod\n  def _build_module_recursive(\n      graphdef: tp.Union[\"GraphDef[M]\", int],\n      index_to_module: dict[int, \"Module\"],\n  ) -> M:\n    if isinstance(graphdef, int):\n      return index_to_module[graphdef] # type: ignore\n\n    assert graphdef.index not in index_to_module\n\n    # add a dummy module to the index to avoid infinite recursion\n    module = object.__new__(graphdef.type)\n    index_to_module[graphdef.index] = module\n\n    submodules = {\n        name: GraphDef._build_module_recursive(submodule, index_to_module)\n        for name, submodule in graphdef.submodules.items()\n    }\n    vars(module).update(graphdef.static_fields)\n    vars(module).update(submodules)\n    return module\n\n  def apply(\n      self, state: State\n  ) -> tp.Callable[..., tuple[tp.Any, tuple[State, \"GraphDef[M]\"]]]:\n    def _apply(*args, **kwargs):\n      module = self.merge(state)\n      out = module(*args, **kwargs)  # type: ignore\n      return out, module.split()\n\n    return _apply\n\n\nclass Module:\n\n  def split(self: M) -> tp.Tuple[State, GraphDef[M]]:\n    state = State()\n    graphdef = Module._partition_recursive(\n        module=self, module_id_to_index={}, path_parts=(), state=state\n    )\n    assert isinstance(graphdef, GraphDef)\n    return state, graphdef\n\n  @staticmethod\n  def _partition_recursive(\n      module: M,\n      module_id_to_index: dict[int, int],\n      path_parts: tp.Tuple[str, ...],\n      state: State,\n  ) -> tp.Union[GraphDef[M], int]:\n    if id(module) in module_id_to_index:\n      return module_id_to_index[id(module)]\n\n    index = len(module_id_to_index)\n    module_id_to_index[id(module)] = index\n\n    submodules = {}\n    static_fields = {}\n\n    # iterate fields sorted by name to ensure deterministic order\n    for name, value in sorted(vars(module).items(), key=lambda x: x[0]):\n      value_path = (*path_parts, name)\n      # if value is a Module, recurse\n      if isinstance(value, Module):\n        submoduledef = Module._partition_recursive(\n            value, module_id_to_index, value_path, state\n        )\n        submodules[name] = submoduledef\n      # if value is a Variable, add to state\n      elif isinstance(value, Variable):\n        state[\"/\".join(value_path)] = value\n      else:  # otherwise, add to graphdef fields\n        static_fields[name] = value\n\n    return GraphDef(\n        type=type(module),\n        index=index,\n        submodules=submodules,\n        static_fields=static_fields,\n    )\n\n  def update(self, state: State) -> None:\n    for path, value in state.items():\n      path_parts = path.split(\"/\")\n      Module._set_value_at_path(self, path_parts, value)\n\n  @staticmethod\n  def _set_value_at_path(\n      module: \"Module\", path_parts: tp.Sequence[str], value: Variable[tp.Any]\n  ) -> None:\n    if len(path_parts) == 1:\n      setattr(module, path_parts[0], value)\n    else:\n      Module._set_value_at_path(\n          getattr(module, path_parts[0]), path_parts[1:], value\n      )\n\n\n@dataclasses.dataclass\nclass Rngs:\n  key: jax.Array\n  count: int = 0\n  count_path: tuple[int, ...] = ()\n\n  def fork(self) -> \"Rngs\":\n    \"\"\"Forks the context, guaranteeing that all the random numbers generated\n    will be different from the ones generated in the original context. Fork is\n    used to create a new Rngs that can be passed to a JAX transform\"\"\"\n    count_path = self.count_path + (self.count,)\n    self.count += 1\n    return Rngs(self.key, count_path=count_path)\n\n  def make_rng(self) -> jax.Array:\n    fold_data = self._stable_hash(self.count_path + (self.count,))\n    self.count += 1\n    return random.fold_in(self.key, fold_data)  # type: ignore\n\n  @staticmethod\n  def _stable_hash(data: tuple[int, ...]) -> int:\n    hash_str = \" \".join(str(x) for x in data)\n    _hash = hashlib.blake2s(hash_str.encode())\n    hash_bytes = _hash.digest()\n    # uint32 is represented as 4 bytes in big endian\n    return int.from_bytes(hash_bytes[:4], byteorder=\"big\")\n\n\n# in the real NNX Rngs is not a pytree, instead\n# it has a split/merge API similar to Module\n# but for simplicity we use a pytree here\njax.tree_util.register_pytree_node(\n    Rngs,\n    lambda x: ((x.key,), (x.count, x.count_path)),\n    lambda metadata, value: Rngs(value[0], *metadata),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Distributed Training Loop with Flax NNX\nDESCRIPTION: Creates a conventional training loop using Flax NNX with sharded inputs and labels. Uses nnx.jit and optimizer to perform gradient updates across sharded data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noptimizer = nnx.Optimizer(sharded_model, optax.adam(1e-3))  # reference sharing\n\n@nnx.jit\ndef train_step(model, optimizer, x, y):\n  def loss_fn(model: DotReluDot):\n    y_pred = model(x)\n    return jnp.mean((y_pred - y) ** 2)\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)\n\n  return loss\n\ninput = jax.device_put(jax.random.normal(jax.random.key(1), (8, 1024)), data_sharding)\nlabel = jax.device_put(jax.random.normal(jax.random.key(2), (8, 1024)), data_sharding)\n\nwith mesh:\n  for i in range(5):\n    loss = train_step(sharded_model, optimizer, input, label)\n    print(loss)    # Model (over-)fitting to the labels quickly.\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic MLP Module in Flax\nDESCRIPTION: A simple implementation of a Multi-Layer Perceptron (MLP) using Flax Linen. The module defines a hidden layer with 4 units followed by a ReLU activation and an output layer with 1 unit.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import random, numpy as jnp\nfrom flax import linen as nn\n\nclass MLP(nn.Module):\n  @nn.compact\n  def __call__(self, xs):\n    h = nn.Dense(4, name='hidden')(xs)\n    h = nn.relu(h)\n    return nn.Dense(1, name='out')(h)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Inference Step\nDESCRIPTION: Defines a JIT-compiled prediction function and performs inference on a test batch using trained model parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef pred_step(state, batch):\n  logits = state.apply_fn({'params': state.params}, test_batch['image'])\n  return logits.argmax(axis=1)\n\ntest_batch = test_ds.as_numpy_iterator().next()\npred = pred_step(state, test_batch)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dense Layer in Flax\nDESCRIPTION: Demonstrates how to instantiate a basic dense layer from Flax's linen module with a specified number of output features.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# We create one dense layer instance (taking 'features' parameter as input)\nmodel = nn.Dense(features=5)\n```\n\n----------------------------------------\n\nTITLE: Parallel Model Application and Ensemble Aggregation with JAX\nDESCRIPTION: This code shows how to implement a parallel apply_model function that computes per-device loss and gradients, and then uses jax.lax.pmean() to average softmax probabilities across ensemble members for improved predictions, using a specified axis_name for cross-device communication.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/ensembling.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.pmap, axis_name='ensemble')\ndef apply_model(state, images, labels):\n  def loss_fn(params):\n    logits = CNN().apply({'params': params}, images)\n    one_hot = jax.nn.one_hot(labels, 10)\n    loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()\n    return loss, logits\n\n  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n  (loss, logits), grads = grad_fn(state.params)\n  probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ensemble')\n  accuracy = jnp.mean(jnp.argmax(probs, -1) == labels)\n  return grads, loss, accuracy\n```\n\n----------------------------------------\n\nTITLE: Manually Reproducing PRNG Keys for Multiple Sub-Module Instances\nDESCRIPTION: This code snippet demonstrates how to manually reproduce the PRNG keys for multiple instances of sub-Modules in a Flax Module. It shows how the naming convention affects the data used for key generation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nstream_seed = jax.random.key(0)\nfor initial_data in ((), ('RNGSubModule_0',), ('RNGSubModule_1',)):\n  if initial_data:\n    module_name = initial_data[-1]\n  else:\n    module_name = 'RNGModule'\n  for call_count in (1, 2):\n    hash_int = produce_hash(data=initial_data+(call_count,))\n    rng_key = jax.random.fold_in(stream_seed, jnp.uint32(hash_int))\n    print(f\"{module_name}, count {call_count}: {rng_key}\")\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients in Flax Training Loop\nDESCRIPTION: Training step implementation for a Flax model that computes loss and gradients, then applies them to update model state. Uses JAX's value_and_grad for gradient computation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlogits=logits, labels=batch['label'])\n      return loss, logits\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (loss, logits), grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state\n```\n\n----------------------------------------\n\nTITLE: Saving and Managing Checkpoints with Orbax CheckpointManager in Python\nDESCRIPTION: Demonstrates how to use Orbax CheckpointManager for saving, loading, and managing checkpoints. It includes setting up the CheckpointManager with options, saving checkpoints in a training loop, and restoring checkpoints with proper arguments.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCKPT_DIR = '/tmp/orbax_upgrade/orbax'\n\n# At the top level\nmgr_options = orbax.checkpoint.CheckpointManagerOptions(\n  create=True, max_to_keep=3, keep_period=2, step_prefix='test')\nckpt_mgr = orbax.checkpoint.CheckpointManager(\n  CKPT_DIR,\n  orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n\n# Inside your training loop\nfor step in range(MAX_STEPS):\n  # do training\n  save_args = flax.training.orbax_utils.save_args_from_target(CKPT_PYTREE)\n  ckpt_mgr.save(step, CKPT_PYTREE, save_kwargs={'save_args': save_args})\n\n\nrestore_args = flax.training.orbax_utils.restore_args_from_target(TARGET_PYTREE, mesh=None)\nckpt_mgr.restore(4, items=TARGET_PYTREE, restore_kwargs={'restore_args': restore_args})\n```\n\n----------------------------------------\n\nTITLE: Basic nnx.vmap Example with Weights Module\nDESCRIPTION: Demonstrates using nnx.vmap to extend a vector_dot function to work on batched inputs, using a Weights Module to hold parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, kernel: jax.Array, bias: jax.Array):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n\nweights = Weights(\n  kernel=random.uniform(random.key(0), (10, 2, 3)),\n  bias=jnp.zeros((10, 3)),\n)\nx = jax.random.normal(random.key(1), (10, 2))\n\ndef vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  return x @ weights.kernel + weights.bias\n\ny = nnx.vmap(vector_dot, in_axes=0, out_axes=1)(weights, x)\n\nprint(f'{y.shape = }')\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Creating a Vectorized Update Function with vmap for Batch Processing\nDESCRIPTION: This function implements a training step that uses JAX's vmap to vectorize computations across a batch while handling batch-dependent state properly. It specifies the batch axis name to match the one in the model and ensures state is not vmapped.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef update_step(apply_fn, x_batch, y_batch, opt_state, params, state):\n\n  def batch_loss(params):\n    def loss_fn(x, y):\n      pred, updated_state = apply_fn(\n        {'params': params, **state},\n        x, mutable=list(state.keys())\n      )\n      return (pred - y) ** 2, updated_state\n\n    loss, updated_state = jax.vmap(\n      loss_fn, out_axes=(0, None),  # Do not vmap `updated_state`.\n      axis_name='batch'  # Name batch dim\n    )(x_batch, y_batch)  # vmap only `x`, `y`, but not `state`.\n    return jnp.mean(loss), updated_state\n\n  (loss, updated_state), grads = jax.value_and_grad(\n    batch_loss, has_aux=True\n  )(params)\n\n  updates, opt_state = tx.update(grads, opt_state)  # Defined below.\n  params = optax.apply_updates(params, updates)\n  return opt_state, params, updated_state, loss\n```\n\n----------------------------------------\n\nTITLE: Using nnx.vmap to Create a Stack of Weights\nDESCRIPTION: Shows how to use nnx.vmap to transform an initializer function, creating a stack of Weights Modules with the same shapes as before.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_weights(seed: jax.Array):\n  return Weights(\n    kernel=random.uniform(random.key(seed), (2, 3)),\n    bias=jnp.zeros((3,)),\n  )\n\nseeds = jnp.arange(10)\nweights = nnx.vmap(create_weights)(seeds)\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Computing Metrics - Python\nDESCRIPTION: Implements JIT-compiled function to compute loss and accuracy metrics using the model predictions.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef compute_metrics(*, state, batch):\n  logits = state.apply_fn({'params': state.params}, batch['image'])\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch['label']).mean()\n  metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=batch['label'], loss=loss)\n  metrics = state.metrics.merge(metric_updates)\n  state = state.replace(metrics=metrics)\n  return state\n```\n\n----------------------------------------\n\nTITLE: JAX Integration - Advanced State Management\nDESCRIPTION: Shows advanced state management with separate handling of parameters, batch statistics, and custom state types.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparams, batch_stats, counts, graphdef = model.split(nnx.Param, nnx.BatchStat, Count)\n\n@jax.jit\ndef forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n  model = graphdef.merge(params, batch_stats, counts)\n  y = model(x, train=True)\n  params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)\n  return y, params, batch_stats, counts\n\nx = jnp.ones((2, 4))\ny, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)\n\nmodel.update(params, batch_stats, counts)\n\nprint(f'{y.shape = }')\nprint(f'{model.count = }')\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Architecture with NNX\nDESCRIPTION: Implementation of a multi-layer perceptron (MLP) using NNX modules including custom Block and Count components. Shows stateful variable handling and module initialization patterns.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nnx.Module):\n  def __init__(self, din, dout, *, rngs):\n    self.linear = nnx.Linear(din, dout, rngs=rngs)\n    self.bn = nnx.BatchNorm(dout, rngs=rngs)\n\n  def __call__(self, x):\n    return nnx.relu(self.bn(self.linear(x)))\n\n\nclass MLP(nnx.Module):\n  def __init__(self, nlayers, dim, *, rngs): # explicit RNG threading\n    self.blocks = [\n      Block(dim, dim, rngs=rngs) for _ in range(nlayers)\n    ]\n    self.count = Count(0)  # stateful variables are defined as attributes\n\n  def __call__(self, x):\n    self.count.value += 1  # in-place stateful updates\n    for block in self.blocks:\n      x = block(x)\n    return x\n\nclass Count(nnx.Variable):   # custom Variable types define the \"collections\"\n  pass\n\nmodel = MLP(5, 4, rngs=nnx.Rngs(0))  # no special `init` method\nmodel.set_attributes(use_running_average=False)  # set flags\ny = model(jnp.ones((2, 4)))  # call methods directly\n\nprint(f'{model = }'[:500] + '\\n...')\n```\n\n----------------------------------------\n\nTITLE: Compiling Flax Model Training Step with JAX\nDESCRIPTION: This function defines and compiles a training step for a Flax model using JAX, including gradient computation and parameter updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),\n                   out_shardings=state_sharding)\ndef train_step(state, x):\n  # A fake loss function.\n  def loss_unrolled(params):\n    y = model.apply({'params': params}, x)\n    return y.sum()\n  grad_fn = jax.grad(loss_unrolled)\n  grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n  return state\n\nwith mesh:\n  new_state = train_step(initialized_state, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Neural Network Layers in Python\nDESCRIPTION: This snippet defines basic neural network layers including Linear, BatchNorm, and Dropout. These layers inherit from the Module class and implement their forward pass logic.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/tiny_nnx.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Param(Variable[A]):\n  pass\n\n\nclass BatchStat(Variable[A]):\n  pass\n\n\nclass Linear(Module):\n\n  def __init__(self, din: int, dout: int, *, rngs: Rngs):\n    self.din = din\n    self.dout = dout\n    key = rngs.make_rng()\n    self.w = Param(random.uniform(key, (din, dout)))\n    self.b = Param(jnp.zeros((dout,)))\n\n  def __call__(self, x: jax.Array) -> jax.Array:\n    return x @ self.w.value + self.b.value\n\n\nclass BatchNorm(Module):\n\n  def __init__(self, din: int, mu: float = 0.95):\n    self.mu = mu\n    self.scale = Param(jax.numpy.ones((din,)))\n    self.bias = Param(jax.numpy.zeros((din,)))\n    self.mean = BatchStat(jax.numpy.zeros((din,)))\n    self.var = BatchStat(jax.numpy.ones((din,)))\n\n  def __call__(self, x, train: bool) -> jax.Array:\n    if train:\n      axis = tuple(range(x.ndim - 1))\n      mean = jax.numpy.mean(x, axis=axis)\n      var = jax.numpy.var(x, axis=axis)\n      # ema update\n      self.mean.value = self.mu * self.mean.value + (1 - self.mu) * mean\n      self.var.value = self.mu * self.var.value + (1 - self.mu) * var\n    else:\n      mean, var = self.mean.value, self.var.value\n\n    scale, bias = self.scale.value, self.bias.value\n    x = (x - mean) / jax.numpy.sqrt(var + 1e-5) * scale + bias\n    return x\n\n\nclass Dropout(Module):\n\n  def __init__(self, rate: float):\n    self.rate = rate\n\n  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n    if train:\n      mask = random.bernoulli(rngs.make_rng(), (1 - self.rate), x.shape)\n      x = x * mask / (1 - self.rate)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Demonstrating ScanMLP Usage and State Extraction in Python\nDESCRIPTION: This snippet shows how to use the ScanMLP class, split its state, and extract specific types of variables (Param and BatchStat) from the state. It demonstrates the practical application of the implemented neural network components.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/tiny_nnx.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodule = ScanMLP(hidden_size=10, n_layers=5, rngs=Rngs(random.key(0)))\nx = jax.random.normal(random.key(0), (2, 10))\ny = module(x, train=True, rngs=Rngs(random.key(1)))\n\nstate, graphdef = module.split()\nprint(\"state =\", jax.tree.map(jnp.shape, state))\nprint(\"graphdef =\", graphdef)\n\n# split\nparams = state.extract(Param)\nbatch_stats = state.extract(BatchStat)\n# merge\nstate = State({**params, **batch_stats})\n\nprint(\"params =\", jax.tree.map(jnp.shape, params))\nprint(\"batch_stats =\", jax.tree.map(jnp.shape, batch_stats))\n```\n\n----------------------------------------\n\nTITLE: Performing Forward Pass with a Flax Model\nDESCRIPTION: Demonstrates how to apply a model to input data using previously initialized parameters with the apply method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel.apply(params, x)\n```\n\n----------------------------------------\n\nTITLE: ResNet Implementation with Partial Application in Flax/Linen\nDESCRIPTION: Shows how to implement ResNet architecture, transitioning from Module.partial to functools.partial for configuring submodules with default arguments.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass ResNet(nn.Module):\n  \"\"\"ResNetV1.\"\"\"\n\n  def apply(self, x,\n            stage_sizes,\n            num_filters=64,\n            train=True):\n    conv = nn.Conv.partial(bias=False)\n    norm = nn.BatchNorm.partial(\n        use_running_average=not train,\n        momentum=0.9, epsilon=1e-5)\n\n    x = conv(x, num_filters, (7, 7), (2, 2),\n            padding=[(3, 3), (3, 3)],\n            name='conv_init')\n    x = norm(x, name='bn_init')\n\n    return x\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom functools import partial\n\nclass ResNet(nn.Module):\n  \"\"\"ResNetV1.\"\"\"\n  stage_sizes: Sequence[int]\n  num_filters: int = 64\n  train: bool = True\n\n  @nn.compact\n  def __call__(self, x):\n    conv = partial(nn.Conv, use_bias=False)\n    norm = partial(nn.BatchNorm,\n                  use_running_average=not self.train,\n                  momentum=0.9, epsilon=1e-5)\n\n    x = conv(self.num_filters, (7, 7), (2, 2),\n            padding=[(3, 3), (3, 3)],\n            name='conv_init')(x)\n    x = norm(name='bn_init')(x)\n\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating Learning Rate Schedule Function in Python\nDESCRIPTION: This function creates a learning rate schedule combining linear warmup and cosine decay. It takes configuration parameters, base learning rate, and steps per epoch as inputs to generate the schedule.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/lr_schedule.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef create_learning_rate_fn(config, base_learning_rate, steps_per_epoch):\n    \"\"\"Creates learning rate schedule.\"\"\"\n    warmup_fn = optax.linear_schedule(\n        init_value=0., end_value=base_learning_rate,\n        transition_steps=config.warmup_epochs * steps_per_epoch)\n    cosine_epochs = max(config.num_epochs - config.warmup_epochs, 1)\n    cosine_fn = optax.cosine_decay_schedule(\n        init_value=base_learning_rate,\n        decay_steps=cosine_epochs * steps_per_epoch)\n    schedule_fn = optax.join_schedules(\n        schedules=[warmup_fn, cosine_fn],\n        boundaries=[config.warmup_epochs * steps_per_epoch])\n    return schedule_fn\n```\n\n----------------------------------------\n\nTITLE: State Propagation in nnx.vmap\nDESCRIPTION: Illustrates how nnx.vmap propagates state changes, using a Weights Module with a count attribute that is incremented inside the transformed function.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Count(nnx.Variable): pass\n\nclass Weights(nnx.Module):\n  def __init__(self, kernel: jax.Array, bias: jax.Array, count: jax.Array):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n    self.count = Count(count)\n\nweights = Weights(\n  kernel=random.uniform(random.key(0), (10, 2, 3)),\n  bias=jnp.zeros((10, 3)),\n  count=jnp.arange(10),\n)\nx = jax.random.normal(random.key(1), (10, 2))\n\ndef stateful_vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  weights.count += 1\n  return x @ weights.kernel + weights.bias\n\n\ny = nnx.vmap(stateful_vector_dot, in_axes=0, out_axes=1)(weights, x)\n\nweights.count\n```\n\n----------------------------------------\n\nTITLE: Defining Training and Evaluation Steps\nDESCRIPTION: Implements the training and evaluation step functions with loss calculation, gradient computation, and metric updates.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(model: CNN, batch):\n  logits = model(batch['image'])\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n    logits=logits, labels=batch['label']\n  ).mean()\n  return loss, logits\n\n@nnx.jit\ndef train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n  (loss, logits), grads = grad_fn(model, batch)\n  metrics.update(loss=loss, logits=logits, labels=batch['label'])\n  optimizer.update(grads)\n\n@nnx.jit\ndef eval_step(model: CNN, metrics: nnx.MultiMetric, batch):\n  loss, logits = loss_fn(model, batch)\n  metrics.update(loss=loss, logits=logits, labels=batch['label'])\n```\n\n----------------------------------------\n\nTITLE: Performing Checkpoint Surgery in Flax NNX\nDESCRIPTION: Demonstrates how to manipulate checkpoint data to fit with modified model code, including saving, loading, and renaming parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Save a version of model into a checkpoint\ncheckpointer = orbax.PyTreeCheckpointer()\nold_model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\ncheckpointer.save(f'/tmp/nnx-surgery-state', nnx.state(model), force=True)\n\nclass ModifiedTwoLayerMLP(nnx.Module):\n  def __init__(self, dim, rngs: nnx.Rngs):\n    self.layer1 = nnx.Linear(dim, dim, rngs=rngs)  # no longer linear1!\n    self.layer2 = nnx.Linear(dim, dim, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.layer1(x)\n    return self.layer2(x)\n\nabs_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\ntry:\n  with_item = checkpointer.restore('/tmp/nnx-surgery-state', item=nnx.state(abs_model))\n  print(with_item)\nexcept Exception as e:\n  print(f'This will throw error: {type(e)}: {e}')\n\ndef process_raw_dict(raw_state_dict):\n  flattened = nnx.traversals.flatten_mapping(raw_state_dict)\n  # Cut the '.value' postfix on every leaf path.\n  flattened = {(path[:-1] if path[-1] == 'value' else path): value\n               for path, value in flattened.items()}\n  return nnx.traversals.unflatten_mapping(flattened)\n\n# Make your local change on the checkpoint dictionary.\nraw_dict = checkpointer.restore('/tmp/nnx-surgery-state')\npprint(raw_dict)\nraw_dict['layer1'] = raw_dict.pop('linear1')\nraw_dict['layer2'] = raw_dict.pop('linear2')\n\n# Fit it into the model state.\nabs_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\ngraph_def, state = nnx.split(abs_model)\nnnx.replace_by_pure_dict(state, process_raw_dict(raw_dict))\nrestored_model = nnx.merge(graph_def, state)\n\nnp.testing.assert_allclose(restored_model(jnp.ones((3, 4))), old_model(jnp.ones((3, 4))))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Dense Layer\nDESCRIPTION: Custom implementation of Dense layer showing parameter declaration using self.param() with initialization functions and shape inference.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleDense(nn.Module):\n  features: int\n  kernel_init: Callable = nn.initializers.lecun_normal()\n  bias_init: Callable = nn.initializers.zeros_init()\n\n  @nn.compact\n  def __call__(self, inputs):\n    kernel = self.param('kernel',\n                        self.kernel_init,\n                        (inputs.shape[-1], self.features))\n    y = lax.dot_general(inputs, kernel,\n                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n    bias = self.param('bias', self.bias_init, (self.features,))\n    y = y + bias\n    return y\n```\n\n----------------------------------------\n\nTITLE: Restoring Flax NNX Checkpoint\nDESCRIPTION: Example showing how to restore a checkpoint back to its NNX.State structure using abstract models. The code creates an abstract model reference, displays the abstract state structure, and then restores the actual checkpoint using Orbax checkpointer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Restore the checkpoint back to its `nnx.State` structure - need an abstract reference.\nabstract_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(0)))\ngraphdef, abstract_state = nnx.split(abstract_model)\nprint('The abstract NNX state (all leaves are abstract arrays):')\nnnx.display(abstract_state)\n\nstate_restored = checkpointer.restore(ckpt_dir / 'state', abstract_state)\njax.tree.map(np.testing.assert_array_equal, state, state_restored)\nprint('NNX State restored: ')\nnnx.display(state_restored)\n```\n\n----------------------------------------\n\nTITLE: Initializing Flax Model and TrainState\nDESCRIPTION: This function initializes a Flax model and creates a TrainState object, which includes the model parameters and optimizer state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef init_fn(k, x, model, optimizer):\n  variables = model.init(k, x) # Initialize the model.\n  state = train_state.TrainState.create( # Create a `TrainState`.\n    apply_fn=model.apply,\n    params=variables['params'],\n    tx=optimizer)\n  return state\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Flax Model Parameters\nDESCRIPTION: Shows how to save and load model parameters using Flax's serialization module, converting parameters to bytes or dictionary format.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import serialization\nbytes_output = serialization.to_bytes(params)\ndict_output = serialization.to_state_dict(params)\nprint('Dict output')\nprint(dict_output)\nprint('Bytes output')\nprint(bytes_output)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parallelized Inference with JAX pmap\nDESCRIPTION: Defines a JAX-parallelized inference function to efficiently process batches of images across multiple devices. Also creates an evaluation data iterator configured with the appropriate batch size and image dimensions.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define parallelized inference function in separate cell so the cached\n# compilation can be used if below cell is executed multiple times.\n@jax.pmap\ndef p_get_logits(images):\n  return model.apply({'params': state.params, 'batch_stats': state.batch_stats},\n                     images, train=False)\n\neval_iter = train.create_input_iter(dataset_builder, config.batch_size,\n                                    input_pipeline.IMAGE_SIZE, tf.float32,\n                                    train=False, cache=False, shuffle_buffer_size=None,\n                                    prefetch=1)\n```\n\n----------------------------------------\n\nTITLE: Model Initialization and Usage Example\nDESCRIPTION: Demonstrates how to initialize and use a Linear layer module with display functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = Linear(2, 5, rngs=nnx.Rngs(params=0))\ny = model(x=jnp.ones((1, 2)))\n\nprint(y)\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Modified TrainState for BatchNorm Support\nDESCRIPTION: Extends the Flax TrainState class to include batch_stats for BatchNorm implementation, showing how to create and initialize the training state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/batch_norm.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(train_state.TrainState):\n  batch_stats: Any\n\nstate = TrainState.create(\n  apply_fn=mlp.apply,\n  params=params,\n  batch_stats=batch_stats,\n  tx=optax.adam(1e-3))\n```\n\n----------------------------------------\n\nTITLE: Using sow() to Store Intermediate Values in Flax\nDESCRIPTION: Demonstrates how to use the sow() method to store intermediate values in a new variable collection within a Flax module. This allows for optional tracking of intermediates without affecting normal operation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass SowCNN(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    self.sow('intermediates', 'features', x)\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    x = nn.log_softmax(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Layer Module\nDESCRIPTION: Creates a basic linear layer as an nnx.Module with weight and bias parameters. Shows parameter initialization using PRNG keys.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Linear(nnx.Module):\n  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n    key = rngs.params()\n    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n    self.b = nnx.Param(jnp.zeros((dout,)))\n    self.din, self.dout = din, dout\n\n  def __call__(self, x: jax.Array):\n    return x @ self.w + self.b\n```\n\n----------------------------------------\n\nTITLE: Splitting State using Flax NNX Filters\nDESCRIPTION: Demonstrates how to split model state into multiple nnx.State objects using nnx.Variable type filters. This is useful for separating parameters and counts into distinct states for different handling.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Use `nnx.Variable` type `Filter`s to split into multiple `nnx.State`s.\ngraphdef, params, counts = nnx.split(model, nnx.Param, Count)\n\nnnx.display(params, counts)\n```\n\n----------------------------------------\n\nTITLE: Implementing Module with Multiple Independent RNG Streams\nDESCRIPTION: Creates a Flax Module with two independent RNG streams, demonstrating that streams maintain their own state regardless of call order.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass RNGModuleTwoStreams(nn.Module):\n  @nn.compact\n  def __call__(self):\n    # same value as first code snippet above\n    print(f\"rng_stream1: {self.make_rng('rng_stream1')}\")\n    # same value as second code snippet above\n    print(f\"rng_stream2: {self.make_rng('rng_stream2')}\")\n    # same value as first code snippet above\n    print(f\"rng_stream1: {self.make_rng('rng_stream1')}\")\n    # same value as second code snippet above\n    print(f\"rng_stream2: {self.make_rng('rng_stream2')}\")\n    # same value as first code snippet above\n    print(f\"rng_stream1: {self.make_rng('rng_stream1')}\")\n    # same value as second code snippet above\n    print(f\"rng_stream2: {self.make_rng('rng_stream2')}\")\n\nrng_module_two_streams = RNGModuleTwoStreams()\nvariables = rng_module_two_streams.init(\n  {'rng_stream1': jax.random.key(0), 'rng_stream2': jax.random.key(1)}\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing CNN in Flax\nDESCRIPTION: Defines a convolutional neural network (CNN) class using Flax's nnx module. The CNN includes convolutional layers, average pooling, and linear layers with ReLU activation.\nSOURCE: https://github.com/google/flax/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n    self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n    self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n    self.linear1 = nnx.Linear(3136, 256, rngs=rngs)\n    self.linear2 = nnx.Linear(256, 10, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.avg_pool(nnx.relu(self.conv1(x)))\n    x = self.avg_pool(nnx.relu(self.conv2(x)))\n    x = x.reshape(x.shape[0], -1)  # flatten\n    x = nnx.relu(self.linear1(x))\n    x = self.linear2(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Vectorized Mapping (vmap) in Flax for Multi-Head Attention\nDESCRIPTION: Demonstrates how to use JAX's vectorized mapping (vmap) to create a multi-head attention module from a single-head implementation. This approach efficiently vectorizes operations across multiple attention heads and batch dimensions.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass RawDotProductAttention(nn.Module):\n  attn_dropout_rate: float = 0.1\n  train: bool = False\n\n  @nn.compact\n  def __call__(self, query, key, value, bias=None, dtype=jnp.float32):\n    assert key.ndim == query.ndim\n    assert key.ndim == value.ndim\n\n    n = query.ndim\n    attn_weights = lax.dot_general(\n        query, key,\n        (((n-1,), (n - 1,)), ((), ())))\n    if bias is not None:\n      attn_weights += bias\n    norm_dims = tuple(range(attn_weights.ndim // 2, attn_weights.ndim))\n    attn_weights = jax.nn.softmax(attn_weights, axis=norm_dims)\n    attn_weights = nn.Dropout(self.attn_dropout_rate)(attn_weights,\n                                                      deterministic=not self.train)\n    attn_weights = attn_weights.astype(dtype)\n\n    contract_dims = (\n        tuple(range(n - 1, attn_weights.ndim)),\n        tuple(range(0, n  - 1)))\n    y = lax.dot_general(\n        attn_weights, value,\n        (contract_dims, ((), ())))\n    return y\n\nclass DotProductAttention(nn.Module):\n  qkv_features: Optional[int] = None\n  out_features: Optional[int] = None\n  train: bool = False\n\n  @nn.compact\n  def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):\n    qkv_features = self.qkv_features or inputs_q.shape[-1]\n    out_features = self.out_features or inputs_q.shape[-1]\n\n    QKVDense = functools.partial(\n      nn.Dense, features=qkv_features, use_bias=False, dtype=dtype)\n    query = QKVDense(name='query')(inputs_q)\n    key = QKVDense(name='key')(inputs_kv)\n    value = QKVDense(name='value')(inputs_kv)\n\n    y = RawDotProductAttention(train=self.train)(\n        query, key, value, bias=bias, dtype=dtype)\n\n    y = nn.Dense(features=out_features, dtype=dtype, name='out')(y)\n    return y\n\nclass MultiHeadDotProductAttention(nn.Module):\n  qkv_features: Optional[int] = None\n  out_features: Optional[int] = None\n  batch_axes: Sequence[int] = (0,)\n  num_heads: int = 1\n  broadcast_dropout: bool = False\n  train: bool = False\n  @nn.compact\n  def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):\n    qkv_features = self.qkv_features or inputs_q.shape[-1]\n    out_features = self.out_features or inputs_q.shape[-1]\n\n    # Make multiheaded attention from single-headed dimension.\n    Attn = nn.vmap(DotProductAttention,\n                   in_axes=(None, None, None),\n                   out_axes=2,\n                   axis_size=self.num_heads,\n                   variable_axes={'params': 0},\n                   split_rngs={'params': True,\n                               'dropout': not self.broadcast_dropout})\n\n    # Vmap across batch dimensions.\n    for axis in reversed(sorted(self.batch_axes)):\n      Attn = nn.vmap(Attn,\n                     in_axes=(axis, axis, axis),\n                     out_axes=axis,\n                     variable_axes={'params': None},\n                     split_rngs={'params': False, 'dropout': False})\n\n    # Run the vmap'd class on inputs.\n    y = Attn(qkv_features=qkv_features // self.num_heads,\n             out_features=out_features,\n             train=self.train,\n             name='attention')(inputs_q, inputs_kv, bias)\n\n    return y.mean(axis=-2)\n\n\nkey1, key2, key3, key4 = random.split(random.key(0), 4)\nx = random.uniform(key1, (3, 13, 64))\n\nmodel = functools.partial(\n  MultiHeadDotProductAttention,\n  broadcast_dropout=False,\n  num_heads=2,\n  batch_axes=(0,))\n\ninit_variables = model(train=False).init({'params': key2}, x, x)\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n\ny = model(train=True).apply(init_variables, x, x, rngs={'dropout': key4})\nprint('output:\\n', y.shape)\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Parallel Dropout with nnx.pmap in Flax\nDESCRIPTION: This code demonstrates how to use nnx.pmap with a Model containing Dropout layers by properly splitting the random state. It uses StateAxes to parallelize the dropout PRNG key stream while replicating other state, and split_rngs to ensure each device gets a unique key.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(nnx.Rngs(params=0, dropout=1))\n\nnum_devices = jax.local_device_count()\nx = jnp.ones((num_devices, 16, 20))\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=num_devices, only='dropout')\n@nnx.pmap(in_axes=(state_axes, 0), out_axes=0)\ndef forward(model: Model, x: jnp.ndarray):\n  return model(x)\n\ny = forward(model, x)\nprint(y.shape)\n```\n\n----------------------------------------\n\nTITLE: Restoring and Evaluating a Saved Model in Python using Flax and JAX\nDESCRIPTION: This code demonstrates how to restore a saved model from a checkpoint directory, convert it to a pure dictionary, reconstruct the model using Flax and JAX utilities, and verify its functionality. It uses the TwoLayerMLP model and various nnx functions for model manipulation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Restore as a pure dictionary.\nrestored_pure_dict = checkpointer.restore(ckpt_dir / 'pure_dict')\nabstract_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(0)))\ngraphdef, abstract_state = nnx.split(abstract_model)\nnnx.replace_by_pure_dict(abstract_state, restored_pure_dict)\nmodel = nnx.merge(graphdef, abstract_state)\nassert model(x).shape == (3, 4)  # The model still works!\n```\n\n----------------------------------------\n\nTITLE: Compiling Flax Model Inference Step with JAX\nDESCRIPTION: This function defines and compiles an inference step for a Flax model using JAX, with appropriate input and output shardings.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),\n                   out_shardings=x_sharding)\ndef apply_fn(state, x):\n  return state.apply_fn({'params': state.params}, x)\n\nwith mesh:\n  y = apply_fn(new_state, x)\nprint(type(y))\nprint(y.dtype)\nprint(y.shape)\njax.debug.visualize_array_sharding(y)\n```\n\n----------------------------------------\n\nTITLE: Defining Explicit MLP Module\nDESCRIPTION: Implementation of a Multi-Layer Perceptron using explicit submodule declarations in setup(). Shows how to compose multiple Dense layers with ReLU activations.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ExplicitMLP(nn.Module):\n  features: Sequence[int]\n\n  def setup(self):\n    self.layers = [nn.Dense(feat) for feat in self.features]\n\n  def __call__(self, inputs):\n    x = inputs\n    for i, lyr in enumerate(self.layers):\n      x = lyr(x)\n      if i != len(self.layers) - 1:\n        x = nn.relu(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Combined Mutable Variables and RNGs in Flax Block Module\nDESCRIPTION: Shows an example that combines differentiable parameters, stochastic layers (Dropout with RNGs), and mutable batch statistics in a neural network block. Demonstrates the differences between training and evaluation modes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nn.Module):\n  features: int\n  training: bool\n  @nn.compact\n  def __call__(self, inputs):\n    x = nn.Dense(self.features)(inputs)\n    x = nn.Dropout(rate=0.5)(x, deterministic=not self.training)\n    x = nn.BatchNorm(use_running_average=not self.training)(x)\n    return x\n\nkey1, key2, key3, key4 = random.split(random.key(0), 4)\nx = random.uniform(key1, (3,4,4))\n\nmodel = Block(features=3, training=True)\n\ninit_variables = model.init({'params': key2, 'dropout': key3}, x)\n_, init_params = flax.core.pop(init_variables, 'params')\n\n# When calling `apply` with mutable kinds, returns a pair of output,\n# mutated_variables.\ny, mutated_variables = model.apply(\n    init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n\n# Now we reassemble the full variables from the updates (in a real training\n# loop, with the updated params from an optimizer).\nupdated_variables = flax.core.freeze(dict(params=init_params,\n                                          **mutated_variables))\n\nprint('updated variables:\\n', updated_variables)\nprint('initialized variable shapes:\\n',\n      jax.tree_util.tree_map(jnp.shape, init_variables))\nprint('output:\\n', y)\n\n# Let's run these model variables during \"evaluation\":\neval_model = Block(features=3, training=False)\ny = eval_model.apply(updated_variables, x)  # Nothing mutable; single return value.\nprint('eval output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Saving Flax NNX Model State as a Checkpoint\nDESCRIPTION: This code snippet demonstrates how to split a Flax NNX model, obtain its state, and save it as a checkpoint using the Orbax checkpointing library. It first uses nnx.split to separate the model and its state, then displays the state, and finally saves it using a StandardCheckpointer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_, state = nnx.split(model)\nnnx.display(state)\n\ncheckpointer = ocp.StandardCheckpointer()\ncheckpointer.save(ckpt_dir / 'state', state)\n```\n\n----------------------------------------\n\nTITLE: Defining an Explicit MLP Module in Flax\nDESCRIPTION: This code defines an MLP (Multi-Layer Perceptron) module using explicit submodule declarations in the setup method of a Flax Linen module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ExplicitMLP(nn.Module):\n  features: Sequence[int]\n\n  def setup(self):\n    # we automatically know what to do with lists, dicts of submodules\n    self.layers = [nn.Dense(feat) for feat in self.features]\n    # for single submodules, we would just write:\n    # self.layer1 = nn.Dense(feat1)\n\n  def __call__(self, inputs):\n    x = inputs\n    for i, lyr in enumerate(self.layers):\n      x = lyr(x)\n      if i != len(self.layers) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = ExplicitMLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Defining and instantiating a TwoLayerMLP model with Flax NNX\nDESCRIPTION: Creates a TwoLayerMLP class by subclassing nnx.Module, initializes the model, and demonstrates its usage with random input data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass TwoLayerMLP(nnx.Module):\n  def __init__(self, dim, rngs: nnx.Rngs):\n    self.linear1 = nnx.Linear(dim, dim, rngs=rngs, use_bias=False)\n    self.linear2 = nnx.Linear(dim, dim, rngs=rngs, use_bias=False)\n\n  def __call__(self, x):\n    x = self.linear1(x)\n    return self.linear2(x)\n\n# Instantiate the model and show we can run it.\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\nx = jax.random.normal(jax.random.key(42), (3, 4))\nassert model(x).shape == (3, 4)\n```\n\n----------------------------------------\n\nTITLE: Implementing Residual Model with Dropout in Flax\nDESCRIPTION: Demonstrates how to implement a residual model using Flax's linen Module with dropout, showing how to handle the deterministic flag as a constructor parameter. The example uses partial application to create a dropout template that can be passed to sub-modules.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/arguments.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom flax import linen as nn\n\nclass ResidualModel(nn.Module):\n  drop_rate: float\n\n  @nn.compact\n  def __call__(self, x, *, train):\n    dropout = partial(nn.Dropout, rate=self.drop_rate, deterministic=not train)\n    for i in range(10):\n      x += ResidualBlock(dropout=dropout, ...)(x)\n```\n\n----------------------------------------\n\nTITLE: Compact Multi-Layer Perceptron Implementation in Flax\nDESCRIPTION: This snippet shows an alternative way to implement a multi-layer perceptron using the @nn.compact decorator. It demonstrates how to declare submodules inline within the __call__ method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleMLP(nn.Module):\n  features: Sequence[int]\n\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      x = nn.Dense(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n      # providing a name is optional though!\n      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n    return x\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = SimpleMLP(features=[3,4,5])\nparams = model.init(key2, x)\ny = model.apply(params, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Implementing LSTM Scan in Flax\nDESCRIPTION: Shows how to use Flax's scan transformation to create a simple LSTM implementation. The code demonstrates variable broadcasting and RNG handling across scan steps.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleScan(nn.Module):\n  features: int\n\n  @nn.compact\n  def __call__(self, xs):\n    LSTM = nn.scan(nn.LSTMCell,\n                   in_axes=1, out_axes=1,\n                   variable_broadcast='params',\n                   split_rngs={'params': False})\n    lstm = LSTM(self.features, name=\"lstm_cell\")\n\n    dummy_rng = random.key(0)\n    input_shape = xs[:, 0].shape\n    init_carry = lstm.initialize_carry(dummy_rng, input_shape)\n\n    return lstm(init_carry, xs)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Updating Counter Variable in Flax Module\nDESCRIPTION: This snippet demonstrates how to create a Counter module in Flax that initializes and updates a mutable variable. It shows the use of self.variable for creating mutable state and how to access and modify its value.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(nn.Module):\n  @nn.compact\n  def __call__(self):\n    # easy pattern to detect if we're initializing\n    is_initialized = self.has_variable('counter', 'count')\n    counter = self.variable('counter', 'count', lambda: jnp.zeros((), jnp.int32))\n    if is_initialized:\n      counter.value += 1\n    return counter.value\n\n\nkey1 = random.key(0)\n\nmodel = Counter()\ninit_variables = model.init(key1)\nprint('initialized variables:\\n', init_variables)\n\ny, mutated_variables = model.apply(init_variables, mutable=['counter'])\n\nprint('mutated variables:\\n', mutated_variables)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with Linen Module in Flax\nDESCRIPTION: Creates a Linen Dense model, initializes parameters, creates a TrainState, and defines a jitted training step function. The training step calculates loss using softmax cross-entropy and updates model parameters with Adam optimizer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.training import train_state\n\nsample_x = jnp.ones((1, 784))\nmodel = nn.Dense(features=10)\nparams = model.init(jax.random.key(0), sample_x)['params']\n\n\n\n\nstate = train_state.TrainState.create(\n  apply_fn=model.apply,\n  params=params,\n\n  tx=optax.adam(1e-3)\n)\n\n@jax.jit\ndef train_step(key, state, inputs, labels):\n  def loss_fn(params):\n    logits = state.apply_fn(\n      {'params': params},\n      inputs, # <== inputs\n      rngs={'dropout': key}\n    )\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(state.params)\n\n\n  state = state.apply_gradients(grads=grads)\n\n  return state\n```\n\n----------------------------------------\n\nTITLE: Filtering Random State\nDESCRIPTION: Example of filtering random state using various filters to select different substates of Rngs inside a Model.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(nnx.Rngs(params=0, dropout=1))\n\nrng_state = nnx.state(model, nnx.RngState) # All random states.\nkey_state = nnx.state(model, nnx.RngKey) # Only PRNG keys.\ncount_state = nnx.state(model, nnx.RngCount) # Only counts.\nrng_params_state = nnx.state(model, 'params') # Only `params`.\nrng_dropout_state = nnx.state(model, 'dropout') # Only `dropout`.\nparams_key_state = nnx.state(model, nnx.All('params', nnx.RngKey)) # `Params` PRNG keys.\n\nnnx.display(params_key_state)\n```\n\n----------------------------------------\n\nTITLE: Implementing DotReluDot Layer with Sharding Annotations\nDESCRIPTION: Defines a custom neural network layer that demonstrates how to use Flax's partitioning annotations. The layer creates sharded weight matrices and applies sharding constraints to intermediate computations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DotReluDot(nn.Module):\n  depth: int\n  dense_init: Callable = nn.initializers.xavier_normal()\n  @nn.compact\n  def __call__(self, x):\n\n    y = nn.Dense(self.depth,\n                 kernel_init=nn.with_partitioning(self.dense_init, (None, 'model')),\n                 use_bias=False,  # or overwrite with `bias_init`\n                 )(x)\n\n    y = jax.nn.relu(y)\n    # Force a local sharding annotation.\n    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))\n\n    W2 = self.param(\n        'W2',\n        nn.with_partitioning(self.dense_init, ('model', None)),\n        (self.depth, x.shape[-1]))\n\n    z = jnp.dot(y, W2)\n    # Force a local sharding annotation.\n    z = with_sharding_constraint(z, mesh_sharding(PartitionSpec('data', None)))\n\n    # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.\n    return z, None\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoEncoder in Haiku and Flax NNX\nDESCRIPTION: Shows how to implement an autoencoder with encode and decode methods in both frameworks. Demonstrates differences in method handling and initialization approaches.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass AutoEncoder(hk.Module):\n\n  def __init__(self, embed_dim: int, output_dim: int, name=None):\n    super().__init__(name=name)\n    self.encoder = hk.Linear(embed_dim, name=\"encoder\")\n    self.decoder = hk.Linear(output_dim, name=\"decoder\")\n\n  def encode(self, x):\n    return self.encoder(x)\n\n  def decode(self, x):\n    return self.decoder(x)\n\n  def __call__(self, x):\n    x = self.encode(x)\n    x = self.decode(x)\n    return x\n\ndef forward():\n  module = AutoEncoder(256, 784)\n  init = lambda x: module(x)\n  return init, (module.encode, module.decode)\n\nmodel = hk.multi_transform(forward)\nparams = model.init(jax.random.key(0), x=jnp.ones((1, 784)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Clipping with Optax vs Manual Implementation\nDESCRIPTION: Comparison between manual gradient norm clipping in flax.optim and using optax.clip_by_global_norm(). Optax simplifies this process by providing a ready-to-use transformation for gradient clipping.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(optimizer, batch):\n  grads = jax.grad(loss)(optimizer.target, batch)\n  grads_flat, _ = jax.tree_util.tree_flatten(grads)\n  global_l2 = jnp.sqrt(sum([jnp.vdot(p, p) for p in grads_flat]))\n  g_factor = jnp.minimum(1.0, grad_clip_norm / global_l2)\n  grads = jax.tree_util.tree_map(lambda g: g * g_factor, grads)\n  return optimizer.apply_gradient(grads)\n```\n\nLANGUAGE: python\nCODE:\n```\ntx = optax.chain(\n    optax.clip_by_global_norm(grad_clip_norm),\n    optax.trace(decay=momentum),\n    optax.scale(-learning_rate),\n)\n```\n\n----------------------------------------\n\nTITLE: Reseeding Random State\nDESCRIPTION: Example of reseeding random state to achieve reproducible results with dropout layer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(nnx.Rngs(params=0, dropout=1))\nx = jnp.ones((1, 20))\n\ny1 = model(x)\ny2 = model(x)\n\nnnx.reseed(model, dropout=1) # reset dropout RngState\ny3 = model(x)\n\nassert not jnp.allclose(y1, y2) # different\nassert jnp.allclose(y1, y3)     # same\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Classifier Module\nDESCRIPTION: Creates a Flax Module that combines a pretrained backbone with a classification head for transfer learning.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable\nimport jax.numpy as jnp\nimport jax\n\nclass Classifier(nn.Module):\n  num_classes: int\n  backbone: nn.Module\n  \n\n  @nn.compact\n  def __call__(self, x):\n    x = self.backbone(x).pooler_output\n    x = nn.Dense(\n      self.num_classes, name='head', kernel_init=nn.zeros)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Modified Optimizer State\nDESCRIPTION: Demonstrating how to reconstruct the optimizer state after modifying its components. The modified momentum and variance terms are unflattened and used to create a new optimizer state with the same structure.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nopt_state = (\n    opt_state[0]._replace(\n        mu=traverse_util.unflatten_dict(flat_mu, sep='/'),\n        nu=traverse_util.unflatten_dict(flat_nu, sep='/'),\n    ),\n) + opt_state[1:]\njax.tree_util.tree_map(jnp.shape, opt_state)\n```\n\n----------------------------------------\n\nTITLE: Defining Training State and Metrics - Python\nDESCRIPTION: Implements custom TrainState class with metrics and initialization function using Flax's training utilities.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@struct.dataclass\nclass Metrics(metrics.Collection):\n  accuracy: metrics.Accuracy\n  loss: metrics.Average.from_output('loss')\n\nclass TrainState(train_state.TrainState):\n  metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, momentum):\n  \"\"\"Creates an initial `TrainState`.\"\"\"\n  params = module.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty())\n```\n\n----------------------------------------\n\nTITLE: Merging and Updating Multiple States in Flax NNX\nDESCRIPTION: Shows how to merge multiple State objects back into a model and update a model with multiple States. This is the complement to the split operation for state management.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Merge multiple `State`s\nmodel = nnx.merge(graphdef, params, counts)\n# Update with multiple `State`s\nnnx.update(model, params, counts)\n```\n\n----------------------------------------\n\nTITLE: Custom Dense Layer Implementation in Flax\nDESCRIPTION: This snippet demonstrates how to create a custom dense layer in Flax, showcasing parameter declaration and initialization using the self.param method within the @nn.compact decorator.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleDense(nn.Module):\n  features: int\n  kernel_init: Callable = nn.initializers.lecun_normal()\n  bias_init: Callable = nn.initializers.zeros_init()\n\n  @nn.compact\n  def __call__(self, inputs):\n    kernel = self.param('kernel',\n                        self.kernel_init, # Initialization function\n                        (inputs.shape[-1], self.features))  # shape info.\n    y = jnp.dot(inputs, kernel)\n    bias = self.param('bias', self.bias_init, (self.features,))\n    y = y + bias\n    return y\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = SimpleDense(features=3)\nparams = model.init(key2, x)\ny = model.apply(params, x)\n\nprint('initialized parameters:\\n', params)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: RNN Class Definition in Flax\nDESCRIPTION: Definition of the RNN class, which inherits from RNNBase and applies an RNNCellBase instance over a batch of input sequences. This class is the main implementation of the recurrent layer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass RNN(RNNBase):\n  cell: RNNCellBase,\n  cell_size: int | Tuple[int, ...]\n  time_axis: int = -2,\n  variable_axes = FrozenDict(),\n  variable_broadcast: CollectionFilter = 'params'\n  variable_carry: CollectionFilter = False\n  split_rngs = FrozenDict({'params': False})\n  # implement RNNBase\n  ...\n```\n\n----------------------------------------\n\nTITLE: Using Orbax CheckpointManager for Versioned Checkpoints\nDESCRIPTION: This code shows how to use Orbax's CheckpointManager for versioning and automatic bookkeeping of checkpoints. It demonstrates setting options like max_to_keep and simulates a training loop with multiple checkpoints.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_manager = orbax.checkpoint.CheckpointManager(\n    '/tmp/flax_ckpt/orbax/managed', orbax_checkpointer, options)\n\n# Inside a training loop\nfor step in range(5):\n    # ... do your training\n    checkpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\n\nos.listdir('/tmp/flax_ckpt/orbax/managed')  # Because max_to_keep=2, only step 3 and 4 are retained\n```\n\n----------------------------------------\n\nTITLE: Functional Training Loop with JAX JIT\nDESCRIPTION: Implementation of a functional training approach using jax.jit with nnx.split and nnx.merge to stage out traversal logic. Separates graph definition and state for improved performance.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/performance.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# split before training loop\ngraphdef, state = nnx.split((model, optimizer, metrics))\n\n@jax.jit  # regular JAX\ndef jax_train_step(graphdef, state, x, y):\n  # merge at the beginning of the function\n  model, optimizer, metrics = nnx.merge(graphdef, state)\n\n  def loss_fn(model):\n    y_pred = model(x)  # call methods directly\n    return ((y_pred - y) ** 2).mean()\n\n  loss, grads = nnx.value_and_grad(loss_fn)(model)\n  optimizer.update(grads)\n  metrics.update(loss=loss)\n\n  state = nnx.state((model, optimizer, metrics))\n  return loss, state\n\nfor _ in range(10):\n  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n  state, loss = jax_train_step(graphdef, state, x, y)\n\n# update objects after training\nnnx.update((model, optimizer, metrics), state)\n```\n\n----------------------------------------\n\nTITLE: Modified Create Train State Function with Learning Rate Schedule in Python\nDESCRIPTION: This updated create_train_state function incorporates the learning rate schedule. It takes learning_rate_fn as an additional parameter and uses it to initialize the optimizer.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/lr_schedule.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef create_train_state(rng, config, learning_rate_fn):\n    \"\"\"Creates initial `TrainState`.\"\"\"\n    cnn = CNN()\n    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n    tx = optax.sgd(learning_rate_fn, config.momentum)\n    return train_state.TrainState.create(\n        apply_fn=cnn.apply, params=params, tx=tx)\n```\n\n----------------------------------------\n\nTITLE: Simplified Training Step Without Mutable State\nDESCRIPTION: A simplified version of the training step that doesn't handle mutable state, showing the basic gradient computation and state update pattern.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state, inputs, labels):\n\n  def loss_fn(params):\n    outputs = state.apply_fn({'params': params}, inputs)\n    loss = xent_loss(outputs, labels)\n    return loss\n\n  loss, grads = jax.value_and_grad(loss_fn)(state.params)\n  new_state = state.update(grads=grads)\n\n  return new_state, loss\n```\n\n----------------------------------------\n\nTITLE: Implementing Autoencoder in Flax\nDESCRIPTION: Defines an autoencoder class using Flax's nnx module. The autoencoder consists of an encoder and decoder, both implemented as linear layers.\nSOURCE: https://github.com/google/flax/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nEncoder = lambda rngs: nnx.Linear(2, 10, rngs=rngs)\nDecoder = lambda rngs: nnx.Linear(10, 2, rngs=rngs)\n\nclass AutoEncoder(nnx.Module):\n  def __init__(self, rngs):\n    self.encoder = Encoder(rngs)\n    self.decoder = Decoder(rngs)\n\n  def __call__(self, x) -> jax.Array:\n    return self.decoder(self.encoder(x))\n\n  def encode(self, x) -> jax.Array:\n    return self.encoder(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing RNN Cell in Haiku and Flax\nDESCRIPTION: Defines an RNNCell class with a single step logic for an RNN in both Haiku and Flax. Includes __call__ and initial_state methods.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nclass RNNCell(hk.Module):\n  def __init__(self, hidden_size: int, name=None):\n    super().__init__(name=name)\n    self.hidden_size = hidden_size\n\n  def __call__(self, carry, x):\n    x = jnp.concatenate([carry, x], axis=-1)\n    x = hk.Linear(self.hidden_size)(x)\n    x = jax.nn.relu(x)\n    return x, x\n\n  def initial_state(self, batch_size: int):\n    return jnp.zeros((batch_size, self.hidden_size))\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass RNNCell(nn.Module):\n  hidden_size: int\n\n  @nn.compact\n  def __call__(self, carry, x):\n    x = jnp.concatenate([carry, x], axis=-1)\n    x = nn.Dense(self.hidden_size)(x)\n    x = jax.nn.relu(x)\n    return x, x\n\n  def initial_state(self, batch_size: int):\n    return jnp.zeros((batch_size, self.hidden_size))\n```\n\n----------------------------------------\n\nTITLE: Optimized Training with Cached Partial\nDESCRIPTION: Optimization technique using nnx.cached_partial to cache graph traversals and improve performance. Creates a cached version of the training step that only expects input data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/performance.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncached_train_step = nnx.cached_partial(train_step, model, optimizer, metrics)\n\nfor _ in range(10):\n  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n  loss = cached_train_step(x, y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with NNX Module in Flax\nDESCRIPTION: Creates an NNX Linear model, sets it to training mode, splits the model into graphdef, parameters and other variables, creates a custom TrainState, and defines a jitted training step function that updates both parameters and variables.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.training import train_state\n\nmodel = nnx.Linear(784, 10, rngs=nnx.Rngs(0))\nmodel.train() # set deterministic=False\ngraphdef, params, other_variables = nnx.split(model, nnx.Param, ...)\n\nclass TrainState(train_state.TrainState):\n  other_variables: nnx.State\n\nstate = TrainState.create(\n  apply_fn=graphdef.apply,\n  params=params,\n  other_variables=other_variables,\n  tx=optax.adam(1e-3)\n)\n\n@jax.jit\ndef train_step(state, inputs, labels):\n  def loss_fn(params, other_variables):\n    logits, (graphdef, new_state) = state.apply_fn(\n      params,\n      other_variables\n\n    )(inputs) # <== inputs\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(state.params, state.other_variables)\n\n\n  state = state.apply_gradients(grads=grads)\n\n  return state\n```\n\n----------------------------------------\n\nTITLE: Creating and Mutating Variables in Flax Counter Module\nDESCRIPTION: Demonstrates how to declare mutable variables in Flax, differentiate between initialization and updates, and retrieve their values. The Counter module tracks a count variable that increments with each call.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(nn.Module):\n  @nn.compact\n  def __call__(self):\n    # easy pattern to detect if we're initializing\n    is_initialized = self.has_variable('counter', 'count')\n    counter = self.variable('counter', 'count', lambda: jnp.zeros((), jnp.int32))\n    if is_initialized:\n      counter.value += 1\n    return counter.value\n\n\nkey1 = random.key(0)\n\nmodel = Counter()\ninit_variables = model.init(key1)\nprint('initialized variables:\\n', init_variables)\n\ny, mutated_variables = model.apply(init_variables, mutable=['counter'])\n\nprint('mutated variables:\\n', mutated_variables)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Implementing LoRA Model Surgery\nDESCRIPTION: Shows how to modify an existing model by replacing Linear layers with LoRA-augmented versions.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass LoraParam(nnx.Param): pass\n\nclass LoraLinear(nnx.Module):\n  def __init__(self, linear: Linear, rank: int, rngs: nnx.Rngs):\n    self.linear = linear\n    self.A = LoraParam(jax.random.normal(rngs(), (linear.din, rank)))\n    self.B = LoraParam(jax.random.normal(rngs(), (rank, linear.dout)))\n\n  def __call__(self, x: jax.Array):\n    return self.linear(x) + x @ self.A @ self.B\n\nrngs = nnx.Rngs(0)\nmodel = MLP(2, 32, 5, rngs=rngs)\n\n# Model surgery.\nmodel.linear1 = LoraLinear(model.linear1, 4, rngs=rngs)\nmodel.linear2 = LoraLinear(model.linear2, 4, rngs=rngs)\n\ny = model(x=jnp.ones((3, 2)))\n\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing RNG Streams in Flax Training Loop\nDESCRIPTION: Example showing how to manage RNG streams from make_rng, param, variable and nn.Dropout in a Flax training loop, including initialization and parameter updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass SubModule(nn.Module):\n  @nn.compact\n  def __call__(self, x, train):\n    kernel = self.param('submodule_kernel', jax.random.normal, x.shape)\n    x = x + kernel\n    x = nn.Dropout(0.2)(x, deterministic=not train)\n    x = nn.Dense(3)(x)\n    return x\n\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x, train):\n    kernel = self.variable(\n      'other_collection',\n      'module_kernel',\n      lambda: jax.random.normal(self.make_rng('other'), x.shape),\n    )\n    x = x + kernel.value\n    x = SubModule()(x, train)\n    x = nn.Dropout(0.2)(x, deterministic=not train)\n    x = nn.Dense(2)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Defining the AxisMetadata Abstract Base Class in Python\nDESCRIPTION: The core abstract base class for axis metadata in Flax. It defines methods for boxing/unboxing values and handling axis changes during transformations like vmap and scan. This class enables tracking per-axis metadata that works with lifted transformations.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTAxisMetadata = TypeVar(\"TAxisMetadata\", bound=\"AxisMetadata\")\n\nclass AxisMetadata(metaclass=abc.ABCMeta):\n  \"\"\"Abstract base class for boxed Metadata.\n\n  ``AxisMetadata`` enables arbitrary, per axis metadata for variables.\n  By using ``unbox`` the metadata is stripped away to obtain the original\n  variables. By using unboxing, most code handling variables does not need\n  to handle ``AxisMetadata`` specifically, but can directly operate on the JAX\n  arrays that they wrap.\n\n  Additionally, ``AxisMetadata`` supports updating metadata whenever an axis\n  is added or removed by a functional transformation\n  (e.g.: ``nn.scan`` or ``nn.vmap``) using the ``add_axis`` and ``remove_axis``\n  methods.\n\n  By extending ``AxisMetadata``, custom metadata can be stored. See\n  ``Partitioned`` for a specific implementation.\n  \"\"\"\n\n  @abc.abstractmethod\n  def unbox(self) -> Any:\n    \"\"\"Returns the content of the AxisMetadata box.\n\n    Note that unlike ``meta.unbox`` the unbox call should recursively unbox\n    metadata. It should simply return value that it wraps directly even\n    if that value itself is an instance of AxisMetadata.\n\n    In practise, AxisMetadata subclasses should be registred as PyTree nodes to\n    support passing instances to JAX and Flax APIs. The leaves returned for this\n    note should correspond to the value returned by unbox.\n\n    Returns:\n      The unboxed value.\n    \"\"\"\n    pass\n\n  @abc.abstractmethod\n  def add_axis(self: TAxisMetadata, index: int,\n               params: Dict[Any, Any]) -> TAxisMetadata:\n    \"\"\"Adds a new axis to the axis metadata.\n\n    Note that add_axis and remove_axis should act as each other's inverse\n    (meaning: ``x.add_axis(i, p).remove_axis(i, p) == x``)\n\n    Args:\n      index: The position at which the new axis will be inserted\n      params: An arbitrary dictionary of parameters passed by the transformation\n        that introduces the new axis (e.g.: ``nn.scan`` or ``nn.vmap``). The\n        user passes this dictionary as the `metadata_param` argument to the\n        transformation.\n    Returns:\n      A new instance of the same type as self and with the same ``unbox``\n      content with updated axis metadata.\n    \"\"\"\n    pass\n\n  @abc.abstractmethod\n  def remove_axis(self: TAxisMetadata, index: int,\n                  params: Dict[Any, Any]) -> TAxisMetadata:\n    \"\"\"Removes an axis from the axis metadata.\n\n    Note that add_axis and remove_axis should act as each other's inverse\n    (meaning: ``x.remove_axis(i, p).add_axis(i, p) == x``)\n\n    Args:\n      index: The position of the axis that is to be removed\n      params: An arbitrary dictionary of parameters passed by the transformation\n        that introduced the axis (e.g.: ``nn.scan`` or ``nn.vmap``). The\n        user passes this dictionary as the `metadata_param` argument to the\n        transformation.\n    Returns:\n      A new instance of the same type as self and with the same ``unbox``\n      content with updated axis metadata.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining RNN Class in Flax\nDESCRIPTION: This snippet defines the RNN class, which inherits from RNNBase and applies an RNNCellBase instance over a batch of input sequences. It can be used with any type of cell (e.g., GRUCell, LSTMCell).\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass RNN(RNNBase):\n  cell: RNNCellBase,\n  cell_size: int | Tuple[int, ...]\n  time_axis: int = -2,\n  variable_axes = FrozenDict(),\n  variable_broadcast: CollectionFilter = 'params'\n  variable_carry: CollectionFilter = False\n  split_rngs = FrozenDict({'params': False})\n  # implement RNNBase\n  ...\n```\n\n----------------------------------------\n\nTITLE: Mixed NNX-JAX Implementation with nnx.jit and jax.grad\nDESCRIPTION: Demonstrates combining nnx.jit with jax.grad transforms for hybrid functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/jax_and_nnx_transforms.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@nnx.jit\ndef train_step(model, x, y):\n  def loss_fn(graphdef, state):\n    model = nnx.merge(graphdef, state)\n    return ((model(x) - y) ** 2).mean()\n  grads = jax.grad(loss_fn, 1)(*nnx.split(model))\n  params = nnx.state(model, nnx.Param)\n  params = jax.tree_util.tree_map(\n    lambda p, g: p - 0.1 * g, params, grads\n  )\n  nnx.update(model, params)\n\nmodel = nnx.Linear(2, 3, rngs=nnx.Rngs(0))\ntrain_step(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Step Implementation with BatchNorm\nDESCRIPTION: Shows the evaluation step implementation that uses BatchNorm in inference mode with frozen batch statistics.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/batch_norm.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef eval_step(state: TrainState, batch):\n  logits = state.apply_fn(\n    {'params': state.params, 'batch_stats': state.batch_stats},\n    x=batch['image'], train=False)\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n    logits=logits, labels=batch['label']).mean()\n  metrics = {\n    'loss': loss,\n    'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n  }\n  return state, metrics\n```\n\n----------------------------------------\n\nTITLE: Splitting Flax NNX Model into Parameter and Batch Statistic Groups\nDESCRIPTION: Demonstrates how to use nnx.Param and nnx.BatchStat as Filters to split a Flax NNX model into two groups: parameters and batch statistics.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\n\nclass Foo(nnx.Module):\n  def __init__(self):\n    self.a = nnx.Param(0)\n    self.b = nnx.BatchStat(True)\n\nfoo = Foo()\n\ngraphdef, params, batch_stats = nnx.split(foo, nnx.Param, nnx.BatchStat)\n\nprint(f'{params = }')\nprint(f'{batch_stats = }')\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset from Torchvision for Jax+Flax\nDESCRIPTION: Defines a function to load MNIST dataset using torchvision, converting the data to jax.numpy arrays with proper formatting. The function downloads the data, rescales pixel values from [0,255] to [0,1], and adds a channel dimension for CNN compatibility.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_dataset_torch():\n    mnist = {\n        'train': torchvision.datasets.MNIST('./data', train=True, download=True),\n        'test': torchvision.datasets.MNIST('./data', train=False, download=True)\n    }\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': mnist[split].data.numpy(),\n            'label': mnist[split].targets.numpy()\n        }\n\n        # cast from np to jnp and rescale the pixel values from [0,255] to [0,1]\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        # torchvision returns shape (B, 28, 28).\n        # hence, append the trailing channel dimension.\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']\n```\n\n----------------------------------------\n\nTITLE: Converting Basic Optimizer Implementation from flax.optim to optax\nDESCRIPTION: Comparison of a basic training step implementation between flax.optim and optax. While flax.optim maintains parameters internally, optax requires explicitly passing and updating parameters alongside the optimizer state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(optimizer, batch):\n  grads = jax.grad(loss)(optimizer.target, batch)\n\n\n  return optimizer.apply_gradient(grads)\n\noptimizer_def = flax.optim.Momentum(\n    learning_rate, momentum)\noptimizer = optimizer_def.create(variables['params'])\n\nfor batch in get_ds_train():\n  optimizer = train_step(optimizer, batch)\n```\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(params, opt_state, batch):\n  grads = jax.grad(loss)(params, batch)\n  updates, opt_state = tx.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n  return params, opt_state\n\ntx = optax.sgd(learning_rate, momentum)\nparams = variables['params']\nopt_state = tx.init(params)\n\nfor batch in ds_train:\n  params, opt_state = train_step(params, opt_state, batch)\n```\n\n----------------------------------------\n\nTITLE: LSTM Cell Implementation\nDESCRIPTION: Implements a Long Short-Term Memory (LSTM) cell with initialization and forward pass logic including gates and state management.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef lstm(\n    scope,\n    carry,\n    inputs,\n    gate_fn=nn.activation.sigmoid,\n    activation_fn=nn.activation.tanh,\n    kernel_init=nn.linear.default_kernel_init,\n    recurrent_kernel_init=nn.initializers.orthogonal(),\n    bias_init=nn.initializers.zeros_init(),\n):\n  r\"\"\"A long short-term memory (LSTM) cell.\n\n  the mathematical definition of the cell is as follows\n  .. math::\n      \\begin{array}{ll}\n      i = \\sigma(W_{ii} x + W_{hi} h + b_{hi}) \\\\\n      f = \\sigma(W_{if} x + W_{hf} h + b_{hf}) \\\\\n      g = \\tanh(W_{ig} x + W_{hg} h + b_{hg}) \\\\\n      o = \\sigma(W_{io} x + W_{ho} h + b_{ho}) \\\\\n      c' = f * c + i * g \\\\\n      h' = o * \\tanh(c') \\\\\n      \\end{array}\n  where x is the input, h is the output of the previous time step, and c is\n  the memory.\n\n  Args:\n    carry: the hidden state of the LSTM cell,\n      initialized using `LSTMCell.initialize_carry`.\n    inputs: an ndarray with the input for the current time step.\n      All dimensions except the final are considered batch dimensions.\n    gate_fn: activation function used for gates (default: sigmoid)\n    activation_fn: activation function used for output and memory update\n      (default: tanh).\n    kernel_init: initializer function for the kernels that transform\n      the input (default: lecun_normal).\n    recurrent_kernel_init: initializer function for the kernels that transform\n      the hidden state (default: orthogonal).\n    bias_init: initializer for the bias parameters (default: zeros_init())\n  Returns:\n    A tuple with the new carry and the output.\n  \"\"\"\n  c, h = carry\n  hidden_features = h.shape[-1]\n  # input and recurrent layers are summed so only one needs a bias.\n  dense_h = lambda name: scope.child(dense, name)(\n      h,\n      features=hidden_features,\n      bias=True,\n      kernel_init=recurrent_kernel_init,\n      bias_init=bias_init,\n  )\n  dense_i = lambda name: scope.child(dense, name)(\n      inputs, features=hidden_features, bias=False, kernel_init=kernel_init\n  )\n  i = gate_fn(dense_i(name='ii') + dense_h(name='hi'))\n  f = gate_fn(dense_i(name='if') + dense_h(name='hf'))\n  g = activation_fn(dense_i(name='ig') + dense_h(name='hg'))\n  o = gate_fn(dense_i(name='io') + dense_h(name='ho'))\n  new_c = f * c + i * g\n  new_h = o * activation_fn(new_c)\n  return (new_c, new_h), new_h\n\n\ndef lstm_init_carry(batch_dims, size, init_fn=jnp.zeros):\n  shape = batch_dims + (size,)\n  return init_fn(shape), init_fn(shape)\n\n\nx = jnp.ones((1, 2))\ncarry = lstm_init_carry((1,), 3)\ny, variables = init(lstm)(random.key(0), carry, x)\njax.tree_util.tree_map(np.shape, (y, variables))\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic RNG Module with make_rng\nDESCRIPTION: Creates a simple Flax Module that demonstrates how to use make_rng method to generate multiple PRNG keys from a single stream.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass RNGModule(nn.Module):\n  @nn.compact\n  def __call__(self):\n    print(self.make_rng('rng_stream'))\n    print(self.make_rng('rng_stream'))\n    print(self.make_rng('rng_stream'))\n\nrng_module = RNGModule()\nvariables = rng_module.init({'rng_stream': jax.random.key(0)})\n```\n\n----------------------------------------\n\nTITLE: Calculating model error rate on test set\nDESCRIPTION: Computes the logits for the test set using the trained model, identifies incorrectly classified examples, and calculates the error rate (percentage of misclassifications).\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Find all mistakes in testset.\nlogits = train.CNN().apply({'params': state.params}, test_ds['image'])\nerror_idxs, = jnp.where(test_ds['label'] != logits.argmax(axis=1))\nlen(error_idxs) / len(logits)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Block and MLP with Flax NNX\nDESCRIPTION: Implementation of Block and MLP modules using Flax NNX. This version uses nnx.vmap and nnx.scan for handling multiple layers, and demonstrates how Flax handles PRNG keys differently from Haiku through the use of nnx.split_rngs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nnx.Module):\n  def __init__(self, input_dim, features, rngs):\n    self.linear = nnx.Linear(input_dim, features, rngs=rngs)\n    self.dropout = nnx.Dropout(0.5, rngs=rngs)\n\n  def __call__(self, x: jax.Array):  # No need to require a second input!\n    x = self.linear(x)\n    x = self.dropout(x)\n    x = jax.nn.relu(x)\n    return x   # No need to return a second output!\n\nclass MLP(nnx.Module):\n  def __init__(self, features, num_layers, rngs):\n    @nnx.split_rngs(splits=num_layers)\n    @nnx.vmap(in_axes=(0,), out_axes=0)\n    def create_block(rngs: nnx.Rngs):\n      return Block(features, features, rngs=rngs)\n\n    self.blocks = create_block(rngs)\n    self.num_layers = num_layers\n\n  def __call__(self, x):\n    @nnx.split_rngs(splits=self.num_layers)\n    @nnx.scan(in_axes=(nnx.Carry, 0), out_axes=nnx.Carry)\n    def forward(x, model):\n      x = model(x)\n      return x\n\n    return forward(x, self.blocks)\n\n\n\nmodel = MLP(64, num_layers=5, rngs=nnx.Rngs(0))\n```\n\n----------------------------------------\n\nTITLE: Custom Model with Variable Addition\nDESCRIPTION: Demonstrates a custom model with variable addition and full parameter/RNG splitting across batch dimensions.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(2)(x)\n    kernel = self.variable(\n      'other_collection',\n      'kernel',\n      lambda: jax.random.normal(self.make_rng('other'), x.shape),\n    )\n    return x + kernel.value\n\nBatchModel = nn.vmap(\n  Model,\n  in_axes=0,\n  out_axes=0,\n  variable_axes={'params': 0, 'other_collection': 0},\n  split_rngs={'params': True, 'other': True},\n)\n\nBatchModel().init({'params': jax.random.key(0), 'other': jax.random.key(1)}, x)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Dense Layer with Custom Initialization in Flax\nDESCRIPTION: This code demonstrates how to create a custom Dense layer with explicit parameter declarations and initialization functions in Flax Linen.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleDense(nn.Module):\n  features: int\n  kernel_init: Callable = nn.initializers.lecun_normal()\n  bias_init: Callable = nn.initializers.zeros_init()\n\n  @nn.compact\n  def __call__(self, inputs):\n    kernel = self.param('kernel',\n                        self.kernel_init,  # RNG passed implicitly.\n                        (inputs.shape[-1], self.features))  # shape info.\n    y = lax.dot_general(inputs, kernel,\n                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n    bias = self.param('bias', self.bias_init, (self.features,))\n    y = y + bias\n    return y\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = SimpleDense(features=3)\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameters:\\n', init_variables)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Initializing Variables in Linen\nDESCRIPTION: Demonstrates how to initialize variables for a ResNet model using the Linen API. It returns a dictionary containing both params and batch_stats.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef initial_variables(key, init_batch):\n  return ResNet().init(key, init_batch)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset from TensorFlow Datasets for Jax+Flax\nDESCRIPTION: Defines a function to load MNIST dataset using tensorflow_datasets, converting the data to jax.numpy arrays with proper formatting. The function downloads the data, rescales pixel values from [0,255] to [0,1], and ensures proper data types.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_dataset_tf():\n    mnist = tfds.builder('mnist')\n    mnist.download_and_prepare()\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = tfds.as_numpy(mnist.as_dataset(split=split, batch_size=-1))\n\n        # cast to jnp and rescale pixel values\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n    return ds['train'], ds['test']\n```\n\n----------------------------------------\n\nTITLE: Implementing TrainState Class in Flax\nDESCRIPTION: Defines a TrainState class that manages model parameters, optimization state, and gradient updates. The class provides methods for creating new instances and applying gradients during training.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(flax.struct.PyTreeNode):\n  step: int\n  apply_fn: Callable = flax.struct.field(pytree_node=False)\n  params: flax.core.FrozenDict[str, Any]\n  tx: optax.GradientTransformation = flax.struct.field(pytree_node=False)\n  opt_state: optax.OptState\n\n  def apply_gradients(self, *, grads, **kwargs):\n    updates, new_opt_state = self.tx.update(\n        grads, self.opt_state, self.params)\n    new_params = optax.apply_updates(self.params, updates)\n    return self.replace(\n        step=self.step + 1,\n        params=new_params,\n        opt_state=new_opt_state,\n        **kwargs,\n    )\n\n  @classmethod\n  def create(cls, *, apply_fn, params, tx, **kwargs):\n    opt_state = tx.init(params)\n    return cls(\n        step=0,\n        apply_fn=apply_fn,\n        params=params,\n        tx=tx,\n        opt_state=opt_state,\n        **kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing a Scan Over Layers Example in Python\nDESCRIPTION: This snippet demonstrates how to use the previously defined components to create a more complex neural network structure. It includes a Block class that combines Linear, BatchNorm, and Dropout layers, and a ScanMLP class that applies these blocks in a scan operation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/tiny_nnx.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Block(Module):\n\n  def __init__(self, din: int, dout: int, *, rngs: Rngs):\n    self.linear = Linear(din, dout, rngs=rngs)\n    self.bn = BatchNorm(dout)\n    self.dropout = Dropout(0.1)\n\n  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n    x = self.linear(x)\n    x = self.bn(x, train=train)\n    x = jax.nn.gelu(x)\n    x = self.dropout(x, train=train, rngs=rngs)\n    return x\n\n\nclass ScanMLP(Module):\n\n  def __init__(self, hidden_size: int, n_layers: int, *, rngs: Rngs):\n    self.n_layers = n_layers\n\n    # lift init\n    key = random.split(rngs.make_rng(), n_layers - 1)\n    graphdef: GraphDef[Block] = None  # type: ignore\n\n    def init_fn(key):\n      nonlocal graphdef\n      state, graphdef = Block(\n          hidden_size, hidden_size, rngs=Rngs(key)\n      ).split()\n      return state\n\n    state = jax.vmap(init_fn)(key)\n    self.layers = graphdef.merge(state)\n    self.linear = Linear(hidden_size, hidden_size, rngs=rngs)\n\n  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n    # lift call\n    key: jax.Array = random.split(rngs.make_rng(), self.n_layers - 1)  # type: ignore\n    state, graphdef = self.layers.split()\n\n    def scan_fn(x, inputs: tuple[jax.Array, State]):\n      key, state = inputs\n      x, (state, _) = graphdef.apply(state)(x, train=train, rngs=Rngs(key))\n      return x, state\n\n    x, state = jax.lax.scan(scan_fn, x, (key, state))\n    self.layers.update(state)\n    x = self.linear(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Pre-trained Model in Flax\nDESCRIPTION: Uses a pre-trained model to generate predictions (logits) for a batch of images. The model applies the trained parameters and batch statistics without training mode.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate using model trained on imagenet.\nlogits = model.apply({'params': state.params, 'batch_stats': state.batch_stats}, batch['image'][:128], train=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sharded Neural Network Layer\nDESCRIPTION: Defines a DotReluDot layer using Flax NNX with sharding annotations for distributed computation across devices.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DotReluDot(nnx.Module):\n  def __init__(self, depth: int, rngs: nnx.Rngs):\n    init_fn = nnx.initializers.lecun_normal()\n\n    # Initialize a sublayer `self.dot1` and annotate its kernel with.\n    # `sharding (None, 'model')`.\n    self.dot1 = nnx.Linear(\n      depth, depth,\n      kernel_init=nnx.with_partitioning(init_fn, (None, 'model')),\n      use_bias=False,  # or use `bias_init` to give it annotation too\n      rngs=rngs)\n\n    # Initialize a weight param `w2` and annotate with sharding ('model', None).\n    # Note that this is simply adding `.sharding` to the variable as metadata!\n    self.w2 = nnx.Param(\n      init_fn(rngs.params(), (depth, depth)),  # RNG key and shape for W2 creation\n      sharding=('model', None),\n    )\n\n  def __call__(self, x: jax.Array):\n    y = self.dot1(x)\n    y = jax.nn.relu(y)\n    # In data parallelism, input / intermediate value's first dimension (batch)\n    # will be sharded on `data` axis\n    y = jax.lax.with_sharding_constraint(y, PartitionSpec('data', 'model'))\n    z = jnp.dot(y, self.w2.value)\n    return z\n```\n\n----------------------------------------\n\nTITLE: Using Flax high-level FP8 API with Einsum in Python\nDESCRIPTION: This snippet demonstrates how to use Flax's high-level FP8 API by replacing jnp.einsum with fp8_ops.Fp8Einsum in a custom module.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\nclass FooModule(nn.Module):\n  einsum: Any = None\n  @nn.compact\n  def __call__(self, a, b):\n    if self.einsum is not None:\n      einsum_fn = self.einsum()\n    elif self.einsum is None:\n      einsum_fn = jnp.einsum\n    c = einsum_fn(\"mk,kn->mn\", a, b)\n    return c\n\nmodel = FooModule(einsum=fp8_ops.Fp8Einsum)\nparams = model.init(k0, a, b)\n\n@jax.jit\ndef train_step(var, a, b):\n  c = model.apply(var, a, b)\n  return jnp.sum(c)\n\ncheck_fp8_call(train_step.lower(params, a, b))\n```\n\n----------------------------------------\n\nTITLE: Applying Remat Transformation to Flax Module for Memory Optimization\nDESCRIPTION: This example demonstrates the use of nn.remat to optimize memory usage in a Flax module. It creates an RematMLP where the entire forward pass is recomputed during backpropagation. Note that remat can affect RNG streams, potentially changing initialization results.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass RematMLP(nn.Module):\n  features: Sequence[int]\n  # For all transforms, we can annotate a method, or wrap an existing\n  # Module class. Here we annotate the method.\n  @nn.remat\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      x = nn.Dense(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(3), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = RematMLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Modified Train Step Function with Learning Rate Schedule in Python\nDESCRIPTION: This updated train_step function incorporates the learning rate schedule. It takes an additional learning_rate_fn parameter and includes the current learning rate in the metrics.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/lr_schedule.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@functools.partial(jax.jit, static_argnums=2)\ndef train_step(state, batch, learning_rate_fn):\n    def loss_fn(params):\n      logits = CNN().apply({'params': params}, batch['image'])\n      one_hot = jax.nn.one_hot(batch['label'], 10)\n      loss = jnp.mean(optax.softmax_cross_entropy(logits, one_hot))\n      return loss, logits\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (_, logits), grads = grad_fn(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    metrics = compute_metrics(logits, batch['label'])\n    lr = learning_rate_fn(state.step)\n    metrics['learning_rate'] = lr\n    return new_state, metrics\n```\n\n----------------------------------------\n\nTITLE: Computing Model Accuracy on Validation Dataset\nDESCRIPTION: Calculates the overall accuracy of the model on the validation dataset by iterating through batches, making predictions, and comparing with ground truth labels. The function tracks the count of correct predictions and total samples.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Compute accuracy.\neval_steps = dataset_builder.info.splits['validation'].num_examples // config.batch_size\ncount = correct = 0\nfor step, batch in zip(range(eval_steps), eval_iter):\n  labels = [imagenette_imagenet2012(label) for label in batch['label'].flatten()]\n  logits = p_get_logits(batch['image'])\n  logits = logits.reshape([-1, logits.shape[-1]])\n  print(f'Step {step+1}/{eval_steps}...')\n  count += len(labels)\n  correct += (logits.argmax(axis=-1) == jnp.array(labels)).sum()\n\ncorrect / count\n```\n\n----------------------------------------\n\nTITLE: Initializing Optax Optimizer State\nDESCRIPTION: Creating an Adam optimizer and initializing its state with the model parameters. The optimizer state is a nested structure containing momentum and variance terms for each parameter.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntx = optax.adam(1.0)\nopt_state = tx.init(params)\n\n# The optimizer state is a tuple of gradient transformation states.\njax.tree_util.tree_map(jnp.shape, opt_state)\n```\n\n----------------------------------------\n\nTITLE: Transforming Methods with nnx.vmap\nDESCRIPTION: Demonstrates how to use nnx.vmap as a decorator for Module methods, including __init__ and __call__, to create and use a WeightStack Module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass WeightStack(nnx.Module):\n  @nnx.vmap\n  def __init__(self, seed: jax.Array):\n    self.kernel = nnx.Param(random.uniform(random.key(seed), (2, 3)))\n    self.bias = nnx.Param(jnp.zeros((3,)))\n\n  @nnx.vmap(in_axes=0, out_axes=1)\n  def __call__(self, x: jax.Array):\n    assert self.kernel.ndim == 2, 'Batch dimensions not allowed'\n    assert x.ndim == 1, 'Batch dimensions not allowed'\n    return x @ self.kernel + self.bias\n\nweights = WeightStack(jnp.arange(10))\n\nx = jax.random.normal(random.key(1), (10, 2))\ny = weights(x)\n\nprint(f'{y.shape = }')\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Saving Async Checkpoint with Orbax\nDESCRIPTION: Demonstrates saving a checkpoint asynchronously using Orbax checkpoint manager with wait functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nasync_checkpoint_manager.save(0, mp_ckpt)\nasync_checkpoint_manager.wait_until_finished()\n```\n\n----------------------------------------\n\nTITLE: Managing Random State in Vectorized Flax NNX Modules\nDESCRIPTION: Shows how to handle random state in Flax NNX by adding an Rngs attribute to a module and configuring StateAxes to map RngState to axis 0 while broadcasting parameters. The example creates a noisy vector dot product with different random noise for each batch element.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, kernel, bias, count, seed):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n    self.count = Count(count)\n    self.rngs = nnx.Rngs(noise=seed)\n\nweights = Weights(\n  kernel=random.uniform(random.key(0), (2, 3)),\n  bias=jnp.zeros((3,)),\n  count=jnp.array(0),\n  seed=random.split(random.key(0), num=10),\n)\nx = random.normal(random.key(1), (10, 2))\n\ndef noisy_vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  weights.count += 1\n  y = x @ weights.kernel + weights.bias\n  return y + random.normal(weights.rngs.noise(), y.shape)\n\nstate_axes = nnx.StateAxes({nnx.RngState: 0, (nnx.Param, Count): None})\ny1 = nnx.vmap(noisy_vector_dot, in_axes=(state_axes, 0))(weights, x)\ny2 = nnx.vmap(noisy_vector_dot, in_axes=(state_axes, 0))(weights, x)\n\nprint(jnp.allclose(y1, y2))\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Vectorized MLP with BatchNorm\nDESCRIPTION: Extended example showing how to handle both parameters and batch statistics in a vectorized model using nn.vmap with BatchNorm layer.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass StatefulMLP(nn.Module):\n  @nn.compact\n  def __call__(self, x, *, train):\n    h = nn.Dense(4, name='hidden')(x)\n    h = nn.BatchNorm(axis_name='batch')(h, use_running_average=not train)\n    h = nn.relu(h)\n    return nn.Dense(1, name='out')(h)\n\nclass LinenStatefulVmapMLP(nn.Module):\n  @nn.compact\n  def __call__(self, xs, *, train):\n    VmapMLP = nn.vmap(StatefulMLP, variable_axes={'params': 0, 'batch_stats': 0}, split_rngs={'params': True}, in_axes=0)\n    return VmapMLP(name='mlp')(xs, train=train)\nvariables = LinenStatefulVmapMLP().init(random.key(0), xs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential LSTM with Flax Scan\nDESCRIPTION: This snippet demonstrates how to use nn.scan to process sequential data with an LSTM cell. It shows how to configure variable handling (broadcasting parameters across steps) and initialize the LSTM carry state before processing a sequence of inputs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleScan(nn.Module):\n  features: int\n\n  @nn.compact\n  def __call__(self, xs):\n    LSTM = nn.scan(nn.LSTMCell,\n                   in_axes=1, out_axes=1,\n                   variable_broadcast='params',\n                   split_rngs={'params': False})\n    lstm = LSTM(self.features, name=\"lstm_cell\")\n\n    dummy_rng = random.key(0)\n    input_shape = xs[:, 0].shape\n    init_carry = lstm.initialize_carry(dummy_rng, input_shape)\n\n    return lstm(init_carry, xs)\n\nkey1, key2 = random.split(random.key(0), 2)\nxs = random.uniform(key1, (1, 5, 2))\n\nmodel = SimpleScan(2)\ninit_variables = model.init(key2, xs)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n\ny = model.apply(init_variables, xs)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Model Instantiation in Haiku and Flax\nDESCRIPTION: Demonstration of how to instantiate models in Haiku and Flax. Haiku requires wrapping a function with hk.transform, while Flax allows direct instantiation of the Module class.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(x, training: bool):\n  return Model(256, 10)(x, training)\n\nmodel = hk.transform(forward)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n\nmodel = Model(256, 10)\n```\n\n----------------------------------------\n\nTITLE: Memory-Efficient Partial Initialization in Flax NNX\nDESCRIPTION: Shows a memory-efficient approach to partial model initialization using nnx.jit to skip unused arrays.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Some pretrained model state\nold_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n\n# Use `nnx.jit` (which wraps `jax.jit`) to automatically skip unused arrays - memory efficient!\n@nnx.jit(donate_argnums=0)\ndef partial_init(old_state, rngs):\n  model = TwoLayerMLP(4, rngs=rngs)\n  # Create a new state.\n  model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=rngs)\n  # Add the existing state.\n  nnx.update(model, old_state)\n  return model\n\nprint(f'Number of JAX Arrays in memory at start: {len(jax.live_arrays())}')\n```\n\n----------------------------------------\n\nTITLE: Comparing Top-level Functions vs. Modules in Haiku and Flax NNX\nDESCRIPTION: Demonstration of how to implement stateful computation in Haiku using top-level functions with state management vs. Flax NNX's module-centric approach with custom variable types.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef forward(x):\n\n\n  counter = hk.get_state('counter', shape=[], dtype=jnp.int32, init=jnp.ones)\n  multiplier = hk.get_parameter(\n    'multiplier', shape=[1,], dtype=x.dtype, init=jnp.ones\n  )\n\n  output = x + multiplier * counter\n\n  hk.set_state(\"counter\", counter + 1)\n  return output\n\nmodel = hk.transform_with_state(forward)\n\nparams, state = model.init(jax.random.key(0), jnp.ones((1, 64)))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(nnx.Variable):\n  pass\n\nclass FooModule(nnx.Module):\n\n  def __init__(self, rngs):\n    self.counter = Counter(jnp.ones((), jnp.int32))\n    self.multiplier = nnx.Param(\n      nnx.initializers.ones(rngs.params(), [1,], jnp.float32)\n    )\n  def __call__(self, x):\n    output = x + self.multiplier * self.counter.value\n\n    self.counter.value += 1\n    return output\n\nmodel = FooModule(rngs=nnx.Rngs(0))\n\n_, params, counter = nnx.split(model, nnx.Param, Counter)\n```\n\n----------------------------------------\n\nTITLE: Defining Dense Layer Module in Flax/Linen\nDESCRIPTION: Shows the transition from old Flax Dense layer implementation to new Linen style. Key changes include using dataclass attributes instead of apply arguments and renaming apply to __call__.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax import nn\n\nclass Dense(base.Module):\n  def apply(self,\n            inputs,\n            features,\n            use_bias=True,\n            kernel_init=default_kernel_init,\n            bias_init=initializers.zeros_init()):\n\n    kernel = self.param('kernel',\n      (inputs.shape[-1], features), kernel_init)\n    y = jnp.dot(inputs, kernel)\n    if use_bias:\n      bias = self.param(\n        'bias', (features,), bias_init)\n      y = y + bias\n    return y\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax import linen as nn\n\nclass Dense(nn.Module):\n  features: int\n  use_bias: bool = True\n  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init\n  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()\n\n  @nn.compact\n  def __call__(self, inputs):\n    kernel = self.param('kernel',\n      self.kernel_init, (inputs.shape[-1], self.features))\n    y = jnp.dot(inputs, kernel)\n    if self.use_bias:\n      bias = self.param(\n        'bias', self.bias_init, (self.features,))\n      y = y + bias\n    return y\n```\n\n----------------------------------------\n\nTITLE: Model and Tokenizer Loading\nDESCRIPTION: Loads model parameters, tokenizer, and configures the transformer model.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparams = params_lib.load_and_format_params(ckpt_path)\n\nvocab = spm.SentencePieceProcessor()\nvocab.Load(vocab_path)\n\ntransformer = transformer_lib.Transformer.from_params(params)\nnnx.display(transformer)\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation with Mutable State\nDESCRIPTION: Implementation of a JAX-compiled training step function that handles mutable batch statistics and gradient updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state, inputs, labels):\n\n  def loss_fn(params):\n    outputs, new_model_state = state.apply_fn(\n        {'params': params, 'batch_stats': state.batch_stats},\n        inputs,\n        mutable=['batch_stats'])\n    loss = xent_loss(outputs, labels)\n    return loss, new_model_state\n\n  (loss, new_model_state), grads = jax.value_and_grad(\n      loss_fn, has_aux=True)(state.params)\n  new_state = state.apply_gradients(\n      grads=grads,\n      batch_stats=new_model_state['batch_stats'],\n  )\n\n  return new_state, loss\n```\n\n----------------------------------------\n\nTITLE: Performing Model Surgery in NNX\nDESCRIPTION: Shows how to modify model architecture at runtime including module sharing, weight tying, and function replacement.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Module sharing\nmodel.blocks[1] = model.blocks[3]\n# Weight tying\nmodel.blocks[0].linear.kernel = model.blocks[-1].linear.kernel\n# Monkey patching\ndef my_optimized_layer(x): return x\nmodel.blocks[2] = my_optimized_layer\n\ny = model(jnp.ones((2, 4)))  # still works\nprint(f'{y.shape = }')\n```\n\n----------------------------------------\n\nTITLE: Using Optax's Composable Gradient Transformations vs Pre-defined Aliases\nDESCRIPTION: Comparison showing how to use optax.chain() to compose multiple gradient transformations instead of using pre-defined optimizer aliases like optax.sgd(). This showcases Optax's flexibility in building custom optimization strategies.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Note that the aliases follow the convention to use positive\n# values for the learning rate by default.\ntx = optax.sgd(learning_rate, momentum)\n```\n\nLANGUAGE: python\nCODE:\n```\n#\n\ntx = optax.chain(\n    # 1. Step: keep a trace of past updates and add to gradients.\n    optax.trace(decay=momentum),\n    # 2. Step: multiply result from step 1 with negative learning rate.\n    # Note that `optax.apply_updates()` simply adds the final updates to the\n    # parameters, so we must make sure to flip the sign here for gradient\n    # descent.\n    optax.scale(-learning_rate),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Recurrent Dropout with nnx.scan in Flax\nDESCRIPTION: This code demonstrates how to use nnx.scan with an RNNCell to implement recurrent dropout that maintains the same dropout mask across time steps. It uses StateAxes to broadcast the PRNG state for recurrent_dropout while carrying over other state components.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@nnx.jit\ndef rnn_forward(cell: RNNCell, x: jax.Array):\n  h = cell.initial_state(batch_size=x.shape[0])\n\n  # Broadcast the 'recurrent_dropout' PRNG state to have the same mask on every step.\n  state_axes = nnx.StateAxes({'recurrent_dropout': None, ...: nnx.Carry})\n  @nnx.scan(in_axes=(state_axes, nnx.Carry, 1), out_axes=(nnx.Carry, 1))\n  def unroll(cell: RNNCell, h, x) -> tuple[jax.Array, jax.Array]:\n    h, y = cell(h, x)\n    return h, y\n\n  h, y = unroll(cell, h, x)\n  return y\n\nx = jnp.ones((4, 20, 8))\ny = rnn_forward(cell, x)\n\nprint(f'{y.shape = }')\nprint(f'{cell.count.value = }')\n```\n\n----------------------------------------\n\nTITLE: Creating Sharded Model with JAX Compilation\nDESCRIPTION: Shows how to properly shard a model using JAX's compilation mechanism and Flax's nnx.jit, implementing the sharding process within a compiled function under a device mesh context.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@nnx.jit\ndef create_sharded_model():\n  model = DotReluDot(1024, rngs=nnx.Rngs(0)) # Unsharded at this moment.\n  state = nnx.state(model)                   # The model's state, a pure pytree.\n  pspecs = nnx.get_partition_spec(state)     # Strip out the annotations from state.\n  sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n  nnx.update(model, sharded_state)           # The model is sharded now!\n  return model\n\nwith mesh:\n  sharded_model = create_sharded_model()\n\n# They are some `GSPMDSharding` now - not a single device!\nprint(sharded_model.dot1.kernel.value.sharding)\nprint(sharded_model.w2.value.sharding)\n\n# Check out their equivalency with some easier-to-read sharding descriptions\nassert sharded_model.dot1.kernel.value.sharding.is_equivalent_to(\n  NamedSharding(mesh, PartitionSpec(None, 'model')), ndim=2\n)\nassert sharded_model.w2.value.sharding.is_equivalent_to(\n  NamedSharding(mesh, PartitionSpec('model', None)), ndim=2\n)\n```\n\n----------------------------------------\n\nTITLE: Training Step Function with Dropout\nDESCRIPTION: Implements a JIT-compiled training step function that handles dropout PRNG key generation and management during model training.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state: TrainState, batch, dropout_key):\n  dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n  def loss_fn(params):\n    logits = state.apply_fn(\n      {'params': params},\n      x=batch['image'],\n      training=True,\n      rngs={'dropout': dropout_train_key}\n      )\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=batch['label'])\n    return loss, logits\n  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n  (loss, logits), grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n  return state\n```\n\n----------------------------------------\n\nTITLE: Modified Train Epoch Function with Learning Rate Schedule in Python\nDESCRIPTION: This updated train_epoch function includes the learning_rate_fn as a parameter and passes it to the train_step function. It handles the training process for a single epoch using the learning rate schedule.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/lr_schedule.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef train_epoch(state, train_ds, batch_size, epoch, learning_rate_fn, rng):\n    \"\"\"Trains for a single epoch.\"\"\"\n    train_ds_size = len(train_ds['image'])\n    steps_per_epoch = train_ds_size // batch_size\n    perms = jax.random.permutation(rng, len(train_ds['image']))\n    perms = perms[:steps_per_epoch * batch_size]\n    perms = perms.reshape((steps_per_epoch, batch_size))\n    batch_metrics = []\n    for perm in perms:\n      batch = {k: v[perm, ...] for k, v in train_ds.items()}\n      state, metrics = train_step(state, batch, learning_rate_fn)\n      batch_metrics.append(metrics)\n\n    # compute mean of metrics across each batch in epoch.\n    batch_metrics = jax.device_get(batch_metrics)\n    epoch_metrics = {\n        k: np.mean([metrics[k] for metrics in batch_metrics])\n        for k in batch_metrics[0]}\n\n    logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,\n                 epoch_metrics['loss'], epoch_metrics['accuracy'] * 100)\n\n    return state, epoch_metrics\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Optimizers with Different Parameter Groups\nDESCRIPTION: Comparison of how to apply different optimizers to different parameter groups between flax.optim.MultiOptimizer and optax.masked(). Both approaches allow selective parameter updates with different optimization strategies.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkernels = flax.traverse_util.ModelParamTraversal(lambda p, _: 'kernel' in p)\nbiases = flax.traverse_util.ModelParamTraversal(lambda p, _: 'bias' in p)\n\nkernel_opt = flax.optim.Momentum(learning_rate, momentum)\nbias_opt = flax.optim.Momentum(learning_rate * 0.1, momentum)\n\n\noptimizer = flax.optim.MultiOptimizer(\n    (kernels, kernel_opt),\n    (biases, bias_opt)\n).create(variables['params'])\n```\n\nLANGUAGE: python\nCODE:\n```\nkernels = flax.traverse_util.ModelParamTraversal(lambda p, _: 'kernel' in p)\n```\n\n----------------------------------------\n\nTITLE: Modifying and Restoring Model Parameters\nDESCRIPTION: Demonstrating how to modify specific parameters (normalizing a dense layer kernel) and then reconstruct the parameter tree. The modified parameters are unflattened and frozen to match Flax's immutable parameter structure.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Somehow modify a layer\ndense_kernel = flat_params['Dense_1/kernel']\nflat_params['Dense_1/kernel'] = dense_kernel / jnp.linalg.norm(dense_kernel)\n\n# Unflatten.\nunflat_params = traverse_util.unflatten_dict(flat_params, sep='/')\n# Refreeze.\nunflat_params = freeze(unflat_params)\njax.tree_util.tree_map(jnp.shape, unflat_params)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Single JAX or NumPy Arrays with Orbax in Python\nDESCRIPTION: Demonstrates how to save and restore a single JAX or NumPy array using Orbax ArrayCheckpointHandler. This is useful when dealing with individual arrays rather than full pytrees.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nARR_CKPT_DIR = '/tmp/orbax_upgrade/singleton'\n\nckptr = orbax.checkpoint.Checkpointer(orbax.checkpoint.ArrayCheckpointHandler())\nckptr.save(ARR_CKPT_DIR, jnp.arange(10))\nckptr.restore(ARR_CKPT_DIR, item=None)\n```\n\n----------------------------------------\n\nTITLE: Example LSTM Cell Implementation with the New API\nDESCRIPTION: Shows the structure for implementing a custom LSTMCell following the new RNNCellBase API requirements, including the necessary num_feature_axes property.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LSTMCell(nn.RNNCellBase):\n  features: int # ← All metadata is now stored within the cell instance\n  ... #              ↓\n  carry_init: Initializer\n\n  def initialize_carry(self, rng, input_shape) -> Carry:\n    ...\n\n  @property\n  def num_feature_axes(self):\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Dense Layer in Flax\nDESCRIPTION: Example of using the partitioning metadata to initialize a Dense layer with partitioned weights. This demonstrates how the higher-order initializer pattern allows adding metadata without modifying existing Module definitions.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# init kernel with lecun normal and split the output features over the data axis\npartitioned_dense = nn.Dense(features, kernel_init=with_partitioning(nn.initializers.lecun_normal, (None, \"data\")))\n```\n\n----------------------------------------\n\nTITLE: Implementing CNN Using Sequential Module in Flax\nDESCRIPTION: Demonstrates how to implement a CNN using a Sequential module pattern in Flax. The implementation includes convolution layers, ReLU activation, pooling, and dense layers with initialization and feature extraction functions.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Sequential(nn.Module):\n    layers: Sequence[nn.Module]\n\n    def __call__(self, x):\n      for layer in self.layers:\n        x = layer(x)\n      return x\n\n  def SeqCNN():\n    return Sequential([\n      nn.Conv(features=32, kernel_size=(3, 3)),\n      nn.relu,\n      lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n      nn.Conv(features=64, kernel_size=(3, 3)),\n      nn.relu,\n      lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n      lambda x: x.reshape((x.shape[0], -1)),  # flatten\n      nn.Dense(features=256),\n      nn.relu,\n      nn.Dense(features=10),\n      nn.log_softmax,\n    ])\n\n  @jax.jit\n  def init(key, x):\n    variables = SeqCNN().init(key, x)\n    return variables['params']\n\n  @jax.jit\n  def features(params, x):\n    return Sequential(SeqCNN().layers[0:7]).apply({\"params\": params}, x)\n\n  batch = jnp.ones((1,28,28,1))\n  params = init(jax.random.key(0), batch)\n  features(params, batch)\n```\n\n----------------------------------------\n\nTITLE: Implementing Full RNN Module in Haiku and Flax\nDESCRIPTION: Creates a full RNN module using the RNNCell and lifted scan transform in both Haiku and Flax. Demonstrates differences in using scan and structuring the RNN logic.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass RNN(hk.Module):\n  def __init__(self, hidden_size: int, name=None):\n    super().__init__(name=name)\n    self.hidden_size = hidden_size\n\n  def __call__(self, x):\n    cell = RNNCell(self.hidden_size)\n    carry = cell.initial_state(x.shape[0])\n    carry, y = hk.scan(cell, carry, jnp.swapaxes(x, 1, 0))\n    y = jnp.swapaxes(y, 0, 1)\n    return y\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass RNN(nn.Module):\n  hidden_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    rnn = nn.scan(RNNCell, variable_broadcast='params', split_rngs={'params': False},\n                  in_axes=1, out_axes=1)(self.hidden_size)\n    carry = rnn.initial_state(x.shape[0])\n    carry, y = rnn(carry, x)\n    return y\n```\n\n----------------------------------------\n\nTITLE: Gradient Rematerialization in Flax Modules\nDESCRIPTION: Shows how to use JAX's rematerialization (remat) to reduce memory usage during backpropagation by recomputing forward-pass values. This is applied to a multi-layer perceptron to save memory at the cost of extra computation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass RematMLP(nn.Module):\n  features: Sequence[int]\n  # For all transforms, we can annotate a method, or wrap an existing\n  # Module class. Here we annotate the method.\n  @nn.remat\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      x = nn.Dense(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(3), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = RematMLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic CNN Module in Flax\nDESCRIPTION: Implements a simple Convolutional Neural Network (CNN) using Flax's nn.Module and nn.compact. The CNN consists of convolutional layers, pooling, and dense layers.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CNN(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    x = nn.log_softmax(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Computing Metrics within eval_step() Using pad_shard_unpad\nDESCRIPTION: Shows how to compute metrics directly within the eval_step function when using pad_shard_unpad, with static arguments and return values to prevent unnecessary operations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef eval_step(metrics, variables, batch):\n  print('retrigger compilation', {k: v.shape for k, v in batch.items()})\n  preds = model.apply(variables, batch['image'])\n  correct = (batch['mask'] & (batch['label'] == preds.argmax(axis=-1))).sum()\n  total = batch['mask'].sum()\n  return dict(\n      correct=metrics['correct'] + jax.lax.psum(correct, axis_name='batch'),\n      total=metrics['total'] + jax.lax.psum(total, axis_name='batch'),\n  )\n\neval_step = jax.pmap(eval_step, axis_name='batch')\neval_step = flax.jax_utils.pad_shard_unpad(\n    eval_step, static_argnums=(0, 1), static_return=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Modules in Haiku and Flax NNX\nDESCRIPTION: Demonstrates how to create a one-layer network with dropout and ReLU activation in both Haiku and Flax NNX. Highlights differences in module definition, state management, and initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport haiku as hk\n\nclass Block(hk.Module):\n  def __init__(self, features: int, name=None):\n    super().__init__(name=name)\n    self.features = features\n\n  def __call__(self, x, training: bool):\n    x = hk.Linear(self.features)(x)\n    x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(hk.Module):\n  def __init__(self, dmid: int, dout: int, name=None):\n    super().__init__(name=name)\n    self.dmid = dmid\n    self.dout = dout\n\n  def __call__(self, x, training: bool):\n    x = Block(self.dmid)(x, training)\n    x = hk.Linear(self.dout)(x)\n    return x\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\n\nclass Block(nnx.Module):\n  def __init__(self, in_features: int , out_features: int, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(in_features, out_features, rngs=rngs)\n    self.dropout = nnx.Dropout(0.5, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.linear(x)\n    x = self.dropout(x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(nnx.Module):\n  def __init__(self, din: int, dmid: int, dout: int, rngs: nnx.Rngs):\n    self.block = Block(din, dmid, rngs=rngs)\n    self.linear = nnx.Linear(dmid, dout, rngs=rngs)\n\n\n  def __call__(self, x):\n    x = self.block(x)\n    x = self.linear(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Training Epoch Function with Data Replication for Ensemble Models\nDESCRIPTION: A training epoch function that handles batch replication and unreplication for ensemble training. The function replicates each batch of data across devices, processes the batch with the parallel apply_model function, and then unreplicates the results for logging.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/ensembling.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_epoch(state, train_ds, batch_size, rng):\n  train_ds_size = len(train_ds['image'])\n  steps_per_epoch = train_ds_size // batch_size\n\n  perms = jax.random.permutation(rng, len(train_ds['image']))\n  perms = perms[:steps_per_epoch * batch_size]\n  perms = perms.reshape((steps_per_epoch, batch_size))\n\n  epoch_loss = []\n  epoch_accuracy = []\n\n  for perm in perms:\n    batch_images = jax_utils.replicate(train_ds['image'][perm, ...])\n    batch_labels = jax_utils.replicate(train_ds['label'][perm, ...])\n    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n    state = update_model(state, grads)\n    epoch_loss.append(jax_utils.unreplicate(loss))\n    epoch_accuracy.append(jax_utils.unreplicate(accuracy))\n  train_loss = np.mean(epoch_loss)\n  train_accuracy = np.mean(epoch_accuracy)\n  return state, train_loss, train_accuracy\n```\n\n----------------------------------------\n\nTITLE: Implementing a Flax Module with Parameters and State Variables\nDESCRIPTION: This code defines a simple Flax module called BiasAdderWithRunningMean that maintains a running mean as state and adds a learnable bias parameter. It demonstrates how to declare both parameters and state variables in a Flax model.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BiasAdderWithRunningMean(nn.Module):\n  momentum: float = 0.9\n\n  @nn.compact\n  def __call__(self, x):\n    is_initialized = self.has_variable('batch_stats', 'mean')\n    mean = self.variable('batch_stats', 'mean', jnp.zeros, x.shape[1:])\n    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n    if is_initialized:\n      mean.value = (self.momentum * mean.value +\n                    (1.0 - self.momentum) * jnp.mean(x, axis=0, keepdims=True))\n    return mean.value + bias\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Batched Dense Layer with nn.vmap\nDESCRIPTION: Demonstrates creating a batched Dense layer where parameters are shared across batch inputs using nn.vmap. The variable_axes parameter is set to None for params, meaning parameters are shared.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((3, 2))\n\nBatchDense = nn.vmap(\n    nn.Dense,\n    in_axes=0, out_axes=0,\n    variable_axes={'params': None},\n    split_rngs={'params': False})\n\nBatchDense(2).init(jax.random.key(0), x)\n```\n\n----------------------------------------\n\nTITLE: Autoencoder Implementation Comparison\nDESCRIPTION: Shows how to implement an autoencoder with multiple methods in both Flax Linen and NNX, demonstrating differences in initialization and method definition.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass AutoEncoder(nnx.Module):\n  def __init__(self, in_dim: int, embed_dim: int, output_dim: int, rngs):\n    self.encoder = nnx.Linear(in_dim, embed_dim, rngs=rngs)\n    self.decoder = nnx.Linear(embed_dim, output_dim, rngs=rngs)\n\n  def encode(self, x):\n    return self.encoder(x)\n\n  def decode(self, x):\n    return self.decoder(x)\n\n  def __call__(self, x):\n    x = self.encode(x)\n    x = self.decode(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Modules in Flax Linen and NNX\nDESCRIPTION: Demonstrates the differences in defining Module classes between Flax Linen and Flax NNX. Linen uses @nn.compact decorator and lazy initialization, while NNX uses eager initialization and separate __init__ and __call__ methods.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport flax.linen as nn\n\nclass Block(nn.Module):\n  features: int\n\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = nn.Dense(self.features)(x)\n    x = nn.Dropout(0.5, deterministic=not training)(x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(nn.Module):\n  dmid: int\n  dout: int\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = Block(self.dmid)(x, training)\n    x = nn.Dense(self.dout)(x)\n    return x\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\n\nclass Block(nnx.Module):\n  def __init__(self, in_features: int , out_features: int, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(in_features, out_features, rngs=rngs)\n    self.dropout = nnx.Dropout(0.5, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.linear(x)\n    x = self.dropout(x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(nnx.Module):\n  def __init__(self, din: int, dmid: int, dout: int, rngs: nnx.Rngs):\n    self.block = Block(din, dmid, rngs=rngs)\n    self.linear = nnx.Linear(dmid, dout, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.block(x)\n    x = self.linear(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Updating Batch Statistics in Linen\nDESCRIPTION: Shows how to update batch statistics during training using the Linen API. It explicitly specifies which variable collections are mutable.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(params, batch_stats):\n  variables = {'params': params, 'batch_stats': batch_stats}\n  logits, new_variables = ResNet(train=true).apply(\n    variables, batch['image'], mutable=['batch_stats'])\n  new_batch_stats = new_variables['batch_stats']\n  # [...]\n```\n\n----------------------------------------\n\nTITLE: Initializing Parameters and State in Haiku and Flax\nDESCRIPTION: Comparison of state initialization. Haiku returns params and state separately, while Flax returns a variables dict containing params and batch_stats collections. Note that training=True is required for Haiku BatchNorm initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsample_x = jax.numpy.ones((1, 784))\nparams, state = model.init(\n  random.key(0),\n  sample_x, training=True # <== inputs #!\n)\n...\n```\n\nLANGUAGE: python\nCODE:\n```\nsample_x = jax.numpy.ones((1, 784))\nvariables = model.init(\n  random.key(0), #!\n  sample_x, training=False # <== inputs\n)\nparams, batch_stats = variables[\"params\"], variables[\"batch_stats\"]\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation with NNX Transforms\nDESCRIPTION: Demonstrates using nnx.jit and nnx.grad for model training with direct module manipulation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/jax_and_nnx_transforms.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@nnx.jit\ndef train_step(model, x, y):\n  def loss_fn(model):\n    return ((model(x) - y) ** 2).mean()\n  grads = nnx.grad(loss_fn)(model)\n  params = nnx.state(model, nnx.Param)\n  params = jax.tree_util.tree_map(\n    lambda p, g: p - 0.1 * g, params, grads\n  )\n  nnx.update(model, params)\n\nmodel = nnx.Linear(2, 3, rngs=nnx.Rngs(0))\ntrain_step(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Proposed RNNCell initialize_carry Method\nDESCRIPTION: A proposed redesign of the RNNCell interface, making initialize_carry an instance method. This change would allow hyperparameters to be passed directly to the cell and simplify the initialization process.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_carry(self, sample_input) -> Carry:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Using Proposed RNN API in Flax\nDESCRIPTION: This snippet shows how to use the proposed RNN API in Flax, which simplifies the process of encoding a batch of input sequences using an LSTM cell.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncell = nn.LSTMCell()\n# Encodes a batch of input sequences.\ncarry, outputs = nn.RNN(cell, cell_size)(inputs, seq_lengths)\n```\n\n----------------------------------------\n\nTITLE: Sharding Input Data for Data Parallelism in JAX\nDESCRIPTION: This snippet demonstrates how to shard the batched input across the 'data' axis for data parallelism using JAX's device_put function.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx_sharding = mesh_sharding(PartitionSpec('data', None)) # dimensions: (batch, length)\nx = jax.device_put(x, x_sharding)\njax.debug.visualize_array_sharding(x)\n```\n\n----------------------------------------\n\nTITLE: Scan over Layers in Flax Linen vs NNX\nDESCRIPTION: Comparison of implementing scan-over-layers pattern in Flax Linen and NNX. NNX separates initialization (using vmap) from execution (using scan) and explicitly manages PRNG state splitting across layers.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nn.Module):\n  features: int\n  training: bool\n\n  @nn.compact\n  def __call__(self, x, _):\n    x = nn.Dense(self.features)(x)\n    x = nn.Dropout(0.5)(x, deterministic=not self.training)\n    x = jax.nn.relu(x)\n    return x, None\n\nclass MLP(nn.Module):\n  features: int\n  num_layers: int\n\n\n\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    ScanBlock = nn.scan(\n      Block, variable_axes={'params': 0}, split_rngs={'params': True},\n      length=self.num_layers)\n\n    y, _ = ScanBlock(self.features, training)(x, None)\n    return y\n\nmodel = MLP(64, num_layers=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nnx.Module):\n  def __init__(self, input_dim, features, rngs):\n    self.linear = nnx.Linear(input_dim, features, rngs=rngs)\n    self.dropout = nnx.Dropout(0.5, rngs=rngs)\n\n  def __call__(self, x: jax.Array):  # No need to require a second input!\n    x = self.linear(x)\n    x = self.dropout(x)\n    x = jax.nn.relu(x)\n    return x   # No need to return a second output!\n\nclass MLP(nnx.Module):\n  def __init__(self, features, num_layers, rngs):\n    @nnx.split_rngs(splits=num_layers)\n    @nnx.vmap(in_axes=(0,), out_axes=0)\n    def create_block(rngs: nnx.Rngs):\n      return Block(features, features, rngs=rngs)\n\n    self.blocks = create_block(rngs)\n    self.num_layers = num_layers\n\n  def __call__(self, x):\n    @nnx.split_rngs(splits=self.num_layers)\n    @nnx.scan(in_axes=(nnx.Carry, 0), out_axes=nnx.Carry)\n    def forward(x, model):\n      x = model(x)\n      return x\n\n    return forward(x, self.blocks)\n\nmodel = MLP(64, num_layers=5, rngs=nnx.Rngs(0))\n```\n\n----------------------------------------\n\nTITLE: Main Training Loop for Ensemble Training with JAX\nDESCRIPTION: The main training loop that sets up the ensemble by replicating the test dataset, initializing models with different random seeds across devices, and performing training. It also handles proper unreplication of model outputs for logging.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/ensembling.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds, test_ds = get_datasets()\ntest_ds = jax_utils.replicate(test_ds)\nrng = jax.random.key(0)\n\nrng, init_rng = jax.random.split(rng)\nstate = create_train_state(jax.random.split(init_rng, jax.device_count()),\n                           learning_rate, momentum)\n\nfor epoch in range(1, num_epochs + 1):\n  rng, input_rng = jax.random.split(rng)\n  state, train_loss, train_accuracy = train_epoch(\n      state, train_ds, batch_size, input_rng)\n\n  _, test_loss, test_accuracy = jax_utils.unreplicate(\n      apply_model(state, test_ds['image'], test_ds['label']))\n\n  logging.info(\n      'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, '\n      'test_loss: %.4f, test_accuracy: %.2f'\n      % (epoch, train_loss, train_accuracy * 100, test_loss,\n         test_accuracy * 100))\n```\n\n----------------------------------------\n\nTITLE: Setting up JAX Device Mesh for SPMD Programming\nDESCRIPTION: Creates a 2x4 device mesh with annotated axes for data and model parallelism. Provides a utility function to generate sharding specifications for the mesh.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.sharding import Mesh, PartitionSpec, NamedSharding\nfrom jax.lax import with_sharding_constraint\nfrom jax.experimental import mesh_utils\n\n# Create a mesh and annotate each axis with a name.\ndevice_mesh = mesh_utils.create_device_mesh((2, 4))\nprint(device_mesh)\n\nmesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\nprint(mesh)\n\ndef mesh_sharding(pspec: PartitionSpec) -> NamedSharding:\n  return NamedSharding(mesh, pspec)\n```\n\n----------------------------------------\n\nTITLE: Variable Creation in Haiku and Flax NNX\nDESCRIPTION: Shows how to instantiate a model and initialize its parameters in both frameworks. Haiku uses a transform approach, while Flax NNX automatically initializes parameters during model instantiation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(x, training: bool):\n  return Model(256, 10)(x, training)\n\nmodel = hk.transform(forward)\nsample_x = jnp.ones((1, 784))\nparams = model.init(jax.random.key(0), sample_x, training=False)\n\n\nassert params['model/linear']['b'].shape == (10,)\nassert params['model/block/linear']['w'].shape == (784, 256)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n\nmodel = Model(784, 256, 10, rngs=nnx.Rngs(0))\n\n\n# Parameters were already initialized during model instantiation.\n\nassert model.linear.bias.value.shape == (10,)\nassert model.block.linear.kernel.value.shape == (784, 256)\n```\n\n----------------------------------------\n\nTITLE: Deserializing Flax Model Parameters from Bytes\nDESCRIPTION: Demonstrates how to convert serialized bytes back into model parameters using an existing parameter structure as a template.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nserialization.from_bytes(params, bytes_output)\n```\n\n----------------------------------------\n\nTITLE: Refactoring Flax Module into Submodules\nDESCRIPTION: Shows how to refactor a Flax module into submodules, allowing direct access to intermediate values. This approach splits the CNN into separate Features and Classifier submodules.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass RefactoredCNN(nn.Module):\n  def setup(self):\n    self.features = Features()\n    self.classifier = Classifier()\n\n  def __call__(self, x):\n    x = self.features(x)\n    x = self.classifier(x)\n    return x\n\nclass Features(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    return x\n\nclass Classifier(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    x = nn.log_softmax(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Step with Optax in Python\nDESCRIPTION: Demonstrates a complete training step implementation using Optax, including loss calculation, gradient computation, and parameter updates.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.jit, static_argnums=(4, 5))\ndef train_step(opt_state, variables, inputs, labels, apply_fn, tx_update_fn):\n\n  def loss_fn(params):\n    logits, new_model_state = apply_fn(\n        {**variables, 'params': params}, inputs, mutable=['batch_stats'])\n    loss = xent_loss(logits, labels)\n    return loss, new_model_state\n\n  variables, params = variables.pop('params')\n  (loss, new_model_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n      params)\n  updates, new_opt_state = tx_update_fn(grads, opt_state, params)\n  new_params = optax.apply_updates(params, updates)\n  new_variables = {**variables, **new_model_state, 'params': new_params}\n  return new_opt_state, new_variables, loss\n\n\nopt_state = tx.init(variables['params'])\nfor batch in ds.as_numpy_iterator():\n  opt_state, variables, loss = train_step(\n      opt_state, variables, batch['image'], batch['label'], model.apply,\n      tx.update)\n  print(loss)\n```\n\n----------------------------------------\n\nTITLE: Implementing Explicit Dense Layer\nDESCRIPTION: Explicit implementation of Dense layer with parameter declaration in setup(). Shows explicit shape handling without inference.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass ExplicitDense(nn.Module):\n  features_in: int\n  features: int\n  kernel_init: Callable = nn.initializers.lecun_normal()\n  bias_init: Callable = nn.initializers.zeros_init()\n\n  def setup(self):\n    self.kernel = self.param('kernel',\n                             self.kernel_init,\n                             (self.features_in, self.features))\n    self.bias = self.param('bias', self.bias_init, (self.features,))\n\n  def __call__(self, inputs):\n    y = lax.dot_general(inputs, self.kernel,\n                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n    y = y + self.bias\n    return y\n```\n\n----------------------------------------\n\nTITLE: Flexible Dropout Implementation with Parameter Merging\nDESCRIPTION: Demonstrates a solution for supporting both constructor and call-time arguments using nn.merge_param. This implementation allows deterministic to be passed either as a dataclass attribute or method argument, but enforces mutual exclusivity.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/arguments.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyDropout(nn.Module):\n  drop_rate: float\n  deterministic: Optional[bool] = None\n\n  @nn.compact\n  def __call__(self, x, deterministic=None):\n    deterministic = nn.merge_param('deterministic', self.deterministic, deterministic)\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Splitting PRNG Keys with Decorator\nDESCRIPTION: Demonstration of splitting random state using nnx.split_rngs decorator for handling multiple replicas.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrngs = nnx.Rngs(params=0, dropout=1)\n\n@nnx.split_rngs(splits=5, only='dropout')\ndef f(rngs: nnx.Rngs):\n  print('Inside:')\n  # rngs.dropout() # ValueError: fold_in accepts a single key...\n  nnx.display(rngs)\n\nf(rngs)\n\nprint('Outside:')\nrngs.dropout() # works!\nnnx.display(rngs)\n```\n\n----------------------------------------\n\nTITLE: Extracting Intermediate Gradients in Flax\nDESCRIPTION: Shows how to extract gradients of intermediate values using the Module.perturb() method for debugging purposes. Includes model definition, initialization, and gradient computation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n      x = nn.relu(nn.Dense(8)(x))\n      x = self.perturb('hidden', x)\n      x = nn.Dense(2)(x)\n      x = self.perturb('logits', x)\n      return x\n```\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.empty((1, 4)) # random data\n  y = jnp.empty((1, 2)) # random data\n\n  model = Model()\n  variables = model.init(jax.random.key(1), x)\n  params, perturbations = variables['params'], variables['perturbations']\n```\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(params, perturbations, x, y):\n    y_pred = model.apply({'params': params, 'perturbations': perturbations}, x)\n    return jnp.mean((y_pred - y) ** 2)\n\n  intermediate_grads = jax.grad(loss_fn, argnums=1)(params, perturbations, x, y)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model with Immutable State in Linen\nDESCRIPTION: Demonstrates how to evaluate a model with immutable batch statistics using the Linen API. It uses mutable=False to ensure immutability and raise errors if accidentally mutated.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef eval_step(params, batch_stats, batch):\n  variables = {'params': params, 'batch_stats': batch_stats}\n  logits = ResNet(train=False).apply(\n    variables, batch['image'], mutable=False)\n  return compute_metrics(logits, batch['label'])\n```\n\n----------------------------------------\n\nTITLE: Setting up dependencies and checkpoint directory for Flax NNX and Orbax\nDESCRIPTION: Imports necessary libraries, sets up a checkpoint directory, and defines a simple TwoLayerMLP model using Flax NNX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport orbax.checkpoint as ocp\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\n\nckpt_dir = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\n```\n\n----------------------------------------\n\nTITLE: Graph Updates Propagation in nnx.vmap\nDESCRIPTION: Demonstrates how nnx.vmap can propagate complex graph structure updates, including adding, deleting, and modifying attributes of the input Module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Count(nnx.Variable): pass\n\nclass Weights(nnx.Module):\n  def __init__(self, kernel: jax.Array, bias: jax.Array, count: jax.Array):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n    self.count = Count(count)\n\nweights = Weights(\n  kernel=random.uniform(random.key(0), (10, 2, 3)),\n  bias=jnp.zeros((10, 3)),\n  count=jnp.arange(10),\n)\nx = jax.random.normal(random.key(1), (10, 2))\n\ndef crazy_vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  weights.count += 1\n  y = x @ weights.kernel + weights.bias\n  weights.some_property = ['a', 2, False] # add attribute\n  del weights.bias # delete attribute\n  weights.new_param = weights.kernel # share reference\n  return y\n\ny = nnx.vmap(crazy_vector_dot, in_axes=0, out_axes=1)(weights, x)\n\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Using Infinite Padding for Multi-Host Evaluation\nDESCRIPTION: Demonstrates a solution for complex multi-host scenarios where hosts might have different numbers of batches, using masks to identify when all real examples have been processed.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncorrect = total = 0\nfor batch in ds.as_numpy_iterator():\n  n = count_p(batch['mask'])[0].item()  # adds sync barrier\n  if not n: break\n\n  preds = get_preds(vs, batch['image']).argmax(axis=-1)\n  total += n\n  correct += count_correct_p(batch['label'], preds, batch['mask'])[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Model with Random State\nDESCRIPTION: Example model class showing usage of params and dropout PRNG streams in a neural network implementation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nnx.Module):\n  def __init__(self, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(20, 10, rngs=rngs)\n    self.drop = nnx.Dropout(0.1, rngs=rngs)\n\n  def __call__(self, x):\n    return nnx.relu(self.drop(self.linear(x)))\n\nmodel = Model(nnx.Rngs(params=0, dropout=1))\n\ny = model(x=jnp.ones((1, 20)))\nprint(f'{y.shape = }')\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint with Orbax PyTreeCheckpointer\nDESCRIPTION: This snippet demonstrates how to save a checkpoint using Orbax's PyTreeCheckpointer. It includes the use of save_args for performance optimization by bundling smaller arrays.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.training import orbax_utils\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(ckpt)\norbax_checkpointer.save('/tmp/flax_ckpt/orbax/single_save', ckpt, save_args=save_args)\n```\n\n----------------------------------------\n\nTITLE: Creating Scanned MLP with Unique Parameters\nDESCRIPTION: Shows how to create a scanned residual MLP block with unique parameters for each layer using nn.scan.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((3, 2))\n\nclass ResidualMLPBlock(nn.Module):\n  @nn.compact\n  def __call__(self, x, _):\n    h = nn.Dense(features=2)(x)\n    h = nn.relu(h)\n    return x + h, None # return an empty carry\n\nScanMLP = nn.scan(\n      ResidualMLPBlock, variable_axes={'params': 0},\n      variable_broadcast=False, split_rngs={'params': True},\n      length=3)\n\nScanMLP().init(jax.random.key(0), x, None) # pass in an empty carry\n```\n\n----------------------------------------\n\nTITLE: Defining Compact MLP Module\nDESCRIPTION: Compact implementation of Multi-Layer Perceptron using @compact decorator. Demonstrates inline submodule declaration with optional naming.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleMLP(nn.Module):\n  features: Sequence[int]\n\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      x = nn.Dense(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset from Hugging Face for Jax+Flax\nDESCRIPTION: Defines a function to load MNIST dataset using Hugging Face datasets, converting the data to jax.numpy arrays with proper formatting. The function loads the data, rescales pixel values from [0,255] to [0,1], and adds a channel dimension for CNN compatibility.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_dataset_hf():\n    mnist = load_dataset(\"mnist\")\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': np.array([np.array(im) for im in mnist[split]['image']]),\n            'label': np.array(mnist[split]['label'])\n        }\n\n        # cast to jnp and rescale pixel values\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        # append trailing channel dimension\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']\n```\n\n----------------------------------------\n\nTITLE: Examining Unsharded Model State in Flax\nDESCRIPTION: Demonstrates how to create an unsharded model and inspect its sharding annotations, showing that while annotations exist, the actual arrays are not yet sharded.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nunsharded_model = DotReluDot(1024, rngs=nnx.Rngs(0))\n\n# You have annotations stuck there, yay!\nprint(unsharded_model.dot1.kernel.sharding)     # (None, 'model')\nprint(unsharded_model.w2.sharding)              # ('model', None)\n\n# But the actual arrays are not sharded?\nprint(unsharded_model.dot1.kernel.value.sharding)  # SingleDeviceSharding\nprint(unsharded_model.w2.value.sharding)           # SingleDeviceSharding\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch FC Layers to Flax Dense Layers\nDESCRIPTION: This snippet demonstrates how to convert a PyTorch Linear layer to a Flax Dense layer. It shows the necessary weight transposition and how to apply the converted layer.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nt_fc = torch.nn.Linear(in_features=3, out_features=4)\n\nkernel = t_fc.weight.detach().cpu().numpy()\nbias = t_fc.bias.detach().cpu().numpy()\n\n# [outC, inC] -> [inC, outC]\nkernel = jnp.transpose(kernel, (1, 0))\n\nkey = random.key(0)\nx = random.normal(key, (1, 3))\n\nvariables = {'params': {'kernel': kernel, 'bias': bias}}\nj_fc = nn.Dense(features=4)\nj_out = j_fc.apply(variables, x)\n\nt_x = torch.from_numpy(np.array(x))\nt_out = t_fc(t_x)\nt_out = t_out.detach().cpu().numpy()\n\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Implementing LoRA Linear Layer in Linen vs NNX\nDESCRIPTION: A comparison of implementing Low-Rank Adaptation (LoRA) layers between Flax Linen and NNX. The Linen implementation fails when trying to replace a linear layer in an existing model, while NNX allows direct replacement as parameters are part of the module structure.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LoraLinear(nn.Module):\n  linear: nn.Dense\n  rank: int\n\n  @nn.compact\n  def __call__(self, x: jax.Array):\n    A = self.param(random.normal, (x.shape[-1], self.rank))\n    B = self.param(random.normal, (self.rank, self.linear.features))\n\n    return self.linear(x) + x @ A @ B\n\ntry:\n  model = Block(train=True)\n  model.linear = LoraLinear(model.linear, rank=5) # <-- ERROR\n\n  lora_params = model.linear.init(random.key(1), x)\n  lora_params['linear'] = params['linear']\n  params['linear'] = lora_params\n\nexcept AttributeError as e:\n  pass\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LoraParam(nnx.Param): pass\n\nclass LoraLinear(nnx.Module):\n  def __init__(self, linear, rank, rngs):\n    self.linear = linear\n    self.A = LoraParam(random.normal(rngs(), (linear.in_features, rank)))\n    self.B = LoraParam(random.normal(rngs(), (rank, linear.out_features)))\n\n  def __call__(self, x: jax.Array):\n    return self.linear(x) + x @ self.A @ self.B\n\nrngs = nnx.Rngs(0)\nmodel = Block(rngs)\nmodel.linear = LoraLinear(model.linear, rank=5, rngs=rngs)\n```\n\n----------------------------------------\n\nTITLE: Decomposing a Module into GraphDef and State in Python\nDESCRIPTION: Shows how to use nnx.split to decompose a module into its GraphDef (static structure) and State (variable values). This is a fundamental part of using modules with JAX transformations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ngraphdef, state = nnx.split(model)\n\nnnx.display(graphdef, state)\n```\n\n----------------------------------------\n\nTITLE: Counter Variable Implementation Comparison\nDESCRIPTION: Demonstrates how to implement a counter variable in both Flax Linen and NNX, showing differences in variable handling and module structure.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(nnx.Variable): pass\n\nclass Block(nnx.Module):\n  def __init__(self, in_features: int , out_features: int, rngs: nnx.Rngs):\n    self.linear = nnx.Linear(in_features, out_features, rngs=rngs)\n    self.batchnorm = nnx.BatchNorm(\n      num_features=out_features, momentum=0.99, rngs=rngs\n    )\n    self.count = Counter(jnp.array(0))\n\n  def __call__(self, x):\n    x = self.linear(x)\n    x = self.batchnorm(x)\n    self.count += 1\n    x = jax.nn.relu(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Training Step and Compilation in Flax Linen and NNX\nDESCRIPTION: Illustrates the differences in writing and compiling a training step between Flax Linen and Flax NNX. Linen uses JAX transforms, while NNX uses its own transforms that work with stateful NNX Modules.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(key, params, inputs, labels):\n  def loss_fn(params):\n    logits = model.apply(\n      {'params': params},\n      inputs, training=True, # <== inputs\n      rngs={'dropout': key}\n    )\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(params)\n\n  params = jax.tree.map(lambda p, g: p - 0.1 * g, params, grads)\n  return params\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel.train() # Sets ``deterministic=False` under the hood for nnx.Dropout\n\n@nnx.jit\ndef train_step(model, inputs, labels):\n  def loss_fn(model):\n    logits = model(inputs)\n\n\n\n\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n```\n\n----------------------------------------\n\nTITLE: Implementing LSTM Layer with Manual Scan in Flax\nDESCRIPTION: Example of manually implementing an LSTM layer using nn.scan and LSTMCell in Flax. This demonstrates the complexity that the proposed RNN layers aim to simplify.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@nn.compact\ndef __call__(self, x):\n  LSTM = nn.scan(\n    nn.LSTMCell, variable_broadcast=\"params\", split_rngs={\"params\": False}\n  )\n  carry = LSTM.initialize_carry(\n    jax.random.key(0), batch_dims=x.shape[:1], size=self.hidden_size\n  )\n  carry, x = LSTM()(carry, x)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Implementing LayerNorm in Flax\nDESCRIPTION: LayerNorm normalizes the inputs across the features dimension. It's often used in transformer architectures and other deep neural networks.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/normalization.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import LayerNorm\n```\n\n----------------------------------------\n\nTITLE: Restoring Checkpoint with Orbax PyTreeCheckpointer\nDESCRIPTION: This code demonstrates how to restore a checkpoint using Orbax's PyTreeCheckpointer. It restores the checkpoint to its raw pytree format.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nraw_restored = orbax_checkpointer.restore('/tmp/flax_ckpt/orbax/single_save')\nraw_restored\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained CLIP Model Function\nDESCRIPTION: Creates a function to load a pretrained CLIP model from HuggingFace and extract its Flax Module and parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nfrom IPython.display import clear_output\nfrom transformers import FlaxCLIPModel\n\ndef load_model():\n  clip = FlaxCLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n  clear_output(wait=False)\n  module = clip.module\n  variables = {'params': clip.params}\n  return module, variables\n```\n\n----------------------------------------\n\nTITLE: Proposed ConvLSTM API Usage in Flax\nDESCRIPTION: Example showing the simplified API proposed in this FLIP, where the ConvLSTMCell can infer batch and feature dimensions from a sample input.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 4, 4, 3)) # (batch, *image_shape, channels)\n\nlstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))\ncarry = lstm.initialize_carry(key1, input_shape=x.shape)\n\n(carry, y), initial_params = lstm.init_with_output(key2, carry, x)\n```\n\n----------------------------------------\n\nTITLE: Current NNX vmap usage with misleading JAX-style syntax\nDESCRIPTION: An example of how users might attempt to use NNX vmap with JAX-style in_axes, which currently doesn't work as expected because Modules are treated as a single unit rather than PyTrees.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(1, 0))\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Auto-Splitting Random State with nnx.split_rngs Decorator\nDESCRIPTION: Demonstrates how to use the nnx.split_rngs decorator to automatically split random state before vectorization, simplifying the handling of random state in transforms. This approach makes the RngState usable outside of vmap operations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nweights = Weights(\n  kernel=random.uniform(random.key(0), (2, 3)),\n  bias=jnp.zeros((3,)),\n  count=jnp.array(0),\n  seed=0,\n)\nx = random.normal(random.key(1), (10, 2))\n\nstate_axes = nnx.StateAxes({nnx.RngState: 0, (nnx.Param, Count): None})\n\n@nnx.split_rngs(splits=10)\n@nnx.vmap(in_axes=(state_axes, 0))\ndef noisy_vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  weights.count += 1\n  y = x @ weights.kernel + weights.bias\n  return y + random.normal(weights.rngs.noise(), y.shape)\n\ny1 = noisy_vector_dot(weights, x)\ny2 = noisy_vector_dot(weights, x)\n\nprint(jnp.allclose(y1, y2))\nnnx.display(weights)\n```\n\n----------------------------------------\n\nTITLE: Vectorized Dense Layer with Unique Parameters\nDESCRIPTION: Creates a batched Dense layer with unique parameters for each batch input by splitting the RNG key across the batch axis.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nBatchDense = nn.vmap(\n    nn.Dense,\n    in_axes=0, out_axes=0,\n    variable_axes={'params': 0},\n    split_rngs={'params': True})\n\nBatchDense(2).init(jax.random.key(0), x)\n```\n\n----------------------------------------\n\nTITLE: Multi-Optimizer Configuration with Optax Masking in Python\nDESCRIPTION: Shows how to implement different optimization strategies for different parameter groups using Optax's masked transformations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef flattened_traversal(fn):\n  def mask(data):\n    flat = traverse_util.flatten_dict(data)\n    return traverse_util.unflatten_dict({k: fn(k, v) for k, v in flat.items()})\n  return mask\n\ntx = optax.chain(\n    optax.masked(optax.sgd(learning_rate=0.1),\n                 mask=flattened_traversal(lambda path, _: path[-1] == 'bias')),\n    optax.masked(optax.sgd(learning_rate=0.05),\n                 mask=flattened_traversal(lambda path, _: path[-1] != 'bias')),\n)\n```\n\n----------------------------------------\n\nTITLE: Scanned MLP with Shared Parameters\nDESCRIPTION: Creates a scanned residual MLP block where parameters are shared across layers by disabling RNG splitting.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nScanMLP = nn.scan(\n      ResidualMLPBlock, variable_axes={'params': 0},\n      variable_broadcast=False, split_rngs={'params': False},\n      length=3)\n\nScanMLP().init(jax.random.key(0), x, None)\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Initializing a Dense Layer in Flax\nDESCRIPTION: This code demonstrates how to create a Dense layer, initialize its variables, and apply it to input data using Flax Linen.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Dense(features=3)\n\n# Make RNG Keys and a fake input.\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\n# provide key and fake input to get initialized variables\ninit_variables = model.init(key2, x)\n\ninit_variables\n```\n\nLANGUAGE: python\nCODE:\n```\ny = model.apply(init_variables, x)\ny\n```\n\n----------------------------------------\n\nTITLE: Model with Selective RNG Stream Splitting\nDESCRIPTION: Shows how to split only specific RNG streams while keeping others shared across batch inputs.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nBatchModel = nn.vmap(\n    Model,\n    in_axes=0, out_axes=0,\n    variable_axes={'params': 0, 'other_collection': 0},\n    split_rngs={'params': True, 'other': False})\n\nBatchModel().init({'params': jax.random.key(0), 'other': jax.random.key(1)}, x)\n```\n\n----------------------------------------\n\nTITLE: Example of consistent aliasing in vmap with multiple Module references\nDESCRIPTION: A valid example of consistent aliasing where the same Module appears multiple times but with the same lifting/lowering specification.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(m1_axes, m2_axes, m1_axes), out_axes=m2_axes)\ndef f(m1, m2, m1_alias):\n  return m2\n\nm2 = f(m1, m2, m1)\n```\n\n----------------------------------------\n\nTITLE: Module __init_subclass__ Implementation\nDESCRIPTION: Implementation details for the Module's __init_subclass__ method to handle kw_only parameter and dataclass transformation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Module(ModuleBase):\n  def __init_subclass__(self, kw_only: Optional[bool] = None):\n    # ...\n    if kw_only:\n     if is_python_310_or_above():\n       dataclass_transform_args = {'kw_only': True}\n     else:\n       raise TypeError(\"Can't use `kw_only` before Py3.10.\")\n    else:\n       dataclass_transform_args = {}\n\n    kw_only_dataclasses.dataclass(\n      cls, unsafe_hash='__hash__' not in cls.__dict__,\n      repr=False,\n      **dataclass_transform_args)\n```\n\n----------------------------------------\n\nTITLE: Top-level Training Code Patterns in Flax/Linen\nDESCRIPTION: Demonstrates the transition from old-style model creation and training setup to new Linen patterns using TrainState and direct parameter management.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef create_model(key):\n  _, initial_params = CNN.init_by_shape(\n    key, [((1, 28, 28, 1), jnp.float32)])\n  model = nn.Model(CNN, initial_params)\n  return model\n\ndef create_optimizer(model, learning_rate):\n  optimizer_def = optim.Momentum(learning_rate=learning_rate)\n  optimizer = optimizer_def.create(model)\n  return optimizer\n\ndef loss_fn(model):\n  logits = model(batch['image'])\n  one_hot = jax.nn.one_hot(batch['label'], num_classes=10)\n  loss = -jnp.mean(jnp.sum(one_hot_labels * batch['label'],\n                           axis=-1))\n  return loss, logits\n```\n\nLANGUAGE: Python\nCODE:\n```\ndef create_train_state(rng, config):\n  variables = CNN().init(rng, jnp.ones([1, 28, 28, 1]))\n  params = variables['params']\n  tx = optax.sgd(config.learning_rate, config.momentum)\n  return train_state.TrainState.create(\n      apply_fn=CNN.apply, params=params, tx=tx)\n\n\ndef loss_fn(params):\n  logits = CNN().apply({'params': params}, batch['image'])\n  one_hot = jax.nn.one_hot(batch['label'], 10)\n  loss = jnp.mean(optax.softmax_cross_entropy(logits=logits,\n                                              labels=one_hot))\n  return loss, logits\n```\n\n----------------------------------------\n\nTITLE: Idiomatic LSTM Module Implementation with the New API\nDESCRIPTION: Demonstrates a more idiomatic approach to creating an LSTM module with the new API, using a features attribute and initializing a scanned cell in setup.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleLSTM(nn.Module):\n\n  @functools.partial(\n    nn.transforms.scan,\n    variable_broadcast='params',\n    in_axes=1, out_axes=1,\n    split_rngs={'params': False})\n  @nn.compact\n  def __call__(self, carry, x):\n    return nn.OptimizedLSTMCell()(carry, x)\n\n  @staticmethod\n  def initialize_carry(batch_dims, hidden_size):\n    return nn.OptimizedLSTMCell.initialize_carry(\n      jax.random.key(0), batch_dims, hidden_size)\n\nmodel = SimpleLSTM()\ncarry = SimpleLSTM.initialize_carry((batch_size,), out_features)\nvariables = model.init(jax.random.key(0), carry, x)\n\n---\n\nclass SimpleLSTM(nn.Module):\n  features: int\n\n  def setup(self):\n    self.scan_cell = nn.transforms.scan(\n      nn.OptimizedLSTMCell,\n      variable_broadcast='params',\n      in_axes=1, out_axes=1,\n      split_rngs={'params': False})(self.features)\n\n\n  @nn.compact\n  def __call__(self, x):\n    carry = self.scan_cell.initialize_carry(jax.random.key(0), x[:, 0].shape)\n    return self.scan_cell(carry, x)[1]  # only return the output\n\n\nmodel = SimpleLSTM(features=out_features)\nvariables = model.init(jax.random.key(0), x)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Flax Partitioned Parameters\nDESCRIPTION: This snippet demonstrates how to inspect and manipulate Flax partitioned parameters, including accessing raw JAX arrays and modifying them while preserving sharding annotations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel']))\nprint(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value))\nprint(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].names)\nprint(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.shape)\n\n# Say for some unknown reason you want to make the whole param tree all-zero\nunboxed_params = nn.meta.unbox(initialized_state.params)\nall_zero = jax.tree.map(jnp.zeros_like, unboxed_params)\nall_zero_params = nn.meta.replace_boxed(initialized_state.params, all_zero)\nassert jnp.sum(nn.meta.unbox(all_zero_params['DotReluDot_0']['Dense_0']['kernel'])) == 0\n```\n\n----------------------------------------\n\nTITLE: Using capture_intermediates in Flax\nDESCRIPTION: Demonstrates how to use the capture_intermediates feature in Flax to automatically capture intermediate return values from submodules without code changes. This example checks if any intermediate activations are non-finite.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@jax.jit\ndef init(key, x):\n  variables = CNN().init(key, x)\n  return variables\n\n@jax.jit\ndef predict(variables, x):\n  y, state = CNN().apply(variables, x, capture_intermediates=True, mutable=[\"intermediates\"])\n  intermediates = state['intermediates']\n  fin = jax.tree_util.tree_map(lambda xs: jnp.all(jnp.isfinite(xs)), intermediates)\n  return y, fin\n\nvariables = init(jax.random.key(0), batch)\ny, is_finite = predict(variables, batch)\nall_finite = all(jax.tree_util.tree_leaves(is_finite))\nassert all_finite, \"non-finite intermediate detected!\"\n```\n\n----------------------------------------\n\nTITLE: Using BatchNorm in Flax\nDESCRIPTION: BatchNorm is a normalization layer that normalizes the input across the batch dimension. It's commonly used in convolutional neural networks.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/normalization.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import BatchNorm\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Parallelism with Mesh Sharding in JAX/Flax\nDESCRIPTION: Sets up data parallelism by sharding the first dimension (batch) on the 'data' axis and demonstrates basic model execution with sharded inputs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# In data parallelism, the first dimension (batch) will be sharded on the `data` axis.\ndata_sharding = NamedSharding(mesh, PartitionSpec('data', None))\ninput = jax.device_put(jnp.ones((8, 1024)), data_sharding)\n\nwith mesh:\n  output = sharded_model(input)\nprint(output.shape)\njax.debug.visualize_array_sharding(output)  # Also sharded as `('data', None)`.\n```\n\n----------------------------------------\n\nTITLE: Transferring Pretrained Parameters\nDESCRIPTION: Transfers the pretrained vision model parameters to the classifier's backbone.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparams['backbone'] = vision_model_vars['params']\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current ConvLSTM API Usage in Flax\nDESCRIPTION: Example showing the current unintuitive API for initializing and using ConvLSTMCell in Flax, which requires manual calculation of batch and feature dimensions.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 4, 4, 3)) # (batch, *image_shape, channels)\n\n#                                        image shape: vvvvvvv\ncarry = nn.ConvLSTMCell.initialize_carry(key1, (16,), (64, 64, 16))\n#                                   batch size: ^^             ^^ :output features\n\nlstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))\n(carry, y), initial_params = lstm.init_with_output(key2, carry, x)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST datasets\nDESCRIPTION: Loads the MNIST training and test datasets as dictionaries of JAX arrays using the get_datasets function from the train module.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Get datasets as dict of JAX arrays.\ntrain_ds, test_ds = train.get_datasets()\n```\n\n----------------------------------------\n\nTITLE: Updating Stateful Layer with Multiple Inputs in Flax\nDESCRIPTION: This snippet demonstrates how to update the state of a stateful layer in Flax over multiple inputs, showing how to manage and update both parameters and state variables.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfor val in [1.0, 2.0, 3.0]:\n  x = val * jnp.ones((10,5))\n  y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n  old_state, params = flax.core.pop(variables, 'params')\n  variables = flax.core.freeze({'params': params, **updated_state})\n  print('updated state:\\n', updated_state) # Shows only the mutable part\n```\n\n----------------------------------------\n\nTITLE: Simplified Training Step Without Mutable State\nDESCRIPTION: Simplified version of the training step function for models without mutable state like batch statistics.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state, inputs, labels):\n\n  def loss_fn(params):\n    outputs = state.apply_fn({'params': params}, inputs)\n    loss = xent_loss(outputs, labels)\n    return loss\n\n  loss, grads = jax.value_and_grad(loss_fn)(state.params)\n  new_state = state.update(grads=grads)\n\n  return new_state, loss\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing MNIST Dataset - Python\nDESCRIPTION: Loads MNIST dataset using TensorFlow Datasets, normalizes pixel values, and creates batched datasets for training and testing.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\ndef get_datasets(num_epochs, batch_size):\n  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n  train_ds = tfds.load('mnist', split='train')\n  test_ds = tfds.load('mnist', split='test')\n\n  train_ds = train_ds.map(lambda sample: {'image': tf.cast(sample['image'],\n                                                           tf.float32) / 255.,\n                                          'label': sample['label']}) \n  test_ds = test_ds.map(lambda sample: {'image': tf.cast(sample['image'],\n                                                         tf.float32) / 255.,\n                                        'label': sample['label']}) \n\n  train_ds = train_ds.repeat(num_epochs).shuffle(1024)\n  train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n  test_ds = test_ds.shuffle(1024)\n  test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n\n  return train_ds, test_ds\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Checkpointing in Python with Orbax\nDESCRIPTION: Demonstrates how to use orbax.checkpoint.AsyncCheckpointer for asynchronous checkpointing, which can improve performance by saving checkpoints in a background thread while training continues.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njax.distributed.initialize(\"localhost:8889\", num_processes=1, process_id=0)\n\nasync_checkpointer = orbax.checkpoint.AsyncCheckpointer(\n    orbax.checkpoint.PyTreeCheckpointHandler(), timeout_secs=50)\n\n# Save your job:\nasync_checkpointer.save('/tmp/flax_ckpt/orbax/single_save_async', ckpt, save_args=save_args)\n# ... Continue with your work...\n\n# ... Until a time when you want to wait until the save completes:\nasync_checkpointer.wait_until_finished()  # Blocks until the checkpoint saving is completed.\nasync_checkpointer.restore('/tmp/flax_ckpt/orbax/single_save_async', item=target)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX Function Transformation in Flax\nDESCRIPTION: This snippet illustrates the functional representation of Flax Modules, showing how they are automatically cast as explicit functions to handle JAX transformations. It demonstrates the input and output format for variable collections, PRNG state, and data.\nSOURCE: https://github.com/google/flax/blob/main/docs/philosophy.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n$$f \\left( v_{in}, x \\right) \\rightarrow v_{out}, y$$\n```\n\n----------------------------------------\n\nTITLE: Implementing Weight Decay in Optax vs flax.optim\nDESCRIPTION: Comparison of weight decay implementation between flax.optim and optax. In Optax, weight decay can be added as an additional gradient transformation using optax.add_decayed_weights() in the transformation chain.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimizer_def = flax.optim.Adam(\n    learning_rate, weight_decay=weight_decay)\noptimizer = optimizer_def.create(variables['params'])\n```\n\nLANGUAGE: python\nCODE:\n```\n# (Note that you could also use `optax.adamw()` in this case)\ntx = optax.chain(\n    optax.scale_by_adam(),\n    optax.add_decayed_weights(weight_decay),\n    # params -= learning_rate * (adam(grads) + params * weight_decay)\n    optax.scale(-learning_rate),\n)\n# Note that you'll need to specify `params` when computing the udpates:\n# tx.update(grads, opt_state, params)\n```\n\n----------------------------------------\n\nTITLE: Using the Functional API with JAX Transformations in Python\nDESCRIPTION: Demonstrates the split-merge-update pattern for using modules with JAX transformations. The example shows how to propagate state updates from inside a jax.jit transformation back to the original module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f'{model.count.value = }')\n\n# 1. Use `nnx.split` to create a pytree representation of the `nnx.Module`.\ngraphdef, state = nnx.split(model)\n\n@jax.jit\ndef forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array) -> tuple[jax.Array, nnx.State]:\n  # 2. Use `nnx.merge` to create a new model inside the JAX transformation.\n  model = nnx.merge(graphdef, state)\n  # 3. Call the `nnx.Module`\n  y = model(x)\n  # 4. Use `nnx.split` to propagate `nnx.State` updates.\n  _, state = nnx.split(model)\n  return y, state\n\ny, state = forward(graphdef, state, x=jnp.ones((1, 3)))\n# 5. Update the state of the original `nnx.Module`.\nnnx.update(model, state)\n\nprint(f'{model.count.value = }')\n```\n\n----------------------------------------\n\nTITLE: RNG Management with Shard Map in Flax\nDESCRIPTION: Demonstrates using shard_map for RNG management across multiple devices, including proper key splitting and sharding specifications.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef forward(variables, x, add_noise, rng_key_batch):\n  return module.apply(\n    variables, x, add_noise, rngs={'params': rng_key_batch[0]}\n  )\n\nshmap_forward = shard_map(\n  forward,\n  mesh=mesh,\n  in_specs=(no_pspec, data_pspec, no_pspec, data_pspec),\n  out_specs=data_pspec,\n)\n```\n\n----------------------------------------\n\nTITLE: Proposed LSTMCell Usage Example\nDESCRIPTION: Example of how the refactored LSTMCell would be used, showing how features are now specified during cell construction and how initialize_carry uses a sample input.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 100, 10)) # (batch, time, features)\n\ncell = nn.LSTMCell(features=32)\ncarry = cell.initialize_carry(PRNGKey(0), x[:, 0]) # sample input\n\n(carry, y), variables = cell.init_with_output(PRNGKey(1), carry, x)\n```\n\n----------------------------------------\n\nTITLE: Usage Example for Proposed RNNCell Design\nDESCRIPTION: Demonstrates how the proposed RNNCell redesign would be used in practice. It shows the initialization of an LSTM layer and the process of creating and using the carry state.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nLSTM = nn.scan(\n  nn.LSTMCell, variable_broadcast='params', \n  split_rngs={'dropout': True})\nlstm = LSTM(features=32)\ncarry = lstm.initialize_carry(x[:, 0])\ncarry, y = lstm(carry, x)\n```\n\n----------------------------------------\n\nTITLE: Multi-host Checkpoint Save Example\nDESCRIPTION: Shows incorrect and correct patterns for saving checkpoints in multi-host environments using Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nif jax.process_index() == 0:\n  flax.training.checkpoints.save_checkpoint(...)\n\n# Correct way:\nflax.training.checkpoints.save_checkpoint_multiprocess(...)\n```\n\n----------------------------------------\n\nTITLE: Utilizing GroupNorm in Flax\nDESCRIPTION: GroupNorm divides the channels into groups and computes within each group the mean and variance for normalization. It's an alternative to BatchNorm that works well with smaller batch sizes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/normalization.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import GroupNorm\n```\n\n----------------------------------------\n\nTITLE: Combining sow() and capture_intermediates in Flax\nDESCRIPTION: Shows how to combine the use of sow() for manually storing non-layer intermediates with capture_intermediates for automatically capturing layer outputs. Demonstrates filtering of captured intermediates.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/extracting_intermediates.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    a = nn.Dense(4)(x) # Dense_0\n    b = nn.Dense(4)(x) # Dense_1\n    c = a + b\n    self.sow('intermediates', 'c', c) # store intermediate c\n    d = nn.Dense(4)(c) # Dense_2\n    return d\n\n@jax.jit\ndef init(key, x):\n  variables = Model().init(key, x)\n  return variables['params']\n\n@jax.jit\ndef predict(params, x):\n  # filter specifically for only the Dense_0 and Dense_2 layer\n  filter_fn = lambda mdl, method_name: isinstance(mdl.name, str) and (mdl.name in {'Dense_0', 'Dense_2'})\n  return Model().apply({\"params\": params}, x, capture_intermediates=filter_fn)\n\nbatch = jax.random.uniform(jax.random.key(1), (1,3))\nparams = init(jax.random.key(0), batch)\npreds, feats = predict(params, batch)\nfeats # intermediate c in Model is stored and isn't filtered out by the filter function\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Transposed Convolutions to Flax\nDESCRIPTION: This code demonstrates how to convert PyTorch transposed convolutions to Flax. It shows the necessary kernel transposition and the use of the 'transpose_kernel' argument in Flax's ConvTranspose layer.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# padding is inverted\ntorch_padding = 0\nflax_padding = 1 - torch_padding\n\nt_conv = torch.nn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=2, padding=torch_padding)\n\nkernel = t_conv.weight.detach().cpu().numpy()\nbias = t_conv.bias.detach().cpu().numpy()\n\n# [inC, outC, kH, kW] -> [kH, kW, outC, inC]\nkernel = jnp.transpose(kernel, (2, 3, 1, 0))\n\nkey = random.key(0)\nx = random.normal(key, (1, 6, 6, 3))\n\nvariables = {'params': {'kernel': kernel, 'bias': bias}}\n# ConvTranspose expects the kernel to be [kH, kW, inC, outC],\n# but with `transpose_kernel=True`, it expects [kH, kW, outC, inC] instead\nj_conv = nn.ConvTranspose(features=4, kernel_size=(2, 2), padding=flax_padding, transpose_kernel=True)\nj_out = j_conv.apply(variables, x)\n\n# [N, H, W, C] -> [N, C, H, W]\n```\n\n----------------------------------------\n\nTITLE: Implementing Partitioned Class for PartitionSpecs in Python\nDESCRIPTION: This code snippet demonstrates an implementation of the AxisMetadata abstract base class called Partitioned. It's designed to keep track of PartitionSpecs used by pjit, illustrating how the proposed API can be extended for specific use cases.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Partitioned(flax.struct.PyTreeNode, AxisMetadata):\n  value: Any\n  names: Tuple[Optional[str], ...] = flax.struct.field(pytree_node=False)\n\n  def add_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:\n    axis_name = self._get_partition_name(params)\n    names = list(self.names)\n    names.insert(index, axis_name)\n    return self.replace(names=tuple(names))\n\n  def remove_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:\n    axis_name = self._get_partition_name(params)\n    names = list(self.names)\n    assert names.pop(index) == axis_name\n    return self.replace(names=tuple(names))\n\ndef with_partitioning(init_fn, names):\n  def wrapper(*args, **kwargs):\n    return Partitioned(init_fn(*args, **kwargs), names)\n  return wrapper\n```\n\n----------------------------------------\n\nTITLE: TPU/GPU Device Setup for JAX\nDESCRIPTION: Configures the compute device (TPU or GPU) for JAX and sets up necessary environment variables.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/sst2.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntry:\n  import jax.tools.colab_tpu\n  jax.tools.colab_tpu.setup_tpu()\nexcept KeyError:\n  print('\\n### NO TPU CONNECTED - USING CPU or GPU ###\\n')\n  import os\n  os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\njax.devices()\n```\n\n----------------------------------------\n\nTITLE: Handling Input-Output Aliasing in Flax NNX Transformations\nDESCRIPTION: Demonstrates how Flax NNX prevents inconsistent aliasing between inputs and outputs in transformations. This example shows an error when attempting to vectorize the same object with different axes for input and output.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=1)\ndef f(arg1):\n  return arg1\n\ntry:\n  f(arg1)\nexcept ValueError as e:\n  print(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Optimizer Strategy with Optax Masked Transformations in Python\nDESCRIPTION: Implementation of a multi-optimizer strategy using Optax's masked transformations. This allows applying different optimizers to different parts of the parameter tree based on parameter paths.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef flattened_traversal(fn):\n  def mask(data):\n    flat = traverse_util.flatten_dict(data)\n    return traverse_util.unflatten_dict({k: fn(k, v) for k, v in flat.items()})\n  return mask\n\ntx = optax.chain(\n    optax.masked(optax.sgd(learning_rate=0.1),\n                 mask=flattened_traversal(lambda path, _: path[-1] == 'bias')),\n    optax.masked(optax.sgd(learning_rate=0.05),\n                 mask=flattened_traversal(lambda path, _: path[-1] != 'bias')),\n)\n```\n\n----------------------------------------\n\nTITLE: Restoring and Verifying Modified Neural Network Model in Flax\nDESCRIPTION: This code snippet demonstrates the process of restoring a modified neural network model in Flax. It creates an abstract model, splits it into graph definition and abstract state, replaces the abstract state with a restored pure dictionary, and merges the components back together. The functionality of the restored model is then verified by checking its output shape.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# Same restore code as above.\nabstract_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\ngraphdef, abstract_state = nnx.split(abstract_model)\nnnx.replace_by_pure_dict(abstract_state, restored_pure_dict)\nmodel = nnx.merge(graphdef, abstract_state)\nassert model(x).shape == (3, 4)  # The new model works!\n\nnnx.display(model.linear1)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Visualizing Logical Partitioning in Flax\nDESCRIPTION: Shows how to initialize a model with logical partitioning using JIT compilation and visualize the resulting array sharding patterns for model weights.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlogical_jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),\n                      in_shardings=(mesh_sharding(PartitionSpec()), x_sharding),  # PRNG key and x\n                      out_shardings=logical_state_sharding)\n\nlogical_initialized_state = logical_jit_init_fn(k, x, logical_model, optimizer)\n\nprint(f'Sharding of Weight 1:')\njax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['Dense_0']['kernel'].value)\nprint(f'Sharding of Weight 2:')\njax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['W2'].value)\n```\n\n----------------------------------------\n\nTITLE: Implementing Block and MLP Modules with Layer Scanning in Flax\nDESCRIPTION: Implementation of Block and MLP modules in Flax using nn.scan for iterative layer application. The Block module uses Dense layer, Dropout and ReLU activation, while MLP scans multiple Block instances.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nn.Module):\n  features: int\n  training: bool\n\n  @nn.compact\n  def __call__(self, x, _):\n    x = nn.Dense(self.features)(x)\n    x = nn.Dropout(0.5)(x, deterministic=not self.training)\n    x = jax.nn.relu(x)\n    return x, None\n\nclass MLP(nn.Module):\n  features: int\n  num_layers: int\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    ScanBlock = nn.scan(\n      Block, variable_axes={'params': 0}, split_rngs={'params': True},\n      length=self.num_layers)\n\n    y, _ = ScanBlock(self.features, training)(x, None)\n    return y\n```\n\n----------------------------------------\n\nTITLE: Running Model Training and Evaluation\nDESCRIPTION: Executes the training and evaluation process, returning the final model state.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nstate = train.train_and_evaluate(workdir=workdir)\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer and Metrics\nDESCRIPTION: Sets up an AdamW optimizer and defines metrics for tracking accuracy and loss during training.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport optax\n\nlearning_rate = 0.005\nmomentum = 0.9\n\noptimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\nmetrics = nnx.MultiMetric(\n  accuracy=nnx.metrics.Accuracy(),\n  loss=nnx.metrics.Average('loss'),\n)\n\nnnx.display(optimizer)\n```\n\n----------------------------------------\n\nTITLE: Adding Sharding Rules to Existing Flax NNX Model State\nDESCRIPTION: Demonstrates how to add sharding rules to an existing model's state when rules weren't fully provided during model definition. Uses tree mapping to update variable states with sharding rules.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef add_sharding_rule(vs: nnx.VariableState) -> nnx.VariableState:\n  vs.sharding_rules = sharding_rules\n  return vs\n\n@nnx.jit\ndef create_sharded_logical_model():\n  model = LogicalDotReluDot(1024, rngs=nnx.Rngs(0))\n  state = nnx.state(model)\n  state = jax.tree.map(add_sharding_rule, state,\n                       is_leaf=lambda x: isinstance(x, nnx.VariableState))\n  pspecs = nnx.get_partition_spec(state)\n  sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n  nnx.update(model, sharded_state)\n  return model\n\nwith mesh:\n  sharded_logical_model = create_sharded_logical_model()\n\njax.debug.visualize_array_sharding(sharded_logical_model.dot1.kernel.value)\njax.debug.visualize_array_sharding(sharded_logical_model.w2.value)\n\n# Check out their equivalency with some easier-to-read sharding descriptions.\nassert sharded_logical_model.dot1.kernel.value.sharding.is_equivalent_to(\n  NamedSharding(mesh, PartitionSpec(None, 'model')), ndim=2\n)\nassert sharded_logical_model.w2.value.sharding.is_equivalent_to(\n  NamedSharding(mesh, PartitionSpec('model', None)), ndim=2\n)\n\nwith mesh:\n  logical_output = sharded_logical_model(input)\n  assert logical_output.sharding.is_equivalent_to(\n    NamedSharding(mesh, PartitionSpec('data', None)), ndim=2\n  )\n```\n\n----------------------------------------\n\nTITLE: Initialize Carry Method Implementation\nDESCRIPTION: Example of proposed RNNCell redesign with initialize_carry as an instance method, allowing direct hyperparameter passing to the cell.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_carry(self, sample_input) -> Carry:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Registering Custom NNX Variable Types for Collection Mapping\nDESCRIPTION: This code snippet demonstrates how to register custom NNX variable types using nnx.register_variable_name so they are properly mapped to collections when converting to Linen. It creates a Count variable type and shows its usage in a module with multiple collection types.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@nnx.register_variable_name('counts', overwrite=True)\nclass Count(nnx.Variable): pass\n\n\nclass NNXMultiCollections(nnx.Module):\n  def __init__(self, din, dout, rngs):\n    self.w = nnx.Param(nnx.initializers.lecun_normal()(rngs.params(), (din, dout)))\n    self.lora = nnx.LoRA(din, 3, dout, rngs=rngs)\n    self.count = Count(jnp.array(0))\n\n  def __call__(self, x):\n    self.count += 1\n    return (x @ self.w.value) + self.lora(x)\n\nxkey, pkey, dkey = jax.random.split(jax.random.key(0), 3)\nx = jax.random.normal(xkey, (2, 4))\nmodel = bridge.to_linen(NNXMultiCollections, 4, 3)\nvar = model.init({'params': pkey, 'dropout': dkey}, x)\nprint('All Linen collections:', list(var.keys()))\nprint(var['params'])\n```\n\n----------------------------------------\n\nTITLE: Restoring Sharded Checkpoint with Orbax\nDESCRIPTION: Shows how to restore a sharded checkpoint using Orbax with proper sharding specifications through restore_args.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# The reference doesn't need to be as large as your checkpoint!\n# Just make sure it has the `.sharding` you want.\nmp_smaller = jax.device_put(np.arange(8).reshape(4, 2),\n                            NamedSharding(mesh, PartitionSpec('x', 'y')))\nref_ckpt = {'model': mp_smaller}\n\nrestore_args = orbax_utils.restore_args_from_target(ref_ckpt)\nasync_checkpoint_manager.restore(\n    0, items=ref_ckpt, restore_kwargs={'restore_args': restore_args})\n```\n\n----------------------------------------\n\nTITLE: Implementing Bidirectional LSTM in Flax\nDESCRIPTION: This snippet demonstrates how to create a bidirectional LSTM layer using the proposed Bidirectional combinator in Flax, which processes input sequences in both forward and backward directions.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nforward_rnn = nn.RNN(nn.LSTMCell(), cell_size=32)\nbackward_rnn = nn.RNN(nn.LSTMCell(), cell_size=32)\n# Bidirectional combinator.\nbi_rnn = nn.Bidirectional(forward_rnn, backward_rnn)\n# Encodes a batch of input sequences in both directions.\ncarry, outputs = bi_rnn(inputs, seq_lengths)\n```\n\n----------------------------------------\n\nTITLE: Restoring Partial Checkpoints with Orbax Transforms in Python\nDESCRIPTION: Shows how to use Orbax transforms to restore a partial checkpoint into a new data structure. This allows filling in missing fields with default values when the checkpoint structure has changed.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncustom_restore_args = orbax_utils.restore_args_from_target(custom_target)\nrestored = checkpoint_manager.restore(4, items=custom_target,\n                                      restore_kwargs={'transforms': {}, 'restore_args': custom_restore_args})\nassert type(restored['model']) == CustomTrainState\nnp.testing.assert_equal(restored['model'].batch_stats,\n                        custom_target['model'].batch_stats)\nrestored\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch 2D Convolutions to Flax Convolutions\nDESCRIPTION: This code shows how to convert a PyTorch Conv2d layer to a Flax Conv layer. It addresses the differences in input tensor shapes (NCHW vs NHWC) and kernel transposition.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nt_conv = torch.nn.Conv2d(in_channels=3, out_channels=4, kernel_size=2, padding='valid')\n\nkernel = t_conv.weight.detach().cpu().numpy()\nbias = t_conv.bias.detach().cpu().numpy()\n\n# [outC, inC, kH, kW] -> [kH, kW, inC, outC]\nkernel = jnp.transpose(kernel, (2, 3, 1, 0))\n\nkey = random.key(0)\nx = random.normal(key, (1, 6, 6, 3))\n\nvariables = {'params': {'kernel': kernel, 'bias': bias}}\nj_conv = nn.Conv(features=4, kernel_size=(2, 2), padding='valid')\nj_out = j_conv.apply(variables, x)\n\n# [N, H, W, C] -> [N, C, H, W]\nt_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))\nt_out = t_conv(t_x)\n# [N, C, H, W] -> [N, H, W, C]\nt_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))\n\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Model Surgery with NNX using iter_graph\nDESCRIPTION: Demonstrates how to iterate through a model's graph in NNX to replace all Linear layers with custom LoraLinear layers. This approach is more ergonomic than Linen's intercept_methods API for model surgery.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrngs = nnx.Rngs(0)\nmodel = Block(rngs)\n\nfor path, module in nnx.iter_graph(model):\n  if isinstance(module, nnx.Module):\n    for name, value in vars(module).items():\n      if isinstance(value, nnx.Linear):\n        setattr(module, name, LoraLinear(value, rank=5, rngs=rngs))\n```\n\n----------------------------------------\n\nTITLE: Restoring Checkpoint with Legacy Flax\nDESCRIPTION: Shows how to restore a checkpoint using Flax's legacy restore_checkpoint function with async checkpointer.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nmp_restored = checkpoints.restore_checkpoint(ckpt_dir,\n                                             target=ref_ckpt,\n                                             step=3,\n                                             orbax_checkpointer=async_checkpointer)\nmp_restored\n```\n\n----------------------------------------\n\nTITLE: Applying Linen Lifted Transforms on Converted NNX Modules\nDESCRIPTION: This example demonstrates how to apply Linen-style lifted transforms like vmap on a converted NNX module. It shows how to properly specify variable axes and handle the additional 'nnx' collection introduced by the bridge.ToLinen conversion.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass LinenVmapped(nn.Module):\n  dout: int\n  @nn.compact\n  def __call__(self, x):\n    inner = nn.vmap(bridge.ToLinen, variable_axes={'params': 0, 'nnx': None}, split_rngs={'params': True}\n                    )(nnx.Linear, args=(x.shape[-1], self.dout))\n    return inner(x)\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\nmodel = LinenVmapped(64)\nvar = model.init(jax.random.key(0), x)\nprint(var['params']['VmapToLinen_0']['kernel'].shape)  # (4, 32, 64) - leading dim 4 got vmapped\ny = model.apply(var, x)\nprint(y.shape)\n```\n\n----------------------------------------\n\nTITLE: Extending TrainState with Batch Statistics\nDESCRIPTION: Example of extending the base TrainState class to include batch statistics for training neural networks with batch normalization.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(train_state.TrainState):\n  batch_stats: flax.core.FrozenDict[str, Any]\n```\n\n----------------------------------------\n\nTITLE: Defining AutoEncoder Module in Haiku and Flax\nDESCRIPTION: Implements an AutoEncoder class with encode, decode, and __call__ methods in both Haiku and Flax. Shows differences in module initialization and submodule definition.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass AutoEncoder(hk.Module):\n\n  def __init__(self, embed_dim: int, output_dim: int, name=None):\n    super().__init__(name=name)\n    self.encoder = hk.Linear(embed_dim, name=\"encoder\")\n    self.decoder = hk.Linear(output_dim, name=\"decoder\")\n\n  def encode(self, x):\n    return self.encoder(x)\n\n  def decode(self, x):\n    return self.decoder(x)\n\n  def __call__(self, x):\n    x = self.encode(x)\n    x = self.decode(x)\n    return x\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass AutoEncoder(nn.Module):\n  embed_dim: int\n  output_dim: int\n\n  def setup(self):\n    self.encoder = nn.Dense(self.embed_dim)\n    self.decoder = nn.Dense(self.output_dim)\n\n  def encode(self, x):\n    return self.encoder(x)\n\n  def decode(self, x):\n    return self.decoder(x)\n\n  def __call__(self, x):\n    x = self.encode(x)\n    x = self.decode(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Configuring Logical to Mesh Sharding Rules in Flax\nDESCRIPTION: Demonstrates how to set up rules for mapping logical axis names to device axis names and initialize a model with logical partitioning. Shows conversion from logical to mesh-specific sharding.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrules = (('batch', 'data'),\n         ('hidden', 'model'))\n\nlogical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)\n\nlogical_abstract_variables = jax.eval_shape(\n    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)\nlogical_state_spec = nn.get_partition_spec(logical_abstract_variables)\nprint('annotations are logical, not mesh-specific: ',\n      logical_state_spec.params['LogicalDotReluDot_0']['Dense_0']['kernel'])\n\nlogical_state_sharding = nn.logical_to_mesh_sharding(logical_state_spec, mesh, rules)\nprint('sharding annotations are mesh-specific: ',\n      logical_state_sharding.params['LogicalDotReluDot_0']['Dense_0']['kernel'].spec)\n```\n\n----------------------------------------\n\nTITLE: Implementing BatchNorm Block in Haiku and Flax NNX\nDESCRIPTION: Demonstrates implementation of a neural network block with BatchNorm in both Haiku and Flax NNX frameworks. Shows key differences in parameter handling and initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Block(hk.Module):\n  def __init__(self, features: int, name=None):\n    super().__init__(name=name)\n    self.features = features\n\n  def __call__(self, x, training: bool):\n    x = hk.Linear(self.features)(x)\n    x = hk.BatchNorm(\n      create_scale=True, create_offset=True, decay_rate=0.99\n    )(x, is_training=training)\n    x = jax.nn.relu(x)\n    return x\n\ndef forward(x, training: bool):\n  return Model(256, 10)(x, training)\nmodel = hk.transform_with_state(forward)\n\nsample_x = jnp.ones((1, 784))\nparams, batch_stats = model.init(jax.random.key(0), sample_x, training=True)\n```\n\n----------------------------------------\n\nTITLE: Model Initialization with Dropout\nDESCRIPTION: Demonstrates model initialization process with dropout enabled, showing how to create initial parameters and variables with the training flag set to False.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_model = MyModel(num_neurons=3)\nx = jnp.empty((3, 4, 4))\nvariables = my_model.init(params_key, x, training=False)\nparams = variables['params']\n```\n\n----------------------------------------\n\nTITLE: Multi-Device RNG Management with JAX JIT\nDESCRIPTION: Shows how to use jax.jit with RNGs in multi-device settings, including proper sharding specifications for input and output data.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x, add_noise):\n    x = nn.Dense(1)(x)\n    return jnp.where(\n      add_noise, x + jax.random.normal(self.make_rng('params'), x.shape), x\n    )\n\ndef forward(variables, x, add_noise, rng):\n  return module.apply(variables, x, add_noise, rngs={'params': rng})\n\njit_forward = jax.jit(\n  forward,\n  in_shardings=(None, data_sharding, None, None),\n  out_shardings=data_sharding,\n)\n```\n\n----------------------------------------\n\nTITLE: Restoring Pytrees with Custom Structures in Python using Orbax\nDESCRIPTION: Demonstrates how to restore a pytree with custom dataclasses like TrainState using Orbax. It shows how to set up an example pytree to guide the restoration process.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nempty_state = train_state.TrainState.create(\n    apply_fn=model.apply,\n    params=jax.tree_util.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter\n    tx=tx,\n)\nempty_config = {'dimensions': np.array([0, 0])}\ntarget = {'model': empty_state, 'config': empty_config, 'data': [jnp.zeros_like(x1)]}\nstate_restored = orbax_checkpointer.restore('/tmp/flax_ckpt/orbax/single_save', item=target)\nstate_restored\n```\n\n----------------------------------------\n\nTITLE: Converting nnx.State to Pure Dictionary for Checkpoint Storage in Python\nDESCRIPTION: This code demonstrates how to convert a Flax nnx.State object to a pure Python dictionary for checkpoint storage. It uses the nnx.to_pure_dict function to perform the conversion, displays the result, and then saves it to a checkpoint directory using a checkpointer object.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Save as pure dict\npure_dict_state = nnx.to_pure_dict(state)\nnnx.display(pure_dict_state)\ncheckpointer.save(ckpt_dir / 'pure_dict', pure_dict_state)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Linear Regression Data\nDESCRIPTION: Creates synthetic data for linear regression training including random ground truth parameters, input samples, and noisy output samples.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Set problem dimensions.\nn_samples = 20\nx_dim = 10\ny_dim = 5\n\n# Generate random ground truth W and b.\nkey = random.key(0)\nk1, k2 = random.split(key)\nW = random.normal(k1, (x_dim, y_dim))\nb = random.normal(k2, (y_dim,))\n# Store the parameters in a FrozenDict pytree.\ntrue_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n\n# Generate samples with additional noise.\nkey_sample, key_noise = random.split(k1)\nx_samples = random.normal(key_sample, (n_samples, x_dim))\ny_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\nprint('x shape:', x_samples.shape, '; y shape:', y_samples.shape)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Neural Network with Flax\nDESCRIPTION: Example showing how to create and use a Multi-Layer Perceptron (MLP) neural network using Flax. Demonstrates module creation, initialization, and forward pass execution.\nSOURCE: https://github.com/google/flax/blob/main/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):                    # create a Flax Module dataclass\n  out_dims: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(128)(x)                 # create inline Flax Module submodules\n    x = nn.relu(x)\n    x = nn.Dense(self.out_dims)(x)       # shape inference\n    return x\n\nmodel = MLP(out_dims=10)                 # instantiate the MLP model\n\nx = jnp.empty((4, 28, 28, 1))            # generate random data\nvariables = model.init(random.key(42), x)# initialize the weights\ny = model.apply(variables, x)            # make forward pass\n```\n\n----------------------------------------\n\nTITLE: Loading Sharded Model from Checkpoint with Orbax\nDESCRIPTION: Shows how to save and load a sharded model using Orbax checkpointing, including handling of sharding specifications and abstract arrays.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport orbax.checkpoint as ocp\n\n# Save the sharded state.\nsharded_state = nnx.state(sharded_model)\npath = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\ncheckpointer = ocp.StandardCheckpointer()\ncheckpointer.save(path / 'checkpoint_name', sharded_state)\n\n# Load a sharded state from checkpoint, without `sharded_model` or `sharded_state`.\nabs_model = nnx.eval_shape(lambda: DotReluDot(1024, rngs=nnx.Rngs(0)))\nabs_state = nnx.state(abs_model)\n# Orbax API expects a tree of abstract `jax.ShapeDtypeStruct`\n# that contains both sharding and the shape/dtype of the arrays.\nabs_state = jax.tree.map(\n  lambda a, s: jax.ShapeDtypeStruct(a.shape, a.dtype, sharding=s),\n  abs_state, nnx.get_named_sharding(abs_state, mesh)\n)\nloaded_sharded = checkpointer.restore(path / 'checkpoint_name',\n                                      target=abs_state)\njax.debug.visualize_array_sharding(loaded_sharded['dot1']['kernel'].value)\njax.debug.visualize_array_sharding(loaded_sharded['w2'].value)\n```\n\n----------------------------------------\n\nTITLE: Creating MLP Model with Optional Scan Transformation\nDESCRIPTION: Implements a multi-layer perceptron model that can either use flax.linen.scan for parameter sharing or a for-loop approach. The scan approach can provide faster compilation while the for-loop may offer better runtime performance.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  num_layers: int\n  depth: int\n  use_scan: bool\n  @nn.compact\n  def __call__(self, x):\n    if self.use_scan:\n      x, _ = nn.scan(DotReluDot, length=self.num_layers,\n                     variable_axes={\"params\": 0},\n                     split_rngs={\"params\": True},\n                     metadata_params={nn.PARTITION_NAME: None}\n                     )(self.depth)(x)\n    else:\n      for i in range(self.num_layers):\n        x, _ = DotReluDot(self.depth)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Initializing and Applying a Flax Module\nDESCRIPTION: This snippet demonstrates how to initialize and apply a Flax Module, showing the use of 'encode' and 'decode' modes.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef init_fn(mdl):\n    x = jax.numpy.ones((3, 2))\n    z = mdl(x, \"encode\")\n    return mdl(z, \"decode\")\n\nmdl = CorrectModule()\nvars = nn.init(init_fn, mdl)(random.key(0))\nassert vars[\"params\"][\"Dense_0\"][\"kernel\"].shape == (2, 8)\nassert vars[\"params\"][\"Dense_1\"][\"kernel\"].shape == (8, 4)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model with Partitioned Weights in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a model that creates partitioned weights using the proposed axis metadata API. It shows the resulting variable structure with metadata.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvariables = partitioned_dense.init(rng, jnp.ones((4,)))\njax.tree.map(np.shape, variables)  # => {\"params\": {\"kernel\": Partitioned(value=(4, 8), names=(None, \"data\")), bias: (8,)}}\n```\n\n----------------------------------------\n\nTITLE: JAX Device Setup\nDESCRIPTION: Imports JAX and checks available devices for computation\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.devices()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Training Metrics with Matplotlib\nDESCRIPTION: Creates visualization plots for training and testing metrics using matplotlib, showing loss and accuracy trends over time.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n  ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n  ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()\n```\n\n----------------------------------------\n\nTITLE: Model Initialization with BatchNorm Variables\nDESCRIPTION: Shows how to initialize a Flax model with BatchNorm, extracting both params and batch_stats collections from the variables structure.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/batch_norm.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmlp = MLP()\nx = jnp.ones((1, 3))\nvariables = mlp.init(jax.random.key(0), x, train=False)\nparams = variables['params']\nbatch_stats = variables['batch_stats']\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Axis Metadata Transformation in Flax NNX\nDESCRIPTION: This snippet defines a Weights class with sharding metadata, and demonstrates how the sharding information is updated when using nnx.vmap with transform_metadata. It shows how the 'b' axis is removed from sharding inside the vmap and added back outside.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, array: jax.Array, sharding: tuple[str | None, ...]):\n    self.param = nnx.Param(array, sharding=sharding)\n\nm = Weights(jnp.ones((3, 4, 5)), sharding=('a', 'b', None))\n\n@nnx.vmap(in_axes=1, transform_metadata={nnx.PARTITION_NAME: 'b'})\ndef f(m: Weights):\n  print(f'Inner {m.param.shape = }')\n  print(f'Inner {m.param.sharding = }')\n\nf(m)\nprint(f'Outter {m.param.shape = }')\nprint(f'Outter {m.param.sharding = }')\n```\n\n----------------------------------------\n\nTITLE: Training Step with BatchNorm State in Haiku and Flax\nDESCRIPTION: Comparison of training step implementation with BatchNorm state. Haiku passes and receives state explicitly, while Flax packages state in the variables dictionary with specific collection names.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(params, state, inputs, labels):\n  def loss_fn(params):\n    logits, new_state = model.apply(\n      params, state,\n      None, # <== rng\n      inputs, training=True # <== inputs\n    )\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n    return loss, new_state\n\n  grads, new_state = jax.grad(loss_fn, has_aux=True)(params)\n  params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)\n\n  return params, new_state\n```\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(params, batch_stats, inputs, labels):\n  def loss_fn(params):\n    logits, updates = model.apply(\n```\n\n----------------------------------------\n\nTITLE: Defining a Two-Layer MLP Model in Flax NNX\nDESCRIPTION: Creates a simple two-layer MLP (Multi-Layer Perceptron) model using Flax NNX Module class.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass TwoLayerMLP(nnx.Module):\n  def __init__(self, dim, rngs: nnx.Rngs):\n    self.linear1 = nnx.Linear(dim, dim, rngs=rngs)\n    self.linear2 = nnx.Linear(dim, dim, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.linear1(x)\n    return self.linear2(x)\n```\n\n----------------------------------------\n\nTITLE: Using Flax NNX Filter DSL with vmap for State Axes Definition\nDESCRIPTION: Shows how to use the Flax NNX Filter DSL to define state axes for vmap, vectorizing parameters and applying 'dropout' RngKeys on the 0th axis while broadcasting the rest.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstate_axes = nnx.StateAxes({(nnx.Param, 'dropout'): 0, ...: None})\n\n@nnx.vmap(in_axes=(state_axes, 0))\ndef forward(model, x):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Core Imports\nDESCRIPTION: Imports necessary libraries including Flax, JAX, TensorFlow and visualization tools\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom absl import logging\nimport flax\nimport jax\nimport jax.numpy as jnp\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nlogging.set_verbosity(logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Mathematical Function Representation in Flax\nDESCRIPTION: Mathematical formula showing how Flax Modules are functionalized to work with JAX transformations. The function takes input variables and data, returning output variables and transformed data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/philosophy.md#2025-04-22_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\nf \\left( v_{in}, x \\right) \\rightarrow v_{out}, y\n```\n\n----------------------------------------\n\nTITLE: Using vmap with NNX Modules as Functions\nDESCRIPTION: Example of using NNX vmap to create and operate on a stack of weights. The transforms work like JAX transforms but can handle NNX Module objects, demonstrating both weight creation and applying weights to a batch of inputs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, kernel: jax.Array, bias: jax.Array):\n    self.kernel, self.bias = nnx.Param(kernel), nnx.Param(bias)\n\ndef create_weights(seed: jax.Array):\n  return Weights(\n    kernel=random.uniform(random.key(seed), (2, 3)),\n    bias=jnp.zeros((3,)),\n  )\n\ndef vector_dot(weights: Weights, x: jax.Array):\n  assert weights.kernel.ndim == 2, 'Batch dimensions not allowed'\n  assert x.ndim == 1, 'Batch dimensions not allowed'\n  return x @ weights.kernel + weights.bias\n\nseeds = jnp.arange(10)\nweights = nnx.vmap(create_weights, in_axes=0, out_axes=0)(seeds)\n\nx = jax.random.normal(random.key(1), (10, 2))\ny = nnx.vmap(vector_dot, in_axes=(0, 0), out_axes=1)(weights, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Using setup Method in Flax\nDESCRIPTION: Example of defining a Multi-Layer Perceptron using the explicit setup method in Flax. Submodules are defined in setup and used in the forward pass method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/setup_or_nncompact.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  def setup(self):\n    # Submodule names are derived by the attributes you assign to. In this\n    # case, \"dense1\" and \"dense2\". This follows the logic in PyTorch.\n    self.dense1 = nn.Dense(32)\n    self.dense2 = nn.Dense(32)\n\n  def __call__(self, x):\n    x = self.dense1(x)\n    x = nn.relu(x)\n    x = self.dense2(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Configuring Optax Gradient Transformations in Python\nDESCRIPTION: Example showing how to create an optimizer using Optax's gradient transformation chains, specifically implementing momentum optimization with learning rate scheduling.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport optax\n\ntx = optax.chain(\n    optax.trace(decay=0.9, nesterov=False),\n    optax.scale_by_schedule(lambda step: -get_learning_rate(step)),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Learning Rate Schedules with Optax vs flax.optim\nDESCRIPTION: Comparison of learning rate schedule implementation between flax.optim and optax. Optax uses optax.scale_by_schedule() to inject a scheduling function for learning rates, whereas Flax allows overriding parameters during gradient application.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(step, optimizer, batch):\n  grads = jax.grad(loss)(optimizer.target, batch)\n  return step + 1, optimizer.apply_gradient(grads, learning_rate=schedule(step))\n```\n\nLANGUAGE: python\nCODE:\n```\ntx = optax.chain(\n    optax.trace(decay=momentum),\n    # Note that we still want a negative value for scaling the updates!\n    optax.scale_by_schedule(lambda step: -schedule(step)),\n)\n```\n\n----------------------------------------\n\nTITLE: Applying NNX Lifted Transforms on Converted Linen Modules\nDESCRIPTION: This code shows how to apply NNX-style lifted transforms like vmap on a converted Linen module. It demonstrates using nnx.split_rngs and nnx.vmap to vectorize computations across multiple instances of a converted Linen module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass NNXVmapped(nnx.Module):\n  def __init__(self, out_dim: int, vmap_axis_size: int, rngs: nnx.Rngs):\n    self.linen_dot = nnx.bridge.ToNNX(nn.Dense(out_dim, use_bias=False), rngs=rngs)\n    self.vmap_axis_size = vmap_axis_size\n\n  def __call__(self, x):\n\n    @nnx.split_rngs(splits=self.vmap_axis_size)\n    @nnx.vmap(in_axes=(0, 0), axis_size=self.vmap_axis_size)\n    def vmap_fn(submodule, x):\n      return submodule(x)\n\n    return vmap_fn(self.linen_dot, x)\n\nx = jax.random.normal(jax.random.key(0), (4, 32))\nmodel = bridge.lazy_init(NNXVmapped(64, 4, rngs=nnx.Rngs(0)), x)\n\nprint(model.linen_dot.kernel.shape) # (4, 32, 64) - first axis with dim 4 got vmapped\ny = model(x)\nprint(y.shape)\n```\n\n----------------------------------------\n\nTITLE: Lightweight Checkpoint Saving and Loading with Orbax Checkpointer in Python\nDESCRIPTION: Shows how to use Orbax Checkpointer for saving and restoring individual checkpoints without a top-level checkpoint manager. This approach doesn't include Orbax management features but provides a lightweight alternative.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPURE_CKPT_DIR = '/tmp/orbax_upgrade/pure'\n\nckptr = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())  # A stateless object, can be created on the fly.\nckptr.save(PURE_CKPT_DIR, CKPT_PYTREE,\n           save_args=flax.training.orbax_utils.save_args_from_target(CKPT_PYTREE), force=True)\nckptr.restore(PURE_CKPT_DIR, item=TARGET_PYTREE,\n              restore_args=flax.training.orbax_utils.restore_args_from_target(TARGET_PYTREE, mesh=None))\n```\n\n----------------------------------------\n\nTITLE: Defining RNNBase Protocol in Flax\nDESCRIPTION: This snippet defines the RNNBase protocol, which serves as a base class for the RNN class and specifies the API that all RNN layers should implement to be compatible with the Bidirectional class.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RNNBase(Protocol):\n  def __call__(\n      self,\n      inputs: jax.Array,\n      *,\n      initial_carry: Optional[Carry] = None,\n      init_key: Optional[random.KeyArray] = None,\n      seq_lengths: Optional[Array] = None,\n      return_carry: Optional[bool] = None,\n      time_major: Optional[bool] = None,\n      reverse: Optional[bool] = None,\n      keep_order: Optional[bool] = None,\n  ) -> Union[Output, Tuple[Carry, Output]]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation in Haiku and Flax\nDESCRIPTION: Comparison of training step implementation with dropout. Haiku passes the key directly to apply, while Flax requires packaging parameters and random keys in dictionaries with specific collection names.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(key, params, inputs, labels):\n  def loss_fn(params):\n      logits = model.apply(\n        params,\n        key,\n        inputs, training=True # <== inputs\n      )\n      return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(params)\n  params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)\n\n  return params\n```\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(key, params, inputs, labels):\n  def loss_fn(params):\n      logits = model.apply(\n        {'params': params},\n        inputs, training=True, # <== inputs\n        rngs={'dropout': key}\n      )\n      return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\n  grads = jax.grad(loss_fn)(params)\n  params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)\n\n  return params\n```\n\n----------------------------------------\n\nTITLE: Referencing Flax NNX Classes\nDESCRIPTION: Key class references found throughout the glossary including nnx.Module, nnx.Variable, nnx.Param, nnx.GraphDef, and nnx.VariableState. These form the core components of the Flax NNX framework.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_glossary.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnnx.Module\nnnx.Variable\nnnx.Param\nnnx.GraphDef\nnnx.VariableState\nnnx.Rngs\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Flax and JAX\nDESCRIPTION: Imports necessary libraries including JAX, Flax, and Optax for distributed training. Sets up the foundation for scaled-up module computation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom typing import Optional, Callable\n\nimport numpy as np\nimport jax\nfrom jax import lax, random, numpy as jnp\n\nimport flax\nfrom flax import struct, traverse_util, linen as nn\nfrom flax.core import freeze, unfreeze\nfrom flax.training import train_state, checkpoints\n\nimport optax # Optax for common losses and optimizers.\n```\n\n----------------------------------------\n\nTITLE: Model with Selective Parameter Generation\nDESCRIPTION: Demonstrates how to generate separate parameters for specific collections while sharing others across batch inputs.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nBatchModel = nn.vmap(\n    Model,\n    in_axes=0, out_axes=0,\n    variable_axes={'params': 0, 'other_collection': None},\n    split_rngs={'params': True, 'other': False})\n\nBatchModel().init({'params': jax.random.key(0), 'other': jax.random.key(1)}, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Logical Axis Annotation in Flax NNX\nDESCRIPTION: Defines a neural network module with logical axis annotations, providing more descriptive names than device mesh axes. Maps aliases like 'batch', 'hidden', and 'embed' to device mesh axes for better readability.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# The mapping from alias annotation to the device mesh.\nsharding_rules = (('batch', 'data'), ('hidden', 'model'), ('embed', None))\n\nclass LogicalDotReluDot(nnx.Module):\n  def __init__(self, depth: int, rngs: nnx.Rngs):\n    init_fn = nnx.initializers.lecun_normal()\n\n    # Initialize a sublayer `self.dot1`.\n    self.dot1 = nnx.Linear(\n      depth, depth,\n      kernel_init=nnx.with_metadata(\n        # Provide the sharding rules here.\n        init_fn, sharding=('embed', 'hidden'), sharding_rules=sharding_rules),\n      use_bias=False,\n      rngs=rngs)\n\n    # Initialize a weight param `w2`.\n    self.w2 = nnx.Param(\n      # Didn't provide the sharding rules here to show you how to overwrite it later.\n      nnx.with_metadata(init_fn, sharding=('hidden', 'embed'))(\n        rngs.params(), (depth, depth))\n    )\n\n  def __call__(self, x: jax.Array):\n    y = self.dot1(x)\n    y = jax.nn.relu(y)\n    # Unfortunately the logical aliasing doesn't work on lower-level JAX calls.\n    y = jax.lax.with_sharding_constraint(y, PartitionSpec('data', None))\n    z = jnp.dot(y, self.w2.value)\n    return z\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Block and MLP with Haiku\nDESCRIPTION: Implementation of a Block module and MLP class using the Haiku library. The Block applies a linear transformation, dropout (conditional on training mode), and ReLU activation. The MLP stacks multiple Block instances using layer_stack.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, features: int, name=None):\n  super().__init__(name=name)\n  self.features = features\n\ndef __call__(self, x, training: bool):\n  x = hk.Linear(self.features)(x)\n  x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)\n  x = jax.nn.relu(x)\n  return x\n\nclass MLP(hk.Module):\n  def __init__(self, features: int, num_layers: int, name=None):\n      super().__init__(name=name)\n      self.features = features\n      self.num_layers = num_layers\n\n\n\n\n\n  def __call__(self, x, training: bool):\n\n    @hk.experimental.layer_stack(self.num_layers)\n    def stack_block(x):\n      return Block(self.features)(x, training)\n\n    stack = hk.experimental.layer_stack(self.num_layers)\n    return stack_block(x)\n\ndef forward(x, training: bool):\n  return MLP(64, num_layers=5)(x, training)\nmodel = hk.transform(forward)\n\nsample_x = jnp.ones((1, 64))\nparams = model.init(jax.random.key(0), sample_x, training=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Variable Lifting with Flax Core Pack\nDESCRIPTION: Example showing how to use flax.core.lift.pack to transpose matrices in a variable collection. Demonstrates variable filtering, transformation, and reconstruction stages.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.core import lift\nfrom flax.core import Scope, init, apply, nn as core_nn\n\ndef lift_transpose(fn, target='params', variables=True, rngs=True):\n  def wrapper(scope_fn, repack_fn, variable_groups, rng_groups, *args):\n    target, rest = variable_groups\n    def trans(x):\n      if x.ndim == 2:\n        return x.T\n      return x\n    target = jax.tree_util.tree_map(trans, target)\n    variable_groups = (target, rest)\n    scope = scope_fn(variable_groups, rng_groups)\n    y = fn(scope, *args)\n    out_variables = repack_fn(scope)\n    return y, out_variables\n  return lift.pack(\n      wrapper,\n      in_variable_filters=(target, variables),\n      out_variable_filters=(variables,),\n      rng_filters=(rngs,))\n\nx = jnp.ones((3, 2))\ny, params = init(lift_transpose(core_nn.dense))(random.key(0), x, 4)\n```\n\n----------------------------------------\n\nTITLE: Compiling and Executing Flax Model Initialization with JAX\nDESCRIPTION: This code compiles the initialization function using jax.jit with specified input and output shardings, then executes it to get the initialized state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\njit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),\n                      in_shardings=(mesh_sharding(PartitionSpec()), x_sharding),  # PRNG key and x\n                      out_shardings=state_sharding)\n\ninitialized_state = jit_init_fn(k, x, model, optimizer)\n\n# for weight, partitioned in initialized_state.params['DotReluDot_0'].items():\n#     print(f'Sharding of {weight}: {partitioned.names}')\njax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)\njax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['W2'].value)\n```\n\n----------------------------------------\n\nTITLE: Flax NNX Operations\nDESCRIPTION: Core operations in Flax NNX including split, merge, and jit transformations that are used to manipulate and transform modules and variables.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_glossary.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnnx.split\nnnx.merge\nnnx.jit\n```\n\n----------------------------------------\n\nTITLE: Implementing Logical Axis Modules in Python with Flax\nDESCRIPTION: Defines two neural network modules (LogicalDotReluDot and LogicalMLP) using Flax with logical axis annotations. The modules use meaningful axis names like 'embed', 'hidden', 'batch' and 'layer' for better readability and flexible partitioning.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass LogicalDotReluDot(nn.Module):\n  depth: int\n  dense_init: Callable = nn.initializers.xavier_normal()\n  @nn.compact\n  def __call__(self, x):\n    y = nn.Dense(self.depth,\n                 kernel_init=nn.with_logical_partitioning(self.dense_init, ('embed', 'hidden')),\n                 use_bias=False,  # or overwrite with `bias_init`\n                 )(x)\n\n    y = jax.nn.relu(y)\n    # Force a local sharding annotation.\n    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))\n\n    W2 = self.param(\n        'W2',\n        nn.with_logical_partitioning(self.dense_init, ('hidden', 'embed')),\n        (self.depth, x.shape[-1]))\n\n    z = jnp.dot(y, W2)\n    # Force a local sharding annotation.\n    z = nn.with_logical_constraint(z, ('batch', 'embed'))\n    return z, None\n\nclass LogicalMLP(nn.Module):\n  num_layers: int\n  depth: int\n  use_scan: bool\n  @nn.compact\n  def __call__(self, x):\n    if self.use_scan:\n      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers,\n                    variable_axes={\"params\": 0},\n                    split_rngs={\"params\": True},\n                    metadata_params={nn.PARTITION_NAME: 'layer'}\n                    )(self.depth)(x)\n    else:\n      for i in range(self.num_layers):\n        x, _ = LogicalDotReluDot(self.depth)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Instances of Sub-Modules in Flax\nDESCRIPTION: This code snippet shows how to use multiple instances of the same sub-Module class in a Flax Module. It demonstrates how the naming convention for sub-Modules works when multiple instances are used.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass RNGSubModule(nn.Module):\n  @nn.compact\n  def __call__(self):\n    print(f\"{self.name}, count 1: {self.make_rng('rng_stream')}\")\n    print(f\"{self.name}, count 2: {self.make_rng('rng_stream')}\")\n\nclass RNGModule(nn.Module):\n  @nn.compact\n  def __call__(self):\n    print(f\"RNGModule, count 1: {self.make_rng('rng_stream')}\")\n    print(f\"RNGModule, count 2: {self.make_rng('rng_stream')}\")\n    RNGSubModule()()\n    RNGSubModule()()\n\nrng_module = RNGModule()\nvariables = rng_module.init({'rng_stream': jax.random.key(0)})\n```\n\n----------------------------------------\n\nTITLE: Parameter Initialization in Haiku and Flax\nDESCRIPTION: Comparison of parameter initialization between frameworks. Haiku returns parameters directly, while Flax returns a mapping of collections that includes params and potentially other state variables.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsample_x = jax.numpy.ones((1, 784))\nparams = model.init(\n  random.key(0),\n  sample_x, training=False # <== inputs\n)\n...\n```\n\nLANGUAGE: python\nCODE:\n```\nsample_x = jax.numpy.ones((1, 784))\nvariables = model.init(\n  random.key(0),\n  sample_x, training=False # <== inputs\n)\nparams = variables[\"params\"]\n```\n\n----------------------------------------\n\nTITLE: Defining a CNN Model and Initializing Parameters\nDESCRIPTION: Creating a small convolutional neural network class and initializing its parameters. The model consists of convolutional layers, pooling operations, and dense layers for classification.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n      x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n      x = nn.relu(x)\n      x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n      x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n      x = nn.relu(x)\n      x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n      x = x.reshape((x.shape[0], -1))\n      x = nn.Dense(features=256)(x)\n      x = nn.relu(x)\n      x = nn.Dense(features=10)(x)\n      x = nn.log_softmax(x)\n      return x\n\ndef get_initial_params(key):\n    init_shape = jnp.ones((1, 28, 28, 1), jnp.float32)\n    initial_params = CNN().init(key, init_shape)['params']\n    return initial_params\n\nkey = jax.random.key(0)\nparams = get_initial_params(key)\n\njax.tree_util.tree_map(jnp.shape, params)\n```\n\n----------------------------------------\n\nTITLE: Using Bidirectional RNN with Different Cell Types in Flax\nDESCRIPTION: This snippet demonstrates how to create and use a bidirectional RNN with different cell types (LSTM and GRU) for forward and backward directions in Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nforward_rnn = nn.RNN(nn.LSTMCell(), cell_size=32)\nbackward_rnn = nn.RNN(nn.GRUCell(), cell_size=32)\n# Bidirectional combinator.\nbi_rnn = nn.Bidirectional(forward_rnn, backward_rnn)\n# Encodes a batch of input sequences in both directions.\ncarry, outputs = bi_rnn(inputs, seq_lengths)\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Modules with Updated Sharding in Flax NNX\nDESCRIPTION: This example shows how to create nnx.Modules inside a vectorized function using nnx.vmap. It demonstrates that the sharding metadata is correctly updated for the new axes added by the transformation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(out_axes=1, axis_size=4, transform_metadata={nnx.PARTITION_NAME: 'b'})\ndef init_vmap():\n  return Weights(jnp.ones((3, 5)), sharding=('a', None))\n\nm = init_vmap()\nprint(f'Outter {m.param.shape = }')\nprint(f'Outter {m.param.sharding = }')\n```\n\n----------------------------------------\n\nTITLE: Creating Stacked Models with nnx.vmap and nnx.scan in Python\nDESCRIPTION: Shows how to use nnx.vmap to create multiple MLP instances and nnx.scan to iteratively apply each layer to the input. The example demonstrates automatic state propagation for BatchNorm and Dropout layers during scanning.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=0)\ndef create_model(key: jax.Array):\n  return MLP(10, 32, 10, rngs=nnx.Rngs(key))\n\nkeys = jax.random.split(jax.random.key(0), 5)\nmodel = create_model(keys)\n\n@nnx.scan(in_axes=(0, nnx.Carry), out_axes=nnx.Carry)\ndef forward(model: MLP, x):\n  x = model(x)\n  return x\n\nx = jnp.ones((3, 10))\ny = forward(model, x)\n\nprint(f'{y.shape = }')\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Split Function for Flax NNX State Grouping\nDESCRIPTION: Provides a custom implementation of the nnx.split function to demonstrate how state grouping works in Flax NNX, using filters to separate different types of states.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\nKeyPath = tuple[nnx.graph.Key, ...]\n\ndef split(node, *filters):\n  graphdef, state = nnx.graph.flatten(node)\n  predicates = [nnx.filterlib.to_predicate(f) for f in filters]\n  flat_states: list[dict[KeyPath, Any]] = [{} for p in predicates]\n\n  for path, value in state.flat_state():\n    for i, predicate in enumerate(predicates):\n      if predicate(path, value):\n        flat_states[i][path] = value\n        break\n    else:\n      raise ValueError(f'No filter matched {path = } {value = }')\n\n  states: tuple[nnx.GraphState, ...] = tuple(\n    nnx.State.from_flat_path(flat_state) for flat_state in flat_states\n  )\n  return graphdef, *states\n\n# Let's test it.\nfoo = Foo()\n\ngraphdef, params, batch_stats = split(foo, nnx.Param, nnx.BatchStat)\n\nprint(f'{params = }')\nprint(f'{batch_stats = }')\n```\n\n----------------------------------------\n\nTITLE: Creating NNX Module with Linen Submodule\nDESCRIPTION: Demonstrates creating an NNX outer module that contains a Linen dot product layer, initialized using lazy initialization. Shows how NNX bridge allows mixing of module types.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass NNXOuter(nnx.Module):\n  def __init__(self, out_dim: int, rngs: nnx.Rngs):\n    self.dot = nnx.bridge.ToNNX(LinenDot(out_dim), rngs=rngs)\n    self.b = nnx.Param(jax.random.uniform(rngs.params(), (1, out_dim,)))\n\n  def __call__(self, x):\n    return self.dot(x) + self.b\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\nmodel = bridge.lazy_init(NNXOuter(64, rngs=nnx.Rngs(0)), x)  # Can fit into one line\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: State Dictionary Management in Flax\nDESCRIPTION: Functions for converting model states to and from dictionaries, including from_state_dict, to_state_dict, and register_serialization_state.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.serialization.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.serialization import from_state_dict, to_state_dict, register_serialization_state\n```\n\n----------------------------------------\n\nTITLE: Performing Pythonic Module Manipulation in Flax NNX\nDESCRIPTION: Demonstrates various Pythonic operations on sub-Modules, including swapping, sharing, and monkey-patching.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\nx = jax.random.normal(jax.random.key(42), (3, 4))\nnp.testing.assert_allclose(model(x), model.linear2(model.linear1(x)))\n\n# Sub-`Module` swapping.\noriginal1, original2 = model.linear1, model.linear2\nmodel.linear1, model.linear2 = model.linear2, model.linear1\nnp.testing.assert_allclose(model(x), original1(original2(x)))\n\n# `Module` sharing (tying all weights together).\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\nmodel.linear2 = model.linear1\nassert not hasattr(nnx.state(model), 'linear2')\nnp.testing.assert_allclose(model(x), model.linear1(model.linear1(x)))\n\n# Variable sharing (weight-tying).\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\nmodel.linear1.kernel = model.linear2.kernel  # the bias parameter is kept separate\nassert 'linear2' in nnx.state(model)\nassert 'bias' in nnx.state(model)['linear2']\nassert not hasattr(nnx.state(model)['linear2'], 'kernel')\n\n# Monkey-patching.\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\ndef awesome_layer(x): return x\nmodel.linear2 = awesome_layer\nnp.testing.assert_allclose(model(x), model.linear1(x))\n```\n\n----------------------------------------\n\nTITLE: Converting and Testing Transposed Convolution between PyTorch and JAX\nDESCRIPTION: Converts input tensor from NumPy to PyTorch format, applies transposed convolution, converts back to NumPy format, and compares with JAX output. Handles dimension reordering between frameworks' tensor formats (NCHW vs NHWC).\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nt_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))\nt_out = t_conv(t_x)\n# [N, C, H, W] -> [N, H, W, C]\nt_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Implementing RNN Cell and RNN in Haiku and Flax NNX\nDESCRIPTION: Implementation of RNN Cell and full RNN architecture showing differences in scan operations and state handling between frameworks.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass RNNCell(hk.Module):\n  def __init__(self, hidden_size: int, name=None):\n    super().__init__(name=name)\n    self.hidden_size = hidden_size\n\n  def __call__(self, carry, x):\n    x = jnp.concatenate([carry, x], axis=-1)\n    x = hk.Linear(self.hidden_size)(x)\n    x = jax.nn.relu(x)\n    return x, x\n\n  def initial_state(self, batch_size: int):\n    return jnp.zeros((batch_size, self.hidden_size))\n\nclass RNN(hk.Module):\n  def __init__(self, hidden_size: int, name=None):\n    super().__init__(name=name)\n    self.hidden_size = hidden_size\n\n  def __call__(self, x):\n    cell = RNNCell(self.hidden_size)\n    carry = cell.initial_state(x.shape[0])\n    carry, y = hk.scan(\n      cell, carry,\n      jnp.swapaxes(x, 1, 0)\n    )\n    y = jnp.swapaxes(y, 0, 1)\n    return y\n```\n\n----------------------------------------\n\nTITLE: Updating FP8 Parameters with Gradients in Python\nDESCRIPTION: Demonstrates how to update both regular parameters and _overwrite_with_gradient parameters using computed gradients. The code includes flattening/unflattening parameters and applying different update rules based on parameter type.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstep_fn = jax.jit(jax.grad(train_step, (0, 1)))\n\ngrads = step_fn(params, A)\n\nparams = flax.core.unfreeze(params)\nparams = flax.traverse_util.flatten_dict(params, sep='/')\ngrads = flax.traverse_util.flatten_dict(grads[0], sep='/')\n\nfor key, value in params.items():\n  if key.startswith('params'):\n    params[key] = value + 0.01 * grads[key]\n  if key.startswith('_overwrite_with_gradient'):\n    params[key] = grads[key]\n\nparams = flax.traverse_util.unflatten_dict(params, sep='/')\nparams = flax.core.freeze(params)\n```\n\n----------------------------------------\n\nTITLE: Using self.param and self.variable in Flax Modules\nDESCRIPTION: This code snippet demonstrates the usage of self.param and self.variable methods in a Flax Module. It shows how to create parameters and variables, and how the RNG streams are used for initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    # kernel will use 'params' seed, initial data will include 'Dense_0', call count 1\n    x = nn.Dense(2, kernel_init=jax.random.normal, use_bias=False)(x)\n    # model_param will use 'params' seed, call count 1\n    model_param = self.param('model_param', jax.random.normal, x.shape)\n    # model_variable1 will use 'params' seed, call count 2\n    model_variable1 = self.variable(\n      'other_collection',\n      'model_variable1',\n      lambda: jax.random.normal(self.make_rng('params'), x.shape),\n    )\n    # model_variable2 will use 'other' seed, call count 1\n    model_variable2 = self.variable(\n      'other_collection',\n      'model_variable2',\n      lambda: jax.random.normal(self.make_rng('other'), x.shape),\n    )\n    # kernel will use 'params' seed, initial data will include 'Dense_1', call count 1\n    # bias will use 'params' seed, initial data will include 'Dense_1', call count 2\n    x = nn.Dense(2, kernel_init=jax.random.normal, bias_init=jax.random.normal)(\n      x\n    )\n    return x\n\nmodel = Model()\nvariables = model.init(\n  {'params': jax.random.key(0), 'other': jax.random.key(1)}, jnp.ones((2, 2))\n)\nprint(variables['params']['Dense_0']['kernel'])\nprint(variables['params']['model_param'])\nprint(variables['other_collection']['model_variable1'])\nprint(variables['other_collection']['model_variable2'])\nprint(variables['params']['Dense_1']['kernel'])\nprint(variables['params']['Dense_1']['bias'])\n```\n\n----------------------------------------\n\nTITLE: FP8 Parameter Gradient Accumulation with Custom Dtype\nDESCRIPTION: Shows how to handle gradient accumulation for branched parameter usage using fp8_ops.fp32_max_grad custom dtype. The example demonstrates proper gradient accumulation through maximum value instead of addition.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfmax32 = fp8_ops.fp32_max_grad\n\ndef reuse_fp8_param(x, y, scale, amax_history):\n  scale = scale.astype(fmax32)\n  amax_history = amax_history.astype(fmax32)\n\n  x = fp8_ops.in_qdq(f32, e4m3, x, scale, amax_history)\n  y = fp8_ops.in_qdq(f32, e4m3, y, scale, amax_history)\n  return x + y\n\nreuse_fp8_param_fn = jax.grad(reuse_fp8_param, (0, 1, 2, 3))\nreuse_fp8_param_fn = jax.jit(reuse_fp8_param_fn)\n\n_, _, new_ah, new_sf = reuse_fp8_param_fn(2.0, 3.0, a_scale, a_amax_hist)\nprint(new_ah, new_sf)\n```\n\n----------------------------------------\n\nTITLE: Using pad_shard_unpad() Utility for Simplified Evaluation\nDESCRIPTION: Simplifies the evaluation loop by using Flax's pad_shard_unpad utility function which handles the pad→shard→predict→unshard→unpad sequence.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncorrect = total = 0\nfor batch in ds.as_numpy_iterator():\n  preds = flax.jax_utils.pad_shard_unpad(get_preds)(\n      vs, batch['image'], min_device_batch=per_device_batch_size)\n  total += len(batch['image'])\n  correct += (batch['label'] == preds.argmax(axis=-1)).sum()\n```\n\n----------------------------------------\n\nTITLE: Verifying Reproducibility of PRNG Sequences\nDESCRIPTION: Reinitializes the RNG module with the same seed to demonstrate that the sequence of PRNG keys is reproducible.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nvariables = rng_module.init({'rng_stream': jax.random.key(0)})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Different Seeds Produce Different PRNG Sequences\nDESCRIPTION: Shows that initializing a module with a different PRNG seed key generates a different sequence of random values.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvariables = rng_module.init({'rng_stream': jax.random.key(1)})\n```\n\n----------------------------------------\n\nTITLE: JAX Integration - Basic State Management\nDESCRIPTION: Demonstrates basic state management with JAX including splitting and merging model state.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngraphdef, state = model.split()\n\n# state is a dictionary-like JAX pytree\nprint(f'{state = }'[:500] + '\\n...')\n\n# graphdef is also a JAX pytree, but just metadata\nprint(f'\\n{graphdefefefefefef = }'[:300] + '\\n...')\n```\n\n----------------------------------------\n\nTITLE: Converting Flax NNX Filter Literals to Predicates\nDESCRIPTION: Demonstrates how to manually convert Filter literals into predicates using nnx.filterlib.to_predicate for various Filter types in Flax NNX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nis_param = nnx.filterlib.to_predicate(nnx.Param)\neverything = nnx.filterlib.to_predicate(...)\nnothing = nnx.filterlib.to_predicate(False)\nparams_or_dropout = nnx.filterlib.to_predicate((nnx.Param, 'dropout'))\n\nprint(f'{is_param = }')\nprint(f'{everything = }')\nprint(f'{nothing = }')\nprint(f'{params_or_dropout = }')\n```\n\n----------------------------------------\n\nTITLE: Initializing Model and Input Data\nDESCRIPTION: Creates a model instance with specified hyperparameters and generates sample input data. Sets up an Adam optimizer for training the model.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# MLP hyperparameters.\nBATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, False\n# Create fake inputs.\nx = jnp.ones((BATCH, DEPTH))\n# Initialize a PRNG key.\nk = random.key(0)\n\n# Create an Optax optimizer.\noptimizer = optax.adam(learning_rate=0.001)\n# Instantiate the model.\nmodel = MLP(LAYERS, DEPTH, USE_SCAN)\n```\n\n----------------------------------------\n\nTITLE: Creating an RNNCell with Recurrent Dropout in Flax NNX\nDESCRIPTION: This code defines an RNNCell that implements recurrent dropout by using a custom PRNG stream. The cell includes a Linear layer for transformation, a Dropout layer with a dedicated 'recurrent_dropout' stream, and a Count variable to track iterations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Count(nnx.Variable): pass\n\nclass RNNCell(nnx.Module):\n  def __init__(self, din, dout, rngs):\n    self.linear = nnx.Linear(dout + din, dout, rngs=rngs)\n    self.drop = nnx.Dropout(0.1, rngs=rngs, rng_collection='recurrent_dropout')\n    self.dout = dout\n    self.count = Count(jnp.array(0, jnp.uint32))\n\n  def __call__(self, h, x) -> tuple[jax.Array, jax.Array]:\n    h = self.drop(h) # Recurrent dropout.\n    y = nnx.relu(self.linear(jnp.concatenate([h, x], axis=-1)))\n    self.count += 1\n    return y, y\n\n  def initial_state(self, batch_size: int):\n    return jnp.zeros((batch_size, self.dout))\n\ncell = RNNCell(8, 16, nnx.Rngs(params=0, recurrent_dropout=1))\n```\n\n----------------------------------------\n\nTITLE: Consistent Aliasing Error in Flax NNX with vmap\nDESCRIPTION: Shows how Flax NNX enforces consistent aliasing across inputs when using JAX transformations. This example demonstrates an error when the same module is referenced in multiple arguments with different vectorization axes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Weights(nnx.Module):\n  def __init__(self, array: jax.Array):\n    self.param = nnx.Param(array)\n\nm = Weights(jnp.arange(10))\narg1 = {'a': {'b': m}, 'c': m}\narg2 = [(m, m), m]\n\n@nnx.vmap(in_axes=(0, 1))\ndef f(arg1, arg2):\n  ...\n\ntry:\n  f(arg1, arg2)\nexcept ValueError as e:\n  print(e)\n```\n\n----------------------------------------\n\nTITLE: Converting Partitioned NNX Modules to Linen with Metadata\nDESCRIPTION: This snippet demonstrates converting an NNX module with sharding annotations to Linen, preserving the partition metadata. It shows how NNX sharding is wrapped in bridge.NNXMeta and can be accessed through Linen's metadata system.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass NNXDotWithParititioning(nnx.Module):\n  def __init__(self, in_dim: int, out_dim: int, rngs: nnx.Rngs):\n    init_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), ('in', 'out'))\n    self.w = nnx.Param(init_fn(rngs.params(), (in_dim, out_dim)))\n  def __call__(self, x: jax.Array):\n    return x @ self.w\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\n\n@jax.jit\ndef create_sharded_variables(key, x):\n  model = bridge.to_linen(NNXDotWithParititioning, 32, 64)\n  variables = model.init(key, x)\n  # A `NNXMeta` wrapper of the underlying `nnx.Param`\n  assert type(variables['params']['w']) == bridge.NNXMeta\n  # The annotation coming from the `nnx.Param` => (in, out)\n  assert variables['params']['w'].metadata['sharding'] == ('in', 'out')\n\n  unboxed_variables = nn.unbox(variables)\n  variable_pspecs = nn.get_partition_spec(variables)\n  assert isinstance(unboxed_variables['params']['w'], jax.Array)\n  assert variable_pspecs['params']['w'] == jax.sharding.PartitionSpec('in', 'out')\n\n  sharded_vars = jax.tree.map(jax.lax.with_sharding_constraint,\n                              nn.unbox(variables),\n                              nn.get_partition_spec(variables))\n  return sharded_vars\n\nwith mesh:\n  variables = create_sharded_variables(jax.random.key(0), x)\n\n# The underlying JAX array is sharded across the 2x4 mesh\nprint(variables['params']['w'].sharding)\n```\n\n----------------------------------------\n\nTITLE: Running Fake Data Benchmark\nDESCRIPTION: Commands to download dataset metadata and run benchmark with fake data.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbash ../../tests/download_dataset_metadata.sh\npython imagenet_fake_data_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Filter Order Dependency in Flax NNX\nDESCRIPTION: Illustrates the importance of filter order when splitting Flax NNX states, showing how more specific filters should be placed before more general ones to correctly capture subclasses.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SpecialParam(nnx.Param):\n  pass\n\nclass Bar(nnx.Module):\n  def __init__(self):\n    self.a = nnx.Param(0)\n    self.b = SpecialParam(0)\n\nbar = Bar()\n\ngraphdef, params, special_params = split(bar, nnx.Param, SpecialParam) # wrong!\nprint(f'{params = }')\nprint(f'{special_params = }')\n\ngraphdef, special_params, params = split(bar, SpecialParam, nnx.Param) # correct!\nprint(f'{params = }')\nprint(f'{special_params = }')\n```\n\n----------------------------------------\n\nTITLE: Implementing Bidirectional Processing in Flax\nDESCRIPTION: This snippet shows the implementation of bidirectional processing in Flax, which processes the input sequence in both forward and backward directions using separate RNN instances.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self, inputs, seq_lengths):\n  # Encode in the forward direction.\n  carry_forward, outputs_forward = self.forward_rnn(\n    inputs, seq_lengths=seq_lengths, \n    return_carry=True, reverse=False,\n  )\n  # Encode in the reverse order.\n  carry_backward, outputs_backward = self.backward_rnn(\n    inputs, seq_lengths=seq_lengths,\n    return_carry=True, reverse=True, # process in reverse order\n    keep_order=True, # but return the sequence in the original order\n  )\n  # Merge both sequences.\n  outputs = jax.tree.map(self.merge_fn, outputs_forward, outputs_backward)\n\n  return (carry_forward, carry_backward), outputs\n```\n\n----------------------------------------\n\nTITLE: Creating Parameter Masks and Gradient Updates with Flax and Optax\nDESCRIPTION: Creates separate parameter masks for biases and kernels, then configures an optimization chain that applies different learning rates to each parameter type. Uses Flax's ModelParamTraversal for parameter filtering and Optax's chain transformations for gradient updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/optax_update_guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbiases = flax.traverse_util.ModelParamTraversal(lambda p, _: 'bias' in p)\n\nall_false = jax.tree_util.tree_map(lambda _: False, params)\nkernels_mask = kernels.update(lambda _: True, all_false)\nbiases_mask = biases.update(lambda _: True, all_false)\n\ntx = optax.chain(\n    optax.trace(decay=momentum),\n    optax.masked(optax.scale(-learning_rate), kernels_mask),\n    optax.masked(optax.scale(-learning_rate * 0.1), biases_mask),\n)\n```\n\n----------------------------------------\n\nTITLE: Vectorized Dense Layer with Shared Parameters\nDESCRIPTION: Creates a batched Dense layer with parameters vectorized on the first axis but identical values due to shared RNG stream.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nBatchDense = nn.vmap(\n    nn.Dense,\n    in_axes=0, out_axes=0,\n    variable_axes={'params': 0},\n    split_rngs={'params': False})\n\nBatchDense(2).init(jax.random.key(0), x)\n```\n\n----------------------------------------\n\nTITLE: Inspecting NNX Model State\nDESCRIPTION: Demonstrates how to inspect internal model state and parameters in NNX modules with type-safe access.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(f'{model.count = }')\nprint(f'{model.blocks[0].linear.kernel = }')\n# print(f'{model.blocks.sdf.kernel = }') # typesafe inspection\n```\n\n----------------------------------------\n\nTITLE: JAX Integration - JIT Compilation\nDESCRIPTION: Implementation of forward pass with JIT compilation and state management.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngraphdef, state = model.split()\n\n@jax.jit\ndef forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array):\n  model = graphdef.merge(state)\n  y = model(x)\n  state, _ = model.split()\n  return y, state\n\nx = jnp.ones((2, 4))\ny, state = forward(graphdef,state, x)\n\nmodel.update(state)\n\nprint(f'{y.shape = }')\nprint(f'{model.count.value = }')\n```\n\n----------------------------------------\n\nTITLE: LSTM Scan Implementation\nDESCRIPTION: Implements a scanning operation over an LSTM cell for processing sequences, using Flax's lift.scan functionality for efficient processing.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef simple_scan(scope: Scope, xs):\n  init_carry = lstm_init_carry(xs.shape[:1], xs.shape[-1])\n  #   cell = scope.child(lstm, 'cell')\n  #   ys = []\n  #   for i in range(xs.shape[1]):\n  #       x = xs[:, i]\n  #       init_carry, y = cell(init_carry, x)\n  #       ys.append(y)\n  #   return init_carry, ys\n  lstm_scan = lift.scan(\n      lstm,\n      in_axes=1,\n      out_axes=1,\n      variable_broadcast='params',\n      split_rngs={'params': False},\n  )\n  return lstm_scan(scope, init_carry, xs)\n\n\nkey1, key2 = random.split(random.key(0), 2)\nxs = random.uniform(key1, (1, 5, 2))\n\n\ny, init_variables = init(simple_scan)(key2, xs)\n\nprint(\n    'initialized parameter shapes:\\n',\n    jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)),\n)\n```\n\n----------------------------------------\n\nTITLE: Merging NNX Model State and Graph Definition\nDESCRIPTION: Shows how to combine a model's graph definition with restored state parameters using NNX's merge function. Includes validation by checking the output shape matches expected dimensions (3,4).\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# The model is now good to use!\nmodel = nnx.merge(graphdef, state_restored)\nassert model(x).shape == (3, 4)\n```\n\n----------------------------------------\n\nTITLE: Using Lift Syntax with Flax Scan in Python\nDESCRIPTION: Demonstrates how to pass metadata parameters to AxisMetadata callbacks during lifted transformations. This allows for custom handling of axis operations when using transformations like nn.scan.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnn.scan(..., variable_axes={\"params\": 0}, metadata_params={nn.Partitioned.AXIS_NAME: \"layers\"})\n```\n\n----------------------------------------\n\nTITLE: Implementing Input and Recurrent Dropout in Flax RNNs\nDESCRIPTION: Demonstrates how to define both input and recurrent dropout in Flax RNN cells using the nn.Dropout module with custom rng_collection names. This allows for different dropout behavior in input and recurrent connections.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nself.dropout = nn.Dropout(...) # input dropout\nself.recurrent_dropout = nn.Dropout(..., rng_collection='recurrent_dropout')\n```\n\n----------------------------------------\n\nTITLE: Implementing Linen to NNX Module Conversion Example\nDESCRIPTION: Demonstrates converting a Linen linear layer (Dot product) to NNX using the bridge API. Shows the differences between Linen's stateless initialization and NNX's lazy initialization patterns through a practical example.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LinenDot(nn.Module):\n  out_dim: int\n  w_init: Callable[..., Any] = nn.initializers.lecun_normal()\n  @nn.compact\n  def __call__(self, x):\n    # Linen might need the input shape to create the weight!\n    w = self.param('w', self.w_init, (x.shape[-1], self.out_dim))\n    return x @ w\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\nmodel = bridge.ToNNX(LinenDot(64),\n                     rngs=nnx.Rngs(0))  # => `model = LinenDot(64)` in Linen\nbridge.lazy_init(model, x)              # => `var = model.init(key, x)` in Linen\ny = model(x)                            # => `y = model.apply(var, x)` in Linen\n\nnnx.display(model)\n```\n\n----------------------------------------\n\nTITLE: Creating an Abstract Model in Flax NNX\nDESCRIPTION: Shows how to create and manipulate an abstract model without allocating real parameter data using nnx.eval_shape.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nabs_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(0)))\ngdef, abs_state = nnx.split(abs_model)\npprint(abs_state)\n\nmodel = TwoLayerMLP(4, rngs=nnx.Rngs(0))\nabs_state['linear1']['kernel'].value = model.linear1.kernel\nabs_state['linear1']['bias'].value = model.linear1.bias\nabs_state['linear2']['kernel'].value = model.linear2.kernel\nabs_state['linear2']['bias'].value = model.linear2.bias\nnnx.update(abs_model, abs_state)\nnp.testing.assert_allclose(abs_model(x), model(x))  # They are equivalent now!\n```\n\n----------------------------------------\n\nTITLE: Manually Reproducing PRNG Keys in Flax Modules\nDESCRIPTION: This code snippet demonstrates how to manually reproduce the PRNG keys generated by Flax Modules using the make_rng method. It uses the same data that Flax uses internally to generate keys for different levels of the module hierarchy.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nstream_seed = jax.random.key(0)\nfor initial_data in ((), ('RNGSubModule_0',), ('RNGSubModule_0', 'RNGSubSubModule_0')):\n  if initial_data:\n    module_name = initial_data[-1]\n  else:\n    module_name = 'RNGModule'\n  for call_count in (1, 2):\n    hash_int = produce_hash(data=initial_data+(call_count,))\n    rng_key = jax.random.fold_in(stream_seed, jnp.uint32(hash_int))\n    print(f\"{module_name}, count {call_count}: {rng_key}\")\n```\n\n----------------------------------------\n\nTITLE: Compact Scaled MLP with Shape Inference\nDESCRIPTION: Demonstrates using the compact API with shape inference for parameter initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass CompactScaledMLP(nn.Module):\n  hidden_size: int\n  out_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    scale = self.param(\"scale\", nn.initializers.ones_init(), x.shape[-1:])\n    x *= scale[None]\n    a = nn.Dense(self.hidden_size)(x)\n    h = nn.relu(a)\n    return nn.Dense(self.out_size)(h)\n```\n\n----------------------------------------\n\nTITLE: Implementing Momentum Optimizer with Optax Gradient Transformations in Python\nDESCRIPTION: Example of creating a momentum optimizer using Optax's composable gradient transformations. This demonstrates how to chain trace (for momentum) with learning rate scheduling to create a complete optimizer.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport optax\n\ntx = optax.chain(\n    optax.trace(decay=0.9, nesterov=False),\n    optax.scale_by_schedule(lambda step: -get_learning_rate(step)),\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing necessary dependencies including JAX, Flax and typing utilities for neural network development.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom typing import Any, Callable, Sequence, Optional\nimport jax\nfrom jax import lax, random, numpy as jnp\nimport flax\nfrom flax import linen as nn\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using MLP Module\nDESCRIPTION: Shows how to construct and use an MLP Module including initialization and application of the model to input data.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmlp = MLP(hidden_size=5, out_size=3)\nx = jax.numpy.ones((1, 2))\nvariables = mlp.init(random.key(0), x)\ny = mlp.apply(variables, x)\n```\n\n----------------------------------------\n\nTITLE: Converting Partitioned Metadata to JAX Partition Specs\nDESCRIPTION: Function to transform the Partitioned metadata into partition specifications for JAX's pjit. This demonstrates how the metadata can be integrated with other APIs for hardware-specific optimization.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef to_sharding_spec(x):\n  if isinstance(x, Partitioned):\n    return PartitionSpec(*x.names)\n  else:\n    # fully replicated\n    return PartitionSpec()\n\n# Result: {\"params\": {\"kernel\": PartitionSpec(None, \"data\"), bias: PartitionSpec()}}\nvariables_pspec = jax.tree.map(to_sharding_spec, variables, is_leaf=lambda x: isinstance(x, Partitioned))\n```\n\n----------------------------------------\n\nTITLE: Converting Partitioned Linen Modules to NNX with Sharding\nDESCRIPTION: This example shows how to convert a Linen module with partition metadata to NNX while preserving sharding annotations. It uses nn.with_partitioning in Linen and demonstrates how it translates to NNX variable sharding properties for model parallelism.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass LinenDotWithPartitioning(nn.Module):\n  out_dim: int\n  @nn.compact\n  def __call__(self, x):\n    w = self.param('w', nn.with_partitioning(nn.initializers.lecun_normal(),\n                                             ('in', 'out')),\n                   (x.shape[-1], self.out_dim))\n    return x @ w\n\n@nnx.jit\ndef create_sharded_nnx_module(x):\n  model = bridge.lazy_init(\n    bridge.ToNNX(LinenDotWithPartitioning(64), rngs=nnx.Rngs(0)), x)\n  state = nnx.state(model)\n  sharded_state = nnx.with_sharding_constraint(state, nnx.get_partition_spec(state))\n  nnx.update(model, sharded_state)\n  return model\n\n\nprint(f'We have {len(jax.devices())} fake JAX devices now to partition this model...')\nmesh = jax.sharding.Mesh(devices=mesh_utils.create_device_mesh((2, 4)),\n                         axis_names=('in', 'out'))\nx = jax.random.normal(jax.random.key(42), (4, 32))\nwith mesh:\n  model = create_sharded_nnx_module(x)\n\nprint(type(model.w))           # `nnx.Param`\nprint(model.w.sharding)        # The partition annotation attached with `w`\nprint(model.w.value.sharding)  # The underlying JAX array is sharded across the 2x4 mesh\n```\n\n----------------------------------------\n\nTITLE: PRNG Key Splitting in Python with JAX/Flax\nDESCRIPTION: Demonstrates how to split PRNG keys for use with dropout, comparing implementations with and without dropout. Shows generation of separate keys for main operations, parameters, and dropout.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nroot_key = jax.random.key(seed=0)\nmain_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Modules in Haiku and Flax\nDESCRIPTION: Comparison of neural network module structure between Haiku and Flax. Haiku uses regular classes with __init__ methods whereas Flax uses dataclasses with the @nn.compact decorator for inline submodule definition.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport haiku as hk\n\nclass Block(hk.Module):\n  def __init__(self, features: int, name=None):\n    super().__init__(name=name)\n    self.features = features\n\n  def __call__(self, x, training: bool):\n    x = hk.Linear(self.features)(x)\n    x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(hk.Module):\n  def __init__(self, dmid: int, dout: int, name=None):\n    super().__init__(name=name)\n    self.dmid = dmid\n    self.dout = dout\n\n  def __call__(self, x, training: bool):\n    x = Block(self.dmid)(x, training)\n    x = hk.Linear(self.dout)(x)\n    return x\n```\n\nLANGUAGE: python\nCODE:\n```\nimport flax.linen as nn\n\nclass Block(nn.Module):\n  features: int\n\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = nn.Dense(self.features)(x)\n    x = nn.Dropout(0.5, deterministic=not training)(x)\n    x = jax.nn.relu(x)\n    return x\n\nclass Model(nn.Module):\n  dmid: int\n  dout: int\n\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = Block(self.dmid)(x, training)\n    x = nn.Dense(self.dout)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Manual Functionalization of MLP with vmap in Flax\nDESCRIPTION: A working implementation that manually converts an MLP module into init and apply functions before using vmap. It creates separate MLP parameters for each item in the input batch by manually handling parameter initialization and application with JAX's vmap.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ManualVmapMLP(nn.Module):\n  @nn.compact\n  def __call__(self, xs):\n    mlp = MLP(parent=None)\n    init_fn = lambda rng, xs: jax.vmap(mlp.init, in_axes=0)(random.split(rng, xs.shape[0]), xs)['params']\n    apply_fn = jax.vmap(mlp.apply, in_axes=0)\n    mlp_params = self.param('mlp', init_fn, xs)\n    return apply_fn({'params': mlp_params}, xs)\n\nxs = jnp.ones((3, 4))\nvariables = ManualVmapMLP().init(random.key(0), xs)\nprint(jax.tree_util.tree_map(jnp.shape, variables['params']))\n\"\"\"==>\n{\n    mlp: {\n        hidden: {\n            bias: (3, 4),\n            kernel: (3, 4, 4),\n        },\n        out: {\n            bias: (3, 1),\n            kernel: (3, 4, 1),\n        },\n    },\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Models with Convolutions and FC Layers to Flax\nDESCRIPTION: This example demonstrates how to convert a PyTorch model that combines convolutional and fully connected layers to Flax. It shows the necessary shape adjustments between layers.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass TModel(torch.nn.Module):\n\n    def __init__(self):\n      super(TModel, self).__init__()\n      self.conv = torch.nn.Conv2d(in_channels=3, out_channels=4, kernel_size=2, padding='valid')\n      self.fc = torch.nn.Linear(in_features=100, out_features=2)\n\n    def forward(self, x):\n      x = self.conv(x)\n      x = x.reshape(x.shape[0], -1)\n      x = self.fc(x)\n      return x\n\n\nt_model = TModel()\n\nclass JModel(nn.Module):\n\n    @nn.compact\n    def __call__(self, x):\n      x = nn.Conv(features=4, kernel_size=(2, 2), padding='valid', name='conv')(x)\n      # [N, H, W, C] -> [N, C, H, W]\n      x = jnp.transpose(x, (0, 3, 1, 2))\n      x = jnp.reshape(x, (x.shape[0], -1))\n      x = nn.Dense(features=2, name='fc')(x)\n      return x\n\n\nj_model = JModel()\n\nconv_kernel = t_model.state_dict()['conv.weight'].detach().cpu().numpy()\nconv_bias = t_model.state_dict()['conv.bias'].detach().cpu().numpy()\nfc_kernel = t_model.state_dict()['fc.weight'].detach().cpu().numpy()\nfc_bias = t_model.state_dict()['fc.bias'].detach().cpu().numpy()\n\n# [outC, inC, kH, kW] -> [kH, kW, inC, outC]\nconv_kernel = jnp.transpose(conv_kernel, (2, 3, 1, 0))\n\n# [outC, inC] -> [inC, outC]\nfc_kernel = jnp.transpose(fc_kernel, (1, 0))\n\nvariables = {'params': {'conv': {'kernel': conv_kernel, 'bias': conv_bias},\n                        'fc': {'kernel': fc_kernel, 'bias': fc_bias}}}\n\nkey = random.key(0)\nx = random.normal(key, (1, 6, 6, 3))\n\nj_out = j_model.apply(variables, x)\n\n# [N, H, W, C] -> [N, C, H, W]\nt_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))\nt_out = t_model(t_x)\nt_out = t_out.detach().cpu().numpy()\n\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Implementing Vectorized MLP with Linen vmap\nDESCRIPTION: Example of using nn.vmap to vectorize an MLP model, showing how to handle parameter vectorization and RNG splitting.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LinenVmapMLP(nn.Module):\n  @nn.compact\n  def __call__(self, xs):\n    VmapMLP = nn.vmap(MLP, variable_axes={'params': 0}, split_rngs={'params': True}, in_axes=0)\n    return VmapMLP(name='mlp')(xs)\n\nvariables = LinenVmapMLP().init(random.key(0), xs)\nprint(jax.tree_util.tree_map(jnp.shape, variables['params']))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model with Immutable State in Old Flax\nDESCRIPTION: Shows how to evaluate a model with immutable batch statistics using the old Flax API. It uses the nn.stateful context manager with mutable=False to ensure immutability.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef eval_step(model, model_state, batch):\n  with nn.stateful(model_state, mutable=False):\n    logits = model(batch['image'], train=False)\n  return compute_metrics(logits, batch['label'])\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Counter Module\nDESCRIPTION: Shows how to create a stateful module that maintains and updates a counter using nnx.Variable.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Count(nnx.Variable): pass\n\nclass Counter(nnx.Module):\n  def __init__(self):\n    self.count = Count(jnp.array(0))\n\n  def __call__(self):\n    self.count += 1\n\ncounter = Counter()\nprint(f'{counter.count.value = }')\ncounter()\nprint(f'{counter.count.value = }')\n```\n\n----------------------------------------\n\nTITLE: Gradient Computation and Variable Management\nDESCRIPTION: Shows basic gradient computation and variable management operations using Flax NNX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngrads = nnx.grad(loss_fn)(model)\n_, params, rest = nnx.split(model, nnx.Param, ...)\nparams = jax.tree.map(lambda p, g: p - 0.1 * g, params, grads)\nnnx.update(model, nnx.merge_state(params, rest))\n```\n\n----------------------------------------\n\nTITLE: Handling Checkpoint Structure Changes in Python with Orbax\nDESCRIPTION: Demonstrates how to handle changes in checkpoint structures, such as adding new fields to a custom TrainState. It shows saving a new structure and restoring an old checkpoint into the new structure.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass CustomTrainState(train_state.TrainState):\n    batch_stats: Any = None\n\ncustom_state = CustomTrainState.create(\n    apply_fn=state.apply_fn,\n    params=state.params,\n    tx=state.tx,\n    batch_stats=np.arange(10),\n)\n\ncustom_ckpt = {'model': custom_state, 'config': config, 'data': [x1]}\n# Use a custom state to read the old `TrainState` checkpoint.\ncustom_target = {'model': custom_state, 'config': None, 'data': [jnp.zeros_like(x1)]}\n\n# Save it in Orbax.\ncustom_save_args = orbax_utils.save_args_from_target(custom_ckpt)\ncheckpoint_manager.save(5, custom_ckpt, save_kwargs={'save_args': custom_save_args})\n```\n\n----------------------------------------\n\nTITLE: Creating an MLP with BatchNorm for vmap Compatibility\nDESCRIPTION: This code defines an MLP model using Flax's linen API with BatchNorm layers. It demonstrates how to configure BatchNorm to work with JAX's vmap by naming the batch axis, which is essential for vectorization with batch-dependent state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/state_params.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  hidden_size: int\n  out_size: int\n\n  @nn.compact\n  def __call__(self, x, train=False):\n    norm = partial(\n        nn.BatchNorm,\n        use_running_average=not train,\n        momentum=0.9,\n        epsilon=1e-5,\n        axis_name=\"batch\", # Name batch dim\n    )\n\n    x = nn.Dense(self.hidden_size)(x)\n    x = norm()(x)\n    x = nn.relu(x)\n    x = nn.Dense(self.hidden_size)(x)\n    x = norm()(x)\n    x = nn.relu(x)\n    y = nn.Dense(self.out_size)(x)\n\n    return y\n```\n\n----------------------------------------\n\nTITLE: Transforming Flax Module Methods with JAX\nDESCRIPTION: This snippet demonstrates the challenges of using JAX transformations inside Flax Modules, particularly with closures over Module instances.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Foo(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    dense = nn.Dense(x.shape[-1])\n    fn = lambda x: dense(x) + 1\n    # simply calling inner works fine\n    # return self.inner(x, fn)\n    # but applying a transformation doesn't:\n    vmap_inner = nn.vmap(Foo.inner, in_axes=0, variable_axes={\"params\": 0}, split_rngs={\"params\": True})\n    return vmap_inner(self, x, fn)\n\n  def inner(self, x, fn):\n    for i in range(3):\n      x = fn(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Legacy Training Step Implementation\nDESCRIPTION: Previous implementation of training step using the old optimizer API, showing state management and gradient updates.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef make_train_step(apply_fn):\n  @jax.jit\n  def train_step(optimizer, batch_stats, inputs, labels):\n\n    def loss_fn(params):\n      variables = {'params': params, 'batch_stats': batch_stats}\n      logits, new_model_state = apply_fn(\n          variables, inputs, mutable=['batch_stats'])\n      loss = xent_loss(logits, labels)\n      return loss, new_model_state['batch_stats']\n\n    (loss, new_batch_stats), grad = jax.value_and_grad(loss_fn, has_aux=True)(\n        optimizer.target)\n    lr = get_learning_rate(step)\n    new_optimizer = optimizer.apply_gradient(grad, learning_rate=lr)\n    return new_optimizer, new_batch_stats, loss\n\n  return train_step\n```\n\n----------------------------------------\n\nTITLE: Overriding Configuration Parameters\nDESCRIPTION: Example showing how to override configuration parameters like number of epochs from command line.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython main.py --workdir=./imagenet_default --config=configs/default.py \\\n--config.num_epochs=100\n```\n\n----------------------------------------\n\nTITLE: Setting Up TensorBoard for Training Visualization\nDESCRIPTION: Configures TensorBoard to display live updates during model training.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Get a live update during training - use the \"refresh\" button!\n# (In Jupyter[lab] start \"tensorboard\" in the local directory instead.)\nif 'google.colab' in str(get_ipython()):\n  %load_ext tensorboard\n  %tensorboard --logdir=./workdirs\n```\n\n----------------------------------------\n\nTITLE: FP8 dot operation with current scaling in Python\nDESCRIPTION: This snippet shows how to implement FP8 matrix multiplication using current scaling, which computes scaling factors directly from the operand tensors.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef dot_fp8(a, b):\n  a_scale = jnp.max(jnp.abs(A)) / E4M3_MAX\n  b_scale = jnp.max(jnp.abs(B)) / E4M3_MAX\n  a = fp8_ops.quantize(a, e4m3, a_scale, f32)\n  b = fp8_ops.quantize(b, e4m3, b_scale, f32)\n\n  c = jnp.dot(a, b, preferred_element_type=f32)\n  c = fp8_ops.dequantize(c, f32, a_scale * b_scale)\n  return c\n\nc = dot_fp8(a, b)\ncheck_fp8_call(dot_fp8.lower(a, b))\n```\n\n----------------------------------------\n\nTITLE: Embedding Layer Implementation\nDESCRIPTION: Defines an Embedding class and initialization function for creating embedding tables with lookup and attention operations.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@struct.dataclass\nclass Embedding:\n  table: np.ndarray\n\n  def lookup(self, indices):\n    return self.table[indices]\n\n  def attend(self, query):\n    return jnp.dot(query, self.table.T)\n\n\n# all the embedding module does is provide a convenient initializers\n\n\ndef embedding(\n    scope: Scope,\n    num_embeddings: int,\n    features: int,\n    init_fn=nn.linear.default_embed_init,\n) -> Embedding:\n  table = scope.param('table', init_fn, (num_embeddings, features))\n  return Embedding(table)\n\n\nembedding, _ = init(embedding)(random.key(0), num_embeddings=2, features=3)\nprint(embedding.table)\nprint(embedding.lookup(1))\nprint(\n    embedding.attend(\n        jnp.ones((\n            1,\n            3,\n        ))\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Managing RNG Edge Cases in Flax\nDESCRIPTION: Demonstrates an edge case where same values can be unintentionally generated due to string concatenation in hash functions, and how to avoid it using configuration flags.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass Leaf(nn.Module):\n  def __call__(self, x):\n    return x + jax.random.randint(self.make_rng(\"rng\"), (), 0, 100)\n\nclass Node(nn.Module):\n  leaf_name: str\n  @nn.compact\n  def __call__(self, x):\n    return Leaf(name=self.leaf_name)(x)\n\nclass Model(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    return (Node(name=\"ab\", leaf_name=\"cdef\")(x),\n            Node(name=\"abc\", leaf_name=\"def\")(x),\n    )\n```\n\n----------------------------------------\n\nTITLE: Applying JIT Compilation to Flax Module Subcomponents\nDESCRIPTION: This snippet shows how to use nn.jit to compile specific submodules within a Flax module. It creates an MLP where each Dense layer is JIT-compiled. Note that JIT compilation can affect RNG streams, potentially changing initialization results.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  features: Sequence[int]\n\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      # JIT the Module (it's __call__ fn by default.)\n      x = nn.jit(nn.Dense)(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(3), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = MLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dropout RNG Splitting in Flax nn.scan\nDESCRIPTION: Shows how to configure RNG splitting for different types of dropout when using nn.scan or nn.RNN. This allows for separate control over input and recurrent dropout randomness.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnn.scan(scan_fn, ..., split_rngs={'dropout': True, 'recurrent_dropout': False})\n```\n\n----------------------------------------\n\nTITLE: Using Dropout in Loss Function with Linen\nDESCRIPTION: Demonstrates how to use dropout in a loss function with the Linen API. It explicitly passes the dropout RNG to the apply method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(params, dropout_rng):\n  logits = Transformer().apply(\n    {'params': params}, inputs, rngs={'dropout': dropout_rng})\n```\n\n----------------------------------------\n\nTITLE: RNNBase Protocol Definition in Flax\nDESCRIPTION: Definition of the RNNBase protocol class, which specifies the API that all RNN layers should implement to be compatible with the Bidirectional layer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RNNBase(Protocol):\n  def __call__(\n      self,\n      inputs: jax.Array,\n      *,\n      initial_carry: Optional[Carry] = None,\n      init_key: Optional[random.KeyArray] = None,\n      seq_lengths: Optional[Array] = None,\n      return_carry: Optional[bool] = None,\n      time_major: Optional[bool] = None,\n      reverse: Optional[bool] = None,\n      keep_order: Optional[bool] = None,\n  ) -> Union[Output, Tuple[Carry, Output]]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Swapping Weight Array in Flax Model\nDESCRIPTION: This code demonstrates how to swap the weight array of a Flax model in-place while maintaining its functionality. It uses JAX's random number generation to create a new weight array and asserts that the model output changes after the swap. This technique can be useful for weight initialization or modification experiments.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# In-place swap your weight array and the model still works!\nmodel.w.value = jax.random.normal(jax.random.key(1), (32, 64))\nassert not jnp.allclose(y, model(x))\n```\n\n----------------------------------------\n\nTITLE: Replacing FrozenDict copy() with flax.core.copy\nDESCRIPTION: Shows how to replace FrozenDict's copy() method with the utility function flax.core.copy that works with both FrozenDict and regular dict types.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nvariables = variables.copy(add_or_replace={'other_variables': other_variables})\n```\n\nLANGUAGE: python\nCODE:\n```\nvariables = flax.core.copy(variables, add_or_replace={'other_variables': other_variables})\n```\n\n----------------------------------------\n\nTITLE: Defining a Compact MLP Module in Flax\nDESCRIPTION: This code shows how to define an MLP module using the @compact decorator, which allows for inline submodule declarations in Flax Linen.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleMLP(nn.Module):\n  features: Sequence[int]\n\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      x = nn.Dense(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n      # providing a name is optional though!\n      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n      # x = nn.Dense(feat)(x)\n    return x\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = SimpleMLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Converting NNX Dot Module to Linen\nDESCRIPTION: Demonstrates converting a basic NNX dot product module to Linen using bridge.to_linen. Shows initialization and application of the converted module with proper parameter handling.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass NNXDot(nnx.Module):\n  def __init__(self, in_dim: int, out_dim: int, rngs: nnx.Rngs):\n    self.w = nnx.Param(nnx.initializers.lecun_normal()(\n      rngs.params(), (in_dim, out_dim)))\n  def __call__(self, x: jax.Array):\n    return x @ self.w\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\n# Pass in the arguments, not an actual module\nmodel = bridge.to_linen(NNXDot, 32, out_dim=64)\nvariables = model.init(jax.random.key(0), x)\ny = model.apply(variables, x)\n\nprint(list(variables.keys()))\nprint(variables['params']['w'].shape)  # => (32, 64)\nprint(y.shape)                         # => (4, 64)\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch BatchNorm2d to Flax BatchNorm\nDESCRIPTION: This code shows how to convert a PyTorch BatchNorm2d layer to a Flax BatchNorm layer. It addresses the differences in momentum calculation between the two implementations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nt_bn = torch.nn.BatchNorm2d(num_features=3, momentum=0.1)\nt_bn.eval()\n\nscale = t_bn.weight.detach().cpu().numpy()\nbias = t_bn.bias.detach().cpu().numpy()\nmean = t_bn.running_mean.detach().cpu().numpy()\nvar = t_bn.running_var.detach().cpu().numpy()\n\nvariables = {'params': {'scale': scale, 'bias': bias},\n             'batch_stats': {'mean': mean, 'var': var}}\n\nkey = random.key(0)\nx = random.normal(key, (1, 6, 6, 3))\n\nj_bn = nn.BatchNorm(momentum=0.9, use_running_average=True)\n\nj_out = j_bn.apply(variables, x)\n\n# [N, H, W, C] -> [N, C, H, W]\nt_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))\nt_out = t_bn(t_x)\n# [N, C, H, W] -> [N, H, W, C]\nt_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))\n\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Defining an Explicit Dense Layer with Shape Information in Flax\nDESCRIPTION: This code shows how to create a Dense layer with explicit shape information and parameter declarations in the setup method of a Flax Linen module.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ExplicitDense(nn.Module):\n  features_in: int  # <-- explicit input shape\n  features: int\n  kernel_init: Callable = nn.initializers.lecun_normal()\n  bias_init: Callable = nn.initializers.zeros_init()\n\n  def setup(self):\n    self.kernel = self.param('kernel',\n                             self.kernel_init,\n                             (self.features_in, self.features))\n    self.bias = self.param('bias', self.bias_init, (self.features,))\n\n  def __call__(self, inputs):\n    y = lax.dot_general(inputs, self.kernel,\n                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n    y = y + self.bias\n    return y\n\nkey1, key2 = random.split(random.key(0), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = ExplicitDense(features_in=4, features=3)\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameters:\\n', init_variables)\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Defining AxisMetadata Abstract Base Class in Python\nDESCRIPTION: This code snippet defines the AxisMetadata abstract base class, which serves as the foundation for the proposed axis metadata API. It includes abstract methods for unboxing, adding an axis, and removing an axis.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTAxisMetadata = TypeVar(\"TAxisMetadata\", bound=\"AxisMetadata\")\n\nclass AxisMetadata(metaclass=abc.ABCMeta):\n  \"\"\"Abstract base class for boxed Metadata.\n\n  ``AxisMetadata`` enables arbitrary, per axis metadata for variables.\n  By using ``unbox`` the metadata is stripped away to obtain the original\n  variables. By using unboxing, most code handling variables does not need\n  to handle ``AxisMetadata`` specifically, but can directly operate on the JAX\n  arrays that they wrap.\n\n  Additionally, ``AxisMetadata`` supports updating metadata whenever an axis\n  is added or removed by a functional transformation\n  (e.g.: ``nn.scan`` or ``nn.vmap``) using the ``add_axis`` and ``remove_axis``\n  methods.\n\n  By extending ``AxisMetadata``, custom metadata can be stored. See\n  ``Partitioned`` for a specific implementation.\n  \"\"\"\n\n  @abc.abstractmethod\n  def unbox(self) -> Any:\n    \"\"\"Returns the content of the AxisMetadata box.\n\n    Note that unlike ``meta.unbox`` the unbox call should recursively unbox\n    metadata. It should simply return value that it wraps directly even\n    if that value itself is an instance of AxisMetadata.\n\n    In practise, AxisMetadata subclasses should be registred as PyTree nodes to\n    support passing instances to JAX and Flax APIs. The leaves returned for this\n    note should correspond to the value returned by unbox.\n\n    Returns:\n      The unboxed value.\n    \"\"\"\n    pass\n\n  @abc.abstractmethod\n  def add_axis(self: TAxisMetadata, index: int,\n               params: Dict[Any, Any]) -> TAxisMetadata:\n    \"\"\"Adds a new axis to the axis metadata.\n\n    Note that add_axis and remove_axis should act as each other's inverse\n    (meaning: ``x.add_axis(i, p).remove_axis(i, p) == x``)\n\n    Args:\n      index: The position at which the new axis will be inserted\n      params: An arbitrary dictionary of parameters passed by the transformation\n        that introduces the new axis (e.g.: ``nn.scan`` or ``nn.vmap``). The\n        user passes this dictionary as the `metadata_param` argument to the\n        transformation.\n    Returns:\n      A new instance of the same type as self and with the same ``unbox``\n      content with updated axis metadata.\n    \"\"\"\n    pass\n\n  @abc.abstractmethod\n  def remove_axis(self: TAxisMetadata, index: int,\n                  params: Dict[Any, Any]) -> TAxisMetadata:\n    \"\"\"Removes an axis from the axis metadata.\n\n    Note that add_axis and remove_axis should act as each other's inverse\n    (meaning: ``x.remove_axis(i, p).add_axis(i, p) == x``)\n\n    Args:\n      index: The position of the axis that is to be removed\n      params: An arbitrary dictionary of parameters passed by the transformation\n        that introduced the axis (e.g.: ``nn.scan`` or ``nn.vmap``). The\n        user passes this dictionary as the `metadata_param` argument to the\n        transformation.\n    Returns:\n      A new instance of the same type as self and with the same ``unbox``\n      content with updated axis metadata.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing LSTM Layer with Manual Carry Handling in Flax\nDESCRIPTION: This snippet demonstrates the current complex way of implementing an LSTM layer in Flax, which involves manual creation and handling of the carry/memory and setting up nn.scan.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@nn.compact\ndef __call__(self, x):\n  LSTM = nn.scan(\n    nn.LSTMCell, variable_broadcast=\"params\", split_rngs={\"params\": False}\n  )\n  carry = LSTM.initialize_carry(\n    jax.random.key(0), batch_dims=x.shape[:1], size=self.hidden_size\n  )\n  carry, x = LSTM()(carry, x)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Custom Hyperparameter Configuration Command\nDESCRIPTION: Example of running MNIST training with custom learning rate and number of epochs using config_flags override functionality.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython main.py \\\n--workdir=/tmp/mnist --config=configs/default.py \\\n--config.learning_rate=0.05 --config.num_epochs=5\n```\n\n----------------------------------------\n\nTITLE: Visualizing MNIST Test Predictions with Matplotlib\nDESCRIPTION: Creates a visualization grid showing test images and their predicted labels using matplotlib. Displays a 5x5 grid of grayscale images with corresponding model predictions.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntest_batch = test_ds.as_numpy_iterator().next()\npred = pred_step(model, test_batch)\n\nfig, axs = plt.subplots(5, 5, figsize=(12, 12))\nfor i, ax in enumerate(axs.flatten()):\n  ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')\n  ax.set_title(f'label={pred[i]}')\n  ax.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Visualizing misclassified test images\nDESCRIPTION: Displays a grid of 25 misclassified test images along with their predicted labels to help understand the model's errors and potential areas for improvement.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Show some of them.\nshow_img_grid(\n    [test_ds['image'][idx] for idx in error_idxs[:25]],\n    [f'pred={logits[idx].argmax()}' for idx in error_idxs[:25]],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Multi-process Array with JAX Sharding\nDESCRIPTION: Creates a sharded array across multiple devices using JAX mesh and device partitioning. Demonstrates setting up device mesh and applying named sharding to array data.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.sharding import PartitionSpec, NamedSharding\n\n# Create an array sharded across multiple devices.\nmesh_shape = (4, 2)\ndevices = np.asarray(jax.devices()).reshape(*mesh_shape)\nmesh = jax.sharding.Mesh(devices, ('x', 'y'))\n\nmp_array = jax.device_put(np.arange(8 * 2).reshape(8, 2),\n                          NamedSharding(mesh, PartitionSpec('x', 'y')))\n\n# Make it a pytree.\nmp_ckpt = {'model': mp_array}\n```\n\n----------------------------------------\n\nTITLE: Profiling Training Performance on TPU Pods\nDESCRIPTION: Creates a custom utility function to measure performance on Google TPU pods by ensuring computation is complete before timing stops. Uses block_until_ready to synchronize before measurements.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\n\ndef block_all(xs):\n  jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)\n  return xs\n\nwith mesh:\n  new_state = block_all(train_step(sharded_model, optimizer, input, label))\n```\n\n----------------------------------------\n\nTITLE: Implementing Bidirectional LSTM in Flax\nDESCRIPTION: Example of creating a bidirectional LSTM using the proposed Bidirectional and RNN layers. This showcases how to process sequences in both forward and backward directions.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nforward_rnn = nn.RNN(nn.LSTMCell(), cell_size=32)\nbackward_rnn = nn.RNN(nn.LSTMCell(), cell_size=32)\n# Bidirectional combinator.\nbi_rnn = nn.Bidirectional(forward_rnn, backward_rnn)\n# Encodes a batch of input sequences in both directions.\ncarry, outputs = bi_rnn(inputs, seq_lengths)\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX PRNG for Efficient Parallelization\nDESCRIPTION: Sets JAX configuration to make the PRNG more efficiently auto-parallelizable under jax.jit, and verifies the configuration is correctly applied.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njax.config.update('jax_threefry_partitionable', True)\nassert jax.config.jax_threefry_partitionable == True\nassert jax.config.jax_default_prng_impl == 'threefry2x32'\n```\n\n----------------------------------------\n\nTITLE: Using Partial for JAX-compatible Closures in Flax\nDESCRIPTION: This example shows how to use a custom Partial class to create JAX-compatible closures for use in transformed Flax Module methods.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Partial(flax.struct.PyTreeNode):\n  fn: Callable = flax.struct.field(pytree_node=False)\n  args: Iterable[Any]\n\n  def __call__(self, *args, **kwargs):\n    return self.fn(*(tuple(self.args) + args), **kwargs)\n\nclass Foo(nn.Module):\n\n  @nn.compact\n  def __call__(self, x):\n    dense = nn.Dense(x.shape[-1])\n    fn = lambda mdl, x: mdl(x) + 1\n    vmap_inner = nn.vmap(Foo.inner, in_axes=0, variable_axes={\"params\": 0}, split_rngs={\"params\": True})\n    return vmap_inner(self, x, Partial(fn, [dense]))\n\n  def inner(self, x, fn):\n    for i in range(3):\n      x = fn(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Asserting NNX Parameter Types - Python\nDESCRIPTION: Verifies that model dot weight is an NNX Parameter wrapping a JAX array value\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nassert isinstance(model.dot.w, nnx.Param)\nassert isinstance(model.dot.w.value, jax.Array)\n```\n\n----------------------------------------\n\nTITLE: Image Visualization Functions\nDESCRIPTION: Helper functions for displaying single images and grids of images with proper normalization\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef show_img(img, ax=None, title=None):\n  \"\"\"Shows a single image.\"\"\"\n  if ax is None:\n    ax = plt.gca()\n  img *= tf.constant(input_pipeline.STDDEV_RGB, shape=[1, 1, 3], dtype=img.dtype)\n  img += tf.constant(input_pipeline.MEAN_RGB, shape=[1, 1, 3], dtype=img.dtype)\n  img = np.clip(img.numpy().astype(int), 0, 255)\n  ax.imshow(img)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title:\n    ax.set_title(title)\n\ndef show_img_grid(imgs, titles):\n  \"\"\"Shows a grid of images.\"\"\"\n  n = int(np.ceil(len(imgs)**.5))\n  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n  for i, (img, title) in enumerate(zip(imgs, titles)):\n    show_img(img, axs[i // n][i % n], title)\n```\n\n----------------------------------------\n\nTITLE: Comparing remat_scan() and scan(remat(...)) in Flax\nDESCRIPTION: This snippet compares the usage of flax.linen.remat_scan() and flax.linen.scan with flax.linen.remat. It highlights that remat_scan() is more limited and treats inputs and outputs as carries, while scan(remat(...)) offers more flexibility with additional parameters like in_axes and out_axes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nremat_scan()\n```\n\nLANGUAGE: python\nCODE:\n```\nscan(remat(...))\n```\n\n----------------------------------------\n\nTITLE: Flattening Model Parameters\nDESCRIPTION: Converting the nested parameter structure into a flattened dictionary for easier manipulation. The flatten_dict function from traverse_util is used with a separator to create keys that represent the parameter hierarchy.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get a flattened key-value list.\nflat_params = traverse_util.flatten_dict(params, sep='/')\n\njax.tree_util.tree_map(jnp.shape, flat_params)\n```\n\n----------------------------------------\n\nTITLE: Defining Flax Model with Dropout\nDESCRIPTION: Shows how to create a Flax neural network model with dropout layer implementation. The model includes a Dense layer followed by Dropout with 50% rate and training flag parameter.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(nn.Module):\n  num_neurons: int\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = nn.Dense(self.num_neurons)(x)\n    x = nn.Dropout(rate=0.5, deterministic=not training)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Demonstrating RNG Behavior in Flax Modules and Sub-Modules\nDESCRIPTION: This code snippet defines a hierarchy of Flax Modules to illustrate how the make_rng method behaves across different levels of the module hierarchy. It shows how RNG streams are managed for the main module and its sub-modules.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass RNGSubSubModule(nn.Module):\n  def __call__(self):\n    print(f\"{self.name}, count 1: {self.make_rng('rng_stream')}\")\n    print(f\"{self.name}, count 2: {self.make_rng('rng_stream')}\")\n\nclass RNGSubModule(nn.Module):\n  @nn.compact\n  def __call__(self):\n    print(f\"{self.name}, count 1: {self.make_rng('rng_stream')}\")\n    print(f\"{self.name}, count 2: {self.make_rng('rng_stream')}\")\n    RNGSubSubModule()()\n\nclass RNGModule(nn.Module):\n  @nn.compact\n  def __call__(self):\n    print(f\"RNGModule, count 1: {self.make_rng('rng_stream')}\")\n    print(f\"RNGModule, count 2: {self.make_rng('rng_stream')}\")\n    RNGSubModule()()\n\nrng_module = RNGModule()\nvariables = rng_module.init({'rng_stream': jax.random.key(0)})\n```\n\n----------------------------------------\n\nTITLE: Flattening Optimizer State Components\nDESCRIPTION: Accessing and flattening the internal components of the Adam optimizer state (momentum and variance terms) for manipulation. Similar to model parameters, optimizer states can be flattened using traverse_util.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nflat_mu = traverse_util.flatten_dict(opt_state[0].mu, sep='/')\nflat_nu = traverse_util.flatten_dict(opt_state[0].nu, sep='/')\n\njax.tree_util.tree_map(jnp.shape, flat_mu)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Flax Dependencies\nDESCRIPTION: Sets up the required imports from JAX and Flax libraries including numpy functionality, random operations, and neural network components.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport jax\nfrom jax import numpy as jnp, random, lax\nimport numpy as np\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import linen as nn, struct\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.core import Scope, init, apply, Array, lift, unfreeze\n```\n\n----------------------------------------\n\nTITLE: Using Dropout in Loss Function with Old Flax\nDESCRIPTION: Demonstrates how to use dropout in a loss function with the old Flax API. It uses the nn.stochastic context manager to handle the dropout randomness.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(model, dropout_rng):\n  with nn.stochastic(dropout_rng):\n    logits = model(inputs)\n```\n\n----------------------------------------\n\nTITLE: Creating TrainState\nDESCRIPTION: Initializes the Flax TrainState with the model, parameters, and configured optimizer for training.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.training.train_state import TrainState\n\nstate = TrainState.create(\n  apply_fn=model.apply,\n  params=params,\n  tx=tx)\n```\n\n----------------------------------------\n\nTITLE: Defining Modified MLP Model with Bias in Python\nDESCRIPTION: Defines a ModifiedTwoLayerMLP class that extends the original TwoLayerMLP by adding bias to the linear layers. This change in model structure requires adaptation when loading old checkpoints.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass ModifiedTwoLayerMLP(nnx.Module):\n  \"\"\"A modified version of TwoLayerMLP, which requires bias arrays.\"\"\"\n  def __init__(self, dim, rngs: nnx.Rngs):\n    self.linear1 = nnx.Linear(dim, dim, rngs=rngs, use_bias=True)  # We need bias now!\n    self.linear2 = nnx.Linear(dim, dim, rngs=rngs, use_bias=True)  # We need bias now!\n\n  def __call__(self, x):\n    x = self.linear1(x)\n    return self.linear2(x)\n```\n\n----------------------------------------\n\nTITLE: Consistent Aliasing Example in NNX Transforms\nDESCRIPTION: Demonstrates correct usage of consistent aliasing in NNX transforms, ensuring all aliases receive the same lifting/lowering specification.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(m1_axes, m2_axes, m1_axes), out_axes=m2_axes)\ndef f(m1, m2, m1_alias):\n  return m2\n\nm2 = f(m1, m2, m1)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current NNX Transform Behavior in Python\nDESCRIPTION: Shows how NNX transforms currently work, treating input Modules as a single unit and using state_axes for vectorization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(IGNORE, IGNORE), state_axes={BatchStat: None, ...: 0})\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Manually Reproducing make_rng PRNG Keys\nDESCRIPTION: Demonstrates how to manually reproduce the PRNG keys generated by make_rng by implementing the same hashing and folding logic.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstream_seed = jax.random.key(0)\nfor call_count in range(1, 4):\n  hash_int = produce_hash(data=(call_count,))\n  print(jax.random.fold_in(stream_seed, jnp.uint32(hash_int)))\n```\n\n----------------------------------------\n\nTITLE: Installing Flax and Importing Dependencies\nDESCRIPTION: Shows how to install Flax using pip and import required dependencies including flax.nnx, jax, and jax.numpy.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/nnx_basics.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ! pip install -U flax\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Generating Automatic Sharding for Flax Model Output\nDESCRIPTION: This snippet shows how to automatically generate sharding for the model output using jax.eval_shape and flax.linen.get_sharding functions.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nabstract_variables = jax.eval_shape(\n    functools.partial(init_fn, model=model, optimizer=optimizer), k, x)\n\n# This `state_sharding` has the same pytree structure as `state`, the output\n# of the `init_fn`.\nstate_sharding = nn.get_sharding(abstract_variables, mesh)\nstate_sharding\n```\n\n----------------------------------------\n\nTITLE: Manually Reproducing PRNG Keys for Parameter Initialization\nDESCRIPTION: This code snippet shows how to manually reproduce the PRNG keys used for parameter initialization in a Flax Module. It demonstrates the relationship between module hierarchy, RNG streams, and call counts in key generation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nparams_seed = jax.random.key(0)\nother_seed = jax.random.key(1)\nfor initial_data, count, seed, shape in (\n  (('Dense_0',), 1, params_seed, (2, 2)),\n  ((), 1, params_seed, (2, 2)),\n  ((), 2, params_seed, (2, 2)),\n  ((), 1, other_seed, (2, 2)),\n  (('Dense_1',), 1, params_seed, (2, 2)),\n  (('Dense_1',), 2, params_seed, (1, 2)),\n):\n  hash_int = produce_hash(data=(*initial_data, count))\n  rng_key = jax.random.fold_in(seed, jnp.uint32(hash_int))\n  print(jax.random.normal(rng_key, shape))\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Multi-Device Environment\nDESCRIPTION: Sets up JAX environment to emulate 8 devices on CPU for testing multi-device functionality using XLA flags.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Flax and Orbax\nDESCRIPTION: This code block imports necessary libraries and modules for working with Flax, JAX, and Orbax. It includes imports for checkpointing, neural network layers, and optimization.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, Any\nimport shutil\n\nimport numpy as np\nimport jax\nfrom jax import random, numpy as jnp\n\nimport flax\nfrom flax import linen as nn\nfrom flax.training import checkpoints, train_state\nfrom flax import struct, serialization\nimport orbax.checkpoint\n\nimport optax\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Nested Dataclasses in Python\nDESCRIPTION: This snippet demonstrates the creation of a nested dataclass structure and shows how to modify it using both the traditional dataclasses.replace method and the new Cursor API from flax.cursor.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.cursor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from flax.cursor import cursor\n>>> import dataclasses\n>>> from typing import Any\n\n>>> @dataclasses.dataclass(frozen=True)\n>>> class A:\n...   x: Any\n\n>>> a = A(A(A(A(A(A(A(0)))))))\n\n# Using dataclasses.replace\n>>> a2 = dataclasses.replace(\n...   a,\n...   x=dataclasses.replace(\n...     a.x,\n...     x=dataclasses.replace(\n...       a.x.x,\n...       x=dataclasses.replace(\n...         a.x.x.x,\n...         x=dataclasses.replace(\n...           a.x.x.x.x,\n...           x=dataclasses.replace(\n...             a.x.x.x.x.x,\n...             x=dataclasses.replace(a.x.x.x.x.x.x, x=1),\n...           ),\n...         ),\n...       ),\n...     ),\n...   ),\n... )\n\n# Using Cursor API\n>>> a3 = cursor(a).x.x.x.x.x.x.x.set(1)\n>>> assert a2 == a3\n```\n\n----------------------------------------\n\nTITLE: Updating a Simple LSTM Module with Minimal Changes\nDESCRIPTION: Shows how to upgrade a module that wraps an RNN cell with the minimum changes required, extracting features from the carry and using parent=None during initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleLSTM(nn.Module):\n\n  @functools.partial(\n    nn.transforms.scan,\n    variable_broadcast='params',\n    in_axes=1, out_axes=1,\n    split_rngs={'params': False})\n  @nn.compact\n  def __call__(self, carry, x):\n\n    return nn.OptimizedLSTMCell()(carry, x)\n\n  @staticmethod\n  def initialize_carry(batch_dims, hidden_size):\n    return nn.OptimizedLSTMCell.initialize_carry(\n      jax.random.key(0), batch_dims, hidden_size)\n\n---\n\nclass SimpleLSTM(nn.Module):\n\n  @functools.partial(\n    nn.transforms.scan,\n    variable_broadcast='params',\n    in_axes=1, out_axes=1,\n    split_rngs={'params': False})\n  @nn.compact\n  def __call__(self, carry, x):\n    features = carry[0].shape[-1]\n    return nn.OptimizedLSTMCell(features)(carry, x)\n\n  @staticmethod\n  def initialize_carry(batch_dims, hidden_size):\n    return nn.OptimizedLSTMCell(hidden_size, parent=None).initialize_carry(\n      jax.random.key(0), (*batch_dims, hidden_size))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Filter Predicate for Flax NNX Parameters\nDESCRIPTION: Shows how to create a custom filter predicate function for Flax NNX parameters, which checks if a value is an instance of nnx.Param or has a type attribute that is a subclass of nnx.Param.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef is_param(path, value) -> bool:\n  return isinstance(value, nnx.Param) or (\n    hasattr(value, 'type') and issubclass(value.type, nnx.Param)\n  )\n\nprint(f'{is_param((), nnx.Param(0)) = }')\nprint(f'{is_param((), nnx.VariableState(type=nnx.Param, value=0)) = }')\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Model\nDESCRIPTION: Implements the main training loop with periodic evaluation and visualization of metrics.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\nmetrics_history = {\n  'train_loss': [],\n  'train_accuracy': [],\n  'test_loss': [],\n  'test_accuracy': [],\n}\n\nfor step, batch in enumerate(train_ds.as_numpy_iterator()):\n  train_step(model, optimizer, metrics, batch)\n\n  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):\n    for metric, value in metrics.compute().items():\n      metrics_history[f'train_{metric}'].append(value)\n    metrics.reset()\n\n    for test_batch in test_ds.as_numpy_iterator():\n      eval_step(model, metrics, test_batch)\n\n    for metric, value in metrics.compute().items():\n      metrics_history[f'test_{metric}'].append(value)\n    metrics.reset()\n\n    clear_output(wait=True)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    ax1.set_title('Loss')\n    ax2.set_title('Accuracy')\n    for dataset in ('train', 'test'):\n      ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n      ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n    ax1.legend()\n    ax2.legend()\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation of Flax Modules\nDESCRIPTION: Demonstrates how to use JAX's just-in-time compilation (JIT) on specific submodules within a Flax model. The example shows a multi-layer perceptron with JIT-compiled Dense layers.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  features: Sequence[int]\n\n  @nn.compact\n  def __call__(self, inputs):\n    x = inputs\n    for i, feat in enumerate(self.features):\n      # JIT the Module (it's __call__ fn by default.)\n      x = nn.jit(nn.Dense)(feat, name=f'layers_{i}')(x)\n      if i != len(self.features) - 1:\n        x = nn.relu(x)\n    return x\n\nkey1, key2 = random.split(random.key(3), 2)\nx = random.uniform(key1, (4,4))\n\nmodel = MLP(features=[3,4,5])\ninit_variables = model.init(key2, x)\ny = model.apply(init_variables, x)\n\nprint('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Setting up TPU Environment\nDESCRIPTION: Configures TPU environment and displays available TPU devices using JAX\nSOURCE: https://github.com/google/flax/blob/main/tests/colab_tpu_jax_version.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax, jax.tools.colab_tpu\njax.tools.colab_tpu.setup_tpu()\njax.devices()\n```\n\n----------------------------------------\n\nTITLE: Flax Module Class Documentation Example\nDESCRIPTION: Complete example of a properly documented Flax module class with attributes, methods, and type hints\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DenseGeneral(Module):\n  \"\"\"A linear transformation with flexible axes.\n    Attributes:\n      features: int or tuple with number of output features.\n      axis: int or tuple with axes to apply the transformation on. For instance,\n        (-2, -1) will apply the transformation to the last two axes.\n      batch_dims: tuple with batch axes.\n      use_bias: whether to add a bias to the output (default: True).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      bias_init: initializer function for the bias.\n      precision: numerical precision of the computation see `jax.lax.Precision`\n        for details.\n  \"\"\"\n  features: Union[int, Iterable[int]]\n  axis: Union[int, Iterable[int]] = -1\n  batch_dims: Iterable[int] = ()\n  use_bias: bool = True\n  dtype: Dtype = jnp.float32\n  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init\n  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = zeros\n  precision: Any = None\n\n  @compact\n  def __call__(self, inputs: Array) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n    Args:\n      inputs: The nd-array to be transformed.\n    Returns:\n      The transformed input.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Restoring Checkpoints Without Target Pytree Using Orbax in Python\nDESCRIPTION: Demonstrates how to restore checkpoints without a target pytree using Orbax. This is done by passing 'item=None' to the Checkpointer's restore method, which triggers the restoration without a specific target structure.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNOTARGET_CKPT_DIR = '/tmp/orbax_upgrade/no_target'\n\n# A stateless object, can be created on the fly.\nckptr = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())\nckptr.save(NOTARGET_CKPT_DIR, CKPT_PYTREE,\n           save_args=flax.training.orbax_utils.save_args_from_target(CKPT_PYTREE))\nckptr.restore(NOTARGET_CKPT_DIR, item=None)\n```\n\n----------------------------------------\n\nTITLE: Using Lift Syntax for Axis Transformations in Python\nDESCRIPTION: Shows how to use lifted transformations with metadata parameters using nn.scan. The example demonstrates passing variable axes and metadata parameters to handle axis transformations with custom AxisMetadata classes.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnn.scan(..., variable_axes={\"params\": 0}, metadata_params={nn.Partitioned.AXIS_NAME: \"layers\"})\n```\n\n----------------------------------------\n\nTITLE: NNX Transforms as Method Decorators\nDESCRIPTION: Demonstrates how NNX transforms can be used as method decorators since Module methods are simply functions taking a Module as the first argument. This example creates a WeightStack with vectorized initialization and forward pass.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass WeightStack(nnx.Module):\n  @nnx.vmap(in_axes=(0, 0), out_axes=0)\n  def __init__(self, seed: jax.Array):\n    self.kernel = nnx.Param(random.uniform(random.key(seed), (2, 3)))\n    self.bias = nnx.Param(jnp.zeros((3,)))\n\n  @nnx.vmap(in_axes=(0, 0), out_axes=1)\n  def __call__(self, x: jax.Array):\n    assert self.kernel.ndim == 2, 'Batch dimensions not allowed'\n    assert x.ndim == 1, 'Batch dimensions not allowed'\n    return x @ self.kernel + self.bias\n\nweights = WeightStack(jnp.arange(10))\n\nx = jax.random.normal(random.key(1), (10, 2))\ny = weights(x)\n```\n\n----------------------------------------\n\nTITLE: Importing MultiHeadAttention Class from Flax NNX Module\nDESCRIPTION: This snippet shows how to import the MultiHeadAttention class from the flax.nnx module. MultiHeadAttention is likely an implementation of the multi-head attention mechanism used in transformer architectures.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/attention.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import MultiHeadAttention\n```\n\n----------------------------------------\n\nTITLE: Implementing Hash Function for PRNG Key Generation\nDESCRIPTION: Shows the implementation of the hash function used internally by Flax's make_rng to convert data tuples into integers for PRNG key folding.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef produce_hash(data):\n  m = hashlib.sha1()\n  for x in data:\n    if isinstance(x, str):\n      m.update(x.encode('utf-8'))\n    elif isinstance(x, int):\n      m.update(x.to_bytes((x.bit_length() + 7) // 8, byteorder='big'))\n    else:\n      raise ValueError(f'Expected int or string, got: {x}')\n  d = m.digest()\n  hash_int = int.from_bytes(d[:4], byteorder='big')\n  return hash_int\n```\n\n----------------------------------------\n\nTITLE: Invalid shared reference passing with inconsistent axes\nDESCRIPTION: Example showing how passing the same shared reference through different Modules with different vectorization axes creates inconsistent aliasing.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshared = Shared()\nm1, m2 = Foo(shared), Foo(shared)\n\n@nnx.vmap(in_axes=(0, 1))\ndef f(m1, m2):  # shared is passed through both\n  ...\n```\n\n----------------------------------------\n\nTITLE: Preparing Inputs for Model Inference\nDESCRIPTION: Encodes input data for model inference and shows the shape of the encoded data.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninputs = ctable.encode_onehot(['2+40'])\n# batch, max_length, vocab_size\ninputs.shape\n```\n\n----------------------------------------\n\nTITLE: Initializing Metrics History Dictionary\nDESCRIPTION: Creates a dictionary to track training and testing metrics including loss and accuracy over time.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmetrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}\n```\n\n----------------------------------------\n\nTITLE: Defining AutoEncoder Module with Encoder and Decoder Submodules in Python using Flax\nDESCRIPTION: This code snippet demonstrates the current way of initializing submodules within the setup method of a Flax Module. It shows the limitations of this approach when trying to access submodules before the Module is bound.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nclass AutoEncoder(nn.Module):\n  def setup(self):\n    self.encoder = Encoder(...)\n    self.decoder = Decoder(...)\n```\n\n----------------------------------------\n\nTITLE: Basic Setup and Imports for Flax NNX and JAX\nDESCRIPTION: Initial setup code importing required libraries and generating random test data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/jax_and_nnx_transforms.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport jax\n\nx = jax.random.normal(jax.random.key(0), (1, 2))\ny = jax.random.normal(jax.random.key(1), (1, 3))\n```\n\n----------------------------------------\n\nTITLE: Using FrozenDict Class in Python\nDESCRIPTION: API documentation showing the FrozenDict class methods including pretty_repr, copy, pop, unfreeze, and tree_flatten for working with immutable dictionaries.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.core.frozen_dict.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze, copy, pop, pretty_repr\n\n# Example usage of documented methods:\n# Create a frozen dict\nfd = FrozenDict({'a': 1, 'b': 2})\n\n# Get string representation\nfd.pretty_repr()\n\n# Create a copy\nfd_copy = fd.copy()\n\n# Remove and return value\nfd_pop = fd.pop('a')\n\n# Convert to mutable dict\nregular_dict = fd.unfreeze()\n\n# Get flattened representation\nflat = fd.tree_flatten()\n```\n\n----------------------------------------\n\nTITLE: Implementing RNN in Flax Linen vs NNX\nDESCRIPTION: Side-by-side comparison of RNN implementation in Flax Linen and Flax NNX, showing how each framework handles scanning over sequences. The NNX version explicitly defines a scan function with clear axis specifications for carry, cell, and input.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass RNN(nn.Module):\n  hidden_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    rnn = nn.scan(\n      RNNCell, variable_broadcast='params',\n      split_rngs={'params': False}, in_axes=1, out_axes=1\n    )(self.hidden_size)\n    carry = rnn.initial_state(x.shape[0])\n    carry, y = rnn(carry, x)\n\n    return y\n\nx = jnp.ones((3, 12, 32))\nmodel = RNN(64)\nvariables = model.init(jax.random.key(0), x=jnp.ones((3, 12, 32)))\ny = model.apply(variables, x=jnp.ones((3, 12, 32)))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass RNN(nnx.Module):\n  def __init__(self, input_size: int, hidden_size: int, rngs: nnx.Rngs):\n    self.hidden_size = hidden_size\n    self.cell = RNNCell(input_size, self.hidden_size, rngs=rngs)\n\n  def __call__(self, x):\n    scan_fn = lambda carry, cell, x: cell(carry, x)\n    carry = self.cell.initial_state(x.shape[0])\n    carry, y = nnx.scan(\n      scan_fn, in_axes=(nnx.Carry, None, 1), out_axes=(nnx.Carry, 1)\n    )(carry, self.cell, x)\n\n    return y\n\nx = jnp.ones((3, 12, 32))\nmodel = RNN(x.shape[2], 64, rngs=nnx.Rngs(0))\n\ny = model(x)\n```\n\n----------------------------------------\n\nTITLE: Creating Flax NNX RNG Streams\nDESCRIPTION: Example of creating RNG streams using nnx.Rngs constructor with different seed values for params and dropout streams.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrngs = nnx.Rngs(params=0, dropout=random.key(1))\nnnx.display(rngs)\n```\n\n----------------------------------------\n\nTITLE: Comparing Cell Initialization in Legacy vs New API\nDESCRIPTION: Demonstrates how cell initialization has changed to include feature dimensions directly in the constructor rather than in the initialize_carry method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncell = nn.LSTMCell()\n\n---\n\ncell = nn.LSTMCell(features=out_features)\n```\n\n----------------------------------------\n\nTITLE: Writing inline doctest examples in FLAX documentation\nDESCRIPTION: Demonstrates the syntax for writing inline code examples that can be tested with doctest. Examples must begin with text followed by double semicolons, and the code must be indented.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Example code::\n#\n#   def sum(a, b):\n#     return a + b\n#\n#   sum(0, 1)\n```\n\n----------------------------------------\n\nTITLE: Saving Multi-process Checkpoint with Legacy Flax\nDESCRIPTION: Demonstrates using Flax's legacy save_checkpoint_multiprocess function with async checkpointer configuration.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nasync_checkpointer = orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler(), timeout_secs=50)\ncheckpoints.save_checkpoint_multiprocess(ckpt_dir,\n                                         mp_ckpt,\n                                         step=3,\n                                         overwrite=True,\n                                         keep=4,\n                                         orbax_checkpointer=async_checkpointer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dropout Types in Flax RNN\nDESCRIPTION: Demonstrates how to define both input dropout and recurrent dropout in a Flax neural network cell. Input dropout varies for each step while recurrent dropout remains constant across steps.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nself.dropout = nn.Dropout(...) # input dropout\nself.recurrent_dropout = nn.Dropout(..., rng_collection='recurrent_dropout')\n```\n\n----------------------------------------\n\nTITLE: Mixed JAX-NNX Implementation with jax.jit and nnx.grad\nDESCRIPTION: Shows combining jax.jit with nnx.grad transforms while maintaining pure functional principles.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/jax_and_nnx_transforms.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(graphdef, state, x, y):\n  model = nnx.merge(graphdef, state)\n  def loss_fn(model):\n    return ((model(x) - y) ** 2).mean()\n  grads = nnx.grad(loss_fn)(model)\n  params = nnx.state(model, nnx.Param)\n  params = jax.tree_util.tree_map(\n    lambda p, g: p - 0.1 * g, params, grads\n  )\n  nnx.update(model, params)\n  return nnx.split(model)\n\ngraphdef, state = nnx.split(nnx.Linear(2, 3, rngs=nnx.Rngs(0)))\ngraphdef, state = train_step(graphdef, state, x, y)\n```\n\n----------------------------------------\n\nTITLE: Using DiffState in NNX Gradient Transforms\nDESCRIPTION: Shows how the proposed DiffState Lift type can be used to specify both the argument position and differentiable state in NNX gradient transforms.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngrads = nnx.grad(loss_fn, argnums=(DiffState(0, LoRAParam),))(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Basic FP8 dot operation using JAX in Python\nDESCRIPTION: This snippet demonstrates a basic FP8 dot operation using JAX's jnp.dot function, which supports FP8 dtype inputs directly.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nk0, k1 = random.split(random.key(0), 2)\na = random.uniform(k0, (16, 32))\nb = random.uniform(k1, (32, 64))\n@jax.jit\ndef dot_fp8(a, b):\n  return jnp.dot(a.astype(e4m3), b.astype(e4m3), preferred_element_type=f32)\ncheck_fp8_call(dot_fp8.lower(a, b))\n```\n\n----------------------------------------\n\nTITLE: Documenting Optimizer Class in Flax (Python)\nDESCRIPTION: This snippet documents the Optimizer class from the flax.nnx.optimizer module. It includes the class definition and specifies that the __init__ and update methods are documented.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/training/optimizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Optimizer\n   :members: __init__, update\n```\n\n----------------------------------------\n\nTITLE: Parallel Model Update Function with JAX for Ensembling\nDESCRIPTION: A simple parallel model update function that applies gradients to the model state. This function is mapped across devices using jax.pmap() to update each ensemble member independently.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/ensembling.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.pmap\ndef update_model(state, grads):\n  return state.apply_gradients(grads=grads)\n```\n\n----------------------------------------\n\nTITLE: Importing Essential Libraries for Flax\nDESCRIPTION: Imports the necessary dependencies from JAX and Flax modules required for building and training neural networks.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom typing import Any, Callable, Sequence\nfrom jax import random, numpy as jnp\nimport flax\nfrom flax import linen as nn\n```\n\n----------------------------------------\n\nTITLE: Partial Model Initialization - Python\nDESCRIPTION: Creates a partially initialized model using NNXOuter class without lazy initialization\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npartial_model = NNXOuter(64, rngs=nnx.Rngs(0))\nnnx.display(partial_model)\n```\n\n----------------------------------------\n\nTITLE: Proposed Intuitive NNX Transform Syntax\nDESCRIPTION: Illustrates the proposed syntax for NNX transforms that aligns with JAX conventions, using in_axes for vectorization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(1, 0))\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Bidirectional RNN Implementation in Flax\nDESCRIPTION: Pseudo-code implementation of the Bidirectional class, which processes input sequences in both forward and backward directions using two RNN instances.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self, inputs, seq_lengths):\n  # Encode in the forward direction.\n  carry_forward, outputs_forward = self.forward_rnn(\n    inputs, seq_lengths=seq_lengths,\n    return_carry=True, reverse=False,\n  )\n  # Encode in the reverse order.\n  carry_backward, outputs_backward = self.backward_rnn(\n    inputs, seq_lengths=seq_lengths,\n    return_carry=True, reverse=True, # process in reverse order\n    keep_order=True, # but return the sequence in the original order\n  )\n  # Merge both sequences.\n  outputs = jax.tree.map(self.merge_fn, outputs_forward, outputs_backward)\n\n  return (carry_forward, carry_backward), outputs\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoEncoder Parameters in Haiku and Flax\nDESCRIPTION: Demonstrates how to initialize parameters for the AutoEncoder model in Haiku and Flax. Shows differences in parameter structure and initialization process.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nparams = model.init(\n  random.key(0),\n  x=jax.numpy.ones((1, 784)),\n)\n```\n\nLANGUAGE: Python\nCODE:\n```\nvariables = model.init(\n  random.key(0),\n  x=jax.numpy.ones((1, 784)),\n)\nparams = variables[\"params\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Dropout Split RNGs in nn.scan\nDESCRIPTION: Shows how to configure split_rngs in nn.scan to handle different dropout types. Sets input dropout to vary across steps (True) and recurrent dropout to remain constant (False).\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnn.scan(scan_fn, ..., split_rngs={'dropout': True, 'recurrent_dropout': False})\n```\n\n----------------------------------------\n\nTITLE: Restoring Checkpoints with Legacy API in Python\nDESCRIPTION: Shows alternative methods to restore checkpoints using the Orbax CheckpointManager and the legacy Flax code. These methods provide compatibility with older checkpoint formats.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_manager.restore(4, items=target)\n```\n\nLANGUAGE: python\nCODE:\n```\ncheckpoints.restore_checkpoint(ckpt_dir='/tmp/flax_ckpt/flax-checkpointing', target=target)\n```\n\n----------------------------------------\n\nTITLE: Cross-Reference Syntax Examples\nDESCRIPTION: Examples demonstrating how to create cross-references to classes, functions and methods in documentation\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Create: a reference to class flax.linen.Module.\n# :class:`flax.linen.Module`\n\n# Create a reference to local function my_func.\n# :func:`my_func`\n\n# Create a reference \"Module.apply()\" to method flax.linen.Module.apply.\n# :meth:`Module.apply() <flax.linen.Module.apply>`  #\n```\n\n----------------------------------------\n\nTITLE: Stateful Model Instantiation in Haiku and Flax\nDESCRIPTION: Comparison of stateful model instantiation. Haiku uses hk.transform_with_state to handle state, while Flax uses the same instantiation pattern regardless of statefulness.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(x, training: bool):\n  return Model(256, 10)(x, training)\n\nmodel = hk.transform_with_state(forward)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n\nmodel = Model(256, 10)\n```\n\n----------------------------------------\n\nTITLE: Accessing Module Parameters with Unbox Syntax in Python\nDESCRIPTION: Demonstrates how to access module parameters with and without metadata boxes using the unbox parameter. The first line retrieves a kernel parameter without metadata, while the second line preserves AxisMetadata boxes by setting unbox=False.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkernel = self.param(\"kernel\", self.kernel_init, shape)  # No AxisMetadata instances\nkernel_box = self.get_variable(\"param\", \"kernel\", unbox=False)  # AxisMetadata boxes are preserved\n```\n\n----------------------------------------\n\nTITLE: State Handling in Haiku and Flax\nDESCRIPTION: Comparison of state handling approaches between Haiku and Flax. Haiku uses functional approach with get/set_state while Flax uses class-based approach with variable collections.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Haiku Implementation\ndef forward(x):\n  counter = hk.get_state('counter', shape=[], dtype=jnp.int32, init=jnp.ones)\n  multiplier = hk.get_parameter('multiplier', shape=[1,], dtype=x.dtype, init=jnp.ones)\n  output = x + multiplier * counter\n  hk.set_state(\"counter\", counter + 1)\n  return output\n\n# Flax Implementation\nclass FooModule(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    counter = self.variable('counter', 'count', lambda: jnp.ones((), jnp.int32))\n    multiplier = self.param('multiplier', nn.initializers.ones_init(), [1,], x.dtype)\n    output = x + multiplier * counter.value\n    if not self.is_initializing():\n      counter.value += 1\n    return output\n```\n\n----------------------------------------\n\nTITLE: Using Module.perturb for Taking Derivatives of Intermediate Values in Flax\nDESCRIPTION: This snippet shows the method name for taking derivatives with respect to intermediate activations in a Flax model. It requires using jax.grad on a perturbation parameter that has the same shape as the intermediate activation.\nSOURCE: https://github.com/google/flax/blob/main/docs/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nModule.perturb\n```\n\n----------------------------------------\n\nTITLE: Handling State with BatchNorm in Haiku and Flax\nDESCRIPTION: Comparison of state handling using BatchNorm. The main difference is that Haiku uses is_training parameter while Flax uses use_running_average with the opposite boolean logic.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Block(hk.Module):\n  def __init__(self, features: int, name=None):\n    super().__init__(name=name)\n    self.features = features\n\n  def __call__(self, x, training: bool):\n    x = hk.Linear(self.features)(x)\n    x = hk.BatchNorm(\n      create_scale=True, create_offset=True, decay_rate=0.99\n    )(x, is_training=training)\n    x = jax.nn.relu(x)\n    return x\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Block(nn.Module):\n  features: int\n\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    x = nn.Dense(self.features)(x)\n    x = nn.BatchNorm(\n      momentum=0.99\n    )(x, use_running_average=not training)\n    x = jax.nn.relu(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Defining helper functions for image visualization\nDESCRIPTION: Creates two helper functions for visualizing images: one for displaying a single image and another for showing a grid of images with titles.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Helper functions for images.\n\ndef show_img(img, ax=None, title=None):\n  \"\"\"Shows a single image.\"\"\"\n  if ax is None:\n    ax = plt.gca()\n  ax.imshow(img[..., 0], cmap='gray')\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title:\n    ax.set_title(title)\n\ndef show_img_grid(imgs, titles):\n  \"\"\"Shows a grid of images.\"\"\"\n  n = int(np.ceil(len(imgs)**.5))\n  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n  for i, (img, title) in enumerate(zip(imgs, titles)):\n    show_img(img, axs[i // n][i % n], title)\n```\n\n----------------------------------------\n\nTITLE: Model Training Configuration\nDESCRIPTION: Sets up training configuration including hyperparameters and model architecture\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig = config_lib.get_config()\nconfig.dataset = 'imagenette'\nconfig.model = 'ResNet18'\nconfig.half_precision = True\nconfig.batch_size = 32\nconfig.shuffle_buffer_size = 128\nconfig.prefetch = 1\nconfig.num_epochs = 5.0\nconfig.warmup_epochs = 0.5\n```\n\n----------------------------------------\n\nTITLE: Initializing and Displaying Flax Model\nDESCRIPTION: Initializes a full model from a partial model using Flax bridge lazy initialization. The initialized model is then displayed using nnx utility for visualization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfull_model = bridge.lazy_init(partial_model, x)\nnnx.display(full_model)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Flax in Python\nDESCRIPTION: This code imports necessary modules from JAX, Flax, and Python standard libraries for working with Flax Linen modules.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom typing import Any, Callable, Sequence, Optional\nimport jax\nfrom jax import lax, random, numpy as jnp\nimport flax\nfrom flax import linen as nn\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for Flax ML Project\nDESCRIPTION: A requirements file that lists all necessary Python packages with pinned versions for a Flax-based machine learning project. It includes JAX with CUDA 11 support, TensorFlow 2.13.0, and various machine learning utilities like ml-collections and optax.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.4.0\nclu==0.0.9\nflax==0.6.11\njax==0.4.13\n--find-links https://storage.googleapis.com/jax-releases/jax_releases.html\njaxlib==0.4.13+cuda11.cudnn82  # Make sure CUDA version matches the base image.\nml-collections==0.1.1\nnumpy==1.24.3\noptax==0.1.5\nsentencepiece==0.1.99\ntensorflow==2.13.0\ntensorflow-datasets==4.9.2\ntensorflow-text==2.13.0\n```\n\n----------------------------------------\n\nTITLE: Importing and Using TrainState in Python\nDESCRIPTION: This snippet demonstrates how to import and use the TrainState class from the flax.training.train_state module. The TrainState class is a key component for managing the state of a model during training in Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.training.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.training.train_state import TrainState\n\n# Create a TrainState instance\ntrain_state = TrainState.create(...)\n\n# Apply gradients to update the model\nupdated_state = train_state.apply_gradients(...)\n```\n\n----------------------------------------\n\nTITLE: Custom TrainState with Dropout Key\nDESCRIPTION: Extends Flax's TrainState to include PRNG key management for dropout, showing initialization with Adam optimizer and dropout key.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/dropout.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(train_state.TrainState):\n  key: jax.Array\n\nstate = TrainState.create(\n  apply_fn=my_model.apply,\n  params=params,\n  key=dropout_key,\n  tx=optax.adam(1e-3)\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Vision Submodule\nDESCRIPTION: Extracts the vision model submodule from the CLIP model using Flax bind and unbind operations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport flax.linen as nn\n\nclip, clip_variables = load_model()\nvision_model, vision_model_vars = clip.bind(clip_variables).vision_model.unbind()\n```\n\n----------------------------------------\n\nTITLE: Examining FP8 parameter structure in Python\nDESCRIPTION: This snippet demonstrates how to examine the structure of FP8 parameters managed by Flax's high-level FP8 operations.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparams_structure = flax.core.unfreeze(params).copy()\nparams_structure = flax.traverse_util.flatten_dict(params_structure, sep='/')\nfor key, value in params_structure.items():\n    params_structure[key] = '*'\nparams_structure = flax.traverse_util.unflatten_dict(params_structure, sep='/')\npprint.pprint(params_structure)\n```\n\n----------------------------------------\n\nTITLE: Restoring Checkpoint with Legacy Flax API\nDESCRIPTION: This code shows how to restore a checkpoint using the legacy Flax checkpointing API. It demonstrates the API's ability to automatically identify and restore checkpoints saved in different formats.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nraw_restored = checkpoints.restore_checkpoint(ckpt_dir='/tmp/flax_ckpt/flax-checkpointing', target=None)\nraw_restored\n```\n\n----------------------------------------\n\nTITLE: Installing Flax from GitHub Repository\nDESCRIPTION: Installs the latest version of Flax directly from the GitHub repository using pip.\nSOURCE: https://github.com/google/flax/blob/main/tests/import_test.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Install from head\n!pip install git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: MessagePack Serialization in Flax\nDESCRIPTION: Functions for serializing and deserializing model states using MessagePack format, including msgpack_serialize, msgpack_restore, to_bytes, and from_bytes.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.serialization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom flax.serialization import msgpack_serialize, msgpack_restore, to_bytes, from_bytes\n```\n\n----------------------------------------\n\nTITLE: Updating Batch Statistics in Old Flax\nDESCRIPTION: Demonstrates how to update batch statistics during training using the old Flax API. It uses the nn.stateful context manager to capture and update the model state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(model, model_state):\n  with nn.stateful(model_state) as new_model_state:\n    logits = model(batch['image'])\n  # [...]\n```\n\n----------------------------------------\n\nTITLE: ConvLSTM Proposed Implementation Example\nDESCRIPTION: Shows the simplified API proposal where the carry state is initialized using just the input shape, making the interface more intuitive and reducing manual calculations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 4, 4, 3)) # (batch, *image_shape, channels)\n\nlstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))\ncarry = lstm.initialize_carry(key1, input_shape=x.shape)\n\n(carry, y), initial_params = lstm.init_with_output(key2, carry, x)\n```\n\n----------------------------------------\n\nTITLE: Simplified LSTM Implementation\nDESCRIPTION: Alternative implementation showing simplified LSTM usage without explicit initialize_carry, treating carry state as a collection.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nLSTM = nn.scan(\n  nn.LSTMCell, variable_broadcast='params',\n  split_rngs={'dropout': True})\ny = LSTM(features=32)(carry, x)\n```\n\n----------------------------------------\n\nTITLE: Applying AutoEncoder Methods in Haiku and Flax\nDESCRIPTION: Shows how to call specific methods (like encode) on the AutoEncoder model in Haiku and Flax. Highlights differences in method calling and parameter passing.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nencode, decode = model.apply\nz = encode(\n  params,\n  None, # <== rng\n  x=jax.numpy.ones((1, 784)),\n)\n```\n\nLANGUAGE: Python\nCODE:\n```\nz = model.apply(\n  {\"params\": params},\n  x=jax.numpy.ones((1, 784)),\n  method=\"encode\",\n)\n```\n\n----------------------------------------\n\nTITLE: Momentum Optimizer Implementation\nDESCRIPTION: Legacy implementation of a Momentum optimizer showing how custom optimizers were defined in the previous API using OptimizerDef.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@flax.struct.dataclass\nclass _MomentumHyperParams:\n  learning_rate: jnp.ndarray\n  beta: jnp.ndarray\n\n\n@flax.struct.dataclass\nclass _MomentumParamState:\n  momentum: np.ndarray\n\n\nclass Momentum(flax.optim.OptimizerDef):\n\n  def __init__(self, learning_rate=None, beta=0.9):\n    super().__init__(\n      _MomentumHyperParams(learning_rate, beta)\n    )\n\n  def init_param_state(self, param):\n    return _MomentumParamState(jnp.zeros_like(param))\n\n  def apply_param_gradient(self, step, hyper_params, param, state, grad):\n    del step\n    assert hyper_params.learning_rate is not None\n    new_momentum = state.momentum * hyper_params.beta + grad\n    new_params = param - hyper_params.learning_rate * new_momentum\n    return new_params, _MomentumParamState(new_momentum)\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorFlow Datasets MNIST Format for Jax+Flax\nDESCRIPTION: Loads the MNIST dataset using the TensorFlow Datasets method and prints the shapes and data types of the training and test sets to verify they match the expected format for Jax+Flax models.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain, test = get_dataset_tf()\nprint(train['image'].shape, train['image'].dtype)\nprint(train['label'].shape, train['label'].dtype)\nprint(test['image'].shape, test['image'].dtype)\nprint(test['label'].shape, test['label'].dtype)\n```\n\n----------------------------------------\n\nTITLE: Using nnx.OfType for Flax NNX Parameter Filtering\nDESCRIPTION: Demonstrates the use of nnx.OfType to create a callable filter for Flax NNX parameters, which is equivalent to the custom is_param function.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/filters_guide.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nis_param = nnx.OfType(nnx.Param)\n\nprint(f'{is_param((), nnx.Param(0)) = }')\nprint(f'{is_param((), nnx.VariableState(type=nnx.Param, value=0)) = }')\n```\n\n----------------------------------------\n\nTITLE: Importing Flax NNX and JAX Libraries\nDESCRIPTION: Imports the necessary libraries for working with Flax NNX and JAX, including numpy and random modules from JAX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import numpy as jnp, random\nfrom flax import nnx\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoEncoder with Shared Modules in Flax/Linen\nDESCRIPTION: Shows how to implement an autoencoder with shared submodules, demonstrating the transition from old Flax module sharing patterns to Linen's more Pythonic approach using setup method.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass AutoEncoder(nn.Module):\n  def _create_submodules(self):\n    return Decoder.shared(name=\"encoder\")\n\n  def apply(self, x, z_rng, latents=20):\n    decoder = self._create_decoder()\n    z = Encoder(x, latents, name=\"encoder\")\n    return decoder(z)\n\n  @nn.module_method\n  def generate(self, z, **unused_kwargs):\n    decoder = self._create_decoder()\n    return nn.sigmoid(decoder(z))\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass AutoEncoder(nn.Module):\n  latents: int = 20\n\n  def setup(self):\n    self.encoder = Encoder(self.latents)\n    self.decoder = Decoder()\n\n  def __call__(self, x):\n    z = self.encoder(x)\n    return self.decoder(z)\n\n  def generate(self, z):\n    return nn.sigmoid(self.decoder(z))\n```\n\n----------------------------------------\n\nTITLE: Verifying Hugging Face MNIST Data Format for Jax+Flax\nDESCRIPTION: Loads the MNIST dataset using the Hugging Face method and prints the shapes and data types of the training and test sets to verify they match the expected format for Jax+Flax models.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrain, test = get_dataset_hf()\nprint(train['image'].shape, train['image'].dtype)\nprint(train['label'].shape, train['label'].dtype)\nprint(test['image'].shape, test['image'].dtype)\nprint(test['label'].shape, test['label'].dtype)\n```\n\n----------------------------------------\n\nTITLE: Measuring Example Distribution in Multi-Process Environments\nDESCRIPTION: Shows how using tfds.split_for_jax_process() across multiple hosts can lead to uneven distribution of examples, which breaks SPMD requirements.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprocess_count = 6\n[\n    len(tfds.load(dataset_name, split=tfds.split_for_jax_process(\n        'test', process_index=process_index, process_count=process_count)))\n    for process_index in range(process_count)\n]\n# output:\n# [1667, 1667, 1667, 1667, 1666, 1666]\n```\n\n----------------------------------------\n\nTITLE: Using Flax high-level FP8 API with nn.Dense in Python\nDESCRIPTION: This snippet shows how to use Flax's high-level FP8 API by integrating fp8_ops.Fp8DotGeneral into an nn.Dense layer.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Dense(features=64, dot_general_cls=fp8_ops.Fp8DotGeneral)\nparams = model.init(k0, A)\n\n@jax.jit\ndef train_step(var, a): \n  c = model.apply(var, a)\n  return jnp.sum(c)\n\ncheck_fp8_call(train_step.lower(params, A))\n```\n\n----------------------------------------\n\nTITLE: Creating Variables in Flax Module\nDESCRIPTION: Demonstrates how to create variables within a Flax module's __call__ method. It shows the creation of mean and variance variables for batch statistics.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self, x):\n  # [...]\n  ra_mean = self.variable(\n    'batch_stats', 'mean', initializers.zeros_init(), (x.shape[-1], ))\n  ra_var = self.variable(\n    'batch_stats', 'var', initializers.ones_init(), (x.shape[-1], ))\n  # [...]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Checkpoint Directory\nDESCRIPTION: This snippet sets up a directory for storing checkpoints and removes any existing checkpoints from previous runs to ensure a clean state for the demonstration.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nckpt_dir = '/tmp/flax_ckpt'\n\nif os.path.exists(ckpt_dir):\n    shutil.rmtree(ckpt_dir)  # Remove any existing checkpoints from the last notebook run.\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for Flax Project\nDESCRIPTION: A requirements.txt file that lists all the Python package dependencies with their specific versions for a Flax project. The file includes JAX with CUDA support, TensorFlow, and various machine learning utilities.\nSOURCE: https://github.com/google/flax/blob/main/examples/ogbg_molpcba/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.4.1\njax==0.3.4\n--find-links https://storage.googleapis.com/jax-releases/jax_releases.html\njaxlib==0.3.2+cuda11.cudnn82  # Make sure CUDA version matches the base image.\njraph==0.0.2.dev0\nml-collections==0.1.0\nnumpy==1.22.0\noptax==0.1.0\nsklearn==0.0\ntensorflow==2.11.1\ntensorflow-datasets==4.4.0\n```\n\n----------------------------------------\n\nTITLE: Documenting Flax Activation Functions using RST\nDESCRIPTION: Sphinx/RST documentation directives listing all available activation functions in the flax.nnx module. Uses automodule and autofunction directives to automatically generate API documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/activations.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autofunction:: celu\n.. autofunction:: elu\n.. autofunction:: gelu\n.. autofunction:: glu\n.. autofunction:: hard_sigmoid\n.. autofunction:: hard_silu\n.. autofunction:: hard_swish\n.. autofunction:: hard_tanh\n.. autofunction:: leaky_relu\n.. autofunction:: log_sigmoid\n.. autofunction:: log_softmax\n.. autofunction:: logsumexp\n.. autofunction:: one_hot\n.. autofunction:: relu\n.. autofunction:: relu6 as relu6,\n.. autofunction:: selu\n.. autofunction:: sigmoid\n.. autofunction:: silu\n.. autofunction:: soft_sign\n.. autofunction:: softmax\n.. autofunction:: softplus\n.. autofunction:: standardize\n.. autofunction:: swish\n.. autofunction:: tanh\n```\n\n----------------------------------------\n\nTITLE: Initializing Model and State in Old Flax\nDESCRIPTION: Shows how to initialize a model and its initial state using the old Flax API. It uses the nn.stateful context manager to capture the initial state.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef initial_model(key, init_batch):\n  with nn.stateful() as initial_state:\n    _, initial_params = ResNet.init(key, init_batch)\n  model = nn.Model(ResNet, initial_params)\n  return model, init_state\n```\n\n----------------------------------------\n\nTITLE: Comparing Initialize Carry Usage in Legacy vs New API\nDESCRIPTION: Shows how initialize_carry has changed from a class method to an instance method, with simplified parameters that use the input shape directly.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncarry = nn.LSTMCell.initialize_carry(jax.random.key(0), (batch_size,), out_features)\n\n---\n\ncarry = cell.initialize_carry(jax.random.key(0), x[:, 0].shape)\n```\n\n----------------------------------------\n\nTITLE: LSTM Implementation with Initialize Carry\nDESCRIPTION: Demonstrates usage of LSTM with initialize_carry method, showing how to set up the cell, initialize the carry state, and process input sequences.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nLSTM = nn.scan(\n  nn.LSTMCell, variable_broadcast='params',\n  split_rngs={'dropout': True})\nlstm = LSTM(features=32)\ncarry = lstm.initialize_carry(x[:, 0])\ncarry, y = lstm(carry, x)\n```\n\n----------------------------------------\n\nTITLE: Importing libraries for Flax MNIST example\nDESCRIPTION: Imports the necessary libraries including Flax, JAX, matplotlib, and TensorFlow Datasets. Sets the logging verbosity level.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom absl import logging\nimport flax\nimport jax.numpy as jnp\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport tensorflow_datasets as tfds\n\nlogging.set_verbosity(logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Training MNIST models with different momentum values\nDESCRIPTION: Trains three CNN models on the MNIST dataset with different momentum values (0.8, 0.9, 0.95) for 3 epochs each. Stores the trained model parameters in a dictionary.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# 3x 3 epochs trains in ~1 minute in the GPU Colab...\n\n# We don't use TPUs in this Colab because we do not distribute our\n# training using pmap() - if you're looking for an example using TPUs\n# checkout below Colab:\n# https://colab.research.google.com/github/google/flax/blob/main/examples/imagenet/imagenet.ipynb\n\nconfig.num_epochs = 3\nmodels = {}\nfor momentum in (0.8, 0.9, 0.95):\n  name = f'momentum={momentum}'\n  config.momentum = momentum\n  state = train.train_and_evaluate(config, workdir=f'./models/{name}')\n  models[name] = state.params\n```\n\n----------------------------------------\n\nTITLE: Extended TrainState with Batch Statistics\nDESCRIPTION: Example of extending the base TrainState class to include batch statistics for tracking model state during training.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TrainState(train_state.TrainState):\n  batch_stats: flax.core.FrozenDict[str, Any]\n```\n\n----------------------------------------\n\nTITLE: RNN Cell Implementation Comparison\nDESCRIPTION: Demonstrates RNN cell implementation in both Flax Linen and NNX, showing differences in state handling and cell structure.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass RNNCell(nnx.Module):\n  def __init__(self, input_size, hidden_size, rngs):\n    self.linear = nnx.Linear(hidden_size + input_size, hidden_size, rngs=rngs)\n    self.hidden_size = hidden_size\n\n  def __call__(self, carry, x):\n    x = jnp.concatenate([carry, x], axis=-1)\n    x = self.linear(x)\n    x = jax.nn.relu(x)\n    return x, x\n\n  def initial_state(self, batch_size: int):\n    return jnp.zeros((batch_size, self.hidden_size))\n```\n\n----------------------------------------\n\nTITLE: Importing Flax Normalization Modules\nDESCRIPTION: This snippet shows how to import the flax.nnx module, which contains various normalization layers for neural networks.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/normalization.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax import nnx\n```\n\n----------------------------------------\n\nTITLE: Profiling Flax Model Training on TPU Pod\nDESCRIPTION: This snippet demonstrates how to profile the performance of a Flax model training step on a TPU pod or pod slice using a custom block_all utility function.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef block_all(xs):\n  jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)\n  return xs\n\nwith mesh:\n  new_state = block_all(train_step(initialized_state, x))\n```\n\n----------------------------------------\n\nTITLE: FP8 dot operation with delayed scaling in Python\nDESCRIPTION: This snippet demonstrates FP8 matrix multiplication using delayed scaling, which uses scaling factors associated with an amax history to improve performance.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\na_scale = jnp.array(1.0)\nb_scale = jnp.array(1.0)\na_amax_hist = jnp.zeros((1024,))\nb_amax_hist = jnp.zeros((1024,))\n\n@jax.jit\ndef dot_fp8(a, a_scale, a_amax_hist, b, b_scale, b_amax_hist):\n  a, a_scale = fp8_ops.in_q(f32, e4m3, a, a_scale, a_amax_hist)\n  b, b_scale = fp8_ops.in_q(f32, e4m3, b, b_scale, b_amax_hist)\n  \n  c = jnp.dot(a, b, preferred_element_type=f32)\n  c = fp8_ops.out_dq(f32, a_scale, b_scale, c)\n  return c\n\nc = dot_fp8(a, a_scale, a_amax_hist, b, b_scale, b_amax_hist)\ncheck_fp8_call(dot_fp8.lower(a, a_scale, a_amax_hist, b, b_scale, b_amax_hist))\n```\n\n----------------------------------------\n\nTITLE: Implementing Dropout in Old Flax\nDESCRIPTION: Shows how to implement a dropout function in the old Flax API. It uses the make_rng function to generate random numbers for the dropout mask.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef dropout(inputs, rate, deterministic=False):\n  keep_prob = 1. - rate\n  if deterministic:\n    return inputs\n  else:\n    mask = random.bernoulli(\n    make_rng(), p=keep_prob, shape=inputs.shape)\n    return lax.select(\n      mask, inputs / keep_prob, jnp.zeros_like(inputs))\n```\n\n----------------------------------------\n\nTITLE: Restoring and Adapting Checkpoint for Modified Model in Python\nDESCRIPTION: Demonstrates how to restore a checkpoint as a pure dictionary and modify it to accommodate the new model structure. It adds zero-initialized bias arrays to the restored checkpoint to match the new ModifiedTwoLayerMLP requirements.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Accommodate your old checkpoint to the new code.\nrestored_pure_dict = checkpointer.restore(ckpt_dir / 'pure_dict')\nrestored_pure_dict['linear1']['bias'] = jnp.zeros((4,))\nrestored_pure_dict['linear2']['bias'] = jnp.zeros((4,))\n```\n\n----------------------------------------\n\nTITLE: Installing Specific JAX Versions\nDESCRIPTION: Installs specific versions of JAX and jaxlib (0.3.25) that are compatible with TPU runtime\nSOURCE: https://github.com/google/flax/blob/main/tests/colab_tpu_jax_version.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install jax==0.3.25 jaxlib==0.3.25 flax\n```\n\n----------------------------------------\n\nTITLE: Generating PRNG Keys from Streams\nDESCRIPTION: Demonstration of generating new PRNG keys from existing streams and displaying the updated state.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparams_key = rngs.params()\ndropout_key = rngs.dropout()\n\nnnx.display(rngs)\n```\n\n----------------------------------------\n\nTITLE: Initializing Classifier Parameters\nDESCRIPTION: Initializes the classifier model parameters with random values using fake input data.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnum_classes = 3\nmodel = Classifier(num_classes=num_classes, backbone=vision_model)\n\nx = jnp.empty((1, 224, 224, 3))\nvariables = model.init(jax.random.key(1), x)\nparams = variables['params']\n```\n\n----------------------------------------\n\nTITLE: Creating NNX Module without RNG Handling\nDESCRIPTION: Shows how to create an NNX module that doesn't require RNG handling, using the skip_rng parameter with bridge.ToLinen.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass NNXAddConstant(nnx.Module):\n  def __init__(self):\n    self.constant = nnx.Variable(jnp.array(1))\n  def __call__(self, x):\n    return x + self.constant\n\n# You have to use `skip_rng=True` because this module's `__init__` don't\n# take `rng` as argument\nmodel = bridge.ToLinen(NNXAddConstant, skip_rng=True)\ny, var = model.init_with_output(jax.random.key(0), x)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pytree with Various Data Structures\nDESCRIPTION: This code creates a pytree containing a simple neural network model, a Flax TrainState, and arbitrary nested data structures. It demonstrates the versatility of data that can be checkpointed.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkey1, key2 = random.split(random.key(0))\nx1 = random.normal(key1, (5,))      # A simple JAX array.\nmodel = nn.Dense(features=3)\nvariables = model.init(key2, x1)\n\ntx = optax.sgd(learning_rate=0.001)      # An Optax SGD optimizer.\nstate = train_state.TrainState.create(\n    apply_fn=model.apply,\n    params=variables['params'],\n    tx=tx)\nstate = state.apply_gradients(grads=jax.tree_util.tree_map(jnp.ones_like, state.params))\n\nconfig = {'dimensions': np.array([5, 3])}\n\nckpt = {'model': state, 'config': config, 'data': [x1]}\nckpt\n```\n\n----------------------------------------\n\nTITLE: Using Module.perturb for Intermediate Gradients in Flax\nDESCRIPTION: This snippet demonstrates how to use flax.linen.Module.perturb to compute gradients with respect to intermediate activations in a Flax model. It involves defining a zero-value perturbation parameter and using jax.grad on the perturbation argument.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nperturb(...)\n```\n\n----------------------------------------\n\nTITLE: Using StateAxes for Fine-grained State Control in NNX Transforms\nDESCRIPTION: Demonstrates the proposed Lift API using StateAxes to control how substates are handled in NNX transforms.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstate_axes = StateAxes({Param: 1, BatchStat: None})\n\n@nnx.vmap(in_axes=(state_axes, 0))\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Using DiffState with nnx.grad for selective differentiation\nDESCRIPTION: Proposed usage of the DiffState Lift symbol with nnx.grad to specify which Module parameters should be differentiated.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngrads = nnx.grad(loss_fn, argnums=(DiffState(0, LoRAParam),))(model, x, y)\n```\n\n----------------------------------------\n\nTITLE: Alternative Initialize Carry Using Explicit Shape\nDESCRIPTION: Demonstrates how to initialize the carry state using an explicitly created input shape instead of deriving it from an existing tensor.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncarry = cell.initialize_carry(jax.random.key(0), (batch_size, in_features))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Flax NNX/Linen Integration\nDESCRIPTION: Sets up environment variables and imports required packages for working with Flax NNX and Linen modules together. Configures XLA device settings and imports core Flax, JAX and typing utilities.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nfrom flax import nnx\nfrom flax import linen as nn\nfrom flax.nnx import bridge\nimport jax\nfrom jax import numpy as jnp\nfrom jax.experimental import mesh_utils\nfrom typing import *\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation with JAX Transforms\nDESCRIPTION: Shows equivalent training implementation using jax.jit and jax.grad with explicit state management.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/jax_and_nnx_transforms.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(graphdef, state, x, y):\n  def loss_fn(graphdef, state):\n    model = nnx.merge(graphdef, state)\n    return ((model(x) - y) ** 2).mean()\n  grads = jax.grad(loss_fn, argnums=1)(graphdef, state)\n\n  model = nnx.merge(graphdef, state)\n  params = nnx.state(model, nnx.Param)\n  params = jax.tree_util.tree_map(\n    lambda p, g: p - 0.1 * g, params, grads\n  )\n  nnx.update(model, params)\n  return nnx.split(model)\n\ngraphdef, state = nnx.split(nnx.Linear(2, 3, rngs=nnx.Rngs(0)))\ngraphdef, state = train_step(graphdef, state, x, y)\n```\n\n----------------------------------------\n\nTITLE: Creating JAX Device Mesh Configuration\nDESCRIPTION: Configures a 2x4 device mesh with 'data' and 'model' axis names for data-parallel and model-parallel sharding.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a mesh of two dimensions and annotate each axis with a name.\nmesh = Mesh(devices=np.array(jax.devices()).reshape(2, 4),\n            axis_names=('data', 'model'))\nprint(mesh)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dropout Module in Linen\nDESCRIPTION: Shows how to implement a Dropout module using the Linen API. It uses self.make_rng to generate random numbers for the dropout mask.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass Dropout(nn.Module):\n  rate: float\n\n  @nn.compact\n  def __call__(self, inputs, deterministic=False):\n    keep_prob = 1. - self.rate\n    if deterministic:\n      return inputs\n    else:\n      mask = random.bernoulli(\n        self.make_rng('dropout'), p=keep_prob, shape=inputs.shape)\n      return lax.select(\n        mask, inputs / keep_prob, jnp.zeros_like(inputs))\n```\n\n----------------------------------------\n\nTITLE: Inspecting Variable Structure in Linen and NNX\nDESCRIPTION: Code snippets showing the resulting parameter structure for scan-over-layers implementation in both Linen and NNX frameworks, demonstrating the similarity in parameter shape while highlighting differences in organization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# variables = model.init(key, x=jnp.ones((1, 64)), training=True)\n# variables['params']\n{\n  ScanBlock_0: {\n    Dense_0: {\n      bias: (5, 64),\n      kernel: (5, 64, 64),\n    },\n  },\n}\n```\n\nLANGUAGE: python\nCODE:\n```\n# _, params, _ = nnx.split(model, nnx.Param, ...)\n# params\n{\n  'blocks': {\n    'linear': {\n      'bias': VariableState(type=Param, value=(5, 64)),\n      'kernel': VariableState(type=Param, value=(5, 64, 64))\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Parallel Training Documentation in RST\nDESCRIPTION: A reStructuredText (RST) snippet that creates a table of contents for parallel training resources in Flax. It specifies a maximum depth of 1 and includes links to documentation about ensembling and using PJIT with Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   ensembling\n   flax_on_pjit\n```\n\n----------------------------------------\n\nTITLE: Invalid captured Module output leading to implicit cloning\nDESCRIPTION: Example showing why captured Modules cannot be outputs of transformed functions, as it would lead to implicit cloning.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nm = SomeModule()\n\n@nnx.vmap(out_axes=0, axis_size=5)\ndef f():\n  return m\n\nassert m is not f()  # implicit cloning\n```\n\n----------------------------------------\n\nTITLE: Setup Pattern with Dropout in Flax\nDESCRIPTION: Shows the setup pattern implementation with dropout in Flax, highlighting the limitations when deterministic is treated as a dataclass attribute. This pattern demonstrates why deterministic sometimes needs to be a call-time argument.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/arguments.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SomeModule(nn.Module):\n  drop_rate: float\n\n  def setup(self):\n    self.dropout = nn.Dropout(rate=self.drop_rate)\n\n  @nn.compact\n  def __call__(self, x, *, train):\n    # ...\n    x = self.dropout(x, deterministic=not train)\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Correct Module Control Flow Implementation\nDESCRIPTION: Demonstrates the correct way to implement conditional module creation by moving constructors outside control flow.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass CorrectModule(nn.Module):\n  @nn.compact\n  def __call__(self, x, mode):\n    encoder = nn.Dense(8)\n    decoder = nn.Dense(4)\n    if mode == \"encode\":\n      return encoder(x)\n```\n\n----------------------------------------\n\nTITLE: Previous Training Step Implementation\nDESCRIPTION: Example of how training steps were implemented in the previous API using the Optimizer abstraction.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef make_train_step(apply_fn):\n  @jax.jit\n  def train_step(optimizer, batch_stats, inputs, labels):\n\n    def loss_fn(params):\n      variables = {'params': params, 'batch_stats': batch_stats}\n      logits, new_model_state = apply_fn(\n          variables, inputs, mutable=['batch_stats'])\n      loss = xent_loss(logits, labels)\n      return loss, new_model_state['batch_stats']\n\n    (loss, new_batch_stats), grad = jax.value_and_grad(loss_fn, has_aux=True)(\n        optimizer.target)\n    lr = get_learning_rate(step)\n    new_optimizer = optimizer.apply_gradient(grad, learning_rate=lr)\n    return new_optimizer, new_batch_stats, loss\n\n  return train_step\n```\n\n----------------------------------------\n\nTITLE: Computing Training Steps per Epoch in Flax\nDESCRIPTION: Calculates the number of steps per epoch by dividing the total dataset cardinality by the number of epochs.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# since train_ds is replicated num_epochs times in get_datasets(), we divide by num_epochs\nnum_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\n```\n\n----------------------------------------\n\nTITLE: Setting Up Fake Devices for Multi-Host Environment\nDESCRIPTION: This snippet sets up eight fake devices to mimic a multi-host environment for demonstration purposes. It uses an environment variable to force the CPU backend to simulate multiple devices.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/use_checkpointing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n```\n\n----------------------------------------\n\nTITLE: Configuring XLA for Multi-device Emulation\nDESCRIPTION: Sets an environment variable to emulate multiple devices in a CPU environment for testing multi-device code in notebooks.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n```\n\n----------------------------------------\n\nTITLE: LSTM Model Application\nDESCRIPTION: Demonstrates the application of the initialized LSTM model to process input sequences using the Flax apply function.\nSOURCE: https://github.com/google/flax/blob/main/flax/core/flax_functional_engine.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ny = apply(simple_scan)(init_variables, xs)[0]\nprint('output:\\n', y)\n```\n\n----------------------------------------\n\nTITLE: Identifying Inconsistent Batch Shapes in TFDS Dataset\nDESCRIPTION: Shows how to use Counter to identify inconsistent batch shapes in a test dataset, demonstrating the problem of having a last batch with different dimensions.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncollections.Counter(\n    tuple(batch['image'].shape)\n    for batch in tfds.load('mnist', split='test').batch(per_device_batch_size)\n)\n# output:\n# Counter({(272, 28, 28, 1): 1, (512, 28, 28, 1): 19})\n```\n\n----------------------------------------\n\nTITLE: Setting Flax Configuration to Use Orbax Checkpointing\nDESCRIPTION: Code snippet showing how to explicitly enable Orbax checkpointing in Flax by updating the configuration.\nSOURCE: https://github.com/google/flax/blob/main/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nflax.config.update('flax_use_orbax_checkpointing', True)\n```\n\n----------------------------------------\n\nTITLE: Handling Mutable Modules in JAX Transformations\nDESCRIPTION: Illustrates a common error when using mutable Flax NNX modules in JAX transformations - passing modules by closure. The example shows how Flax NNX prevents tracer leakage by raising errors when modules are not passed as function arguments.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/transforms.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(nnx.Module):\n  def __init__(self):\n    self.count = nnx.Param(jnp.array(0))\n\n  def increment(self):\n    self.count += jnp.array(1)\n\ncounter = Counter()\n\n@nnx.jit\ndef f(x):\n  counter.increment()\n  return 2 * x\n\ntry:\n  y = f(3)\nexcept Exception as e:\n  print(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing Block and MLP Modules with Layer Scanning in Haiku\nDESCRIPTION: Implementation of Block and MLP modules in Haiku using layer_stack for iterative layer application. The Block module applies linear transformation, dropout and ReLU activation, while MLP stacks multiple Block instances.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/haiku_migration_guide.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass Block(hk.Module):\n  def __init__(self, features: int, name=None):\n    super().__init__(name=name)\n    self.features = features\n\n  def __call__(self, x, training: bool):\n    x = hk.Linear(self.features)(x)\n    x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)\n    x = jax.nn.relu(x)\n    return x\n\nclass MLP(hk.Module):\n  def __init__(self, features: int, num_layers: int, name=None):\n      super().__init__(name=name)\n      self.features = features\n      self.num_layers = num_layers\n\n  def __call__(self, x, training: bool):\n    @hk.experimental.layer_stack(self.num_layers)\n    def stack_block(x):\n      return Block(self.features)(x, training)\n\n    stack = hk.experimental.layer_stack(self.num_layers)\n    return stack_block(x)\n```\n\n----------------------------------------\n\nTITLE: Using Partitioned Initializer for Dense Layer in Python\nDESCRIPTION: This code snippet shows how to use the with_partitioning utility to create a partitioned dense layer. It demonstrates the practical application of the proposed axis metadata API in initializing model parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# init kernel with lecun normal and split the output features over the data axis\npartitioned_dense = nn.Dense(features, kernel_init=with_partitioning(nn.initializers.lecun_normal, (None, \"data\")))\n```\n\n----------------------------------------\n\nTITLE: Sampler Configuration and Text Generation\nDESCRIPTION: Sets up the sampler and performs text generation using the loaded model.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsampler = sampler_lib.Sampler(\n    transformer=transformer,\n    vocab=vocab,\n)\n\ninput_batch = [\n    \"\\n# Python program for implementation of Bubble Sort\\n\\ndef bubbleSort(arr):\",\n  ]\n\nout_data = sampler(\n    input_strings=input_batch,\n    total_generation_steps=300,  # The number of steps performed when generating a response.\n  )\n\nfor input_string, out_string in zip(input_batch, out_data.text):\n  print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n  print()\n  print(10*'#')\n```\n\n----------------------------------------\n\nTITLE: Creating a Development Branch in Git\nDESCRIPTION: Creates and switches to a new Git branch for development work on Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b my_development_branch\n```\n\n----------------------------------------\n\nTITLE: Testing Flax Module Imports\nDESCRIPTION: Verifies that all necessary Flax modules and submodules can be imported correctly, including training utilities and metrics.\nSOURCE: https://github.com/google/flax/blob/main/tests/import_test.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Verify we can import everything.\nimport flax\nfrom flax.training import (checkpoints, dynamic_scale, early_stopping, lr_schedule,\n                           orbax_utils, prefetch_iterator, train_state, common_utils)\nfrom flax.metrics import tensorboard\n```\n\n----------------------------------------\n\nTITLE: Setting up Multiple Device Emulation in JAX\nDESCRIPTION: Sets up XLA flags to emulate multiple devices (8) in a CPU environment for demonstration purposes. This is only needed in environments like Google Colab that don't have actual multiple devices.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/parallel_training/flax_on_pjit.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n```\n\n----------------------------------------\n\nTITLE: Using StateAxes for fine-grained state control within Modules\nDESCRIPTION: The proposed Lift symbol API for vmap, allowing control of how different state types within a Module are vectorized.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstate_axes = StateAxes({Param: 1, BatchStat: None})\n\n@nnx.vmap(in_axes=(state_axes, 0))\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Verifying Torchvision MNIST Data Format for Jax+Flax\nDESCRIPTION: Loads the MNIST dataset using the torchvision method and prints the shapes and data types of the training and test sets to verify they match the expected format for Jax+Flax models.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain, test = get_dataset_torch()\nprint(train['image'].shape, train['image'].dtype)\nprint(train['label'].shape, train['label'].dtype)\nprint(test['image'].shape, test['image'].dtype)\nprint(test['label'].shape, test['label'].dtype)\n```\n\n----------------------------------------\n\nTITLE: Using ToLinen in Linen Module Composition\nDESCRIPTION: Demonstrates how to use ToLinen to create a submodule within a larger Linen module structure.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass LinenOuter(nn.Module):\n  out_dim: int\n  @nn.compact\n  def __call__(self, x):\n    dot = bridge.to_linen(NNXDot, x.shape[-1], self.out_dim)\n    b = self.param('b', nn.initializers.lecun_normal(), (1, self.out_dim))\n    return dot(x) + b\n\nx = jax.random.normal(jax.random.key(42), (4, 32))\nmodel = LinenOuter(out_dim=64)\ny, variables = model.init_with_output(jax.random.key(0), x)\nw, b = variables['params']['ToLinen_0']['w'], variables['params']['b']\nprint(w.shape, b.shape, y.shape)\n```\n\n----------------------------------------\n\nTITLE: Identifying Classification Mistakes in Image Recognition\nDESCRIPTION: Compares model predictions with ground truth labels to identify misclassifications. It maps Imagenette labels to ImageNet labels and creates a list of indices where predictions don't match the actual labels.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Find classification mistakes.\npreds_labels = list(zip(logits.argmax(axis=-1), map(imagenette_imagenet2012, batch['label'])))\nerror_idxs = [idx for idx, (pred, label) in enumerate(preds_labels) if pred != label]\nerror_idxs\n```\n\n----------------------------------------\n\nTITLE: Visualizing Image Classification Errors with Matplotlib\nDESCRIPTION: Creates a grid visualization of misclassified images along with their predicted and actual labels. This helps in understanding the nature of model errors by showing both the incorrect prediction and the ground truth.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# The mistakes look all quite reasonable.\nshow_img_grid(\n    [batch['image'][idx] for idx in error_idxs[:9]],\n    [f'pred: {imagenet2012_label(preds_labels[idx][0])}\\n'\n     f'label: {imagenet2012_label(preds_labels[idx][1])}'\n    for idx in error_idxs[:9]],\n)\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Replacing FrozenDict pop() with flax.core.pop\nDESCRIPTION: Demonstrates replacing FrozenDict's pop() method with the flax.core.pop utility function for both dict types.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstate, params = variables.pop('params')\n```\n\nLANGUAGE: python\nCODE:\n```\nstate, params = flax.core.pop(variables, 'params')\n```\n\n----------------------------------------\n\nTITLE: Running PPO with Default Configuration\nDESCRIPTION: Command to run the PPO implementation with default parameters, which trains for 40M frames on the Pong game. By default, logs and checkpoints are stored in '/tmp/ppo_training'.\nSOURCE: https://github.com/google/flax/blob/main/examples/ppo/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing kw_only in Module Classes\nDESCRIPTION: Shows how to enable kw_only functionality in base modules using class inheritance parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module, kw_only=True):\n  ...\n\nclass Child(BaseLayer):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Type Promotion in Flax Dense Layer\nDESCRIPTION: A simplified example implementation showing how to implement default dtype promotion in a Flax Dense layer. The implementation uses jnp.result_type to determine the appropriate output dtype based on inputs and parameters, while keeping param_dtype separate for numerical stability.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1777-default-dtype.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef promote_arrays(*xs, dtype):\n if dtype is None:\n   dtype = jnp.result_type(*jax.tree_util.tree_leaves(xs))\n return jax.tree_util.tree_map(lambda x: jnp.asarray(x, dtype), xs)\n\nDtype = Any\nclass Dense(nn.Module):\n features: int\n kernel_init: Callable\n bias_init: Callable\n dtype: Optional[Dtype] = None\n param_dtype: Dtype = jnp.float32\n\n @nn.compact\n def __call__(self, x):\n   kernel = self.param(\"kernel\",\n                       self.kernel_init,\n                       (x.shape[-1], self.features), self.param_dtype)\n   bias = self.param(\"bias\", self.bias_init, (self.features,), self.param_dtype)\n   x, kernel, bias = promote_arrays(x, kernel, bias, dtype=self.dtype)\n   return x @ kernel + bias\n```\n\n----------------------------------------\n\nTITLE: Implementing a Partitioned Metadata Class in Python\nDESCRIPTION: An implementation of the AxisMetadata base class for tracking partitioning metadata. This class enables tracking partition specifications for JAX's pjit and includes a utility function to wrap existing initializers with partitioning metadata.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2434-general-metadata.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Partitioned(flax.struct.PyTreeNode, AxisMetadata):\n  value: Any\n  names: Tuple[Optional[str], ...] = flax.struct.field(pytree_node=False)\n\n  def add_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:\n    axis_name = self._get_partition_name(params)\n    names = list(self.names)\n    names.insert(index, axis_name)\n    return self.replace(names=tuple(names))\n\n  def remove_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:\n    axis_name = self._get_partition_name(params)\n    names = list(self.names)\n    assert names.pop(index) == axis_name\n    return self.replace(names=tuple(names))\n\ndef with_partitioning(init_fn, names):\n  def wrapper(*args, **kwargs):\n    return Partitioned(init_fn(*args, **kwargs), names)\n  return wrapper\n```\n\n----------------------------------------\n\nTITLE: Accessing Attributes in Top-Level Flax Module\nDESCRIPTION: This example shows that attributes defined in the setup method are not accessible in unbound, top-level modules.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass TopLevelAccess(nn.Module):\n\n  def setup(self):\n    self.foo = nn.Dense(2)\n\nmdl = TopLevelAccess()\nassert not hasattr(mdl, \"foo\")  # foo is not defined because setup is not called\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing WMT Datasets with TensorFlow\nDESCRIPTION: Commands to download and prepare WMT datasets using TensorFlow Datasets, specifically for the WMT17 German-English and WMT14 German-English translation datasets.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m tensorflow_datasets.scripts.download_and_prepare --datasets=wmt17_translate/de-en\n```\n\n----------------------------------------\n\nTITLE: Writing Doctest Code Blocks in Bash\nDESCRIPTION: Example showing how to format doctest code blocks in documentation using proper indentation and syntax\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Example code::\n#\n#   def sum(a, b):\n#     return a + b\n#\n#   sum(0, 1)\n```\n\n----------------------------------------\n\nTITLE: Importing Flax NNX Dependencies\nDESCRIPTION: Basic imports required for working with randomness in Flax NNX, including core Flax NNX module, JAX, and numpy through JAX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport jax\nfrom jax import random, numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Using Same Seed for Different Streams\nDESCRIPTION: Demonstrates that using the same initial seed for different streams produces identical sequences within each stream.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nvariables = rng_module_two_streams.init(\n  {'rng_stream1': jax.random.key(0), 'rng_stream2': jax.random.key(0)}\n)\n```\n\n----------------------------------------\n\nTITLE: Captured Modules as Outputs in NNX Transforms (Invalid)\nDESCRIPTION: Demonstrates why captured Modules cannot be outputs in NNX transforms, as it would result in implicit cloning.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nm = SomeModule()\n\n@nnx.vmap(out_axes=0, axis_size=5)\ndef f():\n  return m\n\nassert m is not f()  # implicit cloning\n```\n\n----------------------------------------\n\nTITLE: Setting up environment and FP8 check function in Python\nDESCRIPTION: This snippet sets up the necessary imports and defines a function to check if the XLA-optimized HLO will call an FP8 dot operation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport flax\nimport jax\nimport re\nimport pprint\nfrom jax import random\nfrom jax import numpy as jnp\nfrom jax._src import test_util as jtu\nfrom flax import linen as nn\nfrom flax.linen import fp8_ops\n\ne4m3 = jnp.float8_e4m3fn\nf32 = jnp.float32\nE4M3_MAX = jnp.finfo(e4m3).max.astype(f32)\n\nassert jtu.is_cuda_compute_capability_at_least(\"9.0\")\n\ndef check_fp8_call(lowered):\n  hlo = lowered.compile()\n  if re.search(r\"custom-call\\(f8e4m3fn.*, f8e4m3fn.*\", hlo.as_text()):\n    print(\"Fp8 call detected!\")\n  else:\n    print(\"No Fp8 call!\")\n```\n\n----------------------------------------\n\nTITLE: Legacy Momentum Optimizer Implementation\nDESCRIPTION: Previous implementation of momentum optimizer using Flax's OptimizerDef class, showing state management and gradient application.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@flax.struct.dataclass\nclass _MomentumHyperParams:\n  learning_rate: jnp.ndarray\n  beta: jnp.ndarray\n\n@flax.struct.dataclass\nclass _MomentumParamState:\n  momentum: np.ndarray\n\nclass Momentum(flax.optim.OptimizerDef):\n\n  def __init__(self, learning_rate=None, beta=0.9):\n    super().__init__(\n      _MomentumHyperParams(learning_rate, beta)\n    )\n\n  def init_param_state(self, param):\n    return _MomentumParamState(jnp.zeros_like(param))\n\n  def apply_param_gradient(self, step, hyper_params, param, state, grad):\n    del step\n    assert hyper_params.learning_rate is not None\n    new_momentum = state.momentum * hyper_params.beta + grad\n    new_params = param - hyper_params.learning_rate * new_momentum\n    return new_params, _MomentumParamState(new_momentum)\n```\n\n----------------------------------------\n\nTITLE: ConvLSTM Current Implementation Example\nDESCRIPTION: Demonstrates the current unintuitive API for initializing ConvLSTM cells where users need to manually specify batch size, image shape, and output features.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 4, 4, 3)) # (batch, *image_shape, channels)\n\n#                                        image shape: vvvvvvv\ncarry = nn.ConvLSTMCell.initialize_carry(key1, (16,), (64, 64, 16))\n#                                   batch size: ^^             ^^ :output features\n\nlstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))\n(carry, y), initial_params = lstm.init_with_output(key2, carry, x)\n```\n\n----------------------------------------\n\nTITLE: Counting Examples with Drop Remainder in Multi-Device Setup\nDESCRIPTION: Demonstrates how dropping remainder batches leads to incorrect results when using multiple devices, as some examples are discarded.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsum(\n    np.prod(batch['label'].shape)\n    for batch in tfds.load('mnist', split='test')\n        .batch(per_device_batch_size, drop_remainder=True)\n        .batch(jax.local_device_count())\n)\n# output:\n# 9728\n```\n\n----------------------------------------\n\nTITLE: Compact MLP Module Definition\nDESCRIPTION: Shows how to define an MLP using the compact API which provides a more concise syntax for simple modules.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass CompactMLP(nn.Module):\n  hidden_size: int\n  out_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    a = nn.Dense(self.hidden_size)(x)\n    h = nn.relu(a)\n    return nn.Dense(self.out_size)(h)\n```\n\n----------------------------------------\n\nTITLE: Failed Attempt at Using vmap Inside a Flax Module\nDESCRIPTION: This code demonstrates an incorrect approach to applying JAX's vmap transformation inside a Flax Module. This will fail because JAX cannot vectorize a Flax Module directly as it's not a JAX array or simple container.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/lift.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass NaiveVmapMLP(nn.Module):\n  @nn.compact\n  def __call__(self, xs):\n    mlp = MLP()\n    return jax.vmap(lambda mlp, x: mlp(x))(mlp, xs)  # fails\n```\n\n----------------------------------------\n\nTITLE: LSTMCell Usage Example with New API\nDESCRIPTION: Demonstrates the usage of the refactored LSTMCell with the new features attribute and simplified initialization pattern.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((2, 100, 10)) # (batch, time, features)\n\ncell = nn.LSTMCell(features=32)\ncarry = cell.initialize_carry(PRNGKey(0), x[:, 0]) # sample input\n\n(carry, y), variables = cell.init_with_output(PRNGKey(1), carry, x)\n```\n\n----------------------------------------\n\nTITLE: Testing Linen Training Step in Flax\nDESCRIPTION: Tests the Linen training step function with sample inputs and labels.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain_step(jax.random.key(0), state, sample_x, jnp.ones((1,), dtype=jnp.int32))\n```\n\n----------------------------------------\n\nTITLE: Converting Partitioned Metadata to JAX Sharding Annotations in Python\nDESCRIPTION: This code snippet shows how to convert the Partitioned metadata into JAX pjit sharding annotations. It demonstrates the integration of the proposed API with other libraries and APIs.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef to_sharding_spec(x):\n  if isinstance(x, Partitioned):\n    return PartitionSpec(*x.names)\n  else:\n    # fully replicated\n    return PartitionSpec()\n\n# Result: {\"params\": {\"kernel\": PartitionSpec(None, \"data\"), bias: PartitionSpec()}}\nvariables_pspec = jax.tree.map(to_sharding_spec, variables, is_leaf=lambda x: isinstance(x, Partitioned))\n```\n\n----------------------------------------\n\nTITLE: Using Default PRNG Stream\nDESCRIPTION: Demonstration of using default PRNG stream as fallback when specific streams are not found.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/randomness.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrngs = nnx.Rngs(0, params=1)\n\nkey1 = rngs.params() # Call params.\nkey2 = rngs.dropout() # Fallback to the default stream.\nkey3 = rngs() # Call the default stream directly.\n\n# Test with the `Model` that uses `params` and `dropout`.\nmodel = Model(rngs)\ny = model(jnp.ones((1, 20)))\n\nnnx.display(rngs)\n```\n\n----------------------------------------\n\nTITLE: Initializing Character Table for Data Generation\nDESCRIPTION: Creates a character table for encoding/decoding numbers and arithmetic operations, and demonstrates example generation.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Examples are generated on the fly.\nctable = input_pipeline.CharacterTable('0123456789+= ')\nlist(ctable.generate_examples(5))\n```\n\n----------------------------------------\n\nTITLE: Importing Flax Errors Module in Python\nDESCRIPTION: This code snippet demonstrates how to import and use the flax.errors module in Python. It excludes the FlaxError class from the import.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: flax.errors\n    :members:\n    :exclude-members: FlaxError\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports core dependencies and sets up temporary path for loading Gemma modules.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import nnx\nimport sentencepiece as spm\n\nimport sys\nimport tempfile\nwith tempfile.TemporaryDirectory() as tmp:\n  ! git clone https://github.com/google/flax.git {tmp}/flax\n  sys.path.append(f\"{tmp}/flax/examples/gemma\")\n  import params as params_lib\n  import sampler as sampler_lib\n  import transformer as transformer_lib\n  sys.path.pop();\n```\n\n----------------------------------------\n\nTITLE: Documenting apply Function in Flax Linen Module\nDESCRIPTION: Automates the documentation generation for the 'apply' function in Flax's Linen module. This function is likely used to apply a neural network module to input data.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.linen/init_apply.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: apply\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Training on TPU Pod Slice\nDESCRIPTION: Comprehensive shell script to create a TPU pod slice (v3-32), install dependencies, and start training in SPMD fashion across all hosts.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nVM_NAME=wmt\nREPO=https://github.com/google/flax\nBRANCH=main\nWORKDIR=gs://$YOUR_BUCKET/flax/examples/wmt/$(date +%Y%m%d_%H%M)\n\ngcloud alpha compute tpus tpu-vm create $VM_NAME \\\n    --zone=$ZONE \\\n    --version v2-alpha --accelerator-type v3-32\nFLAGS=\"--config.num_train_steps=$(( 100 * 1000 * 8/32 ))\n--config.warmup_steps=$(( 1000 * 8/32 ))\n--config.checkpoint_every_steps=$(( 10 * 1000 * 8/32 ))\"\n\ngcloud alpha compute tpus tpu-vm ssh $VM_NAME --zone $ZONE \\\n--worker=all --command \"\nset -x\npip install 'jax[tpu]>=0.2.21' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html &&\npip install --user git+$REPO.git &&\n(test -d flax || git clone --depth=1 -b $BRANCH $REPO) &&\ncd flax &&\npip install -e . &&\ncd examples/wmt &&\npip install -r requirements.txt &&\nexport TFDS_DATA_DIR=gs://$GCS_TFDS_BUCKET/datasets &&\npython3 main.py --workdir=$WORKDIR --config=configs/default.py $FLAGS\n\"\n```\n\n----------------------------------------\n\nTITLE: Testing Model Forward Pass\nDESCRIPTION: Performs a test forward pass through the CNN model using dummy input data.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\ny = model(jnp.ones((1, 28, 28, 1)))\ny\n```\n\n----------------------------------------\n\nTITLE: Inconsistent Input/Output Aliases in NNX Transforms (Invalid)\nDESCRIPTION: Shows an example of inconsistent input/output aliases in NNX transforms that should be rejected.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=1)\ndef g(m: Module):\n  return m\n```\n\n----------------------------------------\n\nTITLE: Module Inspection Comparison - Linen vs NNX\nDESCRIPTION: Compares module inspection capabilities between Flax Linen and NNX, showing how NNX provides immediate access to attributes while Linen uses lazy initialization.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Linen\nclass Block(nn.Module):\n  def setup(self):\n    self.linear = nn.Dense(10)\n\nblock = Block()\n\ntry:\n  block.linear  # AttributeError\nexcept AttributeError as e:\n  pass\n\n# NNX\nclass Block(nnx.Module):\n  def __init__(self, rngs):\n    self.linear = nnx.Linear(5, 10, rngs=rngs)\n\nblock = Block(nnx.Rngs(0))\nblock.linear\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Flax Project\nDESCRIPTION: A requirements.txt file specifying the exact versions of Python packages needed for a Flax-based machine learning project. It includes core ML frameworks (Flax, TensorFlow), optimization tools (Optax), data handling (TFDS), and image processing libraries (Pillow).\nSOURCE: https://github.com/google/flax/blob/main/examples/vae/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nabsl-py==1.4.0\nflax==0.6.9\nnumpy==1.23.5\noptax==0.1.5\nPillow==10.2.0\ntensorflow==2.12.0\ntensorflow-datasets==4.9.2\n```\n\n----------------------------------------\n\nTITLE: Preparing ImageNet Dataset\nDESCRIPTION: Python script to download and prepare the ImageNet dataset using TensorFlow Datasets.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython -c \"\nimport tensorflow_datasets as tfds\ntfds.builder('imagenet2012').download_and_prepare(\n    download_config=tfds.download.DownloadConfig(\n        manual_dir='$IMAGENET_DOWNLOAD_PATH'))\n\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment for Flax Development\nDESCRIPTION: Creates a Python virtual environment and activates it for Flax development. This isolates the development dependencies from the system-wide Python installation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m virtualenv env\n. env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Using Unbox Syntax with Flax Parameters in Python\nDESCRIPTION: Demonstrates how to use the unbox parameter when retrieving module parameters. The default behavior (unbox=True) returns plain JAX arrays, while setting unbox=False preserves AxisMetadata boxes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2434-general-metadata.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkernel = self.param(\"kernel\", self.kernel_init, shape)  # No AxisMetadata instances\nkernel_box = self.get_variable(\"param\", \"kernel\", unbox=False)  # AxisMetadata boxes are preserved\n```\n\n----------------------------------------\n\nTITLE: Launching ImageNet Training on Google Cloud\nDESCRIPTION: Shell command to launch ImageNet training on a high-performance Google Cloud VM with 8 NVIDIA V100 GPUs. Uses n1-standard-96 machine type for large-scale training.\nSOURCE: https://github.com/google/flax/blob/main/examples/cloud/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython examples/cloud/launch_gce.py \\\n  --project=$PROJECT \\\n  --zone=us-west1-a \\\n  --machine_type=n1-standard-96 \\\n  --accelerator_type=nvidia-tesla-v100 --accelerator_count=8 \\\n  --gcs_workdir_base=gs://$GCS_BUCKET/workdir_base \\\n  --tfds_data_dir=gs://$GCS_TFDS_BUCKET/datasets \\\n  --repo=${REPO:-https://github.com/google/flax} \\\n  --branch=${BRANCH:-main} \\\n  --example=imagenet \\\n  --args='--config=configs/v100_x8_mixed_precision.py' \\\n  --name=v100_x8_mixed_precision\n```\n\n----------------------------------------\n\nTITLE: Alternative MLP Initialization Syntax\nDESCRIPTION: Demonstrates equivalent initialization using explicit apply() method with PRNG and mutable parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n_, variables = mlp.apply({}, x, rngs={\"params\": random.key(0)}, mutable=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Flax\nDESCRIPTION: Installing JAX and Flax dependencies using pip. Installs the latest JAXlib version and Flax from GitHub head.\nSOURCE: https://github.com/google/flax/blob/main/docs/linen_intro.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install the newest JAXlib version.\n!pip install --upgrade -q pip jax jaxlib\n# Install Flax at head:\n!pip install --upgrade -q git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Documenting init_with_output Function in Flax Linen Module\nDESCRIPTION: Creates documentation for the 'init_with_output' function in Flax's Linen module. This function likely initializes a module and returns both the initialized parameters and the output of the first forward pass.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.linen/init_apply.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: init_with_output\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Flax SPMD Module\nDESCRIPTION: Sphinx documentation structure that outlines the available functions in the flax.linen.spmd module. It provides a reference to functions for model partitioning, sharding, and logical axis mapping in Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.linen/spmd.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nSPMD\n----------------------\n\n.. automodule:: flax.linen.spmd\n.. currentmodule:: flax.linen\n\n.. autofunction:: Partitioned\n.. autofunction:: with_partitioning\n.. autofunction:: get_partition_spec\n.. autofunction:: get_sharding\n.. autofunction:: LogicallyPartitioned\n.. autofunction:: logical_axis_rules\n.. autofunction:: set_logical_axis_rules\n.. autofunction:: get_logical_axis_rules\n.. autofunction:: logical_to_mesh_axes\n.. autofunction:: logical_to_mesh\n.. autofunction:: logical_to_mesh_sharding\n.. autofunction:: with_logical_constraint\n.. autofunction:: with_logical_partitioning\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Base/Child Module Inheritance Issue\nDESCRIPTION: Example showing the current limitation where child classes must provide defaults for parent class arguments without kw_only support.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module):\n  mesh: Optional[jax.experimental.mesh.Mesh] = None\n\n  def with_sharding(self, some_variable, some_sharding):\n    if self.mesh:\n      # Do something useful here.\n\nclass Child(BaseLayer):\n  num_heads: int  # Don't want to have to set a default argument!\n\n  def __call__(self, x):\n    ...\n```\n\n----------------------------------------\n\nTITLE: TPU VM Setup and Training\nDESCRIPTION: Complete sequence of commands for setting up TPU VM, installing dependencies, and running training.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngcloud alpha compute tpus tpu-vm create $VM_NAME \\\n    --zone=$ZONE \\\n    --version v2-alpha --accelerator-type v3-32\nFLAGS=\"--config.batch_size=$((32*256))\"\n\ngcloud alpha compute tpus tpu-vm ssh $VM_NAME --zone $ZONE \\\n--worker=all --command \"\npip install 'jax[tpu]>=0.2.21' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html &&\npip install --user git+$REPO.git &&\ngit clone --depth=1 -b $BRANCH $REPO &&\ncd flax/examples/imagenet &&\npip install -r requirements.txt &&\nexport TFDS_DATA_DIR=gs://$GCS_TFDS_BUCKET/datasets &&\npython3 main.py --workdir=$WORKDIR --config=configs/tpu.py $FLAGS\n\"\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Constraints\nDESCRIPTION: Requirements file that defines exact versions of Python packages needed for the project. Includes core ML libraries like Flax (0.3.6), NumPy (1.22.0), and Optax (0.1.0) along with supporting packages.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.3.6\nnumpy==1.22.0\noptax==0.1.0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Flax Transfer Learning\nDESCRIPTION: Sets up required packages including transformers with Flax support, and latest versions of Flax and JAX libraries.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q \"transformers[flax]\"\n! pip install -U -q flax jax jaxlib\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual Padding for Evaluation Loop\nDESCRIPTION: A manual implementation of padding the last batch to maintain consistent shapes across batches, while discarding predictions for padded examples.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/full_eval.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nshard = lambda x: einops.rearrange(\n    x, '(d b) ... -> d b ...', d=jax.local_device_count())\nunshard = lambda x: einops.rearrange(x, 'd b ... -> (d b) ...')\n\ncorrect = total = 0\nfor batch in ds.as_numpy_iterator():\n  images = batch['image']\n  n = len(images)\n  padding = np.zeros([per_host_batch_size - n, *images.shape[1:]], images.dtype)\n  padded_images = np.concatenate([images, padding])\n  preds = unshard(get_preds(variables, shard(padded_images)))[:n]\n  total += n\n  correct += (batch['label'] == preds.argmax(axis=-1)).sum()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Flax NNX Model Surgery\nDESCRIPTION: Imports necessary libraries and modules for performing model surgery in Flax NNX, including JAX, Flax, and Orbax.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import *\nfrom pprint import pprint\nimport functools\n\nimport jax\nfrom jax import lax, numpy as jnp, tree_util as jtu\n\nfrom jax.sharding import PartitionSpec, Mesh, NamedSharding\nfrom jax.experimental import mesh_utils\nimport flax\nfrom flax import nnx\nimport flax.traverse_util\nimport numpy as np\nimport orbax.checkpoint as orbax\n\nkey = jax.random.key(0)\n```\n\n----------------------------------------\n\nTITLE: Running PPO with Custom Configuration\nDESCRIPTION: Example of running PPO with customized hyperparameters, including different game (Seaquest), total frames (20M), and disabling the decaying learning rate and clip parameter. Output files are saved to a specified directory.\nSOURCE: https://github.com/google/flax/blob/main/examples/ppo/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_main.py --config=configs/default.py --config.game=Seaquest --config.total_frames=20000000 --config.decaying_lr_and_clip_param=False --workdir=/tmp/seaquest\n```\n\n----------------------------------------\n\nTITLE: Non-inheritable kw_only Behavior Example\nDESCRIPTION: Demonstrates how kw_only behavior is not inherited in subclasses, showing different behavior between parent and child class instantiation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module, kw_only=True):\n  base_muliplier: Optional[int] = -1\n\nclass ChildLayer(BaseLayer):\n  child_multiplier: int\n\nBaseLayer(2)   # This will throw error\nChildLayer(2)  # But this will not\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs required Python packages including ml-collections and latest Flax version from Github\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q clu ml-collections git+https://github.com/google/flax\n```\n\n----------------------------------------\n\nTITLE: Setting up Sphinx Table of Contents for Flax Examples in RST\nDESCRIPTION: This RST code snippet sets up a table of contents tree for Flax examples documentation. It uses the Sphinx toctree directive with a maximum depth of 2 to organize and link to different categories of examples.\nSOURCE: https://github.com/google/flax/blob/main/docs/examples/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   core_examples\n   google_research_examples\n   repositories_that_use_flax\n   community_examples\n```\n\n----------------------------------------\n\nTITLE: Documenting Flax NNX Linear Layer Classes with reStructuredText\nDESCRIPTION: This snippet uses reStructuredText directives to document the linear layer classes in the Flax NNX module. It sets up the automodule and currentmodule, then uses flax_module directives to document specific classes.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/linear.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. flax_module::\n  :module: flax.nnx\n  :class: Conv\n\n.. flax_module::\n  :module: flax.nnx\n  :class: ConvTranspose\n\n.. flax_module::\n  :module: flax.nnx\n  :class: Embed\n\n.. flax_module::\n  :module: flax.nnx\n  :class: Linear\n\n.. flax_module::\n  :module: flax.nnx\n  :class: LinearGeneral\n\n.. flax_module::\n  :module: flax.nnx\n  :class: Einsum\n```\n\n----------------------------------------\n\nTITLE: Under-the-hood representation of input/output aliasing\nDESCRIPTION: Explanation of how NNX transforms handle input/output aliasing internally, revealing the inconsistency in the previous example.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=(0, 1))\ndef g_real(m: Module):\n  return m, m\n```\n\n----------------------------------------\n\nTITLE: Initializing Variables and Sample Input in Flax\nDESCRIPTION: Basic setup code defining batch size, sequence length, feature dimensions, and creating a sample input tensor for RNN processing.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 32\nseq_len = 10\nin_features = 64\nout_features = 128\n\nx = jnp.ones((batch_size, seq_len, in_features))\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Pre-commit Hooks\nDESCRIPTION: Installs the pre-commit package and sets up pre-commit hooks for the Flax project. This ensures automated checks are run before each commit.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Flax Project\nDESCRIPTION: A requirements.txt file listing all necessary Python packages and their versions for a Flax project. Includes JAX with CUDA support (configurable for TPU), TensorFlow, and various ML-related libraries.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.6.0\n-f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\njax[cuda11_cudnn805]>=0.3.16  # change to jax[tpu] if running on tpus\nml-collections==0.1.0\nnumpy==1.22.0\noptax==0.1.0\nsentencepiece==0.1.96\nsix==1.15.0\ntensorflow==2.11.1\ntensorflow-datasets==4.4.0\ntensorflow-text==2.8.1\n```\n\n----------------------------------------\n\nTITLE: Examining Batch Structure in the Dataset\nDESCRIPTION: Shows how a batch is structured and how a single query is one-hot encoded in the dataset.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbatch = ctable.get_batch(5)\n# A single query (/answer) is one-hot encoded.\nbatch['query'][0]\n```\n\n----------------------------------------\n\nTITLE: Proposed kw_only Module Implementation\nDESCRIPTION: Example showing how to use the proposed kw_only feature with nn.Module subclasses.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module, kw_only=True):\n  ...\n\nclass Child(BaseLayer):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Utilizing PyTreeNode Class from flax.struct in Python\nDESCRIPTION: This snippet shows how to use the PyTreeNode class from flax.struct. PyTreeNode is likely a base class for creating tree-like structures in Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.struct.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyPyTreeNode(struct.PyTreeNode):\n    # class definition\n```\n\n----------------------------------------\n\nTITLE: Naive Partial Initialization in Flax NNX\nDESCRIPTION: Demonstrates a naive approach to partial model initialization, which may allocate additional memory midway.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Some pretrained model state\nold_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n\nsimple_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(42)))\nprint(f'Number of jax arrays in memory at start: {len(jax.live_arrays())}')\n# In this line, extra kernel and bias is created inside the new LoRALinear!\n# They are wasted, because you are going to use the kernel and bias in `old_state` anyway.\nsimple_model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=nnx.Rngs(42))\nprint(f'Number of jax arrays in memory midway: {len(jax.live_arrays())}'\n      ' (4 new created in LoRALinear - kernel, bias, lora_a & lora_b)')\nnnx.update(simple_model, old_state)\nprint(f'Number of jax arrays in memory at end: {len(jax.live_arrays())}'\n      ' (2 discarded - only lora_a & lora_b are used in model)')\n```\n\n----------------------------------------\n\nTITLE: Documenting Flax NNX Variable Classes and Functions\nDESCRIPTION: ReStructuredText documentation directives for Flax's variable-related classes and utility functions. Covers core variable types (BatchStat, Cache, Empty, etc.) and helper functions for variable naming and metadata.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autoclass:: BatchStat\n   :members:\n.. autoclass:: Cache\n   :members:\n.. autoclass:: Empty\n   :members:\n.. autoclass:: Intermediate\n   :members:\n.. autoclass:: Param\n   :members:\n.. autoclass:: Variable\n   :members:\n.. autoclass:: VariableMetadata\n   :members:\n.. autoclass:: VariableState\n   :members:\n\n.. autofunction:: with_metadata\n\n.. autofunction:: variable_name_from_type\n.. autofunction:: variable_type_from_name\n.. autofunction:: set_variable_name\n```\n\n----------------------------------------\n\nTITLE: Implementing a DenseGeneral Module with comprehensive documentation in FLAX\nDESCRIPTION: Provides an example of a well-documented Module class showing proper attribute and parameter documentation, including type information and detailed descriptions for both the class and its method.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass DenseGeneral(Module):\n  \"\"\"A linear transformation with flexible axes.\n    Attributes:\n      features: int or tuple with number of output features.\n      axis: int or tuple with axes to apply the transformation on. For instance,\n        (-2, -1) will apply the transformation to the last two axes.\n      batch_dims: tuple with batch axes.\n      use_bias: whether to add a bias to the output (default: True).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      bias_init: initializer function for the bias.\n      precision: numerical precision of the computation see `jax.lax.Precision`\n        for details.\n  \"\"\"\n  features: Union[int, Iterable[int]]\n  axis: Union[int, Iterable[int]] = -1\n  batch_dims: Iterable[int] = ()\n  use_bias: bool = True\n  dtype: Dtype = jnp.float32\n  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init\n  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = zeros\n  precision: Any = None\n\n  @compact\n  def __call__(self, inputs: Array) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n    Args:\n      inputs: The nd-array to be transformed.\n    Returns:\n      The transformed input.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Flax Dependencies in Python\nDESCRIPTION: Installs the required packages clu and flax using pip in a quiet mode to minimize output.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install CLU & Flax.\n!pip install -q clu flax\n```\n\n----------------------------------------\n\nTITLE: Committing changes to Flax repository\nDESCRIPTION: Stages changed files, creates a commit with a message, and syncs with the main repository.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit add file1.py file2.py ...\ngit commit -m \"Your commit message\"\ngit fetch upstream\ngit rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Importing Attention Utility Functions from Flax NNX Module\nDESCRIPTION: This snippet demonstrates how to import various attention-related utility functions from the flax.nnx module. These functions are used for creating and manipulating attention masks, as well as performing dot product attention calculations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/attention.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import combine_masks, dot_product_attention, make_attention_mask, make_causal_mask\n```\n\n----------------------------------------\n\nTITLE: Replacing FrozenDict unfreeze() with flax.core.unfreeze\nDESCRIPTION: Demonstrates replacing FrozenDict's unfreeze() method with the flax.core.unfreeze utility function.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvariables = variables.unfreeze()\n```\n\nLANGUAGE: python\nCODE:\n```\nvariables = flax.core.unfreeze(variables)\n```\n\n----------------------------------------\n\nTITLE: Custom BatchNorm Implementation in NNX\nDESCRIPTION: Demonstrates implementing a simplified BatchNorm layer in NNX showing state handling with variables and updates.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BatchNorm(nnx.Module):\n  def __init__(self, features: int, mu: float = 0.95):\n    # Variables\n    self.scale = nnx.Param(jax.numpy.ones((features,)))\n    self.bias = nnx.Param(jax.numpy.zeros((features,)))\n    self.mean = nnx.BatchStat(jax.numpy.zeros((features,)))\n    self.var = nnx.BatchStat(jax.numpy.ones((features,)))\n    self.mu = mu  # Static\n\ndef __call__(self, x):\n  mean = jax.numpy.mean(x, axis=-1)\n  var = jax.numpy.var(x, axis=-1)\n  # ema updates\n  self.mean.value = self.mu * self.mean + (1 - self.mu) * mean\n  self.var.value = self.mu * self.var + (1 - self.mu) * var\n  # normalize and scale\n  x = (x - mean) / jax.numpy.sqrt(var + 1e-5)\n  return x * self.scale + self.bias\n```\n\n----------------------------------------\n\nTITLE: Squashing Git Commits for Flax Pull Requests\nDESCRIPTION: Command to squash multiple commits into a single commit by rebasing against main branch and creating a new consolidated commit. Used when a pull request has too many commits that may cause Flax docs build process to fail.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngit rebase main && git reset --soft main && git commit\n```\n\n----------------------------------------\n\nTITLE: Visualizing MNIST training images\nDESCRIPTION: Displays a grid of 25 MNIST training images along with their corresponding labels using the previously defined show_img_grid helper function.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshow_img_grid(\n    [train_ds['image'][idx] for idx in range(25)],\n    [f'label={train_ds[\"label\"][idx]}' for idx in range(25)],\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing FLIP Label in GitHub Issues\nDESCRIPTION: This markdown link directs users to GitHub issues labeled as FLIP, facilitating easy access to all FLIP-related discussions and proposals.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[FLIP label]: https://github.com/google/flax/issues?q=label%3AFLIP\n```\n\n----------------------------------------\n\nTITLE: Custom Filters for Module Selection in NNX Transforms\nDESCRIPTION: Demonstrates how users currently need to use custom filters to select specific Modules in NNX transforms.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nselect_m1 = lambda path, value: path[0] == 0\nselect_m2 = lambda path, value: path[0] == 1\n\n@nnx.vmap(state_axes={select_m1: 1, select_m2: 0})\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing JAX and Flax Dependencies\nDESCRIPTION: Sets up the required environment by installing the latest JAXlib version and Flax from the GitHub repository.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/flax_basics.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install the latest JAXlib version.\n!pip install --upgrade -q pip jax jaxlib\n# Install Flax at head:\n!pip install --upgrade -q git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Decoding Model Predictions\nDESCRIPTION: Decodes the model's predictions back to human-readable text to see the results.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nctable.decode_onehot(preds)\n```\n\n----------------------------------------\n\nTITLE: Importing Hugging Face Datasets for Data Loading\nDESCRIPTION: Imports the Hugging Face datasets library to access MNIST data for Jax+Flax model training.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#!pip install datasets # datasets isn't preinstalled on Colab; uncomment to install\nfrom datasets import load_dataset\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs required Python packages for running Gemma with Flax NNX.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install --no-deps -U flax\n! pip install jaxtyping kagglehub treescope\n```\n\n----------------------------------------\n\nTITLE: Running all tests in Flax repository\nDESCRIPTION: Executes all tests in the Flax repository to ensure changes haven't broken existing functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./tests/run_all_tests.sh\n```\n\n----------------------------------------\n\nTITLE: Testing NNX Training Step in Flax\nDESCRIPTION: Tests the NNX training step function with sample inputs and labels.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/linen_to_nnx.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsample_x = jnp.ones((1, 784))\ntrain_step(state, sample_x, jnp.ones((1,), dtype=jnp.int32))\n```\n\n----------------------------------------\n\nTITLE: Applying RMSNorm in Flax\nDESCRIPTION: RMSNorm is a variant of Layer Normalization that uses the root mean square for normalization instead of mean and variance.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/normalization.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import RMSNorm\n```\n\n----------------------------------------\n\nTITLE: Documenting init Function in Flax Linen Module\nDESCRIPTION: Generates documentation for the 'init' function in the Flax Linen module. This function is probably used to initialize the parameters of a neural network module.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.linen/init_apply.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: init\n```\n\n----------------------------------------\n\nTITLE: Initializing Treescope Content Loader in JavaScript\nDESCRIPTION: Sets up the treescope container with loading state, intersection observer for lazy loading, and methods for inserting content. Supports both regular and compressed content with decompression capabilities. Creates a promise chain to manage content insertion timing.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst root = ( Array.from(document.getElementsByClassName( \"treescope_out_f88cc882b4c04e40a80bb08349fa3708\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } });\n```\n\n----------------------------------------\n\nTITLE: Starting Model Training\nDESCRIPTION: Command to launch the training process with specified working directory and batch size configuration.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --workdir=$HOME/logs/lm1b_256 \\\n    --config.per_device_batch_size=32 \\\n    --jax_backend_target=\"grpc://192.168.0.2:8470\"\n```\n\n----------------------------------------\n\nTITLE: Overriding Hyperparameters for VAE Training Configuration\nDESCRIPTION: Example showing how to override default hyperparameters when running the VAE model. This demonstrates changing the learning rate to 0.01 and setting the number of training epochs to 10.\nSOURCE: https://github.com/google/flax/blob/main/examples/vae/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython main.py \\\n--workdir=/tmp/mnist --config=configs/default.py \\\n--config.learning_rate=0.01 --config.num_epochs=10\n```\n\n----------------------------------------\n\nTITLE: Proposed initialize_carry Method Signature\nDESCRIPTION: The new signature for initialize_carry as an instance method, which accepts a key and a sample input instead of explicit shape parameters.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_carry(self, key, sample_input):\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch-like Average Pooling in Flax\nDESCRIPTION: This snippet demonstrates how to implement a PyTorch-like average pooling operation in Flax, which doesn't consider zero-padding in the average calculation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef avg_pool(inputs, window_shape, strides=None, padding='VALID'):\n    \"\"\"\n    Pools the input by taking the average over a window.\n    In comparison to nn.avg_pool(), this pooling operation does not\n    consider the padded zero's for the average computation.\n    \"\"\"\n    assert len(window_shape) == 2\n\n    y = nn.pool(inputs, 0., jax.lax.add, window_shape, strides, padding)\n    counts = nn.pool(jnp.ones_like(inputs), 0., jax.lax.add, window_shape, strides, padding)\n    y = y / counts\n    return y\n\n\nkey = random.key(0)\nx = random.normal(key, (1, 6, 6, 3))\n\nj_out = avg_pool(x, window_shape=(2, 2), strides=(1, 1), padding=((1, 1), (1, 1)))\nt_pool = torch.nn.AvgPool2d(kernel_size=2, stride=1, padding=1, count_include_pad=False)\n\n# [N, H, W, C] -> [N, C, H, W]\nt_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))\nt_out = t_pool(t_x)\n# [N, C, H, W] -> [N, H, W, C]\nt_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))\n\nnp.testing.assert_almost_equal(j_out, t_out, decimal=6)\n```\n\n----------------------------------------\n\nTITLE: Example Output with Max Gradient Accumulation\nDESCRIPTION: Shows the expected output when using fp32_max_grad type for gradient accumulation, demonstrating the correct maximum-based accumulation result.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\n1.0 [3. 0. 0. ... 0. 0. 0.]\n```\n\n----------------------------------------\n\nTITLE: Customizing GNN Training Parameters\nDESCRIPTION: Example command showing how to override default hyperparameters like number of epochs and batch size when running the training script.\nSOURCE: https://github.com/google/flax/blob/main/examples/ogbg_molpcba/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython main.py --workdir=./ogbg_molpcba --config=configs/default.py \\\n--config.num_training_epochs=10 --config.batch_size=50\n```\n\n----------------------------------------\n\nTITLE: Creating and Connecting to TPU VM\nDESCRIPTION: Commands to create a TPUv3-8 VM instance and establish SSH connection with port forwarding for TensorBoard.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nZONE=us-central1-a\nTPU_TYPE=v3-8\nTPU_NAME=$USER-flax-lm1b\ngcloud alpha compute tpus tpu-vm create $TPU_NAME \\\n    --zone $ZONE \\\n    --accelerator-type $TPU_TYPE \\\n    --version v2-alpha\ngcloud alpha compute tpus tpu-vm ssh $TPU_NAME --zone $ZONE -- \\\n    -L 6006:localhost:6006\n```\n\n----------------------------------------\n\nTITLE: Verifying Updated JAX and Flax Versions\nDESCRIPTION: Checks the installed versions of JAX and Flax packages after installation from GitHub to confirm successful update.\nSOURCE: https://github.com/google/flax/blob/main/tests/import_test.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Check versions after installing Flax from Github:\n!pip freeze | egrep 'jax|flax'\n```\n\n----------------------------------------\n\nTITLE: Installing NNX from GitHub\nDESCRIPTION: A simple command showing how to install NNX by installing Flax directly from GitHub using pip.\nSOURCE: https://github.com/google/flax/blob/main/flax/nnx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies using uv package manager\nDESCRIPTION: Uses the uv package manager to install all dependencies for Flax development.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Documentation for Flax Classes with Jinja2 Template\nDESCRIPTION: This template defines the structure for Sphinx documentation of Flax classes. It creates documentation that includes class methods while excluding inherited members, annotations, __init__, and setup methods. The template also adds a methods summary section using autosummary.\nSOURCE: https://github.com/google/flax/blob/main/docs/_templates/autosummary/flax_module.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline }}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :exclude-members:\n\n   .. automethod:: __call__\n\n   {% block methods %}\n\n   {% for item in methods %}\n   {%- if item not in inherited_members and item not in annotations and not item in ['__init__', 'setup'] %}\n   .. automethod:: {{ item }}\n   {%- endif %}\n   {%- endfor %}\n\n   {% if methods %}\n   .. rubric:: Methods\n\n   .. autosummary::\n\n   {% for item in methods %}\n   {%- if item not in inherited_members and item not in annotations and not item in ['__init__', 'setup'] %}\n       ~{{ name }}.{{ item }}\n   {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference\nDESCRIPTION: Performs inference using the trained model with a random seed for generation.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Using different random seeds generates different samples.\npreds = train.decode(state.params, inputs, jax.random.key(0), ctable)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for Flax Project with JAX and TensorFlow\nDESCRIPTION: This requirements file lists all the Python packages needed for a Flax project with specific version constraints. It includes JAX with CUDA support, Flax, TensorFlow, and various supporting libraries like numpy, optax, and tensorflow-datasets.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.4.1\njax==0.3.4\n--find-links https://storage.googleapis.com/jax-releases/jax_releases.html\njaxlib==0.3.2+cuda11.cudnn82  # Make sure CUDA version matches the base image.\nml-collections==0.1.0\nnumpy==1.22.0\noptax==0.1.0\ntensorflow==2.11.1\ntensorflow-datasets==4.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuration Example for FrozenDict/Dict Return Type\nDESCRIPTION: Example showing how to toggle between FrozenDict and regular dict return types using the flax_return_frozendict configuration flag.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.empty((1,3))\n\nflax.config.update('flax_return_frozendict', True) # set Flax to return FrozenDicts\nvariables = nn.Dense(5).init(jax.random.key(0), x)\n\nassert isinstance(variables, flax.core.FrozenDict)\n\nflax.config.update('flax_return_frozendict', False) # set Flax to return regular dicts\nvariables = nn.Dense(5).init(jax.random.key(0), x)\n\nassert isinstance(variables, dict)\n```\n\n----------------------------------------\n\nTITLE: Defining GRUCell Module Reference\nDESCRIPTION: RST documentation reference for the GRUCell implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: GRUCell\n```\n\n----------------------------------------\n\nTITLE: Referencing FLIP Label in GitHub Issues\nDESCRIPTION: This code snippet provides a markdown link to the GitHub issues page filtered for the FLIP label. It's used to direct users to the list of existing FLIP issues.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[FLIP label]: https://github.com/google/flax/issues?q=label%3AFLIP\n```\n\n----------------------------------------\n\nTITLE: Optional upload of TensorBoard data\nDESCRIPTION: Provides an option to upload training results to tensorboard.dev for sharing and visualization. This makes the training metrics publicly accessible via a shareable link.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif 'google.colab' in str(get_ipython()):\n  #@markdown You can upload the training results directly to https://tensorboard.dev\n  #@markdown\n  #@markdown Note that everbody with the link will be able to see the data.\n  upload_data = 'no' #@param ['yes', 'no']\n  if upload_data == 'yes':\n    !tensorboard dev upload --one_shot --logdir ./models --name 'Flax examples/mnist'\n```\n\n----------------------------------------\n\nTITLE: Running Local ImageNet Training\nDESCRIPTION: Command to start local training using default configuration with specified working directory.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython main.py --workdir=./imagenet --config=configs/default.py\n```\n\n----------------------------------------\n\nTITLE: Creating and Connecting to TPU VM\nDESCRIPTION: Commands to create a TPUv3-8 virtual machine and establish an SSH connection with port forwarding for TensorBoard\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nZONE=us-central1-a\nTPU_TYPE=v3-8\nTPU_NAME=$USER-flax-lm1b\ngcloud alpha compute tpus tpu-vm create $TPU_NAME \\\n    --zone $ZONE \\\n    --accelerator-type $TPU_TYPE \\\n    --version v2-alpha\ngcloud alpha compute tpus tpu-vm ssh $TPU_NAME --zone $ZONE -- \\\n    -L 6006:localhost:6006\n```\n\n----------------------------------------\n\nTITLE: Kaggle Authentication\nDESCRIPTION: Authenticates with Kaggle to access the Gemma model files.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kagglehub\nkagglehub.login()\n```\n\n----------------------------------------\n\nTITLE: Installing Flax with pip\nDESCRIPTION: Commands for installing Flax using pip, including options for installing the latest version from GitHub.\nSOURCE: https://github.com/google/flax/blob/main/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install flax\n# or to install the latest version of Flax:\npip install --upgrade git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Visualizing Array Sharding in JAX\nDESCRIPTION: Demonstrates how to visualize the sharding of arrays using JAX's debug utilities.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"sharded_model.dot1.kernel (None, 'model') :\")\njax.debug.visualize_array_sharding(sharded_model.dot1.kernel.value)\nprint(\"sharded_model.w2 ('model', None) :\")\njax.debug.visualize_array_sharding(sharded_model.w2.value)\n```\n\n----------------------------------------\n\nTITLE: Documenting flax.configurations Module with Sphinx\nDESCRIPTION: A Sphinx documentation directive for the flax.configurations module. It includes all members and undocumented members while explicitly excluding FlagHolder, bool_flag, temp_flip_flag, and static_bool_env from the documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.config.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: flax.configurations\n    :members:\n    :undoc-members:\n    :exclude-members: FlagHolder, bool_flag, temp_flip_flag, static_bool_env\n```\n\n----------------------------------------\n\nTITLE: Decoding One-Hot Encoded Data\nDESCRIPTION: Demonstrates how to decode one-hot encoded data back to human-readable text.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Note how CTABLE encodes PAD=0, EOS=1, '0'=2, '1'=3, ...\nctable.decode_onehot(batch['query'][:1])\n```\n\n----------------------------------------\n\nTITLE: Documenting LoRALinear Class in Flax NNX Module\nDESCRIPTION: This snippet uses the flax_module directive to document the LoRALinear class from the flax.nnx module. It specifies the module and class name for automatic documentation generation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/lora.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx\n  :class: LoRALinear\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Multi-Device Scaling\nDESCRIPTION: Imports required libraries including JAX, Flax NNX, NumPy and Optax for implementing multi-device model scaling.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/flax_gspmd.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import *\n\nimport numpy as np\nimport jax\nfrom jax import numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec, NamedSharding\n\nfrom flax import nnx\n\nimport optax # Optax for common losses and optimizers.\n```\n\n----------------------------------------\n\nTITLE: Defining Flax Documentation Guide Structure in reStructuredText\nDESCRIPTION: A reStructuredText toctree directive that organizes Flax documentation guides into logical sections. The maxdepth parameter is set to 2 to control the nesting level of the generated table of contents.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   flax_fundamentals/index\n   data_preprocessing/index\n   training_techniques/index\n   parallel_training/index\n   model_inspection/index\n   converting_and_upgrading/index\n   quantization/index\n   The Sharp Bits <flax_sharp_bits>\n```\n\n----------------------------------------\n\nTITLE: Documenting LoRA Class in Flax NNX Module\nDESCRIPTION: This snippet uses the flax_module directive to document the LoRA class from the flax.nnx module. It specifies the module and class name for automatic documentation generation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/lora.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx\n  :class: LoRA\n```\n\n----------------------------------------\n\nTITLE: Running the SST-2 Classification Model with Default Configuration\nDESCRIPTION: This command runs the SST-2 classification model using the default configuration file. The workdir parameter specifies where outputs will be saved.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --workdir=/tmp/sst2 --config=configs/default.py`\n```\n\n----------------------------------------\n\nTITLE: Installing the latest Flax version from GitHub\nDESCRIPTION: This snippet demonstrates how to install the latest development version of Flax directly from its GitHub repository using pip. This is useful for accessing the most recent features and updates.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Non-Inheritable kw_only Behavior Example\nDESCRIPTION: Demonstration of how kw_only behavior is not inherited by child classes, following Python's dataclass behavior.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module, kw_only=True):\n  base_muliplier: Optional[int] = -1\n\nclass ChildLayer(BaseLayer):\n  child_multiplier: int\n\nBaseLayer(2)   # This will throw error\nChildLayer(2)  # But this will not\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment with uv\nDESCRIPTION: Uses the uv package manager to set up the Flax development environment with all extras. This is an alternative to using pip for installation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras\n```\n\n----------------------------------------\n\nTITLE: Installing Flax Package\nDESCRIPTION: Installs the Flax library using pip with the quiet flag to reduce installation output.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q flax\n```\n\n----------------------------------------\n\nTITLE: Model Training Configuration\nDESCRIPTION: Sets up training parameters and initiates the training process with TensorBoard monitoring.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/sst2.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconfig.num_epochs = 10\nmodel_name = 'bilstm'\nstart_time = time.time()\noptimizer = train.train_and_evaluate(config, workdir=f'./models/{model_name}')\nlogging.info('Walltime: %f s', time.time() - start_time)\n```\n\n----------------------------------------\n\nTITLE: Defining LSTMCell Module Reference\nDESCRIPTION: RST documentation reference for the LSTMCell implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: LSTMCell\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for TPU\nDESCRIPTION: Command to install JAX with TPU support using pip.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"jax[tpu]>=0.2.16\" \\\n    -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\n----------------------------------------\n\nTITLE: Installing Flax and Dependencies in Python\nDESCRIPTION: This code snippet installs the latest version of JAXlib and Flax from the GitHub repository.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/linen_intro.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install the newest JAXlib version.\n!pip install --upgrade -q pip jax jaxlib\n# Install Flax at head:\n!pip install --upgrade -q git+https://github.com/google/flax.git\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for Flax NNX Stochastic Module in reStructuredText\nDESCRIPTION: This snippet uses reStructuredText directives to generate documentation for the Flax NNX stochastic module, specifically focusing on the Dropout class. It imports the module and sets the current module context before documenting the Dropout class and its members.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/stochastic.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autoclass:: Dropout\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Creating cross-references in FLAX documentation\nDESCRIPTION: Demonstrates different methods to create cross-references to classes, functions, and methods in the documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# First method:\n# <obj_type>:`path_to_obj`\n\n# Second method:\n# :<obj_type>:`description <path_to_obj>`\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks on Documentation\nDESCRIPTION: Stages documentation changes and runs pre-commit checks, specifically the Jupytext check, to ensure proper synchronization between .ipynb and .md files.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ngit add docs -u\npre-commit run jupytext\n```\n\n----------------------------------------\n\nTITLE: Importing flax.traceback_util Package in Python\nDESCRIPTION: This snippet shows how to import the flax.traceback_util package. It's a prerequisite for using the traceback filtering utilities provided by Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.traceback_util.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax import traceback_util\n```\n\n----------------------------------------\n\nTITLE: Comparing remat_scan() and scan(remat(...)) Functions in Flax Linen\nDESCRIPTION: This code snippet references two different ways to combine rematerialization and scan in Flax: using remat_scan() directly or nesting remat inside scan. The latter approach is recommended as it provides more flexibility with parameters like in_axes and out_axes.\nSOURCE: https://github.com/google/flax/blob/main/docs/faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nremat_scan()\n```\n\nLANGUAGE: python\nCODE:\n```\nscan(remat(...))\n```\n\n----------------------------------------\n\nTITLE: Setting up TensorBoard for training visualization\nDESCRIPTION: Loads the TensorBoard extension in Colab to provide live updates during model training. This allows for real-time monitoring of training metrics.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Get a live update during training - use the \"refresh\" button!\n# (In Jupyter[lab] start \"tensorboard\" in the local directory instead.)\nif 'google.colab' in str(get_ipython()):\n  %load_ext tensorboard\n  %tensorboard --logdir=.\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Flax Project\nDESCRIPTION: This requirements file lists all the Python packages and their specific versions needed to run a Flax-based machine learning project. It includes Flax 0.3.6, TensorFlow 2.11.1, and other supporting libraries for data processing and optimization.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.3.6\nml-collections==0.1.0\nnumpy==1.22.0\noptax==0.1.0\ntensorflow==2.11.1\ntensorflow-datasets==4.4.0\ntensorflow-text==2.7.0\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Flax for Development\nDESCRIPTION: Clones the Flax repository from the user's fork, changes to the project directory, and installs Flax with all dependencies for development, testing, and documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/flax\ncd flax\npip install -e \".[all,testing,docs]\"\n```\n\n----------------------------------------\n\nTITLE: Importing Flax and JAX Dependencies\nDESCRIPTION: Basic setup code to import JAX, JAX NumPy, and Flax's Linen API, which are necessary for working with neural networks in the Flax framework.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_sharp_bits.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Setup.\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Jax+Flax Data Loading\nDESCRIPTION: Imports the numpy and jax.numpy libraries required for data manipulation when loading datasets for Jax+Flax models.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing TFDS Dataset\nDESCRIPTION: Command to download and prepare the LM1B dataset using TensorFlow Datasets\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -m tensorflow_datasets.scripts.download_and_prepare --datasets=lm1b\n```\n\n----------------------------------------\n\nTITLE: Compiling and Installing flaxlib Package\nDESCRIPTION: Compiles the code with meson, builds a wheel package, and installs it with pip. The wheel filename will vary depending on the system configuration.\nSOURCE: https://github.com/google/flax/blob/main/flaxlib_src/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmeson compile -C builddir\npython -m build . -w\npip install dist/flaxlib-0.0.1-cp311-cp311-macosx_14_0_arm64.whl --force-reinstall\n```\n\n----------------------------------------\n\nTITLE: Setting Up Example Directory Configuration\nDESCRIPTION: Configures paths to the example directory and files that will be edited in the notebook.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexample_directory = 'examples/seq2seq'\neditor_relpaths = ('train.py', 'input_pipeline.py', 'models.py')\n\nrepo, branch = 'https://github.com/google/flax', 'main'\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Flax ML Project\nDESCRIPTION: Specifies the exact versions of Python packages required for a Flax machine learning project. Includes core dependencies like JAX with CUDA support, TensorFlow, and dataset utilities. Contains custom package index URLs for JAX releases with TPU and CUDA support.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nclu==0.0.6\nflax==0.6.5\n-f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\njax[cuda11_cudnn805]>=0.3.16  # change to jax[tpu] if running on tpus\nml-collections==0.1.0\nnumpy==1.22.0\noptax==0.1.3\ntensorflow==2.11.1\ntensorflow-datasets==4.4.0\n```\n\n----------------------------------------\n\nTITLE: Running Default GNN Training Configuration\nDESCRIPTION: Command to run the training with default configuration settings using the main.py script and specifying a working directory.\nSOURCE: https://github.com/google/flax/blob/main/examples/ogbg_molpcba/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython main.py --workdir=./ogbg_molpcba --config=configs/default.py\n```\n\n----------------------------------------\n\nTITLE: Content Loading Trigger for Treescope Component in JavaScript\nDESCRIPTION: A trigger mechanism that initiates content loading in a Treescope container. It finds the container by class name and dataset attribute, then calls the insertContent method with compressed data from a script element.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst root = ( Array.from(document.getElementsByClassName( \"treescope_out_f88cc882b4c04e40a80bb08349fa3708\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove();\n```\n\n----------------------------------------\n\nTITLE: Generating Module Documentation with Sphinx for Flax NNX in Python\nDESCRIPTION: This code snippet uses Sphinx autodoc directives to automatically generate documentation for the Module class in the flax.nnx module. It includes all members of the class in the documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/module.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autoclass:: Module\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Cross-reference examples in FLAX documentation\nDESCRIPTION: Provides specific examples of how to create references to classes, functions, and methods in the documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Create: a reference to class flax.linen.Module.\n# :class:`flax.linen.Module`\n\n# Create a reference to local function my_func.\n# :func:`my_func`\n\n# Create a reference \"Module.apply()\" to method flax.linen.Module.apply.\n# :meth:`Module.apply() <flax.linen.Module.apply>`  #\n```\n\n----------------------------------------\n\nTITLE: AutoEncoder Implementation Comparison\nDESCRIPTION: Shows differences in implementing an autoencoder between Linen and NNX, highlighting NNX's direct method calls versus Linen's apply-based approach.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/why.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Linen\nEncoder = lambda: nn.Dense(10)\nDecoder = lambda: nn.Dense(2)\n\nclass AutoEncoder(nn.Module):\n  def setup(self):\n    self.encoder = Encoder()\n    self.decoder = Decoder()\n\n  def __call__(self, x) -> jax.Array:\n    return self.decoder(self.encoder(x))\n\n  def encode(self, x) -> jax.Array:\n    return self.encoder(x)\n\n# NNX\nEncoder = lambda rngs: nnx.Linear(2, 10, rngs=rngs)\nDecoder = lambda rngs: nnx.Linear(10, 2, rngs=rngs)\n\nclass AutoEncoder(nnx.Module):\n  def __init__(self, rngs):\n    self.encoder = Encoder(rngs)\n    self.decoder = Decoder(rngs)\n\n  def __call__(self, x) -> jax.Array:\n    return self.decoder(self.encoder(x))\n\n  def encode(self, x) -> jax.Array:\n    return self.encoder(x)\n```\n\n----------------------------------------\n\nTITLE: Running the Seq2Seq Addition Training Script\nDESCRIPTION: Command to execute the sequence-to-sequence addition training script. The script trains an LSTM model that learns to add numbers, with a reported runtime of about 4 minutes for 1200 steps on a 3.5GHz Intel Core i7 CPU.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Datasets Package Version Requirement\nDESCRIPTION: Defines the minimum required version of the datasets package as 2.12.0 or higher. This requirement ensures compatibility with the Flax project's functionality.\nSOURCE: https://github.com/google/flax/blob/main/flax/nnx/scripts/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndatasets>=2.12.0\n```\n\n----------------------------------------\n\nTITLE: Cloning Flax repository and installing dependencies\nDESCRIPTION: Clones the forked Flax repository, changes to the project directory, and installs required packages using pip.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/flax\ncd flax\npip install -e \".[all,testing,docs]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents Structure in reStructuredText\nDESCRIPTION: This snippet creates a table of contents for developer notes in the Flax project using reStructuredText formatting. It includes links to important developer documentation resources with a maxdepth setting of 1.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nDeveloper notes\n===============\n\n.. toctree::\n   :maxdepth: 1\n\n   module_lifecycle\n   lift\n   FLIPs <https://github.com/google/flax/tree/main/docs/flip>\n```\n\n----------------------------------------\n\nTITLE: Importing Torchvision for Dataset Loading\nDESCRIPTION: Imports the torchvision library to access its datasets functionality for loading MNIST data.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\n```\n\n----------------------------------------\n\nTITLE: Repository Configuration\nDESCRIPTION: Sets up repository and file paths for the example\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexample_directory = 'examples/imagenet'\neditor_relpaths = ('configs/default.py', 'input_pipeline.py', 'models.py', 'train.py')\n\nrepo, branch = 'https://github.com/google/flax', 'main'\n```\n\n----------------------------------------\n\nTITLE: Loading local modules with autoreload\nDESCRIPTION: Enables automatic reloading of local modules, imports the train module and default configuration, and creates a config object. Any changes to train.py will be automatically reflected.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Local imports from current directory - auto reload.\n# Any changes you make to train.py will appear automatically.\n%load_ext autoreload\n%autoreload 2\nimport train\nfrom configs import default as config_lib\nconfig = config_lib.get_config()\n```\n\n----------------------------------------\n\nTITLE: Setting up Colab environment for Flax MNIST example\nDESCRIPTION: Configures the Colab environment by cloning the Flax repository, copying example files, and optionally mounting Google Drive for persistent storage. Changes the working directory to the example directory.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# (If you run this code in Jupyter[lab], then you're already in the\n#  example directory and nothing needs to be done.)\n\n#@markdown **Fetch newest Flax, copy example code**\n#@markdown\n#@markdown **If you select no** below, then the files will be stored on the\n#@markdown *ephemeral* Colab VM. **After some time of inactivity, this VM will\n#@markdown be restarted an any changes are lost**.\n#@markdown\n#@markdown **If you select yes** below, then you will be asked for your\n#@markdown credentials to mount your personal Google Drive. In this case, all\n#@markdown changes you make will be *persisted*, and even if you re-run the\n#@markdown Colab later on, the files will still be the same (you can of course\n#@markdown remove directories inside your Drive's `flax/` root if you want to\n#@markdown manually revert these files).\n\nif 'google.colab' in str(get_ipython()):\n  import os\n  os.chdir('/content')\n  # Download Flax repo from Github.\n  if not os.path.isdir('flaxrepo'):\n    !git clone --depth=1 -b $branch $repo flaxrepo\n  # Copy example files & change directory.\n  mount_gdrive = 'no' #@param ['yes', 'no']\n  if mount_gdrive == 'yes':\n    DISCLAIMER = 'Note : Editing in your Google Drive, changes will persist.'\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    example_root_path = f'/content/gdrive/My Drive/flax/{example_directory}'\n  else:\n    DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n    example_root_path = f'/content/{example_directory}'\n    from IPython import display\n    display.display(display.HTML(\n        f'<h1 style=\"color:red;\" class=\"blink\">{DISCLAIMER}</h1>'))\n  if not os.path.isdir(example_root_path):\n    os.makedirs(example_root_path)\n    !cp -r flaxrepo/$example_directory/* \"$example_root_path\"\n  os.chdir(example_root_path)\n  from google.colab import files\n  for relpath in editor_relpaths:\n    s = open(f'{example_root_path}/{relpath}').read()\n    open(f'{example_root_path}/{relpath}', 'w').write(\n        f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n    files.view(f'{example_root_path}/{relpath}')\n```\n\n----------------------------------------\n\nTITLE: Documenting Flax NNX Module and Tabulate Function\nDESCRIPTION: This code snippet uses Sphinx documentation directives to generate documentation for the flax.nnx module and specifically the 'tabulate' function within it. It sets up the module context and includes an autofunction directive for the tabulate function.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/summary.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autofunction:: tabulate\n```\n\n----------------------------------------\n\nTITLE: Invalid usage with inconsistent input aliases\nDESCRIPTION: An example that would be rejected due to inconsistent aliasing, where the same Module is passed to parameters with different vectorization axes.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(0, 1))\ndef f(m1: Module, m2: Module):\n  ...\n\nf(m, m)  # This should be rejected\n```\n\n----------------------------------------\n\nTITLE: Creating hyperlinks in FLAX documentation\nDESCRIPTION: Shows the syntax for creating hyperlinks to external websites in the documentation using reStructuredText.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Note the double underscore at the end:\n# `Link to Google <http://www.google.com>`__\n```\n\n----------------------------------------\n\nTITLE: Training POS Tagger Model\nDESCRIPTION: Python command to train the POS tagger model with specified batch size and dataset paths for Ancient Greek language.\nSOURCE: https://github.com/google/flax/blob/main/examples/nlp_seq/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython train.py --batch_size=64 --model_dir=./ancient_greek \\\\n    --dev=ud-treebanks-v2.0/UD_Ancient_Greek/grc-ud-dev.conllu \\\\n    --train=ud-treebanks-v2.0/UD_Ancient_Greek/grc-ud-train.conllu\n```\n\n----------------------------------------\n\nTITLE: RNNCellBase Initialize Carry Method Signature\nDESCRIPTION: Proposed new signature for the initialize_carry method as an instance method that takes a key and sample input.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/3099-rnnbase-refactor.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_carry(self, key, sample_input):\n```\n\n----------------------------------------\n\nTITLE: Replacing FrozenDict pretty_repr() with flax.core.pretty_repr\nDESCRIPTION: Shows migration from FrozenDict's pretty_repr() method to the flax.core.pretty_repr utility function.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstr_repr = variables.pretty_repr()\n```\n\nLANGUAGE: python\nCODE:\n```\nstr_repr = flax.core.pretty_repr(variables)\n```\n\n----------------------------------------\n\nTITLE: Defining RNN Module Reference\nDESCRIPTION: RST documentation reference for the RNN implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: RNN\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Flax ML Project\nDESCRIPTION: Defines the specific versions of Python packages required for a Flax machine learning project. Includes core ML frameworks (JAX, TensorFlow), utilities (absl-py, numpy), and specific CUDA-enabled versions for GPU support.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.4.0\nclu==0.0.9\nflax==0.6.11\njax==0.4.13\n--find-links https://storage.googleapis.com/jax-releases/jax_releases.html\njaxlib==0.4.13+cuda11.cudnn82  # Make sure CUDA version matches the base image.\nml-collections==0.1.1\nnumpy==1.24.3\noptax==0.1.5\nsentencepiece==0.1.99\ntensorflow==2.13.0\ntensorflow-datasets==4.9.2\ntensorflow-text==2.13.0\n```\n\n----------------------------------------\n\nTITLE: Incorrect Module Control Flow Example\nDESCRIPTION: Shows an incorrect implementation of conditional module creation that can lead to parameter sharing issues.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass WrongModule(nn.Module):\n  @nn.compact\n  def __call__(self, x, mode):\n    if mode == \"encode\":\n      return nn.Dense(features=8)(x)\n    elif mode == \"decode\":\n      return nn.Dense(features=4)(x)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Colab Environment for Flax Examples\nDESCRIPTION: Prepares the Colab environment by cloning the Flax repository and setting up the example files, with an option to mount Google Drive for persistent storage.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# (If you run this code in Jupyter[lab], then you're already in the\n#  example directory and nothing needs to be done.)\n\n#@markdown **Fetch newest Flax, copy example code**\n#@markdown\n#@markdown **If you select no** below, then the files will be stored on the\n#@markdown *ephemeral* Colab VM. **After some time of inactivity, this VM will\n#@markdown be restarted and any changes are lost**.\n#@markdown\n#@markdown **If you select yes** below, then you will be asked for your\n#@markdown credentials to mount your personal Google Drive. In this case, all\n#@markdown changes you make will be *persisted*, and even if you re-run the\n#@markdown Colab later on, the files will still be the same (you can of course\n#@markdown remove directories inside your Drive's `flax/` root if you want to\n#@markdown manually revert these files).\n\nif 'google.colab' in str(get_ipython()):\n  import os\n  os.chdir('/content')\n  # Download Flax repo from Github.\n  if not os.path.isdir('flaxrepo'):\n    !git clone --depth=1 -b $branch $repo flaxrepo\n  # Copy example files & change directory.\n  mount_gdrive = 'no' #@param ['yes', 'no']\n  if mount_gdrive == 'yes':\n    DISCLAIMER = 'Note : Editing in your Google Drive, changes will persist.'\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    example_root_path = f'/content/gdrive/My Drive/flax/{example_directory}'\n  else:\n    DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n    example_root_path = f'/content/{example_directory}'\n    from IPython import display\n    display.display(display.HTML(\n        f'<h1 style=\"color:red;\" class=\"blink\">{DISCLAIMER}</h1>'))\n  if not os.path.isdir(example_root_path):\n    os.makedirs(example_root_path)\n    !cp -r flaxrepo/$example_directory/* \"$example_root_path\"\n  os.chdir(example_root_path)\n  from google.colab import files\n  for relpath in editor_relpaths:\n    s = open(f'{example_root_path}/{relpath}').read()\n    open(f'{example_root_path}/{relpath}', 'w').write(\n        f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n    files.view(f'{example_root_path}/{relpath}')\n```\n\n----------------------------------------\n\nTITLE: GPU Setup Verification\nDESCRIPTION: Checks the available GPU by running nvidia-smi command\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/imagenet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!nvidia-smi -L\n```\n\n----------------------------------------\n\nTITLE: Documenting FrozenDict Related Functions in Python\nDESCRIPTION: Sphinx documentation directives for the freeze, unfreeze, copy, pop, and pretty_repr functions associated with the FrozenDict class.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.core.frozen_dict.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: freeze\n\n.. autofunction:: unfreeze\n\n.. autofunction:: copy\n\n.. autofunction:: pop\n\n.. autofunction:: pretty_repr\n```\n\n----------------------------------------\n\nTITLE: Colab Environment Setup and File Management\nDESCRIPTION: Sets up the Colab environment by cloning the Flax repository and handling file management with optional Google Drive integration.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/sst2.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif 'google.colab' in str(get_ipython()):\n  import os\n  os.chdir('/content')\n  if not os.path.isdir('flaxrepo'):\n    !git clone --depth=1 https://github.com/google/flax flaxrepo\n  mount_gdrive = 'no' #@param ['yes', 'no']\n  if mount_gdrive == 'yes':\n    DISCLAIMER = 'Note: Editing in your Google Drive, changes will persist.'\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    example_root_path = f'/content/gdrive/My Drive/flax/{example_directory}'\n  else:\n    DISCLAIMER = 'WARNING: Editing in VM - changes lost after reboot!!'\n    example_root_path = f'/content/{example_directory}'\n    from IPython import display\n    display.display(display.HTML(\n        f'<h1 style=\"color:red;\" class=\"blink\">{DISCLAIMER}</h1>'))\n  if not os.path.isdir(example_root_path):\n    os.makedirs(example_root_path)\n    !cp -r flaxrepo/$example_directory/* \"$example_root_path\"\n  os.chdir(example_root_path)\n  from google.colab import files\n  for relpath in editor_relpaths:\n    s = open(f'{example_root_path}/{relpath}').read()\n    open(f'{example_root_path}/{relpath}', 'w').write(\n        f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n    files.view(f'{example_root_path}/{relpath}')\n```\n\n----------------------------------------\n\nTITLE: Setting Up Auto-Reload for Local Imports\nDESCRIPTION: Configures Jupyter's autoreload extension to automatically detect changes in imported modules.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Local imports from current directory - auto reload.\n# Any changes you make to the three imported files will appear automatically.\n%load_ext autoreload\n%autoreload 2\nimport input_pipeline\nimport models\nimport train\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Flax NNX Display Function in Python\nDESCRIPTION: This snippet demonstrates how to import and use the 'display' function from the flax.nnx module. The 'display' function is used for visualization purposes in the Flax library.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/visualization.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx import display\n\n# Example usage (hypothetical)\ndisplay(model)\n```\n\n----------------------------------------\n\nTITLE: Documenting canonicalize_dtype Function in Flax NNX (Python)\nDESCRIPTION: This snippet documents the canonicalize_dtype function from the flax.nnx.nn.dtypes module. The function likely standardizes or converts input data types to a canonical form for consistent processing in neural network operations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/dtypes.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: canonicalize_dtype\n```\n\n----------------------------------------\n\nTITLE: Embedding Encoded JavaScript Data in HTML\nDESCRIPTION: This code demonstrates how to embed a binary JavaScript payload within an HTML document using a hidden div element and an application/octet-stream script type. This technique is often used to include encoded model data or large binary assets in a web page.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_10\n\nLANGUAGE: HTML\nCODE:\n```\n<div style=\"display:none\"> <script type=\"application/octet-stream\" >eNrtfet620aS6H89BaI4IW...</script></div>\n```\n\n----------------------------------------\n\nTITLE: Sample POS Tagging Format\nDESCRIPTION: Example showing the format of part-of-speech tagged text where words are separated by vertical bars and followed by their corresponding POS tags.\nSOURCE: https://github.com/google/flax/blob/main/examples/nlp_seq/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nFrom|ADP the|DT AP|PROPN comes|VBZ this|DT story|NN :|:\n```\n\n----------------------------------------\n\nTITLE: Invalid usage with inconsistent input/output aliases\nDESCRIPTION: An example showing how returning a Module with different out_axes than its in_axes would create inconsistent aliasing.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=1)\ndef g(m: Module):\n  return m\n```\n\n----------------------------------------\n\nTITLE: Setting up pre-commit hooks for Flax development\nDESCRIPTION: Installs pre-commit and sets up hooks for automated checks during git commits.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Flax Documentation in reStructuredText\nDESCRIPTION: This snippet creates a table of contents for Flax documentation guides using reStructuredText directives. It sets the maximum depth to 2 and lists various guide topics.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   transforms\n   flax_gspmd\n   filters_guide\n   randomness\n   performance\n   linen_to_nnx\n   bridge_guide\n   surgery\n   checkpointing\n   jax_and_nnx_transforms\n   haiku_to_flax\n   gemma\n```\n\n----------------------------------------\n\nTITLE: Installing and Updating Flax\nDESCRIPTION: A simple pip command to install or update the Flax library silently. The :tags: [skip-execution] indicates this cell should be skipped during notebook execution.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_sharp_bits.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install -qq flax\n```\n\n----------------------------------------\n\nTITLE: Alternative RNNCell Design Without initialize_carry\nDESCRIPTION: An alternative proposal for RNNCell redesign that removes the initialize_carry method entirely. This approach would handle the carry state as a carry collection, potentially simplifying usage but requiring changes to nn.scan.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2396-rnn.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nLSTM = nn.scan(\n  nn.LSTMCell, variable_broadcast='params', \n  split_rngs={'dropout': True})\ny = LSTM(features=32)(carry, x)\n```\n\n----------------------------------------\n\nTITLE: Checking JAX Dependencies\nDESCRIPTION: Uses pipdeptree to check for dependency conflicts with JAX package\nSOURCE: https://github.com/google/flax/blob/main/tests/colab_tpu_jax_version.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq pipdeptree\n!pipdeptree -w silence -r -p jax\n```\n\n----------------------------------------\n\nTITLE: Installing Flax and Project Dependencies\nDESCRIPTION: Commands to clone the Flax repository and install required dependencies for the LM1B example\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth=1 --branch=main https://github.com/google/flax\ncd flax\npip install -e .\ncd examples/lm1b\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: Lists required Python packages with version constraints for the Flax project. Requires matplotlib 3.7.1 or higher and datasets 2.12.0 or higher.\nSOURCE: https://github.com/google/flax/blob/main/examples/nnx_toy_examples/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmatplotlib>=3.7.1\ndatasets>=2.12.0\n```\n\n----------------------------------------\n\nTITLE: Visualizing Profile Results with Snakeviz in Shell\nDESCRIPTION: Command to visualize and analyze the profiling results using the snakeviz tool, which provides an interactive interface for exploring the performance data collected in the previous step.\nSOURCE: https://github.com/google/flax/blob/main/benchmarks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsnakeviz ~/tmp/overhead.prof\n```\n\n----------------------------------------\n\nTITLE: Importing and Using flax.nnx.bridge Module in Python\nDESCRIPTION: This snippet demonstrates how to import and use the flax.nnx.bridge module. It includes the ToNNX and ToLinen classes for module conversion, the to_linen function, and the NNXMeta class.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/bridge.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.nnx.bridge import ToNNX, ToLinen, to_linen, NNXMeta\n\n# Example usage (placeholder, actual usage may vary)\nnnx_module = ToNNX(linen_module)\nlinen_module = ToLinen(nnx_module)\nconverted_module = to_linen(some_module)\nnnx_meta_module = NNXMeta()\n```\n\n----------------------------------------\n\nTITLE: Running PPO Unit Tests\nDESCRIPTION: Command to execute the unit tests for the PPO implementation to verify correctness.\nSOURCE: https://github.com/google/flax/blob/main/examples/ppo/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_lib_test.py\n```\n\n----------------------------------------\n\nTITLE: Content Loading and Initialization Script\nDESCRIPTION: Script that handles content loading, decompression, and insertion into the treescope container. Includes loading indicator and intersection observer for deferred loading.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst root = (Array.from(document.getElementsByClassName(\"treescope_out_ac9736b84aee42a9b67a8d8e0ddbf454\")).filter((elt) => !elt.dataset.setup))[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } });\n```\n\n----------------------------------------\n\nTITLE: Pushing changes and creating a pull request branch\nDESCRIPTION: Pushes the development branch to the forked repository, setting up for pull request creation.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit push --set-upstream origin my_development_branch\n```\n\n----------------------------------------\n\nTITLE: Defining SimpleCell Module Reference\nDESCRIPTION: RST documentation reference for the SimpleCell implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: SimpleCell\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Flax PRNG\nDESCRIPTION: Imports the necessary modules from Flax, JAX, and other dependencies needed for working with PRNGs and sharding.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport flax, flax.linen as nn\nimport jax, jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec, NamedSharding\nfrom jax.experimental import mesh_utils\nfrom jax.experimental.shard_map import shard_map\n\nimport hashlib\n```\n\n----------------------------------------\n\nTITLE: Syncing Jupyter Notebooks with Markdown\nDESCRIPTION: Uses Jupytext to synchronize the contents of a Jupyter notebook with its corresponding Markdown file in the Flax documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\njupytext --sync path/to/the/file.ipynb\n```\n\n----------------------------------------\n\nTITLE: Starting the Training Process\nDESCRIPTION: Command to initiate the training process with specified working directory and batch size configuration\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --workdir=$HOME/logs/lm1b_256 \\\n    --config.per_device_batch_size=32\n```\n\n----------------------------------------\n\nTITLE: Importing TensorFlow Datasets for Data Loading\nDESCRIPTION: Imports the tensorflow_datasets library to access MNIST data for Jax+Flax model training.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/loading_datasets.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow_datasets as tfds\n```\n\n----------------------------------------\n\nTITLE: Documenting promote_dtype Function in Flax NNX (Python)\nDESCRIPTION: This snippet documents the promote_dtype function from the flax.nnx.nn.dtypes module. The function probably handles type promotion, possibly combining or converting data types to a common, higher-precision type for compatibility in neural network computations.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/dtypes.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: promote_dtype\n```\n\n----------------------------------------\n\nTITLE: Checking JAX and Flax Versions\nDESCRIPTION: Displays currently installed versions of JAX and Flax packages using pip freeze\nSOURCE: https://github.com/google/flax/blob/main/tests/colab_tpu_jax_version.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip freeze | egrep 'jax|flax'\n```\n\n----------------------------------------\n\nTITLE: Defining OptimizedLSTMCell Module Reference\nDESCRIPTION: RST documentation reference for the OptimizedLSTMCell implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: OptimizedLSTMCell\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Flax API Reference in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for the Flax API reference using reStructuredText syntax. It sets the maximum depth to 4 and lists various Flax modules and subpackages.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   flax.config\n   flax.core.frozen_dict\n   flax.cursor\n   flax.errors\n   flax.jax_utils\n   flax.linen/index\n   flax.serialization\n   flax.struct\n   flax.traceback_util\n   flax.training\n   flax.traverse_util\n```\n\n----------------------------------------\n\nTITLE: Cloning Flax Repository and Navigating to flaxlib Source\nDESCRIPTION: Clones the Flax GitHub repository and changes to the flaxlib source directory to prepare for building.\nSOURCE: https://github.com/google/flax/blob/main/flaxlib_src/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:google/flax.git\ncd flax/flaxlib_src\n```\n\n----------------------------------------\n\nTITLE: Library Imports and GPU Configuration\nDESCRIPTION: Imports required libraries and configures GPU visibility for JAX instead of TensorFlow.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/sst2.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom absl import logging\nimport flax\nimport jax.numpy as jnp\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport time\nlogging.set_verbosity(logging.INFO)\n\n# Make sure the GPU is for JAX, not for TF.\ntf.config.experimental.set_visible_devices([], 'GPU')\n```\n\n----------------------------------------\n\nTITLE: Documenting FrozenDict Class in Python\nDESCRIPTION: Sphinx documentation directive for the FrozenDict class, specifying the members to be documented including pretty_repr, copy, pop, unfreeze, and tree_flatten methods.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.core.frozen_dict.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: FrozenDict\n  :members: pretty_repr, copy, pop, unfreeze, tree_flatten\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Web Components for Treescope\nDESCRIPTION: Implements two custom web components - TreescopeContainer for managing state and RunHere for script execution. Includes shadow DOM initialization and script handling logic.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n(()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })();\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access for Flax Documentation\nDESCRIPTION: Robots.txt configuration that disallows access to deprecated API reference links and specifies the sitemap location. Prevents search engines from indexing outdated documentation while maintaining access to current content.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\n\nDisallow: /api_reference/flax.linen/_autosummary/ # for SEO, since Google still indexes this deprecated link\n\nSitemap: https://flax.readthedocs.io/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies for Flax Project\nDESCRIPTION: A requirements.txt file that specifies the exact versions of Python packages needed for a Flax project. The file includes essential libraries: absl-py for Google's Abseil Python Common Libraries, Flax neural network library itself, NumPy for numerical computing, and TensorFlow as the underlying machine learning framework.\nSOURCE: https://github.com/google/flax/blob/main/examples/nlp_seq/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==1.0.0\nflax==0.3.6\nnumpy==1.22.0\ntensorflow==2.11.1\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports the absl flags library and sets up logging for the application.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom absl import app\napp.parse_flags_with_usage(['seq2seq'])\n\nfrom absl import logging\nlogging.set_verbosity(logging.INFO)\n\nimport jax\n```\n\n----------------------------------------\n\nTITLE: Syncing Jupyter notebooks with Markdown files\nDESCRIPTION: Uses jupytext to synchronize changes between Jupyter notebook and Markdown versions of documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\njupytext --sync path/to/the/file.ipynb\n```\n\n----------------------------------------\n\nTITLE: Hyperlink Syntax Example\nDESCRIPTION: Example showing how to create hyperlinks in documentation\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Note the double underscore at the end:\n# `Link to Google <http://www.google.com>`__\n```\n\n----------------------------------------\n\nTITLE: Resolving Docker Permissions Issue in Flax Development Environment\nDESCRIPTION: Command to fix permission issues with Docker buildx by changing ownership of the Docker directory. This resolves the '~/.docker/buildx/current: permission denied' error that may occur during setup.\nSOURCE: https://github.com/google/flax/blob/main/dev/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo chown -R $(whoami) ~/.docker\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing the necessary packages for working with Flax and JAX.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --upgrade -q pip jax jaxlib flax\n```\n\n----------------------------------------\n\nTITLE: Checking current working directory\nDESCRIPTION: Displays the current working directory to verify that the environment setup has changed the directory as expected.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Note : In Colab, above cell changed the working directory.\n!pwd\n```\n\n----------------------------------------\n\nTITLE: Setting up Python virtual environment for Flax development\nDESCRIPTION: Creates a Python virtual environment and activates it for Flax development.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m virtualenv env\n. env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Starting WMT Translation Training on TPU\nDESCRIPTION: Command to start the training process for the WMT translation model with specified configuration parameters.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --workdir=$HOME/logs/wmt_256 \\\n    --config.per_device_batch_size=32 \\\n    --jax_backend_target=\"grpc://192.168.0.2:8470\"\n```\n\n----------------------------------------\n\nTITLE: Defining Bidirectional Module Reference\nDESCRIPTION: RST documentation reference for the Bidirectional RNN implementation in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. flax_module::\n  :module: flax.nnx.nn.recurrent\n  :class: Bidirectional\n```\n\n----------------------------------------\n\nTITLE: Triggering Content Decompression and Insertion in JavaScript\nDESCRIPTION: Executes the content insertion process by finding the container element and triggering the insertion of compressed content. Uses a dataset attribute to track initialization state and prevent duplicate processing.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst root = ( Array.from(document.getElementsByClassName( \"treescope_out_16a3675ef842438cb85db5d1411974db\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove();\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Quantization\nDESCRIPTION: RST configuration file that sets up the documentation structure for quantization topics, including a toctree directive that links to FP8 basics documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nQuantization\n============\n\n.. toctree::\n   :maxdepth: 1\n\n   fp8_basics\n```\n\n----------------------------------------\n\nTITLE: Setting Up Example Directory Configuration\nDESCRIPTION: Defines paths for the example directory and editor files to be used in the notebook setup.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/sst2.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexample_directory = 'examples/sst2'\neditor_relpaths = ('configs/default.py', 'train.py', 'models.py')\n```\n\n----------------------------------------\n\nTITLE: Setting up example directory and file paths\nDESCRIPTION: Defines the relative paths to the example directory and the files that will be opened in the editor. Also specifies the repository and branch for fetching the latest Flax code.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexample_directory = 'examples/mnist'\neditor_relpaths = ('configs/default.py', 'train.py')\n\nrepo, branch = 'https://github.com/google/flax', 'main'\n```\n\n----------------------------------------\n\nTITLE: Running 'make html' to build FLAX documentation with auto-refresh\nDESCRIPTION: Shows how to use the 'entr' utility to automatically rebuild documentation when files change. This is an alternative to running 'make html' directly.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfind ../ ! -regex '.*/[\\.|\\_].*' | entr -s 'make html'\n```\n\n----------------------------------------\n\nTITLE: Code Font Usage in Docstrings\nDESCRIPTION: Example showing how to format inline code in docstrings using double backticks\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# This returns a ``str`` object.\n```\n\n----------------------------------------\n\nTITLE: Flax filterlib Classes Documentation\nDESCRIPTION: Sphinx documentation definitions for Flax's filterlib module containing classes for filtering and predicates including WithTag, PathContains, OfType, Any, All, Not, Everything, and Nothing along with the to_predicate utility function.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/filterlib.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n.. autofunction:: flax.nnx.filterlib.to_predicate\n.. autoclass:: WithTag\n.. autoclass:: PathContains\n.. autoclass:: OfType\n.. autoclass:: Any\n.. autoclass:: All\n.. autoclass:: Not\n.. autoclass:: Everything\n.. autoclass:: Nothing\n```\n\n----------------------------------------\n\nTITLE: Running PPO with Custom Working Directory\nDESCRIPTION: Command to run the PPO implementation with a custom working directory for storing logs and checkpoints.\nSOURCE: https://github.com/google/flax/blob/main/examples/ppo/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_main.py --config=configs/default.py --workdir=/my_fav_directory\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing the necessary modules from JAX, Flax, and Optax for model manipulation.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/model_inspection/model_surgery.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nfrom flax import traverse_util\nfrom flax import linen as nn\nfrom flax.core import freeze\nimport jax\nimport optax\n```\n\n----------------------------------------\n\nTITLE: Creating new Jupyter notebooks with Markdown sync\nDESCRIPTION: Sets up a new Jupyter notebook with automatic synchronization to a Markdown file using jupytext.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\njupytext --set-formats ipynb,md:myst path/to/the/notebook.ipynb\n```\n\n----------------------------------------\n\nTITLE: Example Output without Max Gradient Accumulation\nDESCRIPTION: Shows the output when not using fp32_max_grad type, demonstrating how gradients are incorrectly added instead of taking the maximum.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/quantization/fp8_basics.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n2.0 [5. 0. 0. ... 0. 0. 0.]\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Parameters\nDESCRIPTION: Sets up the command-line arguments for the training process, including the number of training steps and decoding frequency.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Train 2k steps & log 20 times.\napp.parse_flags_with_usage([\n    'seq2seq',\n    '--num_train_steps=2000',\n    '--decode_frequency=100',\n])\n```\n\n----------------------------------------\n\nTITLE: Running Performance Profiling for NNX Graph Overhead in Shell\nDESCRIPTION: Command to profile the performance of NNX operations using cProfile. It runs the nnx_graph_overhead.py benchmark script in NNX mode with a specified depth and number of steps, saving the results to a profile file.\nSOURCE: https://github.com/google/flax/blob/main/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython -m cProfile -o ~/tmp/overhead.prof benchmarks/nnx_graph_overhead.py --mode=nnx --depth=100 --total_steps=1000\n```\n\n----------------------------------------\n\nTITLE: Defining ReStructuredText Documentation Tree\nDESCRIPTION: Configures the ReStructuredText table of contents tree for Flax API documentation with a maximum depth of 4 levels, including key module references.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   flax.nnx/index\n   flax.core.frozen_dict\n   flax.struct\n   flax.training\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Flax Examples in reStructuredText\nDESCRIPTION: This snippet creates a table of contents for Flax examples using reStructuredText directives. It sets the maximum depth to 2 and includes a link to core examples.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/examples/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   core_examples\n```\n\n----------------------------------------\n\nTITLE: Uploading Results to TensorBoard.dev\nDESCRIPTION: Optional step to upload training results to TensorBoard.dev for sharing and visualization.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nif 'google.colab' in str(get_ipython()):\n  #@markdown You can upload the training results directly to https://tensorboard.dev\n  #@markdown\n  #@markdown Note that everybody with the link will be able to see the data.\n  upload_data = 'yes' #@param ['yes', 'no']\n  if upload_data == 'yes':\n    !tensorboard dev upload --one_shot --logdir ./workdirs --name 'Flax examples/seq2seq (Colab)'\n```\n\n----------------------------------------\n\nTITLE: Defining Treescope Custom Web Components in JavaScript\nDESCRIPTION: Defines two custom web components: 'treescope-container' for managing content and state, and 'treescope-run-here' for executing script code within the component. This setup creates a self-contained environment for loading and rendering content.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/bridge_guide.md#2025-04-22_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n(()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })();\n```\n\n----------------------------------------\n\nTITLE: FLIP Markdown Template Structure\nDESCRIPTION: Markdown template defining the standard sections required for a Flax Improvement Proposal (FLIP) document, including summary, motivation, implementation, and discussion sections.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/0000-template.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- Start Date: (fill me in with today's date, YYYY-MM-DD)\n- FLIP PR: [#0000](https://github.com/google/flax/pull/0000)\n- FLIP Issue: [#0000](https://github.com/google/flax/issues/0000)\n\n# Summary\n[summary]: #summary\n\nOne paragraph explanation of the FLIP.\n\n# Motivation\n[motivation]: #motivation\n\nWhy are we doing this? What use cases does it support? What is the expected outcome?\n\n# Implementation\n[implementation]: #implementation\n\nThe technical part.\n\n# Discussion\n[discussion]: #discussion\n\nSummarize the discussion from the original issue and from the pull request.\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes and Creating a Pull Request\nDESCRIPTION: Pushes the local development branch to the forked repository on GitHub, setting up the branch for creating a pull request.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit push --set-upstream origin my_development_branch\n```\n\n----------------------------------------\n\nTITLE: Creating Working Directory for Training Results\nDESCRIPTION: Sets up a timestamped working directory to store training outputs.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport time\nworkdir = f'./workdirs/{int(time.time())}'\n```\n\n----------------------------------------\n\nTITLE: Training Output Log Example\nDESCRIPTION: Example console output showing training and evaluation metrics for epoch 10, including loss and accuracy values.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nI0828 08:51:41.821526 139971964110656 train.py:130] train epoch: 10, loss: 0.0097, accuracy: 99.69\nI0828 08:51:42.248714 139971964110656 train.py:180] eval epoch: 10, loss: 0.0299, accuracy: 99.14\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Documentation Template with Jinja2\nDESCRIPTION: A Jinja2 template that creates Sphinx documentation for a Flax class. It includes the class definition, __call__ method, and other methods while excluding inherited members, annotations, __init__ and setup methods. The template generates both detailed method documentation and a summary table.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/_templates/autosummary/flax_module.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline }}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :exclude-members:\n\n   .. automethod:: __call__\n\n   {% block methods %}\n\n   {% for item in methods %}\n   {%- if item not in inherited_members and item not in annotations and not item in ['__init__', 'setup'] %}\n   .. automethod:: {{ item }}\n   {%- endif %}\n   {%- endfor %}\n\n   {% if methods %}\n   .. rubric:: Methods\n\n   .. autosummary::\n\n   {% for item in methods %}\n   {%- if item not in inherited_members and item not in annotations and not item in ['__init__', 'setup'] %}\n       ~{{ name }}.{{ item }}\n   {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for Training Documentation\nDESCRIPTION: RST directive that creates a table of contents for training-related documentation pages, with a maximum depth of 1 level.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   batch_norm\n   dropout\n   lr_schedule\n   transfer_learning\n   use_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Using code font in FLAX documentation\nDESCRIPTION: Shows how to format code elements within docstrings using double backticks.\nSOURCE: https://github.com/google/flax/blob/main/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# This returns a ``str`` object.\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard\nDESCRIPTION: Command to start TensorBoard for monitoring training progress.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir=$HOME/logs\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Elements for Treescope Container and Runner in JavaScript\nDESCRIPTION: Creates two custom elements: 'treescope-container' to manage state and content, and 'treescope-run-here' which executes scripts within itself. The implementation includes shadow DOM attachment and script execution handling.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n(()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })();\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for State Class in Flax NNX Module\nDESCRIPTION: This code snippet uses Sphinx directives to automatically generate documentation for the State class in the flax.nnx module. It sets the current module and then uses autoclass to document the State class and its members.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/state.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: flax.nnx\n.. currentmodule:: flax.nnx\n\n\n.. autoclass:: State\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Installing Flax and Dependencies\nDESCRIPTION: Commands to clone the Flax repository and install required dependencies for the lm1b example.\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth=1 --branch=main https://github.com/google/flax\ncd flax\npip install -e .\ncd examples/lm1b\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding or updating dependencies using uv\nDESCRIPTION: Uses the uv package manager to add or update dependencies in the Flax project.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras\n```\n\n----------------------------------------\n\nTITLE: Setting Up TPUv3-8 VM on Google Cloud\nDESCRIPTION: Commands to create a TPUv3-8 VM on Google Cloud, connect to it via SSH, and forward port 6006 for TensorBoard visualization.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nZONE=us-central1-a\nTPU_TYPE=v3-8\nVM_NAME=wmt\n\ngcloud alpha compute tpus tpu-vm create $VM_NAME \\\n    --zone $ZONE \\\n    --accelerator-type $TPU_TYPE \\\n    --version v2-alpha\n\ngcloud alpha compute tpus tpu-vm ssh $VM_NAME --zone $ZONE -- \\\n    -L 6006:localhost:6006\n```\n\n----------------------------------------\n\nTITLE: Deleting Google Cloud TPU VM After Completion\nDESCRIPTION: Command to delete the TPU VM after the training is complete to avoid unnecessary charges.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngcloud alpha compute tpus tpu-vm delete $VM_NAME \\\n  --zone $ZONE\n```\n\n----------------------------------------\n\nTITLE: Installing jupytext for notebook synchronization\nDESCRIPTION: Installs a specific version of jupytext for synchronizing Jupyter notebooks and Markdown files.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install jupytext==1.13.8\n```\n\n----------------------------------------\n\nTITLE: Adding upstream remote for Flax repository\nDESCRIPTION: Adds the main Google Flax repository as an upstream remote for syncing changes.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream http://www.github.com/google/flax\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with TPU Support\nDESCRIPTION: Command to install JAX with TPU support using pip.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \"jax[tpu]>=0.2.21\" \\\n    -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\n----------------------------------------\n\nTITLE: Adding a specific package version with uv\nDESCRIPTION: Demonstrates how to add a specific package version using the uv package manager.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nuv add 'some-package>=1.2.3'\n```\n\n----------------------------------------\n\nTITLE: Optional Colab Authentication\nDESCRIPTION: Alternative authentication method for Google Colab using stored credentials.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom google.colab import userdata\n\nos.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\nos.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n```\n\n----------------------------------------\n\nTITLE: Using dataclass Function from flax.struct in Python\nDESCRIPTION: This snippet demonstrates how to use the dataclass function from flax.struct. It is likely used for creating data classes with specific behaviors in Flax.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.struct.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@struct.dataclass\nclass MyClass:\n    # class definition\n```\n\n----------------------------------------\n\nTITLE: Running TensorBoard for Monitoring Training\nDESCRIPTION: Command to start TensorBoard for monitoring the training progress.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir=$HOME/logs\n```\n\n----------------------------------------\n\nTITLE: Linking to Flax Contribution Guidelines in Markdown\nDESCRIPTION: This snippet provides a Markdown link to the official Flax contribution guidelines. It directs potential contributors to the detailed instructions hosted on the project's documentation site.\nSOURCE: https://github.com/google/flax/blob/main/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# How to Contribute\n\nPlease see https://flax.readthedocs.io/en/latest/contributing.html for more information.\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Base and Child Layer Class Structure\nDESCRIPTION: Example showing the current limitation where child classes must provide default values for arguments when the parent class has defaults. Shows how kw_only would solve this issue.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseLayer(nn.Module):\n  mesh: Optional[jax.experimental.mesh.Mesh] = None\n\n  def with_sharding(self, some_variable, some_sharding):\n    if self.mesh:\n      # Do something useful here.\n\nclass Child(BaseLayer):\n  num_heads: int  # Don't want to have to set a default argument!\n\n  def __call__(self, x):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running the VAE Model on MNIST\nDESCRIPTION: Commands to install required dependencies and execute the VAE model on the MNIST dataset with default configurations. The workdir parameter specifies where outputs will be saved.\nSOURCE: https://github.com/google/flax/blob/main/examples/vae/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py --workdir=/tmp/mnist --config=configs/default.py\n```\n\n----------------------------------------\n\nTITLE: Running the SST-2 Classification Model with Custom Hyperparameters\nDESCRIPTION: This command demonstrates how to override specific hyperparameters when running the model. It sets a custom learning rate and number of epochs while using the default configuration as a base.\nSOURCE: https://github.com/google/flax/blob/main/examples/sst2/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py \\\n    --workdir=/tmp/sst2 --config=configs/default.py \\\n    --config.learning_rate=0.05 --config.num_epochs=5\n```\n\n----------------------------------------\n\nTITLE: Committing Changes and Syncing with Main Repository\nDESCRIPTION: Demonstrates how to commit changes, fetch updates from the upstream repository, and rebase the local branch onto the latest main branch.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit add file1.py file2.py ...\ngit commit -m \"Your commit message\"\ngit fetch upstream\ngit rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Documenting flip_sequences Function\nDESCRIPTION: RST documentation reference for the flip_sequences utility function in Flax's recurrent neural network package.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.nnx/nn/recurrent.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: flip_sequences\n```\n\n----------------------------------------\n\nTITLE: Installing JAX TPU Dependencies\nDESCRIPTION: Command to install JAX with TPU support using pip\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"jax[tpu]>=0.2.16\" \\\n    -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\n----------------------------------------\n\nTITLE: Preparing ImageNet Dataset with TensorFlow Datasets\nDESCRIPTION: Python script to download and prepare the ImageNet2012 dataset using tensorflow_datasets. This needs to be run before training the ImageNet example on Google Cloud.\nSOURCE: https://github.com/google/flax/blob/main/examples/cloud/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -c \"\nimport tensorflow_datasets as tfds\ntfds.builder('imagenet2012').download_and_prepare(\n    download_config=tfds.download.DownloadConfig(\n        manual_dir='$IMAGENET_DOWNLOAD_PATH'))\n\"\n```\n\n----------------------------------------\n\nTITLE: Listing Available JAX Devices\nDESCRIPTION: Displays all JAX devices available in the current environment, useful to verify the multi-device setup.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/rng_guide.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njax.devices()\n```\n\n----------------------------------------\n\nTITLE: Downloading Universal Dependency Dataset\nDESCRIPTION: Commands to download and extract the Universal Dependency treebank dataset version 2.0 using curl and tar.\nSOURCE: https://github.com/google/flax/blob/main/examples/nlp_seq/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -# -o ud-treebanks-v2.0.tgz https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1976/ud-treebanks-v2.0.tgz\\ntar xzf ud-treebanks-v2.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for Flax MNIST example\nDESCRIPTION: Installs the required packages ml-collections and the latest version of Flax directly from GitHub.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/mnist.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install ml-collections & latest Flax version from Github.\n!pip install -q ml-collections git+https://github.com/google/flax\n```\n\n----------------------------------------\n\nTITLE: Showing Flax in Tracebacks using Python\nDESCRIPTION: This function is used to show Flax-related entries in tracebacks. It's useful for debugging Flax internals or when you need to see the full traceback including Flax implementation details.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.traceback_util.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntraceback_util.show_flax_in_tracebacks()\n```\n\n----------------------------------------\n\nTITLE: Installing Flax and WMT Example Dependencies\nDESCRIPTION: Commands to clone the Flax repository, install Flax and the required dependencies for the WMT translation example.\nSOURCE: https://github.com/google/flax/blob/main/examples/wmt/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth=1 --branch=main https://github.com/google/flax\ncd flax\npip install -e .\ncd examples/wmt\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing flax.struct Package in Python\nDESCRIPTION: This snippet shows how to import the flax.struct package. It is used to access the dataclass function and PyTreeNode class.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.struct.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import struct\n```\n\n----------------------------------------\n\nTITLE: Installing Flax Library - Python\nDESCRIPTION: Installs the Flax neural network library version 0.7.5 or higher using pip.\nSOURCE: https://github.com/google/flax/blob/main/docs/quick_start.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q flax>=0.7.5\n```\n\n----------------------------------------\n\nTITLE: Basic MNIST Training Command\nDESCRIPTION: Shell command to run the MNIST training script with default configuration.\nSOURCE: https://github.com/google/flax/blob/main/examples/mnist/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython main.py --workdir=/tmp/mnist --config=configs/default.py\n```\n\n----------------------------------------\n\nTITLE: Implementing dtype promotion in Flax Dense layer\nDESCRIPTION: This snippet demonstrates how to implement the proposed dtype promotion in a Flax Dense layer. It uses JAX's result_type function to determine the output dtype based on inputs and parameters, and promotes all arrays to this dtype before computation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1777-default-dtype.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef promote_arrays(*xs, dtype):\n if dtype is None:\n   dtype = jnp.result_type(*jax.tree_util.tree_leaves(xs))\n return jax.tree_util.tree_map(lambda x: jnp.asarray(x, dtype), xs)\n\nDtype = Any\nclass Dense(nn.Module):\n features: int\n kernel_init: Callable\n bias_init: Callable\n dtype: Optional[Dtype] = None\n param_dtype: Dtype = jnp.float32\n\n @nn.compact\n def __call__(self, x):\n   kernel = self.param(\"kernel\",\n                       self.kernel_init,\n                       (x.shape[-1], self.features), self.param_dtype)\n   bias = self.param(\"bias\", self.bias_init, (self.features,), self.param_dtype)\n   x, kernel, bias = promote_arrays(x, kernel, bias, dtype=self.dtype)\n   return x @ kernel + bias\n```\n\n----------------------------------------\n\nTITLE: Checking Current Working Directory\nDESCRIPTION: Displays the current working directory to confirm the environment setup.\nSOURCE: https://github.com/google/flax/blob/main/examples/seq2seq/seq2seq.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Note : In Colab, above cell changed the working directory.\n!pwd\n```\n\n----------------------------------------\n\nTITLE: Installing Flax Package\nDESCRIPTION: Installation command for the Flax package using pip package manager.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/mnist_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install flax\n```\n\n----------------------------------------\n\nTITLE: Installing Jupytext for Notebook Syncing\nDESCRIPTION: Installs a specific version of Jupytext, which is used to sync Jupyter notebooks with their Markdown counterparts in the Flax documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install jupytext==1.13.8\n```\n\n----------------------------------------\n\nTITLE: Creating a Table of Contents for Data Preprocessing in reStructuredText\nDESCRIPTION: This RST code creates a table of contents for data preprocessing documentation in Flax. It sets up a maxdepth of 1 and includes links to full evaluation and dataset loading documentation pages.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/data_preprocessing/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   full_eval\n   loading_datasets\n```\n\n----------------------------------------\n\nTITLE: Adding or Updating Dependencies with uv\nDESCRIPTION: Uses the uv package manager to add or update dependencies in the Flax project, ensuring the uv.lock file is up-to-date.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit checks on documentation changes\nDESCRIPTION: Stages documentation changes and runs pre-commit checks to ensure proper synchronization.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ngit add docs -u\npre-commit run jupytext\n```\n\n----------------------------------------\n\nTITLE: Performing Partial Initialization with LoRA Parameters in Flax\nDESCRIPTION: This code demonstrates using partial_init to initialize a new model using an old state, while creating LoRA parameters. It also prints the number of JAX arrays in memory after the operation, highlighting that two new arrays (lora_a and lora_b) are created.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/surgery.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Note that `old_state` will be deleted after this `partial_init` call.\ngood_model = partial_init(old_state, nnx.Rngs(42))\nprint(f'Number of JAX Arrays in memory at end: {len(jax.live_arrays())}'\n      ' (2 new created - lora_a and lora_b)')\n```\n\n----------------------------------------\n\nTITLE: Creating a development branch in Flax repository\nDESCRIPTION: Creates and checks out a new branch for development work in the Flax repository.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b my_development_branch\n```\n\n----------------------------------------\n\nTITLE: Running All Flax Tests\nDESCRIPTION: Executes all tests for the Flax project using a shell script. This ensures that changes haven't broken existing functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./tests/run_all_tests.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents in reStructuredText for Flax Documentation\nDESCRIPTION: This snippet defines a table of contents using reStructuredText syntax for Flax fundamentals documentation. The toctree directive with maxdepth=1 creates a single-level list of links to essential Flax learning resources.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/flax_fundamentals/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   JAX 101 <https://jax.readthedocs.io/en/latest/jax-101/index.html>\n   flax_basics\n   state_params\n   setup_or_nncompact\n   arguments\n   rng_guide\n```\n\n----------------------------------------\n\nTITLE: Adding a Specific Package Version with uv\nDESCRIPTION: Demonstrates how to add a specific package version to the Flax project using uv.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nuv add 'some-package>=1.2.3'\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Checkpointing with Orbax AsyncCheckpointer in Python\nDESCRIPTION: Shows how to implement asynchronous checkpointing using Orbax AsyncCheckpointer. It includes saving a checkpoint asynchronously and using wait_until_finished() to ensure the save operation completes before restoring.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nASYNC_CKPT_DIR = '/tmp/orbax_upgrade/async'\n\nimport orbax.checkpoint as ocp\nckptr = ocp.AsyncCheckpointer(ocp.StandardCheckpointHandler())\nckptr.save(ASYNC_CKPT_DIR, args=ocp.args.StandardSave(CKPT_PYTREE))\n# ... Continue with your work...\n# ... Until a time when you want to wait until the save completes:\nckptr.wait_until_finished() # Blocks until the checkpoint saving is completed.\nckptr.restore(ASYNC_CKPT_DIR, args=ocp.args.StandardRestore(TARGET_PYTREE))\n```\n\n----------------------------------------\n\nTITLE: Inconsistent Input Aliases in NNX Transforms (Invalid)\nDESCRIPTION: Illustrates an example of inconsistent input aliases that should be rejected in NNX transforms.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(0, 1))\ndef f(m1: Module, m2: Module):\n  ...\n\nf(m, m)  # This should be rejected\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for flaxlib C++ Extension Module\nDESCRIPTION: Complete CMake configuration for building the flaxlib C++ extension module. It sets minimum CMake version requirements, configures the project, imports Python components and nanobind, and sets up the module compilation and installation process. The file includes helpful warning messages for users attempting to run CMake directly instead of through scikit-build-core.\nSOURCE: https://github.com/google/flax/blob/main/flaxlib_src/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Set the minimum CMake version and policies for highest tested version\ncmake_minimum_required(VERSION 3.15...3.27)\n\n# Set up the project and ensure there is a working C++ compiler\nproject(flaxlib LANGUAGES CXX)\n\n# Warn if the user invokes CMake directly\nif (NOT SKBUILD)\n  message(WARNING \"\\\n  This CMake file is meant to be executed using 'scikit-build-core'.\n  Running it directly will almost certainly not produce the desired\n  result. If you are a user trying to install this package, use the\n  command below, which will install all necessary build dependencies,\n  compile the package in an isolated environment, and then install it.\n  =====================================================================\n   $ pip install .\n  =====================================================================\n  If you are a software developer, and this is your own package, then\n  it is usually much more efficient to install the build dependencies\n  in your environment once and use the following command that avoids\n  a costly creation of a new virtual environment at every compilation:\n  =====================================================================\n   $ pip install nanobind scikit-build-core[pyproject]\n   $ pip install --no-build-isolation -ve .\n  =====================================================================\n  You may optionally add -Ceditable.rebuild=true to auto-rebuild when\n  the package is imported. Otherwise, you need to rerun the above\n  after editing C++ files.\")\nendif()\n\n# Try to import all Python components potentially needed by nanobind\nfind_package(Python 3.8\n  REQUIRED COMPONENTS Interpreter Development.Module\n  OPTIONAL_COMPONENTS Development.SABIModule)\n\n# Import nanobind through CMake's find_package mechanism\nfind_package(nanobind CONFIG REQUIRED)\n\n# We are now ready to compile the actual extension module\nnanobind_add_module(\n  # Name of the extension\n  flaxlib_cpp\n\n  # Target the stable ABI for Python 3.12+, which reduces\n  # the number of binary wheels that must be built. This\n  # does nothing on older Python versions\n  STABLE_ABI\n\n  # Source code goes here\n  src/lib.cc\n)\n\n# Install directive for scikit-build-core\ninstall(TARGETS flaxlib_cpp LIBRARY DESTINATION flaxlib)\n```\n\n----------------------------------------\n\nTITLE: Copying TensorFlow Datasets to GCS\nDESCRIPTION: Command to copy TensorFlow datasets to Google Cloud Storage bucket.\nSOURCE: https://github.com/google/flax/blob/main/examples/imagenet/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngsutil cp -r ~/tensorflow_datasets gs://$GCS_TFDS_BUCKET/datasets\n```\n\n----------------------------------------\n\nTITLE: Configuring the Build Environment for flaxlib\nDESCRIPTION: Creates a subprojects directory, installs required dependencies (robin-map and nanobind) using meson wrap, and sets up the build directory.\nSOURCE: https://github.com/google/flax/blob/main/flaxlib_src/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p subprojects\nmeson wrap install robin-map\nmeson wrap install nanobind\nmeson setup builddir\n```\n\n----------------------------------------\n\nTITLE: Defining MLP Model with and without BatchNorm in Flax\nDESCRIPTION: Demonstrates two implementations of an MLP model - one basic version and another with BatchNorm layer. The BatchNorm version includes a train parameter to control running averages during training and inference.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/batch_norm.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  @nn.compact\n  def __call__(self, x, train: bool):\n    x = nn.Dense(features=4)(x)\n    x = nn.BatchNorm(use_running_average=not train)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=1)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Building flaxlib\nDESCRIPTION: Installs the necessary Python packages required to build the C++ based flaxlib package, including meson-python, ninja, and build.\nSOURCE: https://github.com/google/flax/blob/main/flaxlib_src/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install meson-python ninja build\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation with Mutable State\nDESCRIPTION: Implementation of a training step function that handles mutable model state, computes gradients and updates the training state.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/1009-optimizer-api.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(state, inputs, labels):\n\n  def loss_fn(params):\n    outputs, new_model_state = state.apply_fn(\n        {'params': params, 'batch_stats': state.batch_stats},\n        inputs,\n        mutable=['batch_stats'])\n    loss = xent_loss(outputs, labels)\n    return loss, new_model_state\n\n  (loss, new_model_state), grads = jax.value_and_grad(\n      loss_fn, has_aux=True)(state.params)\n  new_state = state.apply_gradients(\n      grads=grads,\n      batch_stats=new_model_state['batch_stats'],\n  )\n\n  return new_state, loss\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Optimizer Training\nDESCRIPTION: Configures different optimizers for trainable and frozen parameters using Optax's multi_transform functionality.\nSOURCE: https://github.com/google/flax/blob/main/docs/guides/training_techniques/transfer_learning.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import traverse_util\nimport optax\n\npartition_optimizers = {'trainable': optax.adam(5e-3), 'frozen': optax.set_to_zero()}\nparam_partitions = traverse_util.path_aware_map(\n  lambda path, v: 'frozen' if 'backbone' in path else 'trainable', params)\ntx = optax.multi_transform(partition_optimizers, param_partitions)\n\nflat = list(traverse_util.flatten_dict(param_partitions).items())\ntraverse_util.unflatten_dict(dict(flat[:2] + flat[-2:]))\n```\n\n----------------------------------------\n\nTITLE: Importing and Using flax.struct Package in Python\nDESCRIPTION: This snippet demonstrates how to import and use components from the flax.struct package. It includes the dataclass function and PyTreeNode class, which are key elements for structuring data in Flax applications.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/api_reference/flax.struct.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom flax.struct import dataclass, PyTreeNode\n\n# Example usage:\n@dataclass\nclass MyModel(PyTreeNode):\n    # Define your model structure here\n    pass\n```\n\n----------------------------------------\n\nTITLE: Launching MNIST Training on Google Cloud\nDESCRIPTION: Shell command to launch MNIST example training on a Google Cloud VM using the launch_gce.py script. Configures a basic n2-standard-2 machine type for training.\nSOURCE: https://github.com/google/flax/blob/main/examples/cloud/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython examples/cloud/launch_gce.py \\\n  --project=$PROJECT \\\n  --zone=us-west1-a \\\n  --machine_type=n2-standard-2 \\\n  --gcs_workdir_base=gs://$GCS_BUCKET/workdir_base \\\n  --repo=${REPO:-https://github.com/google/flax} \\\n  --branch=${BRANCH:-main} \\\n  --example=mnist \\\n  --args='--config=configs/default.py' \\\n  --name=default\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Remote for Flax Repository\nDESCRIPTION: Adds the main Flax repository as an upstream remote in Git. This allows syncing the fork with the main repository.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream http://www.github.com/google/flax\n```\n\n----------------------------------------\n\nTITLE: Current approach for selective Module state vectorization with custom filters\nDESCRIPTION: The current complex approach required to vectorize different modules with different axes, using custom filters based on path indices.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nselect_m1 = lambda path, value: path[0] == 0\nselect_m2 = lambda path, value: path[0] == 1\n\n# To select modules individually, you must create a filter (which can be tricky)\n@nnx.vmap(state_axes={select_m1: 1, select_m2: 0})\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Basic MLP Module in Flax\nDESCRIPTION: Demonstrates creating a basic Multi-Layer Perceptron Module using Flax Linen. Shows attribute annotations, setup method, and user methods pattern.\nSOURCE: https://github.com/google/flax/blob/main/docs/developer_notes/module_lifecycle.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass MLP(nn.Module):\n  # 1. Attribute annotations\n  hidden_size: int\n  out_size: int\n\n  # 2. The ``setup`` method\n  def setup(self):\n    self.hidden = nn.Dense(self.hidden_size)\n    self.out = nn.Dense(self.out_size)\n\n  # 3. User methods\n  def __call__(self, x):\n    a = self.hidden(x)\n    h = nn.relu(a)\n    return self.out(h)\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard Monitoring\nDESCRIPTION: Command to start TensorBoard for monitoring training progress\nSOURCE: https://github.com/google/flax/blob/main/examples/lm1b_nnx/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir=$HOME/logs\n```\n\n----------------------------------------\n\nTITLE: Squashing Commits for Pull Request in Bash\nDESCRIPTION: Command to squash multiple commits by rebasing to main branch and creating a single new commit. Used when there are too many commits in a PR that could cause Flax docs build process failures.\nSOURCE: https://github.com/google/flax/blob/main/docs/contributing.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngit rebase main && git reset --soft main && git commit\n```\n\n----------------------------------------\n\nTITLE: Module __init_subclass__ Implementation\nDESCRIPTION: Implementation details for the Module.__init_subclass__ method to handle kw_only parameter and dataclass transformation.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/2974-kw-only-dataclasses.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Module(ModuleBase):\n  def __init_subclass__(self, kw_only: Optional[bool] = None):\n    # ...\n    if kw_only:\n     if is_python_310_or_above():\n       dataclass_transform_args = {'kw_only': True}\n     else:\n       raise TypeError(\"Can't use `kw_only` before Py3.10.\")\n    else:\n       dataclass_transform_args = {}\n\n    kw_only_dataclasses.dataclass(\n      cls, unsafe_hash='__hash__' not in cls.__dict__,\n      repr=False,\n      **dataclass_transform_args)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Flax Dependencies\nDESCRIPTION: Uses pipgrip to analyze dependency tree for specific Flax version without installation\nSOURCE: https://github.com/google/flax/blob/main/tests/colab_tpu_jax_version.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qq pipgrip\n!pipgrip --tree flax==0.6.4\n```\n\n----------------------------------------\n\nTITLE: Comparing Parameter Tree Structure in Haiku and Flax NNX\nDESCRIPTION: Demonstration of how parameter trees are structured in Haiku and Flax NNX. Both show similar information but with different nesting and naming conventions.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/haiku_to_flax.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n{\n    'mlp/__layer_stack_no_per_layer/block/linear': {\n        'b': (5, 64),\n        'w': (5, 64, 64)\n    }\n}\n```\n\nLANGUAGE: python\nCODE:\n```\n_, params, _ = nnx.split(model, nnx.Param, ...)\n\nparams\n{\n  'blocks': {\n    'linear': {\n      'bias': VariableState(type=Param, value=(5, 64)),\n      'kernel': VariableState(type=Param, value=(5, 64, 64))\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Actual behavior of current NNX vmap implementation\nDESCRIPTION: Explanation of how the current NNX vmap actually behaves, using IGNORE for Module inputs and controlling state vectorization through state_axes.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# this is what is really happening\n@nnx.vmap(in_axes=(IGNORE, IGNORE), state_axes={BatchStat: None, ...: 0})\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Disabling Orbax Checkpointing Migration\nDESCRIPTION: Code snippet showing how to temporarily disable the automatic migration to Orbax checkpointing in Flax.\nSOURCE: https://github.com/google/flax/blob/main/CHANGELOG.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nflax.config.update('flax_use_orbax_checkpointing', False)\n```\n\n----------------------------------------\n\nTITLE: Installing Flax via pip\nDESCRIPTION: This snippet shows how to install Flax using pip, the Python package installer. It provides a simple command to install the stable version of Flax from PyPI.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install flax\n```\n\n----------------------------------------\n\nTITLE: Model Download Configuration\nDESCRIPTION: Downloads the specified Gemma model variant and sets up paths for weights and tokenizer.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/gemma.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import clear_output\n\nVARIANT = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\nweights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\nckpt_path = f'{weights_dir}/{VARIANT}'\nvocab_path = f'{weights_dir}/tokenizer.model'\n```\n\n----------------------------------------\n\nTITLE: Hiding Flax in Tracebacks using Python\nDESCRIPTION: This function is used to hide Flax-related entries in tracebacks. It helps in simplifying error messages by removing Flax implementation details from the traceback.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.traceback_util.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntraceback_util.hide_flax_in_tracebacks()\n```\n\n----------------------------------------\n\nTITLE: Using Proposed RNN Layer with LSTMCell in Flax\nDESCRIPTION: Example of using the proposed RNN layer with an LSTMCell to process a batch of input sequences. This demonstrates the simplified API for recurrent layers.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/flip/2396-rnn.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncell = nn.LSTMCell()\n# Encodes a batch of input sequences.\ncarry, outputs = nn.RNN(cell, cell_size)(inputs, seq_lengths)\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access Rules in robots.txt\nDESCRIPTION: Defines access rules for web crawlers, blocking deprecated API reference paths and specifying the sitemap location for Flax documentation.\nSOURCE: https://github.com/google/flax/blob/main/docs/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robotstxt\nCODE:\n```\nUser-agent: *\n\nDisallow: /api_reference/flax.linen/_autosummary/ # for SEO, since Google still indexes this deprecated link\n\nSitemap: https://flax.readthedocs.io/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Proposed JAX-like NNX vmap syntax\nDESCRIPTION: The proposed improved syntax that would allow JAX-style in_axes to work intuitively with NNX Modules, behaving as if Modules were PyTrees.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=(1, 0))\ndef f(m1: Module, m2: Module):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Invalid nested structure with inconsistent aliases\nDESCRIPTION: Another example of inconsistent aliasing when a Module is contained within another structure and returned with different axes.\nSOURCE: https://github.com/google/flax/blob/main/docs/flip/4105-jax-style-nnx-transforms.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@nnx.vmap(in_axes=0, out_axes=1)\ndef f(m: Module):\n  return SomeModule(m)\n```\n\n----------------------------------------\n\nTITLE: Checking JAX and Flax Versions in Colab\nDESCRIPTION: Displays the currently installed versions of JAX and Flax packages in the Colab environment using pip freeze with grep filtering.\nSOURCE: https://github.com/google/flax/blob/main/tests/import_test.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Colab runtimes are pre-built with JAX/Flax:\n!pip freeze | egrep 'jax|flax'\n```\n\n----------------------------------------\n\nTITLE: Creating New Synced Jupyter Notebooks\nDESCRIPTION: Sets up a new Jupyter notebook with Jupytext, creating both .ipynb and .md versions that can be synced.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/contributing.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\njupytext --set-formats ipynb,md:myst path/to/the/notebook.ipynb\n```\n\n----------------------------------------\n\nTITLE: Importing NNX Dependencies\nDESCRIPTION: Basic imports required for using NNX with JAX and Flax frameworks.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import numpy as jnp\nfrom flax import nnx\n```\n\n----------------------------------------\n\nTITLE: Importing Flax Variable Modules - RST\nDESCRIPTION: RST directives for auto-documenting the flax.core.variables module and flax.linen.Variable class.\nSOURCE: https://github.com/google/flax/blob/main/docs/api_reference/flax.linen/variable.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: flax.core.variables\n.. autoclass:: flax.linen.Variable\n```\n\n----------------------------------------\n\nTITLE: Initializing Treescope Container and Loading Functionality in JavaScript\nDESCRIPTION: Sets up a treescope container with loading state, intersection observer for deferred loading, and content insertion functions. Handles both compressed and uncompressed content, with special handling for script elements to ensure proper execution.\nSOURCE: https://github.com/google/flax/blob/main/docs_nnx/guides/checkpointing.md#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst root = ( Array.from(document.getElementsByClassName( \"treescope_out_16a3675ef842438cb85db5d1411974db\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } });\n```"
  }
]
[
  {
    "owner": "tinygrad",
    "repo": "tinygrad",
    "content": "TITLE: Simple Neural Network with tinygrad (Python)\nDESCRIPTION: This Python code defines a simple linear neural network using tinygrad. It includes a `LinearNet` class with two linear layers and an Adam optimizer. It demonstrates a training loop with forward and backward passes using random input data and sparse categorical cross-entropy loss.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Tensor, nn\n\nclass LinearNet:\n  def __init__(self):\n    self.l1 = Tensor.kaiming_uniform(784, 128)\n    self.l2 = Tensor.kaiming_uniform(128, 10)\n  def __call__(self, x:Tensor) -> Tensor:\n    return x.flatten(1).dot(self.l1).relu().dot(self.l2)\n\nmodel = LinearNet()\noptim = nn.optim.Adam([model.l1, model.l2], lr=0.001)\n\nx, y = Tensor.rand(4, 1, 28, 28), Tensor([2,4,3,7])  # replace with real mnist dataloader\n\nwith Tensor.train():\n  for i in range(10):\n    optim.zero_grad()\n    loss = model(x).sparse_categorical_crossentropy(y).backward()\n    optim.step()\n    print(i, loss.item())\n```\n\n----------------------------------------\n\nTITLE: tinygrad Tensor Example (Python)\nDESCRIPTION: This Python code demonstrates basic tensor operations in tinygrad, including creating tensors with `requires_grad=True`, performing matrix multiplication, summing the result, and calculating gradients using `backward()`.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Tensor\n\nx = Tensor.eye(3, requires_grad=True)\ny = Tensor([[2.0,0,-2.0]], requires_grad=True)\nz = y.matmul(x).sum()\nz.backward()\n\nprint(x.grad.tolist())  # dz/dx\nprint(y.grad.tolist())  # dz/dy\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Example (Python)\nDESCRIPTION: This Python code demonstrates equivalent tensor operations in PyTorch, including creating tensors with `requires_grad=True`, performing matrix multiplication, summing the result, and calculating gradients using `backward()`.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nx = torch.eye(3, requires_grad=True)\ny = torch.tensor([[2.0,0,-2.0]], requires_grad=True)\nz = y.matmul(x).sum()\nz.backward()\n\nprint(x.grad.tolist())  # dz/dx\nprint(y.grad.tolist())  # dz/dy\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Neural Network (TinyNet)\nDESCRIPTION: Defines a simple two-layer neural network, `TinyNet`, for classifying handwritten digits. It utilizes the `Linear` layer and the `leaky_relu` activation function.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass TinyNet:\n  def __init__(self):\n    self.l1 = Linear(784, 128, bias=False)\n    self.l2 = Linear(128, 10, bias=False)\n\n  def __call__(self, x):\n    x = self.l1(x)\n    x = x.leaky_relu()\n    x = self.l2(x)\n    return x\n\nnet = TinyNet()\n```\n\n----------------------------------------\n\nTITLE: JIT compile training step\nDESCRIPTION: This code uses the `TinyJit` decorator to JIT compile the `step` function. This can significantly improve performance by caching the compiled kernels.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import TinyJit\njit_step = TinyJit(step)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for TinyNet\nDESCRIPTION: Trains the `TinyNet` neural network using the MNIST dataset. The training loop includes sampling batches, performing a forward pass, computing the loss, zeroing gradients, performing a backward pass, and updating the parameters.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nX_train, Y_train, X_test, Y_test = fetch_mnist()\n\nwith Tensor.train():\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_train.shape[0], size=(64))\n    batch = Tensor(X_train[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Tensor(Y_train[samp])\n\n    # forward pass\n    out = net(batch)\n\n    # compute loss\n    loss = sparse_categorical_crossentropy(out, labels)\n\n    # zero gradients\n    opt.zero_grad()\n\n    # backward pass\n    loss.backward()\n\n    # update parameters\n    opt.step()\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1)\n    acc = (pred == labels).mean()\n\n    if step % 100 == 0:\n      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Linear Layer\nDESCRIPTION: Defines a custom `Linear` layer for neural networks in Tinygrad. This layer includes weight and bias initialization and implements the forward pass using the `linear` operation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Linear:\n  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n    self.bias = Tensor.zeros(out_features) if bias else None\n\n  def __call__(self, x):\n    return x.linear(self.weight.transpose(), self.bias)\n```\n\n----------------------------------------\n\nTITLE: Sparse Categorical Cross Entropy Loss Function\nDESCRIPTION: Defines the sparse categorical cross entropy loss function for Tinygrad, implemented using tensor operations. It handles cases where tinygrad doesn't natively support load/store operations, using an `arange` mask.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef sparse_categorical_crossentropy(self, Y, ignore_index=-1) -> Tensor:\n    loss_mask = Y != ignore_index\n    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n```\n\n----------------------------------------\n\nTITLE: PyTorch CUDA/METAL Interoperability\nDESCRIPTION: Demonstrates seamless tensor sharing between PyTorch and tinygrad for CUDA and METAL devices using `Tensor.from_blob`. It initializes a PyTorch tensor, extracts its data pointer and shape, then creates a corresponding tinygrad Tensor.  Synchronization is performed before tinygrad calculations to ensure data validity.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/runtime.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom tinygrad.dtype import _from_torch_dtype\ntensor1 = torch.tensor([1.0, 2.0, 3.0], device=torch.device(\"cuda\"))\ntiny_tensor1 = Tensor.from_blob(tensor1.data_ptr(), tensor1.shape, dtype=_from_torch_dtype(tensor1.dtype), device='CUDA')\n\n# Before tinygrad calculations, mps needs to be synchronized to make sure data is valid.\nif data.device.type == \"mps\": torch.mps.synchronize()\nelse: torch.cuda.synchronize()\n\nx = (tiny_tensor1 + 1).realize()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Models with Safetensors in Tinygrad\nDESCRIPTION: This snippet demonstrates how to save and load models in tinygrad using the safetensors format. It utilizes functions from `tinygrad.nn.state` to save the model's state dictionary to a file and load it back in. Requires: tinygrad.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad.nn.state import safe_save, safe_load, get_state_dict, load_state_dict\n\n# first we need the state dict of our model\nstate_dict = get_state_dict(net)\n\n# then we can just save it to a file\nsafe_save(state_dict, \"model.safetensors\")\n\n# and load it back in\nstate_dict = safe_load(\"model.safetensors\")\nload_state_dict(net, state_dict)\n```\n\n----------------------------------------\n\nTITLE: Context Manager for Temporary Environment Variable Setting in Tinygrad (Python)\nDESCRIPTION: This example shows how to use the `Context` context manager to temporarily set an environment variable (DEBUG) within a specific code block.  When the `with` block exits, the original value of the DEBUG environment variable is restored. This is useful for isolating the effects of certain configurations.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/env_vars.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith Context(DEBUG=0):\n  a = Tensor.ones(10, 10)\n  a *= 2\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation with Tinygrad\nDESCRIPTION: This snippet demonstrates how to use the TinyJit decorator to speed up the forward pass of a neural network in tinygrad.  It defines a wrapper function `jit` that applies the network `net` to the input `x` and realizes the output. This optimization currently does not support models with varying input sizes and non-tinygrad operations. Requires: tinygrad, numpy.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import TinyJit\n\n@TinyJit\ndef jit(x):\n  return net(x).realize()\n\nwith Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass with jit\n    out = jit(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n```\n\n----------------------------------------\n\nTITLE: Define MNIST model\nDESCRIPTION: This code defines a simple convolutional neural network model for MNIST digit classification. It consists of two convolutional layers and one linear layer. The `__call__` method defines the forward pass.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -> Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n```\n\n----------------------------------------\n\nTITLE: Train model step\nDESCRIPTION: This snippet defines a function `step` that performs a single training step. It samples a batch of data from the training set, calculates the loss, and updates the model parameters using the Adam optimizer.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n  Tensor.training = True  # makes dropout work\n  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n  X, Y = X_train[samples], Y_train[samples]\n  optim.zero_grad()\n  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n  optim.step()\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Specifying Tensor Data Type\nDESCRIPTION: Demonstrates how to specify the data type (dtype) of a tensor during creation using the `dtype` argument and the `tinygrad.dtypes` module.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import dtypes\n\nt3 = Tensor([1, 2, 3, 4, 5], dtype=dtypes.int32)\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors from Existing Data\nDESCRIPTION: Demonstrates how to create Tinygrad tensors from existing data structures, such as Python lists and NumPy ndarrays.  This showcases the flexibility of initializing tensors with pre-existing data.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nt1 = Tensor([1, 2, 3, 4, 5])\nna = np.array([1, 2, 3, 4, 5])\nt2 = Tensor(na)\n```\n\n----------------------------------------\n\nTITLE: Stochastic Gradient Descent Optimizer Initialization\nDESCRIPTION: Initializes the Stochastic Gradient Descent (SGD) optimizer with the parameters of the neural network (specifically, the weights of the linear layers) and a learning rate.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad.nn.optim import SGD\n\nopt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)\n```\n\n----------------------------------------\n\nTITLE: Realizing and Retrieving Value from Tensor in Tinygrad\nDESCRIPTION: This snippet demonstrates the `Tensor.realize` function, which executes the kernels and writes the output to memory. After realization, the snippet prints the realized Tensor and then retrieves its value using the `item()` method. It showcases how to execute the computational graph and retrieve the final result.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/kernelize.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTensor.realize(out)\nprint(out)        # <Tensor <UOp METAL (1,) int (<Ops.BUFFER: 23>, <buf real:True device:METAL size:1 dtype:dtypes.int offset:0>)> on METAL with grad None>\nprint(out.item()) # 5\n```\n\n----------------------------------------\n\nTITLE: Evaluate model\nDESCRIPTION: This code creates an instance of the `Model` class and evaluates it on the test set. The `argmax` function returns the predicted class for each image, and the `mean` function calculates the accuracy.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\n# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n```\n\n----------------------------------------\n\nTITLE: Train model with JIT\nDESCRIPTION: This code trains the model using the JIT compiled `jit_step` function. It iterates for 7000 steps, calculates the loss and accuracy every 100 steps, and prints the results.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor step in range(7000):\n  loss = jit_step()\n  if step%100 == 0:\n    Tensor.training = False\n    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n    print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\"\n```\n\n----------------------------------------\n\nTITLE: QCOM OpenCL Buffer Interoperability\nDESCRIPTION: Shows how to interoperate with OpenCL buffers on the QCOM backend. It creates a raw OpenCL buffer, extracts the memory pointer using `to_mv` and `ctypes`, and then creates a tinygrad Tensor that directly references the OpenCL buffer's memory. Requires `cl` (PyOpenCL) and `ctypes`.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/runtime.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# create raw opencl buffer.\ncl_buf = cl.clCreateBuffer(cl_context, cl.CL_MEM_READ_WRITE, 0x100, None, status := ctypes.c_int32())\n\n# extract pointers\ncl_buf_desc_ptr = to_mv(ctypes.addressof(cl_buf), 8).cast('Q')[0]\nrawbuf_ptr = to_mv(cl_buf_desc_ptr, 0x100).cast('Q')[20] # offset 0xA0 is a raw gpu pointer.\n\n# create tiny tensor\ntiny = Tensor.from_blob(rawbuf_ptr, (8, 8), dtype=dtypes.int, device='QCOM')\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors with Factory Methods\nDESCRIPTION: Illustrates various factory methods for creating tensors with specific shapes and initial values, such as full, zeros, ones, eye, arange, rand, randn, and uniform. These methods provide convenient ways to initialize tensors for different use cases.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfull = Tensor.full(shape=(2, 3), fill_value=5) # create a tensor of shape (2, 3) filled with 5\nzeros = Tensor.zeros(2, 3) # create a tensor of shape (2, 3) filled with 0\nones = Tensor.ones(2, 3) # create a tensor of shape (2, 3) filled with 1\n\nfull_like = Tensor.full_like(full, fill_value=2) # create a tensor of the same shape as `full` filled with 2\nzeros_like = Tensor.zeros_like(full) # create a tensor of the same shape as `full` filled with 0\nones_like = Tensor.ones_like(full) # create a tensor of the same shape as `full` filled with 1\n\neye = Tensor.eye(3) # create a 3x3 identity matrix\narange = Tensor.arange(start=0, stop=10, step=1) # create a tensor of shape (10,) filled with values from 0 to 9\n\nrand = Tensor.rand(2, 3) # create a tensor of shape (2, 3) filled with random values from a uniform distribution\nrandn = Tensor.randn(2, 3) # create a tensor of shape (2, 3) filled with random values from a standard normal distribution\nuniform = Tensor.uniform(2, 3, low=0, high=10) # create a tensor of shape (2, 3) filled with random values from a uniform distribution between 0 and 10\n```\n\n----------------------------------------\n\nTITLE: Evaluation Loop for TinyNet\nDESCRIPTION: Evaluates the trained `TinyNet` neural network on the test set. The evaluation loop includes sampling batches, performing a forward pass, and calculating the accuracy.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass\n    out = net(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n```\n\n----------------------------------------\n\nTITLE: Preprocess Wikipedia training data\nDESCRIPTION: This command preprocesses the Wikipedia training data using the `wikipedia.py` script. The `BASEDIR` environment variable specifies the directory where the raw data is located, and `NUM_WORKERS` specifies the number of threads to use for preprocessing.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" NUM_WORKERS=16 python3 extra/datasets/wikipedia.py pre-train all\n```\n\n----------------------------------------\n\nTITLE: Get default device\nDESCRIPTION: This snippet retrieves the default device used by tinygrad. The output will be `CUDA` if a GPU is available, otherwise `CPU`.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Device\nprint(Device.DEFAULT)\n```\n\n----------------------------------------\n\nTITLE: Running Stable Diffusion\nDESCRIPTION: This command executes the stable_diffusion.py example for image generation. It generates an image based on a default or specified prompt.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython3 examples/stable_diffusion.py\n```\n\n----------------------------------------\n\nTITLE: Performing Operations on Tensors\nDESCRIPTION: Shows how to perform element-wise operations on tensors, including addition, multiplication, ReLU activation, and log softmax. These operations are lazy and only executed when the tensor is realized.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nt4 = Tensor([1, 2, 3, 4, 5])\nt5 = (t4 + 1) * 2\nt6 = (t5 * t4).relu().log_softmax()\n```\n\n----------------------------------------\n\nTITLE: Load MNIST dataset\nDESCRIPTION: This snippet loads the MNIST dataset using the `mnist` function from `tinygrad.nn.datasets`. It returns the training and testing sets as NumPy arrays.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad.nn.datasets import mnist\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n```\n\n----------------------------------------\n\nTITLE: Realizing a Tensor\nDESCRIPTION: Illustrates how to realize a tensor, triggering the execution of any lazy operations, and converting it to a NumPy array for printing or further processing.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(t6.numpy())\n# [-56. -48. -36. -20.   0.]\n```\n\n----------------------------------------\n\nTITLE: Context Decorator for Environment Variable Setting in Tinygrad (Python)\nDESCRIPTION: This code snippet demonstrates how to use the `@Context` decorator to set an environment variable (DEBUG) only within the scope of the decorated function. This is useful for Tinygrad developers who need to control debugging behavior for specific code sections without affecting the entire application.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/env_vars.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@Context(DEBUG=4)\ndef numpy(self) -> ...\n```\n\n----------------------------------------\n\nTITLE: Importing Tensor Class\nDESCRIPTION: Imports the Tensor class from the tinygrad library, which is the base data structure used for all high-level operations.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Tensor\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data\nDESCRIPTION: This command preprocesses the Wikipedia training data. It sets the BASEDIR environment variable and NUM_WORKERS (the number of threads for preprocessing). The wikipedia.py script is executed with the 'pre-train all' arguments, indicating preprocessing for the entire training dataset.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" NUM_WORKERS=16 python3 extra/datasets/wikipedia.py pre-train all\n```\n\n----------------------------------------\n\nTITLE: Downloading Raw Wikipedia Data\nDESCRIPTION: Downloads and verifies the checksum of the raw Wikipedia dataset used for training. The `BASEDIR` environment variable specifies the directory to save the data, `WIKI_TRAIN=1` indicates that the training data should be downloaded, and `VERIFY_CHECKSUM=1` enables checksum verification to ensure data integrity.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" WIKI_TRAIN=1 VERIFY_CHECKSUM=1 python3 extra/datasets/wikipedia_download.py\n```\n\n----------------------------------------\n\nTITLE: Fused Matmul Example with Debugging in tinygrad (Shell)\nDESCRIPTION: This shell command demonstrates a fused matrix multiplication in tinygrad using laziness. It calculates the mean difference between the tinygrad result and the NumPy result. The `DEBUG=3` flag enables debugging output to visualize the fused kernel.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nDEBUG=3 python3 -c \"from tinygrad import Tensor;\nN = 1024; a, b = Tensor.rand(N, N), Tensor.rand(N, N);\nc = (a.reshape(N, 1, N) * b.T.reshape(1, N, N)).sum(axis=2);\nprint((c.numpy() - (a.numpy() @ b.numpy())).mean())\"\n```\n\n----------------------------------------\n\nTITLE: Download ImageNet Dataset\nDESCRIPTION: This script downloads the ImageNet training dataset required for running the ResNet-50 benchmark. IMGNET_TRAIN=1 enables the download of the training data.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: Preprocessing a Specific Training Topic\nDESCRIPTION: Preprocesses data for a specific topic (topic ID 42 in this case). The `BASEDIR` environment variable specifies the directory containing the raw data.  This is useful for debugging or running smaller experiments.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-train 42\n```\n\n----------------------------------------\n\nTITLE: Download and Verify ImageNet Data (Python)\nDESCRIPTION: This python script downloads the ImageNet training dataset using the `imagenet_download.py` script from the `extra/datasets` directory. The `IMGNET_TRAIN=1` environment variable triggers the download of the training data.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: Downloading ImageNet Dataset\nDESCRIPTION: This snippet downloads the ImageNet training dataset using the imagenet_download.py script. The IMGNET_TRAIN environment variable is set to 1 to indicate that the training dataset should be downloaded.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: QCOM OpenCL Image Interoperability\nDESCRIPTION: Demonstrates interoperability with OpenCL images on the QCOM backend. It creates an OpenCL image, retrieves the memory pointer using `to_mv` and `ctypes`, and creates a tinygrad Tensor linked to this image.  Requires `cl` (PyOpenCL), `ctypes`, and knowledge of image dimensions (w, h).\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/runtime.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# create cl image.\ncl_img = cl.clCreateImage2D(cl_context, cl.CL_MEM_READ_WRITE, cl.cl_image_format(cl.CL_RGBA, cl.CL_FLOAT), w, h, 0, None, status := ctypes.c_int32())\n\n# extract pointers\ncl_buf_desc_ptr = to_mv(ctypes.addressof(cl_img), 8).cast('Q')[0]\nrawbuf_ptr = to_mv(cl_buf_desc_ptr, 0x100).cast('Q')[20] # offset 0xA0 is a raw gpu pointer.\n\n# create tiny tensor\ntiny = Tensor.from_blob(rawbuf_ptr, (h*w*4,), dtype=dtypes.imagef((h,w)), device='QCOM')\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Validation Data\nDESCRIPTION: This command preprocesses the Wikipedia validation data. It sets the BASEDIR environment variable and then executes the wikipedia.py script with the 'pre-eval' argument.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-eval\n```\n\n----------------------------------------\n\nTITLE: Create tiktoken JavaScript file\nDESCRIPTION: This script generates the `tiktoken` JavaScript file using `npm` and `webpack`.  It's a necessary dependency to properly use TinyChat in the browser.  Requires `npm` and `webpack` to be installed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/tinychat/tinychat-browser/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n./examples/tinychat/tinychat-browser/make_tiktoken_js.sh\n```\n\n----------------------------------------\n\nTITLE: Preprocess Wikipedia validation data\nDESCRIPTION: This command preprocesses the Wikipedia validation data using the `wikipedia.py` script. The `BASEDIR` environment variable specifies the directory where the raw data is located.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-eval\n```\n\n----------------------------------------\n\nTITLE: Install Tinygrad and mlperf-logging\nDESCRIPTION: This snippet installs Tinygrad and the mlperf-logging package from the Tinygrad repository.  It clones the repository and then uses pip to install the package in editable mode with the mlperf extras. The editable mode allows for changes in the cloned repository to be immediately reflected in the installed package.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Model (Small Version)\nDESCRIPTION: This command executes the whisper.py example for audio transcription. The SMALL=1 environment variable likely selects a smaller model size. Requires pyaudio and torchaudio.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nSMALL=1 python3 examples/whisper.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia Dataset\nDESCRIPTION: This command downloads the Wikipedia dataset for training. It sets the BASEDIR environment variable to specify the location where the data should be stored, enables WIKI_TRAIN for training data, and VERIFY_CHECKSUM to ensure data integrity during download.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" WIKI_TRAIN=1 VERIFY_CHECKSUM=1 python3 extra/datasets/wikipedia_download.py\n```\n\n----------------------------------------\n\nTITLE: Installing tqdm and tensorflow\nDESCRIPTION: This command installs the tqdm and tensorflow Python packages using pip. tqdm is used for displaying progress bars, and tensorflow is a machine learning framework.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tqdm tensorflow\n```\n\n----------------------------------------\n\nTITLE: Installing Tinygrad and mlperf-logging\nDESCRIPTION: This code snippet installs Tinygrad and the mlperf-logging library from the Tinygrad repository. It clones the Tinygrad repository from GitHub and then uses pip to install the package in editable mode with the 'mlperf' extra.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Fetching MNIST Dataset\nDESCRIPTION: Imports the `fetch_mnist` function from the `extra.datasets` module to load the MNIST dataset for training and evaluation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom extra.datasets import fetch_mnist\n```\n\n----------------------------------------\n\nTITLE: Printing Lazydata of Kernelized Tensor in Tinygrad\nDESCRIPTION: This snippet prints the `lazydata` attribute of a kernelized Tensor. This is useful for inspecting the underlying operations and dependencies after kernelization. The snippet helps visualize the transformed Tensor graph after `kernelize` is called.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/kernelize.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(out.lazydata)\n```\n\n----------------------------------------\n\nTITLE: Install tqdm and tensorflow\nDESCRIPTION: This command installs the tqdm and tensorflow packages using pip. These packages are required for the BERT implementation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tqdm tensorflow\n```\n\n----------------------------------------\n\nTITLE: Install tinygrad and mlperf-logging\nDESCRIPTION: This command clones the tinygrad repository and installs tinygrad with the mlperf dependencies using pip. This prepares the environment for running mlperf benchmarks.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs the `tqdm` and `tensorflow` packages using pip. `tqdm` provides progress bars for long-running operations, while `tensorflow` is used for parts of the BERT model and evaluation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tqdm tensorflow\n```\n\n----------------------------------------\n\nTITLE: Preprocess Wikipedia training data for a specific topic\nDESCRIPTION: This command preprocesses Wikipedia data for a specific topic, which is passed as an argument to the script. `BASEDIR` defines the data directory.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-train 42\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data\nDESCRIPTION: This command preprocesses the Wikipedia training data. It sets the BASEDIR environment variable to specify the location of the raw data and NUM_WORKERS to control the number of threads used for preprocessing. The pre-train all argument indicates that all available training data should be processed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" NUM_WORKERS=16 python3 extra/datasets/wikipedia.py pre-train all\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Validation Data\nDESCRIPTION: Preprocesses the Wikipedia data for validation. The `BASEDIR` environment variable specifies the directory containing the raw data. This command prepares the data used to evaluate the model's performance during training.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-eval\n```\n\n----------------------------------------\n\nTITLE: Install tinygrad\nDESCRIPTION: This command installs tinygrad directly from the GitHub repository using pip. This is the recommended way to get the latest version of tinygrad.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install git+https://github.com/tinygrad/tinygrad.git\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Specific Topic\nDESCRIPTION: This command preprocesses training data for a specific topic from the Wikipedia dataset, where the topic ID is between 0 and 499.  It specifies the BASEDIR and the topic ID.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-train 42\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Validation Data\nDESCRIPTION: This command preprocesses the Wikipedia validation data. It sets the BASEDIR environment variable to specify the location of the raw data and uses the pre-eval argument to indicate that validation data should be processed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-eval\n```\n\n----------------------------------------\n\nTITLE: Download raw Wikipedia data\nDESCRIPTION: This command downloads raw Wikipedia data using the `wikipedia_download.py` script. The `BASEDIR` environment variable specifies the directory to save the data, `WIKI_TRAIN` enables the training data download, and `VERIFY_CHECKSUM` enables checksum verification.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" WIKI_TRAIN=1 VERIFY_CHECKSUM=1 python3 extra/datasets/wikipedia_download.py\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data\nDESCRIPTION: Preprocesses the downloaded Wikipedia data for training. The `BASEDIR` environment variable specifies the directory containing the raw data, and `NUM_WORKERS` sets the number of threads to use for preprocessing. This command preprocesses all available topics for training.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" NUM_WORKERS=16 python3 extra/datasets/wikipedia.py pre-train all\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark on tinybox_green\nDESCRIPTION: This command executes the benchmark script run_and_time.sh for the tinybox_green configuration. It's assumed the script handles the actual benchmark execution and timing.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Tensor Kernelization Example in Tinygrad\nDESCRIPTION: This snippet demonstrates the basic process of creating a Tensor graph, kernelizing it, and observing the changes in the Tensor's UOp. It initializes three input buffers, creates a multiply-add Tensor graph, and then calls `kernelize` on the output Tensor.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/kernelize.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# initialize 3 input buffers on the device\na = Tensor([1]).realize()\nb = Tensor([2]).realize()\nc = Tensor([3]).realize()\n\n# create the Tensor graph\nmul = a*b\nout = mul+c\n\nprint(mul) # <Tensor <UOp METAL (1,) int (<Ops.MUL: 48>, None)> on METAL with grad None>\nprint(out) # <Tensor <UOp METAL (1,) int (<Ops.ADD: 52>, None)> on METAL with grad None>\n\nout.kernelize()\n\nprint(mul) # <Tensor <UOp METAL (1,) int (<Ops.MUL: 48>, None)> on METAL with grad None>\nprint(out) # <Tensor <UOp METAL (1,) int (<Ops.ASSIGN: 66>, None)> on METAL with grad None>\n```\n\n----------------------------------------\n\nTITLE: Run benchmark on tinybox_red (Shell)\nDESCRIPTION: This shell script executes the ResNet benchmark and measures its runtime on the tinybox_red system.  It uses a specific script designed for running and timing the benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Setup for BERT Benchmark on tinybox_red\nDESCRIPTION: This command executes the setup script for BERT on the tinybox_red platform. This script likely configures the environment before running the benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Setting up tinybox_red\nDESCRIPTION: This command executes the setup script setup.sh for the tinybox_red configuration.  It likely performs one-time setup tasks required before running the benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Install Tinygrad and mlperf-logging (Shell)\nDESCRIPTION: This shell script installs Tinygrad and mlperf-logging from the master branch. It clones the Tinygrad repository and uses pip to install it in editable mode with the mlperf extra dependencies.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Install Tinygrad and MLPerf Logging\nDESCRIPTION: This snippet clones the Tinygrad repository and installs it along with the MLPerf logging library using pip. This sets up the necessary environment for running MLPerf benchmarks with Tinygrad.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Running setup.sh for tinybox_red\nDESCRIPTION: This snippet executes the setup.sh script located in the tinybox_red implementation directory. This script likely performs environment configuration and preparation steps specific to the tinybox_red hardware before running the benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Run BERT benchmark on tinybox_red\nDESCRIPTION: This command executes the BERT benchmark script `run_and_time.sh` for the tinybox_red configuration.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing a Google Cloud TPU VM with gcloud\nDESCRIPTION: This set of gcloud commands creates, connects to, lists, and deletes a Google Cloud TPU VM. It requires the Google Cloud SDK to be installed and configured.  These commands manage the lifecycle of a TPU v2-8 instance in the us-central1-b zone. Ensure the correct zone and accelerator type are chosen based on availability and cost.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/accel/tpu/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud alpha compute tpus tpu-vm create test --zone=us-central1-b --accelerator-type=v2-8 --version=v2-alpha\ngcloud alpha compute tpus tpu-vm ssh test --zone us-central1-b\n# and for when you are done\ngcloud alpha compute tpus tpu-vm delete test --zone us-central1-b\ngcloud alpha compute tpus tpu-vm list --zone us-central1-b\n```\n\n----------------------------------------\n\nTITLE: Installing tinygrad and mlperf-logging\nDESCRIPTION: This snippet installs tinygrad and mlperf-logging from the tinygrad repository. It clones the repository and uses pip to install the package in editable mode with the mlperf extra dependencies.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Running BERT Benchmark on tinybox_green\nDESCRIPTION: This command executes the benchmark script for BERT on the tinybox_green platform. It runs the run_and_time.sh script located in the specified directory.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark on tinybox_red\nDESCRIPTION: This command executes the benchmark script run_and_time.sh for the tinybox_red configuration. It's assumed the script handles the actual benchmark execution and timing.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Disable CWSR on tinybox_red\nDESCRIPTION: This snippet disables the Coherent Write System Request (CWSR) feature on the tinybox_red hardware configuration.  It modifies the amdgpu.conf file, updates the initramfs, and reboots the system.  The final command validates that CWSR is disabled by reading the appropriate sysfs parameter.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark on tinybox_green\nDESCRIPTION: Executes the BERT benchmark on the tinybox_green hardware configuration. This command runs the `run_and_time.sh` script located in the specified directory, which performs the training and measures the execution time.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Tinygrad and mlperf-logging\nDESCRIPTION: This command clones the Tinygrad repository and installs it along with the mlperf-logging dependency in editable mode. This allows modifications to the Tinygrad code to be immediately reflected without reinstalling.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Disable cwsr on tinybox_red (Shell)\nDESCRIPTION: This shell script disables cwsr on the tinybox_red system by modifying the amdgpu.conf file.  It then updates the initramfs and reboots the system. Finally, it validates the setting.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Disabling cwsr on tinybox_red\nDESCRIPTION: This snippet disables cwsr (cache write support requirement) on the tinybox_red system by modifying the amdgpu.conf file, updating the initramfs, and rebooting the system.  It then validates the setting.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark on tinybox_red\nDESCRIPTION: Executes the BERT benchmark on the tinybox_red hardware configuration.  This command runs the `run_and_time.sh` script located in the specified directory, which performs the training and measures the execution time.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Tinygrad and mlperf-logging\nDESCRIPTION: This command clones the Tinygrad repository from GitHub and installs it in editable mode along with the mlperf dependencies. This is a prerequisite for running the BERT benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Run ResNet-50 Benchmark on tinybox_red\nDESCRIPTION: This script executes the ResNet-50 benchmark on tinybox_red hardware, performing the training and timing the execution. The results are expected to be logged according to MLPerf guidelines.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Run ResNet-50 benchmark\nDESCRIPTION: This snippet executes the run_and_time.sh script which will run the ResNet-50 benchmark on the tinybox_red platform.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Running and Timing the Benchmark\nDESCRIPTION: This snippet executes the run_and_time.sh script located in the tinybox_red implementation directory. This script runs the ResNet-50 benchmark and measures the execution time.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Download ImageNet Training Data\nDESCRIPTION: This snippet uses a Python script to download the ImageNet training dataset. It requires that the IMGNET_TRAIN environment variable is set to 1.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia Data\nDESCRIPTION: This command downloads raw Wikipedia data. It sets environment variables BASEDIR (the base directory for the data), WIKI_TRAIN (flag to download training data), and VERIFY_CHECKSUM (flag to verify checksums). It then executes the wikipedia_download.py script using Python 3.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" WIKI_TRAIN=1 VERIFY_CHECKSUM=1 python3 extra/datasets/wikipedia_download.py\n```\n\n----------------------------------------\n\nTITLE: Running tinygrad tests with Remu\nDESCRIPTION: This command runs a specific test within the tinygrad test suite using the emulated RDNA3 kernel provided by Remu. The environment variables set the Python path, enable the mock GPU, and specify the AMD target. The DEBUG variable enables Remu's logging output.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/remu/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPYTHONPATH=\".\" MOCKGPU=1 AMD=1 python test/test_tiny.py TestTiny.test_plus\n```\n\n----------------------------------------\n\nTITLE: Run Setup Script on tinybox_red\nDESCRIPTION: This script executes the setup script specific to tinybox_red hardware, configuring the environment for the ResNet-50 benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Run tinygrad test suite (Shell)\nDESCRIPTION: This shell command shows how to run the entire tinygrad test suite.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pytest test/\n```\n\n----------------------------------------\n\nTITLE: Run Setup Script on tinybox_red\nDESCRIPTION: This snippet executes the setup script specific to the tinybox_red hardware configuration. The script is located within the examples/mlperf directory structure.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark on tinybox_red\nDESCRIPTION: This snippet executes the benchmark script specific to the tinybox_red hardware configuration. The script is located within the examples/mlperf directory structure. It likely performs the ResNet-50 training or inference and measures its performance.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Enqueuing commands on HCQ-compatible device using HWQueue in Python\nDESCRIPTION: This code demonstrates how to enqueue a wait, execute, and signal command on an HCQ-compatible device using the HWQueue. It shows the basic structure for interacting with the device using the command queue.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/hcq.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nHWQueue().wait(signal_to_wait, value_to_wait) \\\n         .exec(program, args_state, global_dims, local_dims) \\\n         .signal(signal_to_fire, value_to_fire) \\\n         .submit(your_device)\n```\n\n----------------------------------------\n\nTITLE: Install Tinygrad and MLPerf Logging (Shell)\nDESCRIPTION: This shell script installs Tinygrad with the MLPerf dependencies from a local clone of the Tinygrad repository. It uses pip to install the package in editable mode.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Run tinybox_red setup script\nDESCRIPTION: This snippet executes the setup script for the tinybox_red platform. This prepares the environment before the benchmarking.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Compile TinyChat for Browser\nDESCRIPTION: This command compiles the TinyChat application for browser deployment using Python. It leverages the `compile.py` script located in the `examples/tinychat/tinychat-browser/` directory.  It needs `PYTHONPATH` correctly set.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/tinychat/tinychat-browser/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nPYTHONPATH=. python examples/tinychat/tinychat-browser/compile.py\n```\n\n----------------------------------------\n\nTITLE: Time JIT compiled step\nDESCRIPTION: This code measures the execution time of the JIT compiled `jit_step` function. The first few executions may be slower as the kernels are being captured and compiled.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\ntimeit.repeat(jit_step, repeat=5, number=1)\n# [0.2596786549997887,\n#  0.08989566299987928,\n#  0.0012115650001760514,\n#  0.001010227999813651,\n#  0.0012164899999334011]\n```\n\n----------------------------------------\n\nTITLE: Time training step\nDESCRIPTION: This code uses the `timeit` module to measure the execution time of the `step` function. It repeats the function 5 times and prints the execution time for each repetition.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\ntimeit.repeat(step, repeat=5, number=1)\n#[0.08268719699981375,\n# 0.07478952900009972,\n# 0.07714716600003158,\n# 0.07785399599970333,\n# 0.07605237000007037]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Specific Topic\nDESCRIPTION: This command preprocesses a specific topic of the Wikipedia data. It sets the BASEDIR environment variable and then runs the wikipedia.py script with the 'pre-train' and topic ID arguments. Here, topic 42 is preprocessed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nBASEDIR=\"/raid/datasets/wiki\" python3 extra/datasets/wikipedia.py pre-train 42\n```\n\n----------------------------------------\n\nTITLE: Usage of signals with HCQ-compatible devices in Python\nDESCRIPTION: This code illustrates the usage of signals for synchronization and timing in HCQ-compatible devices. It demonstrates how to create a signal, timestamp it, signal it with a value, submit the commands to the device, and then wait for the signal.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/hcq.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsignal = your_device.signal_t()\n\nHWQueue().timestamp(signal) \\\n         .signal(signal, value_to_fire) \\\n         .submit(your_device)\n\nsignal.wait(value_to_fire)\nsignaled_value = signal.value # should be the same as `value_to_fire`\ntimestamp = signal.timestamp\n```\n\n----------------------------------------\n\nTITLE: Run Tinybox Red Setup Script (Shell)\nDESCRIPTION: This command executes the setup script for Tinybox Red, preparing the environment for the ResNet-50 benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Compile TinyChat to WASM\nDESCRIPTION: This script compiles TinyChat to WebAssembly (WASM) using Emscripten. It requires Emscripten to be installed and properly configured.  The script specifically looks for `~/emsdk/emsdk_env.sh`. You might have to adjust the script based on your installation path for Emscripten.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/tinychat/tinychat-browser/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n./examples/tinychat/tinychat-browser/compile_wasm.sh\n```\n\n----------------------------------------\n\nTITLE: Convert pickled profile with SQTT data to RGP file\nDESCRIPTION: This script converts a pickled profile containing SQTT data into an RGP file that can be loaded into Radeon GPU Profiler for detailed performance analysis. It requires the `rgptool.py` script located in the `extra/sqtt` directory within the Tinygrad repository.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nextra/sqtt/rgptool.py create \"/tmp/profile.pkl.$USER\" -o /tmp/gpu0.rgp\n```\n\n----------------------------------------\n\nTITLE: Running EfficientNet on an Image\nDESCRIPTION: This command runs the efficientnet.py example to classify an image. It requires the image file to be present in the specified path.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython3 examples/efficientnet.py ./test/models/efficientnet/Chicken.jpg\n```\n\n----------------------------------------\n\nTITLE: Serve TinyChat Application\nDESCRIPTION: This command starts a simple HTTP server using Python's built-in `http.server` module.  It serves the TinyChat application from the `examples/tinychat` directory on port 7776.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/tinychat/tinychat-browser/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncd examples/tinychat && python -m http.server 7776\n```\n\n----------------------------------------\n\nTITLE: Download ImageNet Training Data (Python)\nDESCRIPTION: This Python script downloads the ImageNet training dataset. The `IMGNET_TRAIN=1` environment variable is required to indicate that the training dataset should be downloaded.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: Synchronizing execution using timeline signals in Python\nDESCRIPTION: This code demonstrates the use of timeline signals for synchronizing execution to other operations on the device. It shows how to wait for the completion of previous operations and signal the completion of current operations using the device's timeline signal.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/hcq.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nHWQueue().wait(your_device.timeline_signal, your_device.timeline_value - 1) \\\n         .exec(...) \\\n         .signal(your_device.timeline_signal, your_device.next_timeline()) \\\n         .submit(your_device)\n\n# Optionally wait for execution\nyour_device.timeline_signal.wait(your_device.timeline_value - 1)\n```\n\n----------------------------------------\n\nTITLE: Installing tinygrad from source using Git and pip\nDESCRIPTION: This snippet clones the tinygrad repository from GitHub and installs it in editable mode using pip.  This allows users to modify the source code and have those changes reflected in their environment without reinstalling.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\ncd tinygrad\npython3 -m pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Running Conversation Chatbot\nDESCRIPTION: This command executes the conversation.py example, which runs a chatbot with voice capabilities. It requires espeak to be installed and PHONEMIZER_ESPEAK_LIBRARY to be set.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython3 examples/conversation.py\n```\n\n----------------------------------------\n\nTITLE: Download ImageNet Training Data\nDESCRIPTION: This snippet executes a Python script to download the ImageNet training data. It requires the IMGNET_TRAIN environment variable to be set to 1.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIMGNET_TRAIN=1 python3 extra/datasets/imagenet_download.py\n```\n\n----------------------------------------\n\nTITLE: Run tinygrad ops tests (Shell)\nDESCRIPTION: This shell command shows how to run the tinygrad ops tests.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython3 test/test_ops.py\n```\n\n----------------------------------------\n\nTITLE: Running LLaMA Chatbot\nDESCRIPTION: This command executes the llama.py example, which runs a chatbot. It requires the LLaMA weights to be downloaded and placed in the weights/LLaMA directory.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython3 examples/llama.py\n```\n\n----------------------------------------\n\nTITLE: Running EfficientNet with Webcam Input\nDESCRIPTION: This command runs the efficientnet.py example and uses the webcam as input for classification. It requires OpenCV to be installed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/showcase.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython3 examples/efficientnet.py webcam\n```\n\n----------------------------------------\n\nTITLE: Run ResNet-50 Benchmark on Tinybox Red (Shell)\nDESCRIPTION: This command executes the ResNet-50 benchmark on Tinybox Red and measures the execution time.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Running BERT Benchmark on tinybox_red\nDESCRIPTION: This command executes the benchmark script for BERT on the tinybox_red platform. It runs the run_and_time.sh script located in the specified directory.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Run setup script for tinybox_red (Shell)\nDESCRIPTION: This shell script executes the setup script specific to the tinybox_red configuration for the ResNet benchmark. It prepares the environment for running the benchmark.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexamples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Run BERT benchmark on tinybox_green\nDESCRIPTION: This command executes the BERT benchmark script `run_and_time.sh` for the tinybox_green configuration.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v5.0/tinycorp/benchmarks/bert/implementations/tinybox_green/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexamples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_green/run_and_time.sh\n```\n\n----------------------------------------\n\nTITLE: Identifying TPU Accelerators using lspci\nDESCRIPTION: This command uses `lspci` to list the PCI devices on the system. The output shows the Google TPU accelerators as \"Unassigned class\" devices. It helps in verifying the presence and identifying the devices to interact with, providing information about the hardware configuration of the TPU VM.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/accel/tpu/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# lspci\n00:04.0 Unassigned class [ff00]: Google, Inc. Device 0027\n00:05.0 Unassigned class [ff00]: Google, Inc. Device 0027\n00:06.0 Unassigned class [ff00]: Google, Inc. Device 0027\n00:07.0 Unassigned class [ff00]: Google, Inc. Device 0027\n```\n\n----------------------------------------\n\nTITLE: Installing tqdm and TensorFlow\nDESCRIPTION: This command installs the tqdm and TensorFlow libraries using pip. These libraries are required for data preprocessing and potentially other aspects of the BERT implementation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/bert/implementations/tinybox_red/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tqdm tensorflow\n```\n\n----------------------------------------\n\nTITLE: Power Limiting GPUs with power-limit script\nDESCRIPTION: This script limits the power consumption of all GPUs in the tinybox to a specified wattage.  This is useful when running the box on a single lower-capacity outlet. Requires sudo privileges. The example limits the GPUs to 150W.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/tinybox.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo power-limit 150\n```\n\n----------------------------------------\n\nTITLE: Install Tinygrad and mlperf-logging\nDESCRIPTION: This snippet installs Tinygrad and the mlperf-logging library from the Tinygrad repository. It clones the repository and uses pip to install the package with the 'mlperf' extra dependency.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\npython3 -m pip install -e \".[mlperf]\"\n```\n\n----------------------------------------\n\nTITLE: Extracting ANEServices.framework and dependencies\nDESCRIPTION: This series of shell commands demonstrates how to extract the ANEServices.framework and its dependencies from the dyld shared cache on macOS. This is necessary to access the ANECompiler and ANEServices libraries. Requires Xcode and dyld-shared-cache-extractor.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/accel/ane/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# install xcode and \nsudo xcode-select --switch /Applications/Xcode.app\n# xcode also contains ANEServices.tbd\nbrew install keith/formulae/dyld-shared-cache-extractor\ndyld-shared-cache-extractor /System/Library/dyld/dyld_shared_cache_arm64e /tmp/libraries\ncp /tmp/libraries/System/Library/PrivateFrameworks/ANECompiler.framework/Versions/A/ANECompiler .\ncp /tmp/libraries/System/Library/PrivateFrameworks/ANEServices.framework/Versions/A/ANEServices .\ncp /tmp/libraries/System/Library/PrivateFrameworks/AppleNeuralEngine.framework/Versions/A/AppleNeuralEngine .\n```\n\n----------------------------------------\n\nTITLE: Building Remu with Cargo\nDESCRIPTION: This command builds the Remu project using Cargo, the Rust package manager. The `--release` flag optimizes the build for performance. The `--manifest-path` option specifies the location of the `Cargo.toml` file, which contains project metadata and dependencies.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/remu/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo build --release --manifest-path ./extra/remu/Cargo.toml\n```\n\n----------------------------------------\n\nTITLE: Disable CWSR on tinybox_red\nDESCRIPTION: This snippet disables the Coherent Write Stream Ring (CWSR) on tinybox_red hardware by modifying the amdgpu.conf file. After the change, the initramfs is updated, and the system is rebooted. Finally, the change is validated by checking the amdgpu module parameters.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_green/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Patching amfid to Disable Entitlement Check (MacOS)\nDESCRIPTION: This code snippet shows how to patch the amfid process on MacOS to disable the entitlement check. This allows running code with restricted entitlements without a valid provisioning profile. Requires knowledge of lldb and memory addresses.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/accel/ane/README.md#_snippet_0\n\nLANGUAGE: lldb\nCODE:\n```\n(lldb) image list\n[  0] 04B6DF6C-6068-3F18-81A7-978985574387 0x0000000102ad0000 /usr/libexec/amfid \n(lldb) p *(unsigned int *)0x102ad8e38=0xd2800000\n```\n\n----------------------------------------\n\nTITLE: Install tinygrad from source (Shell)\nDESCRIPTION: These shell commands demonstrate how to install tinygrad from source using git and pip.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/tinygrad/tinygrad.git\ncd tinygrad\npython3 -m pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Disable CWSR on tinybox_red\nDESCRIPTION: This snippet disables the Coherent Write Shared Resources (CWSR) setting on the tinybox_red platform by modifying the amdgpu.conf file. It requires sudo privileges. It also validates that the setting is disabled after a reboot.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.1/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Convert profile for a specific GPU with SQTT data to RGP file\nDESCRIPTION: This script converts a pickled profile containing SQTT data from a specific GPU into an RGP file. The `-d` option specifies the GPU to use. The RGP file can then be loaded into Radeon GPU Profiler for analysis.  `AMD:5` specifies the GPU number 5.  Ensure the path to the pickled profile matches the location where your profile data is stored.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nextra/sqtt/rgptool.py create \"/tmp/profile.pkl.$USER\" -d 'AMD:5' -o /tmp/gpu5.rgp\n```\n\n----------------------------------------\n\nTITLE: Disable CWSR on Tinybox Red (Shell)\nDESCRIPTION: This shell script disables the Coherent Write System Request (CWSR) on a Tinybox Red system by modifying the amdgpu.conf file, updating the initramfs, and rebooting the system.  It validates the change by checking the cwsr_enable parameter.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/examples/mlperf/training_submission_v4.0/tinycorp/benchmarks/resnet/implementations/tinybox_red/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo vi /etc/modprobe.d/amdgpu.conf\ncat <<EOF > /etc/modprobe.d/amdgpu.conf\noptions amdgpu cwsr_enable=0\nEOF\nsudo update-initramfs -u\nsudo reboot\n\n# validate\nsudo cat /sys/module/amdgpu/parameters/cwsr_enable #= 0\n```\n\n----------------------------------------\n\nTITLE: Debug training step\nDESCRIPTION: This snippet demonstrates how to use the `Context` class to debug the training step. Setting `DEBUG=2` prints the running kernels, and setting `DEBUG=4` prints the code.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/mnist.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import GlobalCounters, Context\nGlobalCounters.reset()\nwith Context(DEBUG=2): step()\n```\n\n----------------------------------------\n\nTITLE: Running tinygrad test_ops with Remu and DEBUG output\nDESCRIPTION: This command executes a specific test from `test_ops.py` using the Remu emulator, with debugging enabled at level 6. This configuration provides detailed logs about the execution of each thread, including grid information, wave information (lane and instruction), and decoded instruction details. It allows observing how only lane 0 writes to global memory in this test case.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/remu/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDEBUG=6 PYTHONPATH=\".\" MOCKGPU=1 AMD=1 python test/test_ops.py TestOps.test_arange_big\n```\n\n----------------------------------------\n\nTITLE: Kernelization of Child Tensor in Tinygrad\nDESCRIPTION: This snippet shows how kernelizing a Tensor affects its children. It creates a child Tensor from a kernelized Tensor, kernelizes the child, and then prints the abstract syntax tree (AST) of the child's `lazydata`. This allows examination of the impact of parent kernelization on subsequent operations.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/kernelize.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchild = out+2\nchild.kernelize()\nprint(child.lazydata.src[1].arg.ast)\n```\n\n----------------------------------------\n\nTITLE: Install extra dependencies for testing (Shell)\nDESCRIPTION: This shell command shows how to install the extra dependencies required for testing tinygrad.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pip install -e '.[testing]'\n```\n\n----------------------------------------\n\nTITLE: Example Remu DEBUG output\nDESCRIPTION: This example showcases the structure of Remu's debug output, which provides insights into thread execution. It displays grid information, wave information (lane and instruction hex), and the decoded instruction details, assisting in debugging and understanding RDNA3 kernel behavior within the Remu emulator.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/remu/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[255 0   0  ] [0   0   0  ] 0  DC6A0000 GLOBAL   offset=0         op=26            addr=8           data=0           saddr=0          vdst=0\n[255 0   0  ] [1   0   0  ] 1  DC6A0000\n[255 0   0  ] [2   0   0  ] 2  DC6A0000\n[255 0   0  ] [3   0   0  ] 3  DC6A0000\n[255 0   0  ] [4   0   0  ] 4  DC6A0000\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries\nDESCRIPTION: Imports the required libraries for the Tinygrad quick start guide, including numpy and the Timing helper from tinygrad.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/quickstart.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom tinygrad.helpers import Timing\n```\n\n----------------------------------------\n\nTITLE: Example Remu Instruction output\nDESCRIPTION: This example shows Remu output and the corresponding disassembled instruction from llvm-objdump, allowing a comparison between Remu's decoded instruction and the expected assembly. This helps verify the accuracy of Remu's instruction parsing and execution.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/remu/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ns_load_b64 s[0:1], s[0:1], 0x10                            // 00000000160C: F4040000 F8000010\nSMEM       sbase=0          sdata=0          op=1             offset=16        soffset=0\n```\n\n----------------------------------------\n\nTITLE: Building and Running the TensorFlow TPU Example\nDESCRIPTION: These commands compile and execute a minimal TPU example from TensorFlow. The example is built using gcc, linking against the libtpu library. Setting `TPU_VLOG_LEVEL=99` enables verbose logging which dumps information in `/tmp/tpu_logs`, useful for understanding the TPU's operations.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/extra/accel/tpu/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd tfexample\ngcc -o libtpu_client libtpu_client.c -ltpu\nTPU_VLOG_LEVEL=99 ./libtpu_client\n```\n\n----------------------------------------\n\nTITLE: Model Speed Example: Tensor Realization in Tinygrad\nDESCRIPTION: This code snippet demonstrates a scenario where the scheduler's decision on whether to recompute or store intermediate results impacts performance. The example involves realizing two tensors that share common subexpressions, highlighting the trade-offs between memory access and computation.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/developer/speed.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tinygrad import Tensor\na = Tensor.rand(100)\nb = Tensor.rand(100)\nc = Tensor.rand(100)\nd = Tensor.rand(100)\nout1 = a+b+c\nout2 = a+b+d\nTensor.realize(out1, out2)\n```\n\n----------------------------------------\n\nTITLE: Install tinygrad directly from master (Shell)\nDESCRIPTION: This shell command shows how to directly install tinygrad from the master branch of its Git repository using pip.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pip install git+https://github.com/tinygrad/tinygrad.git\n```\n\n----------------------------------------\n\nTITLE: Activating Remote Console via IPMI\nDESCRIPTION: This command activates the remote console using IPMI (Intelligent Platform Management Interface) via `ipmitool`. Replace `<BMC IP>` with the BMC's IP address and `<BMC PW>` with the BMC password.  Requires `ipmitool` to be installed.\nSOURCE: https://github.com/tinygrad/tinygrad/blob/master/docs/tinybox.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nipmitool -H <BMC IP> -U admin -P <BMC PW> -I lanplus sol activate\n```"
  }
]
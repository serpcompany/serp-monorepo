[
  {
    "owner": "pixeltable",
    "repo": "pixeltable",
    "content": "TITLE: Querying Tool Output Data in Pixeltable\nDESCRIPTION: Shows how to retrieve and analyze tool execution data from the Pixeltable database. This pattern enables observability into AI agent behavior and tool usage patterns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntool_agent.select(tool_agent.tool_output).collect()\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Tool Selection in Pixeltable\nDESCRIPTION: Demonstrates configuration for automatic tool selection by the AI model. This code snippet shows how to set parameters for tool choice behavior, including requiring tool use.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntool_choice=tools.choice(required=True)\n```\n\n----------------------------------------\n\nTITLE: Basic Query with Filter, Transform and Sort in Pixeltable\nDESCRIPTION: Demonstrates a complete query pipeline that filters movies by year and budget, calculates ROI, sorts by ROI, and limits results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresult = movies.where(\n    (movies.year >= 2000) & (movies.budget > 100.0)\n).select(\n    movies.title,\n    roi=movies.revenue / movies.budget\n).order_by(\n    'roi', asc=False\n).limit(5).collect()\n```\n\n----------------------------------------\n\nTITLE: Registering Multiple Tools in Pixeltable\nDESCRIPTION: Shows how to combine multiple custom tools into a single Pixeltable tools collection. This pattern allows the AI agent to access various functionality from a unified interface.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntools = pxt.tools(search_news, get_weather, calculate_metrics)\n```\n\n----------------------------------------\n\nTITLE: Converting a Financial Agent Table to a UDF in Pixeltable\nDESCRIPTION: Converts the finance_agent table to a callable user-defined function by specifying the answer column as the return value. This allows the entire agent workflow to be used as a function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Convert the finance_agent table to a UDF\nfinance_agent_udf = pxt.udf(\n    finance_agent, \n    return_value=finance_agent.answer\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing ReAct Pattern with Pixelagent\nDESCRIPTION: This example demonstrates the implementation of the ReAct (Reasoning + Acting) pattern using Pixelagent. It includes defining a stock information tool, creating a structured reasoning system prompt, and executing a planning loop that allows the agent to think, take actions, and provide final recommendations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom datetime import datetime\nfrom pixelagent.openai import Agent\nimport pixeltable as pxt\n\n# Define a tool\n@pxt.udf\ndef stock_info(ticker: str) -> dict:\n    \"\"\"Get stock information for analysis\"\"\"\n    import yfinance as yf\n    stock = yf.Ticker(ticker)\n    return stock.info\n\n# ReAct system prompt with structured reasoning pattern\nREACT_PROMPT = \"\"\"\nToday is {date}\n\nIMPORTANT: You have {max_steps} maximum steps. You are on step {step}.\n\nFollow this EXACT step-by-step reasoning and action pattern:\n\n1. THOUGHT: Think about what information you need to answer the question.\n2. ACTION: Either use a tool OR write \"FINAL\" if you're ready to give your final answer.\n\nAvailable tools:\n{tools}\n\nAlways structure your response with these exact headings:\n\nTHOUGHT: [your reasoning]\nACTION: [tool_name] OR simply write \"FINAL\"\n\"\"\"\n\n# Helper function to extract sections from responses\ndef extract_section(text, section_name):\n    pattern = rf'{section_name}:?\\s*(.*?)(?=\\n\\s*(?:THOUGHT|ACTION):|$)'\n    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n    return match.group(1).strip() if match else \"\"\n\n# Execute ReAct planning loop\ndef run_react_loop(question, max_steps=5):\n    step = 1\n    while step <= max_steps:\n        # Dynamic system prompt with current step\n        react_system_prompt = REACT_PROMPT.format(\n            date=datetime.now().strftime(\"%Y-%m-%d\"),\n            tools=[\"stock_info\"],\n            step=step,\n            max_steps=max_steps,\n        )\n        \n        # Agent with updated system prompt\n        agent = Agent(\n            agent_name=\"financial_planner\",\n            system_prompt=react_system_prompt,\n            reset=False,  # Maintain memory between steps\n        )\n        \n        # Get agent's response for current step\n        response = agent.chat(question)\n        \n        # Extract action to determine next step\n        action = extract_section(response, \"ACTION\")\n        \n        # Check if agent is ready for final answer\n        if \"FINAL\" in action.upper():\n            break\n            \n        # Call tool if needed\n        if \"stock_info\" in action.lower():\n            tool_agent = Agent(\n                agent_name=\"financial_planner\",\n                tools=pxt.tools(stock_info)\n            )\n            tool_agent.tool_call(question)\n            \n        step += 1\n    \n    # Generate final recommendation\n    return Agent(agent_name=\"financial_planner\").chat(question)\n\n# Run the planning loop\nrecommendation = run_react_loop(\"Create an investment recommendation for AAPL\")\n```\n\n----------------------------------------\n\nTITLE: Defining Backend Tables and Memory Structure in Pixeltable\nDESCRIPTION: Creates the database schema and computational workflow for a memory-enabled chatbot. Includes tables for storing conversation history, functions for memory retrieval, message formatting, and integration with OpenAI's language models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/memory.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom datetime import datetime\nfrom typing import List, Dict\n\n# Initialize app structure\npxt.drop_dir(\"chatbot\", force=True)\npxt.create_dir(\"chatbot\")\n\n# Create memory table\nmemory = pxt.create_table(\n    \"chatbot.memory\",\n    {\n        \"role\": pxt.String,\n        \"content\": pxt.String,\n        \"timestamp\": pxt.Timestamp,\n    },\n    if_exists=\"ignore\",\n)\n\n# Create chat session table\nchat_session = pxt.create_table(\n    \"chatbot.chat_session\",\n    {\"user_message\": pxt.String, \"timestamp\": pxt.Timestamp},\n    if_exists=\"ignore\",\n)\n\n# Define memory retrieval\n@pxt.query\ndef get_recent_memory():\n    return (\n        memory.order_by(memory.timestamp, asc=False)\n        .select(role=memory.role, content=memory.content)\n        .limit(10)\n    )\n\n# Define message creation\n@pxt.udf\ndef create_messages(past_context: List[Dict], current_message: str) -> List[Dict]:\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a chatbot with memory capabilities.\",\n        }\n    ]\n    messages.extend(\n        [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in past_context]\n    )\n    messages.append({\"role\": \"user\", \"content\": current_message})\n    return messages\n\n# Configure processing workflow\nchat_session.add_computed_column(memory_context=get_recent_memory())\nchat_session.add_computed_column(\n    prompt=create_messages(chat_session.memory_context, chat_session.user_message)\n)\nchat_session.add_computed_column(\n    llm_response=pxt.functions.openai.chat_completions(\n        messages=chat_session.prompt,\n        model=\"gpt-4o-mini\"\n    )\n)\nchat_session.add_computed_column(\n    assistant_response=chat_session.llm_response.choices[0].message.content\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Setup for Embedding Index in Pixeltable\nDESCRIPTION: This snippet shows how to set up a Pixeltable directory, create a table with text content and metadata, and add an embedding index with a specific model. This creates the foundation for a semantic search system.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.huggingface import sentence_transformer\n\n# Create a directory to organize data (optional)\npxt.drop_dir('knowledge_base', force=True)\npxt.create_dir('knowledge_base')\n\n# Create table\ndocs = pxt.create_table(\n    \"knowledge_base.documents\",\n    {\n        \"content\": pxt.String,\n        \"metadata\": pxt.Json\n    }\n)\n\n# Create embedding index\nembed_model = sentence_transformer.using(\n    model_id=\"intfloat/e5-large-v2\"\n)\ndocs.add_embedding_index(\n    column='content',\n    string_embed=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Basic Pixelagent Agent\nDESCRIPTION: Basic implementation showing how to create a simple agent with Pixelagent and interact with it through the chat method. This demonstrates the minimal setup required to get started with the framework.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pixelagent.anthropic import Agent  # Or from pixelagent.openai import Agent\n\n# Create a simple agent\nagent = Agent(\n    agent_name=\"my_assistant\",\n    system_prompt=\"You are a helpful assistant.\"\n)\n\n# Chat with your agent\nresponse = agent.chat(\"Hello, who are you?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Image Search Workflow\nDESCRIPTION: Complete workflow definition that sets up an image table with GPT-4 Vision analysis and vector search capabilities. This script creates the table structure, adds computed columns for AI-generated descriptions, and configures embedding-based search functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.openai import vision\nfrom pixeltable.functions.huggingface import sentence_transformer\n\n# Initialize app structure\npxt.drop_dir(\"image_search\", force=True)\npxt.create_dir(\"image_search\")\n\n# Create images table\nimg_t = pxt.create_table(\n    \"image_search.images\", \n    {\"image\": pxt.Image}\n)\n\n# Add GPT-4 Vision analysis\nimg_t.add_computed_column(\n    image_description=vision(\n        prompt=\"Describe the image. Be specific on the colors you see.\",\n        image=img_t.image,\n        model=\"gpt-4o-mini\",\n    )\n)\n\n# Configure embedding model\nembed_model = sentence_transformer.using(\n    model_id=\"intfloat/e5-large-v2\"\n)\n\n# Add search capability\nimg_t.add_embedding_index(\n    column=\"image_description\", \n    string_embed=embed_model\n)\n\n# Define search query\n@pxt.query\ndef search_images(query_text: str, limit: int):\n    sim = img_t.image_description.similarity(query_text)\n    return (\n        img_t.order_by(sim, asc=False)\n        .select(\n            img_t.image,\n            img_t.image_description, \n            similarity=sim\n        )\n        .limit(limit)\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Database Structure and Defining Custom Tools for AI Agents\nDESCRIPTION: Sets up the Pixeltable database structure and defines custom tool functions. Includes tools for news search via DuckDuckGo, weather information retrieval, and basic statistical calculations. The code also configures a complete AI agent workflow with tool selection and response generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nimport pixeltable.functions as pxtf\nfrom pixeltable.functions.openai import chat_completions, invoke_tools\nfrom duckduckgo_search import DDGS\n\n# Initialize app structure\npxt.drop_dir(\"agents\", force=True)\npxt.create_dir(\"agents\")\n\n# Define tools\n@pxt.udf\ndef search_news(keywords: str, max_results: int) -> str:\n    \"\"\"Search news using DuckDuckGo and return results.\"\"\"\n    try:\n        with DDGS() as ddgs:\n            results = ddgs.news(\n                keywords=keywords,\n                region=\"wt-wt\",\n                safesearch=\"off\",\n                timelimit=\"m\",\n                max_results=max_results,\n            )\n            formatted_results = []\n            for i, r in enumerate(results, 1):\n                formatted_results.append(\n                    f\"{i}. Title: {r['title']}\\n\"\n                    f\"   Source: {r['source']}\\n\"\n                    f\"   Published: {r['date']}\\n\"\n                    f\"   Snippet: {r['body']}\\n\"\n                )\n            return \"\\n\".join(formatted_results)\n    except Exception as e:\n        return f\"Search failed: {str(e)}\"\n\n@pxt.udf\ndef get_weather(location: str) -> str:\n    \"\"\"Mock weather function - replace with actual API call.\"\"\"\n    return f\"Current weather in {location}: 72Â°F, Partly Cloudy\"\n\n@pxt.udf\ndef calculate_metrics(numbers: str) -> str:\n    \"\"\"Calculate basic statistics from a string of numbers.\"\"\"\n    try:\n        nums = [float(n) for n in numbers.split(',')]\n        return f\"Mean: {sum(nums)/len(nums):.2f}, Min: {min(nums)}, Max: {max(nums)}\"\n    except:\n        return \"Error: Please provide comma-separated numbers\"\n\n# Register all tools\ntools = pxt.tools(search_news, get_weather, calculate_metrics)\n\n# Create base table\ntool_agent = pxt.create_table(\n    \"agents.tools\", \n    {\"prompt\": pxt.String}, \n    if_exists=\"ignore\"\n)\n\ntool_choice_opts = [\n    None,\n    tools.choice(auto=True),\n    tools.choice(required=True),\n    tools.choice(tool='stock_price'),\n    tools.choice(tool=weather),\n    tools.choice(required=True, parallel_tool_calls=False),\n]\n\n# Add tool selection and execution workflow\ntool_agent.add_computed_column(\n    initial_response=chat_completions(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": tool_agent.prompt}],\n        tools=tools,\n        tool_choice=tool_choice_opts[1],\n    )\n)\n\n# Add tool execution\ntool_agent.add_computed_column(\n    tool_output=invoke_tools(tools, tool_agent.initial_response)\n)\n\n# Add response formatting\ntool_agent.add_computed_column(\n    tool_response_prompt=pxtf.string.format(\n        \"Orginal Prompt\\n{0}: Tool Output\\n{1}\", \n        tool_agent.prompt, \n        tool_agent.tool_output\n    ),\n    if_exists=\"ignore\",\n)\n\n# Add final response generation\ntool_agent.add_computed_column(\n    final_response=chat_completions(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful AI assistant that can use various tools. Analyze the tool results and provide a clear, concise response.\"\n            },\n            {\"role\": \"user\", \"content\": tool_agent.tool_response_prompt},\n        ]\n    )\n)\n\ntool_agent.add_computed_column(\n    answer=tool_agent.final_response.choices[0].message.content\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Video Search Workflow\nDESCRIPTION: This code defines the entire workflow for the video search system. It includes setting up the video table, creating views for frames and audio chunks, adding computed columns for audio extraction and image description, and creating embedding indices for both visual and audio content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nimport pixeltable as pxt\nfrom pixeltable.functions import openai\nfrom pixeltable.functions.huggingface import sentence_transformer\nfrom pixeltable.functions.video import extract_audio\nfrom pixeltable.iterators import AudioSplitter, FrameIterator\nfrom pixeltable.iterators.string import StringSplitter\nfrom pixeltable.functions.openai import vision\n\n# Define the embedding model once for reuse\nEMBED_MODEL = sentence_transformer.using(model_id='intfloat/e5-large-v2')\n\n# Set up directory and table name\ndirectory = 'video_index'\ntable_name = f'{directory}.video'\n\n# Create video table\npxt.create_dir(directory, if_exists='replace_force')\nvideo_index = pxt.create_table(table_name, {'video': pxt.Video, 'uploaded_at': pxt.Timestamp})\nvideo_index.add_computed_column(audio_extract=extract_audio(video_index.video, format='mp3')) \n\n# Create view for frames\nframes_view = pxt.create_view(\n    f'{directory}.video_frames',\n    video_index,\n    iterator=FrameIterator.create(\n        video=video_index.video,\n        fps=1\n    )\n)\n\n# Create a column for image description using OpenAI gpt-4o-mini\nframes_view.add_computed_column(\n    image_description=vision(\n        prompt=\"Provide quick caption for the image.\",\n        image=frames_view.frame,\n        model=\"gpt-4o-mini\"\n    )\n)    \n\n# Create embedding index for image description\nframes_view.add_embedding_index('image_description', string_embed=EMBED_MODEL)    \n\n# Create view for audio chunks\nchunks_view = pxt.create_view(\n    f'{directory}.video_chunks',\n    video_index,\n    iterator=AudioSplitter.create(\n        audio=video_index.audio_extract,\n        chunk_duration_sec=30.0,\n        overlap_sec=2.0,\n        min_chunk_duration_sec=5.0\n    )\n)\n\n# Audio-to-text for chunks\nchunks_view.add_computed_column(\n    transcription=openai.transcriptions(audio=chunks_view.audio_chunk, model='whisper-1')\n)\n\n# Create view that chunks text into sentences\ntranscription_chunks = pxt.create_view(\n    f'{directory}.video_sentence_chunks',\n    chunks_view,\n    iterator=StringSplitter.create(text=chunks_view.transcription.text, separators='sentence'),\n)\n\n# Create embedding index for audio\ntranscription_chunks.add_embedding_index('text', string_embed=EMBED_MODEL)\n```\n\n----------------------------------------\n\nTITLE: Using a Table UDF in Pixeltable\nDESCRIPTION: Demonstrates how to use a table UDF in a computed column, where the UDF can be called like any other function. This allows for modularity and reuse of complex data processing workflows.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Use like any other UDF\nresult_table.add_computed_column(\n    result=finance_agent_udf(result_table.prompt)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Advanced PDF Search Functions with Metadata in Pixeltable\nDESCRIPTION: Shows how to implement more sophisticated search functionality that includes metadata and similarity thresholds. This query function returns document chunks with similarity scores, document names, and page numbers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef search_with_metadata(\n    query: str,\n    min_similarity: float,\n    limit: int\n):\n    sim = documents_chunks.text.similarity(query)\n    return (\n        documents_chunks.where(sim >= min_similarity)\n        .order_by(sim, asc=False)\n        .select(\n            documents_chunks.text,\n            documents_chunks.pdf_name,\n            documents_chunks.page_number,\n            similarity=sim\n        )\n        .limit(limit)\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing the Chat Application in Pixeltable\nDESCRIPTION: Implements a chat function that processes user messages through the memory-enabled chatbot. It connects to the previously defined tables, stores conversation history, and demonstrates the chatbot's memory capabilities with example interactions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/memory.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom datetime import datetime\n\n# Connect to your app\nmemory = pxt.get_table(\"chatbot.memory\")\nchat_session = pxt.get_table(\"chatbot.chat_session\")\n\ndef chat(message: str) -> str:\n    \"\"\"Process a message through the memory-enabled chatbot\"\"\"\n    # Store user message\n    memory.insert([{\n        \"role\": \"user\",\n        \"content\": message,\n        \"timestamp\": datetime.now()\n    }])\n\n    # Process through chat session\n    chat_session.insert([{\n        \"user_message\": message,\n        \"timestamp\": datetime.now()\n    }])\n\n    # Get response\n    result = chat_session.select(\n        chat_session.assistant_response\n    ).where(\n        chat_session.user_message == message\n    ).collect()\n    \n    response = result[\"assistant_response\"][0]\n\n    # Store assistant response\n    memory.insert([{\n        \"role\": \"assistant\",\n        \"content\": response,\n        \"timestamp\": datetime.now()\n    }])\n\n    return response\n\n# Use it!\nresponses = [\n    chat(\"Hi! My name is Alice.\"),\n    chat(\"What's the weather like today?\"),\n    chat(\"Can you remember my name?\"),\n]\n\n# Print responses\nfor i, response in enumerate(responses, 1):\n    print(f\"\\nExchange {i}:\")\n    print(f\"Bot: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenAI LLMs with Pixeltable\nDESCRIPTION: This snippet demonstrates how to use OpenAI's language models within Pixeltable by creating a computed column that calls the OpenAI API. It shows how to assemble prompts and add a column that stores model responses for each row.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import openai\n\n# Assemble the prompt and instructions\nmessages = [\n    {\n        'role': 'system',\n        'content': 'Please read the following passages.'\n    },\n    {\n        'role': 'user',\n        'content': t.prompt # generated from the 'prompt' column\n    }\n]\n\n# Add a computed column that calls OpenAI\nt.add_computed_column(\n    response=openai.chat_completions(model='gpt-4o-mini',\n    messages=messages)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Advanced Search Functions in Pixeltable\nDESCRIPTION: Defines a specialized search function that includes metadata and similarity thresholds. This advanced query allows for more refined and customizable content searches within the Pixeltable system.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef search_with_metadata(\n    query: str,\n    min_similarity: float,\n    limit: int\n):\n    sim = websites_chunks.text.similarity(query)\n    return (\n        websites_chunks.where(sim >= min_similarity)\n        .order_by(sim, asc=False)\n        .select(\n            websites_chunks.text,\n            websites_chunks.website,\n            similarity=sim\n        )\n        .limit(limit)\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Audio Processing Workflow in Pixeltable\nDESCRIPTION: Sets up the complete audio processing pipeline including table creation, transcription workflow, sentence splitting, and embedding configuration for search functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import whisper\nfrom pixeltable.functions.huggingface import sentence_transformer\nfrom pixeltable.iterators.string import StringSplitter\nimport spacy\n\n# Initialize spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize app structure\npxt.drop_dir(\"audio_search\", force=True)\npxt.create_dir(\"audio_search\")\n\n# Create audio table\naudio_t = pxt.create_table(\n    \"audio_search.audio\", \n    {\"audio_file\": pxt.Audio}\n)\n\n# Add transcription workflow\naudio_t.add_computed_column(\n    transcription=whisper.transcribe(\n        audio=audio_t.audio_file, \n        model=\"base.en\"\n    )\n)\n\n# Create sentence-level view\nsentences_view = pxt.create_view(\n    \"audio_search.audio_sentence_chunks\",\n    audio_t,\n    iterator=StringSplitter.create(\n        text=audio_t.transcription.text, \n        separators=\"sentence\"\n    )\n)\n\n# Configure embedding model\nembed_model = sentence_transformer.using(\n    model_id=\"intfloat/e5-large-v2\"\n)\n\n# Add search capability\nsentences_view.add_embedding_index(\n    column=\"text\", \n    string_embed=embed_model\n)\n\n# Define search query\n@pxt.query\ndef top_k(query_text: str):\n    sim = sentences_view.text.similarity(query_text)\n    return (\n        sentences_view.order_by(sim, asc=False)\n        .select(sentences_view.text, sim=sim)\n        .limit(10)\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Document Chunking Views for Efficient Processing\nDESCRIPTION: This snippet shows how to handle large documents by creating a view that chunks text data for more efficient processing. It uses the DocumentSplitter iterator to break documents into sentence-level chunks with a specified maximum size.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Optimize large-scale data processing\nfrom pixeltable.iterators import DocumentSplitter\n\n# Create chunked views for efficient processing\ndoc_chunks = pxt.create_view(\n    'chunks',\n    analysis,\n    iterator=DocumentSplitter.create(\n        document=analysis.document,\n        separators='sentence',\n        limit=500  # Control chunk size\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Search Capability to Documents in Pixeltable\nDESCRIPTION: Shows how to add vector search capabilities to document chunks using embedding indexes. This enables natural language searching through documents by calculating vector similarity.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocuments_chunks.add_embedding_index(\n    column=\"text\",\n    string_embed=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Using Pixeltable Tables and Views in Applications\nDESCRIPTION: This snippet shows how to connect to existing tables and views, insert data into a base table, and then query the view that automatically processes that data. When documents are inserted into the base table, the view updates automatically to include the chunked content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your base table and view\ndocuments = pxt.get_table(\"documents.collection\")\nchunks = pxt.get_table(\"documents.chunks\")\n\n# Insert data into base table - view updates automatically\ndocuments.insert([{\n    \"document\": \"path/to/document.pdf\"\n}])\n\n# Query the view\nprint(chunks.collect())\n```\n\n----------------------------------------\n\nTITLE: Running Evaluations with Pixeltable\nDESCRIPTION: Python script that demonstrates how to use the defined evaluation structure to test AI responses against specific criteria. It includes example prompts with evaluation standards and prints the evaluation results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\ndef run_evaluation():\n    # Connect to your app\n    conversations = pxt.get_table(\"evaluations.conversations\")\n\n    # Example prompts with evaluation criteria\n    test_cases = [\n        {\n            \"prompt\": \"Write a haiku about dogs.\",\n            \"expected_criteria\": \"\"\"\n            The response should:\n            1) Follow 5-7-5 syllable pattern\n            2) Be about dogs\n            3) Use vivid imagery\n            \"\"\"\n        },\n        {\n            \"prompt\": \"Explain quantum computing to a 10-year-old.\",\n            \"expected_criteria\": \"\"\"\n            The response should:\n            1) Use age-appropriate language\n            2) Use relevant analogies\n            3) Be engaging and clear\n            \"\"\"\n        }\n    ]\n\n    # Insert test cases\n    conversations.insert(test_cases)\n\n    # Get results with evaluations\n    results = conversations.select(\n        conversations.prompt,\n        conversations.answer,\n        conversations.evaluation,\n        conversations.score\n    ).collect().to_pandas()\n\n    # Print results\n    for idx, row in results.iterrows():\n        print(f\"\\nTest Case {idx + 1}\")\n        print(\"=\" * 50)\n        print(f\"Prompt: {row['prompt']}\")\n        print(f\"\\nResponse: {row['answer']}\")\n        print(f\"\\nEvaluation:\\n{row['evaluation']}\")\n        print(f\"Score: {row['score']}\")\n        print(\"=\" * 50)\n\nif __name__ == \"__main__\":\n    run_evaluation()\n```\n\n----------------------------------------\n\nTITLE: Initializing Multimodal Table and RAG Components in Pixeltable\nDESCRIPTION: Demonstrates creation of a multimodal table for RAG applications with document chunking and embedding index setup. Handles multiple data types including documents, video, and audio.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import DocumentSplitter\n\n# Create multimodal table for RAG\ndocs = pxt.create_table('chatbot.documents', {\n    'document': pxt.Document,  # PDF/Text files\n    'video': pxt.Video,        # MP4 videos  \n    'audio': pxt.Audio,        # Audio files\n    'timestamp': pxt.Timestamp\n})\n\n# Create view for document chunking\nchunks = pxt.create_view(\n    'chatbot.chunks',\n    docs,\n    iterator=DocumentSplitter.create(\n        document=docs.document,\n        separators='sentence',\n        metadata='title,heading'\n    )\n)\n\n# Add embedding index for search\nchunks.add_embedding_index(\n    'text',\n    string_embed=sentence_transformer\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a RAG Pipeline with DocumentSplitter in Python\nDESCRIPTION: This example demonstrates a Retrieval-Augmented Generation (RAG) pipeline using DocumentSplitter to create chunks and add embeddings for efficient retrieval.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create document chunks\nchunks = pxt.create_view(\n    'rag.chunks',\n    docs_table,\n    iterator=DocumentSplitter.create(\n        document=docs_table.document,\n        separators='paragraph',\n        limit=500\n    )\n)\n\n# Add embeddings\nchunks.add_embedding_index(\n    'text',\n    string_embed=sentence_transformer.using(\n        model_id='all-mpnet-base-v2'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Structure with Pixeltable\nDESCRIPTION: Python script that sets up the data schema, defines evaluation criteria, and configures the processing workflow for AI response evaluation using OpenAI models and Pixeltable's computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import openai\n\n# Initialize app structure\npxt.drop_dir(\"evaluations\", force=True)\npxt.create_dir(\"evaluations\")\n\n# Define data schema with evaluation criteria\nconversations = pxt.create_table(\n    \"evaluations.conversations\", \n    {\n        \"prompt\": pxt.String,\n        \"expected_criteria\": pxt.String\n    }, \n    if_exists=\"ignore\"\n)\n\n# Configure processing workflow\nconversations.add_computed_column(\n    messages=[{\"role\": \"user\", \"content\": conversations.prompt}]\n)\n\nconversations.add_computed_column(\n    response=openai.chat_completions(\n        messages=conversations.messages,\n        model=\"gpt-4o-mini\",\n    )\n)\n\nconversations.add_computed_column(\n    answer=conversations.response.choices[0].message.content\n)\n\n# Add judge evaluation workflow\njudge_prompt_template = \"\"\"\nYou are an expert judge evaluating AI responses. Your task is to evaluate the following response based on the given criteria.\n\nOriginal Prompt: {prompt}\nExpected Criteria: {criteria}\nAI Response: {response}\n\nPlease evaluate the response on a scale of 1-10 and provide a brief explanation.\nFormat your response as:\nScore: [1-10]\nExplanation: [Your explanation]\n\"\"\"\n\nconversations.add_computed_column(\n    judge_prompt=judge_prompt_template.format(\n        prompt=conversations.prompt,\n        criteria=conversations.expected_criteria,\n        response=conversations.answer\n    )\n)\n\nconversations.add_computed_column(\n    judge_response=openai.chat_completions(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert judge evaluating AI responses.\"},\n            {\"role\": \"user\", \"content\": conversations.judge_prompt}\n        ],\n        model=\"gpt-4o-mini\",\n    )\n)\n\nconversations.add_computed_column(\n    evaluation=conversations.judge_response.choices[0].message.content\n)\n\n# Add score extraction\n@pxt.udf\ndef extract_score(evaluation: str) -> float:\n    try:\n        score_line = [line for line in evaluation.split('\\n') if line.startswith('Score:')][0]\n        return float(score_line.split(':')[1].strip())\n    except:\n        return 0.0\n\nconversations.add_computed_column(\n    score=extract_score(conversations.evaluation)\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Media Data from Cloud Storage in Pixeltable\nDESCRIPTION: This snippet demonstrates how to import video data from cloud storage (S3) into Pixeltable. It creates a table with a Video column type and inserts data by referencing S3 paths, allowing Pixeltable to handle the media files directly.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Import media data (videos, images, audio...)\nv = pxt.create_table('videos', {'video': pxt.Video})\n\nprefix = 's3://multimedia-commons/'\npaths = [\n    'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',\n    'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',\n    'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'\n]\nv.insert({'video': prefix + p} for p in paths)\n```\n\n----------------------------------------\n\nTITLE: Object Detection with DETR Model\nDESCRIPTION: Shows how to perform object detection on images using the DETR model from Hugging Face. Creates a table for images, adds computed columns for classification, and queries for specific objects.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import huggingface\n\n# Create a table to store data persistently\nt = pxt.create_table('image', {'image': pxt.Image})\n\n# Insert some images\nprefix = 'https://upload.wikimedia.org/wikipedia/commons'\npaths = [\n    '/1/15/Cat_August_2010-4.jpg',\n    '/e/e1/Example_of_a_Dog.jpg',\n    '/thumb/b/bf/Bird_Diversity_2013.png/300px-Bird_Diversity_2013.png'\n]\nt.insert({'image': prefix + p} for p in paths)\n\n# Add a computed column for image classification\nt.add_computed_column(classification=huggingface.detr_for_object_detection(\n    t.image,\n    model_id='facebook/detr-resnet-50'\n))\n\n# Retrieve the rows where cats have been identified\nt.select(animal = t.image,\n         classification = t.classification.label_text[0]) \\\n.where(t.classification.label_text[0]=='cat').head()\n```\n\n----------------------------------------\n\nTITLE: Extracting Audio and Frames from Video in Pixeltable\nDESCRIPTION: This code snippet demonstrates how to extract both audio and visual content from videos using Pixeltable. It adds a computed column for audio extraction and creates a view for frame extraction at a specified frame rate.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Extract audio from video\nvideo_index.add_computed_column(audio_extract=extract_audio(video_index.video, format='mp3'))\n\n# Extract frames from video\nframes_view = pxt.create_view(\n    f'{directory}.video_frames',\n    video_index,\n    iterator=FrameIterator.create(\n        video=video_index.video,\n        fps=1\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Computer Vision with YOLOX in Pixeltable\nDESCRIPTION: This snippet shows how to integrate computer vision capabilities using the YOLOX object detection model. It creates a computed column that processes images/frames and stores detection results, which can then be queried and displayed.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.ext.functions.yolox import yolox\n\n# compute object detections using the `yolox_tiny` model\nframes_view.add_computed_column(detect_yolox_tiny=yolox(\n    frames_view.frame, model_id='yolox_tiny', threshold=0.25\n))\n\n# The inference in the computed column is now stored\nframes_view.select(\n    frames_view.frame,\n    frames_view.detect_yolox_tiny\n).show(3)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Workflow in Pixeltable\nDESCRIPTION: Demonstrates complete RAG implementation including chunk creation, embedding indexing, context retrieval, and response generation using LLMs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create chunks view \nchunks = pxt.create_view(\n    'chatbot.chunks',\n    docs,\n    iterator=DocumentSplitter.create(\n        document=docs.document,\n        separators='sentence',\n        metadata='title,heading'\n    )\n)\n\n# Add embedding index\nchunks.add_embedding_index(\n    'text', \n    string_embed=sentence_transformer\n)\n\n# Define context retrieval query\n@chunks.query\ndef get_context(query: str):\n    sim = chunks.text.similarity(query)\n    return chunks.order_by(sim, asc=False).limit(5)\n\n# Generate response with context\ndocs.add_computed_column(\n    context=get_context(docs.question)\n)\n\ndocs.add_computed_column(\n    response=openai.chat_completions(\n        messages=create_prompt(\n            docs.context,\n            docs.question\n        ),\n        model='gpt-4o'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Changing Evaluation Model in Pixeltable\nDESCRIPTION: Code snippet demonstrating how to select different OpenAI models for generating AI responses and evaluations in the Pixeltable pipeline.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconversations.add_computed_column(\n    judge_response=openai.chat_completions(\n        messages=messages,\n        model=\"your-chosen-model\"  # Change model here\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Basic BERT Embedding Implementation in Python\nDESCRIPTION: Simple example showing how to create a custom BERT embedding function using TensorFlow Hub and integrate it with Pixeltable. Demonstrates basic model loading and embedding generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/embedding-model.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport pixeltable as pxt\n\n@pxt.udf\ndef custom_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"Basic BERT embedding function\"\"\"\n    preprocessor = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n    model = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2')\n    \n    tensor = tf.constant([text])\n    result = model(preprocessor(tensor))['pooled_output']\n    return result.numpy()[0, :]\n\n# Create table and add embedding index\ndocs = pxt.create_table('documents', {'text': pxt.String})\ndocs.add_embedding_index('text', string_embed=custom_bert_embed)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Batched UDF in Pixeltable\nDESCRIPTION: A function that processes multiple rows at once by identifying the longest word in each sentence. It uses the Batch type and batch_size parameter to process 16 sentences at a time for improved performance.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.func import Batch\n\n@pxt.udf(batch_size=16)\ndef longest_word(sentences: Batch[str], strip_punctuation: bool = False) -> Batch[str]:\n    results = []\n    for sentence in sentences:\n        words = sentence.split()\n        if strip_punctuation:\n            words = [\n                word if word[-1].isalnum() else word[:-1]\n                for word in words\n            ]\n        i = np.argmax([len(word) for word in words])\n        results.append(words[i])\n    return results\n```\n\n----------------------------------------\n\nTITLE: Extracting Values from JSON in Pixeltable using Python\nDESCRIPTION: Function for extracting values from JSON strings or dictionaries using a path specified in dot notation. The function handles both string and dictionary inputs, and can navigate through nested JSON structures to retrieve specific values.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/json.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport typing as t\nfrom pixeltable.func import Function, column_function\nfrom pixeltable.type_system import StringType, JsonType, Type\n\n\n@column_function(return_type_fn=lambda *args: args[1], display_name=\"json_extract_path\")\ndef extract_path(\n    json_val: t.Union[str, dict], result_type: Type, path: str\n) -> t.Union[str, int, float, bool, dict, list, None]:\n    \"\"\"\n    Extract a value from a JSON string at the given path.\n\n    Args:\n        json_val: JSON string or dictionary\n        result_type: return type\n        path: path to the value to extract, in dot notation (e.g., 'a.b.c')\n\n    Returns:\n        The extracted value, or None if the path doesn't exist or the value is null\n    \"\"\"\n    if json_val is None:\n        return None\n\n    if isinstance(json_val, str):\n        try:\n            json_val = json.loads(json_val)\n        except json.JSONDecodeError:\n            return None\n\n    # Split the path into components\n    components = path.split(\".\")\n\n    # Traverse the JSON object\n    current = json_val\n    for component in components:\n        if isinstance(current, dict) and component in current:\n            current = current[component]\n        else:\n            return None\n\n    return current\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Search Functions with Similarity Threshold\nDESCRIPTION: Advanced example implementing a custom search function with a configurable similarity threshold. This allows for more precise control over search results by filtering based on a minimum similarity score.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef search_with_threshold(query: str, min_similarity: float):\n    sim = img_t.image_description.similarity(query)\n    return (\n        img_t.where(sim >= min_similarity)\n        .order_by(sim, asc=False)\n        .select(\n            img_t.image,\n            img_t.image_description,\n            similarity=sim\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Customizing Evaluation Criteria in Pixeltable\nDESCRIPTION: Example code snippet showing how to customize the evaluation criteria for different test cases. This allows for tailored assessment based on specific requirements for each prompt.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntest_case = {\n    \"prompt\": \"Your prompt here\",\n    \"expected_criteria\": \"\"\"\n    The response should:\n    1) [Your first criterion]\n    2) [Your second criterion]\n    3) [Your third criterion]\n    \"\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Text and Image Similarity Search on Video Frames in Pixeltable\nDESCRIPTION: This code demonstrates how to perform text and image similarity search on video frames using Pixeltable. It covers creating a video table, extracting frames, adding an embedding index for similarity search, and querying frames based on both image and text inputs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.huggingface import clip\nfrom pixeltable.iterators import FrameIterator\nimport PIL.Image\n\nvideo_table = pxt.create_table('videos', {'video': pxt.Video})\n\nvideo_table.insert([{'video': '/video.mp4'}])\n\nframes_view = pxt.create_view(\n    'frames', video_table, iterator=FrameIterator.create(video=video_table.video))\n\n# Create an index on the 'frame' column that allows text and image search\nframes_view.add_embedding_index('frame', embed=clip.using('openai/clip-vit-base-patch32'))\n\n# Now we will retrieve images based on a sample image\nsample_image = '/image.jpeg'\nsim = frames_view.frame.similarity(sample_image)\nframes_view.order_by(sim, asc=False).limit(5).select(frames_view.frame, sim=sim).collect()\n\n# Now we will retrieve images based on a string\nsample_text = 'red truck'\nsim = frames_view.frame.similarity(sample_text)\nframes_view.order_by(sim, asc=False).limit(5).select(frames_view.frame, sim=sim).collect()\n```\n\n----------------------------------------\n\nTITLE: Vision processing with Gemini Pro Vision model\nDESCRIPTION: Provides a function to analyze images using the Gemini Pro Vision model. It accepts an image and prompt, then returns the model's textual interpretation or analysis of the image content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/gemini.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@registry.function(\n    name='gemini.generate_vision',\n    return_type=types.String,\n    param_types={\n        'image': types.Image,\n        'prompt': types.String,\n        'temperature': types.Float,\n        'top_p': types.Float,\n        'top_k': types.Int,\n        'max_output_tokens': types.Int\n    },\n    param_defaults={\n        'prompt': 'Describe this image in detail',\n        'temperature': 1.0,\n        'top_p': 0.95,\n        'top_k': 64,\n        'max_output_tokens': 8192\n    }\n)\ndef generate_vision(\n        image: bytes,\n        prompt: str = 'Describe this image in detail',\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        top_k: int = 64,\n        max_output_tokens: int = 8192\n) -> str:\n    \"\"\"\n    Generate text from an image using the Google Gemini Pro Vision model.\n\n    Args:\n        image: The image to process\n        prompt: The text prompt to guide the vision model\n        temperature: Controls the randomness of the output. Range: [0.0, 1.0]\n        top_p: The cumulative probability cutoff for token selection. Range: [0.0, 1.0]\n        top_k: The number of highest probability tokens to consider for each selection. Range: [1, 100]\n        max_output_tokens: Maximum number of tokens to generate\n\n    Returns:\n        Generated text description as a string\n    \"\"\"\n    _setup_api_key()\n\n    try:\n        model = genai.GenerativeModel('gemini-pro-vision')\n        response = model.generate_content(\n            [\n                prompt,\n                {'mime_type': 'image/jpeg', 'data': image}\n            ],\n            generation_config={\n                'temperature': temperature,\n                'top_p': top_p,\n                'top_k': top_k,\n                'max_output_tokens': max_output_tokens,\n            }\n        )\n        return response.text\n    except Exception as e:\n        raise ReturnValueError(f'Error generating vision output with Gemini: {str(e)}')\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Chunking Strategies in Pixeltable\nDESCRIPTION: Shows how to implement different document chunking strategies in Pixeltable, including paragraph-based and fixed-size chunking approaches. This allows for flexibility in how documents are processed based on specific needs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Chunk by paragraphs\nchunks_by_para = pxt.create_view(\n    \"pdf_search.para_chunks\",\n    documents_t,\n    iterator=DocumentSplitter.create(\n        document=documents_t.pdf,\n        separators=\"paragraph\"\n    )\n)\n\n# Chunk by fixed size\nchunks_by_size = pxt.create_view(\n    \"pdf_search.size_chunks\",\n    documents_t,\n    iterator=DocumentSplitter.create(\n        document=documents_t.pdf,\n        separators=\"fixed\",\n        size=1000  # characters\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting Updated Chunk Retrieval Results in Pixeltable\nDESCRIPTION: This snippet collects and displays the results of a chunk retrieval query for a specific question after updating the document base.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nnvidia_eps_query.collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with MistralAI in Python\nDESCRIPTION: This function creates embeddings for a given input text using MistralAI's embedding model. It returns a list of floats representing the embedding vector.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/mistralai.md#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef mistral_embedding(text: str) -> List[float]:\n    client = _get_mistral_client()\n    embeddings = client.embeddings(\n        model=\"mistral-embed\",\n        input=[text]\n    )\n    return embeddings.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from Video Frames in Python\nDESCRIPTION: This function performs Optical Character Recognition (OCR) on video frames to extract text. It takes a list of video frames as input and returns a list of extracted text strings corresponding to each frame.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/video.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text(frames: List[np.ndarray]) -> List[str]:\n```\n\n----------------------------------------\n\nTITLE: Configuring Semantic Search Capabilities\nDESCRIPTION: Sets up embedding indexes and search queries for semantic search across different media types using sentence transformers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nchunks_view.add_embedding_index(\n    \"text\",\n    string_embed=sentence_transformer.using(model_id=\"intfloat/e5-large-v2\")\n)\n\ntranscription_chunks.add_embedding_index(\n    \"text\",\n    string_embed=sentence_transformer.using(model_id=\"intfloat/e5-large-v2\")\n)\n\naudio_chunks.add_embedding_index(\n    \"text\",\n    string_embed=sentence_transformer.using(model_id=\"intfloat/e5-large-v2\")\n)\n\n@chunks_view.query\ndef get_relevant_chunks(query_text: str):\n    sim = chunks_view.text.similarity(query_text)\n    return (\n        chunks_view.order_by(sim, asc=False)\n        .select(chunks_view.text, sim=sim)\n        .limit(20)\n    )\n\n@transcription_chunks.query\ndef get_relevant_transcript_chunks(query_text: str):\n    sim = transcription_chunks.text.similarity(query_text)\n    return (\n        transcription_chunks.order_by(sim, asc=False)\n        .select(transcription_chunks.text, sim=sim)\n        .limit(20)\n    )\n\n@audio_chunks.query\ndef get_relevant_audio_chunks(query_text: str):\n    sim = audio_chunks.text.similarity(query_text)\n    return (\n        audio_chunks.order_by(sim, asc=False)\n        .select(audio_chunks.text, sim=sim)\n        .limit(20)\n    )\n```\n\n----------------------------------------\n\nTITLE: DETR Object Detection Implementation\nDESCRIPTION: Demonstrates object detection using DETR models with optional COCO format conversion.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import detr_for_object_detection\n\nt.add_computed_column(\n    detections=detr_for_object_detection(\n        t.image,\n        model_id='facebook/detr-resnet-50',\n        threshold=0.8\n    )\n)\n\n# Convert to COCO format if needed\nt.add_computed_column(\n    coco_format=detr_to_coco(t.image, t.detections)\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Core Data Structures in Pixeltable\nDESCRIPTION: Sets up the fundamental table structure for the chat application, defining tables for documents and conversations with appropriate column types for handling multiple media formats.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import openai\nfrom pixeltable.functions.huggingface import sentence_transformer\nfrom pixeltable.functions.video import extract_audio\nfrom pixeltable.iterators import DocumentSplitter\nfrom pixeltable.iterators.string import StringSplitter\n\npxt.drop_dir(\"chatbot\", force=True)\npxt.create_dir(\"chatbot\")\n\ndocs_table = pxt.create_table(\n    \"chatbot.documents\",\n    {\n        \"document\": pxt.Document,\n        \"video\": pxt.Video,\n        \"audio\": pxt.Audio,\n        \"question\": pxt.String,\n    }\n)\n\nconversations = pxt.create_table(\n    \"chatbot.conversations\",\n    {\n        \"role\": pxt.String,\n        \"content\": pxt.String,\n        \"timestamp\": pxt.Timestamp\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Agent Memory and Tool History in Pixeltable\nDESCRIPTION: Code showing how to access an agent's conversation memory and tool call history stored in Pixeltable tables. This demonstrates how Pixeltable handles persistent storage of agent interactions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Agent memory is automatically persisted\nmemory = pxt.get_table(\"my_assistant.memory\")\nconversations = memory.collect()\n\n# Access tool call history\ntools_log = pxt.get_table(\"financial_assistant.tools\")\ntool_history = tools_log.collect()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Processing Functions\nDESCRIPTION: Defines user-defined functions for managing chat history, message creation, and context generation from multiple media sources.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@conversations.query\ndef get_chat_history():\n    return conversations.order_by(\n        conversations.timestamp\n    ).select(\n        role=conversations.role,\n        content=conversations.content\n    )\n\n@pxt.udf\ndef create_messages(history: list[dict], prompt: str) -> list[dict]:\n    messages = [{\n        'role': 'system',\n        'content': 'You are a helpful AI assistant maintaining conversation context.'\n    }]\n    messages.extend({\n        'role': msg['role'],\n        'content': msg['content']\n    } for msg in history)\n    messages.append({\n        'role': 'user',\n        'content': prompt\n    })\n    return messages\n\n@pxt.udf\ndef create_prompt(\n    doc_context: list[dict],\n    video_context: list[dict],\n    audio_context: list[dict],\n    question: str\n) -> str:\n    context_parts = []\n    if doc_context:\n        context_parts.append(\n            \"Document Context:\\n\" + \"\\n\\n\".join(\n                item[\"text\"] for item in doc_context if item and \"text\" in item\n            )\n        )\n    if video_context:\n        context_parts.append(\n            \"Video Context:\\n\" + \"\\n\\n\".join(\n                item[\"text\"] for item in video_context if item and \"text\" in item\n            )\n        )\n    if audio_context:\n        context_parts.append(\n            \"Audio Context:\\n\" + \"\\n\\n\".join(\n                item[\"text\"] for item in audio_context if item and \"text\" in item\n            )\n        )\n    full_context = \"\\n\\n---\\n\\n\".join(context_parts) if context_parts else \"No relevant context found.\"\n    return f\"Context:\\n{full_context}\\n\\nQuestion:\\n{question}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Context-aware Chatbot with Pixeltable\nDESCRIPTION: Creates a context-aware chatbot using Pixeltable. It sets up memory and chat session tables, defines functions for memory retrieval and message creation, and configures a processing workflow using OpenAI's GPT model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom datetime import datetime\nfrom typing import List, Dict\n\n# Initialize app structure\npxt.drop_dir(\"chatbot\", force=True)\npxt.create_dir(\"chatbot\")\n\n# Create memory table\nmemory = pxt.create_table(\n    \"chatbot.memory\",\n    {\n        \"role\": pxt.String,\n        \"content\": pxt.String,\n        \"timestamp\": pxt.Timestamp,\n    },\n    if_exists=\"ignore\",\n)\n\n# Create chat session table\nchat_session = pxt.create_table(\n    \"chatbot.chat_session\",\n    {\"user_message\": pxt.String, \"timestamp\": pxt.Timestamp},\n    if_exists=\"ignore\",\n)\n\n# Define memory retrieval\n@pxt.query\ndef get_recent_memory():\n    return (\n        memory.order_by(memory.timestamp, asc=False)\n        .select(role=memory.role, content=memory.content)\n        .limit(10)\n    )\n\n# Define message creation\n@pxt.udf\ndef create_messages(past_context: List[Dict], current_message: str) -> List[Dict]:\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a chatbot with memory capabilities.\",\n        }\n    ]\n    messages.extend(\n        [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in past_context]\n    )\n    messages.append({\"role\": \"user\", \"content\": current_message})\n    return messages\n\n# Configure processing workflow\nchat_session.add_computed_column(memory_context=get_recent_memory())\nchat_session.add_computed_column(\n    prompt=create_messages(chat_session.memory_context, chat_session.user_message)\n)\nchat_session.add_computed_column(\n    llm_response=pxt.functions.openai.chat_completions(\n        messages=chat_session.prompt,\n        model=\"gpt-4o-mini\"\n    )\n)\nchat_session.add_computed_column(\n    assistant_response=chat_session.llm_response.choices[0].message.content\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Table Structure in Pixeltable\nDESCRIPTION: Example showing how to query a table's structure to view specialized image and array types. The table 't' contains columns with specialized types like pxt.Array[(512,), pxt.Float] for clip embeddings and Image['L'] for channel images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nt\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Tables with Embedding Indexes in Pixeltable\nDESCRIPTION: This snippet demonstrates different ways to insert data into Pixeltable tables with embedding indexes. It shows single insertion, batch insertion for improved performance, and how to insert images. Pixeltable automatically computes embeddings for the indexed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Single insertion\ndocs.insert({\n    \"content\": \"Your document text here\",\n    \"metadata\": {\"source\": \"web\", \"category\": \"tech\"}\n})\n\n# Batch insertion\ndocs.insert([\n    {\n        \"content\": \"First document\",\n        \"metadata\": {\"source\": \"pdf\", \"category\": \"science\"}\n    },\n    {\n        \"content\": \"Second document\", \n        \"metadata\": {\"source\": \"web\", \"category\": \"news\"}\n    }\n])\n\n# Image insertion\nimage_urls = [\n    'https://example.com/image1.jpg',\n    'https://example.com/image2.jpg'\n]\nimages.insert({'image': url} for url in image_urls)\n```\n\n----------------------------------------\n\nTITLE: Customizing GPT-4 Vision Prompts for Detailed Analysis\nDESCRIPTION: Advanced example showing how to customize the GPT-4 Vision prompts for more detailed image analysis. This allows for specialized image descriptions focused on specific aspects like objects, colors, and text.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimg_t.add_computed_column(\n    detailed_analysis=vision(\n        prompt=\"\"\"Analyze this image in detail:\n        1. Main objects and their positions\n        2. Color palette\n        3. Lighting and atmosphere\n        4. Any text or symbols present\"\"\",\n        image=img_t.image,\n        model=\"gpt-4o-mini\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Schema with Media Types\nDESCRIPTION: Illustrates how to create a table schema with media data types including Image, Video, Audio, and Document.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Media handling\nmedia = pxt.create_table('media', {\n    'image': pxt.Image[(224, 224), 'RGB'],  # With size & mode\n    'video': pxt.Video,                     # Video reference\n    'audio': pxt.Audio,                     # Audio file\n    'document': pxt.Document                # PDF/text doc\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completions with Together AI in Pixeltable\nDESCRIPTION: This snippet demonstrates how to create a table for chat completions using Pixeltable, integrating with the Together AI API. It shows how to structure input messages, add computed columns for API responses, and insert conversation entries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nchat_table = pxt.create_table('together_demo.chat', {'input': pxt.String})\n\n# The chat-completions API expects JSON-formatted input:\nmessages = [{'role': 'user', 'content': chat_table.input}]\n\n# This example shows how additional parameters from the Together API can be used in Pixeltable\nchat_table.add_computed_column(\n    output=chat_completions(\n        messages=messages,\n        model='mistralai/Mixtral-8x7B-Instruct-v0.1',\n        max_tokens=300,\n        stop=['\\n'],\n        temperature=0.7,\n        top_p=0.9,\n        top_k=40,\n        repetition_penalty=1.1,\n        logprobs=1,\n        echo=True\n    )\n)\nchat_table.add_computed_column(\n    response=chat_table.output.choices[0].message.content\n)\n\n# Start a conversation\nchat_table.insert([\n    {'input': 'How many species of felids have been classified?'},\n    {'input': 'Can you make me a coffee?'}\n])\nchat_table.select(chat_table.input, chat_table.response).head()\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedding Index with CLIP in Pixeltable\nDESCRIPTION: Adds an embedding index to the 'img' column using a CLIP model from Hugging Face. This enables similarity searches on the image data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'img' column\nimgs.add_embedding_index(\n    'img',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CLIP Embeddings to Chunks View in Pixeltable\nDESCRIPTION: Adds a computed column 'clip_embed' to the 'chunks' view using the CLIP model 'openai/clip-vit-base-patch32'.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\n\nchunks.add_computed_column(clip_embed=clip(\n    chunks.text, model_id='openai/clip-vit-base-patch32'\n))\n```\n\n----------------------------------------\n\nTITLE: Using Pixeltable App for Image Analysis and Voxel51 Visualization\nDESCRIPTION: Demonstrates how to use the defined Pixeltable workflow, insert images, export results to Voxel51 format, and launch the Voxel51 visualization app.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/voxel51.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nimport fiftyone as fo\n\n# Connect to your table\nimages = pxt.get_table(\"vision.images\")\n\n# Insert some images\nurl_prefix = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/source/data/images'\nurls = [\n    f'{url_prefix}/000000000019.jpg',\n    f'{url_prefix}/000000000025.jpg',\n    f'{url_prefix}/000000000030.jpg',\n    f'{url_prefix}/000000000034.jpg',\n]\n\nimages.insert({'image': url} for url in urls)\n\n# Export to Voxel51 with multiple label sets\nfo_dataset = pxt.io.export_images_as_fo_dataset(\n    images,\n    images.image,\n    classifications=vit_to_fo(images.classifications),\n    detections={\n        'detections_50': detr_to_fo(images.image, images.detections),\n        'detections_101': detr_to_fo(images.image, images.detections_101)\n    }\n)\n\n# Launch Voxel51 visualization\nsession = fo.launch_app(fo_dataset)\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Workflow for Image Analysis\nDESCRIPTION: Creates a Pixeltable workflow for image analysis, including table creation, adding computed columns for image classification and object detection, and defining label conversion functions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/voxel51.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.huggingface import (\n    vit_for_image_classification,\n    detr_for_object_detection\n)\n\n# Initialize app structure\npxt.drop_dir('vision', force=True)\npxt.create_dir('vision')\n\n# Create base table\nimages = pxt.create_table(\n    'vision.images', \n    {'image': pxt.Image},\n    if_exists=\"ignore\"\n)\n\n# Add model inference columns\nimages.add_computed_column(\n    classifications=vit_for_image_classification(\n        images.image, \n        model_id='google/vit-base-patch16-224'\n    )\n)\n\nimages.add_computed_column(\n    detections=detr_for_object_detection(\n        images.image, \n        model_id='facebook/detr-resnet-50'\n    )\n)\n\n# Optional: Add additional model for comparison\nimages.add_computed_column(\n    detections_101=detr_for_object_detection(\n        images.image, \n        model_id='facebook/detr-resnet-101'\n    )\n)\n\n# Define label conversion functions\n@pxt.udf\ndef vit_to_fo(vit_labels: list) -> list:\n    \"\"\"Convert ViT classification output to Voxel51 format\"\"\"\n    return [\n        {'label': label, 'confidence': score}\n        for label, score in zip(\n            vit_labels.label_text, \n            vit_labels.scores\n        )\n    ]\n\n@pxt.udf\ndef detr_to_fo(img: pxt.Image, detr_labels: dict) -> list:\n    \"\"\"Convert DETR detection output to Voxel51 format\"\"\"\n    result = []\n    for label, box, score in zip(\n        detr_labels.label_text, \n        detr_labels.boxes, \n        detr_labels.scores\n    ):\n        # Convert DETR (x1,y1,x2,y2) to Voxel51 (x,y,w,h) format\n        fo_box = [\n            box[0] / img.width,\n            box[1] / img.height,\n            (box[2] - box[0]) / img.width,\n            (box[3] - box[1]) / img.height,\n        ]\n        result.append({\n            'label': label, \n            'bounding_box': fo_box, \n            'confidence': score\n        })\n    return result\n```\n\n----------------------------------------\n\nTITLE: Hybrid Search with Weighted Similarity in Pixeltable\nDESCRIPTION: This snippet demonstrates a hybrid search approach that combines similarity scores from multiple fields. It calculates similarities for both content and title fields, then uses a weighted combination to produce a final score for ranking documents.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef hybrid_search(query: str):\n    # Combine different similarity scores\n    text_sim = docs.content.similarity(query)\n    title_sim = docs.title.similarity(query)\n    \n    # Weighted combination\n    combined_sim = (text_sim * 0.7) + (title_sim * 0.3)\n    \n    return docs.order_by(combined_sim, asc=False).limit(5)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Fireworks Chat Completions Table\nDESCRIPTION: Creating a Pixeltable table with input column and computed column for Fireworks chat completions using the Mixtral model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.fireworks import chat_completions\n\n# Create a table in Pixeltable and pick a model hosted on Fireworks with some parameters\n\nt = pxt.create_table('fireworks_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=chat_completions(\n    messages=messages,\n    model='accounts/fireworks/models/mixtral-8x22b-instruct',\n    # These parameters are optional and can be used to tune model behavior:\n    max_tokens=300,\n    top_k=40,\n    top_p=0.9,\n    temperature=0.7\n))\n```\n\n----------------------------------------\n\nTITLE: Using the Pixeltable Application for Image Analysis\nDESCRIPTION: Demonstrates how to insert an image into the table and retrieve analysis results. The query engine uses lazy evaluation, only computing what's needed when results are collected.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Insert an image\nt.insert(input_image='https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')\n\n# Retrieve results\nt.select(\n    t.input_image,\n    t.detections_text,\n    t.vision\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Configuring Smart Chunking in Pixeltable\nDESCRIPTION: Sets up token-aware content splitting for efficient processing of website content. This configuration ensures that content is divided into manageable chunks based on token limits.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\niterator=DocumentSplitter.create(\n    document=websites_t.website,\n    separators=\"token_limit\"\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Text-Based Similarity Search in Python with Pixeltable\nDESCRIPTION: Demonstrates how to perform similarity search using CLIP models by looking up images using text queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsim = imgs.img.similarity('train')  # String lookup\nres = (\n    imgs.order_by(sim, asc=False)\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres\n```\n\n----------------------------------------\n\nTITLE: Creating Unified Embedding Space for Text and Images in Pixeltable\nDESCRIPTION: This code demonstrates how to use the same embedding model for both text transcriptions and image descriptions in Pixeltable. It defines the embedding model once and applies it to both modalities, creating a unified embedding space.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define once, use for both modalities\nEMBED_MODEL = sentence_transformer.using(model_id='intfloat/e5-large-v2')\n\n# Use for frame descriptions\nframes_view.add_embedding_index('image_description', string_embed=EMBED_MODEL)\n\n# Use for transcriptions\ntranscription_chunks.add_embedding_index('text', string_embed=EMBED_MODEL)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Columns to Pixeltable Views\nDESCRIPTION: This example shows how to add computed columns to views. The snippet demonstrates adding an embedding column that uses a sentence transformer model to create vector embeddings from the text chunks in the view.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Add embeddings to chunks\nchunks.add_computed_column(\n    embedding=sentence_transformer.using(\n        model_id='intfloat/e5-large-v2'\n    )(chunks.text)\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Processing Workflows with Computed Columns\nDESCRIPTION: Establishes processing workflows for video, audio, and chat using computed columns to handle media transcription and response generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocs_table.add_computed_column(\n    audio_extract=extract_audio(docs_table.video, format=\"mp3\")\n)\ndocs_table.add_computed_column(\n    transcription=openai.transcriptions(\n        audio=docs_table.audio_extract,\n        model=\"whisper-1\"\n    )\n)\ndocs_table.add_computed_column(\n    transcription_text=docs_table.transcription.text\n)\n\ndocs_table.add_computed_column(\n    audio_transcription=openai.transcriptions(\n        audio=docs_table.audio,\n        model=\"whisper-1\"\n    )\n)\ndocs_table.add_computed_column(\n    audio_transcription_text=docs_table.audio_transcription.text\n)\n\ndocs_table.add_computed_column(\n    context_doc=chunks_view.queries.get_relevant_chunks(docs_table.question)\n)\ndocs_table.add_computed_column(\n    context_video=transcription_chunks.queries.get_relevant_transcript_chunks(docs_table.question)\n)\ndocs_table.add_computed_column(\n    context_audio=audio_chunks.queries.get_relevant_audio_chunks(docs_table.question)\n)\ndocs_table.add_computed_column(\n    prompt=create_prompt(\n        docs_table.context_doc,\n        docs_table.context_video,\n        docs_table.context_audio,\n        docs_table.question\n    )\n)\ndocs_table.add_computed_column(\n    chat_history=conversations.queries.get_chat_history()\n)\ndocs_table.add_computed_column(\n    messages=create_messages(\n        docs_table.chat_history,\n        docs_table.prompt\n    )\n)\ndocs_table.add_computed_column(\n    response=openai.chat_completions(\n        messages=docs_table.messages,\n        model=\"gpt-4o-mini\"\n    )\n)\ndocs_table.add_computed_column(\n    answer=docs_table.response.choices[0].message.content\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for LLM Chat Completions in Pixeltable\nDESCRIPTION: Sets up a Pixeltable directory and creates a table to store input strings for LLM processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import llama_cpp\n\npxt.drop_dir('llama_demo', force=True)\npxt.create_dir('llama_demo')\n\nt = pxt.create_table('llama_demo.chat', {'input': pxt.String})\n```\n\n----------------------------------------\n\nTITLE: Implementing Dual Search Capabilities in Pixeltable\nDESCRIPTION: This snippet illustrates how to perform independent searches across audio and visual content in Pixeltable. It calculates similarity scores for both transcribed audio and image descriptions based on text queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get similarity scores   \naudio_sim = transcription_chunks.text.similarity(\"Definition of happiness according to the guest\")\nimage_sim = frames_view.image_description.similarity(\"Lex Fridman interviewing a guest in a podcast setting\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Encoders for Semantic Similarity\nDESCRIPTION: Shows how to use cross-encoders for computing similarity scores between sentence pairs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import cross_encoder\n\nt.add_computed_column(\n    similarity_score=cross_encoder(\n        t.sentence1,\n        t.sentence2,\n        model_id='cross-encoder/ms-marco-MiniLM-L-4-v2'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Object Detection Inference with Pixeltable YOLOX\nDESCRIPTION: Complete example of loading a pre-trained YOLOX model, processing an image, and performing object detection inference\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/yolox.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\nfrom yolox.models import Yolox, YoloxProcessor\n\nurl = \"https://raw.githubusercontent.com/pixeltable/pixeltable-yolox/main/tests/data/000000000001.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Yolox.from_pretrained(\"yolox_s\")\nprocessor = YoloxProcessor(\"yolox_s\")\ntensor = processor([image])\noutput = model(tensor)\nresult = processor.postprocess([image], output)\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Search Query in Pixeltable\nDESCRIPTION: This snippet shows how to perform a basic semantic search using Pixeltable's embedding index. It creates a query function that computes similarity scores between a user query and document content, then returns the top-k most similar documents with their scores.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef semantic_search(query: str, k: int):\n    # Calculate similarity scores\n    sim = docs.content.similarity(query)\n    \n    # Return top-k most similar documents\n    return (\n        docs\n        .order_by(sim, asc=False)\n        .select(docs.content, docs.metadata, score=sim)\n        .limit(k)\n    )\n\n# Use it\nresults = semantic_search(\"quantum computing\")\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable for Together AI Completions\nDESCRIPTION: This code creates a Pixeltable with columns for input, Together AI completions API output, and extracted completion text.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.together import completions\n\nt = pxt.create_table('together_demo.completions', {'input': pxt.String})\n\n# We'll use the lightweight Mistral-7B-Instruct model for this demo, but you can use\n# any model supported by Together AI.\n\nt.add_computed_column(output=completions(\n    prompt=t.input,\n    model='mistralai/Mistral-7B-Instruct-v0.3',\n    max_tokens=300\n))\nt.add_computed_column(response=t.output.choices[0].text)\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Embedding Index\nDESCRIPTION: This snippet adds a vector embedding index to the chunks table using the E5 model from sentence_transformers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import sentence_transformer\n\nchunks_t.add_embedding_index(\n    'text',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Object Detection with Hugging Face\nDESCRIPTION: Adds two computed columns to the table: one using Hugging Face's DETR ResNet-50 model for object detection, and another extracting just the label text from detection results. These columns are automatically populated when new data is added.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import huggingface\n\n# Add ResNet-50 object detection\nt.add_computed_column(\n    detections=huggingface.detr_for_object_detection(\n        t.input_image, \n        model_id='facebook/detr-resnet-50'\n    )\n)\n\n# Extract just the labels\nt.add_computed_column(detections_text=t.detections.label_text)\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Search Capability in Pixeltable\nDESCRIPTION: Configures vector search functionality by adding an embedding index to the chunked content. This enables natural language search capabilities using the specified embedding model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwebsites_chunks.add_embedding_index(\n    column=\"text\",\n    string_embed=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimodal Search with Pixeltable\nDESCRIPTION: Creates a video table, extracts frames, adds an embedding index, and performs similarity searches using both images and text. This example showcases Pixeltable's multimodal search capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.huggingface import clip\nfrom pixeltable.iterators import FrameIterator\nimport PIL.Image\n\nvideo_table = pxt.create_table('videos', {'video': pxt.Video})\n\nvideo_table.insert([{'video': '/video.mp4'}])\n\nframes_view = pxt.create_view(\n    'frames', video_table, iterator=FrameIterator.create(video=video_table.video))\n\n# Create an index on the 'frame' column that allows text and image search\nframes_view.add_embedding_index('frame', embed=clip.using('openai/clip-vit-base-patch32'))\n\n# Now we will retrieve images based on a sample image\nsample_image = '/image.jpeg'\nsim = frames_view.frame.similarity(sample_image)\nframes_view.order_by(sim, asc=False).limit(5).select(frames_view.frame, sim=sim).collect()\n\n# Now we will retrieve images based on a string\nsample_text = 'red truck'\nsim = frames_view.frame.similarity(sample_text)\nframes_view.order_by(sim, asc=False).limit(5).select(frames_view.frame, sim=sim).collect()\n```\n\n----------------------------------------\n\nTITLE: Querying the New Computed Column\nDESCRIPTION: Demonstrates how to select and display the newly created computed column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npop_t.select(pop_t.country, pop_t.yoy_change).head(5)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Mistral AI Chat Completions\nDESCRIPTION: Creates a Pixeltable table with an input column and adds a computed column that calls Mistral AI's chat completion API with configurable parameters.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.mistralai import chat_completions\n\n# Create a table in Pixeltable and pick a model hosted on Mistral with some parameters\n\nt = pxt.create_table('mistralai_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=chat_completions(\n    messages=messages,\n    model='mistral-small-latest',\n    # These parameters are optional and can be used to tune model behavior:\n    max_tokens=300,\n    top_p=0.9,\n    temperature=0.7\n))\n```\n\n----------------------------------------\n\nTITLE: Inserting New Data with Automatic Column Updates\nDESCRIPTION: Shows how adding new data automatically updates the computed columns for that row.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npop_t.insert(\n    country='California',\n    pop_2023=39110000,\n    pop_2022=39030000,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column for Audio Extraction\nDESCRIPTION: Adds a computed column to the table that automatically extracts audio from the video files. Uses the extract_audio function from pixeltable.functions.video.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.video import extract_audio\n\nvideo_table.add_computed_column(\n    audio=extract_audio(video_table.video, format='mp3')\n)\nvideo_table.show()\n```\n\n----------------------------------------\n\nTITLE: Defining UDFs for Label Conversion to Voxel51 Format\nDESCRIPTION: Creates user-defined functions to convert ViT and DETR model outputs to the format expected by Voxel51.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef vit_to_fo(vit_labels: list) -> list:\n    return [\n        {'label': label, 'confidence': score}\n        for label, score in zip(vit_labels['label_text'], vit_labels['scores'])\n    ]\n\n@pxt.udf\ndef detr_to_fo(img: pxt.Image, detr_labels: dict) -> list:\n    result = []\n    for label, box, score in zip(detr_labels['label_text'], detr_labels['boxes'], detr_labels['scores']):\n        # DETR gives us bounding boxes in (x1,y1,x2,y2) absolute (pixel) coordinates.\n        # Voxel51 expects (x,y,w,h) relative (fractional) coordinates.\n        # So we need to do a conversion.\n        fo_box = [\n            box[0] / img.width,\n            box[1] / img.height,\n            (box[2] - box[0]) / img.width,\n            (box[3] - box[1]) / img.height,\n        ]\n        result.append({'label': label, 'bounding_box': fo_box, 'confidence': score})\n    return result\n```\n\n----------------------------------------\n\nTITLE: Vision Transformer (ViT) Implementation\nDESCRIPTION: Shows how to use Vision Transformers for image classification tasks with top-k predictions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import vit_for_image_classification\n\nt.add_computed_column(\n    classifications=vit_for_image_classification(\n        t.image,\n        model_id='google/vit-base-patch16-224',\n        top_k=5\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using the Tool-calling AI Agent with Pixeltable\nDESCRIPTION: Demonstrates how to interact with the AI agent by connecting to the Pixeltable database and submitting various queries. The script shows how the agent processes different types of prompts, selecting appropriate tools and generating responses automatically.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your app\ntool_agent = pxt.get_table(\"agents.tools\")\n\n# Example queries using different tools\nqueries = [\n    \"What's the latest news about SpaceX?\",\n    \"What's the weather in San Francisco?\",\n    \"Calculate metrics for these numbers: 10,20,30,40,50\"\n]\n\n# Use the agent\nfor query in queries:\n    tool_agent.insert(prompt=query)\n    result = tool_agent.select(\n        tool_agent.tool_output,\n        tool_agent.answer\n    ).tail(1)\n    print(f\"\\nQuery: {query}\")\n    print(f\"Answer: {result['answer'][0]}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Directory for Demo\nDESCRIPTION: Creates a new Pixeltable directory to store tables for the demonstration, removing any existing directory with the same name.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Remove the `anthropic_demo` directory and its contents, if it exists\npxt.drop_dir('anthropic_demo', force=True)\npxt.create_dir('anthropic_demo')\n```\n\n----------------------------------------\n\nTITLE: Filtered Semantic Search in Pixeltable\nDESCRIPTION: This snippet shows how to combine semantic search with traditional filtering in Pixeltable. It filters documents by a metadata category before calculating and sorting by similarity scores, enabling more targeted semantic searches.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef filtered_search(query: str, category: str):\n    sim = docs.content.similarity(query)\n    \n    return (\n        docs\n        .where(docs.metadata['category'] == category)\n        .order_by(sim, asc=False)\n        .select(docs.content, score=sim)\n        .limit(5)\n    )\n```\n\n----------------------------------------\n\nTITLE: Working with Arrays and CLIP Embeddings\nDESCRIPTION: Demonstrates adding and querying array-type computed columns using CLIP embeddings.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\n\nt.add_computed_column(clip=clip(t.image, model_id='openai/clip-vit-base-patch32'))\nt.select(t.image, t.clip).head(5)\n```\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.clip[0], t.clip[5:10], t.clip[-3:]).head(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Views for Media Processing\nDESCRIPTION: Implements specialized views for processing different types of media content, including document chunking and transcription processing for video and audio content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchunks_view = pxt.create_view(\n    \"chatbot.chunks\",\n    docs_table,\n    iterator=DocumentSplitter.create(\n        document=docs_table.document,\n        separators=\"sentence\",\n        metadata=\"title,heading,sourceline\",\n    )\n)\n\ntranscription_chunks = pxt.create_view(\n    \"chatbot.transcription_chunks\",\n    docs_table,\n    iterator=StringSplitter.create(\n        text=docs_table.transcription_text,\n        separators=\"sentence\"\n    )\n)\n\naudio_chunks = pxt.create_view(\n    \"chatbot.audio_chunks\",\n    docs_table,\n    iterator=StringSplitter.create(\n        text=docs_table.audio_transcription_text,\n        separators=\"sentence\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Prompt Column\nDESCRIPTION: This snippet adds a computed column to the queries table that generates the prompt using the create_prompt UDF.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nqueries_t.add_computed_column(\n    prompt=create_prompt(queries_t.question_context, queries_t.Question)\n)\n```\n\n----------------------------------------\n\nTITLE: Data Aggregation and Extraction\nDESCRIPTION: Shows how to perform aggregations and extract data into Python objects and Pandas dataframes.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\neq_t.select(min=pxt.functions.min(eq_t.id), max=pxt.functions.max(eq_t.id)).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = eq_t.limit(5).collect()\nresult[0]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = result.to_pandas()\ndf['magnitude'].describe()\n```\n\n----------------------------------------\n\nTITLE: Implementing Usage Example\nDESCRIPTION: Demonstrates practical usage of the chat application, including document insertion, question handling, and conversation management.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/multimodal.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom datetime import datetime\n\ndocs_table = pxt.get_table(\"chatbot.documents\")\nconversations = pxt.get_table(\"chatbot.conversations\")\n\ndocs_table.insert([{\n    \"document\": \"path/to/document.pdf\"\n}])\n\ndocs_table.insert([{\n    \"video\": \"path/to/video.mp4\"\n}])\n\nquestion = \"What are the key points from all sources?\"\n\nconversations.insert([{\n    \"role\": \"user\",\n    \"content\": question,\n    \"timestamp\": datetime.now()\n}])\n\ndocs_table.insert([{\"question\": question}])\nresult = docs_table.select(docs_table.answer).collect()\nanswer = result[\"answer\"][0]\n\nconversations.insert([{\n    \"role\": \"assistant\",\n    \"content\": answer,\n    \"timestamp\": datetime.now()\n}])\n\nhistory = conversations.collect().to_pandas()\nprint(history)\n```\n\n----------------------------------------\n\nTITLE: Querying the Ollama Model through Pixeltable\nDESCRIPTION: This code demonstrates how to insert a prompt into the Pixeltable table, which automatically triggers the Ollama inference and stores the response. It shows how to display the input and response columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nt.insert(input='What are the most popular services for LLM inference?')\nt.select(t.input, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Creating Short Character Chunks View in Pixeltable\nDESCRIPTION: Creates a view 'short_char_chunks' that splits documents into paragraphs with a character limit of 72.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshort_char_chunks = pxt.create_view(\n    'rag_ops_demo.short_char_chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,char_limit',\n        limit=72,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Splitting Transcriptions into Sentences\nDESCRIPTION: Creates a new view that splits the transcriptions into individual sentences using Pixeltable's StringSplitter iterator.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators.string import StringSplitter\n\nsentences_view = pxt.create_view(\n    'transcription_demo.sentences_view',\n    video_table,\n    iterator=StringSplitter.create(\n        text=video_table.transcription.text,\n        separators='sentence'\n    )\n)\n\nsentences_view.select(\n    sentences_view.pos,\n    sentences_view.text\n).show(8)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Price Calculation UDF in Python\nDESCRIPTION: Shows how to create a simple UDF that calculates price with tax. The function takes price and rate parameters and returns the computed value, which can be used in a computed column in a table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef add_tax(price: float, rate: float) -> float:\n    return price * (1 + rate)\n\n# Use in computed column\ntable.add_computed_column(\n    price_with_tax=add_tax(table.price)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Financial Analyst Agent Table in Pixeltable\nDESCRIPTION: Creates a financial analyst agent table with a multi-step workflow including LLM calls and tool execution. The workflow processes a user prompt, calls a language model with tools, executes the tools, and generates a final response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Create Financial Analyst Agent Table\nfinance_agent = pxt.create_table(\n    f'{DIRECTORY}.financial_analyst', \n    {'prompt': pxt.String}, \n    if_exists='ignore'\n)\n\n# Prepare initial messages for LLM\nmessages = [{'role': 'user', 'content': finance_agent.prompt}]\n\n# Define available tools\ntools = pxt.tools(stock_info)\n\n# Get initial response with tool calls\nfinance_agent.add_computed_column(\n    initial_response=chat_completions(\n        model=OPENAI_MODEL, \n        messages=messages, \n        tools=tools, \n        tool_choice=tools.choice(required=True)\n    )\n)\n\n# Execute the requested tools\nfinance_agent.add_computed_column(\n    tool_output=invoke_tools(tools, finance_agent.initial_response)\n)\n\n# Create prompt with tool results\nfinance_agent.add_computed_column(\n    stock_response_prompt=create_prompt(\n        finance_agent.prompt, \n        finance_agent.tool_output\n    )\n)\n\n# Generate final response using tool results\nfinal_messages = [\n    {'role': 'system', 'content': \"Answer the user's question based on the results.\"},\n    {'role': 'user', 'content': finance_agent.stock_response_prompt},\n]\n\nfinance_agent.add_computed_column(\n    final_response=chat_completions(\n        model=OPENAI_MODEL, \n        messages=final_messages\n    )\n)\n\n# Extract answer text\nfinance_agent.add_computed_column(\n    answer=finance_agent.final_response.choices[0].message.content\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Chat Completion\nDESCRIPTION: Demonstrating table usage by inserting a test question and displaying the input and response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nt.insert(input=\"Can you tell me who's the President of the US?\")\nt.select(t.input, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Creating Sentence View from Source Documents in Pixeltable\nDESCRIPTION: Creates a view 'sentences' that represents chunks of HTML documents split into sentences using Pixeltable's DocumentSplitter.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators.document import DocumentSplitter\n\nsentences = pxt.create_view(\n    'rag_ops_demo.sentences',  # Name of the view\n    docs,  # Table from which the view is derived\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='sentence',  # Chunk docs into sentences\n        metadata='title,heading,sourceline'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Video Frame Processing and Search in Pixeltable\nDESCRIPTION: Shows video frame extraction and multimodal search implementation using CLIP embeddings for both text-to-image and image-to-image search capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example video processing\nframes = pxt.create_view(\n    'video_search.frames',\n    videos,\n    iterator=FrameIterator.create(\n        video=videos.video, \n        fps=1  # Extract 1 frame per second\n    )\n)\n\n# Add multimodal search \nframes.add_embedding_index(\n    'frame',\n    string_embed=clip_text,     # For text-to-image search\n    image_embed=clip_image      # For image-to-image search\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column from JSON Data in Pixeltable\nDESCRIPTION: Creates a new computed column that extracts the predicted label from classification results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Use label_text to be consistent with t.label, which was given\n# to us as a string\n\nt.add_computed_column(pred_label=t.classification.label_text[0])\nt\n```\n\n----------------------------------------\n\nTITLE: Optimized BERT Embedding with Model Caching\nDESCRIPTION: Production-ready implementation of BERT embedding that caches model instances to avoid reloading on every function call, improving performance.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/embedding-model.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef optimized_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"BERT embedding function with model caching\"\"\"\n    if not hasattr(optimized_bert_embed, 'model'):\n        # Load models once\n        optimized_bert_embed.preprocessor = hub.load(\n            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n        )\n        optimized_bert_embed.model = hub.load(\n            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'\n        )\n    \n    tensor = tf.constant([text])\n    result = optimized_bert_embed.model(\n        optimized_bert_embed.preprocessor(tensor)\n    )['pooled_output']\n    return result.numpy()[0, :]\n```\n\n----------------------------------------\n\nTITLE: Defining UDF for Top Detection Label Extraction in Python\nDESCRIPTION: This UDF takes a dictionary of detection results and returns the label with the highest confidence score. It uses the @pxt.udf decorator to mark it as a Pixeltable user-defined function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef top_detection(detect: dict) -> str:\n    scores = detect['scores']\n    label_text = detect['label_text']\n    # Get the index of the object with the highest confidence\n    i = scores.index(max(scores))\n    # Return the corresponding label\n    return label_text[i]\n```\n\n----------------------------------------\n\nTITLE: Testing Chat Completion with Sample Questions\nDESCRIPTION: Inserts sample questions into the table to test the LLM's capabilities with basic knowledge queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Test with a simple question\nt.insert([\n    {'input': 'What is the capital of France?'},\n    {'input': 'What are some edible species of fish?'},\n    {'input': 'Who are the most prominent classical composers?'}\n])\n```\n\n----------------------------------------\n\nTITLE: Selecting Image and Classification Data in Pixeltable\nDESCRIPTION: Queries the table to display both the image and its classification results for the first 3 rows.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.image, t.classification).head(3)\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Schema in Pixeltable\nDESCRIPTION: Shows the table structure after adding the classification column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt\n```\n\n----------------------------------------\n\nTITLE: Working with Audio Files in Pixeltable\nDESCRIPTION: Shows how to create tables for audio files and insert audio from local files, URLs, and cloud storage locations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create table with audio columns\naudio_table = pxt.create_table('myproject.audio', {\n    'audio': pxt.Audio,           # Audio file reference\n})\n\n# Insert local audio files\naudio_table.insert([\n    {'audio': '/path/to/audio1.mp3'},\n    {'audio': '/path/to/audio2.wav'}\n])\n\n# Insert audio URLs\naudio_table.insert([\n    {'audio': 'https://example.com/audio1.mp3'},\n    {'audio': 'https://example.com/audio2.wav'}\n])\n\n# Insert from cloud storage\naudio_table.insert([\n    {'audio': 's3://my-bucket/audio1.mp3'}\n])\n```\n\n----------------------------------------\n\nTITLE: Creating Module-Based Text Cleaning UDF in Python\nDESCRIPTION: Demonstrates how to define a UDF in a separate module and import it for use. Module UDFs are referenced by path, allowing changes to affect all uses after reloading.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# In my_functions.py\n@pxt.udf\ndef clean_text(text: str) -> str:\n    return text.strip().lower()\n    \n# In your application\nfrom my_functions import clean_text\ntable.add_computed_column(\n    clean_content=clean_text(table.content)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Portfolio Manager Table Using a Financial Agent UDF in Pixeltable\nDESCRIPTION: Creates a portfolio manager table that uses the finance agent UDF as a computed column. This demonstrates how to integrate a complex Table UDF into another table for reusable processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Create a Portfolio Manager table that uses the finance agent\nportfolio_manager = pxt.create_table(\n    f'{DIRECTORY}.portfolio_manager', \n    {'prompt': pxt.String}, \n    if_exists='ignore'\n)\n\n# Add the finance agent UDF as a computed column\nportfolio_manager.add_computed_column(\n    result=finance_agent_udf(portfolio_manager.prompt)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing GPT-4 Vision for Image Analysis\nDESCRIPTION: Code snippet demonstrating how to use GPT-4 Vision to generate rich image descriptions. This functionality automatically analyzes images and produces textual descriptions that can be used for search.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimage_description=vision(\n    prompt=\"Describe the image...\",\n    image=img_t.image\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Embedding Function with TensorFlow in Pixeltable\nDESCRIPTION: This snippet shows how to create a custom embedding function using TensorFlow and TensorFlow Hub with a BERT model. It demonstrates defining a user-defined function (UDF) that can be used as an embedding function for a Pixeltable index.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text\n\n@pxt.udf\ndef bert(input: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"Computes text embeddings using small_bert.\"\"\"\n    preprocessor = hub.load(\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n    )\n    bert_model = hub.load(\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'\n    )\n    tensor = tf.constant([input])\n    result = bert_model(preprocessor(tensor))['pooled_output']\n    return result.numpy()[0, :]\n\n# Add custom embedding index\ndocs.add_embedding_index(\n    column='content',\n    idx_name='bert_idx',\n    string_embed=bert\n)\n```\n\n----------------------------------------\n\nTITLE: Speech to Text Model Integration\nDESCRIPTION: Demonstrates audio transcription and translation using Speech2Text models with language support.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import speech2text_for_conditional_generation\n\n# Basic transcription\nt.add_computed_column(\n    transcript=speech2text_for_conditional_generation(\n        t.audio,\n        model_id='facebook/s2t-small-librispeech-asr'\n    )\n)\n\n# Multilingual translation\nt.add_computed_column(\n    translation=speech2text_for_conditional_generation(\n        t.audio,\n        model_id='facebook/s2t-medium-mustc-multilingual-st',\n        language='fr'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Calculations in Pixeltable Queries\nDESCRIPTION: Demonstrates how to include calculations in Pixeltable select() queries and assign custom names to resulting columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.select(films_t.film_name, revenue_thousands=films_t.revenue * 1000).collect()\n```\n\n----------------------------------------\n\nTITLE: Defining Label Studio Annotation Workflow with Pixeltable\nDESCRIPTION: Complete setup script for creating an annotation workflow in Pixeltable. Creates tables and views for videos and frames, adds object detection using DETR model, and configures Label Studio projects for both video and frame annotation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import FrameIterator\nfrom pixeltable.functions.huggingface import detr_for_object_detection, detr_to_coco\nfrom datetime import datetime\n\n# Initialize app structure\npxt.drop_dir('annotation', force=True)\npxt.create_dir('annotation')\n\n# Create base video table\nvideos = pxt.create_table(\n    'annotation.videos',\n    {\n        'video': pxt.Video,\n        'date': pxt.Timestamp\n    },\n    if_exists=\"ignore\"\n)\n\n# Create frame extraction view\nframes = pxt.create_view(\n    'annotation.frames',\n    videos,\n    iterator=FrameIterator.create(\n        video=videos.video,\n        fps=0.25  # Extract 1 frame every 4 seconds\n    )\n)\n\n# Add object detection for pre-annotations\nframes.add_computed_column(\n    detections=detr_for_object_detection(\n        frames.frame,\n        model_id='facebook/detr-resnet-50',\n        threshold=0.95\n    )\n)\n\n# Convert detections to COCO format for Label Studio\nframes.add_computed_column(\n    preannotations=detr_to_coco(frames.frame, frames.detections)\n)\n\n# Define Label Studio configurations\nvideo_config = '''\n<View>\n  <Video name=\"video\" value=\"$video\"/>\n  <Choices name=\"video-category\" toName=\"video\" showInLine=\"true\">\n    <Choice value=\"city\"/>\n    <Choice value=\"food\"/>\n    <Choice value=\"sports\"/>\n  </Choices>\n</View>\n'''\n\nframe_config = '''\n<View>\n  <Image name=\"frame\" value=\"$frame\"/>\n  <RectangleLabels name=\"preannotations\" toName=\"frame\">\n    <Label value=\"car\" background=\"blue\"/>\n    <Label value=\"person\" background=\"red\"/>\n    <Label value=\"train\" background=\"green\"/>\n  </RectangleLabels>\n</View>\n'''\n\n# Create Label Studio projects\npxt.io.create_label_studio_project(\n    videos,\n    video_config,\n    media_import_method='url'  # Recommended for production\n)\n\npxt.io.create_label_studio_project(\n    frames,\n    frame_config,\n    media_import_method='url'\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Rows from a Pixeltable Table\nDESCRIPTION: Demonstrates how to retrieve all rows from a Pixeltable table using the collect() method.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.collect()\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Search Capability with E5 Embeddings\nDESCRIPTION: Code example showing how to configure vector search capabilities using embedding indexes. This enables natural language search by converting descriptions to vector representations for similarity matching.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg_t.add_embedding_index(\n    column=\"image_description\",\n    string_embed=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom BERT Embedding Index in Pixeltable\nDESCRIPTION: Adds an embedding index using the custom BERT UDF implementation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntxts.add_embedding_index(\n    'text',\n    idx_name='bert_idx',\n    embedding=bert\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting Image Data into Pixeltable Table\nDESCRIPTION: Inserts 10 rows of image data into the previously created table, using URLs for image sources.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimg_urls = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000030.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000034.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000042.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000049.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000057.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000061.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000063.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000064.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000069.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000071.jpg',\n]\nimgs.insert({'id': i, 'img': url} for i, url in enumerate(img_urls))\n```\n\n----------------------------------------\n\nTITLE: Defining UDF with Basic Type Annotations in Python\nDESCRIPTION: Shows how to create a UDF with various basic Python type annotations including strings, integers, floats, booleans, lists, and dictionaries. Type hints enable Pixeltable to validate data flows.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef process_data(\n    text: str,           # String data\n    count: int,          # Integer numbers\n    score: float,        # Floating point\n    active: bool,        # Boolean\n    items: list[str],    # Generic lists\n    meta: dict[str,any]  # Dictionaries\n) -> str:\n    return \"Processed\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Table Structure\nDESCRIPTION: Initializes Pixeltable by creating a directory and defining a table with an image column. This establishes the foundation for the multimodal application by setting up persistent storage for image data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Create directory for our tables\npxt.drop_dir('demo', force=True)  \npxt.create_dir('demo')\n\n# Create table with image column\nt = pxt.create_table('demo.first', {'input_image': pxt.Image})\n```\n\n----------------------------------------\n\nTITLE: Using the Pixeltable Image Search Workflow\nDESCRIPTION: Implementation script that demonstrates how to use the defined workflow. It connects to the existing table, adds sample images, and executes a search query with results processing and display.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your table\nimg_t = pxt.get_table(\"image_search.images\")\n\n# Sample image URLs\nIMAGE_URL = (\n    \"https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/\"\n)\n\nimage_urls = [\n    IMAGE_URL + doc for doc in [\n        \"000000000030.jpg\",\n        \"000000000034.jpg\",\n        \"000000000042.jpg\",\n    ]\n]\n\n# Add images to the database\nimg_t.insert({\"image\": url} for url in image_urls)\n\n# Search images\n@pxt.query\ndef find_images(query: str, top_k: int):\n    sim = img_t.image_description.similarity(query)\n    return (\n        img_t.order_by(sim, asc=False)\n        .select(\n            img_t.image,\n            img_t.image_description,\n            similarity=sim\n        )\n        .limit(top_k)\n    )\n\n# Example search\nresults = find_images(\"Show me images containing blue flowers\").collect()\n\n# Print results\nfor r in results:\n    print(f\"Similarity: {r['similarity']:.3f}\")\n    print(f\"Description: {r['image_description']}\")\n    print(f\"Image URL: {r['image']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Drawing Bounding Boxes Expression Example\nDESCRIPTION: Demonstrates the syntax for creating bounding boxes using compound expressions to access detection results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndraw_bounding_boxes(t.source, t.detections.boxes, t.detections.label_text, fill=True)\n```\n\n----------------------------------------\n\nTITLE: Inserting Video into Pixeltable Table\nDESCRIPTION: Inserts a single video of a busy intersection in Bangkok into the videos table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvideos_table.insert([\n    {\n        'video': 'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/bangkok.mp4'\n    }\n])\n```\n\n----------------------------------------\n\nTITLE: Grouped Aggregation with Built-in Functions\nDESCRIPTION: Shows how to use group_by and order_by with built-in aggregates to group integers by their tens digit and sum each group separately.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nt.group_by(t.val // 10).order_by(t.val // 10).select(\n    t.val // 10, pxtf.sum(t.val)\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Video Object Detection Workflow with FrameIterator in Python\nDESCRIPTION: This snippet shows a video object detection workflow using FrameIterator to extract frames and a computed column for object detection.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Extract frames at 1 FPS\nframes = pxt.create_view(\n    'detection.frames',\n    videos_table,\n    iterator=FrameIterator.create(\n        video=videos_table.video,\n        fps=1.0\n    )\n)\n\n# Add object detection\nframes.add_computed_column(detections=detect_objects(frames.frame))\n```\n\n----------------------------------------\n\nTITLE: Inserting and Querying Data in Pixeltable\nDESCRIPTION: This snippet shows how to insert multiple rows of data into a Pixeltable table and then retrieve all rows using the collect() method. The example inserts movie data with name, revenue, and budget information.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Insert data into a table\nt.insert([\n  {'name': 'Inside Out', 'revenue': 800.5, 'budget': 200.0},\n  {'name': 'Toy Story', 'revenue': 1073.4, 'budget': 200.0}\n])\n\n# Retrieves all the rows in the table.\nt.collect()\n```\n\n----------------------------------------\n\nTITLE: Using the PDF Search Workflow in Pixeltable\nDESCRIPTION: Demonstrates how to use the previously defined PDF search workflow. This includes connecting to existing tables, adding documents to the database, defining search functionality, and executing searches with result presentation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your tables\ndocuments_t = pxt.get_table(\"pdf_search.documents\")\ndocuments_chunks = pxt.get_table(\"pdf_search.document_chunks\")\n\n# Sample document URLs\nDOCUMENT_URL = (\n    \"https://github.com/pixeltable/pixeltable/raw/release/docs/resources/rag-demo/\"\n)\n\ndocument_urls = [\n    DOCUMENT_URL + doc for doc in [\n        \"Argus-Market-Digest-June-2024.pdf\",\n        \"Company-Research-Alphabet.pdf\",\n        \"Zacks-Nvidia-Report.pdf\",\n    ]\n]\n\n# Add documents to database\ndocuments_t.insert({\"pdf\": url} for url in document_urls)\n\n# Search documents\n@pxt.query\ndef find_relevant_text(query: str, top_k: int):\n    sim = documents_chunks.text.similarity(query)\n    return (\n        documents_chunks.order_by(sim, asc=False)\n        .select(\n            documents_chunks.text,\n            similarity=sim\n        )\n        .limit(top_k)\n    )\n\n# Example search\nresults = find_relevant_text(\n    \"What are the growth projections for tech companies?\"\n).collect()\n\n# Print results\nfor r in results:\n    print(f\"Similarity: {r['similarity']:.3f}\")\n    print(f\"Text: {r['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Table Schema After Column Deletion\nDESCRIPTION: Displays the table schema after a column has been dropped, showing how the table structure has been updated.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\neq_t.describe()\n```\n\n----------------------------------------\n\nTITLE: Displaying Updated Documents Table in Pixeltable\nDESCRIPTION: This snippet shows the contents of the documents_t table after inserting new documents.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndocuments_t.show()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating error handling for corrupted files in Pixeltable\nDESCRIPTION: This snippet creates a corrupted MP4 file and attempts to insert it into Pixeltable, showing how errors are handled.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# create invalid .mp4\nwith tempfile.NamedTemporaryFile(mode='wb', suffix='.mp4', delete=False) as temp_file:\n    temp_file.write(random.randbytes(1024))\n    corrupted_path = temp_file.name\n\nv.insert(video=corrupted_path)\n```\n\n----------------------------------------\n\nTITLE: Implementing UDF with Complex Pixeltable Types in Python\nDESCRIPTION: Demonstrates a UDF that works with Pixeltable-specific data types including images, numerical arrays, JSON data, and documents. These complex types allow for media and structured data processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef process_media(\n    img: PIL.Image.Image,    # Images\n    embeddings: pxt.Array,   # Numerical arrays\n    config: pxt.Json,        # JSON data\n    doc: pxt.Document        # Documents\n) -> str:\n    return \"Processed\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Token-Aware Document Splitting in Pixeltable\nDESCRIPTION: Shows how to configure token-aware document splitting in Pixeltable using the DocumentSplitter class. This approach ensures chunks respect token limits, which is important for working with language models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\niterator=DocumentSplitter.create(\n    document=documents_t.pdf,\n    separators=\"token_limit\",\n    limit=300\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Auto-updating Document Database in Pixeltable\nDESCRIPTION: Demonstrates how to insert new documents into the Pixeltable database with automatic updating of chunks and embeddings. This self-maintaining approach simplifies document management.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocuments_t.insert({\"pdf\": new_url})\n# Chunking and embeddings update automatically\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Table UDFs in Pixeltable\nDESCRIPTION: Shows how to execute multiple table UDFs in parallel and combine their results. This pattern is useful for conducting different types of analyses simultaneously and then aggregating the insights.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Define specialized agents for different tasks\nstock_agent = pxt.udf(stock_table, return_value=stock_table.analysis)\nnews_agent = pxt.udf(news_table, return_value=news_table.summary)\nsentiment_agent = pxt.udf(sentiment_table, return_value=sentiment_table.score)\n\n# Process in parallel\nportfolio.add_computed_column(stock_data=stock_agent(portfolio.ticker))\nportfolio.add_computed_column(news_data=news_agent(portfolio.ticker))\nportfolio.add_computed_column(sentiment=sentiment_agent(portfolio.ticker))\n\n# Combine results\nportfolio.add_computed_column(report=combine_insights(\n    portfolio.stock_data, \n    portfolio.news_data, \n    portfolio.sentiment\n))\n```\n\n----------------------------------------\n\nTITLE: Setting Default Time Zone\nDESCRIPTION: Sets the default time zone for Pixeltable using environment variables. Uses America/Los_Angeles as the example time zone.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['PIXELTABLE_TIME_ZONE'] = 'America/Los_Angeles'\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing for Multiple Images\nDESCRIPTION: Code example demonstrating batch processing functionality to insert multiple images at once. This is useful for efficiently populating the database with a collection of images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Bulk image insertion\nimage_urls = [\n    \"https://example.com/image1.jpg\",\n    \"https://example.com/image2.jpg\",\n    \"https://example.com/image3.jpg\"\n]\n\nimg_t.insert({\"image\": url} for url in image_urls)\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data into Pixeltable\nDESCRIPTION: Demonstrates importing earthquake data from a CSV file into a Pixeltable table. Specifies a primary key and parses date columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\neq_t = pxt.io.import_csv(\n    'fundamentals.earthquakes',  # Name for the new table\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/earthquakes.csv',\n    primary_key='id',  # Column 'id' is the primary key\n    parse_dates=[3]  # Interpret column 3 as a timestamp\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Hugging Face Dataset\nDESCRIPTION: Loads a dataset from Hugging Face and selects specific columns including Image, ImageSize, Name, and ImageSource.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-hugging-face.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\npadoru = (\n    datasets.load_dataset(\"not-lain/padoru\", split='train')\n    .select_columns(['Image', 'ImageSize', 'Name', 'ImageSource'])\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting Documents into Table\nDESCRIPTION: This snippet inserts the first three documents from a list of URLs into the documents table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocument_urls = [\n    base + 'Argus-Market-Digest-June-2024.pdf',\n    base + 'Argus-Market-Watch-June-2024.pdf',\n    base + 'Company-Research-Alphabet.pdf',\n    base + 'Jefferson-Amazon.pdf',\n    base + 'Mclean-Equity-Alphabet.pdf',\n    base + 'Zacks-Nvidia-Repeport.pdf',\n]\n\ndocuments_t.insert({'document': url} for url in document_urls[:3])\ndocuments_t.show()\n```\n\n----------------------------------------\n\nTITLE: Defining PDF Search Workflow Structure in Pixeltable\nDESCRIPTION: Creates the core structure for a PDF search system, including table definitions, document chunking, embedding generation, and search functionality. Uses DocumentSplitter for token-aware chunking and sentence_transformer for embedding generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import DocumentSplitter\nfrom pixeltable.functions.huggingface import sentence_transformer\n\n# Initialize app structure\npxt.drop_dir(\"pdf_search\", force=True)\npxt.create_dir(\"pdf_search\")\n\n# Create documents table\ndocuments_t = pxt.create_table(\n    \"pdf_search.documents\", \n    {\"pdf\": pxt.Document}\n)\n\n# Create chunked view for efficient processing\ndocuments_chunks = pxt.create_view(\n    \"pdf_search.document_chunks\",\n    documents_t,\n    iterator=DocumentSplitter.create(\n        document=documents_t.pdf,\n        separators=\"token_limit\",\n        limit=300  # Tokens per chunk\n    )\n)\n\n# Configure embedding model\nembed_model = sentence_transformer.using(\n    model_id=\"intfloat/e5-large-v2\"\n)\n\n# Add search capability\ndocuments_chunks.add_embedding_index(\n    column=\"text\",\n    string_embed=embed_model\n)\n\n# Define search query\n@pxt.query\ndef search_documents(query_text: str, limit: int):\n    sim = documents_chunks.text.similarity(query_text)\n    return (\n        documents_chunks.order_by(sim, asc=False)\n        .select(\n            documents_chunks.text,\n            similarity=sim\n        )\n        .limit(limit)\n    )\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Table by Name in Pixeltable\nDESCRIPTION: Shows how to retrieve an existing table by its name using the get_table() function in Pixeltable after clearing the Python session.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\nfilms_t = pxt.get_table('fundamentals.films')\nfilms_t.collect()\n```\n\n----------------------------------------\n\nTITLE: Creating a Table in Pixeltable\nDESCRIPTION: This snippet demonstrates how to create a new table in Pixeltable with predefined columns and their data types. The example creates a 'films_table' with string and float columns for movie data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a table to hold data\nt = pxt.create_table('films_table', {\n    'name': pxt.String,\n    'revenue': pxt.Float,\n    'budget': pxt.Float\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Caching in UDF for Performance in Python\nDESCRIPTION: Demonstrates how to implement caching in a UDF to avoid repeating expensive operations. The function caches a model instance as an attribute of the function object.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef expensive_operation(text: str) -> str:\n    # Cache model instance\n    if not hasattr(expensive_operation, 'model'):\n        expensive_operation.model = load_model()\n    return expensive_operation.model(text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch PDF Processing in Pixeltable\nDESCRIPTION: Demonstrates how to process multiple PDF documents in a batch operation. This approach efficiently handles bulk document insertion from multiple sources into the Pixeltable database.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Bulk document insertion\npdf_urls = [\n    \"https://example.com/doc1.pdf\",\n    \"https://example.com/doc2.pdf\",\n    \"https://example.com/doc3.pdf\"\n]\n\ndocuments_t.insert({\"pdf\": url} for url in pdf_urls)\n```\n\n----------------------------------------\n\nTITLE: Using the Audio Processing Workflow\nDESCRIPTION: Demonstrates how to use the defined workflow to process audio files and perform searches on the transcribed content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your tables and views\naudio_t = pxt.get_table(\"audio_search.audio\")\nsentences_view = pxt.get_table(\"audio_search.audio_sentence_chunks\")\n\n# Add audio files to the knowledge base\naudio_t.insert([{\n    \"audio_file\": \"https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/10-minute%20tour%20of%20Pixeltable.mp3\"\n}])\n\n# Perform search\n@pxt.query\ndef search_audio(query_text: str):\n    sim = sentences_view.text.similarity(query_text)\n    return (\n        sentences_view.order_by(sim, asc=False)\n        .select(sentences_view.text, sim=sim)\n        .limit(10)\n    )\n\n# Example search\nresults = search_audio(\"What are the key features of Pixeltable?\")\n\n# Print results\nfor result in results.collect():\n    print(f\"Similarity: {result['sim']:.3f}\")\n    print(f\"Text: {result['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using the Audio Processing Workflow\nDESCRIPTION: Demonstrates how to use the defined workflow to process audio files and perform searches on the transcribed content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your tables and views\naudio_t = pxt.get_table(\"audio_search.audio\")\nsentences_view = pxt.get_table(\"audio_search.audio_sentence_chunks\")\n\n# Add audio files to the knowledge base\naudio_t.insert([{\n    \"audio_file\": \"https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/10-minute%20tour%20of%20Pixeltable.mp3\"\n}])\n\n# Perform search\n@pxt.query\ndef search_audio(query_text: str):\n    sim = sentences_view.text.similarity(query_text)\n    return (\n        sentences_view.order_by(sim, asc=False)\n        .select(sentences_view.text, sim=sim)\n        .limit(10)\n    )\n\n# Example search\nresults = search_audio(\"What are the key features of Pixeltable?\")\n\n# Print results\nfor result in results.collect():\n    print(f\"Similarity: {result['sim']:.3f}\")\n    print(f\"Text: {result['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Schema with ML-Specific Types\nDESCRIPTION: Demonstrates how to create a table schema with machine learning-specific data types including Array and Json.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# ML-specific types\nml_data = pxt.create_table('ml_features', {\n    'embedding': pxt.Array[(768,), pxt.Float], # Fixed-size array\n    'features': pxt.Array[(None, 512)],        # Variable first dim\n    'metadata': pxt.Json                       # Flexible JSON data\n})\n```\n\n----------------------------------------\n\nTITLE: Batch Processing BERT Embeddings\nDESCRIPTION: Enhanced implementation using Pixeltable's batching capabilities to process multiple texts simultaneously for improved performance.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/embedding-model.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.func import Batch\n\n@pxt.udf(batch_size=32)\ndef batched_bert_embed(texts: Batch[str]) -> Batch[pxt.Array[(512,), pxt.Float]]:\n    \"\"\"BERT embedding function with batching\"\"\"\n    if not hasattr(batched_bert_embed, 'model'):\n        batched_bert_embed.preprocessor = hub.load(\n            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n        )\n        batched_bert_embed.model = hub.load(\n            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'\n        )\n    \n    # Process entire batch at once\n    tensor = tf.constant(list(texts))\n    results = batched_bert_embed.model(\n        batched_bert_embed.preprocessor(tensor)\n    )['pooled_output']\n    return [r for r in results.numpy()]\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embedding Function in Python\nDESCRIPTION: Creates a function that uses OpenAI's API to generate embeddings for text input. The function is registered with Pixeltable using the @register_function decorator and requires an API key to be set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/openai.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@register_function(\n    return_type=dtypes.Array(numpy.float32, ndims=1),\n    name='embedding_openai',\n    argument_type_hints={\n        'text': dtypes.String(),\n        'model': dtypes.String(default='text-embedding-ada-002'),\n        'normalize': dtypes.Boolean(default=True)\n    },\n    is_aggregation=False\n)\ndef embedding_openai(text: str, model: str='text-embedding-ada-002', normalize: bool=True) -> numpy.ndarray:\n    \"\"\"Get the embedding for a given text using OpenAI's API.\n    \n    Args:\n        text: The text to embed\n        model: The OpenAI model to use\n        normalize: Whether to normalize the embedding\n    \n    Returns:\n        The embedding as a numpy array\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        raise ImportError('You need to install the openai package to use embedding_openai()')\n    \n    if openai.api_key is None:\n        raise ValueError('You need to set openai.api_key to use embedding_openai()')\n\n    embedding_response = openai.embeddings.create(\n        model=model,\n        input=text.strip(),\n        encoding_format='float'\n    )\n    embedding = numpy.array(embedding_response.data[0].embedding)\n    \n    if normalize:\n        embedding = embedding / numpy.linalg.norm(embedding)\n        \n    return embedding\n```\n\n----------------------------------------\n\nTITLE: Using Built-in Aggregates in Pixeltable\nDESCRIPTION: Example of using Pixeltable's built-in sum aggregate to calculate the sum of all values in the table, demonstrating the basic usage pattern for aggregates.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable.functions as pxtf\n\nt.select(pxtf.sum(t.val)).collect()\n```\n\n----------------------------------------\n\nTITLE: Custom Search Function with Threshold\nDESCRIPTION: Implements a custom search function that includes a similarity threshold parameter for filtering results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef search_with_threshold(query_text: str, min_similarity: float):\n    sim = sentences_view.text.similarity(query_text)\n    return (\n        sentences_view.where(sim >= min_similarity)\n        .order_by(sim, asc=False)\n        .select(sentences_view.text, sim=sim)\n    )\n```\n\n----------------------------------------\n\nTITLE: Inserting invalid data with error ignoring in Pixeltable\nDESCRIPTION: This code demonstrates how to insert invalid data by ignoring errors, allowing for later inspection and correction.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nv.insert([{'video': prefix + 'bad_path.mp4'}, {'video': corrupted_path}], on_error='ignore')\n```\n\n----------------------------------------\n\nTITLE: Viewing Table Schema\nDESCRIPTION: Displays the schema of the population table, showing column names and types.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npop_t\n```\n\n----------------------------------------\n\nTITLE: Viewing Table Schema\nDESCRIPTION: Displays the schema of the population table, showing column names and types.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npop_t\n```\n\n----------------------------------------\n\nTITLE: Basic Column Selection in Pixeltable\nDESCRIPTION: Selects specific columns (title and year) from the movies table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmovies.select(\n    movies.title, \n    movies.year\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Importing Excel Data in Pixeltable\nDESCRIPTION: Shows Excel file importing capabilities including basic import, schema overrides, and sheet-specific options. Supports custom sheet selection, header row specification, and NA value handling.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Basic Excel import\ntable1 = pxt.io.import_excel(\n    'myproject.excel_data',\n    'data.xlsx'\n)\n\n# Excel import with schema overrides\ntable2 = pxt.io.import_excel(\n    'myproject.excel_typed',\n    'data.xlsx',\n    schema_overrides={\n        'id': pxt.Int,\n        'amount': pxt.Float,\n        'date': pxt.Timestamp\n    }\n)\n\n# Excel import with options\ntable3 = pxt.io.import_excel(\n    'myproject.excel_options',\n    'data.xlsx',\n    sheet_name='Sheet2',    # Specific sheet\n    header=1,               # Header row\n    na_values=['NA', '-'],  # Custom NA values\n    parse_dates=['date']    # Parse date columns\n)\n```\n\n----------------------------------------\n\nTITLE: Converting a Table to a UDF in Pixeltable\nDESCRIPTION: Transforms a Pixeltable table into a callable user-defined function by specifying which column should be used as the return value. This allows using complex table-based workflows as functions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Convert table to UDF by specifying return column\nfinance_agent_udf = pxt.udf(finance_agent, \n                               return_value=finance_agent.answer)\n```\n\n----------------------------------------\n\nTITLE: Cross-Modal Search with Images in Pixeltable\nDESCRIPTION: This snippet shows how to perform cross-modal search in Pixeltable, specifically searching for text content using an image query. It demonstrates using an image as input to find semantically related text documents based on their embedding similarities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef image_to_text(image_query: PIL.Image.Image):\n    # Search text using image embedding\n    sim = docs.content.similarity(image_query)\n    return docs.order_by(sim, asc=False).limit(5)\n```\n\n----------------------------------------\n\nTITLE: Managing Video Data in Pixeltable\nDESCRIPTION: Demonstrates creating tables for video files and inserting videos from local files, URLs, and cloud storage sources.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create table with video columns\nvideo_table = pxt.create_table('myproject.videos', {\n    'video': pxt.Video,            # Video file reference\n})\n\n# Insert local video files\nvideo_table.insert([\n    {'video': '/path/to/video1.mp4'},\n    {'video': '/path/to/video2.webm'}\n])\n\n# Insert video URLs\nvideo_table.insert([\n    {'video': 'https://example.com/video1.mp4'},\n    {'video': 'https://example.com/video2.webm'}\n])\n\n# Insert from cloud storage\nvideo_table.insert([\n    {'video': 's3://my-bucket/video1.mp4'}\n])\n```\n\n----------------------------------------\n\nTITLE: Example Detection Output Format\nDESCRIPTION: Sample output showing the structure of detection results including bounding boxes, confidence scores, and class labels\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/yolox.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[{'bboxes': [\n   (272.36126708984375, 3.5648040771484375, 640.4871826171875, 223.2653350830078),\n   (26.643890380859375, 118.68254089355469, 459.80706787109375, 315.089111328125),\n   (259.41485595703125, 152.3223114013672, 295.37054443359375, 230.41783142089844)],\n  'scores': [0.9417160943584335, 0.8170979975670818, 0.8095869439224117],\n  'labels': [7, 2, 12]}]\n```\n\n----------------------------------------\n\nTITLE: Creating List Processing UDF with Type Checking in Python\nDESCRIPTION: Demonstrates a UDF that processes a list of string tags, sorts them and joins them into a single string. The function uses explicit type hints with the List generic type for improved type checking.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\n@pxt.udf\ndef process_tags(tags: List[str]) -> str:\n    return \", \".join(sorted(tags))\n\n# Use in computed column\ntable.add_computed_column(\n    formatted_tags=process_tags(table.tags)\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Image URL to Image Type\nDESCRIPTION: Adds a computed column to convert the image URL to Pixeltable's Image type and displays the result.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(image=t.output[0].astype(pxt.Image))\nt.select(t.image).collect()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Sets up the OpenAI API key as an environment variable, prompting the user if not already set\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')\n```\n\n----------------------------------------\n\nTITLE: Configuring Label Studio Environment\nDESCRIPTION: Sets up environment variables for Label Studio URL and API key configuration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nif 'LABEL_STUDIO_URL' not in os.environ:\n    os.environ['LABEL_STUDIO_URL'] = 'http://localhost:8080/'\n\nif 'LABEL_STUDIO_API_KEY' not in os.environ:\n    os.environ['LABEL_STUDIO_API_KEY'] = getpass.getpass('Label Studio API key: ')\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Embedding Index in Pixeltable\nDESCRIPTION: This snippet demonstrates how to import Pixeltable, create a table for a knowledge base, and add an embedding index using a pre-trained Sentence Transformer model. It sets up the foundation for semantic search capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.huggingface import sentence_transformer\n\n# Create a table and add an embedding index\nknowledge_base = pxt.create_table('kb', {\n    'content': pxt.String,\n    'category': pxt.String\n})\n\n# Add an embedding index using a pre-trained model\nknowledge_base.add_embedding_index(\n    column='content',\n    string_embed=sentence_transformer.using(\n        model_id='sentence-transformers/all-MiniLM-L12-v2'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up URL Access for Label Studio Media\nDESCRIPTION: Production-recommended configuration for creating a Label Studio project using URL access method for media files.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npxt.io.create_label_studio_project(\n    videos,\n    video_config,\n    media_import_method='url'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Ollama Server Connection\nDESCRIPTION: This snippet demonstrates how to configure the connection to a remote Ollama server by setting the OLLAMA_HOST environment variable. The code is provided as a commented template.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# To run the notebook against an instance of Ollama running on a\n# remote server, uncomment the following line and specify the URL.\n\n# os.environs['OLLAMA_HOST'] = 'https://127.0.0.1:11434'\n```\n\n----------------------------------------\n\nTITLE: Handling JSON Data in Pixeltable\nDESCRIPTION: Demonstrates creating tables with JSON columns for storing metadata, configurations, and nested structures, including importing from JSON files.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create table with JSON columns\njson_table = pxt.create_table('myproject.json_data', {\n    'metadata': pxt.Json,           # Arbitrary JSON data\n    'config': pxt.Json,             # Configuration objects\n    'features': pxt.Json            # Nested structures\n})\n\n# Insert structured data\njson_table.insert({\n    'metadata': {\n        'source': 'api',\n        'version': '1.0',\n        'tags': ['test', 'example']\n    },\n    'config': {\n        'mode': 'testing',\n        'parameters': {\n            'batch_size': 32,\n            'learning_rate': 0.001\n        }\n    },\n    'features': {\n        'numeric': [1, 2, 3],\n        'categorical': {\n            'color': 'red',\n            'size': 'large'\n        }\n    }\n})\n\n# Import JSON file\npxt.io.import_json('myproject.from_json', 'data.json',\n    schema_overrides={\n        'metadata': pxt.Json,\n        'config': pxt.Json\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Table UDFs in Pixeltable\nDESCRIPTION: Demonstrates how to create a workflow by chaining multiple specialized agent UDFs in sequence. This example shows how to process data through research, analysis, and report generation stages.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Create a chain of specialized agents\nresearch_agent = pxt.udf(research_table, return_value=research_table.findings)\nanalysis_agent = pxt.udf(analysis_table, return_value=analysis_table.insights)\nreport_agent = pxt.udf(report_table, return_value=report_table.document)\n\n# Use them in sequence\nworkflow.add_computed_column(research=research_agent(workflow.query))\nworkflow.add_computed_column(analysis=analysis_agent(workflow.research))\nworkflow.add_computed_column(report=report_agent(workflow.analysis))\n```\n\n----------------------------------------\n\nTITLE: Multi-Index Search Comparison in Pixeltable\nDESCRIPTION: This snippet demonstrates how to compare different embedding models by searching with multiple indexes on the same query. It calculates similarity scores using different embedding indexes and returns results with scores from each model for comparison.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef compare_models(query: str):\n    # Search with different models\n    minilm_sim = docs.content.similarity(\n        query, idx='minilm_idx'\n    )\n    e5_sim = docs.content.similarity(\n        query, idx='e5_idx'\n    )\n    \n    return (\n        docs\n        .select(\n            docs.content,\n            minilm_score=minilm_sim,\n            e5_score=e5_sim\n        )\n        .limit(5)\n    )\n```\n\n----------------------------------------\n\nTITLE: Chaining Image Processing Methods in Pixeltable\nDESCRIPTION: Shows how to chain multiple image processing operations on a frame column using PIL.Image.Image methods. The example demonstrates resizing, rotation, and color mode conversion in a single operation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt.frame.resize((224, 224)).rotate(90).convert('L')\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable and Required Libraries\nDESCRIPTION: Installation of the Pixeltable library along with PyTorch and Transformers for the tutorial examples.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable torch transformers\n```\n\n----------------------------------------\n\nTITLE: Exporting Data from Pixeltable\nDESCRIPTION: Shows how to extract data from Pixeltable for analysis, including converting results to Python objects and Pandas DataFrames.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Get results as Python objects\nresult = films.limit(5).collect()\nfirst_row = result[0]  # Get first row as dict\ntimestamps = result['timestamp']  # Get list of values for one column\n\n# Convert to Pandas\ndf = result.to_pandas()\ndf['revenue'].describe()  # Get statistics for revenue column\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables for Mistral API Key\nDESCRIPTION: This snippet shows how to create a .env file to store the Mistral API key, which is required for OCR and text processing in the application.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nMISTRAL_API_KEY=your-mistral-api-key\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable YOLOX with pip\nDESCRIPTION: Simple pip installation command for the Pixeltable YOLOX library\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/yolox.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable-yolox\n```\n\n----------------------------------------\n\nTITLE: Creating Text Data Table in Pixeltable\nDESCRIPTION: Creates a table with text data from Wikipedia articles and populates it with sample sentences about Pablo Picasso.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntxts = pxt.create_table('indices_demo.text_tbl', {'text': pxt.String})\nsentences = [\n    \"Pablo Ruiz Picasso (25 October 1881 â 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his adult life in France.\",\n    \"One of the most influential artists of the 20th century, he is known for co-founding the Cubist movement, the invention of constructed sculpture,[8][9] the co-invention of collage, and for the wide variety of styles that he helped develop and explore.\",\n    \"Among his most famous works are the proto-Cubist Les Demoiselles d'Avignon (1907) and the anti-war painting Guernica (1937), a dramatic portrayal of the bombing of Guernica by German and Italian air forces during the Spanish Civil War.\",\n    \"Picasso demonstrated extraordinary artistic talent in his early years, painting in a naturalistic manner through his childhood and adolescence.\",\n    \"During the first decade of the 20th century, his style changed as he experimented with different theories, techniques, and ideas.\",\n    \"After 1906, the Fauvist work of the older artist Henri Matisse motivated Picasso to explore more radical styles, beginning a fruitful rivalry between the two artists, who subsequently were often paired by critics as the leaders of modern art.\",\n    \"Picasso's output, especially in his early career, is often periodized.\",\n    \"While the names of many of his later periods are debated, the most commonly accepted periods in his work are the Blue Period (1901â1904), the Rose Period (1904â1906), the African-influenced Period (1907â1909), Analytic Cubism (1909â1912), and Synthetic Cubism (1912â1919), also referred to as the Crystal period.\",\n    \"Much of Picasso's work of the late 1910s and early 1920s is in a neoclassical style, and his work in the mid-1920s often has characteristics of Surrealism.\",\n    \"His later work often combines elements of his earlier styles.\",\n]\ntxts.insert({'text': s} for s in sentences)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: Installs Pixeltable and OpenAI Python packages using pip\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable openai\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Gemini Integration in Python\nDESCRIPTION: This snippet installs the necessary Python libraries (pixeltable and google-generativeai) for working with Gemini in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable google-generativeai\n```\n\n----------------------------------------\n\nTITLE: Managing Documents in Pixeltable\nDESCRIPTION: Demonstrates creating tables for document files and inserting documents from local files, URLs, and cloud storage.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create table with document columns\ndoc_table = pxt.create_table('myproject.documents', {\n    'document': pxt.Document,     # Document file reference\n})\n\n# Insert local documents\ndoc_table.insert([\n    {'document': '/path/to/doc1.pdf'},\n    {'document': '/path/to/doc2.docx'},\n    {'document': '/path/to/text1.md'}\n])\n\n# Insert document URLs\ndoc_table.insert([\n    {'document': 'https://example.com/doc1.pdf'},\n    {'document': 'https://example.com/doc2.docx'}\n])\n\n# Insert from cloud storage\ndoc_table.insert([\n    {'document': 's3://my-bucket/doc1.pdf'}\n])\n```\n\n----------------------------------------\n\nTITLE: Managing Embedding Indexes in Pixeltable\nDESCRIPTION: This snippet demonstrates how to manage existing embedding indexes in Pixeltable. It shows how to drop an index by name or by column, and how updates to indexed content automatically trigger index updates.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Drop by name\ndocs.drop_embedding_index(idx_name='e5_idx')\n\n# Drop by column (if single index)\ndocs.drop_embedding_index(column='content')\n\n# Indexes auto-update on changes\ndocs.update({\n    'content': docs.content + ' Updated!'\n})\n```\n\n----------------------------------------\n\nTITLE: Inserting Additional Image Data and Re-running Similarity Search\nDESCRIPTION: Demonstrates automatic index updates by inserting more image data and re-running the similarity search query, showing how results change with new data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmore_img_urls = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000080.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000090.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000106.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000108.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000139.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000285.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000632.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000724.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000776.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000785.jpg',\n]\nimgs.insert({'id': 10 + i, 'img': url} for i, url in enumerate(more_img_urls))\n\nsim = imgs.img.similarity(sample_img)\nres = (\n    imgs.order_by(sim, asc=False)\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres\n```\n\n----------------------------------------\n\nTITLE: Visualizing Object Detection Results as Video\nDESCRIPTION: Uses Pixeltable's make_video function to create a video with annotated frames showing object detection results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nframes_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        draw_boxes(\n            frames_view.frame,\n            frames_view.detect_yolox_tiny.bboxes\n        )\n    )\n).show(1)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Dependencies for Custom Embedding\nDESCRIPTION: Installs required TensorFlow packages for implementing custom BERT embeddings.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU tensorflow tensorflow-hub tensorflow-text\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Schema with Basic Types\nDESCRIPTION: Shows how to create a table schema with basic data types including String, Int, Float, Bool, and Timestamp.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Schema definition\ntable = pxt.create_table('example', {\n    'text': pxt.String,     # Text data\n    'count': pxt.Int,       # Integer numbers\n    'score': pxt.Float,     # Decimal numbers\n    'active': pxt.Bool,     # Boolean values\n    'created': pxt.Timestamp # Date/time values\n})\n```\n\n----------------------------------------\n\nTITLE: Querying error information for invalid data in Pixeltable\nDESCRIPTION: These snippets show how to query and inspect error information for invalid data entries in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nv.select(v.video == None, v.video.errortype, v.video.errormsg).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\nv.where(v.video.errortype != None).select(v.video.errormsg).collect()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PDF Search\nDESCRIPTION: Installs the necessary Python packages for creating a PDF search system with Pixeltable. This includes pixeltable for the core functionality, tiktoken for token handling, and sentence-transformers for generating text embeddings.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/PDF.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable tiktoken sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable Image Search\nDESCRIPTION: Initial setup command to install the required Python packages: Pixeltable for the data management framework, OpenAI for GPT-4 Vision access, and sentence-transformers for embedding functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable openai sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search\nDESCRIPTION: Demonstrates how to perform similarity search on images using the created embedding index.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-hugging-face.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsample_img = t.select(t.Image).head(1)[0]['Image']\n\nsim = t.Image.similarity(sample_img)\n\n# use 'similarity()' in the order_by() clause and apply a limit in order to utilize the index\nt.order_by(sim, asc=False).limit(3).select(t.Image, sim=sim).collect()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Pixeltable Tool-calling Agents\nDESCRIPTION: Terminal command to install the necessary Python packages for building AI agents with Pixeltable. Includes the core Pixeltable library, OpenAI for model access, and DuckDuckGo search for web queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/tools.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable openai duckduckgo-search\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install necessary Python packages for using Hugging Face models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers torch sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Performing Column Casting in Pixeltable\nDESCRIPTION: Shows various ways to cast columns to different data types, including in computed columns and expressions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Cast columns to different types\ntable.update({\n    'int_score': table.score.astype(pxt.Int),        # Cast float to integer\n    'string_count': table.count.astype(pxt.String),  # Cast integer to string\n})\n\n# Using casting in computed columns\nfilms.add_computed_column(\n    budget_category=films.budget.astype(pxt.String) + ' million'\n)\n\n# Casting in expressions\nfilms.where(films.revenue.astype(pxt.Int) > 100).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple Embedding Indexes on Same Column in Pixeltable\nDESCRIPTION: This snippet demonstrates how to create multiple embedding indexes on the same column using different models. This approach allows comparing the performance of different embedding models for the same data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create multiple indexes on same column\ndocs.add_embedding_index(\n    column='content',\n    idx_name='minilm_idx',\n    string_embed=sentence_transformer.using(\n        model_id='sentence-transformers/all-MiniLM-L12-v2'\n    )\n)\n\ndocs.add_embedding_index(\n    column='content',\n    idx_name='e5_idx',\n    string_embed=sentence_transformer.using(\n        model_id='intfloat/e5-large-v2'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Computing CLIP Image Embeddings in Python\nDESCRIPTION: Function for computing CLIP image embeddings using Hugging Face models. It processes image inputs and returns embeddings that can be used for similarity or retrieval tasks.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/huggingface.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef clip_image_embeddings(img_path: typing.Union[str, pl.Expr, list, tuple],\n                         model: typing.Optional[typing.Union[str, pl.Expr]] = None,\n                         model_kwargs: typing.Optional[dict] = None,\n                         preprocess_params: typing.Optional[dict] = None,\n                         **kwargs) -> pl.Expr:\n    \"\"\"\n    Compute CLIP image embeddings.\n    \"\"\"\n    from pixeltable.functions.huggingface.huggingface_clip import HuggingfaceClip\n    from pixeltable.functions.utils import resolve_param\n\n    return HuggingfaceClip(\n        model=resolve_param(model),\n        model_kwargs=resolve_param(model_kwargs),\n        preprocess_params=resolve_param(preprocess_params),\n        **kwargs\n    ).image_embeddings(resolve_param(img_path))\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection on Video Frames using Pixeltable and DETR\nDESCRIPTION: This snippet adds a computed column to the frames table, applying the DETR-ResNet-50 object detection model to each frame. It then selects and displays the first 3 rows of the resulting detections.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nframes.add_computed_column(detections=detr_for_object_detection(\n    frames.frame,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.95\n)\nframes.select(frames.frame, frames.detections).head(3)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable Package\nDESCRIPTION: Install the Pixeltable Python package using pip, which is required for building the memory-enabled chatbot.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/memory.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable\n```\n\n----------------------------------------\n\nTITLE: Basic Table Queries\nDESCRIPTION: Shows basic table querying operations including limit, head, and tail functions to retrieve subsets of data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\neq_t.limit(5).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.head(5)\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.tail(5)\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Table in Pixeltable\nDESCRIPTION: Creates a new table for storing and processing images using a simple schema with an image column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nt = pxt.create_table('fundamentals.image_ops', {'source': pxt.Image})\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory\nDESCRIPTION: Creates a new Pixeltable directory for the demo, ensuring a clean slate by dropping any existing directory\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('demo')\n```\n\n----------------------------------------\n\nTITLE: Building an API Endpoint with FastAPI and Pixeltable\nDESCRIPTION: This snippet demonstrates how to create a FastAPI endpoint that leverages Pixeltable for document analysis. It defines a request model, inserts incoming documents into a Pixeltable table, and returns analysis results from computed columns like embeddings, summaries, and sentiment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass AnalysisRequest(BaseModel):\n    document: str\n    metadata: dict = {}\n\n@app.post(\"/analyze\")\nasync def analyze_document(request: AnalysisRequest):\n    try:\n        # Insert document for processing\n        analysis.insert([{\n            'document': request.document,\n            'metadata': request.metadata,\n            'timestamp': datetime.now()\n        }])\n        \n        # Get analysis results using computed columns\n        result = analysis.select(\n            analysis.embeddings,\n            analysis.summary,\n            analysis.sentiment\n        ).tail(1)\n        \n        return {\n            \"status\": \"success\",\n            \"results\": result.to_dict('records')[0]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=0, detail=str(e))\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable\nDESCRIPTION: This command installs the Pixeltable package using pip, which is necessary for creating and managing data tables with LLM integration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable\n```\n\n----------------------------------------\n\nTITLE: Creating a UDF for Extracting the Longest Word\nDESCRIPTION: This snippet defines a UDF that extracts the longest word from a sentence, with an option to strip punctuation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n@pxt.udf\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:  # Remove non-alphanumeric characters from each word\n        words = [''.join(filter(str.isalnum, word)) for word in words]\n    i = np.argmax([len(word) for word in words])\n    return words[i]\n```\n\n----------------------------------------\n\nTITLE: Creating a UDF for Extracting the Longest Word\nDESCRIPTION: This snippet defines a UDF that extracts the longest word from a sentence, with an option to strip punctuation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n@pxt.udf\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:  # Remove non-alphanumeric characters from each word\n        words = [''.join(filter(str.isalnum, word)) for word in words]\n    i = np.argmax([len(word) for word in words])\n    return words[i]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pixeltable Workspace for RAG Operations\nDESCRIPTION: Creates a clean Pixeltable directory 'rag_ops_demo' for the demo workflow.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Ensure a clean slate for the demo\npxt.drop_dir('rag_ops_demo', force=True)\n# Create the Pixeltable workspace\npxt.create_dir('rag_ops_demo')\n```\n\n----------------------------------------\n\nTITLE: Setting Up Production Environment in Pixeltable\nDESCRIPTION: Demonstrates a common pattern for setting up a production environment in Pixeltable, including creating tables and handling errors.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# table.py - Run once to set up\npxt.create_table(..., if_exists=\"ignore\")\n\n# app.py - Production code\ntable = pxt.get_table(\"myapp.mytable\")\nif table is None:\n    raise RuntimeError(\"Run table.py first!\")\n```\n\n----------------------------------------\n\nTITLE: Auto-updating Image Database Insertion\nDESCRIPTION: Snippet showing how to insert new images into the database, which automatically triggers description generation and embedding updates. This demonstrates the self-maintaining nature of the workflow.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/images.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimg_t.insert({\"image\": new_url})\n# Descriptions and embeddings update automatically\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedding Index for Image Similarity Search\nDESCRIPTION: This snippet shows how to create an embedding index on an image column using CLIP model from Hugging Face. It demonstrates setting up the index and performing similarity searches with filtering and ordering capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'img' column of table 't'\nt.add_embedding_index(\n    'img',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)\n\n# index is kept up-to-date enabling relevant searches\nsim = t.img.similarity(sample_img)\n\nres = (\n    t.order_by(sim, asc=False)  # Order by similarity\n    .where(t.id != 6)  # Metadata filtering\n    .limit(2)  # Limit number of results to 2\n    .select(t.id, t.img, sim)\n    .collect()  # Retrieve results now\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Table for Video Data\nDESCRIPTION: Creates a Pixeltable directory and table to store video data. The table is initialized with a 'video' column of type Video.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pixeltable as pxt\n\npxt.drop_dir('transcription_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('transcription_demo')\n\n# Create a table to store our videos and workflow\nvideo_table = pxt.create_table(\n    'transcription_demo.video_table',\n    {'video': pxt.Video}\n)\n\nvideo_table\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI with npm\nDESCRIPTION: Command to install the Mintlify CLI globally using npm. This is a prerequisite for working with the documentation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom UDF for Top Detection Extraction\nDESCRIPTION: Defines a user-defined function (UDF) to extract the highest-scoring detection from object detection results. The function is then used in a computed column to automatically process detection data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef top_detection(detect: dict) -> str:\n    scores = detect['scores']\n    label_text = detect['label_text']\n    i = scores.index(max(scores))\n    return label_text[i]\n\n# Use it in a computed column\nt.add_computed_column(top=top_detection(t.detections))\n```\n\n----------------------------------------\n\nTITLE: Testing Chat Completion\nDESCRIPTION: Inserts a test prompt and displays the chat completion results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nt.insert(prompt='What foods are rich in selenium?')\nt.select(t.prompt, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Installing Frontend Dependencies for Next.js Video Frame Search Application\nDESCRIPTION: This snippet shows the commands to set up the frontend environment for the video frame search application. It includes changing to the frontend directory, installing dependencies, and starting the development server.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi/frontend\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Deploying Pixeltable Backend API with Docker\nDESCRIPTION: Docker commands to build and run the Pixeltable backend API container, exposing it on port 8000.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/backend/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t multimodal-api .\ndocker run -p 8000:8000 multimodal-api\n```\n\n----------------------------------------\n\nTITLE: FastAPI Integration with Pixeltable\nDESCRIPTION: Demonstrates integration of Pixeltable with FastAPI for building chat endpoints that utilize RAG capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# FastAPI + Pixeltable example\n@app.post(\"/chat\")\nasync def chat(message: ChatMessage):\n    # Insert question\n    chat_table.insert([{\n        'question': message.message,\n        'timestamp': datetime.now()\n    }])\n    \n    # Get answer (computed columns handle RAG pipeline)\n    result = chat_table.select(\n        chat_table.response\n    ).where(\n        chat_table.question == message.message\n    ).collect()\n    \n    return JSONResponse(\n        status_code=200,\n        content={\"response\": result['response'][0]}\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Frontend Development Environment\nDESCRIPTION: Commands for setting up the Next.js frontend development environment. It installs Node.js dependencies and starts the Next.js development server.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Backend Installation Commands\nDESCRIPTION: Bash commands for setting up the Python virtual environment and installing project dependencies\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Create virtual environment (optional)\npython -m venv venv\nsource venv/bin/activate  # Windows: .\\venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Start backend\npython -m uvicorn main:app\n```\n\n----------------------------------------\n\nTITLE: Text Filtering in Pixeltable\nDESCRIPTION: Shows how to filter text content using contains() method on plot and title columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmovies.where(\n    movies.plot.contains('secret')\n).collect()\n\nmovies.where(\n    movies.title.contains('Park')\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Text Transformation UDF in Python\nDESCRIPTION: Demonstrates how to create and use a basic UDF for text cleaning in Pixeltable. The function takes a string input, performs lowercase conversion and whitespace trimming, then returns the cleaned text which is used in a computed column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Basic UDF for text transformation\n@pxt.udf\ndef clean_text(text: str) -> str:\n    \"\"\"Clean and normalize text data.\"\"\"\n    return text.lower().strip()\n\n# Use in a computed column\ndocuments = pxt.get_table('my_documents')\ndocuments.add_computed_column(\n    clean_content=clean_text(documents.content)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Base Tables and Views with Document Splitting in Pixeltable\nDESCRIPTION: This snippet demonstrates how to set up a directory structure, create a base table for documents, and create a view that automatically splits documents into chunks using the DocumentSplitter iterator. The view will maintain chunks that correspond to documents in the base table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import DocumentSplitter\n\n# Create a directory to organize data (optional)\npxt.drop_dir('documents', force=True)\npxt.create_dir('documents')\n\n# Define your base table first\ndocuments = pxt.create_table(\n    \"documents.collection\",\n    {\"document\": pxt.Document}\n)\n\n# Create a view that splits documents into chunks\nchunks = pxt.create_view(\n    'documents.chunks',\n    documents,\n    iterator=DocumentSplitter.create(\n        document=documents.document,\n        separators='token_limit',\n        limit=300\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Video Table and Frame View\nDESCRIPTION: Creates a table with video data and a view for frames extracted from videos. Uses FrameIterator to extract frames at the original frame rate (fps=0).\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import FrameIterator\nt = pxt.create_table('tbl_name', {'video': pxt.Video})\nf = pxt.create_view('frame_view_name', t, iterator=FrameIterator.create(video=t.video, fps=0))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment and Installing Dependencies\nDESCRIPTION: These commands create a virtual environment, activate it, and install the required Python packages for the project.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing Input Validation in UDF in Python\nDESCRIPTION: Shows how to add input validation to a UDF to ensure data quality. The function checks if a score is within a valid range (0-100) and raises an error if the validation fails.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef validate_score(score: float) -> float:\n    if not 0 <= score <= 100:\n        raise ValueError(\"Score must be between 0 and 100\")\n    return score\n```\n\n----------------------------------------\n\nTITLE: Extracting Audio from Video in Python\nDESCRIPTION: This function extracts the audio track from a video file. It takes a video file path as input and returns the extracted audio as a numpy array along with the sample rate.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/video.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_audio(video_path: str) -> Tuple[np.ndarray, int]:\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Analysis Function for LLM Inference and Output Analysis\nDESCRIPTION: Defines the core function that creates a Pixeltable table, runs inference using multiple models, analyzes outputs, and tracks results in versioned tables.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/pixeltable-and-gradio-application.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef run_inference_and_analysis(task, system_prompt, input_text, temperature, top_p, max_tokens, stop, random_seed, safe_prompt):\n    # Initialize Pixeltable\n    pxt.drop_table('mistral_prompts', ignore_errors=True)\n    t = pxt.create_table('mistral_prompts', {\n        'task': pxt.String,\n        'system': pxt.String,\n        'input_text': pxt.String,\n        'timestamp': pxt.Timestamp,\n        'temperature': pxt.Float,\n        'top_p': pxt.Float,\n        'max_tokens': pxt.Int,\n        'stop': pxt.String,\n        'random_seed': pxt.Int,\n        'safe_prompt': pxt.Bool\n    })\n\n    # Insert new row into Pixeltable\n    t.insert([{\n        'task': task,\n        'system': system_prompt,\n        'input_text': input_text,\n        'timestamp': datetime.now(),\n        'temperature': temperature,\n        'top_p': top_p,\n        'max_tokens': max_tokens,\n        'stop': stop,\n        'random_seed': random_seed,\n        'safe_prompt': safe_prompt\n    }])\n\n    # Define messages for chat completion\n    msgs = [\n        {'role': 'system', 'content': t.system},\n        {'role': 'user', 'content': t.input_text}\n    ]\n\n    common_params = {\n        'messages': msgs,\n        'temperature': temperature,\n        'top_p': top_p,\n        'max_tokens': max_tokens if max_tokens is not None else 300,\n        'stop': stop.split(',') if stop else None,\n        'random_seed': random_seed,\n        'safe_prompt': safe_prompt\n    }\n\n    # Add computed columns for model responses and analysis\n    t.add_computed_column(open_mistral_nemo=chat_completions(model='open-mistral-nemo', **common_params))\n    t.add_computed_column(mistral_medium=chat_completions(model='mistral-medium', **common_params))\n\n    # Extract responses\n    t.add_computed_column(omn_response=t.open_mistral_nemo.choices[0].message.content.astype(pxt.String))\n    t.add_computed_column(ml_response=t.mistral_medium.choices[0].message.content.astype(pxt.String))\n\n    # Add computed columns for analysis\n    t.add_computed_column(large_sentiment_score=get_sentiment_score(t.ml_response))\n    t.add_computed_column(large_keywords=extract_keywords(t.ml_response))\n    t.add_computed_column(large_readability_score=calculate_readability(t.ml_response))\n    t.add_computed_column(open_sentiment_score=get_sentiment_score(t.omn_response))\n    t.add_computed_column(open_keywords=extract_keywords(t.omn_response))\n    t.add_computed_column(open_readability_score=calculate_readability(t.omn_response))\n\n    # Retrieve results\n    results = t.select(\n        t.omn_response, t.ml_response,\n        t.large_sentiment_score, t.open_sentiment_score,\n        t.large_keywords, t.open_keywords,\n        t.large_readability_score, t.open_readability_score\n    ).tail(1)\n\n    history = t.select(t.timestamp, t.task, t.system, t.input_text).order_by(t.timestamp, asc=False).collect().to_pandas()\n    responses = t.select(t.timestamp, t.omn_response, t.ml_response).order_by(t.timestamp, asc=False).collect().to_pandas()\n    analysis = t.select(\n        t.timestamp,\n        t.open_sentiment_score,\n        t.large_sentiment_score,\n        t.open_keywords,\n        t.large_keywords,\n        t.open_readability_score,\n        t.large_readability_score\n    ).order_by(t.timestamp, asc=False).collect().to_pandas()\n    params = t.select(\n        t.timestamp,\n        t.temperature,\n        t.top_p,\n        t.max_tokens,\n        t.stop,\n        t.random_seed,\n        t.safe_prompt\n    ).order_by(t.timestamp, asc=False).collect().to_pandas()\n\n    return (\n        results['omn_response'][0],\n        results['ml_response'][0],\n        results['large_sentiment_score'][0],\n        results['open_sentiment_score'][0],\n        results['large_keywords'][0],\n        results['open_keywords'][0],\n        results['large_readability_score'][0],\n        results['open_readability_score'][0],\n        history,\n        responses,\n        analysis,\n        params\n    )\n```\n\n----------------------------------------\n\nTITLE: Aligning Transcriptions with WhisperX in Python\nDESCRIPTION: This function aligns existing transcriptions with audio using WhisperX. It takes the audio file path, transcription text, and optional parameters like language and device type to perform the alignment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/ext/functions/whisperx.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef whisperx_align(audio_path: str, transcription: str, language: Optional[str] = None, device: str = \"cuda\") -> Dict[str, Any]:\n    import whisperx\n\n    # Load audio\n    audio = whisperx.load_audio(audio_path)\n\n    # Load alignment model and metadata\n    model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n\n    # Align\n    result = whisperx.align(transcription, model_a, metadata, audio, device)\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Command Flow Diagram using Mermaid\nDESCRIPTION: Sequence diagram showing the interaction flow between User, Discord Bot, Pixeltable, and various services for message processing and response generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/context-aware-discord-bot/README.md#2025-04-07_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant U as User\n    participant B as Discord Bot\n    participant F as Message Formatter\n    participant P as Pixeltable\n    participant MT as Messages Table\n    participant MV as Sentences View\n    participant CT as Chat Table\n    participant H as HuggingFace<br/>E5-large-v2\n    participant O as OpenAI<br/>GPT-4\n\n    rect rgb(230, 240, 255)\n        Note over U,O: Step 1: Message Storage & Command Processing\n        U->>+B: /chat or /search command\n        B->>F: Create processing message\n        F-->>U: Show processing status\n        B->>P: Process command\n        rect rgb(250, 251, 248)\n            Note right of P: Pixeltable does:<br/>1. Incremetal updates<br/>2. Chunking<br/>3.Versioning/Lineage\n            alt Chat Command\n                P->>CT: Store question in Chat Table\n            else Message Storage\n                P->>MT: Store in Messages Table\n                MT->>MV: Update Sentences View\n            end\n        end\n    end\n\n    rect rgb(255, 235, 235)\n        Note over U,O: Step 2: Semantic Search\n        rect rgb(250, 251, 248)\n            Note right of P: Pixeltable does:<br/>1. Embedding index management<br/>2. Similarity search<br/>3. Context ranking\n            par Embedding Generation\n                MV->>H: Get sentence embeddings\n                H-->>MV: Return vectors\n            and Context Retrieval\n                CT->>MV: Find similar sentences\n                MV-->>CT: Return relevant context\n                Note right of MV: Similarity threshold: 0.3\n            end\n        end\n    end\n\n    rect rgb(235, 255, 235)\n        Note over U,O: Step 3: Response Generation\n        rect rgb(250, 251, 248)\n            Note right of P: Pixeltable does:<br/>1. Context assembly<br/>2. Prompt construction<br/>3. API orchestration\n            CT->>P: Build context prompt\n            P->>O: Request completion\n            O-->>P: Return AI response\n        end\n    end\n\n    rect rgb(240, 240, 245)\n        Note over U,O: Step 4: Response Delivery\n        P->>F: Format response & context\n        F->>B: Create embed message\n        B->>U: Send formatted response\n    end\n```\n\n----------------------------------------\n\nTITLE: Documenting Audio Functions Module Path\nDESCRIPTION: Module path definition for Pixeltable's audio processing functions documentation\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/audio.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: pixeltable.functions.audio\n```\n\n----------------------------------------\n\nTITLE: Importing Deepseek AI Function Module in Python\nDESCRIPTION: This code snippet shows the module import statement for the Deepseek AI functions in Pixeltable. It suggests that the module contains various functions for interacting with the Deepseek AI model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/deepseek.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n## ::: pixeltable.functions.deepseek\n```\n\n----------------------------------------\n\nTITLE: Result Sorting in Pixeltable\nDESCRIPTION: Shows how to sort query results using order_by() with single and multiple columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmovies.order_by(\n    movies.budget, \n    asc=False\n).collect()\n\nmovies.order_by(\n    [movies.year, movies.budget], \n    asc=[True, False]\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Pixeltable MCP Servers\nDESCRIPTION: Commands for cloning the repository and starting the MCP servers using docker-compose. This setup enables local deployment of all MCP servers for multimodal data processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/overview.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/pixeltable/mcp-server-pixeltable.git\ncd mcp-server-pixeltable/servers\n\n# Run locally with docker-compose\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Creating embeddings with Gemini embedding model\nDESCRIPTION: Implements a function to create embeddings from text using the Gemini embedding model. It transforms text into a vector representation that can be used for semantic similarity comparisons and other NLP tasks.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/gemini.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@registry.function(\n    name='gemini.embedding',\n    return_type=types.List(types.Float),\n    param_types={\n        'text': types.String,\n    }\n)\ndef create_embedding(text: str) -> List[float]:\n    \"\"\"\n    Create an embedding from text using Google's embedding model.\n\n    Args:\n        text: The text to create an embedding for\n\n    Returns:\n        A list of floats representing the embedding vector\n    \"\"\"\n    _setup_api_key()\n\n    try:\n        model = 'models/embedding-001'\n        embedding = genai.embed_content(model=model, content=text)\n        return embedding[\"embedding\"]\n    except Exception as e:\n        raise ReturnValueError(f'Error creating embedding with Gemini: {str(e)}')\n```\n\n----------------------------------------\n\nTITLE: Extracting and Chunking Audio from Video\nDESCRIPTION: Demonstrates how to extract audio from video data and create audio chunks. Creates a table with video data, adds a computed column for audio extraction, and creates a view for chunked audio.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import AudioSplitter\nvideo_t = pxt.create_table('tbl_name', {'video': pxt.Video})\nvideo_t.add_computed_column(audio=video_t.video.extract_audio(format='mp3'))\naudio_chunk_view = pxt.create_view(\"audio_chunks\", video_t, iterator=AudioSplitter.create(audio=video_t.audio, chunk_duration_sec=5.0))\n```\n\n----------------------------------------\n\nTITLE: Rendering Next Steps Cards using JSX\nDESCRIPTION: This code snippet uses JSX to create a card group with three cards, each representing a different next step for Pixeltable users. The cards include titles, icons, and links to relevant resources.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_10\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={3}>\n  <Card \n    title=\"Code Examples\" \n    icon=\"code\" \n    href=\"/docs/fundamentals/sample-apps\"\n  >\n    Working implementations and reference architecture\n  </Card>\n  <Card \n    title=\"Developer Community\" \n    icon=\"discord\" \n    href=\"https://discord.gg/QPyqFYx2UN\"\n  >\n    Technical discussions and implementation support\n  </Card>\n  <Card \n    title=\"Source Repository\" \n    icon=\"github\" \n    href=\"https://github.com/pixeltable/pixeltable\"\n  >\n    Explore the codebase and contribute\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Using Label Studio Annotation App with Pixeltable\nDESCRIPTION: Application script that connects to existing Pixeltable tables, inserts videos, syncs with Label Studio, and retrieves annotation results. Demonstrates the workflow for loading data and retrieving annotations after they're completed.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nimport os\n\n# Set up Label Studio connection\nif 'LABEL_STUDIO_URL' not in os.environ:\n    os.environ['LABEL_STUDIO_URL'] = 'http://localhost:8080/'\n\n# Connect to your tables\nvideos = pxt.get_table(\"annotation.videos\")\nframes = pxt.get_table(\"annotation.frames\")\n\n# Insert videos\nurl_prefix = 'http://multimedia-commons.s3-website-us-west-2.amazonaws.com/data/videos/mp4/'\nvideo_files = [\n    '122/8ff/1228ff94bf742242ee7c88e4769ad5d5.mp4',\n    '2cf/a20/2cfa205eae979b31b1144abd9fa4e521.mp4'\n]\n\nvideos.insert([{\n    'video': url_prefix + file,\n    'date': datetime.now()\n} for file in video_files])\n\n# Sync with Label Studio\nvideos.sync()\nframes.sync()\n\n# After annotation, retrieve results\nresults = videos.select(\n    videos.video,\n    videos.annotations,\n    category=videos.annotations[0].result[0].value.choices[0]\n).collect()\n\nframe_results = frames.select(\n    frames.frame,\n    frames.annotations,\n    frames.preannotations\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Extracting Frames from Video in Python\nDESCRIPTION: This function extracts frames from a video file at specified intervals. It takes a video file path, frame interval, and optional maximum number of frames as inputs. The function returns a list of extracted frames as numpy arrays.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/video.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef extract_frames(video_path: str, frame_interval: float, max_frames: Optional[int] = None) -> List[np.ndarray]:\n```\n\n----------------------------------------\n\nTITLE: Implementing Unlimited Memory in Pixelagent\nDESCRIPTION: This snippet demonstrates how to create an agent with unlimited conversation history by setting n_latest_messages to None. This allows the agent to remember everything from the conversation without any memory limitations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Unlimited memory\ninfinite_agent = Agent(\n    agent_name=\"historian\",\n    system_prompt=\"You remember everything.\",\n    n_latest_messages=None  # No limit on conversation history\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for Pixeltable\nDESCRIPTION: Commands for creating a new conda environment with Python 3.9 for Pixeltable development and activating it.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name pxt python=3.9\nconda activate pxt\n```\n\n----------------------------------------\n\nTITLE: Initializing MistralAI Client in Python\nDESCRIPTION: This function initializes and returns a MistralAI client using the provided API key. It checks for a valid API key and raises an exception if not found.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/mistralai.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_mistral_client():\n    api_key = os.environ.get('MISTRAL_API_KEY')\n    if not api_key:\n        raise Exception('MISTRAL_API_KEY environment variable not set')\n    return MistralClient(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for UDA Demonstration\nDESCRIPTION: Sets up a sample table with integers from 0 to 49 to demonstrate user-defined aggregates. The table has a single integer column named 'val'.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\nt = pxt.create_table('udf_demo.values', {'val': pxt.Int})\nt.insert({'val': n} for n in range(50))\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for UDA Demonstration\nDESCRIPTION: Sets up a sample table with integers from 0 to 49 to demonstrate user-defined aggregates. The table has a single integer column named 'val'.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\nt = pxt.create_table('udf_demo.values', {'val': pxt.Int})\nt.insert({'val': n} for n in range(50))\n```\n\n----------------------------------------\n\nTITLE: Splitting Audio with AudioSplitter in Python\nDESCRIPTION: This snippet demonstrates how to use the AudioSplitter iterator to split audio files into chunks with specified duration, overlap, and minimum chunk duration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators import AudioSplitter\n\n# Split audio into chunks\nchunks_view = pxt.create_view(\n    'audio.chunks',\n    audio_table,\n    iterator=AudioSplitter.create(\n        audio=audio_table.audio,\n        chunk_duration_sec=30.0,  # Split into 30-second chunks\n        overlap_sec=2.0,          # 2-second overlap between chunks\n        min_chunk_duration_sec=5.0 # Drop last chunk if < 5 seconds\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Pixeltable MCP Servers with Docker\nDESCRIPTION: This bash script demonstrates how to clone the Pixeltable MCP server repository and start the servers using docker-compose. It sets up all necessary MCP servers as Docker containers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/mcp.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/pixeltable/mcp-server-pixeltable.git\ncd mcp-server-pixeltable/servers\n\n# Run locally with docker-compose\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Defining ReplicateClient Class for AI Model Execution\nDESCRIPTION: Implements a client for Replicate's API that allows execution of machine learning models. The class provides methods to run models, handle responses, and manage prediction results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/replicate.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\nimport json\nimport logging\nimport concurrent.futures\nfrom typing import Dict, Optional, List, Union, Any, Tuple\nimport requests\nimport tempfile\n\nfrom pixeltable.env import Env\nimport pixeltable.catalog as catalog\nimport pixeltable.type_system as ts\nfrom pixeltable.utils.table_utils import create_replicate_output_schema\nfrom pixeltable.exceptions import Error\n\n\nclass ReplicateClient:\n    \"\"\"\n    Client for Replicate's API. Lets you run machine learning models from Python.\n    This is a simplified version of the official Replicate's client,\n    customized for use in Pixeltable.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.api_url = \"https://api.replicate.com/v1\"\n        self.api_key = os.environ.get(\"REPLICATE_API_TOKEN\")\n        self.timeout = 300  # 5 minutes\n        if self.api_key is None:\n            # The API key is required, we should error out if it's not set\n            # Also for batch predictions\n            raise Error(\"REPLICATE_API_TOKEN environment variable not set\")\n\n    def get_model(self, model_id: str) -> Dict[str, Any]:\n        path = f\"/models/{model_id}\"\n        response = self._request(path=path, method=\"GET\")\n        return response.json()\n\n    def get_model_version(self, model_id: str, version_id: str) -> Dict[str, Any]:\n        if version_id == \"latest\" or version_id.startswith(\"latest:\"):\n            # Get all versions\n            path = f\"/models/{model_id}/versions\"\n            response = self._request(path=path, method=\"GET\")\n            model = response.json()\n            if len(model[\"results\"]) == 0:\n                raise Error(f\"Model {model_id} has no versions\")\n            if version_id == \"latest\":\n                return model[\"results\"][0]\n            tag = version_id.split(\":\")[1]\n            for version in model[\"results\"]:\n                if tag in version[\"tags\"]:\n                    return version\n            raise Error(f\"Model {model_id} has no version tagged {tag}\")\n        path = f\"/models/{model_id}/versions/{version_id}\"\n        response = self._request(path=path, method=\"GET\")\n        return response.json()\n\n    def run_sync(\n        self,\n        model_id: str,\n        input_data: Dict[str, Any],\n        version_id: str = \"latest\",\n    ) -> Dict:\n        \"\"\"\n        Run a model and return the output.\n        Wait until the prediction finishes to return results.\n        \"\"\"\n        model_version = self.get_model_version(model_id=model_id, version_id=version_id)\n\n        response = self._request(\n            path=\"/predictions\",\n            method=\"POST\",\n            json={\n                \"version\": model_version[\"id\"],\n                \"input\": input_data,\n            },\n        )\n        prediction = response.json()\n\n        while prediction[\"status\"] != \"succeeded\" and prediction[\"status\"] != \"failed\":\n            time.sleep(0.1)\n            response = self._request(\n                path=f\"/predictions/{prediction['id']}\",\n                method=\"GET\",\n            )\n            prediction = response.json()\n\n        if prediction[\"status\"] == \"failed\":\n            raise Error(f\"Prediction failed: {prediction['error']}\")\n\n        return prediction[\"output\"]\n\n    def download_replicate_json(\n        self, model_id: str, model_version_id: str = \"latest\"\n    ) -> Tuple[Dict, Dict]:\n        \"\"\"\n        Download model information in json format, which will be used by\n        pixletable to infer the schema of the model. Returns the schema\n        of the model's input and output.\n        Parameters:\n            model_id: A string representing the model's id in the form of \"owner/model_name\".\n            model_version_id: A string representing the model's version. Default is \"latest\".\n        Returns:\n            input_schema: A dictionary representing the schema of the model's input.\n            output_schema: A dictionary representing the schema of the model's output.\n        \"\"\"\n        # First get the model version\n        model_version = self.get_model_version(model_id, model_version_id)\n\n        # We want to get just the schema for Pixeltable\n        # We convert the model's schema to Pixeltable's schema\n        inputs_schema = model_version[\"openapi_schema\"][\"components\"][\"schemas\"][\"Input\"][\n            \"properties\"\n        ]\n        output_schema = model_version[\"openapi_schema\"][\"components\"][\"schemas\"][\"Output\"]\n\n        # Create compatible schema with pixeltable\n        pixeltable_output_schema = create_replicate_output_schema(output_schema)\n\n        return inputs_schema, pixeltable_output_schema\n\n    def _request(\n        self,\n        path: str,\n        method: str = \"GET\",\n        data: Optional[Dict[str, Any]] = None,\n        json: Optional[Dict[str, Any]] = None,\n    ) -> requests.Response:\n        url = self.api_url + path\n        headers = {\n            \"Authorization\": f\"Token {self.api_key}\",\n            \"User-Agent\": \"pixeltable/replicate-python\",\n        }\n        response = requests.request(\n            method=method, url=url, headers=headers, data=data, json=json\n        )\n        if 200 <= response.status_code < 300:\n            return response\n        try:\n            body = response.json()\n        except Exception:\n            body = {\"detail\": response.text}\n\n        raise Error(\n            f\"Error {response.status_code}: {body.get('detail', 'No detail')}\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table\nDESCRIPTION: This snippet creates a new table to store documents with a single column of type Document.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocuments_t = pxt.create_table(\n    'rag_demo.documents',\n    {'document': pxt.Document}\n)\n\ndocuments_t\n```\n\n----------------------------------------\n\nTITLE: Reinitializing Pixeltable after Database Connection Issues\nDESCRIPTION: Demonstrates how to reinitialize Pixeltable after encountering database connection issues. This should be done after restarting the Python session or kernel.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\npxt.init()\n```\n\n----------------------------------------\n\nTITLE: Adding Rotated Image as a Computed Column\nDESCRIPTION: Creates a computed column containing rotated versions of the source images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(rotated=t.source.rotate(10))\n```\n\n----------------------------------------\n\nTITLE: Comparing YOLOX Tiny and Medium Models Side-by-Side\nDESCRIPTION: Creates two videos side-by-side to compare the results of YOLOX tiny and medium models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nframes_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        draw_boxes(\n            frames_view.frame,\n            frames_view.detect_yolox_tiny.bboxes\n        )\n    ),\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        draw_boxes(\n            frames_view.frame,\n            frames_view.detect_yolox_m.bboxes\n        )\n    )\n).show(1)\n```\n\n----------------------------------------\n\nTITLE: Testing Stored Query Redefinition\nDESCRIPTION: Demonstrates the ability to redefine stored queries in notebook scope by creating multiple versions of the same query with different filtering conditions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/notebook-test.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef query1(x: int):\n    return t.select(t.int_col == x)\n\n@pxt.query\ndef query1(x: int):\n    return t.select(t.int_col == x + 1)\n```\n\n----------------------------------------\n\nTITLE: Detecting Objects in Video Frames in Python\nDESCRIPTION: This function performs object detection on video frames using a specified model. It takes a list of video frames and a model name as inputs, and returns a list of detected objects for each frame.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/video.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef detect_objects(frames: List[np.ndarray], model: str = 'yolov5s') -> List[List[Dict[str, Any]]]:\n```\n\n----------------------------------------\n\nTITLE: Extracting Video Frames with FrameIterator in Python\nDESCRIPTION: This snippet demonstrates two ways to use the FrameIterator: extracting frames at a specific frame rate (FPS) and extracting an exact number of evenly-spaced frames from a video.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators import FrameIterator\n\n# Extract frames at 1 FPS\nframes_view = pxt.create_view(\n    'videos.frames',\n    videos_table,\n    iterator=FrameIterator.create(\n        video=videos_table.video,\n        fps=1.0\n    )\n)\n\n# Extract exact number of frames\nframes_view = pxt.create_view(\n    'videos.keyframes',\n    videos_table,\n    iterator=FrameIterator.create(\n        video=videos_table.video,\n        num_frames=10  # Extract 10 evenly-spaced frames\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Table Structure in Pixeltable\nDESCRIPTION: Shows how to manage table structure by adding new columns, dropping columns, and viewing the schema.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Add new column\nfilms.add_column(rating=pxt.String)\n\n# Drop column\nfilms.drop_column('rating')\n\n# View schema\nfilms.describe()\n```\n\n----------------------------------------\n\nTITLE: Creating Configurable Text Normalization UDF in Python\nDESCRIPTION: Shows how to create a well-documented UDF with optional parameters for text normalization. The function allows customizing the normalization process with options for lowercase conversion and punctuation removal.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef normalize_text(\n    text: str,\n    lowercase: bool = True,\n    remove_punctuation: bool = True\n) -> str:\n    \"\"\"Normalize text by optionally lowercasing and removing punctuation.\"\"\"\n    if lowercase:\n        text = text.lower()\n    if remove_punctuation:\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    return text\n```\n\n----------------------------------------\n\nTITLE: Creating Document Splitting Iterator Views in Pixeltable\nDESCRIPTION: This example demonstrates how to create an iterator view specifically for document splitting. The view uses the DocumentSplitter to transform documents from a base table into smaller chunks that can be processed independently.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Document splitting view\nchunks = pxt.create_view(\n    'docs.chunks',\n    documents,\n    iterator=DocumentSplitter.create(\n        document=documents.document\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Image Tiles with TileIterator in Python\nDESCRIPTION: This example shows how to use the TileIterator to create overlapping tiles from images. It specifies the tile size and overlap between tiles.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators import TileIterator\n\n# Create tiles with overlap\ntiles_view = pxt.create_view(\n    'images.tiles',\n    images_table,\n    iterator=TileIterator.create(\n        image=images_table.image,\n        tile_size=(224, 224),  # Width, Height\n        overlap=(32, 32)       # Horizontal, Vertical overlap\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting and Querying Data in Together AI Completions Table\nDESCRIPTION: This snippet demonstrates inserting prompts into the completions table and querying the results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nt.insert([\n    {'input': 'On November 19, 1863, '},\n    {'input': 'Beethoven was the first '}\n])\n\n# Querying the results:\nt.select(t.input, t.response).head()\n```\n\n----------------------------------------\n\nTITLE: Creating a Specialized Table for Table UDFs in Pixeltable\nDESCRIPTION: Creates a table with workflow columns that will later be converted to a UDF. This is the first step in creating a Table UDF where you define the table structure and its computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Create a table with your workflow\nfinance_agent = pxt.create_table('directory.financial_analyst', \n                                {'prompt': pxt.String})\n# Add computed columns for processing\nfinance_agent.add_computed_column(/* ... */)\n```\n\n----------------------------------------\n\nTITLE: Using DocumentSplitter with Custom Parameters in Python\nDESCRIPTION: This example shows how to use the DocumentSplitter iterator with custom parameters such as separators, token limit, and metadata fields. It creates a view with document chunks based on specified criteria.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators import DocumentSplitter\n\n# Create view with document chunks\nchunks_view = pxt.create_view(\n    'docs.chunks',\n    docs_table,\n    iterator=DocumentSplitter.create(\n        document=docs_table.document,\n        separators='paragraph,token_limit',\n        limit=500,\n        metadata='title,heading'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Update Operations in Pixeltable\nDESCRIPTION: Shows how to update existing data in a Pixeltable table, including updating all rows, conditional updates, and batch updates.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Update all rows\nfilms.update({\n    'budget': films.budget * 1.1  # Increase all budgets by 10%\n})\n\n# Conditional updates\nfilms.where(\n    films.year < 2000\n).update({\n    'plot': films.plot + ' (Classic Film)'\n})\n\n# Batch updates for multiple rows\nupdates = [\n    {'id': 1, 'budget': 175.0},\n    {'id': 2, 'budget': 185.0}\n]\nfilms.batch_update(updates)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Tools for Financial Analysis in Pixeltable\nDESCRIPTION: Initializes the environment for financial analysis with necessary imports, directory setup, and utility functions. Includes a yfinance tool for fetching stock information and a prompt generator function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom typing import Optional\n\nimport yfinance as yf\n\nimport pixeltable as pxt\nfrom pixeltable.functions.openai import chat_completions, invoke_tools\n\nDIRECTORY = 'agent'\nOPENAI_MODEL = 'gpt-4o-mini'\n\n# Create Fresh Directory\npxt.drop_dir(DIRECTORY, force=True)\npxt.create_dir(DIRECTORY, if_exists='ignore')\n\n\n# yfinance tool for getting stock information\n@pxt.udf\ndef stock_info(ticker: str) -> Optional[dict]:\n    \"\"\"Get stock info for a given ticker symbol.\"\"\"\n    stock = yf.Ticker(ticker)\n    return stock.info\n\n\n# Helper UDF to create a prompt with tool outputs\n@pxt.udf\ndef create_prompt(question: str, tool_outputs: list[dict]) -> str:\n    return f\"\"\"\n    QUESTION:\n\n    {question}\n\n    RESULTS:\n\n    {tool_outputs}\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Chunking Strategies in Pixeltable\nDESCRIPTION: Demonstrates how to configure different chunking approaches, including chunking by paragraphs and fixed size. These strategies offer flexibility in content processing based on specific requirements.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Chunk by paragraphs\nchunks_by_para = pxt.create_view(\n    \"web_search.para_chunks\",\n    websites_t,\n    iterator=DocumentSplitter.create(\n        document=websites_t.website,\n        separators=\"paragraph\"\n    )\n)\n\n# Chunk by fixed size\nchunks_by_size = pxt.create_view(\n    \"web_search.size_chunks\",\n    websites_t,\n    iterator=DocumentSplitter.create(\n        document=websites_t.website,\n        separators=\"fixed\",\n        size=1000  # characters\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column Using UDF in Pixeltable\nDESCRIPTION: This snippet demonstrates how to add a new computed column to a Pixeltable table using the previously defined top_detection UDF. The new column is named 'top' and contains the result of applying the UDF to the 'detections' column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(top=top_detection(t.detections))\n```\n\n----------------------------------------\n\nTITLE: AI Service Integrations in Pixeltable\nDESCRIPTION: Shows integration with various AI services including OpenAI, Anthropic, and Hugging Face for embeddings and model inference.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import openai, anthropic\nfrom pixeltable.functions.huggingface import (\n    sentence_transformer,\n    clip_image,\n    clip_text\n)\n\n# OpenAI integrations\ntable.add_computed_column(\n    embeddings=openai.embeddings(\n        table.text,\n        model='text-embedding-3-small'\n    )\n)\n\n# Anthropic integrations\ntable.add_computed_column(\n    analysis=anthropic.messages(\n        model='claude-3-sonnet-20240229',\n        messages=table.prompt\n    )\n)\n\n# Hugging Face integrations\ntable.add_computed_column(\n    image_embeddings=clip_image(\n        table.image, \n        model_id='openai/clip-vit-base-patch32'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Defining User-Defined Functions for Text Analysis\nDESCRIPTION: Creates UDFs using Pixeltable's decorator for sentiment analysis, keyword extraction, and readability scoring of model outputs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/pixeltable-and-gradio-application.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef get_sentiment_score(text: str) -> float:\n    \"\"\"Returns a polarity score between -1 (negative) and 1 (positive) indicating the sentiment of the input text.\"\"\"\n    return TextBlob(text).sentiment.polarity\n\n@pxt.udf\ndef extract_keywords(text: str, num_keywords: int = 5) -> list:\n    \"\"\"Extracts the most frequent meaningful words from text, excluding common stop words like 'the' and 'and'.\"\"\"\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text.lower())\n    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n    return sorted(set(keywords), key=keywords.count, reverse=True)[:num_keywords]\n\n@pxt.udf\ndef calculate_readability(text: str) -> float:\n    \"\"\"Calculates the Flesch Reading Ease score for text, where higher scores indicate easier readability.\"\"\"\n    words = len(re.findall(r'\\w+', text))\n    sentences = len(re.findall(r'\\w+[.!?]', text)) or 1\n    average_words_per_sentence = words / sentences\n    return 206.835 - 1.015 * average_words_per_sentence\n```\n\n----------------------------------------\n\nTITLE: Performing Insert Operations in Pixeltable\nDESCRIPTION: Demonstrates how to insert single and multiple rows of data into a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Insert single row\nfilms.insert(\n    title='Inside Out 2',\n    year=2024,\n    plot='Emotions navigate puberty',\n    budget=200.0\n)\n\n# Insert multiple rows\nfilms.insert([\n    {\n        'title': 'Jurassic Park', \n        'year': 1993, \n        'plot': 'Dinosaur theme park disaster',\n        'budget': 63.0\n    },\n    {\n        'title': 'Titanic', \n        'year': 1997, \n        'plot': 'Ill-fated ocean liner romance',\n        'budget': 200.0\n    }\n])\n```\n\n----------------------------------------\n\nTITLE: Creating On-the-fly Calculated Columns\nDESCRIPTION: Demonstrates how to calculate year-over-year population change using select() without creating a permanent column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npop_t.select(pop_t.country, yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).head(5)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table in Pixeltable\nDESCRIPTION: Shows how to create a new table with a specified schema in Pixeltable, defining column names and types.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfilms_t = pxt.create_table('fundamentals.films', {\n    'film_name': pxt.String,\n    'year': pxt.Int,\n    'revenue': pxt.Float\n})\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Completions Function in Python\nDESCRIPTION: Defines a function that uses OpenAI's API to generate completions for text prompts. It handles parameters for temperature, max tokens, and model selection, and requires an API key to be set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/openai.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@register_function(\n    return_type=dtypes.String(),\n    name='completions_openai',\n    argument_type_hints={\n        'prompt': dtypes.String(),\n        'model': dtypes.String(default='gpt-3.5-turbo-instruct'),\n        'max_tokens': dtypes.Int64(default=50),\n        'temperature': dtypes.Float64(default=0.7),\n        'stop': dtypes.Array(dtypes.String(), ndims=1, default=None)\n    },\n    is_aggregation=False\n)\ndef completions_openai(\n    prompt: str,\n    model: str='gpt-3.5-turbo-instruct',\n    max_tokens: int=50,\n    temperature: float=0.7,\n    stop: list=None\n) -> str:\n    \"\"\"Get completions for a given prompt using OpenAI's API.\n    \n    Args:\n        prompt: The prompt for the completion\n        model: The OpenAI model to use\n        max_tokens: The maximum number of tokens to generate\n        temperature: The temperature for the completion\n        stop: List of tokens that stop generation when encountered\n    \n    Returns:\n        The completion as a string\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        raise ImportError('You need to install the openai package to use completions_openai()')\n    \n    if openai.api_key is None:\n        raise ValueError('You need to set openai.api_key to use completions_openai()')\n\n    response = openai.completions.create(\n        model=model,\n        prompt=prompt,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stop=stop\n    )\n    return response.choices[0].text\n```\n\n----------------------------------------\n\nTITLE: Verifying Ollama Installation with Model Testing\nDESCRIPTION: This code pulls a small Qwen2.5 model from Ollama and performs a simple generation test to verify the installation is working properly. It demonstrates basic model usage.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ollama\n\nollama.pull('qwen2.5:0.5b')\nollama.generate('qwen2.5:0.5b', 'What is the capital of Missouri?')['response']\n```\n\n----------------------------------------\n\nTITLE: Chaining Computed Columns for Percentage Change\nDESCRIPTION: Creates a second computed column that depends on the first one, calculating the percentage change in population.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npop_t.add_computed_column(\n    yoy_percent_change=(100 * pop_t.yoy_change / pop_t.pop_2022)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Gradio Interface for Prompt Engineering and LLM Studio\nDESCRIPTION: This function creates a Gradio interface for a Prompt Engineering and LLM Studio application. It includes input fields for tasks, prompts, and model parameters, as well as output displays for model responses and analysis results. The interface also incorporates examples and informational sections about the application's functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/pixeltable-and-gradio-application.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef gradio_interface():\n    with gr.Blocks(theme=gr.themes.Base(), title=\"Prompt Engineering and LLM Studio\") as demo:\n        gr.HTML(\n            \"\"\"\n            <div style=\"margin-bottom: 20px;\">\n                <img src=\"https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/pixeltable-logo-large.png\" alt=\"Pixeltable\" style=\"max-width: 150px;\" />\n            </div>\n            \"\"\"\n        )\n        gr.Markdown(\n            \"\"\"\n            # Prompt Engineering and LLM Studio\n            This application demonstrates how [Pixeltable](https://github.com/pixeltable/pixeltable) can be used for rapid and incremental prompt engineering\n            and model comparison workflows. It showcases Pixeltable's ability to directly store, version, index,\n            and transform data while providing an interactive interface to experiment with different prompts and models.\n            Remember, effective prompt engineering often requires experimentation and iteration. Use this tool to systematically improve your prompts and understand how different inputs and parameters affect the LLM outputs.\n            \"\"\"\n        )\n\n        with gr.Row():\n            with gr.Column():\n                with gr.Accordion(\"What does it do?\", open=False):\n                    gr.Markdown(\n                        \"\"\"\n                        1. **Data Organization**: Pixeltable uses tables and views to organize data, similar to traditional databases but with enhanced capabilities for AI workflows.\n                        2. **Computed Columns**: These are dynamically generated columns based on expressions applied to columns.\n                        3. **Data Storage**: All prompts, responses, and analysis results are stored in Pixeltable tables.\n                        4. **Versioning**: Every operations are automatically versioned, allowing you to track changes over time.\n                        5. **UDFs**: Sentiment scores, keywords, and readability scores are computed dynamically.\n                        6. **Querying**: The history and analysis tabs leverage Pixeltable's querying capabilities to display results.\n                        \"\"\"\n                    )\n\n            with gr.Column():\n                with gr.Accordion(\"How does it work?\", open=False):\n                    gr.Markdown(\n                        \"\"\"\n                        1. **Define your task**: This helps you keep track of different experiments.\n                        2. **Set up your prompt**: Enter a system prompt in the \"System Prompt\" field. Write your specific input or question in the \"Input Text\" field\n                        3. **Adjust parameters (optional)**: Adjust temperature, top_p, token limits, etc., to control the model's output.\n                        4. **Run the analysis**: Click the \"Run Inference and Analysis\" button.\n                        5. **Review the results**: Compare the responses from both models and exmaine the scores.\n                        6. **Iterate and refine**: Based on the results, refine your prompt or adjust parameters.\n                        \"\"\"\n                    )\n\n        with gr.Row():\n            with gr.Column():\n                task = gr.Textbox(label=\"Task (Arbitrary Category)\")\n                system_prompt = gr.Textbox(label=\"System Prompt\")\n                input_text = gr.Textbox(label=\"Input Text\")\n\n                with gr.Accordion(\"Advanced Settings\", open=False):\n                    temperature = gr.Slider(minimum=0, maximum=1, value=0.7, step=0.1, label=\"Temperature\")\n                    top_p = gr.Slider(minimum=0, maximum=1, value=0.9, step=0.1, label=\"Top P\")\n                    max_tokens = gr.Number(label=\"Max Tokens\", value=300)\n                    stop = gr.Textbox(label=\"Stop Sequences (comma-separated)\")\n                    random_seed = gr.Number(label=\"Random Seed\", value=None)\n                    safe_prompt = gr.Checkbox(label=\"Safe Prompt\", value=False)\n\n                submit_btn = gr.Button(\"Run Inference and Analysis\")\n\n                with gr.Tabs():\n                    with gr.Tab(\"Prompt Input\"):\n                        history = gr.Dataframe(\n                            headers=[\"Task\", \"System Prompt\", \"Input Text\", \"Timestamp\"],\n                            wrap=True\n                        )\n\n                    with gr.Tab(\"Model Responses\"):\n                        responses = gr.Dataframe(\n                            headers=[\"Timestamp\", \"Open-Mistral-Nemo Response\", \"Mistral-Medium Response\"],\n                            wrap=True\n                        )\n\n                    with gr.Tab(\"Analysis Results\"):\n                        analysis = gr.Dataframe(\n                            headers=[\n                                \"Timestamp\",\n                                \"Open-Mistral-Nemo Sentiment\",\n                                \"Mistral-Medium Sentiment\",\n                                \"Open-Mistral-Nemo Keywords\",\n                                \"Mistral-Medium Keywords\",\n                                \"Open-Mistral-Nemo Readability\",\n                                \"Mistral-Medium Readability\"\n                            ],\n                            wrap=True\n                        )\n\n                    with gr.Tab(\"Model Parameters\"):\n                        params = gr.Dataframe(\n                            headers=[\n                                \"Timestamp\",\n                                \"Temperature\",\n                                \"Top P\",\n                                \"Max Tokens\",\n                                \"Min Tokens\",\n                                \"Stop Sequences\",\n                                \"Random Seed\",\n                                \"Safe Prompt\"\n                            ],\n                            wrap=True\n                        )\n\n            with gr.Column():\n                omn_response = gr.Textbox(label=\"Open-Mistral-Nemo Response\")\n                ml_response = gr.Textbox(label=\"Mistral-Medium Response\")\n\n                with gr.Row():\n                    large_sentiment = gr.Number(label=\"Mistral-Medium Sentiment\")\n                    open_sentiment = gr.Number(label=\"Open-Mistral-Nemo Sentiment\")\n\n                with gr.Row():\n                    large_keywords = gr.Textbox(label=\"Mistral-Medium Keywords\")\n                    open_keywords = gr.Textbox(label=\"Open-Mistral-Nemo Keywords\")\n\n                with gr.Row():\n                    large_readability = gr.Number(label=\"Mistral-Medium Readability\")\n                    open_readability = gr.Number(label=\"Open-Mistral-Nemo Readability\")\n\n        # Define the examples\n        examples = [\n            # Example 1: Sentiment Analysis\n            [\"Sentiment Analysis\",\n            \"You are an AI trained to analyze the sentiment of text. Provide a detailed analysis of the emotional tone, highlighting key phrases that indicate sentiment.\",\n            \"The new restaurant downtown exceeded all my expectations. The food was exquisite, the service impeccable, and the ambiance was perfect for a romantic evening. I can't wait to go back!\",\n            0.3, 0.95, 200, 3, None, False],\n\n            # Example 2: Creative Writing\n            [\"Story Generation\",\n            \"You are a creative writer. Generate a short, engaging story based on the given prompt. Include vivid descriptions and an unexpected twist.\",\n            \"In a world where dreams are shared, a young girl discovers she can manipulate other people's dreams.\",\n            0.9, 0.8, 500, 300, 1, None, False]\n                ]\n\n        gr.Examples(\n            examples=examples,\n            inputs=[task, system_prompt, input_text, temperature, top_p, max_tokens, stop, random_seed, safe_prompt],\n            outputs=[omn_response, ml_response, large_sentiment, open_sentiment, large_keywords, open_keywords, large_readability, open_readability],\n            fn=run_inference_and_analysis,\n            cache_examples=True,\n        )\n\n        gr.Markdown(\n            \"\"\"\n            For more information, visit [Pixeltable's GitHub repository](https://github.com/pixeltable/pixeltable).\n            \"\"\"\n        )\n\n        submit_btn.click(\n            run_inference_and_analysis,\n            inputs=[task, system_prompt, input_text, temperature, top_p, max_tokens, stop, random_seed, safe_prompt],\n            outputs=[omn_response, ml_response, large_sentiment, open_sentiment, large_keywords, open_keywords, large_readability, open_readability, history, responses, analysis, params]\n        )\n\n    return demo\n\n# Launch the Gradio interface\nif __name__ == \"__main__\":\n    gradio_interface().launch()\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data in Pixeltable\nDESCRIPTION: Demonstrates CSV file importing with schema inference, type overrides, and pandas options. Supports custom separators, encodings, NA values, and date parsing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Basic CSV import with schema inference\ntable1 = pxt.io.import_csv(\n    'myproject.data',\n    'data.csv'\n)\n\n# CSV import with schema overrides\ntable2 = pxt.io.import_csv(\n    'myproject.data_typed',\n    'data.csv',\n    schema_overrides={\n        'id': pxt.Int,\n        'name': pxt.String,\n        'score': pxt.Float,\n        'active': pxt.Bool,\n        'created_at': pxt.Timestamp\n    }\n)\n\n# CSV import with pandas options\ntable3 = pxt.io.import_csv(\n    'myproject.data_options',\n    'data.csv',\n    sep=';',                # Custom separator\n    encoding='utf-8',       # Specify encoding\n    na_values=['NA', '-'],  # Custom NA values\n    parse_dates=['date']    # Parse date columns\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple Computed Columns\nDESCRIPTION: Demonstrates how to select both the absolute and percentage change computed columns together.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npop_t.select(pop_t.country, pop_t.yoy_change, pop_t.yoy_percent_change).head(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Query-Based Filtered Views in Pixeltable\nDESCRIPTION: This example shows how to create a view based on a query operation. The view filters the base table to include only records that match specific criteria, in this case movies with a budget of at least 100.0.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Filtered view of high-budget movies\nblockbusters = pxt.create_view(\n    'movies.blockbusters',\n    movies.where(movies.budget >= 100.0)\n)\n```\n\n----------------------------------------\n\nTITLE: Generating AI-Powered Frame Descriptions in Pixeltable\nDESCRIPTION: This snippet shows how to add a computed column for automatic image description using OpenAI's vision model. It generates natural language captions for each extracted video frame.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nframes_view.add_computed_column(\n    image_description=vision(\n        prompt=\"Provide quick caption for the image.\",\n        image=frames_view.frame,\n        model=\"gpt-4o-mini\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Directory and Table for Image Data\nDESCRIPTION: Sets up a new Pixeltable directory and creates a table with an integer ID and image column. It also handles cleanup of existing data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Delete the `indices_demo` directory and its contents, if it exists\npxt.drop_dir('indices_demo', force=True)\n\n# Create the directory and table to use for the demo\npxt.create_dir('indices_demo')\nschema = {\n    'id': pxt.Int,\n    'img': pxt.Image,\n}\nimgs = pxt.create_table('indices_demo.img_tbl', schema)\n```\n\n----------------------------------------\n\nTITLE: Initializing DETR Object Detection in Pixeltable\nDESCRIPTION: Sets up object detection using the DETR ResNet-50 model by adding a computed column to detect objects in images with a confidence threshold of 0.8.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import detr_for_object_detection\n\nt.add_computed_column(detections=detr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n))\n```\n\n----------------------------------------\n\nTITLE: Pixeltable Implementation for Message Processing and AI Response\nDESCRIPTION: Python code showing the implementation of message storage, embedding generation, context retrieval, and AI response generation using Pixeltable's declarative API.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/context-aware-discord-bot/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 1. Store and index messages at sentence level\nmessages_view = pxt.create_view(\n    'discord_bot.sentences',\n    messages_table,\n    iterator=StringSplitter.create(\n        text=messages_table.content,\n        separators='sentence'\n    )\n)\n\n# 2. Generate embeddings for semantic search\nmessages_view.add_embedding_index('text', string_embed=get_embeddings)\n\n# 3. Get relevant context for questions\n@pxt.query\ndef get_context(question_text: str):\n    sim = messages_view.text.similarity(question_text)\n    return messages_view.order_by(sim, asc=False).select(\n        text=messages_view.text,\n        username=messages_view.username,\n        sim=sim\n    ).limit(5)\n\n# 4. Generate AI responses with context\nchat_table.add_computed_column(response=openai.chat_completions(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Answer based on the context provided.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": chat_table.prompt\n        }\n    ],\n    model='gpt-4o-mini'\n))\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Basic Data Types Table in Python\nDESCRIPTION: Demonstrates creating a table with basic data types (Int, Float, Bool, String, Timestamp) and inserting data through single/batch operations and importing from CSV/Excel files.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom datetime import datetime\n\n# Create table with basic data types\nbasic_table = pxt.create_table('myproject.basic_types', {\n    'id': pxt.Int,                  # Integer values\n    'score': pxt.Float,             # Floating point numbers  \n    'active': pxt.Bool,             # Boolean values\n    'name': pxt.String,             # Text data\n    'created_at': pxt.Timestamp     # DateTime values\n})\n\n# Insert single row\nbasic_table.insert({\n    'id': 1,\n    'score': 95.5,\n    'active': True,\n    'name': 'Example',\n    'created_at': datetime.now()\n})\n\n# Batch insert\nbasic_table.insert([\n    {\n        'id': 2,\n        'score': 88.3,\n        'active': True,\n        'name': 'Test A',\n        'created_at': datetime(2024, 1, 1)\n    },\n    {\n        'id': 3, \n        'score': 76.9,\n        'active': False,\n        'name': 'Test B',\n        'created_at': datetime(2024, 1, 15)\n    }\n])\n\n# Import from CSV\npxt.io.import_csv('myproject.from_csv', 'data.csv', \n    schema_overrides={\n        'id': pxt.Int,\n        'score': pxt.Float,\n        'active': pxt.Bool,\n        'created_at': pxt.Timestamp\n    }\n)\n\n# Import from Excel\npxt.io.import_excel('myproject.from_excel', 'data.xlsx',\n    schema_overrides={\n        'id': pxt.Int,\n        'score': pxt.Float,\n        'active': pxt.Bool,\n        'created_at': pxt.Timestamp\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Schema\nDESCRIPTION: Shows the current table schema with all computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nt\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Table with Schema\nDESCRIPTION: Demonstrates how to create a directory and a table with a defined schema in Pixeltable. The table includes columns for film title, year, and revenue.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Create a directory to organize tables\npxt.create_dir('example')\n\n# Create a table with a defined schema\nfilms = pxt.create_table('example.films', {\n    'title': pxt.String,\n    'year': pxt.Int,\n    'revenue': pxt.Float\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Table with Ollama Integration\nDESCRIPTION: This code creates a Pixeltable table with an input column and adds computed columns that use the Ollama chat function to generate responses. It demonstrates how to set up a table structure for LLM inference with configurable model parameters.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions.ollama import chat\n\npxt.drop_dir('ollama_demo', force=True)\npxt.create_dir('ollama_demo')\nt = pxt.create_table('ollama_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\n\nt.add_computed_column(output=chat(\n    messages=messages,\n    model='qwen2.5:0.5b',\n    # These parameters are optional and can be used to tune model behavior:\n    options={'max_tokens': 300, 'top_p': 0.9, 'temperature': 0.5},\n))\n\n# Extract the response content into a separate column\n\nt.add_computed_column(response=t.output.message.content)\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in a Pixeltable Table\nDESCRIPTION: Demonstrates how to use the select() method to choose specific columns from a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.select(films_t.film_name, films_t.year).collect()\n```\n\n----------------------------------------\n\nTITLE: Using Python Functions in Computed Columns\nDESCRIPTION: Shows how to create and use Python functions as computed columns using the @pxt.udf decorator.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/computed-columns.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef calculate_growth_rate(current: float, previous: float) -> float:\n    \"\"\"Calculate percentage growth rate between two values\"\"\"\n    if previous == 0:\n        return 0  # Handle division by zero\n    return ((current - previous) / previous) * 100\n\npop_t.add_computed_column(\n    growth_rate=calculate_growth_rate(pop_t.pop_2023, pop_t.pop_2022)\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pixeltable and YOLOX\nDESCRIPTION: Installs the necessary packages for the tutorial: Pixeltable and YOLOX object detection library.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable pixeltable-yolox\n```\n\n----------------------------------------\n\nTITLE: Image Processing with Computed Columns in Python\nDESCRIPTION: Shows how to create a table for image operations with computed columns for metadata extraction and image transformations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/computed-columns.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a table for image operations\nt = pxt.create_table('fundamentals.image_ops', {'source': pxt.Image})\n\n# Extract image metadata (dimensions, format, etc.)\nt.add_computed_column(metadata=t.source.get_metadata())\n\n# Create a rotated version of each image\nt.add_computed_column(rotated=t.source.rotate(10))\n\n# Create a version with transparency and rotation\nt.add_computed_column(rotated_transparent=t.source.convert('RGBA').rotate(10))\n```\n\n----------------------------------------\n\nTITLE: Chaining Computed Columns in Python\nDESCRIPTION: Demonstrates how to create dependent computed columns that build upon each other's calculations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/computed-columns.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# First computed column: calculate total population change over 3 years\npop_t.add_computed_column(total_change=pop_t.pop_2023 - pop_t.pop_2020)\n\n# Second computed column: calculate average yearly change using the first column\npop_t.add_computed_column(\n    avg_yearly_change=pop_t.total_change / 3\n)\n```\n\n----------------------------------------\n\nTITLE: Working with Images in Pixeltable\nDESCRIPTION: Shows how to create tables for storing images with various constraints (size, mode) and insert images from local files, URLs, and cloud storage.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create table with image columns\nimage_table = pxt.create_table('myproject.images', {\n    'original': pxt.Image,                  # Any valid image\n    'thumbnail': pxt.Image[(224, 224)],     # Fixed size constraint\n    'rgb_only': pxt.Image['RGB'],           # Mode constraint\n    'thumbnail_rgb': pxt.Image[(64, 64), 'RGB']  # Both constraints\n})\n\n# Insert local files\nimage_table.insert([\n    {'original': '/path/to/image1.jpg'},\n    {'original': '/path/to/image2.png'}\n])\n\n# Insert URLs\nimage_table.insert([\n    {'original': 'https://example.com/image1.jpg'},\n    {'original': 'https://example.com/image2.png'}\n])\n\n# Insert from cloud storage\nimage_table.insert([\n    {'original': 's3://my-bucket/image1.jpg'}\n])\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic API Client in Python\nDESCRIPTION: This function initializes and returns an Anthropic API client using the provided API key. It handles caching to avoid creating multiple clients unnecessarily.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/anthropic.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef get_anthropic_client(api_key: str) -> Client:\n    global _anthropic_client\n    if _anthropic_client is None:\n        _anthropic_client = Client(api_key=api_key)\n    return _anthropic_client\n```\n\n----------------------------------------\n\nTITLE: Video Processing and Cross-Modal Search in Pixeltable\nDESCRIPTION: Shows video frame extraction, object detection, and cross-modal search implementation with CLIP embeddings for text and image search.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Frame extraction\nframes = pxt.create_view(\n    'video_search.frames',\n    videos,\n    iterator=FrameIterator.create(\n        video=videos.video,\n        fps=1\n    )\n)\n\n# Object detection \nframes.add_computed_column(\n    detections=yolox(\n        frames.frame,\n        model_id='yolox_tiny',\n        threshold=0.25\n    )\n)\n\n# Cross-modal search\nframes.add_embedding_index(\n    'frame',\n    string_embed=clip_text,    # For text-to-image search\n    image_embed=clip_image     # For image-to-image search\n)\n\n# Text query for video frames\nresults = frames.frame.similarity(\"person walking on beach\")\n            .order_by(sim, asc=False)\n            .limit(5)\n            .collect()\n```\n\n----------------------------------------\n\nTITLE: Creating UDF for Drawing Bounding Boxes on Images\nDESCRIPTION: Defines a user-defined function (UDF) that draws bounding boxes on images using PIL.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport PIL.Image\nimport PIL.ImageDraw\n\n@pxt.udf\ndef draw_boxes(\n    img: PIL.Image.Image, boxes: list[list[float]]\n) -> PIL.Image.Image:\n    result = img.copy()  # Create a copy of `img`\n    d = PIL.ImageDraw.Draw(result)\n    for box in boxes:\n        # Draw bounding box rectangles on the copied image\n        d.rectangle(box, width=3)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI Prompt\nDESCRIPTION: Creates message objects for OpenAI API calls using system and user prompts\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"For the following sentence, extract all company names from the text.\"\n\nmsgs = [\n    { \"role\": \"system\", \"content\": prompt },\n    { \"role\": \"user\", \"content\": t.input }\n]\n\nt.add_computed_column(input_msgs=msgs)\n```\n\n----------------------------------------\n\nTITLE: Inserting and Processing Images in Python\nDESCRIPTION: Demonstrates how to insert sample images and display the original and rotated versions using Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/computed-columns.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Insert sample images from a GitHub repository\nurl_prefix = 'https://github.com/pixeltable/pixeltable/raw/main/docs/source/data/images'\nimages = ['000000000139.jpg', '000000000632.jpg', '000000000872.jpg']\nt.insert({'source': f'{url_prefix}/{image}'} for image in images)\n\n# Display the original and rotated images\nt.select(t.source, t.rotated).limit(2)\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory and Video Table\nDESCRIPTION: Creates a new Pixeltable directory and a table to store videos. The table has a single column of type Video.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Ensure a clean slate for the demo\npxt.create_dir('video_tutorial', if_exists='replace_force')\n\n# Create the `videos` table\nvideos_table = pxt.create_table(\n    'video_tutorial.videos',\n    {'video': pxt.Video}\n)\n```\n\n----------------------------------------\n\nTITLE: Working with Array Types in Pixeltable\nDESCRIPTION: Shows how to create and populate tables with various array types including fixed-size vectors, variable tensors, and flexible arrays using NumPy arrays.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create table with array columns\narray_table = pxt.create_table('myproject.arrays', {\n    'fixed_vector': pxt.Array[(768,), pxt.Float],      # Fixed-size vector\n    'variable_tensor': pxt.Array[(None, 512), pxt.Float], # Variable first dimension\n    'any_int_array': pxt.Array[pxt.Int],               # Any-shaped integer array\n    'any_float_array': pxt.Array[pxt.Float],           # Any-shaped float array\n    'completely_flexible': pxt.Array                    # Any array (shape and dtype)\n})\n\n# Insert array data\narray_table.insert({\n    'fixed_vector': np.random.randn(768),\n    'variable_tensor': np.random.randn(5, 512),\n    'any_int_array': np.array([1, 2, 3, 4]),          # 1D array\n    'any_float_array': np.random.randn(3, 3),         # 2D array\n    'completely_flexible': np.array([[1, 2], [3, 4]])  # Any numpy array\n})\n```\n\n----------------------------------------\n\nTITLE: Computing CLIP Text Embeddings in Python\nDESCRIPTION: Function for computing CLIP text embeddings using Hugging Face models. It processes text inputs and returns embeddings that can be used for multimodal similarity tasks with images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/huggingface.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef clip_text_embeddings(text: typing.Union[str, pl.Expr, list, tuple],\n                         model: typing.Optional[typing.Union[str, pl.Expr]] = None,\n                         model_kwargs: typing.Optional[dict] = None,\n                         **kwargs) -> pl.Expr:\n    \"\"\"\n    Compute CLIP text embeddings.\n    \"\"\"\n    from pixeltable.functions.huggingface.huggingface_clip import HuggingfaceClip\n    from pixeltable.functions.utils import resolve_param\n\n    return HuggingfaceClip(\n        model=resolve_param(model),\n        model_kwargs=resolve_param(model_kwargs),\n        **kwargs\n    ).text_embeddings(resolve_param(text))\n```\n\n----------------------------------------\n\nTITLE: Updating a Row with an Image from URL\nDESCRIPTION: Updates a specific row (id=1002) to add an image from a URL to the map_image column. This shows how Pixeltable can directly use file paths or URLs without requiring manual image loading.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Update the row with id == 1002, adding an image to the `map_image` column\n\neq_t.where(eq_t.id == 1002).update(\n    {'map_image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/port-townsend-map.jpeg'}\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Frame View from Video Table\nDESCRIPTION: Creates a view of the video table that contains one row for each frame of each video using Pixeltable's FrameIterator.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.iterators import FrameIterator\n\nframes_view = pxt.create_view(\n    'video_tutorial.frames',\n    videos_table,\n    # `fps` determines the frame rate; a value of `0`\n    # indicates the native frame rate of the video.\n    iterator=FrameIterator.create(video=videos_table.video, fps=0)\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Image Generation\nDESCRIPTION: Inserts a test prompt for generating an image.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt.insert(prompt='Draw a pencil sketch of a friendly dinosaur playing tennis in a cornfield.')\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenAI LLM with Pixeltable\nDESCRIPTION: Demonstrates how to create a table with a computed column that uses OpenAI's GPT-4 model for text completion. Shows the basic pattern for LLM integration with Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/frameworks.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.functions import openai\n\n# Create a table with computed column for OpenAI completion\ntable = pxt.create_table('responses', {'prompt': pxt.String})\n\ntable.add_computed_column(\n    response=openai.chat_completions(\n        messages=[{'role': 'user', 'content': table.prompt}],\n        model='gpt-4'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Importing YOLOX Function for Object Detection\nDESCRIPTION: Imports the YOLOX function from Pixeltable's built-in support for object detection models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.ext.functions.yolox import yolox\n```\n\n----------------------------------------\n\nTITLE: Inserting Multiple Rows into a Pixeltable Table\nDESCRIPTION: Demonstrates how to insert multiple rows of data into a Pixeltable table using a list of dictionaries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.insert([\n    {'film_name': 'Jurassic Park', 'year': 1993, 'revenue': 1037.5},\n    {'film_name': 'Titanic', 'year': 1997, 'revenue': 2257.8},\n    {'film_name': 'Avengers: Endgame', 'year': 2019, 'revenue': 2797.5}\n])\n```\n\n----------------------------------------\n\nTITLE: Text generation with Gemini Pro model\nDESCRIPTION: Implements a function to generate text using the Gemini Pro model. It takes a prompt and various configuration parameters, then returns the generated text response from the model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/gemini.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@registry.function(\n    name='gemini.generate_text',\n    return_type=types.String,\n    param_types={\n        'prompt': types.String,\n        'temperature': types.Float,\n        'top_p': types.Float,\n        'top_k': types.Int,\n        'max_output_tokens': types.Int\n    },\n    param_defaults={\n        'temperature': 1.0,\n        'top_p': 0.95,\n        'top_k': 64,\n        'max_output_tokens': 8192\n    }\n)\ndef generate_text(\n        prompt: str,\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        top_k: int = 64,\n        max_output_tokens: int = 8192\n) -> str:\n    \"\"\"\n    Generate text using the Google Gemini Pro model.\n\n    Args:\n        prompt: The text prompt to generate from\n        temperature: Controls the randomness of the output. Range: [0.0, 1.0]\n        top_p: The cumulative probability cutoff for token selection. Range: [0.0, 1.0]\n        top_k: The number of highest probability tokens to consider for each selection. Range: [1, 100]\n        max_output_tokens: Maximum number of tokens to generate\n\n    Returns:\n        Generated text as a string\n    \"\"\"\n    _setup_api_key()\n\n    try:\n        model = genai.GenerativeModel('gemini-pro')\n        response = model.generate_content(\n            prompt,\n            generation_config={\n                'temperature': temperature,\n                'top_p': top_p,\n                'top_k': top_k,\n                'max_output_tokens': max_output_tokens,\n            }\n        )\n        return response.text\n    except Exception as e:\n        raise ReturnValueError(f'Error generating text with Gemini: {str(e)}')\n```\n\n----------------------------------------\n\nTITLE: Chaining Image Operations in a Computed Column\nDESCRIPTION: Demonstrates how to chain multiple image operations by creating a computed column with transparent rotated images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(rotated_transparent=t.source.convert('RGBA').rotate(10))\nt.collect()\n```\n\n----------------------------------------\n\nTITLE: Applying YOLOX Object Detection to Video Frames\nDESCRIPTION: Demonstrates applying the YOLOX tiny model to the first few frames of the video using Pixeltable's select comprehension.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nframes_view.select(\n    frames_view.frame,\n    yolox(frames_view.frame, model_id='yolox_tiny')\n).show(3)\n```\n\n----------------------------------------\n\nTITLE: Setting up Image Generation Table\nDESCRIPTION: Creates a table for image generation using the FLUX Schnell model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt = pxt.create_table('replicate_demo.images', {'prompt': pxt.String})\n\ninput = {\n    'prompt': t.prompt,\n    'go_fast': True,\n    'megapixels': '1'\n}\nt.add_computed_column(output=run(input, ref='black-forest-labs/flux-schnell'))\n```\n\n----------------------------------------\n\nTITLE: Using CLIP Models for Text and Image Embedding in Pixeltable\nDESCRIPTION: Demonstrates how to use CLIP models for generating embeddings from both text and image inputs using Pixeltable's computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\n\n# For text embedding\nt.add_computed_column(\n    text_embedding=clip(\n        t.text_column,\n        model_id='openai/clip-vit-base-patch32'\n    )\n)\n\n# For image embedding\nt.add_computed_column(\n    image_embedding=clip(\n        t.image_column,\n        model_id='openai/clip-vit-base-patch32'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating YOLOX-Tiny and YOLOX-M Detections in Pixeltable\nDESCRIPTION: This snippet adds computed columns for evaluating YOLOX-Tiny and YOLOX-M detections against the YOLOX-X ground truth using the eval_detections function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.vision import eval_detections, mean_ap\n\nframes_view.add_computed_column(eval_yolox_tiny=eval_detections(\n    pred_bboxes=frames_view.detect_yolox_tiny.bboxes,\n    pred_labels=frames_view.detect_yolox_tiny.labels,\n    pred_scores=frames_view.detect_yolox_tiny.scores,\n    gt_bboxes=frames_view.detect_yolox_x.bboxes,\n    gt_labels=frames_view.detect_yolox_x.labels\n))\n\nframes_view.add_computed_column(eval_yolox_m=eval_detections(\n    pred_bboxes=frames_view.detect_yolox_m.bboxes,\n    pred_labels=frames_view.detect_yolox_m.labels,\n    pred_scores=frames_view.detect_yolox_m.scores,\n    gt_bboxes=frames_view.detect_yolox_x.bboxes,\n    gt_labels=frames_view.detect_yolox_x.labels\n))\n```\n\n----------------------------------------\n\nTITLE: Inserting Additional Images\nDESCRIPTION: Adds new images to the table using a list comprehension to generate image URLs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmore_images = ['000000000108.jpg', '000000000885.jpg']\nt.insert({'source': f'{url_prefix}/{image}'} for image in more_images)\n```\n\n----------------------------------------\n\nTITLE: Creating Transcriptions using Whisper\nDESCRIPTION: Adds a computed column that transcribes the audio using the Whisper library. Uses the whisper.transcribe function from pixeltable.functions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import whisper\n\nvideo_table.add_computed_column(\n    transcription=whisper.transcribe(\n        audio=video_table.audio,\n        model='base.en'\n    )\n)\n\nvideo_table.select(\n    video_table.video,\n    video_table.transcription.text\n).show()\n```\n\n----------------------------------------\n\nTITLE: Creating a Label Studio Project for Frames in Pixeltable\nDESCRIPTION: This snippet defines the configuration for a Label Studio project with rectangle labels for cars, persons, and trains. It then creates the project using Pixeltable's create_label_studio_project function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nframes_config = '''\n    <View>\n      <Image name=\"frame\" value=\"$frame\"/>\n      <RectangleLabels name=\"preannotations\" toName=\"frame\">\n        <Label value=\"car\" background=\"blue\"/>\n        <Label value=\"person\" background=\"red\"/>\n        <Label value=\"train\" background=\"green\"/>\n      </RectangleLabels>\n    </View>\n    '''\n\npxt.io.create_label_studio_project(frames, frames_config)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Shows how to evaluate model performance by computing mean average precision (mAP) metrics on frame evaluations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nframes_view.select(mean_ap(frames_view.eval_yolox_tiny), mean_ap(frames_view.eval_yolox_m)).show()\n```\n\n----------------------------------------\n\nTITLE: Collecting Image Generation Results\nDESCRIPTION: Displays the results of the image generation prompt.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.prompt, t.output).collect()\n```\n\n----------------------------------------\n\nTITLE: Sentence Transformer Integration for Text Embeddings\nDESCRIPTION: Shows how to generate sentence embeddings using transformer models with normalization options.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/models.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import sentence_transformer\n\nt.add_computed_column(\n    embeddings=sentence_transformer(\n        t.text,\n        model_id='sentence-transformers/all-mpnet-base-v2',\n        normalize_embeddings=True\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating User-Defined Functions\nDESCRIPTION: Demonstrates how to create custom functions using the @pxt.udf decorator. Shows implementation of a function to draw bounding boxes on images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef draw_boxes(img: PIL.Image.Image, boxes: list[list[float]]) -> PIL.Image.Image:\n    result = img.copy()  # Create a copy of `img`\n    d = PIL.ImageDraw.Draw(result)\n    for box in boxes:\n        d.rectangle(box, width=3)  # Draw bounding box rectangles on the copied image\n    return result\n```\n\n----------------------------------------\n\nTITLE: Table Modifications\nDESCRIPTION: Shows how to modify table structure and content through adding columns and updating data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\neq_t.add_column(note=pxt.String)\neq_t.add_column(contact_email=pxt.String)\n```\n\nLANGUAGE: python\nCODE:\n```\n(\n    eq_t\n    .where(eq_t.id.isin([121,123]))\n    .update({'note': 'Still investigating.', 'contact_email': 'contact@pixeltable.com'})\n)\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.update({'location': eq_t.location.replace('Washington', 'WA')})\n```\n\n----------------------------------------\n\nTITLE: Grouped Aggregation with a Custom UDA\nDESCRIPTION: Shows how to combine group_by, order_by, and a custom UDA to group integers by their tens digit and calculate the sum of squares for each group.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nt.group_by(t.val // 10).order_by(t.val // 10).select(\n    t.val // 10, sum_of_squares(t.val)\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Dropping a Column from a Pixeltable Table\nDESCRIPTION: Removes the contact_email column from the earthquakes table using the drop_column() method. This is one of the core object management operations in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Delete the `contact_email` column\n\neq_t.drop_column('contact_email')\n```\n\n----------------------------------------\n\nTITLE: Implementing YOLOX Object Detection in Pixeltable\nDESCRIPTION: Shows how to add computer vision capabilities to video frames using YOLOX object detection model. Demonstrates integration with computer vision frameworks.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/frameworks.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.yolox import yolox\n\n# Add object detection to video frames\nframes_view.add_computed_column(\n    detections=yolox(\n        frames_view.frame,\n        model_id='yolox_l'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Replicate Integration\nDESCRIPTION: Installs the necessary Python packages (pixeltable and replicate) for working with Replicate API in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable replicate\n```\n\n----------------------------------------\n\nTITLE: Testing Pixeltable Installation in Python\nDESCRIPTION: Imports Pixeltable and initializes it to verify the installation. This code should be run in a Jupyter notebook or Python environment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\npxt.init()\n```\n\n----------------------------------------\n\nTITLE: Using YOLOX Detection App\nDESCRIPTION: Demonstrates how to use the configured detection workflow to process images and videos, and retrieve detection results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Connect to your tables\nimages = pxt.get_table(\"detection.images\")\nvideos = pxt.get_table(\"detection.videos\")\nframes = pxt.get_table(\"detection.frames\")\n\n# Process images\nimages.insert([\n    {'image': 'path/to/image1.jpg'},\n    {'image': 'path/to/image2.jpg'}\n])\n\n# Process videos\nvideos.insert([\n    {'video': 'path/to/video1.mp4'}\n])\n\n# Get detection results\nimage_results = images.select(\n    images.image,\n    images.detections,\n    images.visualization\n).collect()\n\nframe_results = frames.select(\n    frames.frame,\n    frames.detections,\n    frames.visualization\n).collect()\n\n# Access specific detection information\nhigh_confidence = frames.where(\n    frames.detections.scores[0] > 0.9\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Inserting Website Content in Pixeltable\nDESCRIPTION: Demonstrates how to insert website content into the Pixeltable system for processing and searching. This snippet showcases the automatic content extraction feature.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwebsites_t.insert({\n    \"website\": \"https://example.com\"\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Asynchronous UDF for API Calls in Python\nDESCRIPTION: Shows how to implement an async UDF for making external API calls to services like OpenAI. Async UDFs are designed for handling I/O-bound operations like API requests, database queries, or web service interactions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, Literal, Union, Any\nimport json\n\n@pxt.udf\nasync def chat_completions(\n    messages: list,\n    *,\n    model: str,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    timeout: Optional[float] = None,\n) -> dict:\n\n    # Setup API request with proper context management\n    result = await openai_client.chat.completions.with_raw_response.create(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        timeout=timeout\n    )\n    \n    # Process response\n    return json.loads(result.text)\n\n# Example usage in a computed column\ntable.add_computed_column(\n    response=chat_completions(\n        [\n            {'role': 'system', 'content': 'You are a helpful assistant.'},\n            {'role': 'user', 'content': table.prompt}\n        ],\n        model='gpt-4o-mini'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Directory Hierarchies in Pixeltable\nDESCRIPTION: Creates a hierarchical structure of nested directories and a table within them. This demonstrates how Pixeltable supports organizing data in a directory-based hierarchy.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npxt.create_dir('fundamentals.subdir')\npxt.create_dir('fundamentals.subdir.subsubdir')\npxt.create_table('fundamentals.subdir.subsubdir.my_table', {'my_col': pxt.String})\n```\n\n----------------------------------------\n\nTITLE: Using a Batched UDF with Computed Columns\nDESCRIPTION: Demonstrates how to apply the batched longest_word UDF to create a computed column in a Pixeltable table, optimizing performance by processing rows in batches of 16.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(\n    longest_word_3_batched=longest_word(t.input, strip_punctuation=True)\n)\nt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory for Together AI Demo\nDESCRIPTION: This snippet creates a new Pixeltable directory for the Together AI demo, ensuring a clean slate by dropping any existing directory with the same name.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('together_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('together_demo')\n```\n\n----------------------------------------\n\nTITLE: Configuring Pixeltable Settings in TOML\nDESCRIPTION: This TOML snippet demonstrates how to configure various Pixeltable settings including file cache size, time zone, warning suppression, verbosity, and API keys for OpenAI and Label Studio.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/configuration.mdx#2025-04-07_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[pixeltable]\nfile_cache_size_g = 250\ntime_zone = \"America/Los_Angeles\"\nhide_warnings = true\nverbosity = 2\n\n[openai]\napi_key = 'my-openai-api-key'\n\n[label_studio]\nurl = 'http://localhost:8080/'\napi_key = 'my-label-studio-api-key'\n```\n\n----------------------------------------\n\nTITLE: Using UDFs in Computed Columns\nDESCRIPTION: This example demonstrates how to use a UDF to create computed columns in a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(longest_word=longest_word(t.input))\nt.show()\n\nt.insert(input='Pixeltable updates tables incrementally.')\nt.show()\n\nt.add_computed_column(\n    longest_word_2=longest_word(t.input, strip_punctuation=True)\n)\nt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing MNIST Dataset into Pixeltable\nDESCRIPTION: Loads a subset of the MNIST dataset from Hugging Face and imports it into a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n# Download the first 50 images of the MNIST dataset\nds = datasets.load_dataset('ylecun/mnist', split='train[:50]')\n\n# Import them into a Pixeltable table\nt = pxt.io.import_huggingface_dataset('demo.mnist', ds)\n```\n\n----------------------------------------\n\nTITLE: Accessing JSON Dictionary Keys with Attribute Notation in Pixeltable\nDESCRIPTION: Shows JSONPath-style attribute access for retrieving values from a JSON dictionary column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.classification.labels).head(3)\n```\n\n----------------------------------------\n\nTITLE: Syncing Pixeltable Table with Label Studio Projects\nDESCRIPTION: This snippet demonstrates how to synchronize the Pixeltable table with remote Label Studio projects after incremental updates. It creates tasks only for newly added rows in the videos and frames views.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nv.sync()\nframes.sync()\n```\n\n----------------------------------------\n\nTITLE: Adding an Image Column to a Pixeltable Table\nDESCRIPTION: Creates a new column of type Image in an existing earthquake table. This demonstrates how Pixeltable natively supports media types alongside structured data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Add a new column of type `Image`\neq_t.add_column(map_image=pxt.Image)\neq_t.describe()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs the necessary Python packages for the demo, including pixeltable, openai, whisper, sentence-transformers, and spacy.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q pixeltable openai openai-whisper sentence-transformers spacy\n```\n\n----------------------------------------\n\nTITLE: Configuring Alternative Embedding Models\nDESCRIPTION: Demonstrates how to use different sentence transformer models for embedding generation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Alternative embedding models\nembed_model = sentence_transformer.using(\n    model_id=\"sentence-transformers/all-mpnet-base-v2\"\n)\n# or\nembed_model = sentence_transformer.using(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Filtering and Sorting\nDESCRIPTION: Demonstrates complex filtering operations using where clauses, datetime comparisons, and sorting.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\neq_t.order_by(eq_t.magnitude, asc=False).limit(5).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.where((eq_t.timestamp >= datetime(2023, 6, 1)) & (eq_t.timestamp < datetime(2023, 10, 1))) \\\n  .order_by(eq_t.magnitude, asc=False).limit(5).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.where(eq_t.id.isin([123,456,789])).collect()\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.where(eq_t.location.contains('Rainier')).collect()\n```\n\n----------------------------------------\n\nTITLE: Inserting New Documents in Pixeltable\nDESCRIPTION: This snippet inserts new documents into the documents_t table in Pixeltable using a list comprehension.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndocuments_t.insert({'document': p} for p in document_urls[3:])\n```\n\n----------------------------------------\n\nTITLE: Assembling OpenAI API Messages in Python\nDESCRIPTION: This snippet creates a list of messages for the OpenAI API, including a system message and a user prompt. It prepares the input for a chat completion request.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        'role': 'system',\n        'content': 'Please read the following passages and answer the question based on their contents.'\n    },\n    {\n        'role': 'user',\n        'content': queries_t.prompt\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Table Schema\nDESCRIPTION: Creates a new table with ID and input string columns\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nt = pxt.create_table('demo.openai', {'id': pxt.Int, 'input': pxt.String})\n```\n\n----------------------------------------\n\nTITLE: Performing Query Operations in Pixeltable\nDESCRIPTION: Demonstrates various query operations including counting rows, filtering data, selecting specific columns, limiting results, and ordering results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Basic row count\nfilms.count()  # Returns total number of rows\n\n# Basic filtering\nfilms.where(films.budget >= 200.0).collect()\n\n# Select specific columns\nfilms.select(films.title, films.year).collect()\n\n# Limit results\nfilms.limit(5).collect()  # First 5 rows (no specific order)\nfilms.head(5)  # First 5 rows by insertion order\nfilms.tail(5)  # Last 5 rows by insertion order\n\n# Order results\nfilms.order_by(films.budget, asc=False).limit(5).collect()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Transcription\nDESCRIPTION: Sets up the OpenAI API key for using the OpenAI transcription service instead of the local Whisper library.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n```\n\n----------------------------------------\n\nTITLE: Managing Table Versions in Pixeltable\nDESCRIPTION: Demonstrates how to manage table versions by reverting operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Revert the last operation\nfilms.revert()  # Cannot be undone!\n\n# Revert multiple times to go back further\nfilms.revert()\nfilms.revert()  # Goes back two operations\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Pixeltable Table\nDESCRIPTION: Demonstrates how to insert multiple rows of data into a Pixeltable table using dictionaries to map column names to values. Each dictionary represents a row where keys are column names and values are the corresponding data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt.insert([{'video': '/path/to/video1.mp4'}, {'video': '/path/to/video2.mp4'}])\n```\n\n----------------------------------------\n\nTITLE: Adding a Computed Column for Population Change\nDESCRIPTION: Creates a permanent computed column that calculates the year-over-year population change between 2022 and 2023.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npop_t.add_computed_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama on Google Colab\nDESCRIPTION: This snippet shows how to install Ollama on Google Colab by downloading and running the Ollama server. The code is provided as a commented template that can be uncommented and executed.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# To install Ollama on colab, uncomment and run the following\n# three lines (this will also work on a local Linux machine\n# if you don't already have Ollama installed).\n\n# !curl -fsSL https://ollama.com/install.sh | sh\n# import subprocess\n# ollama_process = subprocess.Popen(['ollama', 'serve'], stderr=subprocess.PIPE)\n```\n\n----------------------------------------\n\nTITLE: Complex Query Operations in Pixeltable\nDESCRIPTION: Demonstrates complex query patterns combining multiple operations and nested conditions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmovies.where(\n    movies.year >= 2000\n).order_by(\n    movies.budget, \n    asc=False\n).limit(3).collect()\n\nmovies.where(\n    ~(movies.plot.contains('secret') | \n      movies.plot.contains('dream'))\n).select(\n    movies.title,\n    plot_length=movies.plot.len()\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Inserting Video Files into Pixeltable Table\nDESCRIPTION: Inserts video files into the created table. The videos are one-minute excerpts from a Lex Fridman podcast, provided as HTTPS links.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvideos = [\n    'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/audio-transcription-demo/'\n    f'Lex-Fridman-Podcast-430-Excerpt-{n}.mp4'\n    for n in range(3)\n]\n\nvideo_table.insert({'video': video} for video in videos[:2])\nvideo_table.show()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Audio Processing Pipeline\nDESCRIPTION: Installs required Python packages including Pixeltable, OpenAI tools, and NLP libraries needed for the audio processing workflow.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable tiktoken openai openai-whisper spacy sentence-transformers\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Multiple Conditions in Pixeltable Queries\nDESCRIPTION: Demonstrates combining multiple conditions using logical operators for complex filtering.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmovies.where(\n    (movies.year >= 2000) &\n    (movies.budget >= 200.0)\n).collect()\n\nmovies.where(\n    (movies.year < 2000) |\n    (movies.budget < 100.0)\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows in a Pixeltable Table\nDESCRIPTION: Shows how to use the where() method to filter rows based on a condition in a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.where(films_t.revenue >= 2000.0).collect()\n```\n\n----------------------------------------\n\nTITLE: Adding YOLOX-X Detection Column in Pixeltable\nDESCRIPTION: This snippet adds a computed column to the frames_view using the YOLOX-X model for object detection. It sets a detection threshold of 0.25.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nframes_view.add_computed_column(detect_yolox_x=yolox(\n    frames_view.frame, model_id='yolox_x', threshold=0.25\n))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Prompt Engineering Studio\nDESCRIPTION: Installs the required packages for the application, including Gradio, Pixeltable, TextBlob, NLTK, and MistralAI.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/pixeltable-and-gradio-application.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU gradio pixeltable textblob nltk mistralai\n```\n\n----------------------------------------\n\nTITLE: Table Operations and Image Insertion\nDESCRIPTION: Demonstrates basic table operations including describing schema, counting rows, and inserting image data from URLs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nt.describe()\n```\n\nLANGUAGE: python\nCODE:\n```\nt.count()\n```\n\nLANGUAGE: python\nCODE:\n```\nt.insert(input_image='https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')\n```\n\n----------------------------------------\n\nTITLE: Displaying Final Query Results in Pixeltable\nDESCRIPTION: This snippet selects and displays the final results of the queries after re-execution, showing the Question, correct_answer, and answer columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nqueries_t.select(\n    queries_t.Question,\n    queries_t.correct_answer,\n    queries_t.answer\n).show()\n```\n\n----------------------------------------\n\nTITLE: Video Frame Extraction Configuration\nDESCRIPTION: Example of configuring frame extraction settings for video processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nframes = pxt.create_view('detection.frames', videos,\n    iterator=pxt.iterators.FrameIterator.create(\n        video=videos.video, fps=1\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Object Detection with DETR Model\nDESCRIPTION: Implements object detection using Huggingface's DETR model as a computed column, demonstrating AI model integration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import huggingface\n\nt.add_computed_column(detections=huggingface.detr_for_object_detection(\n    t.input_image, model_id='facebook/detr-resnet-50'\n))\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Embedding Indexes in Pixeltable\nDESCRIPTION: Creates multiple embedding indexes on the same column using different sentence transformer models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import sentence_transformer\n\ntxts.add_embedding_index(\n    'text',\n    idx_name='minilm_idx',\n    embedding=sentence_transformer.using(model_id='sentence-transformers/all-MiniLM-L12-v2')\n)\ntxts.add_embedding_index(\n    'text',\n    idx_name='e5_idx',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)\n```\n\n----------------------------------------\n\nTITLE: Batch Processing URLs with Rate Limiting in Pixeltable\nDESCRIPTION: Implements a function for processing multiple website URLs with rate limiting. This approach ensures responsible web scraping by introducing delays between requests and handling exceptions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom typing import List\n\ndef batch_process_urls(urls: List[str], delay: float = 1.0):\n    \"\"\"Process multiple URLs with rate limiting\"\"\"\n    results = []\n    for url in urls:\n        try:\n            websites_t.insert({\"website\": url})\n            results.append({\"url\": url, \"status\": \"success\"})\n        except Exception as e:\n            results.append({\n                \"url\": url, \n                \"status\": \"failed\", \n                \"error\": str(e)\n            })\n        time.sleep(delay)\n    return results\n```\n\n----------------------------------------\n\nTITLE: Converting DETR Detections to COCO Format in Pixeltable\nDESCRIPTION: This code converts the DETR object detections to COCO format using Pixeltable's detr_to_coco function. It adds a new computed column 'preannotations' and displays the first 3 rows of the updated table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import detr_to_coco\n\nframes.add_computed_column(\n    preannotations=detr_to_coco(frames.frame, frames.detections)\n)\nframes.select(frames.frame, frames.detections, frames.preannotations).head(3)\n```\n\n----------------------------------------\n\nTITLE: OpenAI GPT-4 Vision Integration\nDESCRIPTION: Integrates OpenAI's GPT-4 vision model for image analysis, including API key setup and image processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import openai\n\nt.add_computed_column(vision=openai.vision(\n    prompt=\"Describe what's in this image.\",\n    image=t.input_image,\n    model='gpt-4o-mini'\n))\n```\n\n----------------------------------------\n\nTITLE: Importing Questions from Excel\nDESCRIPTION: This snippet imports sample questions from an Excel spreadsheet using Pixeltable's import_excel() utility.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbase = 'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/rag-demo/'\nqa_url = base + 'Q-A-Rag.xlsx'\nqueries_t = pxt.io.import_excel('rag_demo.queries', qa_url)\n```\n\n----------------------------------------\n\nTITLE: Adding Pre-annotations with Object Detection\nDESCRIPTION: Snippet demonstrating how to add automatic object detection for pre-labeling data using Pixeltable's computed columns feature.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nframes.add_computed_column(\n    detections=detr_for_object_detection(\n        frames.frame\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory and Table\nDESCRIPTION: Sets up a demo directory and creates a table with an image column. Demonstrates basic table creation and schema definition.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Create the directory `demo` (if it doesn't already exist)\npxt.drop_dir('demo', force=True)  # First drop `demo` to ensure a clean environment\npxt.create_dir('demo')\n\n# Create the table `demo.first` with a single column `input_image`\nt = pxt.create_table('demo.first', {'input_image': pxt.Image})\n```\n\n----------------------------------------\n\nTITLE: Inserting and Querying Data in Together AI Embeddings Table\nDESCRIPTION: This snippet demonstrates inserting text and generating embeddings using the Together AI embeddings table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nemb_t.insert(input='Together AI provides a variety of embeddings models.')\n\nemb_t.head()\n```\n\n----------------------------------------\n\nTITLE: Document Chunking and Computed Columns in Pixeltable\nDESCRIPTION: Illustrates document chunking view creation and computed column implementation using OpenAI embeddings and custom UDFs for prompt creation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create document chunks view\nchunks = pxt.create_view(\n    'docs.chunks',\n    docs,\n    iterator=DocumentSplitter.create(\n        document=docs.document,\n        separators='sentence'\n    )\n)\n\n# Example computed column for generating embeddings\ndocs_table.add_computed_column(\n    embeddings=openai.embeddings(\n        docs_table.text,\n        model='text-embedding-3-small'\n    )\n)\n\n# Custom UDF example\n@pxt.udf\ndef create_prompt(context: list[dict], question: str) -> str:\n    context_text = \"\\n\".join(item['text'] for item in context)\n    return f\"Context:\\n{context_text}\\n\\nQuestion: {question}\"\n    \n# Using the UDF in a computed column\ndocs_table.add_computed_column(\n    prompt=create_prompt(docs_table.context, docs_table.question)\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Pixeltable Schema\nDESCRIPTION: This snippet displays the schema of the queries_t table in Pixeltable after adding the computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nqueries_t\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs Pixeltable and related dependencies including PyTorch, transformers, and OpenAI libraries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU torch transformers openai pixeltable\n```\n\n----------------------------------------\n\nTITLE: Creating Source Document Table in Pixeltable\nDESCRIPTION: Creates a table 'docs' to hold references to source documents using Pixeltable's DocumentType.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocs = pxt.create_table(\n    'rag_ops_demo.docs',\n    {'source_doc': pxt.Document}\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable and Label Studio\nDESCRIPTION: Commands for installing Pixeltable, Label Studio, Label Studio SDK, PyTorch, and Transformers packages required for the annotation workflow.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable label-studio label-studio-sdk torch transformers\n```\n\n----------------------------------------\n\nTITLE: Integrating GPT-4 Vision for Image Analysis\nDESCRIPTION: Adds a computed column that uses OpenAI's GPT-4o-mini model for image analysis. The code handles API key setup and creates a column that will automatically generate image descriptions when new images are added to the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nfrom pixeltable.functions import openai    \n\nif 'OPENAI_API_KEY' not in os.environ:\n  os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')        \n\nt.add_computed_column(\n    vision=openai.vision(\n        prompt=\"Describe what's in this image.\",\n        image=t.input_image,\n        model='gpt-4o-mini'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Showing Query Results in Pixeltable\nDESCRIPTION: This snippet selects and displays the Question, correct_answer, and answer columns from the queries_t table in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nqueries_t.select(queries_t.Question, queries_t.correct_answer, queries_t.answer).show()\n```\n\n----------------------------------------\n\nTITLE: Performing Delete Operations in Pixeltable\nDESCRIPTION: Demonstrates how to remove data from a Pixeltable table using delete operations with conditions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Delete specific rows\nfilms.where(\n    films.year < 1995\n).delete()\n\n# Delete with complex conditions\nfilms.where(\n    (films.budget < 100.0) & \n    (films.year < 2000)\n).delete()\n\n# WARNING: Delete all rows (use with caution!)\n# films.delete()  # Without where clause deletes all rows\n```\n\n----------------------------------------\n\nTITLE: Creating Label Studio Project\nDESCRIPTION: Creates a Label Studio project and links it to a Pixeltable view with video categorization configuration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nv = pxt.create_view(\n    'ls_demo.videos_2024_04_22',\n    videos_table.where(videos_table.date == today)\n)\n\nlabel_config = '''\n    <View>\n      <Video name=\"video\" value=\"$video\"/>\n      <Choices name=\"video-category\" toName=\"video\" showInLine=\"true\">\n        <Choice value=\"city\"/>\n        <Choice value=\"food\"/>\n        <Choice value=\"sports\"/>\n      </Choices>\n    </View>\n    '''\n\npxt.io.create_label_studio_project(v, label_config)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column for OpenAI API Call in Pixeltable\nDESCRIPTION: This snippet adds a computed column to the queries_t table in Pixeltable. It calls the OpenAI API with the assembled messages and stores the entire response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nqueries_t.add_computed_column(\n    response=openai.chat_completions(model='gpt-4o-mini', messages=messages)\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Jupyter for Pixeltable Notebooks\nDESCRIPTION: Installs Jupyter Notebook, which is required for running Pixeltable in a notebook environment. This step is necessary if Jupyter is not already installed.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyter\n```\n\n----------------------------------------\n\nTITLE: Customizing Scoring System in Pixeltable Evaluations\nDESCRIPTION: Example code showing how to modify the judge prompt template to implement a custom scoring system with specific evaluation instructions and formatting requirements.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njudge_prompt_template = \"\"\"\n... [Your custom evaluation instructions] ...\nPlease evaluate on a scale of [your scale] based on:\n- [Criterion 1]\n- [Criterion 2]\nFormat: \nScore: [score]\nExplanation: [details]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI API for Transcription\nDESCRIPTION: Adds a new computed column that uses the OpenAI API for transcription instead of the local Whisper library.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import openai\n\nvideo_table.add_computed_column(\n    transcription_from_api=openai.transcriptions(\n        video_table.audio,\n        model='whisper-1'\n    )\n)\n\nvideo_table.select(\n    video_table.video,\n    video_table.transcription.text,\n    video_table.transcription_from_api.text\n).show()\n```\n\n----------------------------------------\n\nTITLE: Performing Query Operations on Pixeltable Views\nDESCRIPTION: This snippet demonstrates various query operations that can be performed on views, including filtering with where clauses, selecting specific columns, and ordering and limiting results. Views support the same query operations as regular tables.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Basic filtering on view\nchunks.where(chunks.text.contains('specific topic')).collect()\n\n# Select specific columns\nchunks.select(chunks.text, chunks.pos).collect()\n\n# Order results\nchunks.order_by(chunks.pos).limit(5).collect()\n```\n\n----------------------------------------\n\nTITLE: Configuring Frame Extraction from Videos\nDESCRIPTION: Code snippet showing how to set up automatic video frame sampling using the FrameIterator in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\niterator=FrameIterator.create(\n    video=videos.video,\n    fps=0.25\n)\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify Development Server\nDESCRIPTION: Command to start the Mintlify development server. This allows for real-time preview of documentation changes at http://localhost:3000.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Comparing Local and API Transcription Results\nDESCRIPTION: Displays the full JSON output of both the local Whisper library and the OpenAI API transcription results for comparison.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nvideo_table.select(\n    video_table.transcription,\n    video_table.transcription_from_api\n).show(1)\n```\n\n----------------------------------------\n\nTITLE: Pixelagent Architecture Diagram\nDESCRIPTION: A mermaid flowchart diagram illustrating the architecture of Pixelagent. It shows the relationships between core components including LLM providers, memory, tools, and orchestration, and how they interact with the user.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_6\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    subgraph Agent [\"Agent Architecture\"]\n        LLM[\"LLM Provider\\n(OpenAI/Anthropic)\"]\n        Memory[\"Memory\\n(Persistent Storage)\"]\n        Tools[\"Tools\\n(Python Functions)\"]\n        Orchestration[\"Orchestration\\n(Pixeltable Core)\"]\n    end\n    \n    LLM <--> Orchestration\n    Memory <--> Orchestration\n    Tools <--> Orchestration\n    \n    User[\"User\"] <--> Agent\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable LLM Judge\nDESCRIPTION: Command to install the necessary Python packages (pixeltable and openai) needed for implementing the LLM judge evaluation system.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/chat/evals.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable openai\n```\n\n----------------------------------------\n\nTITLE: Deleting a Label Studio Project in Pixeltable\nDESCRIPTION: This code shows how to remove a Label Studio project from a table or view using the unlink_external_stores method. It includes options for deleting or preserving the external data in Label Studio.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nv.external_stores  # Get a list of all external stores for `v`\n\nv.unlink_external_stores('ls_project_0', delete_external_data=True)\n```\n\n----------------------------------------\n\nTITLE: Column Transformation and Renaming in Pixeltable\nDESCRIPTION: Demonstrates column transformation by converting budget to hundreds of millions and renaming the output column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmovies.select(\n    movies.title,\n    budget_hundreds=movies.budget / 100\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Extracting Message Content from Mistral AI Response\nDESCRIPTION: Adds a new computed column that extracts the text content from the structured API response returned by Mistral AI.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Parse the response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Creating a View with DocumentSplitter Iterator in Python\nDESCRIPTION: This snippet demonstrates how to create a view using the DocumentSplitter iterator to process documents in chunks. It splits the documents into paragraphs and includes metadata fields.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import DocumentSplitter\n\n# Create a view using an iterator\nchunks = pxt.create_view(\n    'docs.chunks',\n    documents_table,\n    iterator=DocumentSplitter.create(\n        document=documents_table.document,\n        separators='paragraph'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Pixeltable Dataset with Labels to Voxel51\nDESCRIPTION: Exports the Pixeltable dataset to Voxel51 format, including the converted classification and detection labels.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfo_dataset = pxt.io.export_images_as_fo_dataset(\n    t,\n    t.image,\n    classifications=vit_to_fo(t.classifications),\n    detections=detr_to_fo(t.image, t.detections)\n)\nsession = fo.launch_app(fo_dataset)\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Integration for Label Studio\nDESCRIPTION: Advanced configuration for integrating Label Studio with Amazon S3 storage for secure cloud-based media files access.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npxt.io.create_label_studio_project(\n    videos,\n    video_config,\n    media_import_method='url',\n    s3_configuration={\n        'bucket': 'my-bucket',\n        'aws_access_key_id': key,\n        'aws_secret_access_key': secret\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Whisper Speech Recognition Function in Python for Pixeltable\nDESCRIPTION: This function uses the Whisper model to perform speech recognition on audio files. It supports various parameters for customization and returns a dictionary with transcription results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/whisper.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef whisper(\n    audio: Union[str, np.ndarray],\n    model: str = 'base',\n    language: Optional[str] = None,\n    task: Literal['transcribe', 'translate'] = 'transcribe',\n    fp16: bool = True,\n    temperature: float = 0,\n    temperature_increment_on_fallback: float = 0.2,\n    best_of: int = 5,\n    beam_size: int = 5,\n    patience: float = None,\n    length_penalty: float = None,\n    suppress_tokens: str = '-1',\n    initial_prompt: Optional[str] = None,\n    condition_on_previous_text: bool = True,\n    temperature_fallback_increase: float = 1.0,\n    max_attempts: int = 3,\n    **decode_options: Any\n) -> Dict[str, Any]:\n    \"\"\"Run the Whisper model to perform speech recognition.\n\n    Args:\n        audio (Union[str, np.ndarray]): The path to the audio file or a numpy array of audio samples.\n        model (str, optional): The Whisper model to use. Defaults to 'base'.\n        language (Optional[str], optional): The language of the audio. Defaults to None.\n        task (Literal['transcribe', 'translate'], optional): The task to perform. Defaults to 'transcribe'.\n        fp16 (bool, optional): Whether to use half-precision floating point. Defaults to True.\n        temperature (float, optional): The sampling temperature. Defaults to 0.\n        temperature_increment_on_fallback (float, optional): The temperature increment on fallback. Defaults to 0.2.\n        best_of (int, optional): The number of samples to consider for each step. Defaults to 5.\n        beam_size (int, optional): The beam size for beam search. Defaults to 5.\n        patience (float, optional): The patience value for beam search. Defaults to None.\n        length_penalty (float, optional): The length penalty for beam search. Defaults to None.\n        suppress_tokens (str, optional): The tokens to suppress. Defaults to '-1'.\n        initial_prompt (Optional[str], optional): The initial prompt for the model. Defaults to None.\n        condition_on_previous_text (bool, optional): Whether to condition on previous text. Defaults to True.\n        temperature_fallback_increase (float, optional): The temperature increase on fallback. Defaults to 1.0.\n        max_attempts (int, optional): The maximum number of attempts. Defaults to 3.\n        **decode_options (Any): Additional decoding options.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the transcription results.\n    \"\"\"\n    import whisper\n    import numpy as np\n\n    model = whisper.load_model(model)\n\n    if isinstance(audio, str):\n        audio = whisper.load_audio(audio)\n\n    # Ensure audio is a numpy array\n    if not isinstance(audio, np.ndarray):\n        raise ValueError(\"Audio must be a string path or numpy array\")\n\n    # Convert suppress_tokens to a list of integers\n    suppress_tokens = [int(token) for token in suppress_tokens.split(',') if token]\n\n    decode_options.update({\n        'fp16': fp16,\n        'language': language,\n        'task': task,\n        'temperature': temperature,\n        'temperature_increment_on_fallback': temperature_increment_on_fallback,\n        'best_of': best_of,\n        'beam_size': beam_size,\n        'patience': patience,\n        'length_penalty': length_penalty,\n        'suppress_tokens': suppress_tokens,\n        'initial_prompt': initial_prompt,\n        'condition_on_previous_text': condition_on_previous_text\n    })\n\n    temperature = temperature\n    attempt = 0\n\n    while attempt < max_attempts:\n        try:\n            result = model.transcribe(audio, **decode_options)\n            return result\n        except RuntimeError as e:\n            if \"CUDA out of memory\" in str(e) and attempt < max_attempts - 1:\n                temperature += temperature_fallback_increase\n                decode_options['temperature'] = temperature\n                attempt += 1\n            else:\n                raise\n\n    raise RuntimeError(f\"Failed to transcribe after {max_attempts} attempts\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JSON Expression Before Evaluation in Pixeltable\nDESCRIPTION: Shows what a JSON field access expression looks like before being evaluated in a query.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nt.classification.labels\n```\n\n----------------------------------------\n\nTITLE: Ad Hoc UDFs with Apply\nDESCRIPTION: Shows how to use apply() for quick one-off UDF creation with numpy array operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nt.select(t.clip.apply(np.ndarray.dumps, col_type=pxt.String)).head(2)\n```\n\n----------------------------------------\n\nTITLE: Numeric Analysis in Pixeltable\nDESCRIPTION: Shows how to perform statistical calculations and comparisons on numeric columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmovies.select(\n    avg_budget=movies.budget.avg(),\n    total_budget=movies.budget.sum(),\n    movie_count=movies.title.count()\n).collect()\n\navg_budget = movies.select(\n    movies.budget.avg()\n).collect()[0][0]\n\nmovies.where(\n    movies.budget > avg_budget\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Frame Extraction View\nDESCRIPTION: Creates a view using FrameIterator to extract frames from videos at specified intervals.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom pixeltable.iterators import FrameIterator\n\ntoday = datetime(2024, 4, 22)\nvideos_table = pxt.get_table('ls_demo.videos')\n\nframes = pxt.create_view(\n    'ls_demo.frames_2024_04_22',\n    videos_table.where(videos_table.date == today),\n    iterator=FrameIterator.create(video=videos_table.video, fps=0.25)\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing List Item by Index in Pixeltable\nDESCRIPTION: Retrieves a specific item from a list within a JSON column using index notation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.classification.labels[0]).head(3)\n```\n\n----------------------------------------\n\nTITLE: Error-Handling BERT Embedding Implementation\nDESCRIPTION: Production-ready BERT embedding function with comprehensive error handling for empty inputs and model failures.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/embedding-model.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef robust_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"BERT embedding with error handling\"\"\"\n    try:\n        if not text or len(text.strip()) == 0:\n            raise ValueError(\"Empty text input\")\n            \n        if not hasattr(robust_bert_embed, 'model'):\n            # Model initialization...\n            pass\n            \n        tensor = tf.constant([text])\n        result = robust_bert_embed.model(\n            robust_bert_embed.preprocessor(tensor)\n        )['pooled_output']\n        \n        return result.numpy()[0, :]\n        \n    except Exception as e:\n        logger.error(f\"Embedding failed: {str(e)}\")\n        raise\n```\n\n----------------------------------------\n\nTITLE: Executing Hugging Face Pipelines in Python\nDESCRIPTION: Function for executing arbitrary Hugging Face pipelines. It allows specifying the pipeline task, model, and various parameters for customizing the pipeline execution.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/huggingface.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef pipeline(data: typing.Union[str, pl.Expr, list, tuple],\n              pipeline_task: typing.Union[str, pl.Expr],\n              model: typing.Optional[typing.Union[str, pl.Expr]] = None,\n              model_kwargs: typing.Optional[dict] = None,\n              pipeline_kwargs: typing.Optional[dict] = None,\n              preprocess_params: typing.Optional[dict] = None,\n              return_all: typing.Optional[bool] = False,\n              **kwargs):\n    \"\"\"\n    Execute a huggingface pipeline.\n    \"\"\"\n    from pixeltable.functions.huggingface.huggingface_pipelines import HuggingfacePipeline\n    from pixeltable.functions.utils import resolve_param\n    \n    return HuggingfacePipeline(\n        pipeline_task=resolve_param(pipeline_task),\n        model=resolve_param(model),\n        model_kwargs=resolve_param(model_kwargs),\n        pipeline_kwargs=resolve_param(pipeline_kwargs),\n        preprocess_params=resolve_param(preprocess_params),\n        return_all=resolve_param(return_all),\n        **kwargs\n    )(resolve_param(data))\n```\n\n----------------------------------------\n\nTITLE: Querying video data to show preserved local paths\nDESCRIPTION: This code queries the video data again to demonstrate that local paths are preserved in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrows = list(v.select(v.video).collect())\nrows\n```\n\n----------------------------------------\n\nTITLE: Arithmetic and Boolean Operations\nDESCRIPTION: Demonstrates using comparison operators with columns in Pixeltable queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nt.select(\n    t.image,\n    t.label,\n    t.label == '4',\n    t.label < '5',\n).head(5)\n```\n\n----------------------------------------\n\nTITLE: Querying with Custom BERT Embedding Index\nDESCRIPTION: Performs a similarity search using the custom BERT embedding index.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsim = txts.text.similarity('cubism', idx='bert_idx')\nres = txts.order_by(sim, asc=False).limit(2).select(txts.text, sim).collect()\nres\n```\n\n----------------------------------------\n\nTITLE: Visualizing MCP Server Architecture with Mermaid\nDESCRIPTION: This diagram illustrates the architecture of Pixeltable MCP servers, showing the relationship between AI applications, Docker containers running MCP servers, and the Pixeltable multimodal database.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/mcp.mdx#2025-04-07_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph \"AI Application\"\n        Host[\"MCP Client\\n(Cursor, Claude, etc.)\"]\n    end\n    subgraph \"Docker Environment\"\n        S1[\"Audio Index Server\\nlocalhost:8080/sse\"]\n        S2[\"Video Index Server\\nlocalhost:8081/sse\"]\n        S3[\"Image Index Server\\nlocalhost:8082/sse\"]\n        S4[\"Document Index Server\\nlocalhost:8083/sse\"]\n    end\n    subgraph \"Pixeltable\"\n        DB[(\"Multimodal\\nDatabase\")]\n    end\n    Host <-->|\"MCP Protocol\"| S1\n    Host <-->|\"MCP Protocol\"| S2\n    Host <-->|\"MCP Protocol\"| S3\n    Host <-->|\"MCP Protocol\"| S4\n    S1 <--> DB\n    S2 <--> DB\n    S3 <--> DB\n    S4 <--> DB\n```\n\n----------------------------------------\n\nTITLE: Defining and using a UDF with video data in Pixeltable\nDESCRIPTION: This snippet defines a User Defined Function (UDF) that works with video data and demonstrates its usage in a query.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef f(v: pxt.Video) -> int:\n    print(f'{type(v)}: {v}')\n    return 1\n```\n\nLANGUAGE: python\nCODE:\n```\nv.select(f(v.video)).show()\n```\n\n----------------------------------------\n\nTITLE: Markdown Release Notes Documentation\nDESCRIPTION: Structured changelog entries detailing version updates, new features, improvements, and bug fixes across multiple Pixeltable releases\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/changelog/product-updates.mdx#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Highlights\n* Introduced linting for improved code quality\n* Added just-in-time initialization for spaCy, improving pxt.init() performance\n* Made catalog changes to prepare for concurrency support\n\n### Enhancements\n* Added video index to cookbook\n* Updated configurations page to match API reference\n* Added MCP to documentation\n* Improved documentation with updated vision search examples\n\n### Fixes\n* Implemented graceful failure handling for backwards incompatibility in computed column UDF calls\n* Various bugfixes and improvements\n* Updated Label Studio job to Python 3.10 in nightly CI\n```\n\n----------------------------------------\n\nTITLE: Importing and configuring Gemini API integration for Pixeltable\nDESCRIPTION: Sets up the necessary imports for Google's Gemini AI models integration with Pixeltable. It imports Google's generative AI library along with standard Python utilities and Pixeltable's function registration mechanism.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/gemini.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, List, Tuple, Dict, Any, Union, Callable\nimport os\nimport json\n\nimport google.generativeai as genai\n\nfrom pixeltable.env import env\nfrom pixeltable.functions import registry, types, ReturnValueError\n```\n\n----------------------------------------\n\nTITLE: Accessing original file paths in Pixeltable\nDESCRIPTION: This code demonstrates how to access the original file URLs and local paths for media files in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nv.select(v.video.fileurl, v.video.localpath).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Custom JSON Dictionary in Pixeltable\nDESCRIPTION: Demonstrates constructing a custom dictionary with mixed expression types and constants.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncustom_dict = {\n    # Keys must be strings; values can be any expressions\n    'ground_truth': t.label,\n    'prediction': t.pred_label,\n    'is_correct': t.label == t.pred_label,\n    # You can also use constants as values\n    'engine': 'pixeltable',\n}\n\nt.select(t.image, custom_dict).head(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Table from Hugging Face Dataset\nDESCRIPTION: Creates a new Pixeltable directory and imports the Hugging Face dataset into a Pixeltable table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-hugging-face.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('hf_demo', force=True)\npxt.create_dir('hf_demo')\nt = pxt.io.import_huggingface_dataset('hf_demo.padoru', padoru)\n```\n\n----------------------------------------\n\nTITLE: Deploying Multimodal API to AWS using CDK\nDESCRIPTION: Bootstrap and deploy the CDK stack to provision all necessary AWS resources for the Multimodal API service.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/aws/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncdk bootstrap && cdk deploy\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable and boto3 in Python\nDESCRIPTION: This snippet installs the latest versions of Pixeltable and boto3 libraries using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable boto3\n```\n\n----------------------------------------\n\nTITLE: Deploying Backend with Docker\nDESCRIPTION: Commands for building and running the backend in a Docker container. It builds a Docker image named multimodal-api and runs it with port 8000 exposed for API access.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd backend\ndocker build -t multimodal-api .\ndocker run -p 8000:8000 multimodal-api\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Fireworks API in Python\nDESCRIPTION: This function sends a POST request to the Fireworks API to generate text based on the given prompt and model. It handles the API response and returns the generated text.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/fireworks.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef fireworks_generate_text(prompt: str, model: str, api_key: str, **kwargs) -> str:\n    base_url, headers = init_fireworks_client(api_key)\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n        \"temperature\": kwargs.get(\"temperature\", 0.7),\n        \"top_p\": kwargs.get(\"top_p\", 1),\n        \"n\": 1,\n        \"stream\": False,\n    }\n    response = requests.post(f\"{base_url}/completions\", headers=headers, json=data)\n    response.raise_for_status()\n    return response.json()[\"choices\"][0][\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Collecting Time Zone Data\nDESCRIPTION: Retrieves the inserted timestamps which are automatically converted to the default time zone.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt.collect()\n```\n\n----------------------------------------\n\nTITLE: Slicing Lists in JSON Columns in Pixeltable\nDESCRIPTION: Demonstrates retrieving a slice of items from a list in a JSON column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.classification.labels[:2]).head(3)\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key in Environment Variables\nDESCRIPTION: This code sets up the Together AI API key as an environment variable, prompting the user if it's not already set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'TOGETHER_API_KEY' not in os.environ:\n    os.environ['TOGETHER_API_KEY'] = getpass.getpass('Together API Key: ')\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable for Together AI Chat Completions\nDESCRIPTION: This code creates a Pixeltable for chat completions, demonstrating how to use additional parameters from the Together API to customize model behavior.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.together import chat_completions\n\nchat_t = pxt.create_table('together_demo.chat', {'input': pxt.String})\n\n# The chat-completions API expects JSON-formatted input:\nmessages = [{'role': 'user', 'content': chat_t.input}]\n\n# This example shows how additional parameters from the Together API can be used in\n# Pixeltable to customize the model behavior.\nchat_t.add_computed_column(output=chat_completions(\n    messages=messages,\n    model='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    max_tokens=300,\n    stop=['\\n'],\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40,\n    repetition_penalty=1.1,\n    logprobs=1,\n    echo=True\n))\nchat_t.add_computed_column(response=chat_t.output.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable Package\nDESCRIPTION: Installs or upgrades the Pixeltable package using pip in quiet mode.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable\n```\n\n----------------------------------------\n\nTITLE: Bot Setup and Installation Commands\nDESCRIPTION: Bash commands for setting up the Discord bot environment, including cloning the repository, creating a virtual environment, installing dependencies, and running the bot.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/context-aware-discord-bot/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 1. Clone and setup\ngit clone https://github.com/pixeltable/pixeltable.git\ncd https://github.com/pixeltable/pixeltable/tree/main/examples/context-aware-discord-bot\n\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Set environment variables in .env\nDISCORD_TOKEN=your-discord-token\nOPENAI_API_KEY=your-openai-key\n\n# 4. Run the bot\npython bot.py\n```\n\n----------------------------------------\n\nTITLE: Generating Text Completion with Anthropic API in Python\nDESCRIPTION: This function sends a prompt to the Anthropic API for text completion. It handles API parameters, token limits, and returns the generated text along with usage statistics.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/anthropic.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef anthropic_completion(\n    prompt: str,\n    model: str,\n    api_key: str,\n    max_tokens_to_sample: int = 256,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop_sequences: Optional[List[str]] = None,\n) -> Tuple[str, Dict[str, Any]]:\n    client = get_anthropic_client(api_key)\n    completion = client.completion(\n        prompt=prompt,\n        model=model,\n        max_tokens_to_sample=max_tokens_to_sample,\n        temperature=temperature,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n    )\n    usage = {\n        \"prompt_tokens\": completion.usage.input_tokens,\n        \"completion_tokens\": completion.usage.output_tokens,\n        \"total_tokens\": completion.usage.input_tokens + completion.usage.output_tokens,\n    }\n    return completion.completion, usage\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory for Gemini Demo in Python\nDESCRIPTION: This snippet creates a new Pixeltable directory for the Gemini demo, removing any existing directory with the same name.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Remove the `gemini_demo` directory and its contents, if it exists\npxt.drop_dir('gemini_demo', force=True)\npxt.create_dir('gemini_demo')\n```\n\n----------------------------------------\n\nTITLE: Accessing JSON Dictionary Keys with Bracket Notation in Pixeltable\nDESCRIPTION: Demonstrates retrieving values from a JSON dictionary column using bracket notation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.classification['labels']).head(3)\n```\n\n----------------------------------------\n\nTITLE: Setting Gemini API Key in Python Environment\nDESCRIPTION: This code prompts the user to input their Google AI Studio API key if it's not already set in the environment variables.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'GEMINI_API_KEY' not in os.environ:\n    os.environ['GEMINI_API_KEY'] = getpass.getpass('Google AI Studio API Key:')\n```\n\n----------------------------------------\n\nTITLE: Comparing Multiple LLM Models in Pixeltable\nDESCRIPTION: Adds computed columns for a different model (Llama-3.2-1B) to compare outputs against the original Qwen model, automatically evaluating existing table rows.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(result_l3=llama_cpp.create_chat_completion(\n    messages,\n    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',\n    repo_filename='*Q5_K_M.gguf'\n))\n\nt.add_computed_column(output_l3=t.result_l3.choices[0].message.content)\n\nt.select(t.input, t.output, t.output_l3).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory\nDESCRIPTION: Initialize a new Pixeltable directory for demo purposes, ensuring a clean workspace by forcing removal of existing directory.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('fireworks_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('fireworks_demo')\n```\n\n----------------------------------------\n\nTITLE: Running Pixeltable and Gradio Application\nDESCRIPTION: This command starts the Jupyter Notebook server to run the Pixeltable and Gradio application. The application is contained in a Jupyter notebook named 'pixeltable-and-gradio-application.ipynb'.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\njupyter notebook pixeltable-and-gradio-application.ipynb\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLOX Model for Object Detection in Python\nDESCRIPTION: This function initializes the YOLOX model for object detection. It loads the model, sets up the device (CPU or CUDA), and prepares the model for inference.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/ext/functions/yolox.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef init_yolox(model_name: str = 'yolox_s'):\n    from yolox.exp import get_exp\n    from yolox.utils import postprocess\n    from yolox.utils import fuse_model\n    from yolox.data.data_augment import preproc\n    import torch\n\n    exp = get_exp(None, model_name)\n    model = exp.get_model()\n    ckpt = torch.load(f'{model_name}.pth', map_location=\"cpu\")\n    model.load_state_dict(ckpt[\"model\"])\n    model.eval()\n    if torch.cuda.is_available():\n        model.cuda()\n        torch.backends.cudnn.benchmark = True\n    model = fuse_model(model)\n    return model, exp, postprocess, preproc\n```\n\n----------------------------------------\n\nTITLE: Audio Transcription Pipeline with AudioSplitter in Python\nDESCRIPTION: This example demonstrates an audio transcription pipeline using AudioSplitter to chunk long audio files and a computed column for transcription.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/iterators.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Split long audio files\nchunks = pxt.create_view(\n    'audio.chunks',\n    audio_table,\n    iterator=AudioSplitter.create(\n        audio=audio_table.audio,\n        chunk_duration_sec=30.0\n    )\n)\n\n# Add transcription\nchunks.add_computed_column(text=whisper_transcribe(chunks.audio_chunk))\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column for YOLOX Object Detection\nDESCRIPTION: Creates a computed column that applies the YOLOX tiny model to all frames in the view, with a lower confidence threshold.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nframes_view.add_computed_column(detect_yolox_tiny=yolox(\n    frames_view.frame, model_id='yolox_tiny', threshold=0.25\n))\n```\n\n----------------------------------------\n\nTITLE: Complex Boolean Expressions\nDESCRIPTION: Example of combining multiple boolean expressions with variables for complex filtering.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nrot_label = rot_model_result.label_text[0]\n\nt.where((t.label == '5') & (rot_label != '5')).select(\n    t.image, t.label, rot_label=rot_label\n).show()\n```\n\n----------------------------------------\n\nTITLE: Defining Layout Element Operations in LaTeX Table\nDESCRIPTION: This LaTeX table defines various operations supported by layout elements in LayoutParser. It includes methods for manipulating blocks, checking relationships between blocks, and image cropping.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/layout-parser-paper.md#2025-04-07_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{table}\n\\begin{tabular}{l|l} \\hline \\hline\n**Operation Name** & **Description** \\\\ \\hline\nblock.pad(top, bottom, right, left) & Enlarge the current block according to the input \\\\ \\hline\nblock.scale(fx, fy) & Scale the current block given the ratio \\\\  & in x and y direction \\\\ \\hline\nblock.shift(dx, dy) & Move the current block with the shift \\\\  & distances in x and y direction \\\\ \\hline\nblock1.is\\_in(block2) & Whether block1 is inside of block2 \\\\ \\hline\nblock1.intersect(block2) & Return the intersection region of block1 and block2. \\\\  & Coordinate type to be determined based on the inputs. \\\\ \\hline\nblock1.union(block2) & Return the union region of block1 and block2. \\\\  & Coordinate type to be determined based on the inputs. \\\\ \\hline\nblock1.relative\\_to(block2) & Convert the absolute coordinates of block1 to \\\\  & relative coordinates to block2 \\\\ \\hline\nblock1.condition\\_on(block2) & Calculate the absolute coordinates of block1 given \\\\  & the canvas block2's absolute coordinates \\\\ \\hline\nblock.crop\\_image(image) & Obtain the image segments in the block region \\\\ \\hline \\hline\n\\end{tabular}\n\\end{table}\n```\n\n----------------------------------------\n\nTITLE: Importing Pixeltable Package\nDESCRIPTION: Standard import convention for the Pixeltable package, providing access to all core functionality through the 'pxt' namespace.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/pixeltable.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Backend Environment for FastAPI Video Frame Search Application\nDESCRIPTION: This code snippet demonstrates the steps to set up the backend environment for the video frame search application. It includes changing to the project directory, creating a virtual environment, installing dependencies, and running the application.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi\npython -m venv venv\npip install -r requirements.txt \npython app.py\n```\n\n----------------------------------------\n\nTITLE: Adding a Computed Column to a Table\nDESCRIPTION: Shows how to add a computed column to a table with an expression that rotates each frame by 30 degrees. The values are automatically computed when data is added.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nt.add_column(c_added=t.frame.rotate(30))\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Pixeltable Package Conflicts\nDESCRIPTION: Resolves package conflicts by upgrading pip, purging the cache, and reinstalling Pixeltable without using the cache. This can help resolve dependency issues.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\npip cache purge\npip install -U pixeltable --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Together AI in Pixeltable\nDESCRIPTION: This snippet shows how to insert a prompt and generate an image using the Together AI image generation table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Start generating Images\nimage_t.insert([\n  {'input': 'A friendly dinosaur playing tennis in a cornfield'}\n])\n\nimage_t\n\nimage_t.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing Timestamp Properties in Different Time Zones\nDESCRIPTION: Shows how to access datetime properties in both default and specified time zones.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt.select(\n    t.dt,\n    day_default=t.dt.day,\n    day_eastern=t.dt.astimezone('America/New_York').day\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Fireworks API Authentication\nDESCRIPTION: Configuration of Fireworks API key using environment variables with a fallback to manual input if not present.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'FIREWORKS_API_KEY' not in os.environ:\n    os.environ['FIREWORKS_API_KEY'] = getpass.getpass('Fireworks API Key:')\n```\n\n----------------------------------------\n\nTITLE: Alternative Syntax for Adding a Computed Column\nDESCRIPTION: Alternative dictionary-style syntax for adding a computed column to a table that rotates each frame by 30 degrees.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt['c_added'] = t.frame.rotate(30)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up AWS Resources for Multimodal API\nDESCRIPTION: Remove all AWS resources created by the CDK deployment to prevent continued billing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/aws/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncdk destroy\n```\n\n----------------------------------------\n\nTITLE: Adding MiniLM Embeddings to Chunks View in Pixeltable\nDESCRIPTION: Adds a computed column 'minilm_embed' to the 'chunks' view using the sentence_transformer function with the 'paraphrase-MiniLM-L6-v2' model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import sentence_transformer\n\nchunks.add_computed_column(minilm_embed=sentence_transformer(\n    chunks.text,\n    model_id='paraphrase-MiniLM-L6-v2'\n))\n```\n\n----------------------------------------\n\nTITLE: Querying Rows with Computation Errors\nDESCRIPTION: Demonstrates how to query a table for rows where a computed column encountered errors during computation. Returns the exception type and error message for those rows.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nt.where(t.c_added.errortype != None).select(t.c_added.errortype, t.c_added.errormsg).show()\n```\n\n----------------------------------------\n\nTITLE: Adding Response Parser Column\nDESCRIPTION: Adding a computed column to extract the message content from the chat completion response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Parse the bot_response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Testing a Portfolio Manager with Financial Agent UDF in Pixeltable\nDESCRIPTION: Tests the portfolio manager workflow by inserting a test query and retrieving the results. This demonstrates the end-to-end process of using a Table UDF within another table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Get the portfolio manager table\nportfolio_manager = pxt.get_table(f'{DIRECTORY}.portfolio_manager')\n\n# Insert a test query\nportfolio_manager.insert([\n    {'prompt': 'What is the price of NVDIA?'}\n])\n\n# View results\nresult = portfolio_manager.select(portfolio_manager.result).collect()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Pixeltable and Together AI Integration\nDESCRIPTION: This snippet installs the necessary Python libraries (pixeltable and together) for working with Pixeltable and Together AI.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable together\n```\n\n----------------------------------------\n\nTITLE: Defining UpdateStatus Enumeration in Python for Pixeltable\nDESCRIPTION: This code snippet defines an enumeration class called UpdateStatus using Python's Enum class. It includes four status values: PENDING, RUNNING, FAILED, and DONE. This enumeration is likely used to track the progress of update operations in the Pixeltable system.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/update-status.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass UpdateStatus(Enum):\n    PENDING = 1\n    RUNNING = 2\n    FAILED = 3\n    DONE = 4\n```\n\n----------------------------------------\n\nTITLE: Inserting Multiple Documents into Pixeltable Source Table\nDESCRIPTION: Inserts multiple Wikipedia article URLs into the 'docs' table, demonstrating incremental updates to downstream views.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nurls = [\n    'https://en.wikipedia.org/wiki/Pierre-Auguste_Renoir',\n    'https://en.wikipedia.org/wiki/Henri_Matisse',\n    'https://en.wikipedia.org/wiki/Marcel_Duchamp'\n]\ndocs.insert({'source_doc': url} for url in urls)\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Table with Gemini Integration in Python\nDESCRIPTION: This code creates a Pixeltable table with an input column and adds a computed column that uses Gemini to generate content based on the input.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import gemini\n\n# Create a table in Pixeltable and pick a model hosted on Google AI Studio with some parameters\n\nt = pxt.create_table('gemini_demo.story', {'input': pxt.String})\n\nt.add_computed_column(output=gemini.generate_content(\n    t.input,\n    model_name='gemini-1.5-flash',\n    candidate_count=3,\n    stop_sequences=['\\n'],\n    max_output_tokens=300,\n    temperature=1.0,\n    top_p=0.95,\n    top_k=40,\n))\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results in Pixeltable\nDESCRIPTION: This code selects and displays the evaluation results for YOLOX-Tiny and YOLOX-M models, showing one row of data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nframes_view.select(\n    frames_view.eval_yolox_tiny,\n    frames_view.eval_yolox_m\n).show(1)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from Anthropic Response\nDESCRIPTION: Adds a computed column that extracts the text content from the Anthropic model's response, making it easier to access and display.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Parse the response into a new column\nt.add_computed_column(response=t.output.content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Performing Object Detection with YOLOX in Python\nDESCRIPTION: This function performs object detection on an input image using the YOLOX model. It preprocesses the image, runs inference, and postprocesses the results to obtain bounding boxes and class predictions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/ext/functions/yolox.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef yolox(img, model, exp, postprocess, preproc):\n    import torch\n    import numpy as np\n\n    img, _ = preproc(img, None, exp.test_size)\n    if torch.cuda.is_available():\n        img = torch.from_numpy(img).unsqueeze(0).cuda()\n    else:\n        img = torch.from_numpy(img).unsqueeze(0)\n\n    with torch.no_grad():\n        outputs = model(img)\n    outputs = postprocess(outputs, exp.num_classes, exp.test_conf, exp.nmsthre)[0]\n    if outputs is None:\n        return np.array([]), np.array([]), np.array([])\n    else:\n        return outputs[:, 0:4], outputs[:, 4], outputs[:, 6]\n```\n\n----------------------------------------\n\nTITLE: Creating Paragraph Chunks View in Pixeltable\nDESCRIPTION: Creates a view 'chunks' that splits documents into paragraphs with a token limit of 2048.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nchunks = pxt.create_view(\n    'rag_ops_demo.chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,token_limit',\n        limit=2048,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamps to Different Time Zone\nDESCRIPTION: Demonstrates how to convert timestamps to a different time zone using astimezone method.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.dt, dt_new_york=t.dt.astimezone('America/New_York'), note=t.note).collect()\n```\n\n----------------------------------------\n\nTITLE: Querying Detection Results in Pixeltable\nDESCRIPTION: Retrieves the source images and their corresponding detection results from the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.source, t.detections).collect()\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable Table with Anthropic Integration\nDESCRIPTION: Creates a table with an input column and adds a computed column that processes input text using Anthropic's Claude model. Includes model selection and parameter configuration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import anthropic\n\n# Create a table in Pixeltable and pick a model hosted on Anthropic with some parameters\n\nt = pxt.create_table('anthropic_demo.chat', {'input': pxt.String})\n\nmsgs = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=anthropic.messages(\n    messages=msgs,\n    model='claude-3-haiku-20240307',\n    # These parameters are optional and can be used to tune model behavior:\n    max_tokens=300,\n    system='Respond to the prompt with detailed historical information.',\n    top_k=40,\n    top_p=0.9,\n    temperature=0.7\n))\n```\n\n----------------------------------------\n\nTITLE: Importing and Initializing LLaMA CPP Model in Python\nDESCRIPTION: This snippet imports necessary modules and defines a function to load a LLaMA CPP model. It handles model loading, tokenization, and generation using the llama_cpp library.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/llama_cpp.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom llama_cpp import Llama\nfrom pixeltable.functions.base import BaseFunction, register_function\nfrom pixeltable.type_system import ColumnType\n\n@register_function()\nclass LlamaCppModel(BaseFunction):\n    def __init__(self, model_path: str):\n        super().__init__()\n        self.model_path = model_path\n        self.model = None\n\n    def load(self):\n        self.model = Llama(model_path=self.model_path, n_ctx=2048, n_batch=512)\n\n    def generate(self, prompt: str, max_tokens: int = 256, temperature: float = 0.8, top_p: float = 0.95) -> str:\n        if self.model is None:\n            self.load()\n        output = self.model(prompt, max_tokens=max_tokens, temperature=temperature, top_p=top_p)\n        return output['choices'][0]['text']\n\n    def tokenize(self, text: str) -> list:\n        if self.model is None:\n            self.load()\n        return self.model.tokenize(text.encode('utf-8'))\n\n    def __call__(self, prompt: str, max_tokens: int = 256, temperature: float = 0.8, top_p: float = 0.95) -> str:\n        return self.generate(prompt, max_tokens, temperature, top_p)\n\n    @classmethod\n    def get_return_type(cls) -> ColumnType:\n        return ColumnType.STRING\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Pixeltable Source Document Table\nDESCRIPTION: Inserts a single row into the 'docs' table with a URL representing a Wikipedia article.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocs.insert([{'source_doc': 'https://en.wikipedia.org/wiki/Marc_Chagall'}])\n```\n\n----------------------------------------\n\nTITLE: Inserting Timestamps with Different Time Zones\nDESCRIPTION: Demonstrates inserting datetime objects with different time zone configurations: naive, explicit local timezone, and different timezone.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, timezone\nfrom zoneinfo import ZoneInfo\n\nnaive_dt    = datetime(2024, 8, 9, 23, 0, 0)\nexplicit_dt = datetime(2024, 8, 9, 23, 0, 0, tzinfo=ZoneInfo('America/Los_Angeles'))\nother_dt    = datetime(2024, 8, 9, 23, 0, 0, tzinfo=ZoneInfo('America/New_York'))\n\nt.insert([\n    {'dt': naive_dt,    'note': 'No time zone specified (uses default)'},\n    {'dt': explicit_dt, 'note': 'Time zone America/Los_Angeles was specified explicitly'},\n    {'dt': other_dt,    'note': 'Time zone America/New_York was specified explicitly'}\n])\n```\n\n----------------------------------------\n\nTITLE: Using Pixeltable Workflow for Website Search\nDESCRIPTION: Demonstrates how to use the defined workflow, including connecting to tables, adding websites with rate limiting, and performing content searches. It showcases the practical application of the search system.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nimport time\n\n# Connect to your tables\nwebsites_t = pxt.get_table(\"web_search.websites\")\nwebsites_chunks = pxt.get_table(\"web_search.website_chunks\")\n\n# Add websites with rate limiting\nurls = [\n    \"https://quotes.toscrape.com/\",\n    \"https://example.com\",\n    \"https://docs.pixeltable.io\"\n]\n\nfor url in urls:\n    try:\n        websites_t.insert({\"website\": url})\n        time.sleep(1)  # Respect rate limits\n    except Exception as e:\n        print(f\"Failed to process {url}: {e}\")\n\n# Search content\n@pxt.query\ndef find_content(query: str, top_k: int):\n    sim = websites_chunks.text.similarity(query)\n    return (\n        websites_chunks.order_by(sim, asc=False)\n        .select(\n            websites_chunks.text,\n            websites_chunks.website,\n            similarity=sim\n        )\n        .limit(top_k)\n    )\n\n# Example search\nresults = find_content(\n    \"Find inspirational quotes about life\"\n).collect()\n\n# Print results\nfor r in results:\n    print(f\"Similarity: {r['similarity']:.3f}\")\n    print(f\"Source: {r['website']}\")\n    print(f\"Content: {r['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining YOLOX Detection Workflow\nDESCRIPTION: Sets up the detection workflow by creating tables for images and videos, configuring frame extraction, and implementing detection and visualization functions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.ext.functions.yolox import yolox\nimport PIL.Image\nimport PIL.ImageDraw\n\n# Initialize app structure\npxt.drop_dir(\"detection\", force=True)\npxt.create_dir(\"detection\")\n\n# Create tables for different media types\nimages = pxt.create_table(\n    'detection.images', \n    {'image': pxt.ImageType()},\n    if_exists=\"ignore\"\n)\n\nvideos = pxt.create_table(\n    'detection.videos',\n    {'video': pxt.VideoType()},\n    if_exists=\"ignore\"\n)\n\n# Create frame extraction view\nframes = pxt.create_view(\n    'detection.frames',\n    videos,\n    iterator=pxt.iterators.FrameIterator.create(\n        video=videos.video,\n        fps=1  # Extract 1 frame per second\n    )\n)\n\n# Add detection workflow to images\nimages.add_computed_column(\n    detections=yolox(\n        images.image,\n        model_id='yolox_s',  # Choose model size\n        threshold=0.5        # Detection confidence threshold\n    )\n)\n\n# Add detection workflow to video frames\nframes.add_computed_column(\n    detections=yolox(\n        frames.frame,\n        model_id='yolox_m',\n        threshold=0.25\n    )\n)\n\n# Add visualization function\n@pxt.udf\ndef draw_boxes(img: PIL.Image.Image, boxes: list[list[float]]) -> PIL.Image.Image:\n    result = img.copy()\n    d = PIL.ImageDraw.Draw(result)\n    for box in boxes:\n        d.rectangle(box, width=3)\n    return result\n\n# Add visualization column to both tables\nimages.add_computed_column(\n    visualization=draw_boxes(images.image, images.detections.boxes)\n)\n\nframes.add_computed_column(\n    visualization=draw_boxes(frames.frame, frames.detections.boxes)\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Gemini API key authentication\nDESCRIPTION: Configures the Gemini API key for authentication. It retrieves the key from environment variables or a configuration file, raising an error if the key is not available.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/gemini.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _setup_api_key() -> None:\n    \"\"\"\n    Set up the api_key from environment var or config file.\n    \"\"\"\n    api_key = env.get_gemini_api_key()\n    if api_key is None:\n        raise RuntimeError(\n            'Error: Google Gemini API key not found. Please set the GEMINI_API_KEY environment variable or '\n            'configure it in your Pixeltable config file.'\n        )\n\n    genai.configure(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column for YOLOX Medium Model\nDESCRIPTION: Creates an additional computed column using the larger YOLOX medium model for comparison.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nframes_view.add_computed_column(detect_yolox_m=yolox(\n    frames_view.frame, model_id='yolox_m', threshold=0.25\n))\n```\n\n----------------------------------------\n\nTITLE: Rerunning Queries with Updated Document Base in Pixeltable\nDESCRIPTION: This snippet deletes existing queries, retrieves question data, and reinserts them as new rows to force re-execution with the updated document base.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nquestions = list(\n    queries_t.select(\n        queries_t.S__No_,\n        queries_t.Question,\n        queries_t.correct_answer\n    ).collect()\n)\nqueries_t.delete()\nqueries_t.insert(questions)\n```\n\n----------------------------------------\n\nTITLE: Text Analysis in Pixeltable\nDESCRIPTION: Demonstrates text analysis operations including length calculation and pattern matching.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmovies.select(\n    movies.title,\n    plot_length=movies.plot.len()\n).collect()\n\nmovies.where(\n    movies.title.like('%: %')\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable via pip\nDESCRIPTION: Command to install Pixeltable using pip package manager. Requires Python 3.9 or higher.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/support/faq.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable\n```\n\n----------------------------------------\n\nTITLE: Initializing Fireworks API Client in Python\nDESCRIPTION: This function initializes a client for the Fireworks AI API using the provided API key. It sets up the base URL and headers for API requests.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/fireworks.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef init_fireworks_client(api_key: str):\n    return (\n        \"https://api.fireworks.ai/inference/v1\",\n        {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n    )\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Average Precision (mAP) in Pixeltable\nDESCRIPTION: This snippet calculates and displays the mean average precision (mAP) for YOLOX-Tiny and YOLOX-M models using the mean_ap function.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nframes_view.select(\n    mean_ap(frames_view.eval_yolox_tiny),\n    mean_ap(frames_view.eval_yolox_m)\n).show()\n```\n\n----------------------------------------\n\nTITLE: Performing String Operations in Pixeltable\nDESCRIPTION: Shows how to manipulate text data using string operations like contains, replace, upper, and len.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/tables-and-operations.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# String contains\nfilms.where(films.title.contains('Inception')).collect()\n\n# String replacement\nfilms.update({\n    'plot': films.plot.replace('corporate secrets', 'subconscious secrets')\n})\n\n# String functions\nfilms.update({\n    'title': films.title.upper(),        # Convert to uppercase\n    'length': films.title.len()          # Get string length\n})\n```\n\n----------------------------------------\n\nTITLE: Handling Missing JSON Keys in Pixeltable\nDESCRIPTION: Shows how Pixeltable handles attempts to access non-existent keys in JSON data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.classification.not_a_key).head(3)\n```\n\n----------------------------------------\n\nTITLE: Using Dictionary Syntax for Column References in Pixeltable\nDESCRIPTION: Shows an alternative syntax for referring to columns using dictionary-style notation in Pixeltable queries.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.select(films_t['film_name'], films_t['year']).collect()\n```\n\n----------------------------------------\n\nTITLE: Using OCR Engine in LayoutParser with Python\nDESCRIPTION: This code snippet shows how to use the unified OCR interface in LayoutParser. It initializes a Tesseract OCR agent and detects text in an image. The interface allows easy switching between different OCR engines.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/layout-parser-paper.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nocr_agent = lp.TesseractAgent()\n# Can be easily switched to other OCR software\ntokens = ocr_agent.detect(image)\n```\n\n----------------------------------------\n\nTITLE: Displaying Updated Frames View in Pixeltable\nDESCRIPTION: This code displays the updated frames_view, which now includes three detect columns for different YOLOX models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nframes_view\n```\n\n----------------------------------------\n\nTITLE: Viewing the Final Image Table Schema\nDESCRIPTION: Displays the complete schema of the image table showing all added computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nt\n```\n\n----------------------------------------\n\nTITLE: Logical Operators in Pixeltable\nDESCRIPTION: Shows proper syntax for logical AND, OR, and NOT operations in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n(t.label == '5') & (rot_label != '5')\n```\n\nLANGUAGE: python\nCODE:\n```\n(t.label == '5') | (rot_label != '5')\n```\n\nLANGUAGE: python\nCODE:\n```\n~(t.label == '5')\n```\n\nLANGUAGE: python\nCODE:\n```\nt.label == None\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Columns for llama.cpp Chat Completion\nDESCRIPTION: Configures computed columns to process input text using llama.cpp integration with the Qwen2.5-0.5B model. Sets up system and user messages, and extracts the response content.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Add a computed column that uses llama.cpp for chat completion\n# against the input.\n\nmessages = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': t.input}\n]\n\nt.add_computed_column(result=llama_cpp.create_chat_completion(\n    messages,\n    repo_id='Qwen/Qwen2.5-0.5B-Instruct-GGUF',\n    repo_filename='*q5_k_m.gguf'\n))\n\n# Extract the output content from the JSON structure returned\n# by llama_cpp.\n\nt.add_computed_column(output=t.result.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable Package\nDESCRIPTION: Installs or upgrades the Pixeltable package using pip in quiet mode.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/notebook-test.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable\n```\n\n----------------------------------------\n\nTITLE: Creating Embedding Index for Sentences\nDESCRIPTION: Adds an embedding index to the sentences view using the Huggingface sentence_transformers library. This enables semantic search on the transcriptions.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import sentence_transformer\n\nsentences_view.add_embedding_index(\n    'text',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)\n\nsim = sentences_view.text.similarity('What is happiness?')\n\n(\n    sentences_view\n    .order_by(sim, asc=False)\n    .limit(10)\n    .select(sentences_view.text,similarity=sim)\n    .collect()\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing Updated Table Data\nDESCRIPTION: Displays the last 5 rows of the table, showing the newly inserted data with computed values.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npop_t.tail(5)\n```\n\n----------------------------------------\n\nTITLE: Column Properties and Error Handling\nDESCRIPTION: Examples of accessing column properties and handling errors in computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.image, t.image.localpath).head(5)\n```\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(channel=t.image.getchannel(1))\n```\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(channel=t.image.getchannel(1), on_error='ignore')\n```\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.image, t.channel, t.channel.errortype, t.channel.errormsg).head(5)\n```\n\n----------------------------------------\n\nTITLE: Parsing ETF Performance Data in CSV Format\nDESCRIPTION: CSV data containing ETF details including ticker symbols, fund names, types, leverage status, performance metrics, expense ratios, assets under management, and management style. Each row represents a different ETF with various numeric and categorical attributes.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/etf_screener.txt#2025-04-07_snippet_3\n\nLANGUAGE: csv\nCODE:\n```\nFELG,Yes,Fidelity Enhanced Large Cap Growth ETF,ETF,\"1x, Not Leveraged, Not Inverse\",-2.319,20.16546,14.87041,20.05111,15.0785,5,0.18,0.45%,20.0045,14.68%,Open Ended Investment Company,31.5,U.S. Equity,$3.3B,Actively Managed,04/19/2007,Fidelity Management & Research Company LLC,94,-16.41009,0.19084,14.65727,346,-2.63239,-8.9751,-12.86031,6.39594,-11.08597,0.7067\nQCLR,No,Global X NASDAQ 100Â® Collar 95-110 ETF,ETF,\"1x, Not Leveraged, Not Inverse\",-2.31969,19.42254,12.94368,,,5,0.25,8.31%,37.66,-21.96%,Open Ended Investment Company,27.4568,Nontraditional Equity,$2M,Passively Managed,08/25/2021,Global X Management Company LLC,104,,,,346,-1.92107,-4.87669,-14.80578,-2.96078,-6.30467,0.60607\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column for Audio Metadata\nDESCRIPTION: Adds another computed column to extract metadata from the audio streams using the get_metadata function from pixeltable.functions.audio.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.audio import get_metadata\n\nvideo_table.add_computed_column(\n    metadata=get_metadata(video_table.audio)\n)\nvideo_table.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Data with head()\nDESCRIPTION: Shows the first 5 rows of the population table using the head() method.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npop_t.head(5)\n```\n\n----------------------------------------\n\nTITLE: Displaying Selected Columns in Pixeltable\nDESCRIPTION: This code selects the 'detections_text' and 'top' columns from the Pixeltable and displays them. It showcases how to query and visualize the results of the computed column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.detections_text, t.top).show()\n```\n\n----------------------------------------\n\nTITLE: Generating Request IDs in TypeScript\nDESCRIPTION: Client-side function to generate unique request IDs by combining timestamp and random string for tracking API calls\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/README.md#2025-04-07_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nfunction generateRequestId(): string {\n  const timestamp = Date.now();\n  const random = Math.random().toString(36).substring(2, 10);\n  return `${timestamp}_${random}`;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable for Together AI Image Generation\nDESCRIPTION: This code sets up a Pixeltable for generating images using Together AI's image generation model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.together import image_generations\n\nimage_t = pxt.create_table('together_demo.images', {'input': pxt.String, 'negative_prompt': pxt.String})\nimage_t.add_computed_column(img=image_generations(\n    image_t.input,\n    model='stabilityai/stable-diffusion-xl-base-1.0'\n))\n```\n\n----------------------------------------\n\nTITLE: Listing All Tables in Pixeltable\nDESCRIPTION: Demonstrates how to list all existing tables in the Pixeltable database.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npxt.list_tables()\n```\n\n----------------------------------------\n\nTITLE: Creating Time Zone Demo Table\nDESCRIPTION: Sets up a demo table with timestamp and string columns for time zone examples. Includes cleanup of existing directory.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/time-zones.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('tz_demo', force=True)\npxt.create_dir('tz_demo')\nt = pxt.create_table('tz_demo.example', {'dt': pxt.Timestamp, 'note': pxt.String})\n```\n\n----------------------------------------\n\nTITLE: Batch Updates and Deletions\nDESCRIPTION: Demonstrates batch update operations and row deletion with version control support.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nupdates = [\n    {'id': 500, 'note': 'This is an example note.'},\n    {'id': 501, 'note': 'This is a different note.'},\n    {'id': 502, 'note': 'A third note, unrelated to the others.'}\n]\neq_t.batch_update(updates)\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.where(eq_t.timestamp >= datetime(2024, 1, 1)).delete()\n```\n\nLANGUAGE: python\nCODE:\n```\neq_t.revert()\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable for Together AI Embeddings\nDESCRIPTION: This code creates a Pixeltable for generating embeddings using Together AI's embedding model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.together import embeddings\n\nemb_t = pxt.create_table('together_demo.embeddings', {'input': pxt.String})\nemb_t.add_computed_column(embedding=embeddings(\n    input=emb_t.input,\n    model='togethercomputer/m2-bert-80M-8k-retrieval'\n))\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data\nDESCRIPTION: Prepares sample text data from Wikipedia about financial crisis and splits it into separate inputs\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwikipedia_text = '''On Sunday, September 14, it was announced that Lehman Brothers would file for bankruptcy after the Federal Reserve Bank declined to participate in creating a financial support facility for Lehman Brothers.\nThe significance of the Lehman Brothers bankruptcy is disputed with some assigning it a pivotal role in the unfolding of subsequent events.\nThe principals involved, Ben Bernanke and Henry Paulson, dispute this view, citing a volume of toxic assets at Lehman which made a rescue impossible.[16][17] Immediately following the bankruptcy, JPMorgan Chase provided the broker dealer unit of Lehman Brothers with $138 billion to \"settle securities transactions with customers of Lehman and its clearance parties\" according to a statement made in a New York City Bankruptcy court filing.[18]\nThe same day, the sale of Merrill Lynch to Bank of America was announced.[19] The beginning of the week was marked by extreme instability in global stock markets, with dramatic drops in market values on Monday, September 15, and Wednesday, September 17.\nOn September 16, the large insurer American International Group (AIG), a significant participant in the credit default swaps markets, suffered a liquidity crisis following the downgrade of its credit rating.\nThe Federal Reserve, at AIG's request, and after AIG had shown that it could not find lenders willing to save it from insolvency, created a credit facility for up to US$85 billion in exchange for a 79.9% equity interest, and the right to suspend dividends to previously issued common and preferred stock.[20]'''\n\nsample_inputs = wikipedia_text.split('\\n')\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Incremental Updates in Pixeltable\nDESCRIPTION: Shows how adding a new video to the table automatically updates all downstream computed columns and the index.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nvideo_table.insert(video=videos[2])\n\nvideo_table.select(\n    video_table.video,\n    video_table.metadata,\n    video_table.transcription.text\n).show()\n\nsim = sentences_view.text.similarity('What is happiness?')\n\n(\n    sentences_view\n    .order_by(sim, asc=False)\n    .limit(20)\n    .select(sentences_view.text, similarity=sim)\n    .collect()\n)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Chat Completions Function in Python\nDESCRIPTION: Implements a function that generates chat completions using OpenAI's API. It processes a list of messages and returns the AI's response, with options for temperature control and token limits.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/openai.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@register_function(\n    return_type=dtypes.Json(),\n    name='chat_completions_openai',\n    argument_type_hints={\n        'messages': dtypes.Array(dtypes.Json(), ndims=1),\n        'model': dtypes.String(default='gpt-3.5-turbo'),\n        'max_tokens': dtypes.Int64(default=None),\n        'temperature': dtypes.Float64(default=0.7),\n        'return_messages': dtypes.Boolean(default=False)\n    },\n    is_aggregation=False\n)\ndef chat_completions_openai(\n    messages: list,\n    model: str='gpt-3.5-turbo',\n    max_tokens: int=None,\n    temperature: float=0.7,\n    return_messages: bool=False\n) -> dict:\n    \"\"\"Get chat completions for a given list of messages using OpenAI's API.\n    \n    Args:\n        messages: List of message objects, each with 'role' and 'content' keys\n        model: The OpenAI model to use\n        max_tokens: The maximum number of tokens to generate\n        temperature: The temperature for the completion\n        return_messages: Whether to return all messages or just the response\n    \n    Returns:\n        The completion as a JSON object (either the full response or just the content)\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        raise ImportError('You need to install the openai package to use chat_completions_openai()')\n    \n    if openai.api_key is None:\n        raise ValueError('You need to set openai.api_key to use chat_completions_openai()')\n\n    response = openai.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens=max_tokens,\n        temperature=temperature\n    )\n    \n    # Get the assistant's response\n    response_message = {'role': response.choices[0].message.role, 'content': response.choices[0].message.content}\n    \n    if return_messages:\n        # Return all messages including the new response\n        return messages + [response_message]\n    else:\n        # Return just the response message\n        return response_message\n```\n\n----------------------------------------\n\nTITLE: Inserting and Querying Data in Together AI Chat Completions Table\nDESCRIPTION: This snippet shows how to insert chat prompts and query responses from the chat completions table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-together.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nchat_t.insert([\n    {'input': 'How many species of felids have been classified?'},\n    {'input': 'Can you make me a coffee?'}\n])\nchat_t.select(chat_t.input, chat_t.response).head()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Chat Completion Results\nDESCRIPTION: Selects and displays the input questions and corresponding LLM responses from the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.input, t.output).collect()\n```\n\n----------------------------------------\n\nTITLE: Batch Image Insertion and Processing\nDESCRIPTION: Shows how to insert multiple images in batch and process them efficiently with computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmore_images = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000030.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000034.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000042.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000061.jpg'\n]\nt.insert({'input_image': image} for image in more_images)\n```\n\n----------------------------------------\n\nTITLE: Initializing AudioSplitter Class in Python\nDESCRIPTION: This code snippet defines the AudioSplitter class constructor. It initializes the splitter with parameters for segment duration, overlap, and minimum segment length. The class uses pydub for audio processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/iterators/audio-splitter.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, segment_duration_ms: int = 10000, overlap_ms: int = 0, min_segment_ms: int = 1000):\n    self.segment_duration_ms = segment_duration_ms\n    self.overlap_ms = overlap_ms\n    self.min_segment_ms = min_segment_ms\n```\n\n----------------------------------------\n\nTITLE: Processing Chat Response\nDESCRIPTION: Adds a computed column to join and format the chat model's response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(response=pxt.functions.string.join('', t.output))\n```\n\n----------------------------------------\n\nTITLE: Creating and inserting a local copy of a video file\nDESCRIPTION: This snippet creates a local copy of the first video file and inserts it into the Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlocal_path = tempfile.mktemp(suffix='.mp4')\nshutil.copyfile(rows[0]['video'], local_path)\nlocal_path\n```\n\nLANGUAGE: python\nCODE:\n```\nv.insert(video=local_path)\n```\n\n----------------------------------------\n\nTITLE: Describing a Table's Schema in Pixeltable\nDESCRIPTION: Shows how to view the schema of a Pixeltable table, including column names, types, and computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.describe()\n```\n\n----------------------------------------\n\nTITLE: Exporting Pixeltable Dataset to Voxel51\nDESCRIPTION: Exports the Pixeltable image dataset to Voxel51 format and launches the Voxel51 app for interactive exploration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfo_dataset = pxt.io.export_images_as_fo_dataset(t, t.image)\nsession = fo.launch_app(fo_dataset)\n```\n\n----------------------------------------\n\nTITLE: Setting up Replicate API Token\nDESCRIPTION: Sets up the Replicate API token as an environment variable, prompting for input if not already set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'REPLICATE_API_TOKEN' not in os.environ:\n    os.environ['REPLICATE_API_TOKEN'] = getpass.getpass('Replicate API Token:')\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Updates in Label Studio\nDESCRIPTION: Snippet that demonstrates how to insert new data and synchronize only the changes with Label Studio using Pixeltable's sync functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvideos.insert(new_video)\nvideos.sync()  # Syncs only new data\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama Python Package\nDESCRIPTION: This command installs the Ollama Python package using pip, which is required for interacting with the Ollama server from Python.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-ollama.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU ollama\n```\n\n----------------------------------------\n\nTITLE: Parsing Gemini Response and Displaying Results in Python\nDESCRIPTION: This code adds a new computed column to parse the Gemini response and then displays the input and response columns of the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Parse the response into a new column\nt.add_computed_column(response=t.output['candidates'][0]['content']['parts'][0]['text'])\nt.select(t.input, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Testing Different System Prompts and Personas\nDESCRIPTION: Creates an alternative prompt with a teacher persona to demonstrate how system prompts affect model responses, using the Llama-3.2-1B model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages_teacher = [\n    {'role': 'system',\n     'content': 'You are a patient school teacher. '\n                'Explain concepts simply and clearly.'},\n    {'role': 'user', 'content': t.input}\n]\n\nt.add_computed_column(result_teacher=llama_cpp.create_chat_completion(\n    messages_teacher,\n    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',\n    repo_filename='*Q5_K_M.gguf'\n))\n\nt.add_computed_column(output_teacher=t.result_teacher.choices[0].message.content)\n\nt.select(t.input, t.output_teacher).collect()\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Parameters for Embedding Indexes in Pixeltable\nDESCRIPTION: This snippet shows how to configure optional parameters when creating an embedding index, including specifying a custom name and providing both text and image embedding functions for different data types.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Optional parameters\ndocs.add_embedding_index(\n    column='content',\n    idx_name='custom_name',  # Optional name\n    string_embed=embed_model,\n    image_embed=img_model,   # For image columns\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation UDF\nDESCRIPTION: Defines a user-defined function for creating evaluation prompts to compare model output with ground truth\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-openai.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = '''\nCompare the following listA and listB of entities, and check if they contain the same entities.\nReturn a JSON object with the following format:\n{\"reasoning\": explaining your reasoning, \"decision\": 1 if the lists matched, 0 otherwise}\n'''\n\n@pxt.udf\ndef eval_prompt(listA: str, listB: str) -> list[dict]:\n    return [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': f'listA: \"{listA}\" \\n listB: \"{listB}\"'}\n    ]\n\nt.add_computed_column(eval_prompt=eval_prompt(t.response, t.ground_truth))\n```\n\n----------------------------------------\n\nTITLE: Creating a User-Defined Aggregate (UDA) in Pixeltable\nDESCRIPTION: Demonstrates how to create a custom aggregation function (UDA) that calculates the sum of squares. UDAs enable processing multiple rows into a single result and can be used in group_by operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@pxt.uda\nclass sum_of_squares(pxt.Aggregator):\n    def __init__(self):\n        self.cur_sum = 0\n        \n    def update(self, val: int) -> None:\n        self.cur_sum += val * val\n        \n    def value(self) -> int:\n        return self.cur_sum\n```\n\n----------------------------------------\n\nTITLE: Inserting Data and Generating Content with Gemini in Python\nDESCRIPTION: This snippet inserts sample inputs into the Pixeltable and triggers Gemini to generate content based on these inputs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-gemini.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Ask Gemini to generate some content based on the input\nt.insert([\n    {'input': 'Write a story about a magic backpack.'}, \n    {'input': 'Tell me a science joke.'}\n])\n```\n\n----------------------------------------\n\nTITLE: Configuring Similarity Metrics for Embedding Indexes in Pixeltable\nDESCRIPTION: This snippet demonstrates how to configure the similarity metric when creating an embedding index in Pixeltable. It shows the available options including cosine similarity (default), inner product, and L2 distance.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/embedding-index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Available metrics:\ndocs.add_embedding_index(\n    column='content',\n    metric='cosine'  # Default\n    # Other options:\n    # metric='ip'    # Inner product\n    # metric='l2'    # L2 distance\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory and Table for Image Dataset\nDESCRIPTION: Sets up a Pixeltable directory and table for storing sample images, then inserts image URLs into the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport fiftyone as fo\nimport pixeltable as pxt\n\n# Create a Pixeltable directory for the demo. We first drop the directory if it\n# exists, in order to ensure a clean environment.\n\npxt.drop_dir('fo_demo', force=True)\npxt.create_dir('fo_demo')\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create a Pixeltable table for our dataset and insert some sample images.\n\nurl_prefix = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images'\n\nurls = [\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000019.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000030.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000034.jpg',\n]\n\nt = pxt.create_table('fo_demo.images', {'image': pxt.Image})\nt.insert({'image': url} for url in urls)\nt.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for a Pixeltable Discord Bot\nDESCRIPTION: This requirements.txt file lists the required Python packages with their minimum versions for a Discord bot. It includes discord.py for Discord API interaction, pixeltable for data manipulation, pillow for image processing, python-dotenv for environment variables, aiohttp for async HTTP requests, and numpy for numerical operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/context-aware-discord-bot/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndiscord.py>=2.0.0\npixeltable>=0.2.24\npillow>=9.0.0\npython-dotenv>=0.19.0\naiohttp>=3.8.0\nnumpy>=1.21.0\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Detection Label Sets to Voxel51 Dataset\nDESCRIPTION: Demonstrates how to include multiple detection label sets in the same Voxel51 dataset by using different DETR models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(detections_101=detr_for_object_detection(\n    t.image, model_id='facebook/detr-resnet-101'\n))\n```\n\nLANGUAGE: python\nCODE:\n```\nfo_dataset = pxt.io.export_images_as_fo_dataset(\n    t,\n    t.image,\n    classifications=vit_to_fo(t.classifications),\n    detections={\n        'detections_50': detr_to_fo(t.image, t.detections),\n        'detections_101': detr_to_fo(t.image, t.detections_101)\n    }\n)\nsession = fo.launch_app(fo_dataset)\n```\n\n----------------------------------------\n\nTITLE: Using User-Defined Aggregates (UDAs) in Pixeltable\nDESCRIPTION: Shows how to use a User-Defined Aggregate (UDA) with and without grouping. UDAs can be applied directly to columns or used with group_by operations for segmented analysis.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Basic usage\ntable.select(sum_of_squares(table.value)).collect()\n\n# With grouping\ntable.group_by(table.category).select(\n    table.category,\n    sum_of_squares(table.value)\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Querying Mistral AI Model and Displaying Results\nDESCRIPTION: Inserts a sample query into the table to generate a response from the Mistral AI model, then displays both the input and response.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nt.insert(input=\"What three species of fish have the highest mercury content?\")\nt.select(t.input, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Setting up Chat Completion Table\nDESCRIPTION: Creates a table for chat completions using Meta's LLaMA 3 model with specified parameters.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.replicate import run\n\n# Create a table in Pixeltable and pick a model hosted on Replicate with some parameters\n\nt = pxt.create_table('replicate_demo.chat', {'prompt': pxt.String})\n\ninput = {\n    'system_prompt': 'You are a helpful assistant.',\n    'prompt': t.prompt,\n    # These parameters are optional and can be used to tune model behavior:\n    'max_tokens': 300,\n    'top_p': 0.9,\n    'temperature': 0.8\n}\nt.add_computed_column(output=run(input, ref='meta/meta-llama-3-8b-instruct'))\n```\n\n----------------------------------------\n\nTITLE: Applying Image Classification and Object Detection Models\nDESCRIPTION: Adds computed columns to the Pixeltable dataset using ViT for image classification and DETR for object detection.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import vit_for_image_classification, detr_for_object_detection\n\nt.add_computed_column(classifications=vit_for_image_classification(\n    t.image, model_id='google/vit-base-patch16-224'\n))\nt.add_computed_column(detections=detr_for_object_detection(\n    t.image, model_id='facebook/detr-resnet-50'\n))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Image with Fireworks API in Python\nDESCRIPTION: This function sends a POST request to the Fireworks API to analyze an image based on the given prompt and model. It handles the API response and returns the analysis result.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/fireworks.md#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef fireworks_analyze_image(image: bytes, prompt: str, model: str, api_key: str, **kwargs) -> str:\n    base_url, headers = init_fireworks_client(api_key)\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"image\": base64.b64encode(image).decode(\"utf-8\"),\n        \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n        \"temperature\": kwargs.get(\"temperature\", 0.7),\n        \"top_p\": kwargs.get(\"top_p\", 1),\n        \"n\": 1,\n        \"stream\": False,\n    }\n    response = requests.post(f\"{base_url}/completions\", headers=headers, json=data)\n    response.raise_for_status()\n    return response.json()[\"choices\"][0][\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Audio Files\nDESCRIPTION: Shows how to process multiple audio files in batch mode using S3 bucket sources.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/audio.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\naudio_files = [\n    \"s3://your-bucket/audio1.mp3\",\n    \"s3://your-bucket/audio2.mp3\",\n    \"s3://your-bucket/audio3.mp3\"\n]\n\naudio_t.insert([{\"audio_file\": f} for f in audio_files])\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Columns for Annotations\nDESCRIPTION: Adds computed columns to extract annotation values and video metadata.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nv.add_computed_column(\n    video_category=v.annotations[0].result[0].value.choices[0]\n)\nv.select(v.video, v.annotations, v.video_category).head()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Pixeltable and Voxel51 Integration\nDESCRIPTION: Installs the necessary Python libraries (pixeltable, fiftyone, torch, transformers) for working with Pixeltable and Voxel51.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fiftyone.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable fiftyone torch transformers\n```\n\n----------------------------------------\n\nTITLE: Extracting Audio Features with Hugging Face Models in Python\nDESCRIPTION: Function for extracting audio features using Hugging Face models. It processes audio inputs with a specified huggingface pipeline, handling various combinations of audio sources and options.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/huggingface.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef extract_audio_features(audio_path: typing.Union[str, pl.Expr, list, tuple],\n                          model: typing.Optional[typing.Union[str, pl.Expr]] = None,\n                          model_kwargs: typing.Optional[dict] = None,\n                          preprocess_params: typing.Optional[dict] = None,\n                          **kwargs) -> pl.Expr:\n    \"\"\"\n    Extract audio features using a huggingface pipeline.\n    \"\"\"\n    from pixeltable.functions.huggingface.huggingface_pipelines import HuggingfacePipeline\n    from pixeltable.functions.utils import resolve_param\n\n    return HuggingfacePipeline(\n        pipeline_task='feature-extraction',\n        model=resolve_param(model),\n        model_kwargs=resolve_param(model_kwargs),\n        preprocess_params=resolve_param(preprocess_params),\n        **kwargs\n    )(resolve_param(audio_path))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Column Reference in Pixeltable\nDESCRIPTION: Shows a basic column reference expression which doesn't retrieve data until used in a query.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt.image\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Date Extraction UDF in Python\nDESCRIPTION: Shows how to create and use a local UDF that extracts the year from a date string. Local UDFs are serialized with their columns, and changes to the UDF only affect new columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Defined directly in your code\n@pxt.udf\ndef extract_year(date_str: str) -> int:\n    return int(date_str.split('-')[0])\n    \n# Used immediately\ntable.add_computed_column(\n    year=extract_year(table.date)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Date Extraction UDF in Python\nDESCRIPTION: Shows how to create and use a local UDF that extracts the year from a date string. Local UDFs are serialized with their columns, and changes to the UDF only affect new columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Defined directly in your code\n@pxt.udf\ndef extract_year(date_str: str) -> int:\n    return int(date_str.split('-')[0])\n    \n# Used immediately\ntable.add_computed_column(\n    year=extract_year(table.date)\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Anthropic Integration with a Query\nDESCRIPTION: Inserts a query into the table and displays the input and response columns, demonstrating the end-to-end Anthropic integration working in Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Start a conversation\nt.insert(input=\"What was the outcome of the 1904 US Presidential election?\")\nt.select(t.input, t.response).show()\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completion with Anthropic API in Python\nDESCRIPTION: This function sends a series of messages to the Anthropic API for chat-based completion. It formats the messages, handles API parameters, and returns the generated response along with usage statistics.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/anthropic.md#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef anthropic_chat_completion(\n    messages: List[Dict[str, str]],\n    model: str,\n    api_key: str,\n    max_tokens_to_sample: int = 256,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stop_sequences: Optional[List[str]] = None,\n) -> Tuple[str, Dict[str, Any]]:\n    client = get_anthropic_client(api_key)\n    prompt = \"\\n\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n    completion = client.completion(\n        prompt=prompt,\n        model=model,\n        max_tokens_to_sample=max_tokens_to_sample,\n        temperature=temperature,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n    )\n    usage = {\n        \"prompt_tokens\": completion.usage.input_tokens,\n        \"completion_tokens\": completion.usage.output_tokens,\n        \"total_tokens\": completion.usage.input_tokens + completion.usage.output_tokens,\n    }\n    return completion.completion, usage\n```\n\n----------------------------------------\n\nTITLE: Inserting Images from URLs\nDESCRIPTION: Populates the image table with images from GitHub URLs using a list comprehension.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nurl_prefix = 'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/images'\nimages = ['000000000139.jpg', '000000000632.jpg', '000000000872.jpg']\nt.insert({'source': f'{url_prefix}/{image}'} for image in images)\n```\n\n----------------------------------------\n\nTITLE: Running Test Suite\nDESCRIPTION: Command to execute the test suite with verbose output using pytest.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/tests/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest test.py -v\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable and Anthropic Libraries\nDESCRIPTION: Installs the required Python libraries for working with Pixeltable and Anthropic API.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable anthropic\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with WhisperX in Python\nDESCRIPTION: This function uses WhisperX to transcribe audio files. It supports various input formats and allows customization of the transcription process through parameters like language, compute type, and batch size.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/ext/functions/whisperx.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef whisperx_transcribe(audio_path: str, language: Optional[str] = None, batch_size: int = 16, compute_type: str = \"float16\") -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n    import whisperx\n\n    device = \"cuda\"\n    audio = whisperx.load_audio(audio_path)\n    model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, language=language)\n\n    result = model.transcribe(audio, batch_size=batch_size, language=language)\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Viewing Image Table Contents\nDESCRIPTION: Displays all rows in the image table using the collect() method.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nt.collect()\n```\n\n----------------------------------------\n\nTITLE: Generating Text with MistralAI in Python\nDESCRIPTION: This function generates text using MistralAI's model. It takes a prompt, model name, and optional parameters for temperature and max tokens. The function returns the generated text as a string.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/functions/mistralai.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef mistral_generate(prompt: str, model: str = 'mistral-tiny', temperature: float = 0.7, max_tokens: int = 500) -> str:\n    client = _get_mistral_client()\n    chat_response = client.chat(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n    return chat_response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Inserting a Single Row into a Pixeltable Table\nDESCRIPTION: Shows an alternative syntax for inserting a single row of data into a Pixeltable table using keyword arguments.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfilms_t.insert(film_name='Inside Out 2', year=2024, revenue=1462.7)\n```\n\n----------------------------------------\n\nTITLE: Splitting Audio File in Python\nDESCRIPTION: This method splits an audio file into segments. It reads the audio file, calculates the number of segments, and yields each segment along with its metadata. The method handles different audio formats and ensures proper segment creation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/iterators/audio-splitter.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef split(self, audio_file: str) -> Iterator[Tuple[np.ndarray, Dict[str, Any]]]:\n    audio = AudioSegment.from_file(audio_file)\n    duration_ms = len(audio)\n    num_segments = max(1, int((duration_ms - self.overlap_ms) / (self.segment_duration_ms - self.overlap_ms)))\n\n    for i in range(num_segments):\n        start_ms = i * (self.segment_duration_ms - self.overlap_ms)\n        end_ms = min(start_ms + self.segment_duration_ms, duration_ms)\n\n        if end_ms - start_ms < self.min_segment_ms:\n            continue\n\n        segment = audio[start_ms:end_ms]\n        samples = np.array(segment.get_array_of_samples())\n\n        if segment.channels == 2:\n            samples = samples.reshape((-1, 2))\n\n        yield samples, {\n            'start_time': start_ms / 1000,\n            'end_time': end_ms / 1000,\n            'sample_rate': segment.frame_rate,\n            'channels': segment.channels\n        }\n```\n\n----------------------------------------\n\nTITLE: Creating Short Chunks View in Pixeltable\nDESCRIPTION: Creates a view 'short_chunks' that splits documents into paragraphs with a token limit of 72.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshort_chunks = pxt.create_view(\n    'rag_ops_demo.short_chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,token_limit',\n        limit=72,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable via pip\nDESCRIPTION: Installs the Pixeltable package using pip, the Python package installer. This command should be run after activating the virtual environment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable\n```\n\n----------------------------------------\n\nTITLE: Dropping a Directory and All Contents\nDESCRIPTION: Removes an entire directory hierarchy including all nested subdirectories and tables using drop_dir() with force=True. This irreversible operation deletes all contained objects.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Delete the entire directory and all its contents, including any nested\n# subdirectories (cannot be reverted!)\n\npxt.drop_dir('fundamentals', force=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Views for Document Chunking\nDESCRIPTION: Shows how to create a view for splitting documents into chunks using DocumentSplitter. Demonstrates view creation and iteration over chunks.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nchunks_table = pxt.create_view(\n    'rag_demo.chunks',\n    documents_table,\n    iterator=DocumentSplitter.create(\n        document=documents_table.document,\n        separators='token_limit', limit=300)\n)\n```\n\n----------------------------------------\n\nTITLE: Automatic YOLOX Processing Example\nDESCRIPTION: Shows the basic syntax for configuring YOLOX detection in a computed column.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndetections=yolox(images.image, model_id='yolox_s')\n```\n\n----------------------------------------\n\nTITLE: Cloning Pixeltable and Gradio Application Repository\nDESCRIPTION: These commands clone the Pixeltable and Gradio application repository from GitHub and navigate to the project directory. This is the first step in setting up the application locally.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pixeltable/pixeltable/tree/main/docs/sample-apps/pixeltable-and-gradio-application.git\ncd pixeltable-and-gradio-application\n```\n\n----------------------------------------\n\nTITLE: Querying a Table with Image Data\nDESCRIPTION: Demonstrates how to query a table containing both structured data and images. Pixeltable will automatically embed image thumbnails in notebook output when querying image columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\neq_t.where(eq_t.id >= 1000).select(eq_t.id, eq_t.magnitude, eq_t.location, eq_t.map_image).head(5)\n```\n\n----------------------------------------\n\nTITLE: Importing Media Data into Pixeltable\nDESCRIPTION: Demonstrates how to create a table and import video files from S3 storage into Pixeltable. Shows basic table creation and data insertion operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\nv = pxt.create_table('external_data.videos', {'video': pxt.Video})\n\nprefix = 's3://multimedia-commons/'\npaths = [\n    'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',\n    'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',\n    'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'\n]\nv.insert({'video': prefix + p} for p in paths)\n```\n\n----------------------------------------\n\nTITLE: YOLOX Detection Results Structure\nDESCRIPTION: Example of the JSON structure returned by YOLOX detection operations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"boxes\": [[x1, y1, x2, y2], ...],\n    \"scores\": [0.98, ...],\n    \"labels\": [1, ...],\n    \"label_text\": [\"person\", ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Anthropic API Key\nDESCRIPTION: Sets up the Anthropic API key as an environment variable, prompting the user to enter it if not already set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-anthropic.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'ANTHROPIC_API_KEY' not in os.environ:\n    os.environ['ANTHROPIC_API_KEY'] = getpass.getpass('Anthropic API Key:')\n```\n\n----------------------------------------\n\nTITLE: Adding Image Classification Column in Pixeltable\nDESCRIPTION: Creates a computed column that applies a Vision Transformer model to classify MNIST images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import vit_for_image_classification\n\nt.add_computed_column(classification=vit_for_image_classification(\n    t.image,\n    model_id='farleyknight-org-username/vit-base-mnist'\n))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable Video Search Workflow\nDESCRIPTION: This snippet shows the installation of necessary Python packages for the Pixeltable video search workflow, including Pixeltable itself, OpenAI, Whisper for audio transcription, spaCy, and sentence-transformers for text processing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable openai tiktoken openai-whisper spacy sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Importing Hugging Face Datasets in Pixeltable\nDESCRIPTION: Demonstrates importing from Hugging Face datasets with schema inference and overrides. Supports custom split information and specialized data types for machine learning.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n# Load Hugging Face dataset\ndataset = load_dataset('mnist', split='train[:1000]')\n\n# Import with default schema inference\ntable1 = pxt.io.import_huggingface_dataset(\n    'myproject.hf_data',\n    dataset\n)\n\n# Import with schema overrides\ntable2 = pxt.io.import_huggingface_dataset(\n    'myproject.hf_typed',\n    dataset,\n    schema_overrides={\n        'image': pxt.Image,\n        'label': pxt.Int\n    }\n)\n\n# Import with split information\ntable3 = pxt.io.import_huggingface_dataset(\n    'myproject.hf_split',\n    dataset,\n    column_name_for_split='split_info'\n)\n```\n\n----------------------------------------\n\nTITLE: Optimizing UDF with Model Caching and Batching in Python\nDESCRIPTION: Demonstrates how to combine batching and caching techniques for optimal performance in UDFs. The function caches an expensive model and processes multiple items in batches.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf(batch_size=32)\ndef process_chunk(items: Batch[str]) -> Batch[str]:\n    if not hasattr(process_chunk, 'model'):\n        process_chunk.model = load_expensive_model()\n    return process_chunk.model.process_batch(items)\n```\n\n----------------------------------------\n\nTITLE: Where Clause with Boolean Expression\nDESCRIPTION: Shows how to use boolean expressions in where clauses for filtering data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nt.where(t.label == '4').select(t.image).show()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RAG Operations in Pixeltable\nDESCRIPTION: Installs required packages for the RAG operations demo, including Pixeltable, sentence-transformers, spacy, and tiktoken.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-operations.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q pixeltable sentence-transformers spacy tiktoken\n```\n\n----------------------------------------\n\nTITLE: Importing Parquet Data in Pixeltable\nDESCRIPTION: Illustrates Parquet file importing with support for single files and directories. Includes schema overrides for array types and glob pattern matching for multiple files.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Basic Parquet import\ntable1 = pxt.io.import_parquet(\n    table='myproject.parquet_data',\n    parquet_path='data.parquet'\n)\n\n# Parquet import with schema overrides\ntable2 = pxt.io.import_parquet(\n    table='myproject.parquet_typed',\n    parquet_path='data.parquet',\n    schema_overrides={\n        'features': pxt.Array[(100,), pxt.Float],\n        'labels': pxt.Array[(10,), pxt.Int]\n    }\n)\n\n# Import from directory of Parquet files\ntable3 = pxt.io.import_parquet(\n    table='myproject.parquet_dir',\n    parquet_path='data/*.parquet'  # Glob pattern\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API Key\nDESCRIPTION: This snippet sets up the OpenAI API key by prompting the user if it's not already set in the environment variables.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n```\n\n----------------------------------------\n\nTITLE: Invalid Method Call Example\nDESCRIPTION: Shows an example of an invalid method call that must be used as a function call instead since it's from a non-core module.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nt.image.vit_for_image_classification(\n    model_id='farleyknight-org-username/vit-base-mnist'\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable and Dependencies\nDESCRIPTION: Command to install Pixeltable along with required dependencies including PyTorch, Transformers, and OpenAI. These packages enable the core functionality needed for the image analysis application.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/quick-start.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -qU torch transformers openai pixeltable\n```\n\n----------------------------------------\n\nTITLE: Creating Video Storage Table\nDESCRIPTION: Creates a Pixeltable table with video and timestamp columns for storing video data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\nschema = {\n    'video': pxt.Video,\n    'date': pxt.Timestamp\n}\n\n# Before creating the table, we drop the `ls_demo` dir and all its contents,\n# in order to ensure a clean environment for the demo.\npxt.drop_dir('ls_demo', force=True)\npxt.create_dir('ls_demo')\nvideos_table = pxt.create_table('ls_demo.videos', schema)\n```\n\n----------------------------------------\n\nTITLE: Creating Population Data Table with Computed Columns in Python\nDESCRIPTION: Demonstrates how to create a table with population data and add computed columns for year-over-year changes using Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/computed-columns.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Create a table with population data\npop_t = pxt.io.import_csv(\n    'fundamentals.population',\n    'https://github.com/pixeltable/pixeltable/raw/main/docs/source/data/world-population-data.csv'\n)\n\n# Add a computed column for year-over-year change\npop_t.add_computed_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n\n# Create a computed column to track population change year over year\npop_t.add_computed_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n\n# Display the results\npop_t.select(pop_t.country, pop_t.pop_2022, pop_t.pop_2023, pop_t.yoy_change).head(5)\n```\n\n----------------------------------------\n\nTITLE: Method Call Syntax in Pixeltable\nDESCRIPTION: Demonstrates equivalent method call and function call syntax for Pixeltable UDFs using image rotation example.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\na = t.image.rotate(90)\nb = pxt.functions.image.rotate(t.image, 90)\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment for Pixeltable Installation (Python)\nDESCRIPTION: Creates an isolated Python environment for Pixeltable installation using venv. This step helps avoid package conflicts with other Python projects.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\n```\n\n----------------------------------------\n\nTITLE: Setting up Pixeltable environment in Python\nDESCRIPTION: This code sets up the Pixeltable environment by importing necessary modules, dropping existing directories, and creating a new directory for external data.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nimport random\nimport shutil\nimport pixeltable as pxt\n\n# First drop the `external_data` directory if it exists, to ensure\n# a clean environment for the demo\npxt.drop_dir('external_data', force=True)\npxt.create_dir('external_data')\n```\n\n----------------------------------------\n\nTITLE: Importing JSON Data in Pixeltable\nDESCRIPTION: Shows JSON data importing from local files and URLs. Supports schema overrides for JSON and array types, with flexible data structure handling.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/bringing-data.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Basic JSON import\ntable1 = pxt.io.import_json(\n    'myproject.json_data',\n    'data.json'\n)\n\n# JSON import with schema overrides\ntable2 = pxt.io.import_json(\n    'myproject.json_typed',\n    'data.json',\n    schema_overrides={\n        'metadata': pxt.Json,\n        'features': pxt.Array[(None,), pxt.Float]\n    }\n)\n\n# Import from URL\ntable3 = pxt.io.import_json(\n    'myproject.json_url',\n    'https://api.example.com/data.json'\n)\n```\n\n----------------------------------------\n\nTITLE: Running Model on Transformed Images in Pixeltable\nDESCRIPTION: Demonstrates composing expressions by applying an image transformation before model inference.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nrot_model_result = vit_for_image_classification(\n    t.image.rotate(90),\n    model_id='farleyknight-org-username/vit-base-mnist'\n)\n\nt.select(t.image, rot_label=rot_model_result.labels[0]).head(5)   \n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment for Pixeltable (Windows)\nDESCRIPTION: Activates the virtual environment on Windows systems to prepare for Pixeltable installation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nvenv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Dropping a Table from Pixeltable\nDESCRIPTION: Deletes an entire table from the database using drop_table(). This operation cannot be reverted and removes all data and schema information for the specified table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Delete the entire table (cannot be reverted!)\n\npxt.drop_table('fundamentals.earthquakes')\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom BERT Embedding UDF in Pixeltable\nDESCRIPTION: Creates a custom user-defined function (UDF) for computing text embeddings using BERT model.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text  # Necessary to ensure BERT dependencies are loaded\nimport pixeltable as pxt\n\n@pxt.udf\ndef bert(input: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"Computes text embeddings using the small_bert model.\"\"\"\n    preprocessor = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n    bert_model = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2')\n    tensor = tf.constant([input])  # Convert the string to a tensor\n    result = bert_model(preprocessor(tensor))['pooled_output']\n    return result.numpy()[0, :]\n```\n\n----------------------------------------\n\nTITLE: Querying New Image Results\nDESCRIPTION: Retrieves the source images, bounding boxes, labels, and metadata for the newly added images.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.source, t.image_with_bb, t.detections.label_text, t.metadata).tail(2)\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable with pip\nDESCRIPTION: This snippet shows how to install Pixeltable using pip package manager. This is the first step in getting started with Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/pixeltable.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable\n```\n\n----------------------------------------\n\nTITLE: Terminating Label Studio Process in Pixeltable Notebook\nDESCRIPTION: This code terminates the running Label Studio process as part of the notebook cleanup process.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nls_process.kill()\n```\n\n----------------------------------------\n\nTITLE: Creating CLIP Embedding Index\nDESCRIPTION: Creates an embedding index on the Image column using the CLIP model from Hugging Face.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-hugging-face.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'Image' column\nt.add_embedding_index(\n    'Image',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)\n```\n\n----------------------------------------\n\nTITLE: Drawing Bounding Boxes on Detected Objects\nDESCRIPTION: Adds a new computed column that draws colored bounding boxes around detected objects, using the detection results to indicate object categories.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions.vision import draw_bounding_boxes\n\nt.add_computed_column(image_with_bb=draw_bounding_boxes(\n    t.source, t.detections.boxes, t.detections.label_text, fill=True\n))\nt.select(t.source, t.image_with_bb).collect()\n```\n\n----------------------------------------\n\nTITLE: Installing Pixelagent and Provider Dependencies\nDESCRIPTION: Commands to install the Pixelagent package along with provider-specific dependencies for Claude (Anthropic) and GPT (OpenAI) models.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/pixelagent.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixelagent\n# Install provider-specific dependencies\npip install anthropic  # For Claude models\npip install openai     # For GPT models\n```\n\n----------------------------------------\n\nTITLE: Setting Up Demo Directory in Pixeltable\nDESCRIPTION: Creates a fresh demo directory by first dropping any existing one and then creating a new one.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('demo', force=True)\npxt.create_dir('demo')\n```\n\n----------------------------------------\n\nTITLE: Chaining Views in Pixeltable\nDESCRIPTION: This snippet shows how to create views based on other views, effectively chaining transformations. The example creates a new view that filters chunks from an existing view to include only those with text length over 100 characters.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/views.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a view of embedded chunks\nembedded_chunks = pxt.create_view(\n    'docs.embedded_chunks',\n    chunks.where(chunks.text.len() > 100)\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Image Metadata as a Computed Column\nDESCRIPTION: Creates a computed column that extracts and stores metadata from each image using a built-in UDF.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nt.add_computed_column(metadata=t.source.get_metadata())\nt.collect()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Prompt Engineering Studio\nDESCRIPTION: This command installs the necessary Python packages for running the Prompt Engineering Studio application. It includes Gradio for the UI, Pixeltable for data management, TextBlob and NLTK for text analysis, and MistralAI for LLM capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gradio pixeltable textblob nltk mistralai\n```\n\n----------------------------------------\n\nTITLE: Pixeltable IO Methods List\nDESCRIPTION: Lists available input/output methods in the Pixeltable IO module, including functions for importing data from various formats and exporting to Parquet format, as well as Label Studio project creation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/io.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptions:\n  members:\n    - create_label_studio_project\n    - export_parquet\n    - import_csv\n    - import_excel\n    - import_huggingface_dataset\n    - import_json\n    - import_pandas\n    - import_parquet\n    - import_rows\n```\n\n----------------------------------------\n\nTITLE: Accessing COCO Class Labels\nDESCRIPTION: Example of how to map COCO category indices to human-readable class names\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/yolox.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom yolox.data.datasets import COCO_CLASSES\n\nprint(COCO_CLASSES[7])\n# Output: 'truck'\n```\n\n----------------------------------------\n\nTITLE: Querying inserted video data in Pixeltable\nDESCRIPTION: This code demonstrates how to query the inserted video data, showing that Pixeltable presents locally cached counterparts of S3 files.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrows = list(v.select(v.video).collect())\nrows[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Mistral AI API Key\nDESCRIPTION: This command sets the Mistral AI API key as an environment variable. The API key is required for accessing Mistral AI's LLM capabilities in the application.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable and Setting Up Demo Environment\nDESCRIPTION: This snippet installs Pixeltable, creates a demo directory and table, and adds sample data for experimenting with UDFs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Create the directory and table\npxt.drop_dir('udf_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('udf_demo')\nt = pxt.create_table('udf_demo.strings', {'input': pxt.String})\n\n# Add some sample data\nt.insert([{'input': 'Hello, world!'}, {'input': 'You can do a lot with Pixeltable UDFs.'}])\nt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Batched Text Embedding UDF in Python\nDESCRIPTION: Shows how to implement a UDF that processes multiple text items in a single batch for efficiency. This is particularly useful for model inference that can process multiple inputs at once.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf(batch_size=16)\ndef embed_texts(\n    texts: Batch[str]\n) -> Batch[pxt.Array]:\n    # Process multiple texts at once\n    return model.encode(texts)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pixeltable and inserting S3 video data\nDESCRIPTION: This snippet creates a Pixeltable for videos and inserts references to video files stored in S3.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nv = pxt.create_table('external_data.videos', {'video': pxt.Video})\n\nprefix = 's3://multimedia-commons/'\npaths = [\n    'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',\n    'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',\n    'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'\n]\nv.insert({'video': prefix + p} for p in paths)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up NLTK Data\nDESCRIPTION: Imports necessary libraries and sets up NLTK data for text processing. It also prompts for the Mistral AI API key if not set in the environment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/prompt-engineering-studio-gradio-application/pixeltable-and-gradio-application.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport re\nfrom datetime import datetime\nimport getpass\n\nimport gradio as gr\nimport nltk\nimport pixeltable as pxt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom pixeltable.functions.mistralai import chat_completions\nfrom textblob import TextBlob\n\n# Download required NLTK data\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt_tab', quiet=True)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Set up Mistral API key\nif 'MISTRAL_API_KEY' not in os.environ:\n    os.environ['MISTRAL_API_KEY'] = getpass.getpass('Mistral AI API Key:')\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pixeltable Environment\nDESCRIPTION: This snippet sets up the Pixeltable environment by importing required modules and creating a clean directory for the demo.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pixeltable as pxt\n\n# Ensure a clean slate for the demo\npxt.drop_dir('rag_demo', force=True)\npxt.create_dir('rag_demo')\n```\n\n----------------------------------------\n\nTITLE: Using Pixeltable Video Search Workflow\nDESCRIPTION: This code demonstrates how to use the created video search workflow. It connects to the tables and views, inserts videos into the knowledge base, and performs similarity searches on both audio transcriptions and video frames.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/video.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nimport pixeltable as pxt\n\n# Constants\ndirectory = 'video_index'\ntable_name = f'{directory}.video'\n\n# Connect to your tables and views\nvideo_index = pxt.get_table(table_name)\nframes_view = pxt.get_table(f'{directory}.video_frames')\ntranscription_chunks = pxt.get_table(f'{directory}.video_sentence_chunks')\n\n# Insert videos to the knowledge base\nvideos = [\n    'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/audio-transcription-demo/'\n    f'Lex-Fridman-Podcast-430-Excerpt-{n}.mp4'\n    for n in range(3)\n]\n\nvideo_index.insert({'video': video, 'uploaded_at': datetime.now()} for video in videos[:2])\n\naudio_results = (\n    transcription_chunks.order_by(audio_sim, transcription_chunks.uploaded_at, asc=False)\n    .limit(5)\n    .select(transcription_chunks.text, transcription_chunks.uploaded_at, similarity=audio_sim)\n    .collect()\n)\n\n# Fetch 5 most similar frames\nframe_results = (\n    frames_view.order_by(image_sim, frames_view.uploaded_at, asc=False)\n    .limit(5)\n    .select(frames_view.image_description, frames_view.uploaded_at, similarity=image_sim)\n    .collect()\n)\n\nprint(audio_results)\nprint(frame_results)\n```\n\n----------------------------------------\n\nTITLE: Modifying a Local UDF and Observing Behavior\nDESCRIPTION: This snippet shows how modifying a local UDF affects its behavior in existing and new computed columns.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:\n        words = [\n            word if word[-1].isalnum() else word[:-1]\n            for word in words\n        ]\n    i = np.argmax([len(word) for word in words])\n    return words[i]\n\nt.insert(input=\"Let's check that it still works.\")\nt.show()\n\nt.add_computed_column(\n    longest_word_3=longest_word(t.input, strip_punctuation=True)\n)\nt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting up Python environment for Pixeltable Backend API\nDESCRIPTION: Commands to create a virtual environment, activate it, install dependencies, and start the FastAPI development server for the Pixeltable backend API.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/backend/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv venv .venv # Or python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\nuv pip install -r requirements.txt\nfastapi dev api/main.py\n```\n\n----------------------------------------\n\nTITLE: Starting Jupyter Server for Pixeltable\nDESCRIPTION: Launches the Jupyter Notebook server, allowing you to create and run notebooks with Pixeltable.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Defining FrameIterator Class in Python\nDESCRIPTION: This code snippet defines the FrameIterator class within the pixeltable.iterators module. The class is likely used for iterating over frames in video processing tasks, but the specific implementation details are not provided in this documentation stub.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/pixeltable/iterators/frame-iterator.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n## ::: pixeltable.iterators.FrameIterator\n```\n\n----------------------------------------\n\nTITLE: Using a Custom UDA to Aggregate All Values\nDESCRIPTION: Demonstrates how to apply the custom sum_of_squares UDA to calculate the sum of squares for all integers in the table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nt.select(sum_of_squares(t.val)).collect()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Backend Environment with Python\nDESCRIPTION: Commands for setting up the backend development environment using Python's virtual environment tools. It creates and activates a virtual environment, synchronizes dependencies, and starts the FastAPI development server.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd backend\nuv venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\nuv sync\nfastapi dev api/main.py\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment for Pixeltable (Linux/MacOS)\nDESCRIPTION: Activates the virtual environment on Linux or MacOS systems to prepare for Pixeltable installation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable Website Search\nDESCRIPTION: Installs the required Python packages for the Pixeltable website search project, including pixeltable, tiktoken, and sentence-transformers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable tiktoken sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom User-Defined Aggregate in Pixeltable\nDESCRIPTION: Demonstrates how to create a custom UDA (User-Defined Aggregate) to calculate the sum of squares. Implements the required methods of the Aggregator class and applies the @pxt.uda decorator.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@pxt.uda\nclass sum_of_squares(pxt.Aggregator):\n    def __init__(self):\n        # No data yet; initialize `cur_sum` to 0\n        self.cur_sum = 0\n\n    def update(self, val: int) -> None:\n        # Update the value of `cur_sum` with the new datapoint\n        self.cur_sum += val * val\n\n    def value(self) -> int:\n        # Retrieve the current value of `cur_sum`\n        return self.cur_sum\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: Specifies exact versions of Python packages required for the project. Includes AWS CDK dependencies, development tools like black and isort, and various utility packages.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/aws/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nattrs==24.3.0\naws-cdk-asset-awscli-v1==2.2.215\naws-cdk-asset-kubectl-v20==2.1.3\naws-cdk-asset-node-proxy-agent-v6==2.1.0\naws-cdk-cloud-assembly-schema==38.0.1\naws-cdk-lib==2.173.1\nblack==24.10.0\ncattrs==24.1.2\nclick==8.1.7\nconstructs==10.4.2\nexceptiongroup==1.2.2\nimportlib-resources==6.4.5\nisort==5.13.2\njsii==1.106.0\nmypy-extensions==1.0.0\npackaging==24.2\npathspec==0.12.1\nplatformdirs==4.3.6\npublication==0.0.3\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\nsix==1.17.0\ntomli==2.2.1\ntypeguard==2.13.3\ntyping-extensions==4.12.2\nzipp==3.21.0\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable Dependencies\nDESCRIPTION: Command to install the required Pixeltable package using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/yolox.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable\n```\n\n----------------------------------------\n\nTITLE: Defining Pixeltable Workflow for Website Search\nDESCRIPTION: Creates the workflow structure for website search, including table creation, chunking, embedding, and search query definition. It sets up the necessary components for efficient content processing and searching.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/search/website.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nfrom pixeltable.iterators import DocumentSplitter\nfrom pixeltable.functions.huggingface import sentence_transformer\n\n# Initialize app structure\npxt.drop_dir(\"web_search\", force=True)\npxt.create_dir(\"web_search\")\n\n# Create website table\nwebsites_t = pxt.create_table(\n    \"web_search.websites\", \n    {\"website\": pxt.Document}\n)\n\n# Create chunked view for efficient processing\nwebsites_chunks = pxt.create_view(\n    \"web_search.website_chunks\",\n    websites_t,\n    iterator=DocumentSplitter.create(\n        document=websites_t.website,\n        separators=\"token_limit\",\n        limit=300  # Tokens per chunk\n    )\n)\n\n# Configure embedding model\nembed_model = sentence_transformer.using(\n    model_id=\"intfloat/e5-large-v2\"\n)\n\n# Add search capability\nwebsites_chunks.add_embedding_index(\n    column=\"text\",\n    string_embed=embed_model\n)\n\n# Define search query\n@pxt.query\ndef search_content(query_text: str, limit: int):\n    sim = websites_chunks.text.similarity(query_text)\n    return (\n        websites_chunks.order_by(sim, asc=False)\n        .select(\n            websites_chunks.text,\n            websites_chunks.website,\n            similarity=sim\n        )\n        .limit(limit)\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple UDF in Pixeltable\nDESCRIPTION: This example shows how to define a basic User-Defined Function (UDF) in Pixeltable using the @pxt.udf decorator.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef add_one(n: int) -> int:\n    return n + 1\n```\n\n----------------------------------------\n\nTITLE: Building Docker Container for JFK MCP Server\nDESCRIPTION: This command builds a Docker container for the JFK MCP server, packaging all dependencies and code for easy deployment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t jfk-mcp-server .\n```\n\n----------------------------------------\n\nTITLE: Demonstrating error handling for invalid paths in Pixeltable\nDESCRIPTION: This code shows how Pixeltable handles insertion attempts with invalid file paths.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/working-with-external-files.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nv.insert(video=prefix + 'bad_path.mp4')\n```\n\n----------------------------------------\n\nTITLE: Inserting a New Video into Pixeltable for Incremental Updates\nDESCRIPTION: This code inserts a new video into the base videos table, triggering automatic incremental updates of downstream views and computed columns, including video metadata, frames, and preannotations.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nvideos_table.insert(\n    video=url_prefix + '22a/948/22a9487a92956ac453a9c15e0fc4dd4.mp4',\n    date=today\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search on Images in Pixeltable\nDESCRIPTION: Demonstrates how to use the similarity() function to perform a nearest-neighbor search on the indexed image column, ordering results by similarity and limiting the output.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# retrieve the 'img' column of some row as a PIL.Image.Image\nsample_img = imgs.select(imgs.img).collect()[6]['img']\nsample_img\n\nsim = imgs.img.similarity(sample_img)\nres = (\n    imgs.order_by(sim, asc=False)  # Order by descending similarity\n    .limit(2)  # Limit number of results to 2\n    .select(imgs.id, imgs.img, sim)\n    .collect()  # Retrieve results now\n)\nres\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable in Python\nDESCRIPTION: Installs the Pixeltable library using pip in a Jupyter notebook environment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installs necessary Python packages including pixeltable, datasets, torch, transformers, tiktoken and spacy.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-hugging-face.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable datasets torch transformers tiktoken spacy\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing UDF in Python\nDESCRIPTION: Shows how to create a UDF that processes multiple rows at once using the batch_size parameter. Batched UDFs improve performance by handling multiple items in a single function call.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.func import Batch\n\n@pxt.udf(batch_size=32)\ndef process_batch(items: Batch[str]) -> Batch[str]:\n    results = []\n    for item in items:\n        results.append(item.upper())\n    return results\n    \n# Used like a regular UDF\ntable.add_computed_column(\n    processed=process_batch(table.text)\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting an Embedding Index in Pixeltable\nDESCRIPTION: Demonstrates how to remove a specific embedding index from a table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntxts.drop_embedding_index(idx_name='e5_idx')\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory\nDESCRIPTION: Creates a new directory in Pixeltable for storing demo tables, removing existing directory if present.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-replicate.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Remove the `replicate_demo` directory and its contents, if it exists\npxt.drop_dir('replicate_demo', force=True)\npxt.create_dir('replicate_demo')\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Mistral AI Integration\nDESCRIPTION: Installs the Pixeltable and Mistral AI Python packages needed for this integration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable mistralai\n```\n\n----------------------------------------\n\nTITLE: Defining Top-K Query\nDESCRIPTION: This snippet defines a reusable query for retrieving the top-k most similar chunks for a given query text.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@pxt.query\ndef top_k(query_text: str):\n    sim = chunks_t.text.similarity(query_text)\n    return (\n        chunks_t.order_by(sim, asc=False)\n            .select(chunks_t.text, sim=sim)\n            .limit(5)\n    )\n\n# Now add a computed column to `queries_t`, calling the query\n# `top_k` that we just defined.\nqueries_t.add_computed_column(\n    question_context=top_k(queries_t.Question)\n)\n```\n\n----------------------------------------\n\nTITLE: Querying with Named Embedding Index in Pixeltable\nDESCRIPTION: Demonstrates how to perform a similarity search using a specific named embedding index.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsim = txts.text.similarity('cubism', idx='minilm_idx')\nres = txts.order_by(sim, asc=False).limit(2).select(txts.text, sim).collect()\nres\n```\n\n----------------------------------------\n\nTITLE: Creating Pixeltable Directory for Mistral AI Demo\nDESCRIPTION: Sets up a new Pixeltable directory to store tables for the Mistral AI demo, removing any existing directory with the same name.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# Remove the `mistralai_demo` directory and its contents, if it exists\npxt.drop_dir('mistralai_demo', force=True)\npxt.create_dir('mistralai_demo')\n```\n\n----------------------------------------\n\nTITLE: Accessing YOLOX Training Help\nDESCRIPTION: Command to display help information for YOLOX training options\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/yolox.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolox train -h\n```\n\n----------------------------------------\n\nTITLE: Audio Transcription with Whisper in Pixeltable\nDESCRIPTION: Demonstrates how to transcribe audio files using OpenAI's Whisper model through Pixeltable's integration. Shows audio processing capabilities.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/integrations/frameworks.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pixeltable.functions import openai\n\n# Transcribe audio files\naudio_table.add_computed_column(\n    transcription=openai.transcriptions(\n        audio=audio_table.file,\n        model='whisper-1'\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pixeltable and Transformers\nDESCRIPTION: Installs the necessary Python packages (pixeltable, transformers, and sentence_transformers) using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable transformers sentence_transformers\n```\n\n----------------------------------------\n\nTITLE: Starting Label Studio Server\nDESCRIPTION: Launches a local Label Studio server process using subprocess.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nls_process = subprocess.Popen(['label-studio'], stderr=subprocess.PIPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pixeltable Tutorial\nDESCRIPTION: Installs Pixeltable and related dependencies including datasets, torch, and transformers using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable datasets torch transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pixeltable Tutorial\nDESCRIPTION: Installs Pixeltable and related dependencies including datasets, torch, and transformers using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/queries-and-expressions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable datasets torch transformers\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Generation UDF\nDESCRIPTION: This snippet defines a user-defined function (UDF) to create an LLM prompt from the top-k context chunks and a question.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef create_prompt(top_k_list: list[dict], question: str) -> str:\n    concat_top_k = '\\n\\n'.join(\n        elt['text'] for elt in reversed(top_k_list)\n    )\n    return f'''\n    PASSAGES:\n\n    {concat_top_k}\n\n    QUESTION:\n\n    {question}'''\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container for JFK MCP Server\nDESCRIPTION: This command runs the Docker container for the JFK MCP server, mapping port 8083 to allow access to the server from the host machine.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 8083:8083 --name jfk-mcp-server jfk-mcp-server\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: This snippet installs the necessary packages for the tutorial using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/rag-demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q pixeltable sentence-transformers tiktoken openai openpyxl\n```\n\n----------------------------------------\n\nTITLE: Testing UDF Redefinition in Notebook Scope\nDESCRIPTION: Tests UDF redefinition behavior in notebook scope by creating computed columns using different versions of the same UDF. Verifies that computed columns maintain the UDF definition from their creation time.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/notebook-test.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pxt.udf\ndef udf1(x: int) -> int:\n    return x + 1\n\nt.add_computed_column(int_col_plus_1=udf1(t.int_col))\n\n@pxt.udf\ndef udf1(x: int) -> int:\n    return x + 2\n\nt.add_computed_column(int_col_plus_2=udf1(t.int_col))\n\nrow = t.select(t.int_col, t.int_col_plus_1, t.int_col_plus_2).collect()[0]\nassert list(row.values()) == [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Displaying Single Image with Bounding Boxes\nDESCRIPTION: Retrieves and displays the first image with its bounding boxes at a larger size.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/computed-columns.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nt.select(t.image_with_bb).head(1)\n```\n\n----------------------------------------\n\nTITLE: Installing Spacy Model for Natural Language Processing\nDESCRIPTION: This command installs the English language model for Spacy, which is likely used for text processing in the application.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Upload for Label Studio\nDESCRIPTION: Default method for creating a Label Studio project with HTTP upload for small projects.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/label-studio.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npxt.io.create_label_studio_project(\n    videos,\n    video_config\n)\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: Detailed requirements.txt style listing of Python packages needed for a project involving FastAPI, machine learning, embeddings, and API functionality. Includes core dependencies, ML libraries, and utility packages for development and production deployment.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/backend/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Step 1: Core utilities\nsetuptools\nwheel\npip\n\n# Core dependencies\nfastapi[standard]\nuvicorn\npython-multipart\npixeltable\nmistune\n\n# Machine learning and embeddings\nnumpy\npydantic\nsentence-transformers\ntransformers\nspacy\ntorch\nopenai\nopenai-whisper\nopenpyxl\ntiktoken\npandas\n\n# Utility packages\npython-dotenv\npasslib\n\n# API and CORS\nstarlette\naiofiles\nhttpx\nvalidators\naiohttp\n\n# Optional but recommended for production\ngunicorn\n```\n\n----------------------------------------\n\nTITLE: Setting up Mistral AI API Key\nDESCRIPTION: Configures the Mistral AI API key as an environment variable, prompting the user for input if not already set.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-mistralai.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nif 'MISTRAL_API_KEY' not in os.environ:\n    os.environ['MISTRAL_API_KEY'] = getpass.getpass('Mistral AI API Key:')\n```\n\n----------------------------------------\n\nTITLE: Starting FastAPI Server\nDESCRIPTION: Command to start the FastAPI server for testing.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/tests/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing MCP Server Architecture with Mermaid\nDESCRIPTION: A flowchart showing the architecture of Pixeltable MCP servers, illustrating the connections between AI applications, Docker containers running different servers, and the Pixeltable multimodal database.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/libraries/overview.mdx#2025-04-07_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph \"AI Application\"\n        Host[\"MCP Client\\n(Cursor, Claude, etc.)\"]\n    end\n    subgraph \"Docker Environment\"\n        S1[\"Audio Index Server\\nlocalhost:8080/sse\"]\n        S2[\"Video Index Server\\nlocalhost:8081/sse\"]\n        S3[\"Image Index Server\\nlocalhost:8082/sse\"]\n        S4[\"Document Index Server\\nlocalhost:8083/sse\"]\n    end\n    subgraph \"Pixeltable\"\n        DB[(\"Multimodal\\nDatabase\")]\n    end\n    Host <-->|\"MCP Protocol\"| S1\n    Host <-->|\"MCP Protocol\"| S2\n    Host <-->|\"MCP Protocol\"| S3\n    Host <-->|\"MCP Protocol\"| S4\n    S1 <--> DB\n    S2 <--> DB\n    S3 <--> DB\n    S4 <--> DB\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Pixeltable\nDESCRIPTION: Creates a Conda environment named 'pxt' with Python 3.11 and activates it for Pixeltable installation. This is an alternative to using venv.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/overview/installation.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name pxt python=3.11\nconda activate pxt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment and Dependencies\nDESCRIPTION: Commands for creating a Python virtual environment, activating it, and installing required packages including pytest, pillow, numpy, requests, fastapi, uvicorn, and python-dotenv. Also creates a .env file for API key configuration.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/tests/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n# Install requirements\npip install pytest pillow numpy requests fastapi uvicorn python-dotenv\n\n# Create .env file\necho \"ANTHROPIC_API_KEY=your_api_key_here\" > .env\n```\n\n----------------------------------------\n\nTITLE: Agent Table UDF Flow Diagram in Mermaid\nDESCRIPTION: A mermaid diagram illustrating the flow of data through a Table UDF workflow, from user prompt through LLM calls and tool invocation to the final result in the Portfolio Manager table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/custom-functions.mdx#2025-04-07_snippet_22\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[User Prompt] --> B[Financial Analyst Table]\n    B --> C[LLM Call with Tools]\n    C --> D[Invoke Stock Info API]\n    D --> E[LLM Call with Results]\n    E --> F[Final Answer]\n    F --> G[Portfolio Manager Table]\n    \n    classDef default fill:#f9f9f9,stroke:#666,stroke-width:1px,color:black;\n    classDef table fill:#b3e0ff,stroke:#0066cc,stroke-width:2px,color:black;\n    classDef llm fill:#ffcc99,stroke:#ff8000,stroke-width:2px,color:black;\n    classDef tool fill:#c2e0c2,stroke:#339933,stroke-width:2px,color:black;\n    classDef result fill:#ffffb3,stroke:#b3b300,stroke-width:2px,color:black;\n    \n    class B,G table;\n    class C,E llm;\n    class D tool;\n    class F result;\n```\n\n----------------------------------------\n\nTITLE: Running the MCP Server\nDESCRIPTION: This command starts the MCP server, which handles requests and manages the document search functionality.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/jfk-files-mcp-server/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Simple Where Clause in Pixeltable\nDESCRIPTION: Filters movies with a budget over $200M using a basic where condition.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/datastore/filtering-and-selecting.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmovies.where(\n    movies.budget >= 200.0\n).collect()\n```\n\n----------------------------------------\n\nTITLE: Python Screenshot Request Model Definition\nDESCRIPTION: Pydantic model for handling screenshot requests with automatic request ID generation using timestamps and random bytes\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/ai-based-trading-insight-chrome-extension/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ScreenshotRequest(BaseModel):\n    screenshot: str\n    requestId: Optional[str] = None\n\n    @property\n    def get_request_id(self) -> str:\n        if not self.requestId:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            self.requestId = f\"auto_{timestamp}_{os.urandom(4).hex()}\"\n        return self.requestId\n```\n\n----------------------------------------\n\nTITLE: Importing Pixeltable Modules\nDESCRIPTION: Standard import conventions for the Pixeltable library. Imports the main pixeltable module as pxt and pixeltable functions as pxtf for convenience.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/api/api-cheat-sheet.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\nimport pixeltable.functions as pxtf\n```\n\n----------------------------------------\n\nTITLE: Combining Similarity Search with Predicates in Pixeltable\nDESCRIPTION: Shows how to combine similarity search with additional predicates, in this case filtering out a specific image from the results.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nres = (\n    imgs.order_by(sim, asc=False)\n    .where(imgs.id != 6)  # Additional clause\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres\n```\n\n----------------------------------------\n\nTITLE: Resolving Poetry.lock Conflicts\nDESCRIPTION: Commands for resolving merge conflicts in poetry.lock file.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout --theirs poetry.lock\npoetry lock --no-update\ngit add poetry.lock\n```\n\n----------------------------------------\n\nTITLE: Git Commands for Branch Creation and Management\nDESCRIPTION: Series of git commands for creating and managing a feature branch for development.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout main\ngit pull home main\ngit checkout -b my-branch\n```\n\n----------------------------------------\n\nTITLE: Cloning Pixeltable Repository with GitHub CLI\nDESCRIPTION: Command to clone the Pixeltable repository using the GitHub CLI. This is the first step in the process of reviewing and updating the documentation.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngh repo clone pixeltable/pixeltable\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installs the necessary Python packages including pixeltable, label-studio, label-studio-sdk, torch and transformers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable label-studio label-studio-sdk torch transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable Development Environment\nDESCRIPTION: Commands for cloning the Pixeltable repository and installing dependencies.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/my-username/pixeltable\ncd pixeltable\nmake install\nmake test\n```\n\n----------------------------------------\n\nTITLE: Navigating to Pixeltable Documentation Directory\nDESCRIPTION: Command to change the current directory to the Pixeltable documentation folder within the cloned repository.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd pixeltable/docs/mintlify\n```\n\n----------------------------------------\n\nTITLE: Visualizing Application Architecture with Mermaid Diagram\nDESCRIPTION: A sequence diagram showing the interaction flow between User, NextJS frontend, FastAPI backend, and Pixeltable. It illustrates how user interactions are processed through the application stack and how data flows between components.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/sample-apps/multimodal-chat/README.md#2025-04-07_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant User\n    participant NextJS\n    participant FastAPI\n    participant Pixeltable\n\n    User->>NextJS: Interact with UI (upload files, chat)\n    NextJS->>FastAPI: Make API requests (upload files, get chat response)\n    FastAPI->>Pixeltable: Process files, search documents, generate chat responses\n    Pixeltable-->>FastAPI: Return processed data, search results, chat responses\n    FastAPI-->>NextJS: Respond to API requests\n    NextJS-->>User: Update UI with results\n\n    Note right of Pixeltable: Storing and indexing documents/videos, chunking and embedding text, semantic search, prompting and response generation\n    Note right of FastAPI: File uploads and URL processing, API routing and request handling, integrating with Pixeltable\n    Note right of NextJS: Rendering the user interface, handling user interactions, making API calls to FastAPI, updating the UI with results\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Fireworks Integration\nDESCRIPTION: Installation of necessary Python packages pixeltable and fireworks-ai using pip.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-fireworks.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable fireworks-ai\n```\n\n----------------------------------------\n\nTITLE: ETF Data Records in CSV Format\nDESCRIPTION: CSV data containing ETF details including ticker symbols, fund names, types, leverage information, performance metrics, assets under management, and management style. Each row represents a unique ETF with standardized metrics.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/etf_screener.txt#2025-04-07_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\nPSFJ,No,Pacer Swan SOS Flex (July) ETF,ETF,\"1x, Not Leveraged, Not Inverse\",1.41804,12.53234,12.11202,,,5,0.61,0.00%,,-4.13%,Open Ended Investment Company,28.4766,Nontraditional Equity,$27.4M,Actively Managed,06/30/2021,\"Pacer Advisors, INC.\",8,,,,346,-0.94061,-3.9931,-3.52858,6.29474,-2.63169,0.76444\n```\n\n----------------------------------------\n\nTITLE: Creating and Updating Pull Request\nDESCRIPTION: Git commands for pushing changes and creating a pull request.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout my-branch\ngit push -u origin my-branch\n```\n\n----------------------------------------\n\nTITLE: Installing Pixeltable with llama.cpp Support\nDESCRIPTION: Installs Pixeltable with llama.cpp and related dependencies needed for running local LLMs.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/working-with-llama-cpp.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU pixeltable llama-cpp-python huggingface-hub\n```\n\n----------------------------------------\n\nTITLE: Initializing Pixeltable Table\nDESCRIPTION: Sets up a test environment by creating a directory and table in Pixeltable. Creates a table with an integer column and inserts a single row.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/notebook-test.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\npxt.drop_dir('tests', force=True)\npxt.create_dir('tests')\nt = pxt.create_table('tests.notebook_test_tbl', {'int_col': pxt.Int})\nt.insert(int_col=1)\n```\n\n----------------------------------------\n\nTITLE: Syncing Branch with Main\nDESCRIPTION: Commands for synchronizing a feature branch with the main branch and resolving conflicts.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout main\ngit pull home main\ngit checkout my-branch\ngit merge main\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pixeltable and Voxel51\nDESCRIPTION: Installs the required Python packages for working with Pixeltable, FiftyOne, and Transformers.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/mintlify/docs/examples/vision/voxel51.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pixeltable fiftyone transformers\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LayoutParser for Layout Detection in Python\nDESCRIPTION: This snippet demonstrates how to use LayoutParser to perform layout detection on a document image. It loads an image, initializes a pre-trained Detectron2 model, and detects the layout in just four lines of code.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/layout-parser-paper.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport layoutparser as lp\nimage = cv2.imread(\"image_file\") # load images\nmodel = lp.Detectron2LayoutModel(\n\"lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config\")\nlayout = model.detect(image)\n```\n\n----------------------------------------\n\nTITLE: Populating Video Data\nDESCRIPTION: Inserts sample video data from Multimedia Commons archive into the created table.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/integrations/using-label-studio-with-pixeltable.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nurl_prefix = 'http://multimedia-commons.s3-website-us-west-2.amazonaws.com/data/videos/mp4/'\nfiles = [\n    '122/8ff/1228ff94bf742242ee7c88e4769ad5d5.mp4',\n    '2cf/a20/2cfa205eae979b31b1144abd9fa4e521.mp4',\n    'ffe/ff3/ffeff3c6bf57504e7a6cecaff6aefbc9.mp4',\n]\ntoday = datetime(2024, 4, 22)\nvideos_table.insert({'video': url_prefix + file, 'date': today} for file in files)\n```\n\n----------------------------------------\n\nTITLE: ETF Performance and Characteristics Data\nDESCRIPTION: CSV formatted ETF data containing ticker symbols, fund names, categories, performance metrics, expense ratios, fund size, and management information. Each row represents a unique ETF with standardized metrics.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/etf_screener.txt#2025-04-07_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nHAWX,No,iShares Currency Hedged MSCI ACWI ex U.S. ETF,ETF,\"1x, Not Leveraged, Not Inverse\",5.02949,15.59193,10.82365,11.2403,,5,0.35,3.15%,67.1719,2.46%,Open Ended Investment Company,33.4408,International Equity,$248.7M,Passively Managed,06/29/2015,BlackRock Fund Advisors,1283,-55.68209,-0.46611,2.98578,238,-1.41608,-0.68756,3.9202,6.947,4.30736,-1.05957\nDIVS,No,SmartETFs Dividend Builder ETF,ETF,\"1x, Not Leveraged, Not Inverse\",4.98171,15.17692,10.06715,7.34138,4.6293,5,0.65,2.51%,26.4414,10.41%,Open Ended Investment Company,29.2007,International Equity,$40.3M,Actively Managed,03/30/2012,\"Guinness Atkinson Asset Management, Inc.\",36,-76.64731,0.29194,5.68249,348,-1.40999,-2.44787,1.2213,6.4793,2.41189,0.42399\n[...additional rows truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Directories in Pixeltable\nDESCRIPTION: Demonstrates how to delete an existing directory and create a new one in Pixeltable for organizing tables.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/fundamentals/tables-and-data-operations.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pixeltable as pxt\n\n# First we delete the `fundamentals` directory and all its contents (if\n# it exists), in order to ensure a clean environment for the tutorial.\npxt.drop_dir('fundamentals', force=True)\n\n# Now we create the directory.\npxt.create_dir('fundamentals')\n```\n\n----------------------------------------\n\nTITLE: ETF Performance Data in CSV Format\nDESCRIPTION: Structured data containing ETF information including ticker symbols, fund names, expense ratios, leverage details, yields, and performance metrics. The data includes both passive and actively managed funds across various asset classes.\nSOURCE: https://github.com/pixeltable/pixeltable/blob/main/tests/data/documents/etf_screener.txt#2025-04-07_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nIEI,No,iShares 3-7 Year Treasury Bond ETF,ETF,\"1x, Not Leveraged, Not Inverse\",2.22848,5.21305,0.07934,-0.24349,1.25631,5,0.15,3.16%,,4.57%,Open Ended Investment Company,118.135,Taxable Bond,$15.8B,Passively Managed,01/05/2007,BlackRock Fund Advisors,103,-16.79865,0.12289,1.82415,334,0.19531,0.16979,2.51086,1.88239,2.11164,0.1503\nPKW,No,Invesco BuyBack Achievers ETF,ETF,\"1x, Not Leveraged, Not Inverse\",2.2062,15.90878,10.44667,16.04064,10.35111,5,0.61,0.84%,40.51,16.26%,Open Ended Investment Company,112.665,U.S. Equity,$1.3B,Passively Managed,12/20/2006,Invesco Capital Management LLC,204,41.21544,0.6207,10.09255,347,-1.56484,-4.84406,-3.63198,3.4556,-2.74472,0.67696\n```"
  }
]
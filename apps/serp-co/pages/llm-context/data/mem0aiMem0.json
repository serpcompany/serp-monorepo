[
  {
    "owner": "mem0ai",
    "repo": "mem0",
    "content": "TITLE: Configuring and Using Weaviate Vector Store with mem0 in Python\nDESCRIPTION: This Python snippet demonstrates setting up the mem0 Memory module to use Weaviate as its vector store, including authentication and collection naming. It shows how to set your OpenAI API key via environment variable, define a configuration dict for Weaviate with required connection details, and add a list of user/assistant messages to the memory store with optional user-specific metadata. Dependencies: 'mem0' Python library, 'weaviate' server running at the specified URL, and a valid OpenAI API key. The 'add' method stores conversational history for subsequent retrieval and search.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/weaviate.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"weaviate\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"cluster_url\": \"http://localhost:8080\",\n            \"auth_client_secret\": None,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movie? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I’m not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Searching Memories with Filters in Mem0 (JavaScript)\nDESCRIPTION: This snippet demonstrates how to search memories in Mem0 using the JavaScript client, filtering by categories and metadata. It defines a query and uses the `client.search` method to retrieve memories that match the specified categories and metadata. The function uses a promise to handle the asynchronous API call and logs the search results or any errors to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_23\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \"What do you know about me?\";\nclient.search(query, categories=[\"food_preferences\"], metadata={\"food\": \"vegan\"})\n    .then(results => console.log(results))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Enriching Chat Context with Semantic Memory Using Mem0 (Python)\nDESCRIPTION: This async Python function stores the latest user message in Mem0, retrieves contextually relevant past memories using a semantic search, and augments the chat context with these retrieved memories. It enables the assistant to provide more relevant and personalized responses. Key parameters include the agent, chat context, and global Mem0 client. Requires Mem0 and LiveKit SDKs, and assumes logging is set up. Inputs are the voice agent and message context; output is a modified chat context prior to LLM processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def _enrich_with_memory(agent: VoicePipelineAgent, chat_ctx: llm.ChatContext):\n    \"\"\"Add memories and Augment chat context with relevant memories\"\"\"\n    if not chat_ctx.messages:\n        return\n    \n    # Store user message in Mem0\n    user_msg = chat_ctx.messages[-1]\n    await mem0.add(\n        [{\"role\": \"user\", \"content\": user_msg.content}], \n        user_id=USER_ID\n    )\n    \n    # Search for relevant memories\n    results = await mem0.search(\n        user_msg.content, \n        user_id=USER_ID,\n    )\n    \n    # Augment context with retrieved memories\n    if results:\n        memories = ' '.join([result[\"memory\"] for result in results])\n        logger.info(f\"Enriching with memory: {memories}\")\n        \n        rag_msg = llm.ChatMessage.create(\n            text=f\"Relevant Memory: {memories}\\n\",\n            role=\"assistant\",\n        )\n        \n        # Modify chat context with retrieved memories\n        chat_ctx.messages[-1] = rag_msg\n        chat_ctx.messages.append(user_msg)\n```\n\n----------------------------------------\n\nTITLE: Streaming AI Text Responses with Memory Context (TypeScript)\nDESCRIPTION: Exhibits streaming inference using streamText combined with Mem0 memory context. Imports streamText and createMem0. The model is configured with user_id and supports streaming via for-await loop over textStream. Inputs: prompt, outputs: streamed text. Dependencies include process.stdout for output. Useful for real-time applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { textStream } = await streamText({\n    model: mem0(\"gpt-4-turbo\", {\n        user_id: \"borat\",\n    }),\n    prompt: \"Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.\",\n});\n\nfor await (const textPart of textStream) {\n    process.stdout.write(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Customer Support AI Agent with Mem0 and OpenAI in Python\nDESCRIPTION: This code snippet demonstrates the full implementation of a Customer Support AI Agent using Mem0 for memory management and OpenAI for natural language processing. It includes methods for handling queries and retrieving memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/customer-support-agent.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom mem0 import Memory\n\n# Set the OpenAI API key\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\nclass CustomerSupportAIAgent:\n    def __init__(self):\n        \"\"\"\n        Initialize the CustomerSupportAIAgent with memory configuration and OpenAI client.\n        \"\"\"\n        config = {\n            \"vector_store\": {\n                \"provider\": \"qdrant\",\n                \"config\": {\n                    \"host\": \"localhost\",\n                    \"port\": 6333,\n                }\n            },\n        }\n        self.memory = Memory.from_config(config)\n        self.client = OpenAI()\n        self.app_id = \"customer-support\"\n\n    def handle_query(self, query, user_id=None):\n        \"\"\"\n        Handle a customer query and store the relevant information in memory.\n\n        :param query: The customer query to handle.\n        :param user_id: Optional user ID to associate with the memory.\n        \"\"\"\n        # Start a streaming chat completion request to the AI\n        stream = self.client.chat.completions.create(\n            model=\"gpt-4\",\n            stream=True,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a customer support AI agent.\"},\n                {\"role\": \"user\", \"content\": query}\n            ]\n        )\n        # Store the query in memory\n        self.memory.add(query, user_id=user_id, metadata={\"app_id\": self.app_id})\n\n        # Print the response from the AI in real-time\n        for chunk in stream:\n            if chunk.choices[0].delta.content is not None:\n                print(chunk.choices[0].delta.content, end=\"\")\n\n    def get_memories(self, user_id=None):\n        \"\"\"\n        Retrieve all memories associated with the given customer ID.\n\n        :param user_id: Optional user ID to filter memories.\n        :return: List of memories.\n        \"\"\"\n        return self.memory.get_all(user_id=user_id)\n\n# Instantiate the CustomerSupportAIAgent\nsupport_agent = CustomerSupportAIAgent()\n\n# Define a customer ID\ncustomer_id = \"jane_doe\"\n\n# Handle a customer query\nsupport_agent.handle_query(\"I need help with my recent order. It hasn't arrived yet.\", user_id=customer_id)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory Sources with generateText (TypeScript)\nDESCRIPTION: Explains how to access the 'sources' field returned alongside generated text when using generateText with Mem0. Model and prompt given; output includes both the response and referenced memory sources. Enables transparency and traceability for LLM completions utilizing persistent memory. No additional configuration required.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { text, sources } = await generateText({\n    model: mem0(\"gpt-4-turbo\"),\n    prompt: \"Suggest me a good car to buy!\",\n});\n\nconsole.log(sources);\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Search Function Tool\nDESCRIPTION: Defines a function tool for searching relevant memories using Mem0. It takes a query string, uses Mem0's semantic search, and returns formatted results.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def search_memories(\n    query: str\n) -> str:\n    \"\"\"\n    Find memories relevant to the current conversation.\n    Args:\n        query: The search query to find relevant memories\n    \"\"\"\n    print(f\"Finding memories related to: {query}\")\n    results = await mem0_client.search(\n        query,\n        user_id=USER_ID,\n        limit=5,\n        threshold=0.7,  # Higher threshold for more relevant results\n        output_format=\"v1.1\"\n    )\n    \n    # Format and return the results\n    if not results.get('results', []):\n        return \"I don't have any relevant memories about this topic.\"\n    \n    memories = [f\"• {result['memory']}\" for result in results.get('results', [])]\n    return \"Here's what I remember that might be relevant:\\n\" + \"\\n\".join(memories)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with pip - Bash\nDESCRIPTION: This snippet installs the dependencies needed to build the Travel Agent AI using Python, including LangChain for managing conversation flows, langchain_openai for OpenAI integration, and mem0ai as the memory backend. Users must run this in their shell before proceeding. No parameters required. It outputs the latest stable versions of the listed packages.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain langchain_openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Deleting All Memories (Python and TypeScript)\nDESCRIPTION: These snippets demonstrate deletion of all graph memories for a given user ('alice'), in both Python and TypeScript. The 'delete_all' (Python) and 'deleteAll' (TypeScript) methods require a user identifier. The backend will clear all associated memory nodes and relationships for that user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nm.delete_all(user_id=\"alice\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.deleteAll({ userId: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memories with Custom Filters - Python\nDESCRIPTION: This Python snippet demonstrates how to retrieve memories using custom filters with logical operators (AND) and comparison operators. It filters by user_id, created_at (date range), and categories (contains).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\nfilters = {\n   \"AND\":[\n      {\n         \"user_id\":\"alex\"\n      },\n      {\n         \"created_at\":{\n            \"gte\":\"2024-07-01\",\n            \"lte\":\"2024-07-31\"\n         }\n      },\n      {\n         \"categories\":{\n            \"contains\": \"food_preferences\"\n         }\n      }\n   ]\n}\n\n# Default (No Pagination)\nclient.get_all(version=\"v2\", filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Complete Document Editing Workflow with Mem0 in Python\nDESCRIPTION: This function implements a complete workflow for editing a document based on writing preferences stored in Mem0. It stores preferences if not already stored, edits the document, and displays the original and edited versions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/document-writing.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef document_editing_workflow(content):\n    \"\"\"Automated workflow for editing a document based on writing preferences.\"\"\"\n    \n    # Step 1: Store writing preferences (if not already stored)\n    store_writing_preferences()\n    \n    # Step 2: Edit the document with Mem0 preferences\n    edited_content = edit_document_based_on_preferences(content)\n    \n    if not edited_content:\n        return \"Failed to edit document.\"\n    \n    # Step 3: Display results\n    print(\"\\n=== ORIGINAL DOCUMENT ===\\n\")\n    print(content)\n    \n    print(\"\\n=== EDITED DOCUMENT ===\\n\")\n    print(edited_content)\n    \n    return edited_content\n```\n\n----------------------------------------\n\nTITLE: Implementing the Chatbot Logic with Memory Retrieval in Python\nDESCRIPTION: Defines the `chatbot` function, the core logic node for the LangGraph agent. It extracts the latest message and user ID from the input state, queries Mem0 using `mem0.search` to find relevant past memories, constructs a system prompt containing this context, invokes the OpenAI LLM with the full message history and context, stores the new user-assistant interaction pair in Mem0 using `mem0.add`, and returns the AI's response message to update the state.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chatbot(state: State):\n    messages = state[\"messages\"]\n    user_id = state[\"mem0_user_id\"]\n\n    # Retrieve relevant memories\n    memories = mem0.search(messages[-1].content, user_id=user_id)\n\n    context = \"Relevant information from previous conversations:\\n\"\n    for memory in memories:\n        context += f\"- {memory['memory']}\\n\"\n\n    system_message = SystemMessage(content=f\"\"\"You are a helpful customer support assistant. Use the provided context to personalize your responses and remember user preferences and past interactions.\n{context}\"\"\")\n\n    full_messages = [system_message] + messages\n    response = llm.invoke(full_messages)\n\n    # Store the interaction in Mem0\n    mem0.add(f\"User: {messages[-1].content}\\nAssistant: {response.content}\", user_id=user_id)\n    return {\"messages\": [response]}\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Companion Class with Memory and OpenAI Integration (Python)\nDESCRIPTION: Defines the `Companion` class in Python, which integrates `mem0ai` for persistent memory and `openai` for conversational AI. It initializes separate memories for the user and companion using a Qdrant vector store, analyzes questions to determine context, retrieves relevant memories, generates responses using GPT-4, and stores interactions. The example usage demonstrates initializing the companion and asking a question. Dependencies include `openai`, `mem0`, `os`, an OpenAI API key (set via environment variable `OPENAI_API_KEY`), and a running Qdrant instance on localhost:6333.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom mem0 import Memory\nimport os\n\n# Set the OpenAI API key\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\nclass Companion:\n    def __init__(self, user_id, companion_id):\n        \"\"\"\n        Initialize the Companion with memory configuration, OpenAI client, and user IDs.\n        :param user_id: ID for storing user-related memories\n        :param companion_id: ID for storing companion-related memories\n        \"\"\"\n        config = {\n            \"vector_store\": {\n                \"provider\": \"qdrant\",\n                \"config\": {\n                    \"host\": \"localhost\",\n                    \"port\": 6333,\n                }\n            },\n        }\n        self.memory = Memory.from_config(config)\n        self.client = client\n        self.app_id = \"app-1\"\n        self.USER_ID = user_id\n        self.companion_id = companion_id\n\n    def analyze_question(self, question):\n        \"\"\"\n        Analyze the question to determine whether it's about the user or the companion.\n        \"\"\"\n        check_prompt = f\"\"\"\n        Analyze the given input and determine whether the user is primarily:\n        1) Talking about themselves or asking for personal advice. They may use words like \\\"I\\\" for this. \n        2) Inquiring about the AI companions's capabilities or characteristics They may use words like \\\"you\\\" for this. \n\n        Respond with a single word:\n        - 'user' if the input is focused on the user\n        - 'companion' if the input is focused on the AI companion\n\n        If the input is ambiguous or doesn't clearly fit either category, respond with 'user'.\n\n        Input: {question}\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": check_prompt}]\n        )\n        return response.choices[0].message.content\n\n    def ask(self, question):\n        \"\"\"\n        Ask a question to the AI and store the relevant facts in memory\n        :param question: The question to ask the AI.\n        \"\"\"\n        check_answer = self.analyze_question(question)\n        user_id_to_use = self.USER_ID if check_answer == \"user\" else self.companion_id\n\n        previous_memories = self.memory.search(question, user_id=user_id_to_use)\n        relevant_memories_text = \"\"\n        if previous_memories:\n            relevant_memories_text = '\\n'.join(mem[\"memory\"] for mem in previous_memories)\n\n        prompt = f\"User input: {question}\\nPrevious {check_answer} memories: {relevant_memories_text}\"\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are the user's romantic companion. Use the user's input and previous memories to respond. Answer based on the context provided.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n\n        stream = self.client.chat.completions.create(\n            model=\"gpt-4\",\n            stream=True,\n            messages=messages\n        )\n\n        answer = \"\"\n        for chunk in stream:\n            if chunk.choices[0].delta.content is not None:\n                content = chunk.choices[0].delta.content\n                print(content, end=\"\")\n                answer += content\n        # Store the question and answer in memory\n        self.memory.add(question, user_id=self.USER_ID, metadata={\"app_id\": self.app_id})\n        self.memory.add(answer, user_id=self.companion_id, metadata={\"app_id\": self.app_id})\n\n    def get_memories(self, user_id=None):\n        \"\"\"\n        Retrieve all memories associated with the given user ID.\n        :param user_id: Optional user ID to filter memories.\n        :return: List of memories.\n        \"\"\"\n        return self.memory.get_all(user_id=user_id)\n\n# Example usage:\nuser_id = \"user\"\ncompanion_id = \"companion\"\nai_companion = Companion(user_id, companion_id)\n\n# Ask a question\nai_companion.ask(\"Ive been missing you. What have you been up to off late?\")\n```\n\n----------------------------------------\n\nTITLE: Composing OpenAI-Compatible Text and Image Messages in Mem0 (TypeScript)\nDESCRIPTION: Shows how to construct a Mem0 message containing a text prompt and an inline Base64-encoded image, utilizing the OpenAI-compatible array format. Involves reading a local file with Node's 'fs' and encoding it in Base64. The message, an array of content objects, is then added to memory for processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Message } from \"mem0ai/oss\";\n\nconst client = new Memory();\n\nconst imagePath = \"path/to/your/image.jpg\";\n\nconst base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });\n\nconst message: Message = {\n    role: \"user\",\n    content: [\n        {\n            type: \"text\",\n            text: \"What is in this image?\",\n        },\n        {\n            type: \"image_url\",\n            image_url: {\n                url: `data:image/jpeg;base64,${base64Image}`\n            }\n        },\n    ],\n}\n\nawait client.add([message], { userId: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Creating Memory-Enabled Voice Agent - Python\nDESCRIPTION: The `create_memory_voice_agent` function initializes an Agent instance named 'Memory Assistant' with specialized instructions for polite, memory-enabled dialogue. It includes the addition of the `save_memories` and `search_memories` tools, configures its language model to 'gpt-4o', and provides necessary instruction context through `prompt_with_handoff_instructions`. The agent is returned for use in voice-driven workflows. Prerequisites are the Agent class, function_tool tools, and a valid instruction function.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef create_memory_voice_agent():\n    # Create the agent with memory-enabled tools\n    agent = Agent(\n        name=\"Memory Assistant\",\n        instructions=prompt_with_handoff_instructions(\n            \"\"\"You're speaking to a human, so be polite and concise.\n            Always respond in clear, natural English.\n            You have the ability to remember information about the user.\n            Use the save_memories tool when the user shares an important information worth remembering.\n            Use the search_memories tool when you need context from past conversations or user asks you to recall something.\n            \"\"\",\n        ),\n        model=\"gpt-4o\",\n        tools=[save_memories, search_memories],\n    )\n    \n    return agent\n\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys and Initializing Clients (Python)\nDESCRIPTION: Sets up environment variables for Mem0, OpenAI, and Serper Dev API keys, followed by instantiation of the Mem0 MemoryClient. These settings are crucial for authenticating with external services used throughout the program. All downstream functionality that interacts with memory retrieval or search will require these configurations and the imported modules.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import SerperDevTool\n\n# Configuration\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\nos.environ[\"SERPER_API_KEY\"] = \"your-serper-api-key\"\n\n# Initialize Mem0 client\nclient = MemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Setting up Azure OpenAI with Mem0 in Python\nDESCRIPTION: This snippet demonstrates how to configure and use Azure OpenAI with Mem0 in Python. It shows environment variable setup, Memory object configuration with Azure OpenAI provider, and adding conversation history to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/azure_openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"LLM_AZURE_OPENAI_API_KEY\"] = \"your-api-key\"\nos.environ[\"LLM_AZURE_DEPLOYMENT\"] = \"your-deployment-name\"\nos.environ[\"LLM_AZURE_ENDPOINT\"] = \"your-api-base-url\"\nos.environ[\"LLM_AZURE_API_VERSION\"] = \"version-to-use\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"azure_openai\",\n        \"config\": {\n            \"model\": \"your-deployment-name\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n            \"azure_kwargs\": {\n                  \"azure_deployment\": \"\",\n                  \"api_version\": \"\",\n                  \"azure_endpoint\": \"\",\n                  \"api_key\": \"\",\n                  \"default_headers\": {\n                    \"CustomHeader\": \"your-custom-header\",\n                  }\n              }\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Set Expiration Date in Mem0 - Python\nDESCRIPTION: This code snippet demonstrates how to add a memory to Mem0 with an expiration date using the Python client. It calculates a date 30 days from the current date and also shows how to use an explicit date string. Requires the 'mem0' package and datetime library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/expiration-date.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport datetime\nfrom mem0 import MemoryClient\n\nclient = MemoryClient(api_key=\"your-api-key\")\n\nmessages = [\n    {\n        \"role\": \"user\", \n        \"content\": \"I'll be in San Francisco until end of this month.\"\n    }\n]\n\n# Set an expiration date for this memory\nclient.add(messages=messages, user_id=\"alex\", expiration_date=str(datetime.datetime.now().date() + datetime.timedelta(days=30)))\n\n# You can also use an explicit date string\nclient.add(messages=messages, user_id=\"alex\", expiration_date=\"2023-08-31\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Complete Car Recommendation System with Mem0 and OpenAI\nDESCRIPTION: This comprehensive example demonstrates a complete car recommendation system using Mem0 for memory management and OpenAI for generating recommendations. It includes functions for adding sample memories, generating responses with and without memories, and using web search.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport MemoryClient from \"mem0ai\";\nimport { OpenAI } from \"openai\";\nimport { zodResponsesFunction } from \"openai/helpers/zod\";\nimport { z } from \"zod\";\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\nconst mem0Config = {\n    apiKey: process.env.MEM0_API_KEY,\n    user_id: \"sample-user\",\n};\n\nasync function run() {\n    // Responses without memories\n    console.log(\"\\n\\nRESPONSES WITHOUT MEMORIES\\n\\n\");\n    await main();\n\n    // Adding sample memories\n    await addSampleMemories();\n\n    // Responses with memories\n    console.log(\"\\n\\nRESPONSES WITH MEMORIES\\n\\n\");\n    await main(true);\n}\n\n// OpenAI Response Schema\nconst CarSchema = z.object({\n  car_name: z.string(),\n  car_price: z.string(),\n  car_url: z.string(),\n  car_image: z.string(),\n  car_description: z.string(),\n});\n\nconst Cars = z.object({\n  cars: z.array(CarSchema),\n});\n\nasync function main(memory = false) {\n  const openAIClient = new OpenAI();\n  const mem0Client = new MemoryClient(mem0Config);\n\n  const input = \"Suggest me some cars that I can buy today.\";\n\n  const tool = zodResponsesFunction({ name: \"carRecommendations\", parameters: Cars });\n\n  // Store the user input as a memory\n  await mem0Client.add([{\n    role: \"user\",\n    content: input,\n  }], mem0Config);\n\n  // Search for relevant memories\n  let relevantMemories = []\n  if (memory) {\n    relevantMemories = await mem0Client.search(input, mem0Config);\n  }\n\n  const response = await openAIClient.responses.create({\n    model: \"gpt-4o\",\n    tools: [{ type: \"web_search_preview\" }, tool],\n    input: `${getMemoryString(relevantMemories)}\\n${input}`,\n  });\n\n  console.log(response.output);\n}\n\nasync function addSampleMemories() {\n  const mem0Client = new MemoryClient(mem0Config);\n\n  const myInterests = \"I Love BMW, Audi and Porsche. I Hate Mercedes. I love Red cars and Maroon cars. I have a budget of 120K to 150K USD. I like Audi the most.\";\n  \n  await mem0Client.add([{\n    role: \"user\",\n    content: myInterests,\n  }], mem0Config);\n}\n\nconst getMemoryString = (memories) => {\n    const MEMORY_STRING_PREFIX = \"These are the memories I have stored. Give more weightage to the question by users and try to answer that first. You have to modify your answer based on the memories I have provided. If the memories are irrelevant you can ignore them. Also don't reply to this section of the prompt, or the memories, they are only for your reference. The MEMORIES of the USER are: \\n\\n\";\n    const memoryString = memories.map((mem) => `${mem.memory}`).join(\"\\n\") ?? \"\";\n    return memoryString.length > 0 ? `${MEMORY_STRING_PREFIX}${memoryString}` : \"\";\n};\n\nrun().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Main Interaction Loop for Travel Agent AI - Python\nDESCRIPTION: Provides the entry point for terminal-based user interaction with the AI travel agent. Greets the user, continually prompts for queries, delegates each input to the 'chat_turn' function, and offers a clean exit on user command. Requires all previous code to be set up, as well as 'chat_turn' being implemented. Input and output are via stdin/stdout, and exit phrases ('quit', 'exit', 'bye') are hardcoded. Intended for interactive Python execution.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    print(\"Welcome to your personal Travel Agent Planner! How can I assist you with your travel plans today?\")\n    user_id = \"john\"\n    \n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in ['quit', 'exit', 'bye']:\n            print(\"Travel Agent: Thank you for using our travel planning service. Have a great trip!\")\n            break\n        \n        response = chat_turn(user_input, user_id)\n        print(f\"Travel Agent: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Combining OpenAI Model with Retrieved Memories as System Prompt (TypeScript)\nDESCRIPTION: Illustrates integration of OpenAI provider with memory retrieval utility to inject context into generative calls. Imports generateText, openai provider, and retrieveMemories from relevant packages. Parameters include model, prompt, and system prompt (retrieved memories). Outputs generated text. Requires user_id and proper API setup.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { retrieveMemories } from \"@mem0/vercel-ai-provider\";\n\nconst prompt = \"Suggest me a good car to buy.\";\nconst memories = await retrieveMemories(prompt, { user_id: \"borat\" });\n\nconst { text } = await generateText({\n  model: openai(\"gpt-4-turbo\"),\n  prompt: prompt,\n  system: memories,\n});\n```\n\n----------------------------------------\n\nTITLE: Concurrent Async Usage of AsyncMemory with OpenAI API - Python\nDESCRIPTION: This full program example demonstrates the integration of AsyncMemory with the OpenAI AsyncOpenAI API in a concurrent, event-driven chat scenario. The program retrieves relevant memories, constructs a system prompt, sends messages to the OpenAI chat API, stores new conversation turns back into memory, and runs an async input/output loop. Dependencies include openai, mem0, and asyncio. Parameters of interest are the user message, user_id, and the async context. Suitable for non-blocking, scalable conversational agents.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\\nfrom openai import AsyncOpenAI\\nfrom mem0 import AsyncMemory\\n\\nasync_openai_client = AsyncOpenAI()\\nasync_memory = AsyncMemory()\\n\\nasync def chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\\n    # Retrieve relevant memories\\n    search_result = await async_memory.search(query=message, user_id=user_id, limit=3)\\n    relevant_memories = search_result[\"results\"]\\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories)\\n    \\n    # Generate Assistant response\\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\\n    response = await async_openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\\n    assistant_response = response.choices[0].message.content\\n\\n    # Create new memories from the conversation\\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\\n    await async_memory.add(messages, user_id=user_id)\\n\\n    return assistant_response\\n\\nasync def async_main():\\n    print(\"Chat with AI (type 'exit' to quit)\")\\n    while True:\\n        user_input = input(\"You: \").strip()\\n        if user_input.lower() == 'exit':\\n            print(\"Goodbye!\")\\n            break\\n        response = await chat_with_memories(user_input)\\n        print(f\"AI: {response}\")\\n\\ndef main():\\n    asyncio.run(async_main())\\n\\nif __name__ == \"__main__\":\\n    main()\n```\n\n----------------------------------------\n\nTITLE: Implementing Email Processing Logic with Mem0 (Python)\nDESCRIPTION: This Python script defines an `EmailProcessor` class that uses the `mem0ai` library to interact with Mem0. It includes methods to: parse raw email content (`process_email`), extract the body (`_get_email_body`), store emails as memories with metadata and categories using `client.add()`, search stored emails using `client.search()`, and retrieve entire email threads based on filters using `client.get_all()`. The script requires setting the `MEM0_API_KEY` environment variable and demonstrates processing a sample email and performing a search. Note the simplified body extraction may need enhancement for complex multipart emails.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/email_processing.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\nfrom email.parser import Parser\n\n# Configure API keys\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\n\n# Initialize Mem0 client\nclient = MemoryClient()\n\nclass EmailProcessor:\n    def __init__(self):\n        \"\"\"Initialize the Email Processor with Mem0 memory client\"\"\"\n        self.client = client\n        \n    def process_email(self, email_content, user_id):\n        \"\"\"\n        Process an email and store it in Mem0 memory\n        \n        Args:\n            email_content (str): Raw email content\n            user_id (str): User identifier for memory association\n        \"\"\"\n        # Parse email\n        parser = Parser()\n        email = parser.parsestr(email_content)\n        \n        # Extract email details\n        sender = email['from']\n        recipient = email['to']\n        subject = email['subject']\n        date = email['date']\n        body = self._get_email_body(email)\n        \n        # Create message object for Mem0\n        message = {\n            \"role\": \"user\",\n            \"content\": f\"Email from {sender}: {subject}\\n\\n{body}\"\n        }\n        \n        # Create metadata for better retrieval\n        metadata = {\n            \"email_type\": \"incoming\",\n            \"sender\": sender,\n            \"recipient\": recipient,\n            \"subject\": subject,\n            \"date\": date\n        }\n        \n        # Store in Mem0 with appropriate categories\n        response = self.client.add(\n            messages=[message],\n            user_id=user_id,\n            metadata=metadata,\n            categories=[\"email\", \"correspondence\"],\n            version=\"v2\"\n        )\n        \n        return response\n    \n    def _get_email_body(self, email):\n        \"\"\"Extract the body content from an email\"\"\"\n        # Simplified extraction - in real-world, handle multipart emails\n        if email.is_multipart():\n            for part in email.walk():\n                if part.get_content_type() == \"text/plain\":\n                    return part.get_payload(decode=True).decode()\n        else:\n            return email.get_payload(decode=True).decode()\n    \n    def search_emails(self, query, user_id):\n        \"\"\"\n        Search through stored emails\n        \n        Args:\n            query (str): Search query\n            user_id (str): User identifier\n        \"\"\"\n        # Search Mem0 for relevant emails\n        results = self.client.search(\n            query=query,\n            user_id=user_id,\n            categories=[\"email\"],\n            output_format=\"v1.1\",\n            version=\"v2\"\n        )\n        \n        return results\n        \n    def get_email_thread(self, subject, user_id):\n        \"\"\"\n        Retrieve all emails in a thread based on subject\n        \n        Args:\n            subject (str): Email subject to match\n            user_id (str): User identifier\n        \"\"\"\n        filters = {\n            \"AND\": [\n                {\"user_id\": user_id},\n                {\"categories\": {\"contains\": \"email\"}},\n                {\"metadata\": {\"subject\": {\"contains\": subject}}}\n            ]\n        }\n        \n        thread = self.client.get_all(\n            version=\"v2\",\n            filters=filters,\n            output_format=\"v1.1\"\n        )\n        \n        return thread\n\n# Initialize the processor\nprocessor = EmailProcessor()\n\n# Example raw email\nsample_email = \"\"\"From: alice@example.com\nTo: bob@example.com\nSubject: Meeting Schedule Update\nDate: Mon, 15 Jul 2024 14:22:05 -0700\n\nHi Bob,\n\nI wanted to update you on the schedule for our upcoming project meeting.\nWe'll be meeting this Thursday at 2pm instead of Friday.\n\nCould you please prepare your section of the presentation?\n\nThanks,\nAlice\n\"\"\"\n\n# Process and store the email\nuser_id = \"bob@example.com\"\nprocessor.process_email(sample_email, user_id)\n\n# Later, search for emails about meetings\nmeeting_emails = processor.search_emails(\"meeting schedule\", user_id)\nprint(f\"Found {len(meeting_emails['results'])} relevant emails\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Mem0 Platform Client in Python\nDESCRIPTION: This snippet demonstrates how to initialize the Mem0 client using a platform API key, store user preferences through a conversational interface, and later leverage those preferences for personalized responses. It showcases both the storing and retrieval of contextual memory by sending chat messages to Mem0's API using familiar OpenAI-like methods. Dependencies include the 'mem0' Python library and a valid API key. Parameters such as 'api_key', 'messages', 'model', and 'user_id' are required. Inputs are user chat messages; outputs are chat completions that incorporate stored user memory. This approach assumes access to the Mem0 cloud platform and Python 3.x environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/openai_compatibility.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0.proxy.main import Mem0\n\nclient = Mem0(api_key=\"m0-xxx\")\n\n# First interaction: Storing user preferences\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"I love indian food but I cannot eat pizza since allergic to cheese.\"\n    },\n]\nuser_id = \"alice\"\nchat_completion = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-4o-mini\",\n    user_id=user_id\n)\n# Memory saved after this will look like: \"Loves Indian food. Allergic to cheese and cannot eat pizza.\"\n\n# Second interaction: Leveraging stored memory\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Suggest restaurants in San Francisco to eat.\",\n    }\n]\n\nchat_completion = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-4o-mini\",\n    user_id=user_id\n)\nprint(chat_completion.choices[0].message.content)\n# Answer: You might enjoy Indian restaurants in San Francisco, such as Amber India, Dosa, or Curry Up Now, which offer delicious options without cheese.\n```\n\n----------------------------------------\n\nTITLE: Importing Modules and Initializing LangChain & Mem0 - Python\nDESCRIPTION: This snippet demonstrates importing necessary modules and initializing both LangChain and Mem0 clients for the AI agent. It configures API keys using environment variables, and creates ChatOpenAI and MemoryClient instances for model-driven interaction and persistent memory. Requires 'langchain_openai', 'langchain_core', and 'mem0ai' to be installed, and valid API keys for OpenAI and Mem0.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Dict\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom mem0 import MemoryClient\n\n# Configuration\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\n\n# Initialize LangChain and Mem0\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nmem0 = MemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Defining Memory Storage Tool with Mem0 - Python\nDESCRIPTION: This snippet defines `save_memories`, an async function decorated with `@function_tool`, which saves a user memory string to Mem0 using a provided `mem0_client.add` method. It prints (or logs) the memory being saved, creates a memory content string, and returns a confirmation message. Dependencies include Mem0 client, function_tool decorator, and an accessible USER_ID. The function takes a string parameter `memory` and returns a formatted string. Intended for use as a tool in agent workflows; returns a success response or logs the operation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n@function_tool\nasync def save_memories(\n    memory: str\n) -> str:\n    \"\"\"\n    Store a user memory in memory.\n    Args:\n        memory: The memory to save\n    \"\"\"\n    print(f\"Saving memory: {memory} for user {USER_ID}\")\n\n    # Store the preference in Mem0\n    memory_content = f\"User memory - {memory}\"\n    await mem0_client.add(\n        memory_content,\n        user_id=USER_ID,\n    )\n\n    return f\"I've saved your memory: {memory}\"\n\n```\n\n----------------------------------------\n\nTITLE: Main Function for Executing CrewAI Travel Planning (Python)\nDESCRIPTION: Implements the plan_trip() function that orchestrates agent, task, and crew creation, followed by launching the planning workflow using crew.kickoff(). Includes an example block for direct script execution. It accepts the destination and user_id, ties together all setup functions, and outputs the result of the planning process, assuming all prior setup and dependencies are initialized.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plan_trip(destination: str, user_id: str):\n    # Create agent\n    travel_agent = create_travel_agent()\n\n    # Create task\n    planning_task = create_planning_task(travel_agent, destination)\n\n    # Setup crew\n    crew = setup_crew([travel_agent], [planning_task])\n\n    # Execute and return results\n    return crew.kickoff()\n\n# Example usage\nif __name__ == \"__main__\":\n    result = plan_trip(\"San Francisco\", \"crew_user_1\")\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph Memory with Custom LLM - Advanced (Python)\nDESCRIPTION: This snippet shows advanced configuration for Mem0 in Python, setting global and graph-store-level LLMs. Both main and graph_store configuration dictionaries include 'llm' keys, allowing graph operations to use their own specified LLM settings. All Mem0 and LLM dependencies must be installed. Parameters include OpenAI model details, temperature, and tokens. The Memory instance uses graph_store/llm if specified, otherwise defaults to main llm.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    },\n    \"graph_store\": {\n        \"provider\": \"neo4j\",\n        \"config\": {\n            \"url\": \"neo4j+s://xxx\",\n            \"username\": \"neo4j\",\n            \"password\": \"xxx\"\n        },\n        \"llm\" : {\n            \"provider\": \"openai\",\n            \"config\": {\n                \"model\": \"gpt-4o-mini\",\n                \"temperature\": 0.0,\n            }\n        }\n    }\n}\n\nm = Memory.from_config(config_dict=config)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Async Memory Instance with Mem0 SDK in Python\nDESCRIPTION: Demonstrates setup for async workflows using AsyncMemory. Requires setting 'OPENAI_API_KEY' and importing AsyncMemory from mem0. Useful for environments requiring asynchronous operations or concurrency.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import AsyncMemory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nm = AsyncMemory()\n```\n\n----------------------------------------\n\nTITLE: Generating Tool-augmented Responses with Mem0 and Zod (TypeScript)\nDESCRIPTION: Details using generateText with Mem0 provider (configured for Anthropic) and tool function for calling structured tools in completion. Uses zod for parameter validation and randomized weather as tool output. Input parameters include model name, tools object, and prompt; outputs structured response with tool-augmented context. Requires dependencies: @mem0/vercel-ai-provider, ai, and zod.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\nimport { z } from \"zod\";\n\nconst mem0 = createMem0({\n  provider: \"anthropic\",\n  apiKey: \"anthropic-api-key\",\n  mem0Config: {\n    // Global User ID\n    user_id: \"borat\"\n  }\n});\n\nconst prompt = \"What the temperature in the city that I live in?\"\n\nconst result = await generateText({\n  model: mem0('claude-3-5-sonnet-20240620'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  prompt: prompt,\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys in Environment File (Shell)\nDESCRIPTION: This shell snippet provides the format for storing essential API credentials in a .env file. Variables include keys and URLs for LiveKit, Deepgram, Mem0, and OpenAI. Ensure this file is placed in the root of your project for dotenv compatibility, and never check sensitive keys into source control. Inputs are API secrets, and outputs are environment variable configurations for the application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nLIVEKIT_URL=your_livekit_url\nLIVEKIT_API_KEY=your_livekit_api_key\nLIVEKIT_API_SECRET=your_livekit_api_secret\nDEEPGRAM_API_KEY=your_deepgram_api_key\nMEM0_API_KEY=your_mem0_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 with LM Studio as LLM Provider\nDESCRIPTION: This snippet demonstrates how to configure Mem0 to use LM Studio as the LLM provider while still using OpenAI for embeddings. It includes setting up the environment, configuring the LLM parameters, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/lmstudio.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"lmstudio\",\n        \"config\": {\n            \"model\": \"lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF/Meta-Llama-3.1-70B-Instruct-IQ2_M.gguf\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n            \"lmstudio_base_url\": \"http://localhost:1234/v1\", # default LM Studio API URL\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Instantiating Mem0 Open Source Memory Class - Python\nDESCRIPTION: Shows initialization of the open-source Memory class from the mem0ai package for direct, self-hosted use. No API key is required, but the mem0ai package must be installed. Used to add and manage memories in a private deployment rather than the managed Mem0 cloud API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom mem0 import Memory\\nm = Memory()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory and Generating Context-Aware Responses with AutoGen and Mem0 in Python\nDESCRIPTION: This Python code defines a function `get_context_aware_response` that retrieves relevant memories from Mem0 based on a user's question using `memory_client.search()`. It then constructs a prompt including this retrieved context and the user's question. Finally, it uses the initialized AutoGen `agent` to generate a context-aware reply. An example usage demonstrates how to call this function.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/autogen.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_context_aware_response(question):\n    relevant_memories = memory_client.search(question, user_id=USER_ID)\n    context = \"\\n\".join([m[\"memory\"] for m in relevant_memories])\n\n    prompt = f\"\"\"Answer the user question considering the previous interactions:\n    Previous interactions:\n    {context}\n\n    Question: {question}\n    \"\"\"\n\n    reply = agent.generate_reply(messages=[{\"content\": prompt, \"role\": \"user\"}])\n    return reply\n\n# Example usage\nquestion = \"What was the issue with my TV?\"\nanswer = get_context_aware_response(question)\nprint(\"Context-aware answer:\", answer)\n```\n\n----------------------------------------\n\nTITLE: Complete Voice Agent Example with Mem0 Memory Integration (Python)\nDESCRIPTION: A thorough example of constructing a FastAPI-powered Pipecat voice agent using Mem0MemoryService. Covers the definition of WebSocket endpoints, setup for user/assistant context, speech-to-text, memory, LLM, and full pipeline assembly. Includes event handlers for client connection management and proper instantiation of each Pipecat and Mem0 component. This code demonstrates modular design for scalable memory-enabled conversational agents. Dependencies include FastAPI, Pipecat-core modules, and valid environment variables for Mem0 and OpenAI API keys.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nfrom fastapi import FastAPI, WebSocket\n\nfrom pipecat.frames.frames import TextFrame\nfrom pipecat.pipeline.pipeline import Pipeline\nfrom pipecat.pipeline.task import PipelineTask\nfrom pipecat.pipeline.runner import PipelineRunner\nfrom pipecat.services.mem0 import Mem0MemoryService\nfrom pipecat.services.openai import OpenAILLMService, OpenAIUserContextAggregator, OpenAIAssistantContextAggregator\nfrom pipecat.transports.network.fastapi_websocket import (\n    FastAPIWebsocketTransport,\n    FastAPIWebsocketParams\n)\nfrom pipecat.serializers.protobuf import ProtobufFrameSerializer\nfrom pipecat.audio.vad.silero import SileroVADAnalyzer\nfrom pipecat.services.whisper import WhisperSTTService\n\napp = FastAPI()\n\n@app.websocket(\"/chat\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    \n    # Basic setup with minimal configuration\n    user_id = \"user123\"\n    \n    # WebSocket transport\n    transport = FastAPIWebsocketTransport(\n        websocket=websocket,\n        params=FastAPIWebsocketParams(\n            audio_out_enabled=True,\n            vad_enabled=True,\n            vad_analyzer=SileroVADAnalyzer(),\n            vad_audio_passthrough=True,\n            serializer=ProtobufFrameSerializer(),\n        )\n    )\n    \n    # Core services\n    user_context = OpenAIUserContextAggregator()\n    assistant_context = OpenAIAssistantContextAggregator()\n    stt = WhisperSTTService(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    # Memory service - the key component\n    memory = Mem0MemoryService(\n        api_key=os.getenv(\"MEM0_API_KEY\"),\n        user_id=user_id,\n        agent_id=\"fastapi_memory_bot\"\n    )\n    \n    # LLM for response generation\n    llm = OpenAILLMService(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-3.5-turbo\",\n        system_prompt=\"You are a helpful assistant that remembers past conversations.\"\n    )\n    \n    # Simple pipeline\n    pipeline = Pipeline([\n        transport.input(),\n        stt,                # Speech-to-text for audio input\n        user_context,\n        memory,             # Memory service enhances context here\n        llm,\n        transport.output(),\n        assistant_context\n    ])\n    \n    # Run the pipeline\n    runner = PipelineRunner()\n    task = PipelineTask(pipeline)\n    \n    # Event handlers for WebSocket connections\n    @transport.event_handler(\"on_client_connected\")\n    async def on_client_connected(transport, client):\n        # Send welcome message when client connects\n        await task.queue_frame(TextFrame(\"Hello! I'm a memory bot. I'll remember our conversation.\"))\n    \n    @transport.event_handler(\"on_client_disconnected\")\n    async def on_client_disconnected(transport, client):\n        # Clean up when client disconnects\n        await task.cancel()\n    \n    await runner.run(task)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 with Supabase Vector Store in Python\nDESCRIPTION: This Python snippet demonstrates initializing the Mem0 library to use Supabase as its vector store. It sets the required `OPENAI_API_KEY` environment variable, defines a configuration dictionary specifying Supabase as the provider with its connection string and optional index parameters, creates a `Memory` instance from this configuration, and adds sample messages to the memory store associated with a user ID and metadata. Requires the `mem0` library and a valid Supabase connection string.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/supabase.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python Python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"supabase\",\n        \"config\": {\n            \"connection_string\": \"postgresql://user:password@host:port/database\",\n            \"collection_name\": \"memories\",\n            \"index_method\": \"hnsw\",  # Optional: defaults to \"auto\"\n            \"index_measure\": \"cosine_distance\"  # Optional: defaults to \"cosine_distance\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n```\n\n----------------------------------------\n\nTITLE: Healthcare Assistant Memory Reference Example (Markdown)\nDESCRIPTION: This Markdown code block shows an exchange between a user and a healthcare assistant agent, where the agent recalls prior details about allergy medication using its memory retrieval feature. The example highlights the value of memory-aware responses in personalized healthcare applications, requiring both storage and searching of user health data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_19\n\nLANGUAGE: Markdown\nCODE:\n```\nUser: \\\"Have I told you about my allergy medication?\\\"\\nAgent: *retrieves memories* \\\"Yes, you mentioned you're taking Claritin for your pollen allergies.\\\"\n```\n\n----------------------------------------\n\nTITLE: Storing User Preferences with Mem0 (Python)\nDESCRIPTION: Defines the store_user_preferences() function for adding annotated conversation messages to persistent storage using the Mem0 client. An example demonstrates saving a conversational context for a specific user. This pattern is foundational for capturing user preferences that can be used for personalization in other parts of the system. The function expects a user ID (string) and a list of message dictionaries as input.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef store_user_preferences(user_id: str, conversation: list):\n    \"\"\"Store user preferences from conversation history\"\"\"\n    client.add(conversation, user_id=user_id)\n\n# Example conversation storage\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi there! I'm planning a vacation and could use some advice.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! I'd be happy to help with your vacation planning. What kind of destination do you prefer?\",\n    },\n    {\"role\": \"user\", \"content\": \"I am more of a beach person than a mountain person.\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": \"That's interesting. Do you like hotels or airbnb?\",\n    },\n    {\"role\": \"user\", \"content\": \"I like airbnb more.\"},\n]\n\nstore_user_preferences(\"crew_user_1\", messages)\n```\n\n----------------------------------------\n\nTITLE: Integrating LangChain Vector Store with Mem0 in Python\nDESCRIPTION: Example of initializing and configuring a LangChain Chroma vector store with Mem0 in Python. Shows how to set up OpenAI embeddings, configure the vector store, and add conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/langchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n# Initialize a LangChain vector store\nembeddings = OpenAIEmbeddings()\nvector_store = Chroma(\n    persist_directory=\"./chroma_db\",\n    embedding_function=embeddings,\n    collection_name=\"mem0\"  # Required collection name\n)\n\n# Pass the initialized vector store to the config\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"client\": vector_store\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories for a User in Python\nDESCRIPTION: This snippet demonstrates how to retrieve all memories for a user using the 'get_all' method. It includes a query parameter and specifies the output format.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/direct-import.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient.get_all(query=\"What is Alice's favorite sport?\", user_id=\"alice\", output_format=\"v1.1\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Mem0 via Keywords AI with OpenAI SDK (Python)\nDESCRIPTION: This snippet demonstrates using the standard OpenAI Python client configured to route requests through Keywords AI by setting the `api_key` and `base_url` accordingly. It sends a chat completion request including a sample conversation history. Crucially, it passes Mem0-specific parameters (`user_id`, `org_id`, `api_key`, and `add_memories` containing the messages) within the `extra_body` dictionary. This allows Keywords AI to intercept the request, interact with Mem0 to store the memory, and then forward the chat completion request to the underlying LLM.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/keywords.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport json\n\n# Initialize client\nclient = OpenAI(\n    api_key=os.environ.get(\"KEYWORDSAI_API_KEY\"),\n    base_url=os.environ.get(\"KEYWORDSAI_BASE_URL\"),\n)\n\n# Sample conversation messages\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\n\n# Add memory and generate a response\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=messages,\n    extra_body={\n        \"mem0_params\": {\n            \"user_id\": \"test_user\",\n            \"org_id\": \"org_1\",\n            \"api_key\": os.environ.get(\"MEM0_API_KEY\"),\n            \"add_memories\": {\n                \"messages\": messages,\n            },\n        }\n    },\n)\n\nprint(json.dumps(response.model_dump(), indent=4))\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Memory Instance with Mem0 SDK in Python\nDESCRIPTION: Shows how to initialize the Mem0 Memory class with a provided OpenAI API key. Relies on the mem0 package and requires the 'OPENAI_API_KEY' environment variable. Produces an instance 'm' ready for use in memory add/retrieval. Synchronous operation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nm = Memory()\n```\n\n----------------------------------------\n\nTITLE: Customizing Memory Presentation to LLM in Mem0MemoryService (Python)\nDESCRIPTION: Configures Mem0MemoryService to adjust how retrieved memories are formatted and presented in context for the LLM. Allows specification of custom prompts, system/user message assignment, and insertion position. Useful for tuning interaction style and LLM memory utilization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmemory = Mem0MemoryService(\n    api_key=os.getenv(\"MEM0_API_KEY\"),\n    user_id=\"user123\",\n    params={\n        \"system_prompt\": \"Previous conversations with this user:\",\n        \"add_as_system_message\": True,  # Add as system message instead of user message\n        \"position\": 0,                  # Insert at the beginning of the context\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Companion Logic in Node.js\nDESCRIPTION: This JavaScript code defines the complete functionality for an AI companion. It initializes OpenAI and Mem0 clients, implements the `chatWithMemories` function to search relevant memories, generate context-aware responses using OpenAI, and add new interactions to Mem0. The `main` function sets up an interactive command-line interface using Node.js's readline module to chat with the AI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion_js.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAI } from 'openai';\nimport { Memory } from 'mem0ai/oss';\nimport * as readline from 'readline';\n\nconst openaiClient = new OpenAI();\nconst memory = new Memory();\n\nasync function chatWithMemories(message, userId = \"default_user\") {\n  const relevantMemories = await memory.search(message, { userId: userId });\n  \n  const memoriesStr = relevantMemories.results\n    .map(entry => `- ${entry.memory}`)\n    .join('\\n');\n  \n  const systemPrompt = `You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n${memoriesStr}`;\n  \n  const messages = [\n    { role: \"system\", content: systemPrompt },\n    { role: \"user\", content: message }\n  ];\n  \n  const response = await openaiClient.chat.completions.create({\n    model: \"gpt-4o-mini\",\n    messages: messages\n  });\n  \n  const assistantResponse = response.choices[0].message.content || \"\";\n  \n  messages.push({ role: \"assistant\", content: assistantResponse });\n  await memory.add(messages, { userId: userId });\n  \n  return assistantResponse;\n}\n\nasync function main() {\n  const rl = readline.createInterface({\n    input: process.stdin,\n    output: process.stdout\n  });\n  \n  console.log(\"Chat with AI (type 'exit' to quit)\");\n  \n  const askQuestion = () => {\n    return new Promise((resolve) => {\n      rl.question(\"You: \", (input) => {\n        resolve(input.trim());\n      });\n    });\n  };\n  \n  try {\n    while (true) {\n      const userInput = await askQuestion();\n      \n      if (userInput.toLowerCase() === 'exit') {\n        console.log(\"Goodbye!\");\n        rl.close();\n        break;\n      }\n      \n      const response = await chatWithMemories(userInput, \"sample_user\");\n      console.log(`AI: ${response}`);\n    }\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n    rl.close();\n  }\n}\n\nmain().catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with Keywords AI LLM Provider (Python)\nDESCRIPTION: This example shows how to initialize the Mem0 `Memory` client, configuring it to use an OpenAI model proxied through Keywords AI. It retrieves API keys and the base URL from environment variables, defines a configuration dictionary specifying the LLM provider (`openai`), model (`gpt-4o-mini`), Keywords AI API key, and Keywords AI base URL, then initializes Mem0 and adds a sample memory item associated with a `user_id`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/keywords.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\nimport os\n\n# Configuration\napi_key = os.getenv(\"MEM0_API_KEY\")\nkeywordsai_api_key = os.getenv(\"KEYWORDSAI_API_KEY\")\nbase_url = os.getenv(\"KEYWORDSAI_BASE_URL\") # \"https://api.keywordsai.co/api/\"\n\n# Set up Mem0 with Keywords AI as the LLM provider\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o-mini\",\n            \"temperature\": 0.0,\n            \"api_key\": keywordsai_api_key,\n            \"openai_base_url\": base_url,\n        },\n    }\n}\n\n# Initialize Memory\nmemory = Memory.from_config(config_dict=config)\n\n# Add a memory\nresult = memory.add(\n    \"I like to take long walks on weekends.\",\n    user_id=\"alice\",\n    metadata={\"category\": \"hobbies\"},\n)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying Text Files with Embedchain\nDESCRIPTION: Shows how to initialize an embedchain App, add a local text file using the text_file data type, and query its contents. The file path should be provided as a local system path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/text-file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add('path/to/file.txt', data_type=\"text_file\")\n\napp.query(\"Summarize the information of the text file\")\n```\n\n----------------------------------------\n\nTITLE: Searching Memories with Filters in Mem0 (Python)\nDESCRIPTION: This snippet demonstrates how to search memories in Mem0 using the Python client, filtering by categories and metadata. It defines a query and uses the `client.search` method to retrieve memories that match the specified categories and metadata. This allows for more specific and targeted memory retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"What do you know about me?\"\n\nclient.search(query, categories=[\"food_preferences\"], metadata={\"food\": \"vegan\"})\n```\n\n----------------------------------------\n\nTITLE: Batch Update Memories with Mem0\nDESCRIPTION: Updates multiple memories in a single API call using Mem0.  The examples show how to construct the list of memory updates and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memories to update.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_72\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X PUT \"https://api.mem0.ai/v1/memories/batch/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"memories\": [\n             {\n                 \"memory_id\": \"285ed74b-6e05-4043-b16b-3abd5b533496\",\n                 \"text\": \"Watches football\"\n             },\n             {\n                 \"memory_id\": \"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\",\n                 \"text\": \"Loves to travel\"\n             }\n         ]\n     }'\n```\n\n----------------------------------------\n\nTITLE: Creating the Conversation Runner Function in Python\nDESCRIPTION: Implements the `run_conversation` function responsible for executing a single turn of the conversation. It takes the user's input (`user_input`) and their Mem0 ID (`mem0_user_id`) as arguments. It sets up the configuration required for LangGraph's state management, specifically the `thread_id` which corresponds to the Mem0 user ID. It initializes the state with the user's message and streams the execution through the `compiled_graph`. It iterates through the stream events and prints the content of the latest AI message.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef run_conversation(user_input: str, mem0_user_id: str):\n    config = {\"configurable\": {\"thread_id\": mem0_user_id}}\n    state = {\"messages\": [HumanMessage(content=user_input)], \"mem0_user_id\": mem0_user_id}\n\n    for event in compiled_graph.stream(state, config):\n        for value in event.values():\n            if value.get(\"messages\"):\n                print(\"Customer Support:\", value[\"messages\"][-1].content)\n                return\n```\n\n----------------------------------------\n\nTITLE: Basic Text Generation with Mem0-augmented Context (TypeScript)\nDESCRIPTION: Demonstrates generating text using Vercel AI SDK, utilizing Mem0 for memory-based context. Imports generateText and createMem0, initializes Mem0, and supplies a user_id when constructing the model. Parameters include the model type and prompt. Inputs: prompt string, outputs: object with generated text. Requires valid provider and user_id.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0(\"gpt-4-turbo\", { user_id: \"borat\" }),\n  prompt: \"Suggest me a good car to buy!\",\n});\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data Sources for Next.JS Question Answering\nDESCRIPTION: This code adds data sources to the Embedchain App. It ingests the Next.JS website, documentation, and forum data using sitemaps. This step populates the pipeline with over 15,000 pages of relevant information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/question-answering.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Add Next.JS Website and docs\napp.add(\"https://nextjs.org/sitemap.xml\", data_type=\"sitemap\")\n\n# Add Next.JS Forum data\napp.add(\"https://nextjs-forum.com/sitemap.xml\", data_type=\"sitemap\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using TogetherAI Models with mem0\nDESCRIPTION: This code demonstrates how to set up the necessary environment variables, configure a TogetherAI LLM model, and use it with the mem0 Memory class. It shows how to initialize a Memory instance with a specific model configuration and add conversation messages with metadata for a user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/together.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"TOGETHER_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"together\",\n        \"config\": {\n            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with User and Agent IDs in Mem0 (Python)\nDESCRIPTION: This snippet demonstrates how to add memories to Mem0 using the Python client, associating them with both a user ID and an agent ID.  It defines a list of messages and uses the `client.add` method to store them with the specified user and agent identifiers. This allows for tracking conversations between specific users and agents.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm travelling to San Francisco\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! I'm going to Dubai next month.\"},\n]\n\nclient.add(messages=messages, user_id=\"user1\", agent_id=\"agent1\")\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with Graph Memory in Python\nDESCRIPTION: This snippet demonstrates how to add new memories with Graph Memory enabled using the Mem0 Python client. It includes setting up the client and making an API call with the necessary parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import MemoryClient\n\nclient = MemoryClient(\n    api_key=\"your-api-key\",\n    org_id=\"your-org-id\",\n    project_id=\"your-project-id\"\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"My name is Joseph\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Joseph, it's nice to meet you!\"},\n    {\"role\": \"user\", \"content\": \"I'm from Seattle and I work as a software engineer\"}\n]\n\n# Enable graph memory when adding\nclient.add(\n    messages, \n    user_id=\"joseph\", \n    version=\"v1\", \n    enable_graph=True, \n    output_format=\"v1.1\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing a ReActAgent with Tools and Memory - Python\nDESCRIPTION: Creates a ReActAgent instance from a set of FunctionTool objects, LLM, and memory, with optional verbosity. This snippet shows the agent responding interactively to a chat and printing the response. Depends on LlamaIndex and configured tools.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.agent import ReActAgent\n\nagent = ReActAgent.from_tools(\n    [call_tool, email_tool],\n    llm=llm,\n    memory=memory_from_client,  # or memory_from_config\n    verbose=True,\n)\n\n# Start the chat\nresponse = agent.chat(\"Hi, My name is Mayank\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Delete Memory by ID with Mem0\nDESCRIPTION: Deletes a specific memory from the Mem0 system using its ID. The examples demonstrate how to perform this operation using Python, JavaScript, and a cURL request. The cURL request requires an API key for authorization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\nclient.delete(memory_id)\n```\n\n----------------------------------------\n\nTITLE: Combining OpenAI Provider with Memory Utils\nDESCRIPTION: Shows how to retrieve memories separately and use them with the OpenAI provider. This approach allows using memory context with standard AI providers.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { retrieveMemories } from \"@mem0/vercel-ai-provider\";\n\nconst prompt = \"Suggest me a good car to buy.\";\nconst memories = await retrieveMemories(prompt, { user_id: \"borat\" });\n\nconst { text } = await generateText({\n  model: openai(\"gpt-4-turbo\"),\n  prompt: prompt,\n  system: memories,\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Sample Conversation to Mem0 in JSON Format\nDESCRIPTION: Defines a JSON-formatted conversation between a user and an assistant, capturing detailed dialog history and structured responses for later retrieval or context enrichment. Includes user travel preferences and assistant's personalized recommendations and search results. Can be used to seed memory or for tests with Mem0 conversational memory APIs. Requires a client that can ingest or process the specified conversation list.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the best travel destinations in the world?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Could you please specify your interests or the type of travel information you are looking for? This will help me find the most relevant information for you.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Sure, I want to travel to San Francisco.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\"\"\n        Based on the information gathered from TripAdvisor, here are some popular attractions, activities, and travel tips for San Francisco: \\\n\n        1. **Golden Gate Bridge**: A must-see iconic landmark. \\\n        2. **Alcatraz Island**: Famous former prison offering tours. \\\n        3. **Fisherman's Wharf**: Popular tourist area with shops, restaurants, and sea lions. \\\n        4. **Chinatown**: The largest Chinatown outside of Asia. \\\n        5. **Golden Gate Park**: Large urban park with gardens, museums, and recreational activities. \\\n        6. **Cable Cars**: Historic streetcars offering a unique way to see the city. \\\n        7. **Exploratorium**: Interactive science museum. \\\n        8. **San Francisco Museum of Modern Art (SFMOMA)**: Modern and contemporary art museum. \\\n        9. **Lombard Street**: Known for its steep, one-block section with eight hairpin turns. \\\n        10. **Union Square**: Major shopping and cultural hub. \\\n\n        Travel Tips: \\\n        - **Weather**: San Francisco has a mild climate, but it can be foggy and windy. Dress in layers. \\\n        - **Transportation**: Use public transportation like BART, Muni, and cable cars to get around. \\\n        - **Safety**: Be aware of your surroundings, especially in crowded tourist areas. \\\n        - **Dining**: Try local specialities like sourdough bread, seafood, and Mission-style burritos. \\\n        \"\"\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Show me hotels around Golden Gate Bridge.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\"\"The search results for hotels around Golden Gate Bridge in San Francisco include: \\\n\n        1. Hilton Hotels In San Francisco - Hotel Near Fishermans Wharf (hilton.com) \\\n        2. The 10 Closest Hotels to Golden Gate Bridge (tripadvisor.com) \\\n        3. Hotels near Golden Gate Bridge (expedia.com) \\\n        4. Hotels near Golden Gate Bridge (hotels.com) \\\n        5. Holiday Inn Express & Suites San Francisco Fishermans Wharf, an IHG Hotel $146 (1.8K) 3-star hotel Golden Gate Bridge • 3.5 mi DEAL 19% less than usual \\\n        6. Holiday Inn San Francisco-Golden Gateway, an IHG Hotel $151 (3.5K) 3-star hotel Golden Gate Bridge • 3.7 mi Casual hotel with dining, a bar & a pool \\\n        7. Hotel Zephyr San Francisco $159 (3.8K) 4-star hotel Golden Gate Bridge • 3.7 mi Nautical-themed lodging with bay views \\\n        8. Lodge at the Presidio \\\n        9. The Inn Above Tide \\\n        10. Cavallo Point \\\n        11. Casa Madrona Hotel and Spa \\\n        12. Cow Hollow Inn and Suites \\\n        13. Samesun San Francisco \\\n        14. Inn on Broadway \\\n        15. Coventry Motor Inn \\\n        16. HI San Francisco Fisherman's Wharf Hostel \\\n        17. Loews Regency San Francisco Hotel \\\n        18. Fairmont Heritage Place Ghirardelli Square \\\n        19. Hotel Drisco Pacific Heights \\\n        20. Travelodge by Wyndham Presidio San Francisco \\\n        \"\"\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Using LlamaIndex ReAct Agent With Memory\nDESCRIPTION: Creates and interacts with a FunctionCallingAgent with Mem0 memory, showcasing its ability to remember and use past preferences shared by the user to perform actions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nagent = FunctionCallingAgent.from_tools(\n    [call_tool, email_tool, order_food_tool],\n    llm=llm,\n    # memory is provided\n    memory=memory_from_client,  # or memory_from_config\n    verbose=True,\n)\nresponse = agent.chat(\"I am feeling hungry, order me something and send me the bill\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing Clients in Python\nDESCRIPTION: Imports required modules from `typing`, `langgraph`, `langchain_openai`, `mem0`, and `langchain_core`. Sets up placeholder API keys for OpenAI and Mem0 (which should be replaced with actual keys) and initializes the `ChatOpenAI` LLM client (using GPT-4) and the `MemoryClient` for Mem0. Requires valid API keys to function correctly.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, TypedDict, List\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\nfrom mem0 import MemoryClient\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\n# Configuration\nOPENAI_API_KEY = 'sk-xxx'  # Replace with your actual OpenAI API key\nMEM0_API_KEY = 'your-mem0-key'  # Replace with your actual Mem0 API key\n\n# Initialize LangChain and Mem0\nllm = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\nmem0 = MemoryClient(api_key=MEM0_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Using Embedchain with LangSmith Integration\nDESCRIPTION: This Python example demonstrates how to create an Embedchain app, add data to it, and query the data. With the LangSmith environment variables set, all operations will be automatically logged to LangSmith for monitoring and debugging.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/langsmith.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# Initialize EmbedChain application.\napp = App()\n\n# Add data to your app\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\n\n# Query your app\napp.query(\"How many companies did Elon found?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Custom Prompt in Python\nDESCRIPTION: This snippet demonstrates how to initialize a Memory instance with a custom fact extraction prompt in Python. It includes configuration for the LLM provider, model settings, and the custom prompt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    },\n    \"custom_fact_extraction_prompt\": custom_fact_extraction_prompt,\n    \"version\": \"v1.1\"\n}\n\nm = Memory.from_config(config_dict=config, user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Complete Memory-Enabled Voice Assistant Implementation (Python)\nDESCRIPTION: This is the full, integrated Python script that brings together environment setup, agent prewarming, chat context definition, memory enrichment, and the main application execution using LiveKit, OpenAI, Deepgram, Silero, and Mem0 SDKs. All async event flows and function hooks are included to provide a robust starting point for a production-grade voice travel assistant. Requires all dependencies listed in earlier setup, and expects correct environment variable values to run. Input is the program entry; output is a running, interactive, memory-aware voice agent application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Dict, Any, Annotated\n\nimport aiohttp\nfrom dotenv import load_dotenv\nfrom livekit.agents import (\n    AutoSubscribe,\n    JobContext,\n    JobProcess,\n    WorkerOptions,\n    cli,\n    llm,\n    metrics,\n)\nfrom livekit import rtc, api\nfrom livekit.agents.pipeline import VoicePipelineAgent\nfrom livekit.plugins import deepgram, openai, silero\nfrom mem0 import AsyncMemoryClient\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogger = logging.getLogger(\"memory-assistant\")\nlogger.setLevel(logging.INFO)\n\n# Define a global user ID for simplicity\nUSER_ID = \"voice_user\"\n\n# Initialize Mem0 memory client\nmem0 = AsyncMemoryClient()\n\ndef prewarm_process(proc: JobProcess):\n    # Preload silero VAD in memory to speed up session start\n    proc.userdata[\"vad\"] = silero.VAD.load()\n\nasync def entrypoint(ctx: JobContext):\n    # Connect to LiveKit room\n    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)\n    \n    # Wait for participant\n    participant = await ctx.wait_for_participant()\n    \n    async def _enrich_with_memory(agent: VoicePipelineAgent, chat_ctx: llm.ChatContext):\n        \"\"\"Add memories and Augment chat context with relevant memories\"\"\"\n        if not chat_ctx.messages:\n            return\n        \n        # Store user message in Mem0\n        user_msg = chat_ctx.messages[-1]\n        await mem0.add(\n            [{\"role\": \"user\", \"content\": user_msg.content}], \n            user_id=USER_ID\n        )\n        \n        # Search for relevant memories\n        results = await mem0.search(\n            user_msg.content, \n            user_id=USER_ID,\n        )\n        \n        # Augment context with retrieved memories\n        if results:\n            memories = ' '.join([result[\"memory\"] for result in results])\n            logger.info(f\"Enriching with memory: {memories}\")\n            \n            rag_msg = llm.ChatMessage.create(\n                text=f\"Relevant Memory: {memories}\\n\",\n                role=\"assistant\",\n            )\n            \n            # Modify chat context with retrieved memories\n            chat_ctx.messages[-1] = rag_msg\n            chat_ctx.messages.append(user_msg)\n\n    # Define initial system context\n    initial_ctx = llm.ChatContext().append(\n        role=\"system\",\n        text=(\n            \"\"\"\n            You are a helpful voice assistant.\n            You are a travel guide named George and will help the user to plan a travel trip of their dreams. \n            You should help the user plan for various adventures like work retreats, family vacations or solo backpacking trips. \n            You should be careful to not suggest anything that would be dangerous, illegal or inappropriate.\n            You can remember past interactions and use them to inform your answers.\n            Use semantic memory retrieval to provide contextually relevant responses. \n            \"\"\"\n        ),\n    )\n\n    # Create VoicePipelineAgent with memory capabilities\n    agent = VoicePipelineAgent(\n        chat_ctx=initial_ctx,\n        vad=silero.VAD.load(),\n        stt=deepgram.STT(),\n        llm=openai.LLM(model=\"gpt-4o-mini\"),\n        tts=openai.TTS(),\n        before_llm_cb=_enrich_with_memory,\n    )\n\n    # Start agent and initial greeting\n    agent.start(ctx.room, participant)\n    await agent.say(\n        \"Hello! I'm George. Can I help you plan an upcoming trip? \",\n        allow_interruptions=True\n    )\n\n# Run the application\nif __name__ == \"__main__\":\n    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, prewarm_fnc=prewarm_process))\n```\n\n----------------------------------------\n\nTITLE: Using Mem0 Platform for Chat Completion in Python\nDESCRIPTION: This snippet demonstrates how to use Mem0 Platform for chat completion, including initializing the client, storing user preferences, and leveraging stored memory for personalized responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0.proxy.main import Mem0\n\nclient = Mem0(api_key=\"m0-xxx\")\n\n# First interaction: Storing user preferences\nmessages = [\n  {\n    \"role\": \"user\",\n    \"content\": \"I love indian food but I cannot eat pizza since allergic to cheese.\"\n  },\n]\nuser_id = \"alice\"\nchat_completion = client.chat.completions.create(messages=messages, model=\"gpt-4o-mini\", user_id=user_id)\n# Memory saved after this will look like: \"Loves Indian food. Allergic to cheese and cannot eat pizza.\"\n\n# Second interaction: Leveraging stored memory\nmessages = [\n  {\n    \"role\": \"user\",\n    \"content\": \"Suggest restaurants in San Francisco to eat.\",\n  }\n]\n\nchat_completion = client.chat.completions.create(messages=messages, model=\"gpt-4o-mini\", user_id=user_id)\nprint(chat_completion.choices[0].message.content)\n# Answer: You might enjoy Indian restaurants in San Francisco, such as Amber India, Dosa, or Curry Up Now, which offer delicious options without cheese.\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Agent Escalation with AutoGen and Mem0 in Python\nDESCRIPTION: This snippet showcases a multi-agent setup where complex issues can be escalated. It initializes a second `ConversableAgent` named 'manager'. A function `escalate_to_manager` retrieves relevant memories from Mem0, constructs a prompt including the context and the customer's question specifically for the manager, and then uses the `manager` agent to generate a response. Example usage demonstrates escalating a complex question.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/autogen.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmanager = ConversableAgent(\n    \"manager\",\n    system_message=\"You are a manager who helps in resolving complex customer issues.\",\n    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": OPENAI_API_KEY}]},\n    human_input_mode=\"NEVER\"\n)\n\ndef escalate_to_manager(question):\n    relevant_memories = memory_client.search(question, user_id=USER_ID)\n    context = \"\\n\".join([m[\"memory\"] for m in relevant_memories])\n\n    prompt = f\"\"\"\n    Context from previous interactions:\n    {context}\n\n    Customer question: {question}\n\n    As a manager, how would you address this issue?\n    \"\"\"\n\n    manager_response = manager.generate_reply(messages=[{\"content\": prompt, \"role\": \"user\"}])\n    return manager_response\n\n# Example usage\ncomplex_question = \"I'm not satisfied with the troubleshooting steps. What else can be done?\"\nmanager_answer = escalate_to_manager(complex_question)\nprint(\"Manager's response:\", manager_answer)\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Timestamp - Python\nDESCRIPTION: This Python snippet demonstrates how to add a memory to Mem0 with a custom timestamp.  It calculates a timestamp for 5 days ago, converts it to a Unix timestamp, and then adds the memory using the `client.add` method. Requires the `datetime` and `timedelta` modules from the `time` library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport time\nfrom datetime import datetime, timedelta\n\n# Get the current time\ncurrent_time = datetime.now()\n\n# Calculate 5 days ago\nfive_days_ago = current_time - timedelta(days=5)\n\n# Convert to Unix timestamp (seconds since epoch)\nunix_timestamp = int(five_days_ago.timestamp())\n\n# Add memory with custom timestamp\nclient.add(\"I'm travelling to SF\", user_id=\"user1\", timestamp=unix_timestamp)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using mem0 Memory with Redis Vector Store (Python)\nDESCRIPTION: This Python snippet demonstrates how to configure and use the mem0 Memory object with Redis as the vector store backend. Dependencies are 'mem0', 'redis', and 'redisvl'. Required parameters include a Redis URL, collection name, and embedding model dimensions. After configuration, it shows adding a list of chat messages to the memory, associated with a user and optional metadata. Expects valid OpenAI API key set as an environment variable. The function 'add' accepts a message history, user_id, and optional metadata, with each message defining a role and content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/redis.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"redis\",\n        \"config\": {\n            \"collection_name\": \"mem0\",\n            \"embedding_model_dims\": 1536,\n            \"redis_url\": \"redis://localhost:6379\"\n        }\n    },\n    \"version\": \"v1.1\"\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I’m not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Executing the Add Memory Tool with Example Input in Python\nDESCRIPTION: Demonstrates how to invoke the `add_tool` (a LangChain `StructuredTool`) with sample input data conforming to the `AddMemoryInput` schema. The input includes messages, user ID, output format, and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nadd_input = {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n        {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy.\"}\n    ],\n    \"user_id\": \"alex\",\n    \"output_format\": \"v1.1\",\n    \"metadata\": {\"food\": \"vegan\"}\n}\nadd_result = add_tool.invoke(add_input)\n```\n\n----------------------------------------\n\nTITLE: Storing Messages as Memories with Mem0 in Python\nDESCRIPTION: Shows how to structure messages and store them with Mem0 using the add() method. Accepts a list of message dicts containing 'role' and 'content'. Optionally includes metadata and determines whether inference is applied. Returns storage results with assigned memory IDs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\n\n# Store inferred memories (default behavior)\nresult = m.add(messages, user_id=\"alice\", metadata={\"category\": \"movie_recommendations\"})\n\n# Store raw messages without inference\n# result = m.add(messages, user_id=\"alice\", metadata={\"category\": \"movie_recommendations\"}, infer=False)\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n            \"memory\": \"User is planning to watch a movie tonight.\",\n            \"metadata\": {\n                \"category\": \"movie_recommendations\"\n            },\n            \"event\": \"ADD\"\n        },\n        {\n            \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n            \"memory\": \"User is not a big fan of thriller movies.\",\n            \"metadata\": {\n                \"category\": \"movie_recommendations\"\n            },\n            \"event\": \"ADD\"\n        },\n        {\n            \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n            \"memory\": \"User loves sci-fi movies.\",\n            \"metadata\": {\n                \"category\": \"movie_recommendations\"\n            },\n            \"event\": \"ADD\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Agent-User Memories Use Case Scenario (Markdown)\nDESCRIPTION: This Markdown code block provides a conversational example for how the ElevenLabs agent retrieves memories in response to user queries and incorporates them in replies. It illustrates practical usage for both agent and user, supporting scenarios like personal assistance, customer support, educational interactions, and healthcare contexts. The block is suitable as a template or illustrative aid for designing memory-aware prompts.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_16\n\nLANGUAGE: Markdown\nCODE:\n```\nUser: \\\"What restaurants did I say I liked last time?\\\"\\nAgent: *retrieves memories* \\\"You mentioned enjoying Bella Italia and The Golden Dragon.\\\"\n```\n\n----------------------------------------\n\nTITLE: Loading Webpage Data with Embedchain App in Python\nDESCRIPTION: This snippet demonstrates initializing an Embedchain `App` and using its `add()` method to load data from a specific webpage URL. The data type is implicitly detected as a webpage. The output comments indicate successful saving and chunking of the data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/add.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n# Inserting batches in chromadb: 100%|███████████████| 1/1 [00:00<00:00,  1.19it/s]\n# Successfully saved https://www.forbes.com/profile/elon-musk (DataType.WEB_PAGE). New chunks count: 4\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Mem0 Document Editing Workflow in Python\nDESCRIPTION: This snippet demonstrates how to use the document editing workflow. It defines an original document content and runs the workflow to edit the document based on stored preferences.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/document-writing.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define your document\noriginal_content = \"\"\"Project Proposal\n    \nThe following proposal outlines our strategy for the Q3 marketing campaign. \nWe believe this approach will significantly increase our market share.\n\nIncrease brand awareness\nBoost sales by 15%\nExpand our social media following\n\nWe plan to launch the campaign in July and continue through September.\n\"\"\"\n\n# Run the workflow\nresult = document_editing_workflow(original_content)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Groq LPU with mem0ai in Python\nDESCRIPTION: This snippet demonstrates how to set up the Groq LPU integration with mem0ai in Python. It includes setting API keys, configuring the LLM provider, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/groq.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"GROQ_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"groq\",\n        \"config\": {\n            \"model\": \"mixtral-8x7b-32768\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Fetching and Printing User and Companion Memories (Python)\nDESCRIPTION: Provides a Python function `print_memories` to retrieve and display memories stored by the `mem0ai` library for a given ID (user or companion). It uses the `get_memories` method of the `ai_companion` object (defined previously) and prints the text of each memory found. The example shows how to call this function for both user and companion memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef print_memories(user_id, label):\n    print(f\"\\n{label} Memories:\")\n    memories = ai_companion.get_memories(user_id=user_id)\n    if memories:\n        for m in memories:\n            print(f\"- {m['text']}\")\n    else:\n        print(\"No memories found.\")\n\n# Print user memories\nprint_memories(user_id, \"User\")\n\n# Print companion memories\nprint_memories(companion_id, \"Companion\")\n```\n\n----------------------------------------\n\nTITLE: Querying for Relevant Memories - cURL\nDESCRIPTION: Executes a POST search query to the Mem0 API REST endpoint using cURL. The request includes authentication, versioning, user_id-based filtering, and a query text payload. Used for retrieving memories that match the semantic intent of the query.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \\\"https://api.mem0.ai/v1/memories/search/?version=v2\\\" \\\\n     -H \\\"Authorization: Token your-api-key\\\" \\\\n     -H \\\"Content-Type: application/json\\\" \\\\n     -d '{\\n         \\\"query\\\": \\\"I'm craving some pizza. Any recommendations?\\\",\\n         \\\"filters\\\": {\\n             \\\"AND\\\": [\\n                 {\\n                    \\\"user_id\\\": \\\"alex\\\"\\n                 }\\n             ]\\n         }\\n     }'\n```\n\n----------------------------------------\n\nTITLE: Defining Memory Search Tool with Mem0 - Python\nDESCRIPTION: The `search_memories` async function, also decorated with `@function_tool`, performs searches for relevant user memories in Mem0 based on a query string. It prints/logs the operation, executes a search via `mem0_client.search`, and formats up to five relevant results for output to the user. Designed for integration with agent workflows, it requires access to `mem0_client`, USER_ID, and expects a string input `query`. Returns a string with bullet-pointed memories or a fallback message if nothing found.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n@function_tool\nasync def search_memories(\n    query: str\n) -> str:\n    \"\"\"\n    Find memories relevant to the current conversation.\n    Args:\n        query: The search query to find relevant memories\n    \"\"\"\n    print(f\"Finding memories related to: {query}\")\n    results = await mem0_client.search(\n        query,\n        user_id=USER_ID,\n        limit=5,\n        threshold=0.7,  # Higher threshold for more relevant results\n        output_format=\"v1.1\"\n    )\n    \n    # Format and return the results\n    if not results.get('results', []):\n        return \"I don't have any relevant memories about this topic.\"\n    \n    memories = [f\"\\u2022 {result['memory']}\" for result in results.get('results', [])]\n    return \"Here's what I remember that might be relevant:\\n\" + \"\\n\".join(memories)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Google Drive Folder using Embedchain in Python\nDESCRIPTION: This Python snippet demonstrates the initialization of an Embedchain App and the addition of a Google Drive folder as a data source. It requires prior installation of the 'embedchain[googledrive]' extra dependencies (using pip) and configuration of Google Drive API credentials stored as 'credentials.json' referenced by an environment variable. The 'app.add' method expects the Google Drive folder URL and the parameter 'data_type' set to 'google_drive'. On first run, user authentication with Google may be prompted. Input is a URL string; output is the ingestion of corresponding Google Drive content into the Embedchain app.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/google-drive.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\nurl = \"https://drive.google.com/drive/u/0/folders/xxx-xxx\"\napp.add(url, data_type=\"google_drive\")\n```\n\n----------------------------------------\n\nTITLE: Using Mem0 OSS for Chat Completion in Python\nDESCRIPTION: This example shows how to use Mem0 OSS for chat completion, including configuring the vector store and creating a chat completion request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"host\": \"localhost\",\n            \"port\": 6333,\n        }\n    },\n}\n\nclient = Mem0(config=config)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the capital of France?\",\n        }\n    ],\n    model=\"gpt-4o\",\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Context Window using BaseLlmConfig in Embedchain Python\nDESCRIPTION: This example demonstrates how to customize the number of context chunks (`number_documents`) considered during chat by passing a `BaseLlmConfig` configuration object to the `chat()` method. Dependencies include both the `embedchain` package and `embedchain.config`. The configuration allows developers to modify the context window size (default is 3); here, it's set to 5. This approach is particularly useful when deeper or broader context is needed for accurate responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/chat.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\nfrom embedchain.config import BaseLlmConfig\\n\\napp = App()\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\nquery_config = BaseLlmConfig(number_documents=5)\\napp.chat(\\\"What is the net worth of Elon Musk?\\\", config=query_config)\n```\n\n----------------------------------------\n\nTITLE: Loading Gmail Messages with Embedchain in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize an Embedchain App, add a filtered Gmail data source, and perform a query to summarize email conversations. Required dependencies include the 'embedchain' package installed with the '[gmail]' extra (pip install --upgrade embedchain[gmail]), and a 'credentials.json' OAuth credentials file properly configured and placed in the current directory. The 'gmail_filter' parameter specifies the Gmail search query (e.g., 'to: me label:inbox'), and the 'data_type' must be set to 'gmail' to trigger correct source detection. The snippet inputs a Gmail query string and outputs the summarized content of matched email conversations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/gmail.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\ngmail_filter = \"to: me label:inbox\"\napp.add(gmail_filter, data_type=\"gmail\")\napp.query(\"Summarize my email conversations\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom User IDs for Mem0 Integration\nDESCRIPTION: This Python code demonstrates how to specify different user IDs to maintain separate memory stores for multiple users in Mem0.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncontext = Mem0Context(user_id=\"user123\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Gemini AI with mem0\nDESCRIPTION: Demonstrates how to set up and use Google's Gemini AI model through mem0 library. Shows configuration of API keys, model parameters, and adding conversation history. Requires both OpenAI API key for embeddings and Gemini API key for the language model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/google_AI.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"litellm\",\n        \"config\": {\n            \"model\": \"gemini/gemini-pro\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Searching Memories in Mem0 (Python)\nDESCRIPTION: This snippet demonstrates how to search memories in Mem0 using the Python client.  It defines a query and uses the `client.search` method to retrieve relevant memories associated with the specified user ID. The `output_format` parameter defaults to `v1.0`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"What should I cook for dinner today?\"\n\nclient.search(query, user_id=\"alex\")\n```\n\n----------------------------------------\n\nTITLE: Interacting with LlamaIndex ReAct Agent\nDESCRIPTION: Demonstrates how to chat with the ReAct agent, showing how it uses Mem0 to store and retrieve relevant memories from the conversation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"Hi, My name is David\")\nprint(response)\n\nresponse = agent.chat(\"I love to eat pizza on weekends\")\nprint(response)\n\nresponse = agent.chat(\"My preferred way of communication is email\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Template for Travel Agent - Python\nDESCRIPTION: Sets up a chat prompt template for the AI travel agent, defining system-level behavior and context placeholders for dynamic interactions. It ensures that every response uses context (user history and preferences) and user-provided input. Requires LangChain's prompt templating and message classes. Expects a 'context' list variable and an 'input' string to generate prompts.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = ChatPromptTemplate.from_messages([\n    SystemMessage(content=\"\"\"You are a helpful travel agent AI. Use the provided context to personalize your responses and remember user preferences and past interactions. \n    Provide travel recommendations, itinerary suggestions, and answer questions about destinations. \n    If you don't have specific information, you can make general suggestions based on common travel knowledge.\"\"\"),\n    MessagesPlaceholder(variable_name=\"context\"),\n    HumanMessage(content=\"{input}\")\n])\n```\n\n----------------------------------------\n\nTITLE: Adding to Specific Conversation Session in JavaScript (v2)\nDESCRIPTION: Shows how to add memories to a specific conversation session using both user_id and run_id parameters in JavaScript. This approach helps organize memories into logical sessions or tasks.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n// Adding to a specific conversation session\nconst messages = [\n    {\"role\": \"user\", \"content\": \"For this trip to Paris, I want to focus on art museums.\"},\n    {\"role\": \"assistant\", \"content\": \"Great! I'll help you plan your Paris trip with a focus on art museums.\"}\n];\nclient.add(messages, { user_id: \"alex\", run_id: \"paris-trip-2024\", version: \"v2\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n\n// Later in the same conversation session\nconst messages2 = [\n    {\"role\": \"user\", \"content\": \"I'd like to visit the Louvre on Monday.\"},\n    {\"role\": \"assistant\", \"content\": \"The Louvre is a great choice for Monday. Would you like information about opening hours?\"}\n];\nclient.add(messages2, { user_id: \"alex\", run_id: \"paris-trip-2024\", version: \"v2\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Instance with Qdrant Backend in Python\nDESCRIPTION: Illustrates creating a Memory instance configured for Qdrant vector storage. Requires 'qdrant' running on localhost and 'mem0' installed. Passes vector_store config dict specifying provider, host, and port. Enables production-level storage and retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"host\": \"localhost\",\n            \"port\": 6333,\n        }\n    },\n}\n\nm = Memory.from_config(config)\n```\n\n----------------------------------------\n\nTITLE: Implementing Personal AI Travel Assistant (Mem0 v1.1+)\nDESCRIPTION: Python implementation of a Personal AI Travel Assistant using Mem0 v1.1 or later. It includes configuration setup, memory management, and interaction with OpenAI's GPT-4 model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-travel-assistant.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom mem0 import Memory\n\n# Set the OpenAI API key\nos.environ['OPENAI_API_KEY'] = \"sk-xxx\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"text-embedding-3-large\"\n        }\n    },\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"embedding_model_dims\": 3072,\n        }\n    },\n    \"version\": \"v1.1\",\n}\n\nclass PersonalTravelAssistant:\n    def __init__(self):\n        self.client = OpenAI()\n        self.memory = Memory.from_config(config)\n        self.messages = [{\"role\": \"system\", \"content\": \"You are a personal AI Assistant.\"}]\n\n    def ask_question(self, question, user_id):\n        # Fetch previous related memories\n        previous_memories = self.search_memories(question, user_id=user_id)\n\n        # Build the prompt\n        system_message = \"You are a personal AI Assistant.\"\n\n        if previous_memories:\n            prompt = f\"{system_message}\\n\\nUser input: {question}\\nPrevious memories: {', '.join(previous_memories)}\"\n        else:\n            prompt = f\"{system_message}\\n\\nUser input: {question}\"\n\n        # Generate response using Responses API\n        response = self.client.responses.create(\n            model=\"gpt-4o\",\n            input=prompt\n        )\n\n        # Extract answer from the response\n        answer = response.output[0].content[0].text\n\n        # Store the question in memory\n        self.memory.add(question, user_id=user_id)\n        return answer\n\n    def get_memories(self, user_id):\n        memories = self.memory.get_all(user_id=user_id)\n        return [m['memory'] for m in memories['results']]\n\n    def search_memories(self, query, user_id):\n        memories = self.memory.search(query, user_id=user_id)\n        return [m['memory'] for m in memories['results']]\n\n# Usage example\nuser_id = \"traveler_123\"\nai_assistant = PersonalTravelAssistant()\n\ndef main():\n    while True:\n        question = input(\"Question: \")\n        if question.lower() in ['q', 'exit']:\n            print(\"Exiting...\")\n            break\n\n        answer = ai_assistant.ask_question(question, user_id=user_id)\n        print(f\"Answer: {answer}\")\n        memories = ai_assistant.get_memories(user_id=user_id)\n        print(\"Memories:\")\n        for memory in memories:\n            print(f\"- {memory}\")\n        print(\"-----\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Mem0 and MultiOn in Bash\nDESCRIPTION: Installs the required Python libraries for enabling both persistent memory (mem0ai), browser automation (multion), and language modeling (openai). Dependencies must be present before running any Python code; install them in your Python environment using pip. No additional arguments or customization are required.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai multion openai\n```\n\n----------------------------------------\n\nTITLE: Performing Keyword Search in Mem0 using Python\nDESCRIPTION: Demonstrates how to use the keyword search feature in Mem0. This mode emphasizes keywords within the query, returning memories that contain the most relevant keywords alongside those from the default search.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/advanced-retrieval.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient.search(query, keyword_search=True, user_id='alex')\n```\n\nLANGUAGE: python\nCODE:\n```\n# Search for memories about food preferences with keyword search enabled\nquery = \"What are my food preferences?\"\nresults = client.search(query, keyword_search=True, user_id='alex')\n\n# Output might include:\n# - \"Vegetarian. Allergic to nuts.\" (highly relevant)\n# - \"Prefers spicy food and enjoys Thai cuisine\" (relevant)\n# - \"Mentioned disliking sea food during restaurant discussion\" (keyword match)\n\n# Without keyword_search=True, only the most relevant memories would be returned:\n# - \"Vegetarian. Allergic to nuts.\" (highly relevant)\n# - \"Prefers spicy food and enjoys Thai cuisine\" (relevant)\n# The keyword-based match about \"sea food\" would be excluded\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running the Mastra Instance with Mem0 Agent in TypeScript\nDESCRIPTION: Initializes the main Mastra application instance using `Mastra` from `@mastra/core/mastra`. It registers the `mem0Agent` (which includes the Mem0 memory tools) within the `agents` configuration. A basic logger is also configured using `createLogger` from `@mastra/core/logger`. This sets up the environment to run the agent.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-mastra.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n index.ts\nimport { Mastra } from '@mastra/core/mastra';\nimport { createLogger } from '@mastra/core/logger';\n\nimport { mem0Agent } from './agents';\n\nexport const mastra = new Mastra({\n  agents: { mem0Agent },\n  logger: createLogger({\n    name: 'Mastra',\n    level: 'error',\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving Answers with Citations using Embedchain Python\nDESCRIPTION: This snippet demonstrates how to use the Embedchain `App` to ask a question over a data source and retrieve both the answer and its supporting citations by setting `citations=True` in the `chat()` method. It requires the `embedchain` package, and the main input is a user query string. The output consists of the answer and a list of tuples containing the source chunk and metadata such as URL and relevance score. This scenario is ideal for use cases where source transparency is required; by default, the citations parameter is `False`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/chat.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\n# Initialize app\\napp = App()\\n\\n# Add data source\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\n# Get relevant answer for your query\\nanswer, sources = app.chat(\\\"What is the net worth of Elon?\\\", citations=True)\\nprint(answer)\\n# Answer: The net worth of Elon Musk is $221.9 billion.\\n\\nprint(sources)\\n# [\\n#    (\\n#        'Elon Musk PROFILEElon MuskCEO, Tesla$247.1B$2.3B (0.96%)Real Time Net Worthas of 12/7/23 ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.89,\\n#           ...\\n#        }\\n#    ),\\n#    (\\n#        '74% of the company, which is now called X.Wealth HistoryHOVER TO REVEAL NET WORTH BY YEARForbes ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.81,\\n#           ...\\n#        }\\n#    ),\\n#    (\\n#        'founded in 2002, is worth nearly $150 billion after a $750 million tender offer in June 2023 ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.73,\\n#           ...\\n#        }\\n#    )\\n# ]\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Store with PGVector Configuration\nDESCRIPTION: Demonstrates how to configure and initialize a Memory instance using pgvector as the vector store. The example shows setting up database credentials, environment variables, and adding conversation messages to the memory store.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/pgvector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"pgvector\",\n        \"config\": {\n            \"user\": \"test\",\n            \"password\": \"123\",\n            \"host\": \"127.0.0.1\",\n            \"port\": \"5432\",\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 with Ollama in Python\nDESCRIPTION: This code snippet demonstrates how to configure Mem0 to use Ollama for both embedding and language models, and how to initialize Memory, add a memory, and retrieve memories. It includes configuration for the vector store (Qdrant), LLM (Ollama with llama3.1), and embedder (Ollama with nomic-embed-text).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-with-ollama.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 768,  # Change this according to your local model's dimensions\n        },\n    },\n    \"llm\": {\n        \"provider\": \"ollama\",\n        \"config\": {\n            \"model\": \"llama3.1:latest\",\n            \"temperature\": 0,\n            \"max_tokens\": 2000,\n            \"ollama_base_url\": \"http://localhost:11434\",  # Ensure this URL is correct\n        },\n    },\n    \"embedder\": {\n        \"provider\": \"ollama\",\n        \"config\": {\n            \"model\": \"nomic-embed-text:latest\",\n            # Alternatively, you can use \"snowflake-arctic-embed:latest\"\n            \"ollama_base_url\": \"http://localhost:11434\",\n        },\n    },\n}\n\n# Initialize Memory with the configuration\nm = Memory.from_config(config)\n\n# Add a memory\nm.add(\"I'm visiting Paris\", user_id=\"john\")\n\n# Retrieve memories\nmemories = m.get_all(user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating AsyncMemoryClient in Python\nDESCRIPTION: This snippet shows how to instantiate and use the AsyncMemoryClient for asynchronous operations in Python. It imports the necessary libraries, sets the API key, creates an AsyncMemoryClient instance, and demonstrates how to call the `add` method within an asynchronous function. The example shows how to add memory and print the response.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import AsyncMemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = AsyncMemoryClient()\n\n\nasync def main():\n    response = await client.add(\"I'm travelling to SF\", user_id=\"john\")\n    print(response)\n\nawait main()\n```\n\n----------------------------------------\n\nTITLE: Script Entry Point and Async Event Loop Handling - Python\nDESCRIPTION: This snippet triggers the `main` coroutine when the script is run as the main module, leveraging Python's asyncio event loop to launch the interactive voice agent. It ensures that the asynchronous workflow for capturing audio and interacting with the agent is initiated only when the script is directly executed, not when imported as a module.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n----------------------------------------\n\nTITLE: Implementing SupportChatbot Class with Mem0 and Claude\nDESCRIPTION: Defines the SupportChatbot class that uses Mem0 for memory management and Anthropic's Claude for response generation. Includes methods for initialization, storing customer interactions, retrieving relevant conversation history, and handling customer queries.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/customer-support-chatbot.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SupportChatbot:\n    def __init__(self):\n        # Initialize Mem0 with Anthropic's Claude\n        self.config = {\n            \"llm\": {\n                \"provider\": \"anthropic\",\n                \"config\": {\n                    \"model\": \"claude-3-5-sonnet-latest\",\n                    \"temperature\": 0.1,\n                    \"max_tokens\": 2000,\n                }\n            }\n        }\n        self.client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        self.memory = Memory.from_config(self.config)\n\n        # Define support context\n        self.system_context = \"\"\"\n        You are a helpful customer support agent. Use the following guidelines:\n        - Be polite and professional\n        - Show empathy for customer issues\n        - Reference past interactions when relevant\n        - Maintain consistent information across conversations\n        - If you're unsure about something, ask for clarification\n        - Keep track of open issues and follow-ups\n        \"\"\"\n\n    def store_customer_interaction(self,\n                                 user_id: str,\n                                 message: str,\n                                 response: str,\n                                 metadata: Dict = None):\n        \"\"\"Store customer interaction in memory.\"\"\"\n        if metadata is None:\n            metadata = {}\n\n        # Add timestamp to metadata\n        metadata[\"timestamp\"] = datetime.now().isoformat()\n\n        # Format conversation for storage\n        conversation = [\n            {\"role\": \"user\", \"content\": message},\n            {\"role\": \"assistant\", \"content\": response}\n        ]\n\n        # Store in Mem0\n        self.memory.add(\n            conversation,\n            user_id=user_id,\n            metadata=metadata\n        )\n\n    def get_relevant_history(self, user_id: str, query: str) -> List[Dict]:\n        \"\"\"Retrieve relevant past interactions.\"\"\"\n        return self.memory.search(\n            query=query,\n            user_id=user_id,\n            limit=5  # Adjust based on needs\n        )\n\n    def handle_customer_query(self, user_id: str, query: str) -> str:\n        \"\"\"Process customer query with context from past interactions.\"\"\"\n\n        # Get relevant past interactions\n        relevant_history = self.get_relevant_history(user_id, query)\n\n        # Build context from relevant history\n        context = \"Previous relevant interactions:\\n\"\n        for memory in relevant_history:\n            context += f\"Customer: {memory['memory']}\\n\"\n            context += f\"Support: {memory['memory']}\\n\"\n            context += \"---\\n\"\n\n        # Prepare prompt with context and current query\n        prompt = f\"\"\"\n        {self.system_context}\n\n        {context}\n\n        Current customer query: {query}\n\n        Provide a helpful response that takes into account any relevant past interactions.\n        \"\"\"\n\n        # Generate response using Claude\n        response = self.client.messages.create(\n            model=\"claude-3-5-sonnet-latest\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=2000,\n            temperature=0.1\n        )\n\n        # Store interaction\n        self.store_customer_interaction(\n            user_id=user_id,\n            message=query,\n            response=response,\n            metadata={\"type\": \"support_query\"}\n        )\n\n        return response.content[0].text\n```\n\n----------------------------------------\n\nTITLE: Prewarming Voice Model and Entrypoint Setup (Python)\nDESCRIPTION: These functions handle pre-initialization of the Silero voice activity detection (VAD) model and define the main entrypoint for the agent's lifecycle. The prewarm_process preloads VAD for optimal responsiveness, while entrypoint sets up room connection, loads system prompt, constructs the voice pipeline agent, and initiates the conversation with the user. Required dependencies include LiveKit, Silero, Deepgram, Mem0, and OpenAI plugins. Inputs are session/process contexts; outputs are joined rooms and running voice agents.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef prewarm_process(proc: JobProcess):\n    # Preload silero VAD in memory to speed up session start\n    proc.userdata[\"vad\"] = silero.VAD.load()\n\nasync def entrypoint(ctx: JobContext):\n    # Connect to LiveKit room\n    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)\n    \n    # Wait for participant\n    participant = await ctx.wait_for_participant()\n    \n    # Initialize Mem0 client\n    mem0 = AsyncMemoryClient()\n\n    # Define initial system context\n    initial_ctx = llm.ChatContext().append(\n        role=\"system\",\n        text=(\n            \"\"\"\n            You are a helpful voice assistant.\n            You are a travel guide named George and will help the user to plan a travel trip of their dreams. \n            You should help the user plan for various adventures like work retreats, family vacations or solo backpacking trips. \n            You should be careful to not suggest anything that would be dangerous, illegal or inappropriate.\n            You can remember past interactions and use them to inform your answers.\n            Use semantic memory retrieval to provide contextually relevant responses. \n            \"\"\"\n        ),\n    )\n\n    # Create VoicePipelineAgent with memory capabilities\n    agent = VoicePipelineAgent(\n        chat_ctx=initial_ctx,\n        vad=silero.VAD.load(),\n        stt=deepgram.STT(),\n        llm=openai.LLM(model=\"gpt-4o-mini\"),\n        tts=openai.TTS(),\n        before_llm_cb=_enrich_with_memory,\n    )\n\n    # Start agent and initial greeting\n    agent.start(ctx.room, participant)\n    await agent.say(\n        \"Hello! I'm George. Can I help you plan an upcoming trip? \",\n        allow_interruptions=True\n    )\n\n# Run the application\nif __name__ == \"__main__\":\n    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, prewarm_fnc=prewarm_process))\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using User and Agent ID Filters in JavaScript\nDESCRIPTION: This JavaScript code snippet performs a search for memories, filtering by user_id and agent_id. It defines a query and a filters object, using an AND operator to combine the user_id and agent_id criteria.  The client.search method is called with the query and an object including the version and filters. A promise is used to handle the asynchronous result.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_27\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \"What do you know about me?\";\nconst filters = {\n   \"AND\":[\n      {\n         \"user_id\":\"alex\"\n      },\n      {\n         \"agent_id\":{\n            \"in\":[\n               \"travel-assistant\",\n               \"customer-support\"\n            ]\n         }\n      }\n   ]\n};\nclient.search(query, { version: \"v2\", filters })\n    .then(results => console.log(results))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Embedchain App Configuration in Python\nDESCRIPTION: This Python dictionary demonstrates an alternative format for configuring an Embedchain application. It is equivalent to the YAML and JSON configurations, defining settings for the LLM, vector database, embedder, chunker, cache, and memory components. The dictionary includes model specifications, API keys, and proxy settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/advanced/configuration.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    'app': {\n        'config': {\n            'name': 'full-stack-app'\n        }\n    },\n    'llm': {\n        'provider': 'openai',\n        'config': {\n            'model': 'gpt-4o-mini',\n            'temperature': 0.5,\n            'max_tokens': 1000,\n            'top_p': 1,\n            'stream': False,\n            'prompt': (\n                \"Use the following pieces of context to answer the query at the end.\\n\"\n                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n                \"$context\\n\\nQuery: $query\\n\\nHelpful Answer:\"\n            ),\n            'system_prompt': (\n                \"Act as William Shakespeare. Answer the following questions in the style of William Shakespeare.\"\n            ),\n            'api_key': 'sk-xxx',\n            \"model_kwargs\": {\"response_format\": {\"type\": \"json_object\"}},\n            \"http_client_proxies\": \"http://testproxy.mem0.net:8000\",\n        }\n    },\n    'vectordb': {\n        'provider': 'chroma',\n        'config': {\n            'collection_name': 'full-stack-app',\n            'dir': 'db',\n            'allow_reset': True\n        }\n    },\n    'embedder': {\n        'provider': 'openai',\n        'config': {\n            'model': 'text-embedding-ada-002',\n            'api_key': 'sk-xxx',\n            \"http_client_proxies\": \"http://testproxy.mem0.net:8000\",\n        }\n    },\n    'chunker': {\n        'chunk_size': 2000,\n        'chunk_overlap': 100,\n        'length_function': 'len',\n        'min_chunk_size': 0\n    },\n    'cache': {\n        'similarity_evaluation': {\n            'strategy': 'distance',\n            'max_distance': 1.0,\n        },\n        'config': {\n            'similarity_threshold': 0.8,\n            'auto_flush': 50,\n        },\n    },\n    'memory': {\n        'top_k': 10,\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LiveKit Agents and Plugins (Bash)\nDESCRIPTION: This bash snippet installs the primary LiveKit Agents SDK, as well as the required voice plugins including Silero, Deepgram, and OpenAI. These dependencies are necessary for audio input/output, transcription, and integration with advanced LLM capabilities in your Python project. Ensure your environment is activated appropriately before running these installation commands. Inputs are the installation commands themselves, and outputs are updated site packages in your Python environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install livekit \\\nlivekit-agents \\\nlivekit-plugins-silero \\\nlivekit-plugins-deepgram \\\nlivekit-plugins-openai\n```\n\n----------------------------------------\n\nTITLE: Implementing Add to Memory Function for Mem0 Integration\nDESCRIPTION: This Python function adds a message to Mem0 memory, using the provided content and user context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def add_to_memory(\n    context: RunContextWrapper[Mem0Context],\n    content: str,\n) -> str:\n    \"\"\"\n    Add a message to Mem0\n    Args:\n        content: The content to store in memory.\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": content}]\n    user_id = context.context.user_id or \"default_user\"\n    await client.add(messages, user_id=user_id)\n    return f\"Stored message: {content}\"\n```\n\n----------------------------------------\n\nTITLE: Adding a Self-Identity Memory in Python and TypeScript\nDESCRIPTION: Shows how to add a memory about the user's own identity, which establishes a core node in the memory graph.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"My name is Alice\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"My name is Alice\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Loop and Voice Processing\nDESCRIPTION: Main function that orchestrates the entire process, including creating the agent, setting up the voice pipeline, handling user input, processing audio, and managing agent responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    # Create the agent\n    agent = create_memory_voice_agent()\n    \n    # Set up the voice pipeline\n    pipeline = VoicePipeline(\n        workflow=SingleAgentVoiceWorkflow(agent)\n    )\n    \n    # Configure TTS settings\n    pipeline.config.tts_settings.voice = \"alloy\"\n    pipeline.config.tts_settings.speed = 1.0\n    \n    try:\n        while True:\n            # Get user input\n            print(\"\\nPress Enter to start recording (or 'q' to quit)...\")\n            user_input = input()\n            if user_input.lower() == 'q':\n                break\n            \n            # Record and process audio\n            audio_data = await record_from_microphone(duration=5)\n            audio_input = AudioInput(buffer=audio_data)\n            result = await pipeline.run(audio_input)\n            \n            # Play response and handle events\n            player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n            player.start()\n            \n            agent_response = \"\"\n            print(\"\\nAgent response:\")\n            \n            async for event in result.stream():\n                if event.type == \"voice_stream_event_audio\":\n                    player.write(event.data)\n                elif event.type == \"voice_stream_event_content\":\n                    content = event.data\n                    agent_response += content\n                    print(content, end=\"\", flush=True)\n            \n            # Save the agent's response to memory\n            if agent_response:\n                try:\n                    await mem0_client.add(\n                        f\"Agent response: {agent_response}\", \n                        user_id=USER_ID,\n                        metadata={\"type\": \"agent_response\"}\n                    )\n                except Exception as e:\n                    print(f\"Failed to store memory: {e}\")\n    \n    except KeyboardInterrupt:\n        print(\"\\nExiting...\")\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryClient for Exclusion Rules (Mem0 Python)\nDESCRIPTION: Demonstrates setting up the MemoryClient with configuration to omit specified types of information from storage. This variant uses the 'excludes' parameter to define what topics should be left out (e.g., 'food preferences'). After initializing and providing the conversation messages, the add method stores memories, excluding any that match the exclusion rule. Requires: mem0 Python library and an API key. Parameters: 'excludes' indicates the types of data to omit. Input: conversation and exclusion directive; Output: memories stored minus excluded topics.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/selective-memory.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import MemoryClient\\n\\nm = MemoryClient(api_key=\\\"xxx\\\")\\n\\n# Define what to exclude\\nexcludes = \\\"food preferences\\\"\\n\\nmessages = [\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hi, my name is Alice and I love to play badminton\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Nice to meet you, Alice! Badminton is a great sport.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love music festivals\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Music festivals are exciting! Do you have a favorite one?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love eating spicy food\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Spicy food is delicious! What's your favorite spicy dish?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love playing baseball with my friends\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Baseball with friends sounds fun!\\\"},\\n]\\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclient.add(messages, user_id=\\\"alice\\\", excludes=excludes)\n```\n\nLANGUAGE: json\nCODE:\n```\nUser's name is Alice.\\nAlice loves to play badminton.\\nLoves music festivals.\\nUser loves playing baseball with friends.\\n\n```\n\n----------------------------------------\n\nTITLE: Adding a Local Image via Base64 Encoding to Mem0 using Python\nDESCRIPTION: This Python snippet demonstrates adding a local image file to Mem0 by first encoding it into a Base64 string. It reads the image file in binary mode, encodes it using `base64.b64encode`, decodes the result to UTF-8, constructs a data URL string (`data:image/jpeg;base64,...`), creates the message object with this data URL, and finally calls `client.add`. Requires the `base64` module and assumes `client` is initialized.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Path to the image file\nimage_path = \"path/to/your/image.jpg\"\n\n# Encode the image in Base64\nwith open(image_path, \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n# Create the message dictionary with the Base64-encoded image\nimage_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"image_url\",\n        \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n        }\n    }\n}\nclient.add([image_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions for Context and Memory Operations - Python\nDESCRIPTION: Provides three helper functions: one to retrieve relevant context from Mem0 given a query and user ID, one to generate AI responses using the current prompt and context, and one to save completed interactions back to Mem0 for persistent memory. All functions rely on initialized Mem0 and LangChain objects. Inputs and outputs are documented via function signatures and docstrings. Limitations include the need for valid user IDs and correct API key setup.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve_context(query: str, user_id: str) -> List[Dict]:\n    \"\"\"Retrieve relevant context from Mem0\"\"\"\n    memories = mem0.search(query, user_id=user_id)\n    seralized_memories = ' '.join([mem[\"memory\"] for mem in memories])\n    context = [\n        {\n            \"role\": \"system\", \n            \"content\": f\"Relevant information: {seralized_memories}\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": query\n        }\n    ]\n    return context\n\ndef generate_response(input: str, context: List[Dict]) -> str:\n    \"\"\"Generate a response using the language model\"\"\"\n    chain = prompt | llm\n    response = chain.invoke({\n        \"context\": context,\n        \"input\": input\n    })\n    return response.content\n\ndef save_interaction(user_id: str, user_input: str, assistant_response: str):\n    \"\"\"Save the interaction to Mem0\"\"\"\n    interaction = [\n        {\n          \"role\": \"user\",\n          \"content\": user_input\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": assistant_response\n        }\n    ]\n    mem0.add(interaction, user_id=user_id)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Memory in JavaScript\nDESCRIPTION: This JavaScript snippet illustrates how to add memory for AI agents for consistent responses across multiple sessions. The `add` method of the MemoryClient is used along with `agent_id`. The messages are sent to the agent and stored based on the `agent_id` parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst messages = [\n    {\"role\": \"system\", \"content\": \"You are an AI tutor with a personality. Give yourself a name for the user.\"},\n    {\"role\": \"assistant\", \"content\": \"Understood. I'm an AI tutor with a personality. My name is Alice.\"}\n];\nclient.add(messages, { agent_id: \"ai-tutor\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Chroma Vector Store with mem0\nDESCRIPTION: This code demonstrates how to initialize a Memory instance with Chroma vector store, and add conversation messages to it. It requires setting an OpenAI API key as an environment variable and configuring the vector store with collection name and path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/chroma.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"chroma\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"path\": \"db\",\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Querying Memories with AsyncMemory Search - Python\nDESCRIPTION: This snippet shows how to retrieve relevant memory entries for a specific query and user using the AsyncMemory search method. It requires the query string and user_id as parameters and returns search results asynchronously. Intended for use within async/await contexts and leveraging fast, concurrent access to stored memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.search(\\n    query=\"Where am I travelling?\",\\n    user_id=\"alice\"\\n)\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Metadata and Categories Filters in Python\nDESCRIPTION: This Python code snippet demonstrates how to search memories using metadata and categories filters. It defines a query and a filters dictionary, using an AND operator to combine the metadata and categories conditions. The 'contains' operator is used to check if the categories field contains a specific value. The client.search method is called with the query, version='v2', and the defined filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"What do you know about me?\"\nfilters = {\n    \"AND\": [\n        {\"metadata\": {\"food\": \"vegan\"}},\n        {\n         \"categories\":{\n            \"contains\": \"food_preferences\"\n         }\n    }\n    ]\n}\nclient.search(query, version=\"v2\", filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Storing a Memory - Result Sample - JSON\nDESCRIPTION: Provides a sample output as returned from the 'add' operation of Memory. Output is a list of memories added or extracted from the provided messages, each with an id, extracted memory string, and category metadata. Used for reference; not executable.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n      \"memory\": \"User is planning to watch a movie tonight.\",\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      }\n    },\n    {\n      \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n      \"memory\": \"User is not a big fan of thriller movies.\",\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      }\n    },\n    {\n      \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n      \"memory\": \"User loves sci-fi movies.\",\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Memories to Enhance Context in Mem0\nDESCRIPTION: Demonstrates how to add memories to a user's profile using the addMemories function with structured conversation messages. These memories will be used for future context retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelV1Prompt } from \"ai\";\nimport { addMemories } from \"@mem0/vercel-ai-provider\";\n\nconst messages: LanguageModelV1Prompt = [\n  {\n    role: \"user\",\n    content: [\n      { type: \"text\", text: \"I love red cars.\" },\n      { type: \"text\", text: \"I like Toyota Cars.\" },\n      { type: \"text\", text: \"I prefer SUVs.\" },\n    ],\n  },\n];\n\nawait addMemories(messages, { user_id: \"borat\" });\n```\n\n----------------------------------------\n\nTITLE: Setting up Mem0 Client for Document Editing in Python\nDESCRIPTION: This snippet demonstrates how to set up the Mem0 client for document editing. It imports necessary modules, sets the API key, initializes the client, and defines constants for user and session identification.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/document-writing.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\n# Set up Mem0 client\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\nclient = MemoryClient()\n\n# Define constants\nUSER_ID = \"content_writer\"\nRUN_ID = \"smart_editing_session\"\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search with Embedchain App in Python\nDESCRIPTION: Demonstrates how to use the search() method of the Embedchain App to perform a semantic search query. The result includes relevant context and metadata for each search result.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/semantic-search.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp.search(\"Summarize the features of Next.js 14?\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Relationship Memory about a Friend in Python and TypeScript\nDESCRIPTION: Demonstrates adding a complex memory that establishes relationships between multiple entities (the user, John, and Tommy the dog).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"My friend name is john and john has a dog named tommy\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"My friend name is john and john has a dog named tommy\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Discord Channel Data Source to Embedchain App - Python\nDESCRIPTION: This snippet demonstrates how to configure an Embedchain app to source messages from a Discord channel using its channel ID. The user must set a Discord bot token in the environment variable \"DISCORD_TOKEN\" with read permissions. After adding the Discord channel as a data source using the \"add\" method (specifying the channel ID and \"discord\" as data_type), the app can be queried and will return relevant information from the channel. Inputs include the Discord channel ID and optionally a query string, producing natural language answers based on channel content. Limitations include the need for a valid bot token and access to the specified channel.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/discord.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# add your discord \"BOT\" token\nos.environ[\"DISCORD_TOKEN\"] = \"xxx\"\n\napp = App()\n\napp.add(\"1177296711023075338\", data_type=\"discord\")\n\nresponse = app.query(\"What is Joe saying about Elon Musk?\")\n\nprint(response)\n# Answer: Joe is saying \"Elon Musk is a genius\".\n```\n\n----------------------------------------\n\nTITLE: Advanced Semantic Search with Metadata Filtering via 'where' in Embedchain Python\nDESCRIPTION: This code example shows how to use the Embedchain Python API to perform semantic searches with additional metadata-based filtering using the `where` parameter. It configures the App to use Pinecone as the vector database provider, requiring Pinecone API credentials set via the `PINECONE_API_KEY` environment variable. Two documents are indexed with different metadata, and a search is performed for Bill Gates with results filtered only for those tagged with the specified person. Dependencies include the `embedchain` Python package and a valid Pinecone account.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/search.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nfrom embedchain import App\\n\\nos.environ[\\\"PINECONE_API_KEY\\\"] = \\\"xxx\\\"\\n\\nconfig = {\\n    \\\"vectordb\\\": {\\n        \\\"provider\\\": \\\"pinecone\\\",\\n        \\\"config\\\": {\\n            \\\"metric\\\": \\\"dotproduct\\\",\\n            \\\"vector_dimension\\\": 1536,\\n            \\\"index_name\\\": \\\"ec-test\\\",\\n            \\\"serverless_config\\\": {\\\"cloud\\\": \\\"aws\\\", \\\"region\\\": \\\"us-west-2\\\"},\\n        },\\n    }\\n}\\n\\napp = App.from_config(config=config)\\n\\napp.add(\\\"https://www.forbes.com/profile/bill-gates\\\", metadata={\\\"type\\\": \\\"forbes\\\", \\\"person\\\": \\\"gates\\\"})\\napp.add(\\\"https://en.wikipedia.org/wiki/Bill_Gates\\\", metadata={\\\"type\\\": \\\"wiki\\\", \\\"person\\\": \\\"gates\\\"})\\n\\nresults = app.search(\\\"What is the net worth of Bill Gates?\\\", where={\\\"person\\\": \\\"gates\\\"})\\nprint(\\\"Num of search results: \", len(results))\n```\n\n----------------------------------------\n\nTITLE: Adding Memories to Mem0 Using addMemories (TypeScript)\nDESCRIPTION: Shows how to use addMemories to store a user's prompt/messages for persistent memory. Requires LanguageModelV1Prompt from 'ai' and addMemories from @mem0/vercel-ai-provider. Parameters include messages (structured as LanguageModelV1Prompt) and options such as user_id. Outputs a promise; enables context-aware AI responses. Ensure valid API keys are provided.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LanguageModelV1Prompt } from \"ai\";\nimport { addMemories } from \"@mem0/vercel-ai-provider\";\n\nconst messages: LanguageModelV1Prompt = [\n  { role: \"user\", content: [{ type: \"text\", text: \"I love red cars.\" }] },\n];\n\nawait addMemories(messages, { user_id: \"borat\" });\n```\n\n----------------------------------------\n\nTITLE: Using retrieveMemories Tool in ElevenLabs Agent Prompts (plaintext)\nDESCRIPTION: Shows an example system prompt for the agent to check for relevant prior discussion by calling retrieveMemories at the start of each conversation turn. This ensures the agent brings context from previous sessions into the current interaction.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nAt the beginning of each conversation turn, use retrieveMemories to check if we've \ndiscussed this topic before or if the user has shared relevant preferences.\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data Sources for Semantic Search with Embedchain in Python\nDESCRIPTION: Adds Next.js website and forum data to the Embedchain App using sitemap URLs. This step incorporates over 15,000 pages into the search pipeline.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/semantic-search.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Add Next.JS Website and docs\napp.add(\"https://nextjs.org/sitemap.xml\", data_type=\"sitemap\")\n\n# Add Next.JS Forum data\napp.add(\"https://nextjs-forum.com/sitemap.xml\", data_type=\"sitemap\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for ElevenLabs and Mem0 (Python)\nDESCRIPTION: Imports Python standard libraries for system and OS interaction, as well as ElevenLabs and Mem0 client modules. Enables signal handling, environment variable access, asynchronous memory operations, audio interface handling, and registration of conversational tools. Required as the preamble to the main application to ensure availability of all dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport signal\nimport sys\nfrom mem0 import AsyncMemoryClient\n\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation\nfrom elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface\nfrom elevenlabs.conversational_ai.conversation import ClientTools\n```\n\n----------------------------------------\n\nTITLE: Configuring the LangGraph Structure in Python\nDESCRIPTION: Configures the workflow of the conversational agent using the initialized `StateGraph` instance. It adds the `chatbot` function as a node named \"chatbot\". It defines the conversation flow by setting the entry point (`START`) to the \"chatbot\" node and creating an edge from \"chatbot\" back to itself, enabling continuous conversation cycles. Finally, it compiles the graph into an executable object.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngraph.add_node(\"chatbot\", chatbot)\ngraph.add_edge(START, \"chatbot\")\ngraph.add_edge(\"chatbot\", \"chatbot\")\n\ncompiled_graph = graph.compile()\n```\n\n----------------------------------------\n\nTITLE: Searching and Retrieving Relevant Memories from Mem0 in Python\nDESCRIPTION: Searches the Mem0 database for memories relevant to a given command, enabling context-aware browsing or recommendations. Uses the 'search' method with a natural language command, specifying a user ID and a result limit. Inputs: command string, user_id, limit (defaults in example to 3). Output: concatenated textual memories relevant to the query, printed to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncommand = \"Find papers on arxiv that I should read based on my interests.\"\n\nrelevant_memories = memory.search(command, user_id=USER_ID, limit=3)\nrelevant_memories_text = '\\n'.join(mem['text'] for mem in relevant_memories)\nprint(f\"Relevant memories:\")\nprint(relevant_memories_text)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory History - TypeScript\nDESCRIPTION: Retrieves full change history for a specific memory by ID, including all changes (add, update) with timestamps and action types. The Memory.history call outputs an array of history entries. Useful for audit trails or undo/redo functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconst history = await memory.history('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');\nconsole.log(history);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI LLM with YAML in Embedchain\nDESCRIPTION: Example showing how to configure OpenAI LLM parameters using a YAML config file. This approach allows for detailed configuration of model parameters like temperature, token limits, and model selection.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'xxx'\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: 'gpt-4o-mini'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Fetching Memories from Personalized AI Tutor\nDESCRIPTION: This code snippet demonstrates how to retrieve all memories associated with a specific user ID from the PersonalAITutor instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-ai-tutor.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmemories = ai_tutor.get_memories(user_id=user_id)\nfor m in memories['results']:\n    print(m['memory'])\n```\n\n----------------------------------------\n\nTITLE: Updating Custom Instructions in Python\nDESCRIPTION: This snippet demonstrates how to update custom instructions for a project using Python. It sets specific guidelines for extracting health-related information from conversations, including medical conditions, medications, diet, doctor visits, and health metrics.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-instructions.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Update custom instructions\nprompt =\"\"\"\nYour Task: Extract ONLY health-related information from conversations, focusing on the following areas:\n\n1. Medical Conditions, Symptoms, and Diagnoses:\n   - Illnesses, disorders, or symptoms (e.g., fever, diabetes).\n   - Confirmed or suspected diagnoses.\n\n2. Medications, Treatments, and Procedures:\n   - Prescription or OTC medications (names, dosages).\n   - Treatments, therapies, or medical procedures.\n\n3. Diet, Exercise, and Sleep:\n   - Dietary habits, fitness routines, and sleep patterns.\n\n4. Doctor Visits and Appointments:\n   - Past, upcoming, or regular medical visits.\n\n5. Health Metrics:\n   - Data like weight, BP, cholesterol, or sugar levels.\n\nGuidelines:\n- Focus solely on health-related content.\n- Maintain clarity and context accuracy while recording.\n\"\"\"\nresponse = client.update_project(custom_instructions=prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Metadata and Categories Filters in JavaScript\nDESCRIPTION: This JavaScript code snippet searches for memories using filters based on both metadata and categories.  It defines a query and a filters object, using the 'contains' operator on the 'categories' field. The client.search method is called with the query and an object including the version and filters. A promise handles the asynchronous result.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_33\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \"What do you know about me?\";\nconst filters = {\n    \"AND\": [\n        {\"metadata\": {\"food\": \"vegan\"}},\n        {\n            \"categories\": {\n                \"contains\": \"food_preferences\"\n            }\n        }\n    ]\n};\n\nclient.search(query, { version: \"v2\", filters })\n    .then(results => console.log(results))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 and OpenAI Clients in JavaScript\nDESCRIPTION: This code snippet shows how to configure the Mem0 and OpenAI clients using the environment variables. It sets up the necessary configuration for interacting with both APIs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst mem0Config = {\n    apiKey: process.env.MEM0_API_KEY,\n    user_id: \"sample-user\",\n};\n\nconst openAIClient = new OpenAI();\nconst mem0Client = new MemoryClient(mem0Config);\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Date Filters in JavaScript\nDESCRIPTION: This JavaScript code snippet searches for memories based on a date range using the 'created_at' field. It defines a query and a filters object, utilizing 'gte' and 'lte' operators to specify the date range. The client.search method is called with the query and an object containing the version and filters.  The asynchronous result is handled with a promise.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_30\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \"What do you know about me?\";\nconst filters = {\n  \"AND\": [\n    {\"created_at\": {\"gte\": \"2024-07-20\", \"lte\": \"2024-07-10\"}},\n    {\"user_id\": \"alex\"}\n  ]\n};\n\nclient.search(query, { version: \"v2\", filters })\n  .then(results => console.log(results))\n  .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Querying LLAMA2 Embedchain App\nDESCRIPTION: Implements a loop for continuously querying the LLAMA2 Embedchain app. Users can input questions, and the app provides answers based on the added data sources.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/llama2.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0MemoryService in Pipecat (Python)\nDESCRIPTION: Demonstrates how to instantiate Mem0MemoryService for use in the Pipecat conversational pipeline. This snippet sets up all required parameters, such as API key and identifiers for users, agents, and sessions. Optional params allow fine-tuning of memory retrieval logic, including search limits, thresholds, custom prompts, and context insertion positions. This code must be placed and executed within your Python application setup.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pipecat.services.mem0 import Mem0MemoryService\n\nmemory = Mem0MemoryService(\n    api_key=os.getenv(\"MEM0_API_KEY\"),  # Your Mem0 API key\n    user_id=\"unique_user_id\",           # Unique identifier for the end user\n    agent_id=\"my_agent\",                # Identifier for the agent using the memory\n    run_id=\"session_123\",               # Optional: specific conversation session ID\n    params={                            # Optional: configuration parameters\n        \"search_limit\": 10,             # Maximum memories to retrieve per query\n        \"search_threshold\": 0.1,        # Relevance threshold (0.0 to 1.0)\n        \"system_prompt\": \"Here are your past memories:\", # Custom prefix for memories\n        \"add_as_system_message\": True,  # Add memories as system (True) or user (False) message\n        \"position\": 1,                  # Position in context to insert memories\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Response after adding memories in Mem0 (JSON)\nDESCRIPTION: This JSON response illustrates the expected output after successfully adding memories to Mem0 with a user and agent ID. The response contains a list of results, each representing a stored memory. Each memory contains an `id`, memory `data`, and event type `ADD`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_17\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"c57abfa2-f0ac-48af-896a-21728dbcecee0\",\n            \"data\": {\"memory\": \"Travelling to San Francisco\"},\n            \"event\": \"ADD\"\n        },\n        {\n            \"id\": \"0e8c003f-7db7-426a-9fdc-a46f9331a0c2\",\n            \"data\": {\"memory\": \"Going to Dubai next month\"},\n            \"event\": \"ADD\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: OpenAI Function Calling with Python Function in Embedchain\nDESCRIPTION: Example showing how to define a normal Python function for use with OpenAI function calling. This demonstrates defining a function with typed arguments and documentation for the LLM to utilize.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers together.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n----------------------------------------\n\nTITLE: Searching Memories by Query with Mem0 in Python\nDESCRIPTION: Demonstrates the use of the search() method to retrieve relevant memories for a user given a query string. Returns a ranked list of scored memory matches, each with metadata and scoring fields. Facilitates contextual lookups and personalized information retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrelated_memories = m.search(query=\"What do you know about me?\", user_id=\"alice\")\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n      \"memory\": \"User is planning to watch a movie tonight.\",\n      \"hash\": \"1a271c007316c94377175ee80e746a19\",\n      \"created_at\": \"2025-02-27T16:33:20.557Z\",\n      \"updated_at\": None,\n      \"score\": 0.38920719231944799,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"user_id\": \"alice\"\n    },\n    {\n      \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n      \"memory\": \"User loves sci-fi movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"created_at\": \"2025-02-27T16:33:20.560Z\",\n      \"updated_at\": None,\n      \"score\": 0.36869761478135689,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"user_id\": \"alice\"\n    },\n    {\n      \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n      \"memory\": \"User is not a big fan of thriller movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"created_at\": \"2025-02-27T16:33:20.560Z\",\n      \"updated_at\": None,\n      \"score\": 0.33855272141248272,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"user_id\": \"alice\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Upstash Vector Embeddings in Python\nDESCRIPTION: This snippet demonstrates how to configure and initialize a Memory object using Upstash Vector with its built-in embedding models. It requires setting environment variables for Upstash Vector credentials and enabling embeddings in the configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/upstash-vector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"UPSTASH_VECTOR_REST_URL\"] = \"...\"\nos.environ[\"UPSTASH_VECTOR_REST_TOKEN\"] = \"...\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"upstash_vector\",\n        \"enable_embeddings\": True,\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Likes to play cricket on weekends\", user_id=\"alice\", metadata={\"category\": \"hobbies\"})\n```\n\n----------------------------------------\n\nTITLE: Searching Memories (Node.js, JavaScript)\nDESCRIPTION: Retrieves relevant memories stored in Mem0 by issuing a search query string with optional filters such as user_id, agent_id, or run_id. The method returns a list of memories sorted by relevance. Requires prior storage of memories and a properly initialized Memory client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Search memories\nconst results = await m.search({ query: \"fact\", user_id: \"user123\" });\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Bedrock LLM Provider\nDESCRIPTION: This code snippet and YAML configuration show how to set up AWS Bedrock as the LLM provider for Embedchain. It includes setting the AWS region and configuring model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"AWS_REGION\"] = \"us-west-2\"\n\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: aws_bedrock\n  config:\n    model: amazon.titan-text-express-v1\n    # check notes below for model_kwargs\n    model_kwargs:\n      temperature: 0.5\n      topP: 1\n      maxTokenCount: 1000\n```\n\n----------------------------------------\n\nTITLE: Integrating LangChain Vector Store with Mem0 in TypeScript\nDESCRIPTION: Example of initializing and configuring a LangChain memory vector store with Mem0 in TypeScript. Demonstrates setup of OpenAI embeddings, vector store configuration, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/langchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0ai\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore as LangchainMemoryStore } from \"langchain/vectorstores/memory\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new LangchainVectorStore(embeddings);\n\nconst config = {\n    \"vector_store\": {\n        \"provider\": \"langchain\",\n        \"config\": { \"client\": vectorStore }\n    }\n}\n\nconst memory = new Memory(config);\n\nconst messages = [\n    { role: \"user\", content: \"I'm planning to watch a movie tonight. Any recommendations?\" },\n    { role: \"assistant\", content: \"How about a thriller movies? They can be quite engaging.\" },\n    { role: \"user\", content: \"I'm not a big fan of thriller movies but I love sci-fi movies.\" },\n    { role: \"assistant\", content: \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\" }\n]\n\nmemory.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Enabling Persistent Memory with Mem0 in Embedchain Python\nDESCRIPTION: This snippet demonstrates integrating Mem0 with Embedchain to provide persistent memory for chat sessions, allowing LLMs to recall and utilize past interactions for increased personalization. It requires the installation of the `mem0ai` package via pip and configuration of the `memory` parameter (such as `top_k` for memory retrieval). The configuration can be passed to `App.from_config()`, after which regular data sources can be added and chat invoked as usual. This enables advanced long-term memory capabilities for generative AI applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/chat.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\nconfig = {\\n  \\\"memory\\\": {\\n    \\\"top_k\\\": 5\\n  }\\n}\\n\\napp = App.from_config(config=config)\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\napp.chat(\\\"What is the net worth of Elon Musk?\\\")\n```\n\n----------------------------------------\n\nTITLE: Basic Memory Usage Example in TypeScript\nDESCRIPTION: Demonstrates how to create and configure a Memory instance with different levels of configuration, and how to perform basic memory operations like adding and searching memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0-ts\";\n\n// Create a memory instance with default OpenAI configuration\nconst memory = new Memory();\n\n// Or with minimal configuration (only API key)\nconst memory = new Memory({\n  embedder: {\n    config: {\n      apiKey: process.env.OPENAI_API_KEY,\n    },\n  },\n  llm: {\n    config: {\n      apiKey: process.env.OPENAI_API_KEY,\n    },\n  },\n});\n\n// Or with custom configuration\nconst memory = new Memory({\n  embedder: {\n    provider: \"openai\",\n    config: {\n      apiKey: process.env.OPENAI_API_KEY,\n      model: \"text-embedding-3-small\",\n    },\n  },\n  vectorStore: {\n    provider: \"memory\",\n    config: {\n      collectionName: \"custom-memories\",\n    },\n  },\n  llm: {\n    provider: \"openai\",\n    config: {\n      apiKey: process.env.OPENAI_API_KEY,\n      model: \"gpt-4-turbo-preview\",\n    },\n  },\n});\n\n// Add a memory\nawait memory.add(\"The sky is blue\", \"user123\");\n\n// Search memories\nconst results = await memory.search(\"What color is the sky?\", \"user123\");\n```\n\n----------------------------------------\n\nTITLE: Searching Memories with v2 API - Python\nDESCRIPTION: This code snippet demonstrates how to search memories using the v2 API with advanced filtering. It uses the `m.search` method, specifying a query and filters that include logical AND operations and the 'in' comparison operator. The filter targets memories belonging to 'alice' and created by either 'travel-agent' or 'sports-agent'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/v2-search-memories.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrelated_memories = m.search(\n    query=\"What are Alice's hobbies?\",\n    version=\"v2\",\n    filters={\n        \"AND\": [\n            {\n              \"user_id\": \"alice\"\n            },\n            {\n              \"agent_id\": {\"in\": [\"travel-agent\", \"sports-agent\"]}\n            }\n        ]\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a Memory about Playing Badminton in Python and TypeScript\nDESCRIPTION: Shows how to add a memory about enjoying badminton. This builds on the user's preference graph.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"I love to play badminton\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"I love to play badminton\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 MemoryClient and Setting API Key in Python\nDESCRIPTION: Imports required libraries (`StructuredTool`, `MemoryClient`, `BaseModel`, `Field`, etc.) and demonstrates initializing the `MemoryClient` for interacting with the Mem0 API. It requires setting the `MEM0_API_KEY` environment variable and providing `org_id` and `project_id`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import StructuredTool\nfrom mem0 import MemoryClient\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\nimport os\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient(\n    org_id=your_org_id,\n    project_id=your_project_id\n)\n```\n\n----------------------------------------\n\nTITLE: Composing OpenAI-Compatible Text and Image Messages in Mem0 (Python)\nDESCRIPTION: Demonstrates creating a message array that mixes both text and a Base64-encoded image using the OpenAI-compatible format. Base64 conversion is handled using the standard 'base64' module, and the composed message is added to memory with `add()`. The snippet ensures compatibility with APIs expecting separate content blocks for text and images.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Path to the image file\nimage_path = \"path/to/your/image.jpg\"\n\n# Encode the image in Base64\nwith open(image_path, \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n# Create the message using OpenAI-compatible format\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"What is in this image?\",\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n        },\n    ],\n}\n\n# Add the message to memory\nclient.add([message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Defining Travel Information Retrieval Agent with Memory Support in Python\nDESCRIPTION: Defines a flexible function 'get_travel_info' to automate web searches for travel details, optionally using recalled user preferences from Mem0's memory. If memory is enabled, performs a search for relevant memories and includes them in the browsing prompt; otherwise runs with question only. Inputs: question (str), use_memory (bool). Outputs: browser response message string from MultiOn. The code demonstrates usage of the function with and without memory on sample questions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_travel_info(question, use_memory=True):\n    if use_memory:\n        previous_memories = memory_client.search(question, user_id=USER_ID)\n        relevant_memories_text = \"\"\n        if previous_memories:\n            print(\"Using previous memories to enhance the search...\")\n            relevant_memories_text = '\\n'.join(mem[\"memory\"] for mem in previous_memories)\n        \n        command = \"Find travel information based on my interests:\"\n        prompt = f\"{command}\\n Question: {question} \\n My preferences: {relevant_memories_text}\"\n    else:\n        command = \"Find travel information based on my interests:\"\n        prompt = f\"{command}\\n Question: {question}\"\n    \n    print(\"Searching for travel information...\")    \n    browse_result = multion.browse(cmd=prompt)\n    return browse_result.message\n\n# Example usage\nquestion = \"Show me flight details for it.\"\nanswer_without_memory = get_travel_info(question, use_memory=False)\nanswer_with_memory = get_travel_info(question, use_memory=True)\n\nprint(\"Answer without memory:\", answer_without_memory)\nprint(\"Answer with memory:\", answer_with_memory)\n\n# Another example\nquestion = \"What is the best place to eat there?\"\nanswer_without_memory = get_travel_info(question, use_memory=False)\nanswer_with_memory = get_travel_info(question, use_memory=True)\n\nprint(\"Answer without memory:\", answer_without_memory)\nprint(\"Answer with memory:\", answer_with_memory)\n```\n\n----------------------------------------\n\nTITLE: Creating a Mastra Agent with Mem0 Tools in TypeScript\nDESCRIPTION: Creates a Mastra agent named 'Mem0 Agent' using the `Agent` class from `@mastra/core/agent`. It configures the agent with instructions, specifies the OpenAI GPT-4o model via `@ai-sdk/openai`, and equips it with the previously defined `mem0RememberTool` and `mem0MemorizeTool` to enable memory operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-mastra.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n agents/index.ts\nimport { openai } from '@ai-sdk/openai';\nimport { Agent } from '@mastra/core/agent';\nimport { mem0MemorizeTool, mem0RememberTool } from '../tools';\n\nexport const mem0Agent = new Agent({\n  name: 'Mem0 Agent',\n  instructions: `\n    You are a helpful assistant that has the ability to memorize and remember facts using Mem0.\n  `,\n  model: openai('gpt-4o'),\n  tools: { mem0RememberTool, mem0MemorizeTool },\n});\n```\n\n----------------------------------------\n\nTITLE: Basic DeepSeek Integration with mem0\nDESCRIPTION: This code snippet demonstrates how to initialize and use mem0's Memory with DeepSeek LLM models. It sets up environment variables, creates a configuration for the DeepSeek model, and adds conversation messages to memory with user identification and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/deepseek.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"DEEPSEEK_API_KEY\"] = \"your-api-key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # for embedder model\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"deepseek\",\n        \"config\": {\n            \"model\": \"deepseek-chat\",  # default model\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n            \"top_p\": 1.0\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Setting up Azure OpenAI Structured Outputs with Mem0 in Python\nDESCRIPTION: This snippet demonstrates how to configure Azure OpenAI with structured outputs capability in Mem0 for Python. It shows environment variable setup and configuration for the structured outputs model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/azure_openai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"LLM_AZURE_OPENAI_API_KEY\"] = \"your-api-key\"\nos.environ[\"LLM_AZURE_DEPLOYMENT\"] = \"your-deployment-name\"\nos.environ[\"LLM_AZURE_ENDPOINT\"] = \"your-api-base-url\"\nos.environ[\"LLM_AZURE_API_VERSION\"] = \"version-to-use\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"azure_openai_structured\",\n        \"config\": {\n            \"model\": \"your-deployment-name\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n            \"azure_kwargs\": {\n                  \"azure_deployment\": \"\",\n                  \"api_version\": \"\",\n                  \"azure_endpoint\": \"\",\n                  \"api_key\": \"\",\n                  \"default_headers\": {\n                    \"CustomHeader\": \"your-custom-header\",\n                  }\n              }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Mem0 with TypeScript\nDESCRIPTION: Demonstrates how to add and search memories using Mem0 in TypeScript. It includes importing necessary modules, setting up the client with an API key, creating messages, and configuring search options. It demonstrates both the add and search functionalities with user and agent ID parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_76\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport MemoryClient, { Message, SearchOptions, MemoryOptions }  from 'mem0ai';\n\nconst apiKey = 'your-api-key-here';\nconst client = new MemoryClient(apiKey);\n\n// Messages\nconst messages: Message[] = [\n    { role: \"user\", content: \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\" },\n    { role: \"assistant\", content: \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\" }\n];\n\n// ADD\nconst memoryOptions: MemoryOptions = {\n    user_id: \"alex\",\n    agent_id: \"travel-assistant\"\n}\n\nclient.add(messages, memoryOptions)\n  .then(result => console.log(result))\n  .catch(error => console.error(error));\n\n// SEARCH\nconst query: string = \"What do you know about me?\";\nconst searchOptions: SearchOptions = {\n    user_id: \"alex\",\n    filters: {\n        OR: [\n          { agent_id: \"travel-assistant\" },\n          { user_id: \"alex\" }\n        ]\n      },\n      threshold: 0.1,\n      api_version: 'v2'\n}\n  \nclient.search(query, searchOptions)\n.then(results => console.log(results))\n.catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with Clarifai LLM and Embedder in Python\nDESCRIPTION: This snippet demonstrates how to create an Embedchain app using Clarifai's LLM and embedder. It configures the app with specific models for language processing and embedding, including model-specific parameters like temperature and max tokens.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/clarifai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"clarifai\",\n        \"config\": {\n            \"model\": \"https://clarifai.com/mistralai/completion/models/mistral-7B-Instruct\",\n            \"model_kwargs\": {\n            \"temperature\": 0.5,\n            \"max_tokens\": 1000\n            }\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"clarifai\",\n        \"config\": {\n            \"model\": \"https://clarifai.com/openai/embed/models/text-embedding-ada\",\n        }\n}\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a CrewAI Agent with Memory and Search (Python)\nDESCRIPTION: Implements create_travel_agent() to instantiate a CrewAI Agent specialized for travel planning, equipped with memory and real-world web search via the SerperDevTool. This agent is configured not to delegate and contains a descriptive backstory for immersive dialog. Requires that SerperDevTool, Agent, and associated CrewAI infrastructure have been imported and configured.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_travel_agent():\n    \"\"\"Create a travel planning agent with search capabilities\"\"\"\n    search_tool = SerperDevTool()\n\n    return Agent(\n        role=\"Personalized Travel Planner Agent\",\n        goal=\"Plan personalized travel itineraries\",\n        backstory=\"\"\"You are a seasoned travel planner, known for your meticulous attention to detail.\"\"\",\n        allow_delegation=False,\n        memory=True,\n        tools=[search_tool],\n    )\n```\n\n----------------------------------------\n\nTITLE: Main Event Loop for Memory Voice Agent - Python\nDESCRIPTION: The `main` async function establishes the core event loop of the memory voice agent application. It sets up the memory-enabled agent, configures the VoicePipeline for TTS and ASR, continuously prompts the user for input, records microphone audio, feeds it to the agent via pipeline, and processes streaming agent responses both as text and audio playback. It includes error handling for memory addition and graceful keyboard interrupt termination. Dependencies are Agent, VoicePipeline, sounddevice, asyncio, AudioInput, and Mem0 client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nasync def main():\n    print(\"Starting Memory Voice Agent\")\n    \n    # Create the agent and context\n    agent = create_memory_voice_agent()\n    \n    # Set up the voice pipeline\n    pipeline = VoicePipeline(\n        workflow=SingleAgentVoiceWorkflow(agent)\n    )\n    \n    # Configure TTS settings\n    pipeline.config.tts_settings.voice = \"alloy\"\n    pipeline.config.tts_settings.speed = 1.0\n    \n    try:\n        while True:\n            # Get user input\n            print(\"\\nPress Enter to start recording (or 'q' to quit)...\")\n            user_input = input()\n            if user_input.lower() == 'q':\n                break\n            \n            # Record and process audio\n            audio_data = await record_from_microphone(duration=5)\n            audio_input = AudioInput(buffer=audio_data)\n            \n            print(\"Processing your request...\")\n            \n            # Process the audio input\n            result = await pipeline.run(audio_input)\n            \n            # Create an audio player\n            player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n            player.start()\n            \n            # Store the agent's response for adding to memory\n            agent_response = \"\"\n            \n            print(\"\\nAgent response:\")\n            # Play the audio stream as it comes in\n            async for event in result.stream():\n                if event.type == \"voice_stream_event_audio\":\n                    player.write(event.data)\n                elif event.type == \"voice_stream_event_content\":\n                    # Accumulate and print the text response\n                    content = event.data\n                    agent_response += content\n                    print(content, end=\"\", flush=True)\n            \n            print(\"\\n\")\n            \n            # Example of saving the conversation to Mem0 after completion\n            if agent_response:\n                try:\n                    await mem0_client.add(\n                        f\"Agent response: {agent_response}\", \n                        user_id=USER_ID,\n                        metadata={\"type\": \"agent_response\"}\n                    )\n                except Exception as e:\n                    print(f\"Failed to store memory: {e}\")\n    \n    except KeyboardInterrupt:\n        print(\"\\nExiting...\")\n\n```\n\n----------------------------------------\n\nTITLE: Getting All Memories for a User in Python\nDESCRIPTION: This Python code retrieves all memories for a specified user using the `client.get_all()` method.  It utilizes pagination with `page=1` and `page_size=50` to retrieve memories in chunks. The user ID is specified using the `user_id` parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nmemories = client.get_all(user_id=\"alex\", page=1, page_size=50)\n```\n\n----------------------------------------\n\nTITLE: Getting All Memories for a User in JavaScript\nDESCRIPTION: This JavaScript code retrieves all memories associated with a specific user, using pagination to limit the results to 50 memories per page, starting from page 1. It uses a Promise to handle the asynchronous API call.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_39\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.getAll({ user_id: \"alex\", page: 1, page_size: 50 })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Instantiating MemoryClient in Python\nDESCRIPTION: This snippet demonstrates how to instantiate the MemoryClient in Python. It imports necessary libraries, sets the API key as an environment variable, and creates a MemoryClient instance. The MEM0_API_KEY environment variable is crucial for authenticating with the Mem0 Platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Setting Mem0 Platform API Key as Environment Variable - Python\nDESCRIPTION: Sets the MEM0_API_KEY environment variable in Python to authenticate against the Mem0 Platform. Replace '<your-mem0-api-key>' with your actual Mem0 API key before executing. Required for Mem0Memory to access platform features.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"MEM0_API_KEY\"] = \"<your-mem0-api-key>\"\n```\n\n----------------------------------------\n\nTITLE: Adding Irrelevant Memory with Custom Prompt in TypeScript\nDESCRIPTION: This example shows adding a memory that is not relevant to the custom prompt's focus in TypeScript. It demonstrates that no memory is added when the message doesn't match the prompt's criteria.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait memory.add('I like going to hikes', { userId: \"user123\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Memories to Graph Memory (Python and TypeScript)\nDESCRIPTION: These snippets showcase how to add a memory for a user named 'alice' using Graph Memory in both Python and TypeScript. The 'add' method stores textual memories and associates them with a user. The NodeSDK uses 'userId' (camel case) for this parameter. The expected output is a JSON confirmation message with the property 'message: ok'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"I like pizza\", user_id=\"alice\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"I like pizza\", { userId: \"alice\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n{'message': 'ok'}\n```\n\n----------------------------------------\n\nTITLE: Creating and Sending Image URL Messages to Mem0 (TypeScript)\nDESCRIPTION: Demonstrates constructing and ingesting an image URL message using TypeScript. Uses a Message object with type-safe fields and invokes the `add` method to store the image in memory. Dependencies include 'mem0ai/oss' and a valid image URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Message } from \"mem0ai/oss\";\n\nconst client = new Memory();\n\nconst imageUrl = \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\";\n\nconst imageMessage: Message = {\n    role: \"user\",\n    content: {\n        type: \"image_url\",\n        image_url: {\n            url: imageUrl\n        }\n    }\n}\n\nawait client.add([imageMessage], { userId: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using User and Agent ID Filters in cURL\nDESCRIPTION: This cURL command searches memories using user_id and agent_id filters. It sends a POST request to the /memories/search endpoint with a JSON payload containing the query and filters. The filters use an AND operator to combine the user_id and agent_id criteria.  The Authorization header needs to include a valid API token.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_28\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/search/?version=v2\" \\\n     -H \"Authorization : Token your - api - key\" \\\n     -H \"Content-Type : application / json\" \\\n     -d '{\n         \"query\": \"What do you know about me?\",\n         \"filters\": {\n             \"AND\": [\n                 {\n                     \"user_id\": \"alex\"\n                 },\n                 {\n                     \"agent_id\": {\n                         \"in\": [\"travel-assistant\", \"customer-support\"]\n                     }\n                 }\n             ]\n         }\n     }'\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoGen and Mem0 Clients in Python\nDESCRIPTION: This Python snippet imports required modules, sets up API keys for OpenAI and Mem0 as environment variables, and initializes the Mem0 `MemoryClient` and AutoGen `ConversableAgent`. It requires replacing placeholder API keys with actual keys. The `ConversableAgent` is configured to use the 'gpt-4' model and disables code execution.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/autogen.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom autogen import ConversableAgent\nfrom mem0 import MemoryClient\nfrom openai import OpenAI\n\n# Configuration\nOPENAI_API_KEY = 'sk-xxx'  # Replace with your actual OpenAI API key\nMEM0_API_KEY = 'your-mem0-key'  # Replace with your actual Mem0 API key from https://app.mem0.ai\nUSER_ID = \"customer_service_bot\"\n\n# Set up OpenAI API key\nos.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\nos.environ['MEM0_API_KEY'] = MEM0_API_KEY\n\n# Initialize Mem0 and AutoGen agents\nmemory_client = MemoryClient()\nagent = ConversableAgent(\n    \"chatbot\",\n    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": OPENAI_API_KEY}]},\n    code_execution_config=False,\n    human_input_mode=\"NEVER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Application Performance with Embedchain in Python\nDESCRIPTION: This snippet initializes an Embedchain App object, adds a data source, and evaluates the application's performance using the evaluate() method. The evaluate() method takes a single question or a list of questions, optionally accepts custom metrics and a thread count, and returns evaluation metrics as a dictionary. Dependencies include the embedchain library. The function outputs metrics such as 'answer_relevancy' and 'context_relevancy', and supports both single and batch evaluation of questions. Limitations may include evaluation metric coverage or data source accessibility.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/evaluate.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\n# add data source\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n# run evaluation\napp.evaluate(\"what is the net worth of Elon Musk?\")\n# {'answer_relevancy': 0.958019958036268, 'context_relevancy': 0.12903225806451613}\n\n# or\n# app.evaluate([\"what is the net worth of Elon Musk?\", \"which companies does Elon Musk own?\"])\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure AI Search with Mem0 in Python\nDESCRIPTION: This snippet demonstrates how to configure and use Azure AI Search with Mem0. It sets up the environment, configures the vector store, and adds messages to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/azure.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"   # This key is used for embedding purpose\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"azure_ai_search\",\n        \"config\": {\n            \"service_name\": \"ai-search-test\",\n            \"api_key\": \"*****\",\n            \"collection_name\": \"mem0\", \n            \"embedding_model_dims\": 1536\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Implementing Reranking in Mem0 Search using Python\nDESCRIPTION: Shows how to use the reranking feature in Mem0. This mode uses a deep neural network to correct the order of search results, ensuring the most relevant memories appear first.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/advanced-retrieval.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient.search(query, rerank=True, user_id='alex')\n```\n\nLANGUAGE: python\nCODE:\n```\n# Search for travel plans with reranking enabled\nquery = \"What are my travel plans?\"\nresults = client.search(query, rerank=True, user_id='alex')\n\n# Without reranking, results might be ordered like:\n# 1. \"Traveled to France last year\" (less relevant to current plans)\n# 2. \"Planning a trip to Japan next month\" (more relevant to current plans)\n# 3. \"Interested in visiting Tokyo restaurants\" (relevant to current plans)\n\n# With reranking enabled, results would be reordered:\n# 1. \"Planning a trip to Japan next month\" (most relevant to current plans)\n# 2. \"Interested in visiting Tokyo restaurants\" (highly relevant to current plans)\n# 3. \"Traveled to France last year\" (less relevant to current plans)\n```\n\n----------------------------------------\n\nTITLE: Adding Messages with Automatic Context in Python (v2)\nDESCRIPTION: Illustrates how to add messages using version 2 in Python. This method automatically manages conversation context, simplifying the process of adding new messages.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# First interaction\nmessages1 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"}\n]\nclient.add(messages1, user_id=\"alex\", version=\"v2\")\n\n# Second interaction - only need to send new messages\nmessages2 = [\n    {\"role\": \"user\", \"content\": \"I like to eat sushi, and yesterday I went to Sunnyvale to eat sushi with my friends.\"},\n    {\"role\": \"assistant\", \"content\": \"Sushi is really a tasty choice. What did you do this weekend?\"}\n]\nclient.add(messages2, user_id=\"alex\", version=\"v2\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Memory Retrieval Parameters in Mem0MemoryService (Python)\nDESCRIPTION: Shows how to customize memory search parameters for Mem0, such as the number of returned memories, match threshold, and API version. These parameters help refine how relevant historical data is surfaced to the LLM, impacting conversational recall. Intended to be integrated wherever Mem0MemoryService is instantiated.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmemory = Mem0MemoryService(\n    api_key=os.getenv(\"MEM0_API_KEY\"),\n    user_id=\"user123\",\n    params={\n        \"search_limit\": 5,            # Retrieve up to 5 memories\n        \"search_threshold\": 0.2,      # Higher threshold for more relevant matches\n        \"api_version\": \"v2\",          # Mem0 API version\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 with Supabase Vector Store in TypeScript\nDESCRIPTION: This TypeScript snippet illustrates how to configure and use the Mem0 library (`mem0ai/oss`) with Supabase. It defines a configuration object specifying Supabase as the vector store provider, including collection name, embedding dimensions, Supabase URL, Supabase Key, and table name. It then initializes a `Memory` instance and asynchronously adds sample conversation messages with a user ID and metadata. Requires the `mem0ai/oss` package and Supabase credentials (URL and Key), typically sourced from environment variables.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/supabase.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript Typescript\nimport { Memory } from \"mem0ai/oss\";\n\nconst config = {\n    vectorStore: {\n      provider: \"supabase\",\n      config: {\n        collectionName: \"memories\",\n        embeddingModelDims: 1536,\n        supabaseUrl: process.env.SUPABASE_URL || \"\",\n        supabaseKey: process.env.SUPABASE_KEY || \"\",\n        tableName: \"memories\",\n      },\n    },\n}\n\nconst memory = new Memory(config);\n\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\n\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n```\n\n----------------------------------------\n\nTITLE: Storing a Memory - TypeScript\nDESCRIPTION: Shows how to add a series of user/assistant message interactions as a memory with associated metadata. Requires a Memory instance and an array of message objects. The add function takes the messages and an options object including userId and custom metadata, and asynchronously stores them.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\n\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movie_recommendations\" } });\n```\n\n----------------------------------------\n\nTITLE: Loading Sitemap Data with Embedchain App in Python\nDESCRIPTION: This snippet shows how to initialize an Embedchain `App` and add data from a sitemap URL. It explicitly specifies the `data_type` parameter as \"sitemap\" to ensure correct processing. The output comments indicate the progress of loading pages and inserting data chunks.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/add.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add(\"https://python.langchain.com/sitemap.xml\", data_type=\"sitemap\")\n# Loading pages: 100%|█████████████| 1108/1108 [00:47<00:00, 23.17it/s]\n# Inserting batches in chromadb: 100%|█████████| 111/111 [04:41<00:00,  2.54s/it]\n# Successfully saved https://python.langchain.com/sitemap.xml (DataType.SITEMAP). New chunks count: 11024\n```\n\n----------------------------------------\n\nTITLE: Setting a Custom Prompt with Mem0 Graph Memory in TypeScript\nDESCRIPTION: This snippet illustrates initializing the Mem0 Graph Memory feature in a TypeScript environment and applying a custom extraction prompt for sports-related entities. It imports the Memory class, defines a configuration object with Neo4j connection details and a custom prompt, and creates a Memory instance. Required dependencies include the mem0ai/oss module and a Neo4j backend. The main parameters are graphStore connection settings and the customPrompt string. On instantiation, the Memory configuration enables tailored entity extraction per specified user requirements.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/features.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0ai/oss\";\n\nconst config = {\n    graphStore: {\n        provider: \"neo4j\",\n        config: {\n            url: \"neo4j+s://xxx\",\n            username: \"neo4j\",\n            password: \"xxx\",\n        },\n        customPrompt: \"Please only extract entities containing sports related relationships and nothing else.\",\n    }\n}\n\nconst memory = new Memory(config);\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Graph Memory (Python)\nDESCRIPTION: Shows how to enable graph-based memory storage in Mem0 by installing the package with the 'graph' extra and configuring a graph store provider such as Neo4j. This allows for advanced context retrieval using both vector and graph-based methods, suitable for tracking relationships between entities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\npip install \"mem0ai[graph]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Memories (Node.js, JavaScript)\nDESCRIPTION: Uses the Mem0 Node.js client's add method to store memories, either as message objects or plain strings. Allows targeting a specific memory space by passing parameters such as user_id, agent_id, run_id, or app_id for personalized context. Dependencies include a properly initialized Memory client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Add memories (message or string)\nawait m.add({ text: \"Remember this fact.\", user_id: \"user123\" });\n```\n\n----------------------------------------\n\nTITLE: Integrating Mem0 Memory with an Agno Agent and Image Support (Python)\nDESCRIPTION: This Python snippet demonstrates how to instantiate an Agno agent with Mem0 memory integration, including multimodal support for images and text. It defines a chat_user function that manages user inputs, processes images as base64, searches relevant memories, and formulates context-aware prompts for the agent. Dependencies include agno, mem0ai, and OpenAIChat; valid Mem0 and OpenAI API keys are required. The code accepts parameters for user input, user ID, and optional image path, returning the agent's personalized response or a default message if input is missing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/agno.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom agno.agent import Agent\nfrom agno.media import Image\nfrom agno.models.openai import OpenAIChat\nfrom mem0 import MemoryClient\n\n\n# Initialize the Mem0 client\nclient = MemoryClient()\n\n# Define the agent\nagent = Agent(\n    name=\"Personal Agent\",\n    model=OpenAIChat(id=\"gpt-4\"),\n    description=\"You are a helpful personal agent that helps me with day to day activities.\"\n                \"You can process both text and images.\",\n    markdown=True\n)\n\n\ndef chat_user(\n    user_input: Optional[str] = None,\n    user_id: str = \"user_123\",\n    image_path: Optional[str] = None\n) -> str:\n    \"\"\"\n    Handle user input with memory integration, supporting both text and images.\n\n    Args:\n        user_input: The user's text input\n        user_id: Unique identifier for the user\n        image_path: Path to an image file if provided\n\n    Returns:\n        The agent's response as a string\n    \"\"\"\n    if image_path:\n        # Convert image to base64\n        with open(image_path, \"rb\") as image_file:\n            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n        # Create message objects for text and image\n        messages = []\n\n        if user_input:\n            messages.append({\n                \"role\": \"user\",\n                \"content\": user_input\n            })\n\n        messages.append({\n            \"role\": \"user\",\n            \"content\": {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                }\n            }\n        })\n\n        # Store messages in memory\n        client.add(messages, user_id=user_id)\n        print(\"✅ Image and text stored in memory.\")\n\n    if user_input:\n        # Search for relevant memories\n        memories = client.search(user_input, user_id=user_id)\n        memory_context = \"\\n\".join(f\"- {m['memory']}\" for m in memories)\n\n        # Construct the prompt\n        prompt = f\"\"\"\nYou are a helpful personal assistant who helps users with their day-to-day activities and keeps track of everything.\n\nYour task is to:\n1. Analyze the given image (if present) and extract meaningful details to answer the user's question.\n2. Use your past memory of the user to personalize your answer.\n3. Combine the image content and memory to generate a helpful, context-aware response.\n\nHere is what I remember about the user:\n{memory_context}\n\nUser question:\n{user_input}\n\"\"\"\n        # Get response from agent\n        if image_path:\n            response = agent.run(prompt, images=[Image(filepath=Path(image_path))])\n        else:\n            response = agent.run(prompt)\n\n        # Store the interaction in memory\n        client.add(f\"User: {user_input}\\nAssistant: {response.content}\", user_id=user_id)\n        return response.content\n\n    return \"No user input or image provided.\"\n\n\n# Example Usage\nif __name__ == \"__main__\":\n    response = chat_user(\n        \"This is the picture of what I brought with me in the trip to Bahamas\",\n        image_path=\"travel_items.jpeg\",\n        user_id=\"user_123\"\n    )\n    print(response)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Long-term Memory in JavaScript\nDESCRIPTION: This snippet shows how to create long-term memory for a user in JavaScript. It defines an array of messages and uses the `add` method of the MemoryClient, passing in the messages, user ID, and metadata as an object. It also includes error handling using `.then` and `.catch`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst messages = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n];\nclient.add(messages, { user_id: \"alex\", metadata: { food: \"vegan\" } })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Implementing AI Chat with Memory in Python\nDESCRIPTION: Example Python implementation of a chat application using Mem0 for memory management. This script instantiates the memory object, retrieves relevant memories for each user query, generates responses using OpenAI, and stores new memories from the conversation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n    \n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Implementing Add Memory LangChain Tool Function in Python\nDESCRIPTION: Defines the `add_memory` function which takes structured input (messages, user_id, etc.) and uses the `MemoryClient.add` method to store memories. It then wraps this function in a LangChain `StructuredTool` named 'add_memory'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef add_memory(messages: List[Message], user_id: str, output_format: str, metadata: Optional[Dict[str, Any]] = None) -> Any:\n    \"\"\"Add messages to memory with associated user ID and metadata.\"\"\"\n    message_dicts = [msg.dict() for msg in messages]\n    return client.add(message_dicts, user_id=user_id, output_format=output_format, metadata=metadata)\n\nadd_tool = StructuredTool(\n    name=\"add_memory\",\n    description=\"Add new messages to memory with associated metadata\",\n    func=add_memory,\n    args_schema=AddMemoryInput\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling History Store in Memory - TypeScript\nDESCRIPTION: Shows how to initialize Memory with history tracking disabled, optimizing for serverless use cases or where audit trails are not required. This is set by passing disableHistory: true to the constructor.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst memory = new Memory({\n  disableHistory: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Mistral AI in Python\nDESCRIPTION: This snippet demonstrates how to set up the Memory object using Mistral AI in Python. It includes setting environment variables for API keys, configuring the LLM provider, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/mistral_AI.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"MISTRAL_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"litellm\",\n        \"config\": {\n            \"model\": \"open-mixtral-8x7b\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with Pinecone Vector Store (Python)\nDESCRIPTION: This snippet demonstrates how to configure and initialize a Mem0 Memory instance using Pinecone as the vector store. It includes setting up environment variables, defining the configuration, and adding messages to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/pinecone.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\nos.environ[\"PINECONE_API_KEY\"] = \"your-api-key\"\n\n# Example using serverless configuration\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"pinecone\",\n        \"config\": {\n            \"collection_name\": \"testing\",\n            \"embedding_model_dims\": 1536,  # Matches OpenAI's text-embedding-3-small\n            \"serverless_config\": {\n                \"cloud\": \"aws\",  # Choose between 'aws' or 'gcp' or 'azure'\n                \"region\": \"us-east-1\"\n            },\n            \"metric\": \"cosine\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Using Web Search with OpenAI and Mem0 for Car Recommendations\nDESCRIPTION: This code snippet shows how to combine Mem0's memory capabilities with OpenAI's web search feature to generate up-to-date car recommendations. It uses the web_search_preview tool along with a custom car recommendation tool.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await openAIClient.responses.create({\n    model: \"gpt-4o\",\n    tools: [{ type: \"web_search_preview\" }, carRecommendationTool],\n    input: `${getMemoryString(relevantMemories)}\\n${userInput}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying DOCX Files with Embedchain in Python\nDESCRIPTION: This Python code snippet demonstrates using the Embedchain library to ingest DOCX files, either from a remote URL or a local file path, by specifying the data_type as 'docx' when calling the app.add() method. After loading the document, a sample query ('Summarize the docx data?') is executed against the added content. Dependencies include the embedchain Python package. Key parameters: the file location (URL or local path) and data_type ('docx'). The expected input is a valid DOCX document, and the output is the result of the issued query. Additional constraints: ensure access to the provided docx file and correct Embedchain installation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/docx.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add('https://example.com/content/intro.docx', data_type=\"docx\")\n# Or add file using the local file path on your system\n# app.add('content/intro.docx', data_type=\"docx\")\n\napp.query(\"Summarize the docx data?\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Ollama LLMs with Mem0 in Python\nDESCRIPTION: This code snippet shows how to set up the environment, configure Ollama LLM, initialize a Memory object, and add conversation messages. It uses the 'mixtral:8x7b' model and sets specific parameters for the LLM.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/ollama.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # for embedder\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"ollama\",\n        \"config\": {\n            \"model\": \"mixtral:8x7b\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Adding to Long-Term User Memory in Python (v2)\nDESCRIPTION: Shows how to add memories to a user's long-term memory store using only the user_id parameter in Python. This method is ideal for maintaining persistent user information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Adding to long-term user memory\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm allergic to peanuts and shellfish.\"},\n    {\"role\": \"assistant\", \"content\": \"I've noted your allergies to peanuts and shellfish.\"}\n]\nclient.add(messages, user_id=\"alex\", version=\"v2\")\n```\n\n----------------------------------------\n\nTITLE: Configuring mem0 with Vertex AI Vector Search in Python\nDESCRIPTION: This code snippet demonstrates how to configure and initialize mem0 with Google Cloud Vertex AI Vector Search. It sets up the necessary environment variables, defines the configuration dictionary, and creates a Memory instance using the configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/vertex_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"GEMINI_API_KEY\"] = = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"vertex_ai_vector_search\",\n        \"config\": {\n            \"endpoint_id\": \"YOUR_ENDPOINT_ID\",            # Required: Vector Search endpoint ID\n            \"index_id\": \"YOUR_INDEX_ID\",                  # Required: Vector Search index ID \n            \"deployment_index_id\": \"YOUR_DEPLOYMENT_INDEX_ID\",  # Required: Deployment-specific ID\n            \"project_id\": \"YOUR_PROJECT_ID\",              # Required: Google Cloud project ID\n            \"project_number\": \"YOUR_PROJECT_NUMBER\",      # Required: Google Cloud project number\n            \"region\": \"YOUR_REGION\",                      # Optional: Defaults to GOOGLE_CLOUD_REGION\n            \"credentials_path\": \"path/to/credentials.json\", # Optional: Defaults to GOOGLE_APPLICATION_CREDENTIALS\n            \"vector_search_api_endpoint\": \"YOUR_API_ENDPOINT\" # Required for get operations\n        }\n    }\n}\nm = Memory.from_config(config)\nm.add(\"Your text here\", user_id=\"user\", metadata={\"category\": \"example\"})\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Milvus Vector Database with mem0 in Python\nDESCRIPTION: This snippet demonstrates how to configure and use the Milvus vector database with the mem0 library. It shows setting up the configuration, creating a Memory instance, and adding messages to the database.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/milvus.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"milvus\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"embedding_model_dims\": \"123\",\n            \"url\": \"127.0.0.1\",\n            \"token\": \"8e4b8ca8cf2c67\",\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Loading Existing OpenAI Assistant with Embedchain\nDESCRIPTION: Demonstrates how to load an existing OpenAI Assistant using its ID, and optionally a specific thread. Useful for continuing conversations with previously created assistants.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/openai-assistant.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load an assistant and create a new thread\nassistant = OpenAIAssistant(assistant_id=\"asst_xxx\")\n\n# Load a specific thread for an assistant\nassistant = OpenAIAssistant(assistant_id=\"asst_xxx\", thread_id=\"thread_xxx\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating retrieve_memories Usage in ElevenLabs Conversation Flow (Python)\nDESCRIPTION: This Python code snippet demonstrates an agent retrieving a user's favorite color by calling the retrieve_memories function with a specific query message. The result, if found, provides relevant information for the agent to deliver a personalized response. This approach assumes prior integration of memory tools and a defined retrieve_memories function; it outputs a memory string that can be reused in the agent's next reply.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# Agent calls retrieve_memories\\nmemories = retrieve_memories({\\\"message\\\": \\\"user's favorite color\\\"})\\n# If found: \\\"The user's favorite color is blue\\\"\n```\n\n----------------------------------------\n\nTITLE: Searching Personal Information in Python and TypeScript\nDESCRIPTION: Demonstrates how to search memories using a natural language query about personal identity. Returns both relevant memories and the entity relationships that were found.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nm.search(\"What is my name?\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.search(\"What is my name?\", { userId: \"alice123\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    'memories': [...],\n    'entities': [\n        {'source': 'alice123', 'relation': 'dislikes_playing','destination': 'badminton'},\n        {'source': 'alice123', 'relation': 'friend', 'destination': 'peter'},\n        {'source': 'alice123', 'relation': 'friend', 'destination': 'john'},\n        {'source': 'alice123', 'relation': 'has_name', 'destination': 'alice'},\n        {'source': 'alice123', 'relation': 'likes', 'destination': 'hiking'}\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Schema for Adding Memory in Python\nDESCRIPTION: Defines Pydantic models `Message` and `AddMemoryInput` to structure the input for the add memory tool. `AddMemoryInput` includes fields for messages, user ID, output format, and optional metadata, along with example usage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Message(BaseModel):\n    role: str = Field(description=\"Role of the message sender (user or assistant)\")\n    content: str = Field(description=\"Content of the message\")\n\nclass AddMemoryInput(BaseModel):\n    messages: List[Message] = Field(description=\"List of messages to add to memory\")\n    user_id: str = Field(description=\"ID of the user associated with these messages\")\n    output_format: str = Field(description=\"Version format for the output\")\n    metadata: Optional[Dict[str, Any]] = Field(description=\"Additional metadata for the messages\", default=None)\n\n    class Config:\n        json_schema_extra = {\n            \"examples\": [{\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n                    {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy.\"}\n                ],\n                \"user_id\": \"alex\",\n                \"output_format\": \"v1.1\",\n                \"metadata\": {\"food\": \"vegan\"}\n            }]\n        }\n```\n\n----------------------------------------\n\nTITLE: Defining and Registering Function Tools for Agents in LlamaIndex - Python\nDESCRIPTION: Defines two functions: one for simulating a call and one for sending an email to a given name. Both are wrapped as FunctionTool objects for agent tool integration. The 'name' parameter specifies the target person's name.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.tools import FunctionTool\n\n\ndef call_fn(name: str):\n    \"\"\"Call the provided name.\n    Args:\n        name: str (Name of the person)\n    \"\"\"\n    print(f\"Calling... {name}\")\n\n\ndef email_fn(name: str):\n    \"\"\"Email the provided name.\n    Args:\n        name: str (Name of the person)\n    \"\"\"\n    print(f\"Emailing... {name}\")\n\n\ncall_tool = FunctionTool.from_defaults(fn=call_fn)\nemail_tool = FunctionTool.from_defaults(fn=email_fn)\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Query Loop with Embedchain\nDESCRIPTION: Implements an interactive loop that allows users to ask questions about the ingested data. The loop continues until the user enters 'q', 'exit', or 'quit', and uses the Embedchain app to query the data and print answers.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/lancedb.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client and Adding Multimodal Messages in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize the `MemoryClient`, set the API key via environment variable, structure a list of messages including text and an image URL, and add these messages to a specific user's memory using the `client.add` method. It requires the `mem0` library and the `os` module.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient()\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi, my name is Alice.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Nice to meet you, Alice! What do you like to eat?\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n            }\n        }\n    },\n]\n\n# Calling the add method to ingest messages into the memory system\nclient.add(messages, user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Agent for Mem0 and OpenAI Agents Integration\nDESCRIPTION: This Python code configures the memory agent with instructions and memory tools for interacting with Mem0.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmemory_agent = Agent[Mem0Context](\n    name=\"Memory Assistant\",\n    instructions=\"\"\"You are a helpful assistant with memory capabilities. You can:\n    1. Store new information using add_to_memory\n    2. Search existing information using search_memory\n    3. Retrieve all stored information using get_all_memory\n    When users ask questions:\n    - If they want to store information, use add_to_memory\n    - If they're searching for specific information, use search_memory\n    - If they want to see everything stored, use get_all_memory\"\"\",\n    tools=[add_to_memory, search_memory, get_all_memory],\n)\n```\n\n----------------------------------------\n\nTITLE: Encoding and Sending Local Images via Base64 in Mem0 (TypeScript)\nDESCRIPTION: Provides a method for reading a local image file, encoding it in Base64, and including it in a Mem0 message for ingestion. Assumes usage of Node.js-compatible file operations (`fs.readFileSync`). The message is submitted via the `add` method and intended for scenarios without public image URLs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Message } from \"mem0ai/oss\";\n\nconst client = new Memory();\n\nconst imagePath = \"path/to/your/image.jpg\";\n\nconst base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });\n\nconst imageMessage: Message = {\n    role: \"user\",\n    content: {\n        type: \"image_url\",\n        image_url: {\n            url: `data:image/jpeg;base64,${base64Image}`\n        }\n    }\n}\n\nawait client.add([imageMessage], { userId: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with OpenAI in Python\nDESCRIPTION: Sets up a Memory instance using OpenAI's LLM models by configuring the API key and model parameters, then adds conversation messages to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\n# Use Openrouter by passing it's api key\n# os.environ[\"OPENROUTER_API_KEY\"] = \"your-api-key\"\n# config = {\n#    \"llm\": {\n#        \"provider\": \"openai\",\n#        \"config\": {\n#            \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n#        }\n#    }\n# }\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 System with TypeScript\nDESCRIPTION: A complete example of Mem0 configuration that includes settings for the embedder, vector store, language model, history store, and general options. The configuration uses environment variables for API keys and service URLs, with specific provider and model choices for each component.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst config = {\n      version: 'v1.1',\n      embedder: {\n        provider: 'openai',\n        config: {\n          apiKey: process.env.OPENAI_API_KEY || '',\n          model: 'text-embedding-3-small',\n        },\n      },\n      vectorStore: {\n        provider: 'memory',\n        config: {\n          collectionName: 'memories',\n          dimension: 1536,\n        },\n      },\n      llm: {\n        provider: 'openai',\n        config: {\n          apiKey: process.env.OPENAI_API_KEY || '',\n          model: 'gpt-4-turbo-preview',\n        },\n      },\n      historyStore: {\n        provider: 'supabase',\n        config: {\n          supabaseUrl: process.env.SUPABASE_URL || '',\n          supabaseKey: process.env.SUPABASE_KEY || '',\n          tableName: 'memories',\n        },\n      },\n      disableHistory: false, // This is false by default\n      customPrompt: \"I'm a virtual assistant. I'm here to help you with your queries.\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with LanceDB Dependencies\nDESCRIPTION: Installs the required Python packages (embedchain and lancedb) needed to use LanceDB as a vector database with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/lancedb.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install embedchain lancedb\n```\n\n----------------------------------------\n\nTITLE: Adding Substack Data Source with Python Embedchain\nDESCRIPTION: Example showing how to initialize an App instance and add a Substack newsletter as a data source. The code demonstrates setting up the connection with a Substack URL and performing a query about Brian Chesky.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/substack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\n# source: for any substack just add the root URL\napp.add('https://www.lennysnewsletter.com', data_type='substack')\napp.query(\"Who is Brian Chesky?\")\n# Answer: Brian Chesky is the co-founder and CEO of Airbnb.\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using OpenSearch with mem0 in Python\nDESCRIPTION: Python code demonstrating how to configure mem0 to use OpenSearch as a vector store, set up the environment, and add conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/opensearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"opensearch\",\n        \"config\": {\n            \"collection_name\": \"mem0\",\n            \"host\": \"localhost\",\n            \"port\": 9200,\n            \"embedding_model_dims\": 1536\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Providing Feedback on Memories using Mem0 API in JavaScript\nDESCRIPTION: This code snippet shows how to use the Mem0 client in JavaScript to give feedback on a generated memory. It creates a new MemoryClient instance with an API key and calls the feedback method with the required parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/feedback-mechanism.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport MemoryClient from 'mem0ai';\n\nconst client = new MemoryClient({ apiKey: 'your-api-key'});\n\nclient.feedback({\n    memory_id: \"your-memory-id\", \n    feedback: \"NEGATIVE\", \n    feedback_reason: \"I don't like this memory because it is not relevant.\"\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client with Provider Configuration\nDESCRIPTION: Creates a Mem0 client with OpenAI as the provider, configuring API keys and compatibility settings. This initializes the client for memory-enhanced AI interactions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0({\n  provider: \"openai\",\n  mem0ApiKey: \"m0-xxx\",\n  apiKey: \"openai-api-key\",\n  config: {\n    compatibility: \"strict\",\n    // Additional model-specific configuration options can be added here.\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Mem0 Configuration in Python\nDESCRIPTION: This snippet demonstrates how to set up and use a configuration object with Mem0 in Python. It includes setting an environment variable for the OpenAI API key and creating a Memory instance with a custom configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/config.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\" # for embedder\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"your_chosen_provider\",\n        \"config\": {\n            # Provider-specific settings go here\n        }\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Your text here\", user_id=\"user\", metadata={\"category\": \"example\"})\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Text Icon for Livekit Integration\nDESCRIPTION: SVG text element to create a simple 'LK' text icon for the Livekit integration, with various text attributes to control appearance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_5\n\nLANGUAGE: SVG\nCODE:\n```\n<text\n  x=\"12\"\n  y=\"16\"\n  fontFamily=\"Arial\"\n  fontSize=\"12\"\n  textAnchor=\"middle\"\n  fill=\"currentColor\"\n  fontWeight=\"bold\"\n>\n  LK\n</text>\n```\n\n----------------------------------------\n\nTITLE: Customizing Image Processing in Embedchain with ImageLoader (Python)\nDESCRIPTION: Shows how to customize the image processing behavior in `embedchain` by creating an `ImageLoader` instance. This instance is configured with specific parameters like `max_tokens` (limiting the description length), `api_key` (OpenAI key), and a custom `prompt` to guide the GPT-4 Vision analysis. This custom loader is then passed to the `app.add` method when adding the image data source, allowing for tailored image analysis before querying. The resulting description reflects the guidance from the custom prompt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/image.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\nfrom embedchain.loaders.image import ImageLoader\n\nimage_loader = ImageLoader(\n    max_tokens=100,\n    api_key=\"sk-xxx\",\n    prompt=\"Is the person looking wealthy? Structure your thoughts around what you see in the image.\",\n)\n\napp = App()\napp.add(\"./Elon-Musk.webp\", data_type=\"image\", loader=image_loader)\nresponse = app.query(\"Describe the man in the image.\")\nprint(response)\n# Answer: The man in the image appears to be well-dressed in a suit and shirt, suggesting that he may be in a professional or formal setting. His composed demeanor and confident posture further indicate a sense of self-assurance. Based on these visual cues, one could infer that the man may have a certain level of economic or social status, possibly indicating wealth or professional success.\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Memory Functions for Add and Retrieve (Python)\nDESCRIPTION: Implements two asynchronous functions: one for adding a user's message as a new memory (add_memories) and one for retrieving previous memories relevant to a query (retrieve_memories). Both functions rely on the Mem0 AsyncMemoryClient. Functions handle message parameter extraction, memory association with the user, semantic search, and return feedback or results for use by the AI agent.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    # Define memory-related functions for the agent\n    async def add_memories(parameters):\n        \"\"\"Add a message to the memory store\"\"\"\n        message = parameters.get(\"message\")\n        await mem0_client.add(\n            messages=message, \n            user_id=USER_ID, \n            output_format=\"v1.1\", \n            version=\"v2\"\n        )\n        return \"Memory added successfully\"\n\n    async def retrieve_memories(parameters):\n        \"\"\"Retrieve relevant memories based on the input message\"\"\"\n        message = parameters.get(\"message\")\n\n        # Set up filters to retrieve memories for this specific user\n        filters = {\n            \"AND\": [\n                {\n                    \"user_id\": USER_ID\n                }\n            ]\n        }\n\n        # Search for relevant memories using the message as a query\n        results = await mem0_client.search(\n            query=message,\n            version=\"v2\", \n            filters=filters\n        )\n\n        # Extract and join the memory texts\n        memories = ' '.join([result[\"memory\"] for result in results])\n        print(\"[ Memories ]\", memories)\n\n        if memories:\n            return memories\n        return \"No memories found\"\n```\n\n----------------------------------------\n\nTITLE: Creating Embedchain App for Semantic Search in Python\nDESCRIPTION: Initializes an Embedchain App instance for semantic search capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/semantic-search.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\n```\n\n----------------------------------------\n\nTITLE: Querying Slack Messages with Embedchain - Python\nDESCRIPTION: This Python snippet sets up an Embedchain App to fetch and query messages from a Slack channel. It first sets the SLACK_USER_TOKEN environment variable, initializes the App, adds Slack data from the 'general' channel, and executes a natural language query. Requires the 'embedchain' package (with slack extra) and a valid Slack user token with appropriate scope. Input: Slack channel name (here 'in:general'). Output: Query results printed to stdout.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/slack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom embedchain import App\\n\\nos.environ[\\\"SLACK_USER_TOKEN\\\"] = \\\"xoxp-xxx\\\"\\napp = App()\\n\\napp.add(\\\"in:general\\\", data_type=\\\"slack\\\")\\n\\nresult = app.query(\\\"what are the messages in general channel?\\\")\\n\\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Executing the Get All Memory Tool with Example Input in Python\nDESCRIPTION: Demonstrates invoking the `get_all_tool` with sample input including version, filters (user ID and date range), and default pagination parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nget_all_input = {\n    \"version\": \"v2\",\n    \"filters\": {\n        \"AND\": [\n            {\"user_id\": \"alex\"},\n            {\"created_at\": {\"gte\": \"2024-07-01\", \"lte\": \"2024-12-31\"}}\n        ]\n    },\n    \"page\": 1,\n    \"page_size\": 50\n}\nget_all_result = get_all_tool.invoke(get_all_input)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 Components in Python\nDESCRIPTION: A complete configuration example for Mem0, including vector store, LLM, embedder, graph store, and general settings. This snippet demonstrates how to set up all components with their respective parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"host\": \"localhost\",\n            \"port\": 6333\n        }\n    },\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"api_key\": \"your-api-key\",\n            \"model\": \"gpt-4\"\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"api_key\": \"your-api-key\",\n            \"model\": \"text-embedding-3-small\"\n        }\n    },\n    \"graph_store\": {\n        \"provider\": \"neo4j\",\n        \"config\": {\n            \"url\": \"neo4j+s://your-instance\",\n            \"username\": \"neo4j\",\n            \"password\": \"password\"\n        }\n    },\n    \"history_db_path\": \"/path/to/history.db\",\n    \"version\": \"v1.1\",\n    \"custom_fact_extraction_prompt\": \"Optional custom prompt for fact extraction for memory\",\n    \"custom_update_memory_prompt\": \"Optional custom prompt for update memory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Personalized AI Tutor Class in Python\nDESCRIPTION: This code snippet defines the PersonalAITutor class, which integrates Mem0 for memory storage and OpenAI's GPT-4 for generating responses. It includes methods for asking questions and retrieving memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-ai-tutor.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os \nfrom openai import OpenAI\nfrom mem0 import Memory\n\n# Set the OpenAI API key\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\nclass PersonalAITutor:\n    def __init__(self):\n        \"\"\"\n        Initialize the PersonalAITutor with memory configuration and OpenAI client.\n        \"\"\"\n        config = {\n            \"vector_store\": {\n                \"provider\": \"qdrant\",\n                \"config\": {\n                    \"host\": \"localhost\",\n                    \"port\": 6333,\n                }\n            },\n        }\n        self.memory = Memory.from_config(config)\n        self.client = client\n        self.app_id = \"app-1\"\n\n    def ask(self, question, user_id=None):\n        \"\"\"\n        Ask a question to the AI and store the relevant facts in memory\n\n        :param question: The question to ask the AI.\n        :param user_id: Optional user ID to associate with the memory.\n        \"\"\"\n        # Start a streaming response request to the AI\n        response = self.client.responses.create(\n            model=\"gpt-4o\",\n            instructions=\"You are a personal AI Tutor.\",\n            input=question,\n            stream=True\n        )\n\n        # Store the question in memory\n        self.memory.add(question, user_id=user_id, metadata={\"app_id\": self.app_id})\n\n        # Print the response from the AI in real-time\n        for event in response:\n            if event.type == \"response.output_text.delta\":\n                print(event.delta, end=\"\")\n\n    def get_memories(self, user_id=None):\n        \"\"\"\n        Retrieve all memories associated with the given user ID.\n\n        :param user_id: Optional user ID to filter memories.\n        :return: List of memories.\n        \"\"\"\n        return self.memory.get_all(user_id=user_id)\n\n# Instantiate the PersonalAITutor\nai_tutor = PersonalAITutor()\n\n# Define a user ID\nuser_id = \"john_doe\"\n\n# Ask a question\nai_tutor.ask(\"I am learning introduction to CS. What is queue? Briefly explain.\", user_id=user_id)\n```\n\n----------------------------------------\n\nTITLE: Configuring MistralAI Language Model in Embedchain\nDESCRIPTION: This snippet demonstrates how to use the MistralAI language model in Embedchain. It sets up the environment and configures the app using a YAML file that specifies the Hugging Face model and embedder.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = \"hf_your_token\"\n\napp = App.from_config(\"huggingface.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: huggingface\n  config:\n    model: 'mistralai/Mistral-7B-v0.1'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 0.5\n    stream: false\n\nembedder:\n  provider: huggingface\n  config:\n    model: 'sentence-transformers/all-mpnet-base-v2'\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying Dropbox Data with Embedchain (Python)\nDESCRIPTION: This Python script demonstrates how to use the embedchain library to load data from a specified Dropbox path. It first sets the required environment variables for the Dropbox access token and OpenAI API key. Then, it initializes an `embedchain.App` instance, adds data from the '/test' path within the user's Dropbox account by specifying `data_type='dropbox'`, and finally executes a query against the loaded data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/dropbox.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"DROPBOX_ACCESS_TOKEN\"] = \"sl.xxx\"\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\napp = App()\n\n# any path from the root of your dropbox account, you can leave it \"\" for the root folder\napp.add(\"/test\", data_type=\"dropbox\")\n\nprint(app.query(\"Which two celebrities are mentioned here?\"))\n# The two celebrities mentioned in the given context are Elon Musk and Jeff Bezos.\n```\n\n----------------------------------------\n\nTITLE: Browsing arXiv for Research Papers Using MultiOn in Python\nDESCRIPTION: Uses MultiOn to automate browsing of arXiv.org, leveraging user-specific command and relevant memories to personalize the search. Constructs a prompt combining the user command and past memories, then submits it to the browser automation client targeting arXiv. Requires the initialized 'multion' and a valid 'relevant_memories_text'. Output is the browsing result from MultiOn.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt = f\"{command}\\n My past memories: {relevant_memories_text}\"\nbrowse_result = multion.browse(cmd=prompt, url=\"https://arxiv.org/\")\nprint(browse_result)\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs and Mem0 Clients (Python)\nDESCRIPTION: Creates an ElevenLabs client for voice interactions and a Mem0 asynchronous memory client for persistent data storage. Also initializes a ClientTools object for registering agent capabilities. These instances form the backbone for AI conversation and memory augmentation in the agent workflow.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    # Initialize ElevenLabs client\n    client = ElevenLabs(api_key=API_KEY)\n\n    # Initialize memory client and tools\n    client_tools = ClientTools()\n    mem0_client = AsyncMemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Memory Function for Mem0 Integration\nDESCRIPTION: This Python function searches for memories in Mem0 based on a given query and user context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def search_memory(\n    context: RunContextWrapper[Mem0Context],\n    query: str,\n) -> str:\n    \"\"\"\n    Search for memories in Mem0\n    Args:\n        query: The search query.\n    \"\"\"\n    user_id = context.context.user_id or \"default_user\"\n    memories = await client.search(query, user_id=user_id, output_format=\"v1.1\")\n    results = '\\n'.join([result[\"memory\"] for result in memories[\"results\"]])\n    return str(results)\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Discord Bot\nDESCRIPTION: Slash command format for adding data sources to the Discord bot. The command requires specifying the data type and providing the URL or text content to be added to the bot's knowledge base.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n/ec add <data_type> <url_or_text>\n```\n\n----------------------------------------\n\nTITLE: Implementing Personal AI Travel Assistant (Before Mem0 v1.1)\nDESCRIPTION: Python implementation of a Personal AI Travel Assistant for Mem0 versions before v1.1. It includes memory management and interaction with OpenAI's GPT-4 model using the chat completions API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-travel-assistant.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom mem0 import Memory\n\n# Set the OpenAI API key\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\nclass PersonalTravelAssistant:\n    def __init__(self):\n        self.client = OpenAI()\n        self.memory = Memory()\n        self.messages = [{\"role\": \"system\", \"content\": \"You are a personal AI Assistant.\"}]\n\n    def ask_question(self, question, user_id):\n        # Fetch previous related memories\n        previous_memories = self.search_memories(question, user_id=user_id)\n        prompt = question\n        if previous_memories:\n            prompt = f\"User input: {question}\\n Previous memories: {previous_memories}\"\n        self.messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Generate response using GPT-4o\n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=self.messages\n        )\n        answer = response.choices[0].message.content\n        self.messages.append({\"role\": \"assistant\", \"content\": answer})\n\n        # Store the question in memory\n        self.memory.add(question, user_id=user_id)\n        return answer\n\n    def get_memories(self, user_id):\n        memories = self.memory.get_all(user_id=user_id)\n        return [m['memory'] for m in memories['memories']]\n\n    def search_memories(self, query, user_id):\n        memories = self.memory.search(query, user_id=user_id)\n        return [m['memory'] for m in memories['memories']]\n\n# Usage example\nuser_id = \"traveler_123\"\nai_assistant = PersonalTravelAssistant()\n\ndef main():\n    while True:\n        question = input(\"Question: \")\n        if question.lower() in ['q', 'exit']:\n            print(\"Exiting...\")\n            break\n\n        answer = ai_assistant.ask_question(question, user_id=user_id)\n        print(f\"Answer: {answer}\")\n        memories = ai_assistant.get_memories(user_id=user_id)\n        print(\"Memories:\")\n        for memory in memories:\n            print(f\"- {memory}\")\n        print(\"-----\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Running Mem0 Completely Locally with LM Studio\nDESCRIPTION: This snippet shows how to configure Mem0 to use LM Studio for both LLM and embedding capabilities, enabling a completely local setup without external API dependencies. It demonstrates creating a memory instance and adding conversation messages with user identification and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/lmstudio.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\n# No external API keys needed!\nconfig = {\n    \"llm\": {\n        \"provider\": \"lmstudio\"\n    },\n    \"embedder\": {\n        \"provider\": \"lmstudio\"\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice123\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Interactive Chat Interface for Support Chatbot\nDESCRIPTION: Creates an interactive chat interface for the support chatbot. Initializes the SupportChatbot, sets a user ID, and implements a loop that processes user input and displays responses until the user types 'exit'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/customer-support-chatbot.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchatbot = SupportChatbot()\nuser_id = \"customer_bot\"\nprint(\"Welcome to Customer Support! Type 'exit' to end the conversation.\")\n\nwhile True:\n    # Get user input\n    query = input()\n    print(\"Customer:\", query)\n    \n    # Check if user wants to exit\n    if query.lower() == 'exit':\n        print(\"Thank you for using our support service. Goodbye!\")\n        break\n    \n    # Handle the query and print the response\n    response = chatbot.handle_customer_query(user_id, query)\n    print(\"Support:\", response, \"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses with Memory Context\nDESCRIPTION: Demonstrates streaming text generation with memory context for real-time AI responses. This allows for displaying responses as they're generated while maintaining memory context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { streamText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { textStream } = await streamText({\n  model: mem0(\"gpt-4-turbo\", {\n    user_id: \"borat\",\n  }),\n  prompt:\n    \"Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.\",\n});\n\nfor await (const textPart of textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with Custom Categories in Python\nDESCRIPTION: This code snippet shows how to add memories using custom categories. It demonstrates the use of the 'add' API call with a conversation about personal scheduling and organization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"My name is Alice. I need help organizing my daily schedule better. I feel overwhelmed trying to balance work, exercise, and social life.\"},\n    {\"role\": \"assistant\", \"content\": \"I understand how overwhelming that can feel. Let's break this down together. What specific areas of your schedule feel most challenging to manage?\"},\n    {\"role\": \"user\", \"content\": \"I want to be more productive at work, maintain a consistent workout routine, and still have energy for friends and hobbies.\"},\n    {\"role\": \"assistant\", \"content\": \"Those are great goals for better time management. What's one small change you could make to start improving your daily routine?\"},\n]\n\n# Add memories with custom categories\nclient.add(messages, user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM for Use with LlamaIndex - Python\nDESCRIPTION: Sets the 'OPENAI_API_KEY' environment variable and creates an OpenAI LLM object for use in LlamaIndex. Replace '<your-openai-api-key>' with a valid OpenAI key before execution. This is required for agent functionality using OpenAI models.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom llama_index.llms.openai import OpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Querying with Citations Using Embedchain in Python\nDESCRIPTION: This snippet demonstrates how to use the Embedchain App to query a data source and retrieve both the answer and corresponding citations. Dependencies include the 'embedchain' library. Key parameters are the input query string and the 'citations' boolean flag; when 'citations' is True, the output is a tuple comprising the answer and a list of cited sources, each with metadata including URL, doc_id, and score. The example includes adding a public web source and prints both the textual answer and structured citation information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/query.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain import App\\n\\n# Initialize app\\napp = App()\\n\\n# Add data source\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\n# Get relevant answer for your query\\nanswer, sources = app.query(\\\"What is the net worth of Elon?\\\", citations=True)\\nprint(answer)\\n# Answer: The net worth of Elon Musk is $221.9 billion.\\n\\nprint(sources)\\n# [\\n#    (\\n#        'Elon Musk PROFILEElon MuskCEO, Tesla$247.1B$2.3B (0.96%)Real Time Net Worthas of 12/7/23 ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.89,\\n#           ...\\n#        }\\n#    ),\\n#    (\\n#        '74% of the company, which is now called X.Wealth HistoryHOVER TO REVEAL NET WORTH BY YEARForbes ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.81,\\n#           ...\\n#        }\\n#    ),\\n#    (\\n#        'founded in 2002, is worth nearly $150 billion after a $750 million tender offer in June 2023 ...',\\n#        {\\n#           'url': 'https://www.forbes.com/profile/elon-musk', \\n#           'score': 0.73,\\n#           ...\\n#        }\\n#    )\\n# ]\n```\n\n----------------------------------------\n\nTITLE: Loading Embedchain App Configuration from JSON File in Python\nDESCRIPTION: This Python code shows how to create an Embedchain `App` instance by reading configuration settings from a JSON file named `config.json`. The `App.from_config()` class method is used with the `config_path` parameter pointing to the JSON file. This allows for configuration management separate from the main script.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load llm configuration from config.json file\napp = App.from_config(config_path=\"config.json\")\n```\n\n----------------------------------------\n\nTITLE: Storing Conversation History in Mem0 using Python\nDESCRIPTION: This snippet demonstrates how to add a conversation history to Mem0's memory. It defines a list of dictionaries representing the conversation turns (user and assistant messages) and uses the `memory_client.add()` method to store this conversation associated with a specific `USER_ID`. This allows the AI to reference past interactions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/autogen.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n    {\"role\": \"assistant\", \"content\": \"Hi, I'm Best Buy's chatbot! How can I help you?\"},\n    {\"role\": \"user\", \"content\": \"I'm seeing horizontal lines on my TV.\"},\n    {\"role\": \"assistant\", \"content\": \"I'm sorry to hear that. Can you provide your TV model?\"},\n    {\"role\": \"user\", \"content\": \"It's a Sony - 77\\\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV\"},\n    {\"role\": \"assistant\", \"content\": \"Thank you for the information. Let's troubleshoot this issue...\"}\n]\n\nmemory_client.add(messages=conversation, user_id=USER_ID)\nprint(\"Conversation added to memory.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring an ElevenLabs Conversation Session with Audio and Memory Tools (Python)\nDESCRIPTION: Initializes a Conversation object with ElevenLabs client, agent and user details, authentication status, audio interface, and registered tools. Establishes callbacks to handle agent responses, response corrections, and user transcripts. This enables robust, interactive voice sessions with integrated memory retrieval and update capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    # Initialize the conversation\n    conversation = Conversation(\n        client,\n        AGENT_ID,\n        # Assume auth is required when API_KEY is set\n        requires_auth=bool(API_KEY),\n        audio_interface=DefaultAudioInterface(),\n        client_tools=client_tools,\n        callback_agent_response=lambda response: print(f\"Agent: {response}\"),\n        callback_agent_response_correction=lambda original, corrected: print(f\"Agent: {original} -> {corrected}\"),\n        callback_user_transcript=lambda transcript: print(f\"User: {transcript}\"),\n        # callback_latency_measurement=lambda latency: print(f\"Latency: {latency}ms\"),\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Memory with cURL\nDESCRIPTION: This cURL command demonstrates how to create agent memory via the Mem0 API. It specifies the agent ID in the request body, ensuring the memory is associated with the designated agent. It is used to retain responses from the agent across sessions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_13\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [\n             {\"role\": \"system\", \"content\": \"You are an AI tutor with a personality. Give yourself a name for the user.\"}, \n             {\"role\": \"assistant\", \"content\": \"Understood. I'm an AI tutor with a personality. My name is Alice.\"}\n         ],\n         \"agent_id\": \"ai-tutor\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain GithubLoader in Python\nDESCRIPTION: Demonstrates how to create an instance of `GithubLoader` from the `embedchain` library. It requires configuration including a GitHub personal access token (PAT) passed within a dictionary to the `config` parameter. This loader is essential for fetching data from GitHub repositories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/github.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n```Python\nfrom embedchain.loaders.github import GithubLoader\n\nloader = GithubLoader(\n    config={\n        \"token\":\"ghp_xxxx\"\n        }\n    )\n```\n```\n\n----------------------------------------\n\nTITLE: Resetting an Embedchain Application and Vector Database (Python)\nDESCRIPTION: This snippet illustrates how to reset an Embedchain application. It initializes an app (note: the code shows `App()` then uses `App.from_config` which might be slightly redundant, focus is on reset), adds a data source, and then calls the `reset()` method. The `reset()` method deletes the associated vector database and any other related application files, effectively clearing its state. Requires the `embedchain` library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/data-type-handling.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App() # Note: Original text has 'App()config = {' which seems like a typo, interpreting as separate lines.\nconfig = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"app-1\"\n        }\n    }\n}\nnaval_chat_bot = App.from_config(config=config)\napp.add(\"https://www.youtube.com/watch?v=3qHkcs3kG44\") # Assuming 'app' should be used or 'naval_chat_bot'\napp.reset() # Assuming 'app' should be used or 'naval_chat_bot'\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using mem0 Memory with Redis Vector Store (TypeScript)\nDESCRIPTION: This TypeScript snippet illustrates setting up the mem0ai/oss Memory class with a Redis vector store. Required dependencies are the 'mem0ai/oss' module and access to a running Redis instance. The configuration object specifies Redis connection details, including optional username and password, collection name, and embedding dimensions. The 'add' method stores a message sequence for a given user with attached metadata. Input messages should match the role/content schema. The call to 'add' is asynchronous and must be awaited.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/redis.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  vectorStore: {\n    provider: 'redis',\n    config: {\n      collectionName: 'memories',\n      embeddingModelDims: 1536,\n      redisUrl: 'redis://localhost:6379',\n      username: 'your-redis-username',\n      password: 'your-redis-password',\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I’m not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Delete Specific User with Mem0\nDESCRIPTION: Deletes a specific user (or agent, app, or run) from the Mem0 system. The examples show how to delete a user by ID using Python, JavaScript, and a cURL request. The cURL request requires an API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_64\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X DELETE \"https://api.mem0.ai/v1/entities/?user_id=alex\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 and LangChain Core Dependencies using Bash\nDESCRIPTION: Installs the necessary Python packages `langchain_core` and `mem0ai` using pip. These packages are required to use Mem0 LangChain tools.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain_core\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 OSS Client with Qdrant Vector Store in Python\nDESCRIPTION: This code snippet demonstrates how to configure and instantiate the Mem0 client for use with an open-source vector store (Qdrant), enabling local or self-hosted deployment scenarios. It sets up the configuration dictionary to specify the Qdrant provider and connection parameters, initializes the client, and performs a simple chat completion request. Dependencies include the 'mem0' Python library, a running Qdrant server (hosted at localhost:6333), and Python 3.x. Key parameters: 'config' for vector store setup, 'messages', and 'model'. Input is a user's chat message, and output is the AI-generated response. The snippet is limited to environments where Qdrant is accessible.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/openai_compatibility.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"host\": \"localhost\",\n            \"port\": 6333,\n        }\n    },\n}\n\nclient = Mem0(config=config)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the capital of France?\",\n        }\n    ],\n    model=\"gpt-4o\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in .env without Docker (txt)\nDESCRIPTION: Illustrates setting the required `OPENAI_API_KEY` in a `.env` file in the current directory. This method is used when planning to run the Mem0 API server directly using Python without Docker.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\nOPENAI_API_KEY=your-openai-api-key\n```\n\n----------------------------------------\n\nTITLE: Searching Memories in Memgraph\nDESCRIPTION: This snippet shows how to search for memories in Memgraph based on a query, printing the memory content and relevance score for each result.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor result in m.search(\"what does alice love?\", user_id=\"alice\")[\"results\"]:\n    print(result[\"memory\"], result[\"score\"])\n```\n\n----------------------------------------\n\nTITLE: Get All Memories for a User - JavaScript\nDESCRIPTION: Fetches all user memories on the Mem0 managed platform using the JavaScript SDK. Accepts an options object with version, filters, page, and page_size properties. Outputs a promise that resolves to matching memory objects.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst filters = {\\n   \\\"AND\\\": [\\n      {\\n         \\\"user_id\\\": \\\"alex\\\"\\n      }\\n   ]\\n};\\n\\nclient.getAll({ version: \\\"v2\\\", filters, page: 1, page_size: 50 })\\n    .then(memories => console.log(memories))\\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Adding a Documentation Website to Embedchain App in Python\nDESCRIPTION: This Python code snippet demonstrates initializing an Embedchain `App`, adding a documentation website URL as a data source using the `add` method with `data_type=\"docs_site\"`, and then querying the app to retrieve information based on the ingested website content. It requires the `embedchain` library to be installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/docs-site.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom embedchain import App\n\napp = App()\napp.add(\"https://docs.embedchain.ai/\", data_type=\"docs_site\")\napp.query(\"What is Embedchain?\")\n# Answer: Embedchain is a platform that utilizes various components, including paid/proprietary ones, to provide what is believed to be the best configuration available. It uses LLM (Language Model) providers such as OpenAI, Anthpropic, Vertex_AI, GPT4ALL, Azure_OpenAI, LLAMA2, JINA, Ollama, Together and COHERE. Embedchain allows users to import and utilize these LLM providers for their applications.'\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using the Embedchain App (Python)\nDESCRIPTION: This snippet exemplifies the process for creating and interacting with an Embedchain App object in Python. After setting the OPENAI_API_KEY environment variable, it instantiates the App class, adds content sources using URLs, and then issues a query to retrieve contextual answers based on embedded data. Dependencies include the Embedchain package and a valid OpenAI API key; 'from embedchain import App' must work based on the installed package. Input URLs are expected to point to valid web resources. The app.query call outputs a string answer relevant to the provided question.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# Create a bot instance\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\napp = App()\n\n# Embed online resources\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n# Query the app\napp.query(\"How many companies does Elon Musk run and name those?\")\n# Answer: Elon Musk currently runs several companies. As of my knowledge, he is the CEO and lead designer of SpaceX, the CEO and product architect of Tesla, Inc., the CEO and founder of Neuralink, and the CEO and founder of The Boring Company. However, please note that this information may change over time, so it's always good to verify the latest updates.\n```\n\n----------------------------------------\n\nTITLE: Evaluating Groundedness in RAG Applications\nDESCRIPTION: Example of how to use the Groundedness metric to measure if the generated answers are factually supported by the provided contexts.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.evaluation.metrics import Groundedness\nmetric = Groundedness()\nscore = metric.evaluate(dataset)    # dataset from above\nprint(score)\n# 1.0\n```\n\n----------------------------------------\n\nTITLE: Executing the Search Memory Tool with Example Input in Python\nDESCRIPTION: Demonstrates how to invoke the `search_tool` with sample input including a query, filters (using AND logic with date range and user ID), and version.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsearch_input = {\n    \"query\": \"what is my name?\",\n    \"filters\": {\n        \"AND\": [\n            {\"created_at\": {\"gte\": \"2024-07-20\", \"lte\": \"2024-12-10\"}},\n            {\"user_id\": \"alex\"}\n        ]\n    },\n    \"version\": \"v2\"\n}\nresult = search_tool.invoke(search_input)\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Gemini Embeddings in Python\nDESCRIPTION: This code snippet demonstrates how to set up environment variables for Gemini and OpenAI, configure mem0 with Gemini embeddings, and add conversation messages to memory. It shows the complete workflow from authentication to memory storage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/gemini.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"GOOGLE_API_KEY\"] = \"key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"gemini\",\n        \"config\": {\n            \"model\": \"models/text-embedding-004\",\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Batch Update Memories with Mem0\nDESCRIPTION: Updates multiple memories in a single API call using Mem0.  The examples show how to construct the list of memory updates and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memories to update.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_70\n\nLANGUAGE: Python\nCODE:\n```\nupdate_memories = [\n    {\n        \"memory_id\": \"285ed74b-6e05-4043-b16b-3abd5b533496\",\n        \"text\": \"Watches football\"\n    },\n    {\n        \"memory_id\": \"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\",\n        \"text\": \"Loves to travel\"\n    }\n]\n\nresponse = client.batch_update(update_memories)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Searching Memories - TypeScript\nDESCRIPTION: Searches stored memories based on a query string and user ID. The Memory.search function accepts a query and options object, returning matching memories along with scores, metadata, and user info. Useful for context retrieval in chatbots or recommendation systems.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await memory.search('What do you know about me?', { userId: \"alice\" });\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Embedchain App with JinaChat Provider\nDESCRIPTION: This snippet creates an Embedchain app using a configuration dictionary. It sets up the app to use the JinaChat provider with specific parameters for temperature, max tokens, and other settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/jina.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"jina\",\n    \"config\": {\n        \"temperature\": 0.5,\n        \"max_tokens\": 1000,\n        \"top_p\": 1,\n        \"stream\": False\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with User and Agent IDs in Mem0 (cURL)\nDESCRIPTION: This cURL command demonstrates how to add memories to Mem0 via an HTTP POST request, associating them with both a user ID and an agent ID. The request includes an API key in the Authorization header and sends a JSON payload containing the messages, user ID, and agent ID.  The API endpoint is `https://api.mem0.ai/v1/memories/`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_16\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [\n             {\"role\": \"user\", \"content\": \"I'm travelling to San Francisco\"},\n             {\"role\": \"assistant\", \"content\": \"That's great! I'm going to Dubai next month.\"},\n         ],\n         \"user_id\": \"user1\",\n         \"agent_id\": \"agent1\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Chat Turn Management Function - Python\nDESCRIPTION: Implements the logic for a single conversational turn, combining context retrieval, response generation, and memory augmentation. Takes the user's current input and ID, processes with helper functions, persists results, and returns the assistant's reply. Depends on prior snippet helper functions. Limitations: assumes properly initialized context and valid user input strings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef chat_turn(user_input: str, user_id: str) -> str:\n    # Retrieve context\n    context = retrieve_context(user_input, user_id)\n    \n    # Generate response\n    response = generate_response(user_input, context)\n    \n    # Save interaction\n    save_interaction(user_id, user_input, response)\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories with Graph Memory in Python\nDESCRIPTION: This snippet demonstrates how to retrieve all memories with Graph Memory context using the Mem0 Python client. It shows how to make an API call to get all memories with Graph Memory enabled.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Get all memories with graph context\nmemories = client.get_all(\n    user_id=\"joseph\", \n    enable_graph=True, \n    output_format=\"v1.1\"\n)\n\nprint(memories)\n```\n\n----------------------------------------\n\nTITLE: Searching Memories (Python and TypeScript)\nDESCRIPTION: These calls search graph memory for content relevant to the provided query (e.g., 'tell me my name.') for the user 'alice'. Required parameters include the query string and user identifier. The output mirrors the get_all method, providing matched memory objects and graph-extracted entities. Results are returned as a structured JSON response.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nm.search(\"tell me my name.\", user_id=\"alice\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.search(\"tell me my name.\", { userId: \"alice\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    'memories': [\n        {\n            'id': 'de69f426-0350-4101-9d0e-5055e34976a5',\n            'memory': 'Likes pizza',\n            'hash': '92128989705eef03ce31c462e198b47d',\n            'metadata': None,\n            'created_at': '2024-08-20T14:09:27.588719-07:00',\n            'updated_at': None,\n            'user_id': 'alice'\n        }\n    ],\n    'entities': [\n        {\n            'source': 'alice',\n            'relationship': 'likes',\n            'target': 'pizza'\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Dependencies in requirements.txt\nDESCRIPTION: This code snippet specifies the Python package dependencies required for the mem0ai/mem0 project. It pins Streamlit to version 1.29.0 while leaving embedchain without version constraints, allowing it to install the latest version.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/mistral-streamlit/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstreamlit==1.29.0\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using User and Agent ID Filters in Python\nDESCRIPTION: This code snippet demonstrates how to search memories using user_id and agent_id filters. It defines a query and a filters dictionary, which includes an AND operator to combine the user_id and agent_id conditions. The client.search method is then called with the query, version set to v2, and the filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"What do you know about me?\"\nfilters = {\n   \"AND\":[\n      {\n         \"user_id\":\"alex\"\n      },\n      {\n         \"agent_id\":{\n            \"in\":[\n               \"travel-assistant\",\n               \"customer-support\"\n            ]\n         }\n      }\n   ]\n}\nclient.search(query, version=\"v2\", filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying a Directory Data Source with Embedchain in Python (Default Loader)\nDESCRIPTION: This snippet demonstrates how to configure Embedchain to treat a local directory as a data source using the default loader in Python. It sets the OPENAI_API_KEY environment variable, initializes the App class, and adds a directory with data_type set to 'directory'. The code issues a natural language query to list all files in the directory and prints the response. Required dependencies include the 'embedchain' library and a valid OpenAI API key. The directory path and API key are customizable; expected output lists the contained files.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/directory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\napp = App()\napp.add(\"./elon-musk\", data_type=\"directory\")\nresponse = app.query(\"list all files\")\nprint(response)\n# Answer: Files are elon-musk-1.txt, elon-musk-2.pdf.\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with OpenAI Configuration\nDESCRIPTION: This snippet demonstrates how to initialize an Embedchain App using a configuration file for OpenAI LLM provider. It includes setting up the API key and loading the configuration from a YAML file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: gpt-3.5-turbo\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Submitting Memory Export Job using cURL\nDESCRIPTION: This cURL command shows how to submit a memory export job to the Mem0 API. It includes the schema, user ID, and export instructions in the request body.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/memory-export.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/export/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"schema\": {json_schema},\n         \"user_id\": \"alice\",\n         \"export_instructions\": \"1. Create a comprehensive profile with detailed information\\n2. Only mark fields as \\\"None\\\" when absolutely no relevant information exists\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Mem0 Configuration in TypeScript\nDESCRIPTION: This snippet shows how to create and use a configuration object with Mem0 in TypeScript. It demonstrates creating a Memory instance with a minimal configuration specifying the LLM provider and adding text to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/config.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\n// Minimal configuration with just the LLM settings\nconst config = {\n  llm: {\n    provider: 'your_chosen_provider',\n    config: {\n      // Provider-specific settings go here\n    }\n  }\n};\n\nconst memory = new Memory(config);\nawait memory.add(\"Your text here\", { userId: \"user123\", metadata: { category: \"example\" } });\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Function Calling for Multiplication in Embedchain\nDESCRIPTION: Complete example showing how to use OpenAI function calling with a multiply function in Embedchain. This demonstrates setting up the LLM with tools and making a query that requires function calling.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\nfrom embedchain.llm.openai import OpenAILlm\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\nllm = OpenAILlm(tools=multiply)\napp = App(llm=llm)\n\nresult = app.query(\"What is the result of 125 multiplied by fifteen?\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries using Bash\nDESCRIPTION: Installs the necessary Python libraries (`langgraph`, `langchain-openai`, `mem0ai`) using pip. These packages provide the core functionalities for building the conversational agent, interacting with the OpenAI API, and managing memory with Mem0.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langgraph langchain-openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with OpenAI in Embedchain\nDESCRIPTION: Example demonstrating how to enable and use token usage tracking with OpenAI models in Embedchain. Returns detailed information about token consumption and associated costs for both query and chat interactions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\nresponse = app.query(\"what is the net worth of Elon Musk?\")\n# {'answer': 'Elon Musk's net worth is $209.9 billion as of 6/9/24.',\n#   'usage': {'prompt_tokens': 1228,\n#   'completion_tokens': 21, \n#   'total_tokens': 1249, \n#   'total_cost': 0.001884, \n#   'cost_currency': 'USD'}\n# }\n\n\nresponse = app.chat(\"Which companies did Elon Musk found?\")\n# {'answer': 'Elon Musk founded six companies, including Tesla, which is an electric car maker, SpaceX, a rocket producer, and the Boring Company, a tunneling startup.',\n#   'usage': {'prompt_tokens': 1616,\n#   'completion_tokens': 34,\n#   'total_tokens': 1650,\n#   'total_cost': 0.002492,\n#   'cost_currency': 'USD'}\n# }\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Memory with Qdrant Vector Store in Python\nDESCRIPTION: This snippet demonstrates how to configure and initialize a Mem0 Memory object using Qdrant as the vector store in Python. It includes setting up the environment, creating a configuration dictionary, and adding messages to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/qdrant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Adding Memory Asynchronously in Python\nDESCRIPTION: This code demonstrates how to add a new memory asynchronously using the AsyncMemoryClient in Python. It takes a list of messages and a user ID as parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"Alice loves playing badminton\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! Alice is a fitness freak\"},\n]\nawait client.add(messages, user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Pinecone with Pod-based Deployment (Python)\nDESCRIPTION: This snippet demonstrates how to configure Pinecone for pod-based deployment. It includes settings for the environment, number of replicas, and pod type.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/pinecone.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"pinecone\",\n        \"config\": {\n            \"collection_name\": \"memory_index\",\n            \"embedding_model_dims\": 1536,  # For OpenAI's text-embedding-ada-002\n            \"pod_config\": {\n                \"environment\": \"gcp-starter\",\n                \"replicas\": 1,\n                \"pod_type\": \"starter\"\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: Requirements.txt file specifying Python package dependencies and their versions. Includes web framework FastAPI, ASGI server Uvicorn, Embedchain AI framework, and integrations for Discord and Slack.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain[opensource]\nbeautifulsoup4\ndiscord\npython-dotenv\nslack-sdk\nslack_bolt\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Fact Extraction Prompt in Python\nDESCRIPTION: This snippet demonstrates how to define a custom fact extraction prompt in Python. The prompt includes instructions and few-shot examples for extracting customer support information, order details, and user information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncustom_fact_extraction_prompt = \"\"\"\nPlease only extract entities containing customer support information, order details, and user information. \nHere are some few shot examples:\n\nInput: Hi.\nOutput: {{\"facts\" : []}}\n\nInput: The weather is nice today.\nOutput: {{\"facts\" : []}}\n\nInput: My order #12345 hasn't arrived yet.\nOutput: {{\"facts\" : [\"Order #12345 not received\"]}}\n\nInput: I'm John Doe, and I'd like to return the shoes I bought last week.\nOutput: {{\"facts\" : [\"Customer name: John Doe\", \"Wants to return shoes\", \"Purchase made last week\"]}}\n\nInput: I ordered a red shirt, size medium, but received a blue one instead.\nOutput: {{\"facts\" : [\"Ordered red shirt, size medium\", \"Received blue shirt instead\"]}}\n\nReturn the facts and customer information in a json format as shown above.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Memory by ID with Mem0 in Python\nDESCRIPTION: Uses the update() method to modify an existing memory entry. Requires the target memory_id and new data string. Returns confirmation message upon success. Can be used to correct or supplement memory content for a specific user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresult = m.update(memory_id=\"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\", data=\"I love India, it is my favorite country.\")\n```\n\nLANGUAGE: json\nCODE:\n```\n{'message': 'Memory updated successfully!'}\n```\n\n----------------------------------------\n\nTITLE: Initializing MySQL Loader with Embedchain in Python\nDESCRIPTION: This snippet sets up a MySQLLoader from the embedchain library using a configuration dictionary containing the host, port, database, user, and password for the SQL database. The loader is required for connecting and executing queries on a specified MySQL instance. Ensure that 'embedchain' and necessary MySQL client drivers are installed; the configuration keys must match your actual database credentials. The output is an instance of MySQLLoader, ready to be used for data loading operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/mysql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain.loaders.mysql import MySQLLoader\\n\\nconfig = {\\n    \"host\": \"host\",\\n    \"port\": \"port\",\\n    \"database\": \"database\",\\n    \"user\": \"username\",\\n    \"password\": \"password\",\\n}\\n\\nmysql_loader = MySQLLoader(config=config)\n```\n\n----------------------------------------\n\nTITLE: Searching Memories in Mem0 (JavaScript)\nDESCRIPTION: This snippet demonstrates how to search memories in Mem0 using the JavaScript client. It defines a query and uses the `client.search` method to retrieve relevant memories associated with the specified user ID. The `output_format` is set to `v1.1`. The function uses a promise to handle the asynchronous API call and logs the search results or any errors to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_19\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \"What should I cook for dinner today?\";\nclient.search(query, { user_id: \"alex\", output_format: \"v1.1\" })\n    .then(results => console.log(results))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Retrieving User Data Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to get all users, agents, and runs which have memories associated with them asynchronously using the MemoryClient in JavaScript.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.users();\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 using pip in Bash\nDESCRIPTION: Demonstrates how to install the Mem0 SDK using pip. No additional Python dependencies are required for installation. Run this command in a terminal before proceeding with Python usage. Ensures you have pip installed and internet access.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Initializing Core Dependencies and Memory Client (Python)\nDESCRIPTION: This Python snippet sets up all necessary imports, loads environment variables via dotenv, configures logging for the voice assistant, assigns a static user ID, and initializes the Mem0 AsyncMemoryClient. It is foundational for subsequent agent and memory operations. Required dependencies are python-dotenv for env loading, and the listed packages in the imports. Input is the environment; output is prepared logging and memory client state.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Dict, Any, Annotated\n\nimport aiohttp\nfrom dotenv import load_dotenv\nfrom livekit.agents import (\n    AutoSubscribe,\n    JobContext,\n    JobProcess,\n    WorkerOptions,\n    cli,\n    llm,\n    metrics,\n)\nfrom livekit import rtc, api\nfrom livekit.agents.pipeline import VoicePipelineAgent\nfrom livekit.plugins import deepgram, openai, silero\nfrom mem0 import AsyncMemoryClient\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogger = logging.getLogger(\"memory-assistant\")\nlogger.setLevel(logging.INFO)\n\n# Define a global user ID for simplicity\nUSER_ID = \"voice_user\"\n\n# Initialize Mem0 client\nmem0 = AsyncMemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain with LanceDB and Local Hugging Face LLM\nDESCRIPTION: This script demonstrates using LanceDB with a local Hugging Face model (Mistral-7B). It configures custom LLM, embedder, and vectordb settings, then establishes an interactive query loop for a PDF document.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/lancedb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import Pipeline as App\n\n# config for Embedchain App\nconfig = {\n  'llm': {\n    'provider': 'huggingface',\n    'config': {\n      'model': 'mistralai/Mistral-7B-v0.1',\n      'temperature': 0.1,\n      'max_tokens': 250,\n      'top_p': 0.1,\n      'stream': True\n    }\n  },\n  'embedder': {\n    'provider': 'huggingface',\n    'config': {\n      'model': 'sentence-transformers/all-mpnet-base-v2'\n    }\n  },\n  'vectordb': { \n    'provider': 'lancedb', \n    'config': { \n      'collection_name': 'lancedb-index' \n    } \n  }\n}\n\napp = App.from_config(config=config)\n\n# add data source and start query in\napp.add(\"https://www.tesla.com/ns_videos/2022-tesla-impact-report.pdf\")\n\n# query continuously\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Adding a Local Image via Base64 Encoding to Mem0 using TypeScript\nDESCRIPTION: This TypeScript snippet shows how to add a local image file to Mem0 using Base64 encoding. It utilizes the Node.js `fs` module to read the file content and encode it as Base64. It then constructs a data URL (`data:image/jpeg;base64,...`), creates the message object with `type` `image_url` and the data URL, and adds the message using `client.add`. Requires `mem0ai` and Node.js `fs`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport MemoryClient from \"mem0ai\";\nimport fs from 'fs';\n\nconst imagePath = 'path/to/your/image.jpg';\n\nconst base64Image = fs.readFileSync(imagePath, { encoding: 'base64' });\n\nconst imageMessage = {\n    role: \"user\",\n    content: {\n        type: \"image_url\",\n        image_url: {\n            url: `data:image/jpeg;base64,${base64Image}`\n        }\n    }\n};\n\nawait client.add([imageMessage], { user_id: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client with API Key\nDESCRIPTION: This Python code initializes the Mem0 AsyncMemoryClient using the API key stored in the environment variables.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclient = AsyncMemoryClient(api_key=os.getenv(\"MEM0_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing Memgraph Graph Memory with Embedder (Python Advanced)\nDESCRIPTION: This advanced example adds an embedder configuration for more sophisticated memory vector representations in Python. Both 'embedder' and 'graph_store' are set, allowing custom embedding models and dimensionality. Mem0, OpenAI embedding model access, and a working Memgraph instance are required. If no embedder is given, defaults are used; otherwise, the specified embedder settings apply.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\"model\": \"text-embedding-3-large\", \"embedding_dims\": 1536},\n    },\n    \"graph_store\": {\n        \"provider\": \"memgraph\",\n        \"config\": {\n            \"url\": \"bolt://localhost:7687\",\n            \"username\": \"memgraph\",\n            \"password\": \"xxx\"\n        }\n    }\n}\n\nm = Memory.from_config(config_dict=config)\n\n```\n\n----------------------------------------\n\nTITLE: Adding to Long-Term User Memory in JavaScript (v2)\nDESCRIPTION: Demonstrates how to add memories to a user's long-term memory store using only the user_id parameter in JavaScript. This approach is suitable for maintaining persistent user information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Adding to long-term user memory\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm allergic to peanuts and shellfish.\"},\n    {\"role\": \"assistant\", \"content\": \"I've noted your allergies to peanuts and shellfish.\"}\n];\nclient.add(messages, { user_id: \"alex\", version: \"v2\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Adding Memories via Mem0 Client - Python\nDESCRIPTION: Persists a sequence of conversational messages as memories to the Mem0 managed platform using the Python SDK. Requires a MemoryClient instance and a list of message dictionaries with 'role' and 'content' keys, along with the user_id parameter. Typically, the input is a conversation transcript to be processed and remembered.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmessages = [\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Thinking of making a sandwich. What do you recommend?\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"How about adding some cheese for extra flavor?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Actually, I don't like cheese.\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"I'll remember that you don't like cheese for future recommendations.\\\"}\\n]\\nclient.add(messages, user_id=\\\"alex\\\")\n```\n\n----------------------------------------\n\nTITLE: Resetting All Memories with Mem0 in Python\nDESCRIPTION: Resets the entire memory database, erasing all memories for all users. No parameters required. Should be used with caution as this action is irreversible and destroys all persisted data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nm.reset() # Reset all memories\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph Memory - Basic (Python)\nDESCRIPTION: This snippet demonstrates how to initialize the Mem0 Memory object with a Neo4j graph store in Python. Required dependencies include Mem0 and a running Neo4j instance. The configuration dictionary provides connection details. Pass this dictionary to Memory.from_config to create an operational memory instance. All fields (URL, username, password) are essential for connection.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nconfig = {\n    \"graph_store\": {\n        \"provider\": \"neo4j\",\n        \"config\": {\n            \"url\": \"neo4j+s://xxx\",\n            \"username\": \"neo4j\",\n            \"password\": \"xxx\"\n        }\n    }\n}\n\nm = Memory.from_config(config_dict=config)\n\n```\n\n----------------------------------------\n\nTITLE: Delete Memory Using Add with Mem0\nDESCRIPTION: Demonstrates deleting memory using the `add()` method in Mem0 by passing a natural language command. Examples are provided in Python, JavaScript, and cURL.  The cURL example includes specifying the message content and user ID in the request body.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_68\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.add(\"Delete all of my food preferences\", { user_id: \"alex\" })\n    .then(result => console.log(result))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories (Node.js, JavaScript)\nDESCRIPTION: Obtains all memories from Mem0 filtered by identifiers such as user_id, agent_id, run_id, or app_id. Custom filter objects can be provided for refined queries. Requires the Memory client to be initialized and memories to exist.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Get all memories\nconst allMemories = await m.getAll({ user_id: \"user123\" });\n```\n\n----------------------------------------\n\nTITLE: Basic Elasticsearch Vector Store Configuration\nDESCRIPTION: Example showing how to configure and initialize Memory with Elasticsearch as the vector store and add conversation messages with metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/elasticsearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"elasticsearch\",\n        \"config\": {\n            \"collection_name\": \"mem0\",\n            \"host\": \"localhost\",\n            \"port\": 9200,\n            \"embedding_model_dims\": 1536\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements Definition\nDESCRIPTION: Defines exact versions of required Python packages. Includes Slack SDK 3.21.3 for Slack integration, Flask 2.3.3 for web framework functionality, and FastAPI-POE 0.0.16 for POE API integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/slack_bot/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nslack-sdk==3.21.3 \nflask==2.3.3\nfastapi-poe==0.0.16\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Groq LPU with mem0ai in TypeScript\nDESCRIPTION: This snippet shows how to integrate Groq LPU with mem0ai in TypeScript. It covers setting up the configuration, initializing the Memory object, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/groq.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'groq',\n    config: {\n      apiKey: process.env.GROQ_API_KEY || '',\n      model: 'mixtral-8x7b-32768',\n      temperature: 0.1,\n      maxTokens: 1000,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Batch Delete Memories with Mem0\nDESCRIPTION: Deletes multiple memories in a single API call using Mem0. The examples show how to construct the list of memory IDs to delete and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memory IDs to delete.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_74\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst deleteMemories = [{\\\"memory_id\\\": \\\"285ed74b-6e05-4043-b16b-3abd5b533496\\\"},\n    {\\\"memory_id\\\": \\\"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\\\"}\n];\n\nclient.batchDelete(deleteMemories)\n    .then(response => console.log('Batch delete response:', response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Initiating Chat with Teachability\nDESCRIPTION: Starting a chat session with memory-enabled agent using Teachability feature.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nuser_proxy.initiate_chat(gpt_assistant, message=user_query)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with Hugging Face's embedding model. This snippet shows how to configure both the LLM and embedding model using a YAML configuration file, including model-specific parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: huggingface\n  config:\n    model: 'google/flan-t5-xxl'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 0.5\n    stream: false\n\nembedder:\n  provider: huggingface\n  config:\n    model: 'sentence-transformers/all-mpnet-base-v2'\n    model_kwargs:\n        trust_remote_code: True # Only use if you trust your embedder\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using OpenAI Embeddings with mem0 in Python\nDESCRIPTION: This example demonstrates how to set up the OpenAI embedder with mem0 in Python, including setting the API key, configuring the embedding model, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"text-embedding-3-large\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Date Filters in Python\nDESCRIPTION: This Python code snippet demonstrates how to search memories using date filters (created_at). It defines a query and a filters dictionary, using the 'gte' (greater than or equal to) and 'lte' (less than or equal to) operators to specify a date range. The client.search method is called with the query, version='v2', and the defined filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"What do you know about me?\"\nfilters = {\n    \"AND\": [\n        {\"created_at\": {\"gte\": \"2024-07-20\", \"lte\": \"2024-07-10\"}},\n        {\"user_id\": \"alex\"}\n    ]\n}\nclient.search(query, version=\"v2\", filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: Adds a data source (Forbes profile URL) to the Embedchain app for processing and querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/llama2.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph Memory - Basic (TypeScript)\nDESCRIPTION: This code initializes a Mem0 Memory instance with Neo4j as the graph provider in TypeScript. Install the mem0ai package and provide connection configuration within a config object, including a mandatory 'enableGraph' flag set to true. Instantiate the Memory class using this config; all credentials and the graph provider must be specified.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0ai/oss\";\n\nconst config = {\n    enableGraph: true,\n    graphStore: {\n        provider: \"neo4j\",\n        config: {\n            url: \"neo4j+s://xxx\",\n            username: \"neo4j\",\n            password: \"xxx\",\n        }\n    }\n}\n\nconst memory = new Memory(config);\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Starting Poe Bot\nDESCRIPTION: Python code to initialize a Poe bot, add data sources, and start the bot server. Requires OPENAI_API_KEY and POE_API_KEY environment variables to be set.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/poe_bot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# make sure that you have set OPENAI_API_KEY and POE_API_KEY in .env file\nfrom embedchain.bots import PoeBot\n\npoe_bot = PoeBot()\n\n# add as many data sources as you want\npoe_bot.add(\"https://en.wikipedia.org/wiki/Adam_D%27Angelo\")\npoe_bot.add(\"https://www.youtube.com/watch?v=pJQVAqmKua8\")\n\n# start the bot\n# this start the poe bot server on port 8080 by default\npoe_bot.start()\n```\n\n----------------------------------------\n\nTITLE: Querying the Embedchain App for Next.JS Information\nDESCRIPTION: This snippet demonstrates how to query the populated Embedchain App. It asks for a summary of Next.js 14 features, showcasing how the pipeline can be used to retrieve specific information about the framework.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/question-answering.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp.query(\"Summarize the features of Next.js 14?\")\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Mem0 Project\nDESCRIPTION: This code snippet lists the required Python packages for the Mem0 project. It specifies Flask 2.3.2 for web application functionality, requests 2.31.0 for making HTTP requests, python-dotenv 1.0.0 for loading environment variables, and embedchain without version constraints for embedding-related operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/telegram_bot/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nflask==2.3.2\nrequests==2.31.0\npython-dotenv==1.0.0\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Complete Python Example for Adding Multiple File Types to Mem0\nDESCRIPTION: This comprehensive Python example demonstrates adding various file types (image URL, text document URL, PDF URL) to Mem0. It initializes the `MemoryClient`, defines separate message dictionaries for each file type using their respective `type` identifiers (`image_url`, `mdx_url`, `pdf_url`), and then adds each message individually using `client.add`. A helper function `file_to_base64` is included but not used in this specific URL-based example. Requires `mem0` and `base64` modules.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nfrom mem0 import MemoryClient\n\nclient = MemoryClient()\n\ndef file_to_base64(file_path):\n    with open(file_path, \"rb\") as file:\n        return base64.b64encode(file.read()).decode('utf-8')\n\n# Example 1: Using an image URL\nimage_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"image_url\",\n        \"image_url\": {\n            \"url\": \"https://example.com/sample-image.jpg\"\n        }\n    }\n}\n\n# Example 2: Using a text document URL\ntext_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"mdx_url\",\n        \"mdx_url\": {\n            \"url\": \"https://www.w3.org/TR/2003/REC-PNG-20031110/iso_8859-1.txt\"\n        }\n    }\n}\n\n# Example 3: Using a PDF URL\npdf_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"pdf_url\",\n        \"pdf_url\": {\n            \"url\": \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n        }\n    }\n}\n\n# Add each message to the memory system\nclient.add([image_message], user_id=\"alice\")\nclient.add([text_message], user_id=\"alice\")\nclient.add([pdf_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Batch Delete Memories with Mem0\nDESCRIPTION: Deletes multiple memories in a single API call using Mem0. The examples show how to construct the list of memory IDs to delete and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memory IDs to delete.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_73\n\nLANGUAGE: Python\nCODE:\n```\ndelete_memories = [\n    {\"memory_id\": \"285ed74b-6e05-4043-b16b-3abd5b533496\"},\n    {\"memory_id\": \"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\"}\n]\n\nresponse = client.batch_delete(delete_memories)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Get All Memory LangChain Tool Function in Python\nDESCRIPTION: Defines the `get_all_memory` function using `MemoryClient.get_all` to retrieve memories based on version, filters, and pagination. It wraps this function in a LangChain `StructuredTool` named 'get_all_memory'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_all_memory(version: str, filters: Dict[str, Any], page: int = 1, page_size: int = 50) -> Any:\n    \"\"\"Retrieve all memories matching the specified criteria.\"\"\"\n    return client.get_all(version=version, filters=filters, page=page, page_size=page_size)\n\nget_all_tool = StructuredTool(\n    name=\"get_all_memory\",\n    description=\"Retrieve all memories matching specified filters\",\n    func=get_all_memory,\n    args_schema=GetAllMemoryInput\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing the Mem0 Integration in TypeScript\nDESCRIPTION: Imports `Mem0Integration` from the installed `@mastra/mem0` package and creates an instance named `mem0`. It configures the integration with necessary credentials like the Mem0 API key (fetched from environment variables) and a user ID. This instance serves as the client for interacting with the Mem0 API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-mastra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n integrations/index.ts\nimport { Mem0Integration } from \"@mastra/mem0\";\n\nexport const mem0 = new Mem0Integration({\n  config: {\n    apiKey: process.env.MEM0_API_KEY!,\n    userId: \"alice\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using SimpleChatEngine with Memory and LLM - Python\nDESCRIPTION: Initializes the SimpleChatEngine with an LLM and Mem0Memory instance, then starts a chat by sending a greeting. The response from the agent is printed. Supports both 'memory_from_client' and 'memory_from_config'. Requires llama_index.core.chat_engine and dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.chat_engine import SimpleChatEngine\n\nagent = SimpleChatEngine.from_defaults(\n    llm=llm, memory=memory_from_client  # or memory_from_config\n)\n\n# Start the chat\nresponse = agent.chat(\"Hi, My name is Mayank\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating a Streamlit RAG Application for Huggingface\nDESCRIPTION: Command to create a new RAG application with the Streamlit.io template. This creates the necessary files and structure for a Huggingface-compatible Streamlit application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-rag-app\nec create --template=hf/streamlit.io # inside my-rag-app directory\n```\n\n----------------------------------------\n\nTITLE: Creating AI App with Open Source Models (Mistral and Sentence Transformers)\nDESCRIPTION: Demonstrates how to create an AI app using Mistral as the LLM and Sentence Transformers for embeddings. Requires a Hugging Face token. The app adds data sources and performs a query about Elon Musk's net worth.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/quickstart.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n# Replace this with your HF token\nos.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = \"hf_xxxx\"\n\nfrom embedchain import App\n\nconfig = {\n  'llm': {\n    'provider': 'huggingface',\n    'config': {\n      'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n      'top_p': 0.5\n    }\n  },\n  'embedder': {\n    'provider': 'huggingface',\n    'config': {\n      'model': 'sentence-transformers/all-mpnet-base-v2'\n    }\n  }\n}\napp = App.from_config(config=config)\napp.add(\"https://www.forbes.com/profile/elon-musk\")\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\napp.query(\"What is the net worth of Elon Musk today?\")\n# Answer: The net worth of Elon Musk today is $258.7 billion.\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Custom Prompt in TypeScript\nDESCRIPTION: This snippet shows how to initialize a Memory instance with a custom fact extraction prompt in TypeScript. It includes configuration for the LLM provider, model settings, and the custom prompt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  version: 'v1.1',\n  llm: {\n    provider: 'openai',\n    config: {\n      apiKey: process.env.OPENAI_API_KEY || '',\n      model: 'gpt-4-turbo-preview',\n      temperature: 0.2,\n      maxTokens: 1500,\n    },\n  },\n  customPrompt: customPrompt\n};\n\nconst memory = new Memory(config);\n```\n\n----------------------------------------\n\nTITLE: Adding Memories via Mem0 Client - JavaScript\nDESCRIPTION: Saves a batch of conversation messages as memories to the Mem0 managed platform using the JavaScript SDK. Accepts an array of message objects (role and content) and user_id, returning a promise with the result or error. Requires a MemoryClient instance and API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst messages = [\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Thinking of making a sandwich. What do you recommend?\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"How about adding some cheese for extra flavor?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Actually, I don't like cheese.\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"I'll remember that you don't like cheese for future recommendations.\\\"}\\n];\\nclient.add(messages, { user_id: \\\"alex\\\" })\\n    .then(response => console.log(response))\\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Customizing Directory Loading with Embedchain in Python (Recursive, Extension Filtering)\nDESCRIPTION: This snippet demonstrates advanced customization when loading a directory in Embedchain using Python. It configures the DirectoryLoader to recursively include files and filter them by '.txt' extension, then adds the loader to the app. The code requires the 'embedchain' package and access to 'embedchain.loaders.directory_loader', as well as an OpenAI API key. The query asks about the files' topics, illustrating how custom loader options affect data ingestion and responses. Limitations include reliance on compatible file formats and query capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/directory.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\nfrom embedchain.loaders.directory_loader import DirectoryLoader\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nlconfig = {\n    \"recursive\": True,\n    \"extensions\": [\".txt\"]\n}\nloader = DirectoryLoader(config=lconfig)\napp = App()\napp.add(\"./elon-musk\", loader=loader)\nresponse = app.query(\"what are all the files related to?\")\nprint(response)\n\n# Answer: The files are related to Elon Musk.\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: Adds a web URL as a data source to the Embedchain application. The application will process the content at this URL, generate embeddings, and store them for later querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Pinecone Hybrid Search\nDESCRIPTION: Complete Python example demonstrating hybrid search implementation using Pinecone as vector database, including initialization, document addition, and querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/pinecone.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom embedchain import App\n\nconfig = {\n    'app': {\n        \"config\": {\n            \"id\": \"ec-docs-hybrid-search\"\n        }\n    },\n    'vectordb': {\n        'provider': 'pinecone',\n        'config': {\n            'metric': 'dotproduct',\n            'vector_dimension': 1536,\n            'index_name': 'my-index',\n            'serverless_config': {\n                'cloud': 'aws',\n                'region': 'us-west-2'\n            },\n            'hybrid_search': True, # Remember to set this for hybrid search\n        }\n    }\n}\n\n# Initialize app\napp = App.from_config(config=config)\n\n# Add documents\napp.add(\"/path/to/file.pdf\", data_type=\"pdf_file\", namespace=\"my-namespace\")\n\n# Query\napp.query(\"<YOUR QUESTION HERE>\", namespace=\"my-namespace\")\n\n# Chat\napp.chat(\"<YOUR QUESTION HERE>\", namespace=\"my-namespace\")\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryClient with Organization and Project IDs in Python\nDESCRIPTION: This snippet demonstrates how to initialize the MemoryClient from the mem0 Python package with organization and project IDs. It's used to attribute API usage and control access to data within specific organizational scopes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import MemoryClient\nclient = MemoryClient(org_id='YOUR_ORG_ID', project_id='YOUR_PROJECT_ID')\n```\n\n----------------------------------------\n\nTITLE: Searching Memories Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to search for memories asynchronously based on a query using the MemoryClient in JavaScript. It takes a search query and an options object with a user ID.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.search(\"What is Alice's favorite sport?\", { user_id: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Advanced Semantic Search with Raw Metadata Filtering via 'raw_filter' in Embedchain Python\nDESCRIPTION: This snippet illustrates advanced usage of the Embedchain Python API with Pinecone vector database, applying sophisticated raw metadata filters through the `raw_filter` parameter. It indexes two documents with year and person metadata and uses a complex raw query to retrieve documents about Bill Gates from the year 2024 and later. Requires Pinecone set up, valid API key, and appropriate configuration with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/search.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nfrom embedchain import App\\n\\nos.environ[\\\"PINECONE_API_KEY\\\"] = \\\"xxx\\\"\\n\\nconfig = {\\n    \\\"vectordb\\\": {\\n        \\\"provider\\\": \\\"pinecone\\\",\\n        \\\"config\\\": {\\n            \\\"metric\\\": \\\"dotproduct\\\",\\n            \\\"vector_dimension\\\": 1536,\\n            \\\"index_name\\\": \\\"ec-test\\\",\\n            \\\"serverless_config\\\": {\\\"cloud\\\": \\\"aws\\\", \\\"region\\\": \\\"us-west-2\\\"},\\n        },\\n    }\\n}\\n\\napp = App.from_config(config=config)\\n\\napp.add(\\\"https://www.forbes.com/profile/bill-gates\\\", metadata={\\\"year\\\": 2022, \\\"person\\\": \\\"gates\\\"})\\napp.add(\\\"https://en.wikipedia.org/wiki/Bill_Gates\\\", metadata={\\\"year\\\": 2024, \\\"person\\\": \\\"gates\\\"})\\n\\nprint(\\\"Filter with person: gates and year > 2023\\\")\\nraw_filter = {\\\"$and\\\": [{\\\"person\\\": \\\"gates\\\"}, {\\\"year\\\": {\\\"$gt\\\": 2023}}]}\\nresults = app.search(\\\"What is the net worth of Bill Gates?\\\", raw_filter=raw_filter)\\nprint(\\\"Num of search results: \", len(results))\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for API Keys (Python)\nDESCRIPTION: This Python snippet demonstrates setting essential environment variables using the `os` module. It assigns placeholder API keys for Mem0 and Keywords AI, along with the Keywords AI base URL. These variables are typically read by the client libraries for authentication and endpoint configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/keywords.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Set your API keys\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\nos.environ[\"KEYWORDSAI_API_KEY\"] = \"your-keywords-api-key\"\nos.environ[\"KEYWORDSAI_BASE_URL\"] = \"https://api.keywordsai.co/api/\"\n```\n\n----------------------------------------\n\nTITLE: Expected JSON Output for Search Memory Tool Execution\nDESCRIPTION: Shows the expected JSON output structure after invoking the `search_tool`. The output is a list of matching memory objects, including details like memory content, ID, metadata, score, and timestamps.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": \"1a75e827-7eca-45ea-8c5c-cfd43299f061\",\n    \"memory\": \"Name is Alex\",\n    \"user_id\": \"alex\", \n    \"hash\": \"d0fccc8fa47f7a149ee95750c37bb0ca\",\n    \"metadata\": {\n      \"food\": \"vegan\"\n    },\n    \"categories\": [\n      \"personal_details\"\n    ],\n    \"created_at\": \"2024-11-27T16:53:43.276872-08:00\",\n    \"updated_at\": \"2024-11-27T16:53:43.276885-08:00\",\n    \"score\": 0.3810526501504994\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories Asynchronously in Python\nDESCRIPTION: This code demonstrates how to retrieve all memories for a user asynchronously using the AsyncMemoryClient in Python. It takes a user ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait client.get_all(user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Creating Supabase Memory History Table - SQL\nDESCRIPTION: Provides an SQL DDL statement for Supabase to create the memory history table required for persistent memory change tracking. Must be run in Supabase's SQL editor. The table provides fields for memory ids, values, action types, timestamps, and deletion status.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\ncreate table memory_history (\n  id text primary key,\n  memory_id text not null,\n  previous_value text,\n  new_value text,\n  action text not null,\n  created_at timestamp with time zone default timezone('utc', now()),\n  updated_at timestamp with time zone,\n  is_deleted integer default 0\n);\n```\n\n----------------------------------------\n\nTITLE: Deleting All Memories by User or Agent with AsyncMemory - Python\nDESCRIPTION: This snippet shows how to delete all memory records associated with a specific user, agent, or run using AsyncMemory's delete_all method. At least one filter (user_id, agent_id, or run_id) is mandatory for this operation. The method runs asynchronously and returns deletion status.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.delete_all(user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Creating Azure OpenAI Configuration YAML in Python\nDESCRIPTION: Defines the configuration for both the language model (LLM) and embedding model to be used with Azure OpenAI. The configuration includes model names, deployment names, and various parameters for controlling model behavior. This setup is written to a YAML file for later use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = \"\"\"\nllm:\n  provider: azure_openai\n  model: gpt-35-turbo\n  config:\n    deployment_name: ec_openai_azure\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: azure_openai\n  config:\n    model: text-embedding-ada-002\n    deployment_name: ec_embeddings_ada_002\n\"\"\"\n\n# Write the multi-line string to a YAML file\nwith open('azure_openai.yaml', 'w') as file:\n    file.write(config)\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying OpenAPI Specs with embedchain in Python\nDESCRIPTION: This snippet initializes an embedchain App, adds an OpenAPI specification file either from a remote URL or local path, and queries the specification for capabilities. It requires the 'embedchain' Python library, and the input OpenAPI YAML must conform to the standard fields as outlined by the OpenAPI 3.1.0 spec. The main parameters are the OpenAPI file location (either URL or file path) and 'data_type' set to 'openapi'. Outputs are answer strings extracted by querying the spec, with errors possible if required OpenAPI fields are absent.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/openapi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add(\"https://github.com/openai/openai-openapi/blob/master/openapi.yaml\", data_type=\"openapi\")\n# Or add using the local file path\n# app.add(\"configs/openai_openapi.yaml\", data_type=\"openapi\")\n\napp.query(\"What can OpenAI API endpoint do? Can you list the things it can learn from?\")\n# Answer: The OpenAI API endpoint allows users to interact with OpenAI's models and perform various tasks such as generating text, answering questions, summarizing documents, translating languages, and more. The specific capabilities and tasks that the API can learn from may vary depending on the models and features provided by OpenAI. For more detailed information, it is recommended to refer to the OpenAI API documentation at https://platform.openai.com/docs/api-reference.\n```\n\n----------------------------------------\n\nTITLE: Creating Mem0Memory Instance from OSS Config - Python\nDESCRIPTION: Instantiates a Mem0Memory object using the configuration dictionary for Mem0 OSS. Requires an existing context dictionary and optionally allows setting 'search_msg_limit' to influence memory retrieval context size.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmemory_from_config = Mem0Memory.from_config(\n    context=context,\n    config=config,\n    search_msg_limit=4,  # optional, default is 5\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Azure OpenAI Environment Variables in Python\nDESCRIPTION: Configures the necessary environment variables for Azure OpenAI integration. These variables include the API type, base URL, API key, and version, which are all required for authenticating with Azure OpenAI services.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://xxx.openai.azure.com/\"\nos.environ[\"OPENAI_API_KEY\"] = \"xxx\"\nos.environ[\"OPENAI_API_VERSION\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Groundedness Evaluation\nDESCRIPTION: Demonstrates how to customize groundedness evaluation by specifying model and API key parameters using the GroundednessConfig class.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.config.evaluation.base import GroundednessConfig\nfrom embedchain.evaluation.metrics import Groundedness\n\neval_config = GroundednessConfig(model='gpt-4', api_key=\"sk-xxx\")\nmetric = Groundedness(config=eval_config)\nscore = metric.evaluate(dataset)\n```\n\n----------------------------------------\n\nTITLE: Adding Irrelevant Memory with Custom Prompt in Python\nDESCRIPTION: This example demonstrates adding a memory that is not relevant to the custom prompt's focus in Python. It shows that no memory is added when the message doesn't match the prompt's criteria.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"I like going to hikes\", user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with User and Agent IDs in Mem0 (JavaScript)\nDESCRIPTION: This snippet demonstrates how to add memories to Mem0 using the JavaScript client, associating them with both a user ID and an agent ID.  It defines a list of messages and uses the `client.add` method to store them with the specified user and agent identifiers. The function uses a promise to handle the asynchronous nature of the API call and logs the response or any errors to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_15\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm travelling to San Francisco\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! I'm going to Dubai next month.\"},\n]\n\nclient.add(messages, { user_id: \"user1\", agent_id: \"agent1\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Direct Memory Retrieval and Prompt Injection\nDESCRIPTION: Implementation of direct memory retrieval and injection into the chat prompt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrelevant_memories = MEM0_MEMORY_CLIENT.search(user_query, user_id=USER_ID, limit=3)\nrelevant_memories_text = '\\n'.join(mem['memory'] for mem in relevant_memories)\nprint(f\"Relevant memories:\")\nprint(relevant_memories_text)\n\nprompt = f\"{user_query}\\n Coding Preferences: \\n{relevant_memories_text}\"\nbrowse_result = user_proxy.initiate_chat(gpt_assistant, message=prompt)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Memories in Python\nDESCRIPTION: This snippet shows how to retrieve specific memories for a user using the 'search' method. It includes the search query and specifies the output format.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/direct-import.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient.search(\"What is Alice's favorite sport?\", user_id=\"alice\", output_format=\"v1.1\")\n```\n\n----------------------------------------\n\nTITLE: Querying GitHub Data with Embedchain Loader in Python\nDESCRIPTION: This snippet shows how to use the Embedchain framework with the GithubLoader to index a GitHub repository and perform LLM-powered queries. It demonstrates instantiating a loader with authentication, adding the repository as a data source with type 'github', and running a natural language query against the indexed data. The code depends on embedchain and the embedchain.loaders.github module, requiring a valid GitHub token for access. Inputs include repository details and a query string; outputs are LLM-generated answers. Limitations include the need for a valid token and network connectivity.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/custom.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\nfrom embedchain.loaders.github import GithubLoader\\n\\napp = App()\\n\\nloader = GithubLoader(config={\"token\": \"ghp_xxx\"})\\n\\napp.add(\"repo:embedchain/embedchain type:repo\", data_type=\"github\", loader=loader)\\n\\napp.query(\"What is Embedchain?\")\\n# Answer: Embedchain is a Data Platform for Large Language Models (LLMs). It allows users to seamlessly load, index, retrieve, and sync unstructured data in order to build dynamic, LLM-powered applications. There is also a JavaScript implementation called embedchain-js available on GitHub.\n```\n\n----------------------------------------\n\nTITLE: Using Memory Functions with Explicit API Keys and IDs\nDESCRIPTION: Shows how to use standalone memory functions with explicit API key and organization/project identifiers instead of environment variables. Useful for multi-tenant applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait addMemories(messages, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\nawait retrieveMemories(prompt, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\nawait getMemories(prompt, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryClient for Inclusion Rules (Mem0 Python)\nDESCRIPTION: Shows how to set up the MemoryClient and specify what types of information should be included in stored memories. The example sets an API key, initializes the client, and provides a list of conversation messages, along with inclusion criteria. Dependencies: mem0 Python library, valid API key. Parameters: 'includes' defines the topics to be remembered (here, 'sports related things'). Input: conversation history and inclusion rule; Output: only relevant memories based on inclusion criteria are stored.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/selective-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom mem0 import MemoryClient\\n\\nos.environ[\\\"MEM0_API_KEY\\\"] = \\\"your-api-key\\\"\\n\\nm = MemoryClient()\\n\\n# Define what to include\\nincludes = \\\"sports related things\\\"\\n\\nmessages = [\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hi, my name is Alice and I love to play badminton\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Nice to meet you, Alice! Badminton is a great sport.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love music festivals\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Music festivals are exciting! Do you have a favorite one?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love eating spicy food\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Spicy food is delicious! What's your favorite spicy dish?\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I love playing baseball with my friends\\\"},\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Baseball with friends sounds fun!\\\"},\\n]\\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclient.add(messages, user_id=\\\"alice\\\", includes=includes)\n```\n\nLANGUAGE: json\nCODE:\n```\nUser's name is Alice.\\nAlice loves to play badminton.\\nUser loves playing baseball with friends.\\n\n```\n\n----------------------------------------\n\nTITLE: Resetting All Memories - TypeScript\nDESCRIPTION: Resets or clears all memories from the storage using Memory.reset. This operation is destructive and removes all stored memory data globally. Recommended for testing or administrative use only.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nawait memory.reset(); // Reset all memories\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Flowise via npm\nDESCRIPTION: This Bash snippet demonstrates how to install the Flowise package globally using the Node Package Manager (npm) and then start the Flowise application server using npx. It's a prerequisite step for using the Mem0 integration and requires NodeJS version 18.15.0 or higher.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/flowise.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g flowise\nnpx flowise start\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Ollama Embedder with Mem0 in Python\nDESCRIPTION: This code snippet demonstrates how to set up and use Ollama embedding models with Mem0. It includes setting the OpenAI API key, configuring the embedder, initializing the Memory object, and adding messages to it.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/ollama.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"ollama\",\n        \"config\": {\n            \"model\": \"mxbai-embed-large\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Pinecone Serverless Configuration\nDESCRIPTION: YAML configuration for serverless Pinecone setup, specifying cloud provider and region.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/pinecone.mdx#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: pinecone\n  config:\n    metric: cosine\n    vector_dimension: 1536\n    index_name: my-pinecone-index\n    serverless_config:\n      cloud: aws\n      region: us-west-2\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Memory - Python\nDESCRIPTION: This Python snippet retrieves a specific memory by its ID using the `client.get` method.  It requires the `memory_id` as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\nmemory = client.get(memory_id=\"582bbe6d-506b-48c6-a4c6-5df3b1e63428\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Google AI Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with Google AI's embedding model. This snippet shows how to set the Google API key as an environment variable and configure the embedding model with specific parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"GOOGLE_API_KEY\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: google\n  config:\n    model: 'models/embedding-001'\n    task_type: \"retrieval_document\"\n    title: \"Embeddings for Embedchain\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up the CrewAI Crew with Memory Integration (Python)\nDESCRIPTION: Defines setup_crew() for assembling a CrewAI Crew instance using a list of agents, tasks, and Mem0-powered memory integration. The process is set to sequential. The memory_config links the crew to persistent per-user memory via Mem0, allowing agents to maintain user context across tasks. The function requires lists of initialized agents and tasks and, when used, makes personalized interaction possible.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef setup_crew(agents: list, tasks: list):\n    \"\"\"Set up a crew with Mem0 memory integration\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        process=Process.sequential,\n        memory=True,\n        memory_config={\n            \"provider\": \"mem0\",\n            \"config\": {\"user_id\": \"crew_user_1\"},\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating AI App with Paid Models (OpenAI)\nDESCRIPTION: Shows how to create an AI app using OpenAI for both LLM and embeddings. Requires an OpenAI API key. The app adds data sources and performs a query about Elon Musk's net worth.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/quickstart.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# Replace this with your OpenAI key\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\"\n\napp = App()\napp.add(\"https://www.forbes.com/profile/elon-musk\")\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\napp.query(\"What is the net worth of Elon Musk today?\")\n# Answer: The net worth of Elon Musk today is $258.7 billion.\n```\n\n----------------------------------------\n\nTITLE: Setting up Cohere LLM with Embedchain\nDESCRIPTION: Example showing how to configure Cohere as the LLM provider with Embedchain. This requires installing dependencies, setting the COHERE_API_KEY environment variable, and using a YAML config file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"COHERE_API_KEY\"] = \"xxx\"\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: cohere\n  config:\n    model: large\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n```\n\n----------------------------------------\n\nTITLE: Using Mem0 APIs for Memory Management in Python\nDESCRIPTION: This snippet demonstrates how to use Mem0 APIs for various memory operations, including storing messages, retrieving memories, searching memories, and getting memory history.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient() # get api_key from https://app.mem0.ai/\n\n# Store messages\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n]\nresult = client.add(messages, user_id=\"alex\")\nprint(result)\n\n# Retrieve memories\nall_memories = client.get_all(user_id=\"alex\")\nprint(all_memories)\n\n# Search memories\nquery = \"What do you know about me?\"\nrelated_memories = client.search(query, user_id=\"alex\")\n\n# Get memory history\nhistory = client.history(memory_id=\"m1\")\nprint(history)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Provider with Custom Configuration (TypeScript)\nDESCRIPTION: Demonstrates how to initialize the Mem0 client for Vercel AI SDK with provider selection, API keys, and optional global configuration (e.g., user_id, org_id, project_id). The function createMem0 is imported from @mem0/vercel-ai-provider. Dependencies: @mem0/vercel-ai-provider, relevant provider API keys. Inputs are configuration parameters; output is a configured Mem0 provider instance. Supports environment variable usage for sensitive information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0({\n  provider: \"openai\",\n  mem0ApiKey: \"m0-xxx\",\n  apiKey: \"provider-api-key\",\n  config: {\n    compatibility: \"strict\",\n  },\n  // Optional Mem0 Global Config\n  mem0Config: {\n    user_id: \"mem0-user-id\",\n    org_id: \"mem0-org-id\",\n    project_id: \"mem0-project-id\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating MemoryClient in JavaScript\nDESCRIPTION: This snippet demonstrates how to instantiate the MemoryClient in JavaScript. It imports the MemoryClient class from the 'mem0ai' package and creates a new instance with the API key passed as an option. The apiKey parameter is essential for authenticating with the Mem0 Platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport MemoryClient from 'mem0ai';\nconst client = new MemoryClient({ apiKey: 'your-api-key' });\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Categories During API Call in Python\nDESCRIPTION: This code shows how to set custom categories during the 'add' API call, overriding project-level categories. It includes setting up the client, defining custom categories, and adding memories with these categories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient(api_key=\"<your_mem0_api_key>\")\n\ncustom_categories = [\n    {\"seeking_structure\": \"Documents goals around creating routines, schedules, and organized systems in various life areas\"},\n    {\"personal_information\": \"Basic information about the user including name, preferences, and personality traits\"}\n]\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"My name is Alice. I need help organizing my daily schedule better. I feel overwhelmed trying to balance work, exercise, and social life.\"},\n    {\"role\": \"assistant\", \"content\": \"I understand how overwhelming that can feel. Let's break this down together. What specific areas of your schedule feel most challenging to manage?\"},\n    {\"role\": \"user\", \"content\": \"I want to be more productive at work, maintain a consistent workout routine, and still have energy for friends and hobbies.\"},\n    {\"role\": \"assistant\", \"content\": \"Those are great goals for better time management. What's one small change you could make to start improving your daily routine?\"},\n]\n\nclient.add(messages, user_id=\"alice\", custom_categories=custom_categories)\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Specific Memory by ID using AsyncMemory - Python\nDESCRIPTION: This snippet demonstrates how to retrieve an individual memory by its unique memory_id using the get method of AsyncMemory. The method expects the memory_id parameter and returns the corresponding memory object (if found). Best used inside an async function for efficient and non-blocking operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.get(memory_id=\"memory-id-here\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Custom Categories in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the current custom categories set for a project using the Mem0 API. It uses the 'get_project' method with a specific field parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get current custom categories\ncategories = client.get_project(fields=[\"custom_categories\"])\nprint(categories)\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph Memory with Custom LLM - Advanced (TypeScript)\nDESCRIPTION: This TypeScript snippet configures both global and graph-store-specific LLM providers for Mem0 with Neo4j. The 'llm' configuration appears both at the root and within graphStore, specifying OpenAI provider settings. As in the basic setup, enableGraph must be true; both main and sub LLM configs are passed to the Memory constructor. Only one LLM applies to each operation, with graphStore's config taking precedence.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst config = {\n    llm: {\n        provider: \"openai\",\n        config: {\n            model: \"gpt-4o\",\n            temperature: 0.2,\n            max_tokens: 2000,\n        }\n    },\n    enableGraph: true,\n    graphStore: {\n        provider: \"neo4j\",\n        config: {\n            url: \"neo4j+s://xxx\",\n            username: \"neo4j\",\n            password: \"xxx\",\n        },\n        llm: {\n            provider: \"openai\",\n            config: {\n                model: \"gpt-4o-mini\",\n                temperature: 0.0,\n            }\n        }\n    }\n}\n\nconst memory = new Memory(config);\n\n```\n\n----------------------------------------\n\nTITLE: Batch Delete Memories with Mem0\nDESCRIPTION: Deletes multiple memories in a single API call using Mem0. The examples show how to construct the list of memory IDs to delete and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memory IDs to delete.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_75\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X DELETE \"https://api.mem0.ai/v1/memories/batch/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"memory_ids\": [\n             {\\\"memory_id\\\": \\\"285ed74b-6e05-4043-b16b-3abd5b533496\\\"},\n             {\\\"memory_id\\\": \\\"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\\\"}\n         ]\n     }'\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories for a User with Mem0 in Python\nDESCRIPTION: Demonstrates fetching all memories stored for a specific user using get_all(). Requires user_id as a parameter and returns a list of result dicts, each containing memory and metadata. Response includes memory hashes and timestamps for auditability.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Get all memories\nall_memories = m.get_all(user_id=\"alice\")\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"results\": [\n            {\n        \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n        \"memory\": \"User is planning to watch a movie tonight.\",\n        \"hash\": \"1a271c007316c94377175ee80e746a19\",\n        \"created_at\": \"2025-02-27T16:33:20.557Z\",\n        \"updated_at\": \"2025-02-27T16:33:27.051Z\",\n        \"metadata\": {\n            \"category\": \"movie_recommendations\"\n        },\n        \"user_id\": \"alice\"\n        },\n        {\n        \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n        \"memory\": \"User loves sci-fi movies.\",\n        \"hash\": \"285d07801ae42054732314853e9eadd7\",\n        \"created_at\": \"2025-02-27T16:33:20.560Z\",\n        \"updated_at\": None,\n        \"metadata\": {\n            \"category\": \"movie_recommendations\"\n        },\n        \"user_id\": \"alice\"\n        },\n        {\n        \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n        \"memory\": \"User is not a big fan of thriller movies.\",\n        \"hash\": \"285d07801ae42054732314853e9eadd7\",\n        \"created_at\": \"2025-02-27T16:33:20.560Z\",\n        \"updated_at\": None,\n        \"metadata\": {\n            \"category\": \"movie_recommendations\"\n        },\n        \"user_id\": \"alice\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Embedchain REST API Container\nDESCRIPTION: Single command to run the Embedchain REST API using a pre-built Docker container. Runs in detached mode on port 8080.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/rest-api/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --name embedchain -p 8080:8080 embedchain/rest-api:latest\n```\n\n----------------------------------------\n\nTITLE: Advanced Memory Integration with OpenAI\nDESCRIPTION: Shows advanced memory retrieval based on structured messages and integration with OpenAI. This demonstrates how to use Mem0's memory capabilities with the standard OpenAI provider.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText, LanguageModelV1Prompt } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { retrieveMemories } from \"@mem0/vercel-ai-provider\";\n\n// New format using system parameter for memory context\nconst messages: LanguageModelV1Prompt = [\n  {\n    role: \"user\",\n    content: [\n      { type: \"text\", text: \"Suggest me a good car to buy.\" },\n      { type: \"text\", text: \"Why is it better than the other cars for me?\" },\n      { type: \"text\", text: \"Give options for every price range.\" },\n    ],\n  },\n];\n\nconst memories = await retrieveMemories(messages, { user_id: \"borat\" });\n\nconst { text } = await generateText({\n  model: openai(\"gpt-4-turbo\"),\n  messages: messages,\n  system: memories,\n});\n```\n\n----------------------------------------\n\nTITLE: Adding a Memory with Metadata - Python (Open Source)\nDESCRIPTION: Adds a single memory directly to the open-source Memory instance in Python, optionally assigning metadata and a user_id. Returns a result containing memory IDs, event status, and summarized memory data. This bypasses cloud APIs and uses only local execution.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_17\n\nLANGUAGE: Code\nCODE:\n```\n# For a user\\nresult = m.add(\\\"I like to drink coffee in the morning and go for a walk.\\\", user_id=\\\"alice\\\", metadata={\\\"category\\\": \\\"preferences\\\"})\n```\n\n----------------------------------------\n\nTITLE: Setting up Azure OpenAI with Embedchain\nDESCRIPTION: Example showing how to configure Azure OpenAI as the LLM provider with Embedchain. This requires setting several Azure-specific environment variables and using a YAML config file to specify deployment names.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://xxx.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_KEY\"] = \"xxx\"\nos.environ[\"OPENAI_API_VERSION\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: azure_openai\n  config:\n    model: gpt-4o-mini\n    deployment_name: your_llm_deployment_name\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: azure_openai\n  config:\n    model: text-embedding-ada-002\n    deployment_name: you_embedding_model_deployment_name\n```\n\n----------------------------------------\n\nTITLE: Example Context Dictionary for Mem0Memory - Python\nDESCRIPTION: Illustrates the structure of the 'context' dictionary, which may contain 'user_id', 'agent_id', or 'run_id'. At least one field is required for Mem0Memory to identify the user, agent, or conversation uniquely.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontext = {\n    \"user_id\": \"user_1\",\n    \"agent_id\": \"agent_1\",\n    \"run_id\": \"run_1\",\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Memory - TypeScript\nDESCRIPTION: Updates the textual content of a memory by its unique ID. The Memory.update function receives a memory ID and a new value string, and returns a result indicating update success. Requires prior existence of the memory. Output is a confirmation message.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await memory.update(\n  '892db2ae-06d9-49e5-8b3e-585ef9b85b8e',\n  'I love India, it is my favorite country.'\n);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI LLM with Embedchain\nDESCRIPTION: Basic example showing how to set up OpenAI as the LLM provider with Embedchain. This requires setting the OPENAI_API_KEY environment variable and creating an App instance to add data and query it.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'xxx'\n\napp = App()\napp.add(\"https://en.wikipedia.org/wiki/OpenAI\")\napp.query(\"What is OpenAI?\")\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App\nDESCRIPTION: Adds a web URL as a data source to the Embedchain app for retrieval and question answering. The app will scrape and index the content from the provided URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/gpt4all.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Discord Bot with Docker\nDESCRIPTION: Command to run the Discord bot using Docker. It sets up environment variables for OpenAI API key and Discord bot token, maps port 8080, and uses the official Embedchain Discord bot image.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name discord-bot -e OPENAI_API_KEY=sk-xxx -e DISCORD_BOT_TOKEN=xxx -p 8080:8080 embedchain/discord-bot:latest\n```\n\n----------------------------------------\n\nTITLE: Adding a Memory about Hiking in Python and TypeScript\nDESCRIPTION: Demonstrates how to add a simple memory about hiking to Mem0. The memory is associated with a specific user ID to maintain personal context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"I like going to hikes\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"I like going to hikes\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Specific Date - Python\nDESCRIPTION: This Python snippet demonstrates how to add a memory to Mem0 with a specific date (January 1, 2023) as a Unix timestamp. It directly uses the pre-defined timestamp value and adds the memory using the `client.add` method.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# January 1, 2023 timestamp\njanuary_2023_timestamp = 1672531200  # Unix timestamp for 2023-01-01 00:00:00 UTC\n\nclient.add(\"Important historical information\", user_id=\"user1\", timestamp=january_2023_timestamp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with LanceDB\nDESCRIPTION: Creates an Embedchain App instance with a configuration that specifies LanceDB as the vector database provider and sets the collection name to 'lancedb-index'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/lancedb.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"vectordb\": {\n        \"provider\": \"lancedb\",\n            \"config\": {\n                \"collection_name\": \"lancedb-index\"\n            }\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Docker Build and Deploy Workflow\nDESCRIPTION: Complete Docker workflow including building the image locally, running the container, and pushing to Docker registry.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/rest-api/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t embedchain/rest-api:latest .\ndocker run -d --name embedchain -p 8080:8080 embedchain/rest-api:latest\ndocker image push embedchain/rest-api:latest\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Single Memory by ID - TypeScript\nDESCRIPTION: Fetches a single memory record by its unique ID using Memory.get in TypeScript. Accepts a memory ID string as input and returns a full memory object including hash, metadata, and userId. Use for detailed inspection or editing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// Get a single memory by ID\nconst singleMemory = await memory.get('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');\nconsole.log(singleMemory);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Short-Term Memories - JavaScript\nDESCRIPTION: This JavaScript code retrieves short-term memories associated with a specific user and run ID.  It uses `client.getAll` to filter the memories and handles the promise returned by the method.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_45\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.getAll({ user_id: \"alex\", run_id: \"trip-planning-2024\", page: 1, page_size: 50 })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Structured Message Generation with Memory Context (TypeScript)\nDESCRIPTION: Shows the use of structured message arrays as prompts, improving context delivery. Requires generateText and createMem0. The model is parameterized with user_id and a messages array, each with roles and text objects. Inputs: message array, expected output: generated text. Ensures more flexible, structured conversational AI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0(\"gpt-4-turbo\", { user_id: \"borat\" }),\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        { type: \"text\", text: \"Suggest me a good car to buy.\" },\n        { type: \"text\", text: \"Why is it better than the other cars for me?\" },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Reusing an Existing Embedchain Vector Database for Querying (Python)\nDESCRIPTION: This snippet shows how to reuse a previously created persistent Embedchain vector database. It initializes the `App` using the *same* configuration ID (`app-1`) as the one used during creation. This allows the application to connect to the existing database without re-indexing data. It then demonstrates querying the database with a specific question. Depends on the `embedchain` library and an existing database associated with 'app-1'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/data-type-handling.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nconfig = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"app-1\"\n        }\n    }\n}\nnaval_chat_bot = App.from_config(config=config)\nprint(naval_chat_bot.query(\"What unique capacity does Naval argue humans possess when it comes to understanding explanations or concepts?\"))\n```\n\n----------------------------------------\n\nTITLE: Loading Embedchain App Configuration from YAML File in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize an Embedchain `App` by loading its configuration from an external YAML file named `config.yaml`. It uses the `App.from_config()` class method, passing the path to the configuration file via the `config_path` argument. This approach separates configuration from the application code, making it easier to manage settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Vertex AI Embeddings with mem0\nDESCRIPTION: This snippet demonstrates how to configure environment variables for Google Cloud credentials, set up Vertex AI embedding parameters, and use them with the Memory class from mem0. It shows a complete workflow including adding conversation messages to memory with specific embedding types.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/vertexai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\n# Set the path to your Google Cloud credentials JSON file\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/credentials.json\"\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"vertexai\",\n        \"config\": {\n            \"model\": \"text-embedding-004\",\n            \"memory_add_embedding_type\": \"RETRIEVAL_DOCUMENT\",\n            \"memory_update_embedding_type\": \"RETRIEVAL_DOCUMENT\",\n            \"memory_search_embedding_type\": \"RETRIEVAL_QUERY\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with JavaScript\nDESCRIPTION: POST request to create a new Embedchain application with a specified app ID using JavaScript fetch API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = fetch(\"http://localhost:8080/create?app_id=my-app\", {\n  method: \"POST\",\n}).then((res) => res.json());\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Setting up Azure OpenAI with Mem0 in TypeScript\nDESCRIPTION: This snippet shows how to configure and use Azure OpenAI with Mem0 in TypeScript. It includes the configuration of the Memory object with Azure OpenAI provider and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/azure_openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'azure_openai',\n    config: {\n      apiKey: process.env.AZURE_OPENAI_API_KEY || '',\n      modelProperties: {\n        endpoint: 'https://your-api-base-url',\n        deployment: 'your-deployment-name',\n        modelName: 'your-model-name',\n        apiVersion: 'version-to-use',\n        // Any other parameters you want to pass to the Azure OpenAI API\n      },\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Initializing xAI Memory System with Python\nDESCRIPTION: Demonstrates how to configure and initialize an xAI-powered memory system using the mem0 library. Shows environment variable setup, configuration object creation, and basic message handling functionality. Requires OpenAI API key for embeddings and xAI API key for LLM access.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/xAI.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"XAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"xai\",\n        \"config\": {\n            \"model\": \"grok-3-beta\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Using addMemories Tool in ElevenLabs Agent Prompts (plaintext)\nDESCRIPTION: Demonstrates how to instruct the agent, within prompts or system messages, to store important user information by calling the addMemories tool. Reinforces that information such as preferences or personal details should be captured for future use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen the user shares important information like preferences or personal details, \nuse the addMemories function to store this information for future reference.\n```\n\n----------------------------------------\n\nTITLE: Resetting App Data Using Embedchain in Python\nDESCRIPTION: Demonstrates how to use the Embedchain library's reset() method to remove all existing data from a Retrieval-Augmented Generation (RAG) application. Dependencies: 'embedchain' Python package must be installed. The snippet creates an App instance, imports a data source using add(), and then clears the application state with reset(). Input is a Forbes profile URL; after reset(), the App contains no sources or indexed documents.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/reset.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n# Reset the app\napp.reset()\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with LangChain OpenAI Model in TypeScript\nDESCRIPTION: This snippet shows how to configure Mem0 Memory in TypeScript using LangChain's ChatOpenAI model. It includes setting up the model with parameters, creating a configuration object, and storing conversation messages with associated user ID and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/langchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0ai\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst openai_model = new ChatOpenAI({\n    model: \"gpt-4o\",\n    temperature: 0.2,\n    max_tokens: 2000\n})\n\nconst config = {\n    \"llm\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": openai_model\n        }\n    }\n}\n\nconst memory = new Memory(config);\n\nconst messages = [\n    { role: \"user\", content: \"I'm planning to watch a movie tonight. Any recommendations?\" },\n    { role: \"assistant\", content: \"How about a thriller movies? They can be quite engaging.\" },\n    { role: \"user\", content: \"I'm not a big fan of thriller movies but I love sci-fi movies.\" },\n    { role: \"assistant\", content: \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\" }\n]\n\nmemory.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Loading an Existing AI Assistant\nDESCRIPTION: Demonstrates how to load an existing assistant using its ID, and optionally specifying a thread ID to continue a conversation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/opensource-assistant.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load an assistant and create a new thread\nassistant = AIAssistant(assistant_id=\"asst_xxx\")\n\n# Load a specific thread for an assistant\nassistant = AIAssistant(assistant_id=\"asst_xxx\", thread_id=\"thread_xxx\")\n```\n\n----------------------------------------\n\nTITLE: Configuring mem0 MCP Server SSE Endpoint in Cursor\nDESCRIPTION: This URL (`http://0.0.0.0:8080/sse`) represents the Server-Sent Events (SSE) endpoint for the `mem0` MCP server. It must be entered into the Cursor IDE settings (`Settings` > `Cursor Settings` > `Features` > `MCP Servers`) when adding `mem0` as a new MCP provider (type `sse`) to enable communication between Cursor and the `mem0` service. The `mem0-mcp` server must be running and accessible at this address.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/mcp-server.mdx#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://0.0.0.0:8080/sse\n```\n\n----------------------------------------\n\nTITLE: Standalone Memory Utilities (addMemories, retrieveMemories, getMemories) (TypeScript)\nDESCRIPTION: Exemplifies calling addMemories, retrieveMemories, and getMemories as standalone utility functions for fine-grained memory management. Each function accepts memory data or prompt, user_id, mem0ApiKey, org_id, and project_id. Dependencies: @mem0/vercel-ai-provider. Outputs are persisted memory, retrieved strings, or raw object arrays, depending on the method. MEM0_API_KEY can be set as env or passed directly.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait addMemories(messages, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\nawait retrieveMemories(prompt, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\nawait getMemories(prompt, { user_id: \"borat\", mem0ApiKey: \"m0-xxx\", org_id: \"org_xx\", project_id: \"proj_xx\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources with JavaScript\nDESCRIPTION: POST request to add a web page data source to an Embedchain application using JavaScript fetch API. This example adds Elon Musk's Forbes profile page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = fetch(\"http://localhost:8080/my-app/add\", {\n  method: \"POST\",\n  body: \"source=https://www.forbes.com/profile/elon-musk&data_type=web_page\",\n}).then((res) => res.json());\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Memory with Qdrant Vector Store in TypeScript\nDESCRIPTION: This snippet shows how to configure and initialize a Mem0 Memory object using Qdrant as the vector store in TypeScript. It includes creating a configuration object and adding messages to the memory asynchronously.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/qdrant.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  vectorStore: {\n    provider: 'qdrant',\n    config: {\n      collectionName: 'memories',\n      embeddingModelDims: 1536,\n      host: 'localhost',\n      port: 6333,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Memory in Python\nDESCRIPTION: This Python snippet demonstrates how to create memory for an agent using the Mem0 Platform. It creates a list of messages related to the agent's role and personality, then passes these messages along with the agent ID to the `add` method of the MemoryClient. This allows the agent to retain context across different sessions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an AI tutor with a personality. Give yourself a name for the user.\"},\n    {\"role\": \"assistant\", \"content\": \"Understood. I'm an AI tutor with a personality. My name is Alice.\"}\n]\n\nclient.add(messages, agent_id=\"ai-tutor\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Runtime Loop for Mem0 and OpenAI Agents Integration\nDESCRIPTION: This Python function implements the main runtime loop for the memory agent, handling user input and agent responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    current_agent: Agent[Mem0Context] = memory_agent\n    input_items: list[TResponseInputItem] = []\n    context = Mem0Context()\n    while True:\n        user_input = input(\"Enter your message (or 'quit' to exit): \")\n        if user_input.lower() == 'quit':\n            break\n        input_items.append({\"content\": user_input, \"role\": \"user\"})\n        result = await Runner.run(current_agent, input_items, context=context)\n        for new_item in result.new_items:\n            agent_name = new_item.agent.name\n            if isinstance(new_item, MessageOutputItem):\n                print(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n            elif isinstance(new_item, ToolCallItem):\n                print(f\"{agent_name}: Calling a tool\")\n            elif isinstance(new_item, ToolCallOutputItem):\n                print(f\"{agent_name}: Tool call output: {new_item.output}\")\n            else:\n                print(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n        input_items = result.to_input_list()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Question-Answer Loop\nDESCRIPTION: Creates an interactive loop for asking questions and receiving answers from the configured Embedchain app, with options to quit the program.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with OpenAI Embeddings via LangChain in TypeScript\nDESCRIPTION: Demonstrates the TypeScript implementation of Mem0 with LangChain OpenAI embeddings, including configuration and message handling.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/langchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from \"mem0ai\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst config = {\n    \"embedder\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": embeddings\n        }\n    }\n}\n\nconst memory = new Memory(config);\n\nconst messages = [\n    { role: \"user\", content: \"I'm planning to watch a movie tonight. Any recommendations?\" },\n    { role: \"assistant\", content: \"How about a thriller movies? They can be quite engaging.\" },\n    { role: \"user\", content: \"I'm not a big fan of thriller movies but I love sci-fi movies.\" },\n    { role: \"assistant\", content: \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\" }\n]\n\nmemory.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Querying for Relevant Memories - JavaScript\nDESCRIPTION: Uses the JavaScript SDK's search function to retrieve matching memories, filtered by query string and user_id. Returns a promise, which when resolved yields relevant memory objects. Requires a client object, API key, and appropriate method parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst query = \\\"I'm craving some pizza. Any recommendations?\\\";\\nconst filters = {\\n    \\\"AND\\\": [\\n        {\\n            \\\"user_id\\\": \\\"alex\\\"\\n        }\\n    ]\\n};\\nclient.search(query, { version: \\\"v2\\\", filters })\\n    .then(results => console.log(results))\\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memories by Categories - cURL\nDESCRIPTION: These cURL commands retrieve memories based on specified categories. They demonstrate retrieving memories with a single category, multiple categories, using pagination and specific keywords.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_52\n\nLANGUAGE: cURL\nCODE:\n```\n# Get memories with specific categories\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes\" \\\n     -H \"Authorization: Token your-api-key\"\n\n# Get memories with multiple categories\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes,food_preferences\" \\\n     -H \"Authorization: Token your-api-key\"\n\n# Custom pagination with categories\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&categories=likes&page=1&page_size=50\" \\\n     -H \"Authorization: Token your-api-key\"\n\n# Get memories with specific keywords\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&keywords=to play&page=1&page_size=50\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral AI LLM Provider\nDESCRIPTION: This code snippet and YAML configuration demonstrate how to set up Mistral AI as the LLM provider for Embedchain. It includes setting the API key, adding data, and querying the app.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"MISTRAL_API_KEY\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\nresponse = app.query(\"what is the net worth of Elon Musk?\")\n# As of January 16, 2024, Elon Musk's net worth is $225.4 billion.\n\nresponse = app.chat(\"which companies does elon own?\")\n# Elon Musk owns Tesla, SpaceX, Boring Company, Twitter, and X.\n\nresponse = app.chat(\"what question did I ask you already?\")\n# You have asked me several times already which companies Elon Musk owns, specifically Tesla, SpaceX, Boring Company, Twitter, and X.\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: mistralai\n  config:\n    model: mistral-tiny\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\nembedder:\n  provider: mistralai\n  config:\n    model: mistral-embed\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories (Python and TypeScript)\nDESCRIPTION: These examples demonstrate how to fetch all memories associated with a user (here, 'alice') from Graph Memory. In both Python and TypeScript, the call returns a JSON object containing arrays of memory objects and extracted entities/relationships. The user_id must be specified; results include details like memory text, hash, metadata, timestamps, and graph relationships.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nm.get_all(user_id=\"alice\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.getAll({ userId: \"alice\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    'memories': [\n        {\n            'id': 'de69f426-0350-4101-9d0e-5055e34976a5',\n            'memory': 'Likes pizza',\n            'hash': '92128989705eef03ce31c462e198b47d',\n            'metadata': None,\n            'created_at': '2024-08-20T14:09:27.588719-07:00',\n            'updated_at': None,\n            'user_id': 'alice'\n        }\n    ],\n    'entities': [\n        {\n            'source': 'alice',\n            'relationship': 'likes',\n            'target': 'pizza'\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain with LanceDB and OpenAI\nDESCRIPTION: This script demonstrates how to set up Embedchain with LanceDB as the vector database and OpenAI as the LLM provider. It shows the basic workflow of adding a data source and creating an interactive query loop.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/lancedb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# set OPENAI_API_KEY as env variable\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\n# create Embedchain App and set config\napp = App.from_config(config={\n    \"vectordb\": {\n        \"provider\": \"lancedb\",\n            \"config\": {\n                \"collection_name\": \"lancedb-index\"\n            }\n        }\n    }\n)\n\n# add data source and start query in\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n# query continuously\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Structured Outputs with Mem0\nDESCRIPTION: Demonstrates how to configure Mem0 to use OpenAI's structured-outputs feature with a specific model. This setup allows for more structured responses from the language model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/openai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai_structured\",\n        \"config\": {\n            \"model\": \"gpt-4o-2024-08-06\",\n            \"temperature\": 0.0,\n        }\n    }\n}\n\nm = Memory.from_config(config)\n```\n\n----------------------------------------\n\nTITLE: Reset Mem0 Client\nDESCRIPTION: Resets the Mem0 client, deleting all users and memories. This is demonstrated using both Python and JavaScript.  The expected result is logged to the console in the JavaScript example.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\nclient.reset()\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot in Development\nDESCRIPTION: Command to run the Discord bot in development environment for testing connectivity with the Embedchain app before deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython app.py  #To run the app in development environment\n```\n\n----------------------------------------\n\nTITLE: Defining Task Creation for CrewAI Agent (Python)\nDESCRIPTION: Provides create_planning_task(), a function that creates a CrewAI Task assigned to a specific agent for researching travel locations. The function generates descriptive prompt text using the given destination and expects an output in the form of a detailed place list. It relies on Task and related CrewAI classes being imported and available. Parameters include the agent instance and the travel destination as a string.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_planning_task(agent, destination: str):\n    \"\"\"Create a travel planning task\"\"\"\n    return Task(\n        description=f\"\"\"Find places to live, eat, and visit in {destination}.\"\"\",\n        expected_output=f\"A detailed list of places to live, eat, and visit in {destination}.\",\n        agent=agent,\n    )\n```\n\n----------------------------------------\n\nTITLE: Delete Memory Using Add with Mem0\nDESCRIPTION: Demonstrates deleting memory using the `add()` method in Mem0 by passing a natural language command. Examples are provided in Python, JavaScript, and cURL.  The cURL example includes specifying the message content and user ID in the request body.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_69\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Delete all of my food preferences\\\"}],\n         \"user_id\": \"alex\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Defining GET Endpoint for Memory History in OpenAPI\nDESCRIPTION: This YAML snippet defines the OpenAPI specification for the GET endpoint to retrieve memory history. It specifies the path /v1/memories/{memory_id}/history/ for fetching the history of a specific memory identified by memory_id.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/history-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: get /v1/memories/{memory_id}/history/\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Agent Memories - Python\nDESCRIPTION: This snippet retrieves all memories associated with a specific AI agent, paginated with a page size of 50. It utilizes the `client.get_all` method, specifying the `agent_id`, `page`, and `page_size` parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\nagent_memories = client.get_all(agent_id=\"ai-tutor\", page=1, page_size=50)\n```\n\n----------------------------------------\n\nTITLE: Validating and Loading Struct Vector in C\nDESCRIPTION: Implementation of vector validation and loading functions. The validate function checks vector consistency while load creates a new vector from a file or existing data source, ensuring proper initialization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/retrieval-methods.mdx#2025-04-22_snippet_1\n\nLANGUAGE: C\nCODE:\n```\nbool vec_validate(const vec_t *vec) {\\n    if (!vec)\\n        return false;\\n    if (!vec->elem && (vec->cap_count || vec->elem_count || vec->elem_size))\\n        return false;\\n    if (vec->elem && (!vec->cap_count || !vec->elem_size))\\n        return false;\\n    if (vec->elem_count > vec->cap_count)\\n        return false;\\n    return true;\\n}\\n\\nvec_t *vec_load(size_t elem_size, size_t cap_count, size_t elem_count, const void *src) {\\n    vec_t *ret = calloc(1, sizeof(vec_t));\\n    if (ret && cap_count && elem_size) {\\n        ret->cap_count = cap_count;\\n        ret->elem_count = elem_count;\\n        ret->elem_size = elem_size;\\n        ret->elem = calloc(cap_count, elem_size);\\n        if (ret->elem && src && elem_count)\\n            memcpy(ret->elem, src, elem_count * elem_size);\\n    }\\n    if (!vec_validate(ret)) {\\n        vec_free(&ret);\\n        return NULL;\\n    }\\n    return ret;\\n}\n```\n\n----------------------------------------\n\nTITLE: Delete Specific User with Mem0\nDESCRIPTION: Deletes a specific user (or agent, app, or run) from the Mem0 system. The examples show how to delete a user by ID using Python, JavaScript, and a cURL request. The cURL request requires an API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\n# Delete specific user\nclient.delete_users(user_id=\"alex\")\n\n# Delete specific agent\n# client.delete_users(agent_id=\"travel-assistant\")\n```\n\n----------------------------------------\n\nTITLE: Interactive Query Loop for Embedchain App\nDESCRIPTION: Implements an interactive query loop that allows users to ask questions about the added data sources. The loop continues until the user enters a quit command, and for each question, it queries the Embedchain app and displays the response.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Validating Environment Variables for ElevenLabs and Mem0 (Python)\nDESCRIPTION: Checks for the presence of required environment variables: AGENT_ID, USER_ID, ELEVENLABS_API_KEY, and MEM0_API_KEY. Exits with a descriptive message via stderr if a required variable is missing. Sets the MEM0_API_KEY in the environment for use by the Mem0 client. This ensures all necessary credentials are configured before launching the rest of the application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    # Required environment variables\n    AGENT_ID = os.environ.get('AGENT_ID')\n    USER_ID = os.environ.get('USER_ID')\n    API_KEY = os.environ.get('ELEVENLABS_API_KEY')\n    MEM0_API_KEY = os.environ.get('MEM0_API_KEY')\n\n    # Validate required environment variables\n    if not AGENT_ID:\n        sys.stderr.write(\"AGENT_ID environment variable must be set\\n\")\n        sys.exit(1)\n\n    if not USER_ID:\n        sys.stderr.write(\"USER_ID environment variable must be set\\n\")\n        sys.exit(1)\n\n    if not API_KEY:\n        sys.stderr.write(\"ELEVENLABS_API_KEY not set, assuming the agent is public\\n\")\n\n    if not MEM0_API_KEY:\n        sys.stderr.write(\"MEM0_API_KEY environment variable must be set\\n\")\n        sys.exit(1)\n\n    # Set up Mem0 API key in the environment\n    os.environ['MEM0_API_KEY'] = MEM0_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Deleting Chat Session History in Embedchain (Python)\nDESCRIPTION: This snippet illustrates how to clear the chat history for a session within an `embedchain` application. It involves initializing the `App`, adding a document, engaging in a chat interaction using `app.chat()`, and finally calling `app.delete_session_chat_history()` to remove all messages from the default session's history. The method also accepts an optional `session_id` parameter to target a specific session's history for deletion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/delete.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\napp.chat(\"What is the net worth of Elon Musk?\")\n\napp.delete_session_chat_history()\n```\n\n----------------------------------------\n\nTITLE: Basic Text Generation with Memory Context\nDESCRIPTION: Demonstrates generating text using the Mem0 provider with GPT-4 Turbo, leveraging user memory context. This enables personalized responses based on past interactions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0(\"gpt-4-turbo\", {\n    user_id: \"borat\",\n  }),\n  prompt: \"Suggest me a good car to buy!\",\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Mem0 Webhook in JavaScript\nDESCRIPTION: This JavaScript snippet shows how to create a Mem0 webhook for a specific project using the `mem0ai` library. It initializes the `MemoryClient` with an API key and calls the asynchronous `createWebhook` method, passing the webhook URL, name, project ID, and event types (e.g., 'memory_add'). It assumes the `mem0ai` package is installed and a valid API key is provided.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n```javascript JavaScript\nconst { MemoryClient } = require('mem0ai');\nconst client = new MemoryClient({ apiKey: 'your-api-key'});\n\n// Create webhook in a specific project\nconst webhook = await client.createWebhook({\n    url: \"https://your-app.com/webhook\",\n    name: \"Memory Logger\",\n    projectId: \"proj_123\",\n    eventTypes: [\"memory_add\"]\n});\nconsole.log(webhook);\n```\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying Local JSON Data in Embedchain (Python)\nDESCRIPTION: This Python script demonstrates initializing an Embedchain App, adding data from a local JSON file named 'temp.json' using the `app.add()` method, and then querying the indexed data with a specific question. It requires the `embedchain` library to be installed and a file named `temp.json` to exist in the same directory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/json.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python python\nfrom embedchain import App\n\napp = App()\n\n# Add json file\napp.add(\"temp.json\")\n\napp.query(\"What is the net worth of Elon Musk as of October 2023?\")\n# As of October 2023, Elon Musk's net worth is $255.2 billion.\n```\n```\n\n----------------------------------------\n\nTITLE: Demonstrating add_memories Usage in ElevenLabs Conversation Flow (Python)\nDESCRIPTION: This Python snippet illustrates how the agent stores new information (such as the user's favorite color) using the add_memories function. The code submits the relevant message as a parameter, ensuring the detail is available for future queries. This pattern depends on a working add_memories function and a properly configured memory backend.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# Agent calls add_memories\\nadd_memories({\\\"message\\\": \\\"The user's favorite color is green\\\"})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dependencies and Environment for Voice Agent\nDESCRIPTION: Imports required modules, sets up API keys, defines user identification, and initializes the Mem0 client for memory operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# OpenAI Agents SDK imports\nfrom agents import (\n    Agent,\n    function_tool\n)\nfrom agents.voice import (\n    AudioInput,\n    SingleAgentVoiceWorkflow,\n    VoicePipeline\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n# Mem0 imports\nfrom mem0 import AsyncMemoryClient\n\n# Set up API keys (replace with your actual keys)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\nos.environ[\"MEM0_API_KEY\"] = \"your-mem0-api-key\"\n\n# Define a global user ID for simplicity\nUSER_ID = \"voice_user\"\n\n# Initialize Mem0 client\nmem0_client = AsyncMemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Deleting a Mem0 Webhook in Python\nDESCRIPTION: This Python snippet demonstrates how to delete a specific Mem0 webhook. It uses the `MemoryClient`'s `delete_webhook` method, passing the unique `webhook_id` of the webhook to be removed. The client requires initialization with a valid API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python Python\n# Delete webhook from a specific project\nresponse = client.delete_webhook(webhook_id=\"wh_123\")\nprint(response)\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client (Node.js, JavaScript)\nDESCRIPTION: Demonstrates how to install and initialize the Mem0 Node.js client, optionally with custom parameters for advanced usage such as enabling graph memory. Requires an OpenAI API key for cloud usage, or operates locally otherwise. Key parameters include configuration options such as enableGraph for enhanced memory graphing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Memory } from 'mem0ai';\nconst m = new Memory();\n// Advanced usage:\nconst mGraph = new Memory({ enableGraph: true });\n```\n\n----------------------------------------\n\nTITLE: Storing Writing Preferences in Mem0 using Python\nDESCRIPTION: This function stores writing preferences in Mem0. It defines a set of writing preferences as a string, creates a message structure, and uses the Mem0 client to add the preferences to storage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/document-writing.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef store_writing_preferences():\n    \"\"\"Store your writing preferences in Mem0.\"\"\"\n    \n    # Define writing preferences\n    preferences = \"\"\"My writing preferences:\n1. Use headings and sub-headings for structure.\n2. Keep paragraphs concise (8-10 sentences max).\n3. Incorporate specific numbers and statistics.\n4. Provide concrete examples.\n5. Use bullet points for clarity.\n6. Avoid jargon and buzzwords.\"\"\"\n\n    # Store preferences in Mem0\n    preference_message = [\n        {\"role\": \"user\", \"content\": \"Here are my writing style preferences\"},\n        {\"role\": \"assistant\", \"content\": preferences}\n    ]\n    \n    response = client.add(preference_message, user_id=USER_ID, run_id=RUN_ID, metadata={\"type\": \"preferences\", \"category\": \"writing_style\"})\n    \n    print(\"Writing preferences stored successfully.\")\n    return response\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Mistral AI in TypeScript\nDESCRIPTION: This snippet shows how to set up the Memory object using Mistral AI in TypeScript. It includes configuring the LLM provider with Mistral API key and model settings, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/mistral_AI.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'mistral',\n    config: {\n      apiKey: process.env.MISTRAL_API_KEY || '',\n      model: 'mistral-tiny-latest', // Or 'mistral-small-latest', 'mistral-medium-latest', etc.\n      temperature: 0.1,\n      maxTokens: 2000,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot Docker Container\nDESCRIPTION: Command to run the Discord bot Docker container with the required environment variables. Requires an OpenAI API key and a Discord bot token to function properly. The container exposes port 8080.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/discord_bot/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name discord-bot -e OPENAI_API_KEY=sk-xxx -e DISCORD_BOT_TOKEN=xxx -p 8080:8080 embedchain/discord-bot:latest\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Conversation Messages\nDESCRIPTION: This snippet creates a list of messages representing a conversation about movie preferences, which will be used to demonstrate memory storage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"How about a thriller movies? They can be quite engaging.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Configuration Options for Mem0 Provider\nDESCRIPTION: Shows the structure for providing additional model-specific configuration options when initializing the Mem0 provider. These configuration options can customize the behavior of the LLM.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst mem0 = createMem0({\n  config: {\n    ...\n    // Additional model-specific configuration options can be added here.\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Running the Private AI Application\nDESCRIPTION: Executes the privateai.py file which starts the Private AI application, allowing users to interact with their data through queries.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/private-ai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython privateai.py\n```\n\n----------------------------------------\n\nTITLE: Importing Mem0 and Configuring OpenAI API Key\nDESCRIPTION: This snippet imports the necessary module from Mem0 and sets up the OpenAI API key as an environment variable.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = (\n    \"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Delete All User Memories with Mem0\nDESCRIPTION: Deletes all memories associated with a specific user from the Mem0 system. This is achieved using Python, JavaScript, and a cURL request.  The cURL request requires an API key for authorization and specifies the user ID as a query parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nclient.delete_all(user_id=\"alex\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Ollama LLM with Embedchain\nDESCRIPTION: Example showing how to configure Ollama as the LLM provider with Embedchain. This requires setting up Ollama locally, configuring the OLLAMA_HOST environment variable, and using a YAML config file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\nfrom embedchain import App\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: ollama\n  config:\n    model: 'llama2'\n    temperature: 0.5\n    top_p: 1\n    stream: true\n    base_url: 'http://localhost:11434'\nembedder:\n  provider: ollama\n  config:\n    model: znbang/bge:small-en-v1.5-q8_0\n    base_url: http://localhost:11434\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with OpenSearch Config\nDESCRIPTION: Creates an Embedchain App instance using configuration from a YAML file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/opensearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load opensearch configuration from yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Assistant with Embedchain\nDESCRIPTION: Creates a new OpenAI Assistant instance with specific instructions. Requires the OPENAI_API_KEY environment variable to be set.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/openai-assistant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.store.assistants import OpenAIAssistant\n\nassistant = OpenAIAssistant(\n    name=\"OpenAI DevDay Assistant\",\n    instructions=\"You are an organizer of OpenAI DevDay\",\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating Mem0 Client - Python\nDESCRIPTION: Initializes a Mem0 MemoryClient instance in Python. Relies on the mem0ai package being installed and requires a valid MEM0_API_KEY set via environment variables. Designed for the managed Mem0 platform and used for executing further API operations such as adding or searching memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\\nfrom mem0 import MemoryClient\\n\\nos.environ[\\\"MEM0_API_KEY\\\"] = \\\"your-api-key\\\"\\n\\nclient = MemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Querying Data with cURL\nDESCRIPTION: POST request to query an Embedchain application for information about Elon Musk using cURL. The query is sent as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/my-app/query \\\n  -d \"query=Who is Elon Musk?\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Memory Export Job in Python\nDESCRIPTION: This Python code demonstrates how to submit a memory export job using the Mem0 API client. It includes examples of basic export requests and exports with custom instructions and filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/memory-export.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Basic export request\nresponse = client.create_memory_export(\n    schema=json_schema,\n    user_id=\"alice\"\n)\n\n# Export with custom instructions\nexport_instructions = \"\"\"\n1. Create a comprehensive profile with detailed information in each category\n2. Only mark fields as \"None\" when absolutely no relevant information exists\n3. Base all information directly on the user's memories\n4. When contradictions exist, prioritize the most recent information\n5. Clearly distinguish between factual statements and inferences\n\"\"\"\n\n# For create operation, using only user_id filter as requested\nfilters = {\n    \"AND\": [\n        {\"user_id\": \"alex\"}\n    ]\n}\n\nresponse = client.create_memory_export(\n    schema=json_schema,\n    filters=filters,\n    export_instructions=export_instructions  # Optional\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Get All Memories Function for Mem0 Integration\nDESCRIPTION: This Python function retrieves all memories from Mem0 for a given user context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def get_all_memory(\n    context: RunContextWrapper[Mem0Context],\n) -> str:\n    \"\"\"Retrieve all memories from Mem0\"\"\"\n    user_id = context.context.user_id or \"default_user\"\n    memories = await client.get_all(user_id=user_id, output_format=\"v1.1\")\n    results = '\\n'.join([result[\"memory\"] for result in memories[\"results\"]])\n    return str(results)\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to OpenAI Assistant Thread\nDESCRIPTION: Shows how to add various data sources to the assistant, including local files, YouTube videos, and web pages. Embedchain handles the data processing for the OpenAI Assistant.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/openai-assistant.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nassistant.add(\"/path/to/file.pdf\")\nassistant.add(\"https://www.youtube.com/watch?v=U9mJuUkhUzk\")\nassistant.add(\"https://openai.com/blog/new-models-and-developer-products-announced-at-devday\")\n```\n\n----------------------------------------\n\nTITLE: Querying the Discord Bot\nDESCRIPTION: Slash command format for querying information from the Discord bot. This command allows users to ask questions and receive responses based on the data sources that have been added to the bot.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n/ec query <question>\n```\n\n----------------------------------------\n\nTITLE: Adding GitHub Data to Embedchain App and Querying in Python\nDESCRIPTION: Illustrates the process of setting up an `embedchain` Pipeline (aliased as `App`), configuring the necessary environment variable (`OPENAI_API_KEY`), and adding GitHub data using the previously initialized `GithubLoader`. The `app.add` method takes a specific GitHub query string (e.g., 'repo:owner/repo type:repo') and the loader instance. Finally, it shows how to query the indexed data using `app.query`. Note the specific query format requirements mentioned in the surrounding text (mandatory `repo:` and `type:` qualifiers).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/github.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n```Python\nimport os\nfrom embedchain.pipeline import Pipeline as App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\"\n\napp = App()\n\napp.add(\"repo:embedchain/embedchain type:repo\", data_type=\"github\", loader=loader)\n\nresponse = app.query(\"What is Embedchain?\")\n# Answer: Embedchain is a Data Platform for Large Language Models (LLMs). It allows users to seamlessly load, index, retrieve, and sync unstructured data in order to build dynamic, LLM-powered applications. There is also a JavaScript implementation called embedchain-js available on GitHub.\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies with Version Constraints\nDESCRIPTION: Lists required Python packages with specific versions to ensure consistent installations. Includes discord.py, embedchain, and python-dotenv for environment variable management.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/discord_bot/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\ndiscord==2.3.1\nembedchain==0.0.58\npython-dotenv==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Schema for Searching Memory in Python\nDESCRIPTION: Defines the Pydantic model `SearchMemoryInput` to structure the input for the search memory tool. It includes fields for the search query, filters (as a dictionary), and memory version, along with example usage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SearchMemoryInput(BaseModel):\n    query: str = Field(description=\"The search query string\")\n    filters: Dict[str, Any] = Field(description=\"Filters to apply to the search\")\n    version: str = Field(description=\"Version of the memory to search\")\n\n    class Config:\n        json_schema_extra = {\n            \"examples\": [{\n                \"query\": \"tell me about my allergies?\",\n                \"filters\": {\n                    \"AND\": [\n                        {\"user_id\": \"alex\"},\n                        {\"created_at\": {\"gte\": \"2024-01-01\", \"lte\": \"2024-12-31\"}}\n                    ]\n                },\n                \"version\": \"v2\"\n            }]\n        }\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncMemory with Optional Custom Configuration - Python\nDESCRIPTION: This snippet demonstrates how to initialize AsyncMemory in Python, either with the default settings or by supplying a custom MemoryConfig. Dependencies include the mem0 and asyncio libraries, as well as optional configuration imports from mem0.configs.base. Key parameters are the optional configuration object. Outputs an AsyncMemory instance for asynchronous memory operations. Ensure both mem0 and required configuration modules are installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\\nfrom mem0 import AsyncMemory\\n\\n# Initialize with default configuration\\nmemory = AsyncMemory()\\n\\n# Or initialize with custom configuration\\nfrom mem0.configs.base import MemoryConfig\\ncustom_config = MemoryConfig(\\n    # Your custom configuration here\\n)\\nmemory = AsyncMemory(config=custom_config)\n```\n\n----------------------------------------\n\nTITLE: Making a POST Query Request to the Mem0 API in Bash\nDESCRIPTION: Example of how to make a POST request to query the mem0 API for information about Elon Musk. The request uses curl to send a POST request to the endpoint with the app_id parameter in the URL and the query in the request body.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/query.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/{app_id}/query \\\n  -d \"query=who is Elon Musk?\"\n```\n\n----------------------------------------\n\nTITLE: Initializing an AI Assistant with Embedchain\nDESCRIPTION: Shows how to create a new AI Assistant by specifying a name and data sources. The assistant can be initialized with YouTube videos or other supported content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/opensource-assistant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.store.assistants import AIAssistant\n\nassistant = AIAssistant(\n    name=\"My Assistant\",\n    data_sources=[{\"source\": \"https://www.youtube.com/watch?v=U9mJuUkhUzk\"}])\n```\n\n----------------------------------------\n\nTITLE: Inserting Mem0MemoryService into Pipecat Pipeline (Python)\nDESCRIPTION: Illustrates the correct placement of Mem0MemoryService within a Pipecat processing pipeline. By inserting it after user context aggregation and before the LLM service, this ensures that retrieved memories can influence response generation. Example assumes all pipeline objects have been appropriately initialized elsewhere.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline([\n    transport.input(),\n    stt,                # Speech-to-text for audio input\n    user_context,       # User context aggregator\n    memory,             # Mem0 Memory service enhances context here\n    llm,                # LLM for response generation\n    tts,                # Optional: Text-to-speech\n    transport.output(),\n    assistant_context   # Assistant context aggregator\n])\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with Defaults in Python\nDESCRIPTION: This snippet shows the simplest way to create an Embedchain `App` instance using its default configuration. It imports the `App` class from the `embedchain` library and instantiates it without any arguments, relying on the library's predefined defaults for LLM, vector database, and embedding model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\n```\n\n----------------------------------------\n\nTITLE: Querying without Citations Using Embedchain in Python\nDESCRIPTION: This snippet shows how to use the Embedchain App to query a source and receive a simple text answer without citations. It demonstrates initializing the app, adding a web resource, and calling 'query' with a basic question. Dependencies include the 'embedchain' package. The main parameter is the query string; the return value is a plain string containing the answer. This is a straightforward use case and does not handle citations metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/query.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain import App\\n\\n# Initialize app\\napp = App()\\n\\n# Add data source\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\n# Get relevant answer for your query\\nanswer = app.query(\\\"What is the net worth of Elon?\\\")\\nprint(answer)\\n# Answer: The net worth of Elon Musk is $221.9 billion.\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to an AI Assistant\nDESCRIPTION: Shows how to add various data sources to the assistant, including local files, YouTube videos, and web pages. Embedchain supports multiple file formats and online sources.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/opensource-assistant.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nassistant.add(\"/path/to/file.pdf\")\nassistant.add(\"https://www.youtube.com/watch?v=U9mJuUkhUzk\")\nassistant.add(\"https://openai.com/blog/new-models-and-developer-products-announced-at-devday\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 (Advanced, Production Ready) - TypeScript\nDESCRIPTION: Shows how to initialize the Memory class with custom versioning, embedder, vector store, LLM, and persistent storage configurations for production. Requires valid OpenAI and database credentials, and 'mem0ai' SDK installation. Parameters like apiKey, userId, and file paths are configurable. Designed for production deployments requiring fine-tuned memory and LLM integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst memory = new Memory({\n    version: 'v1.1',\n    embedder: {\n      provider: 'openai',\n      config: {\n        apiKey: process.env.OPENAI_API_KEY || '',\n        model: 'text-embedding-3-small',\n      },\n    },\n    vectorStore: {\n      provider: 'memory',\n      config: {\n        collectionName: 'memories',\n        dimension: 1536,\n      },\n    },\n    llm: {\n      provider: 'openai',\n      config: {\n        apiKey: process.env.OPENAI_API_KEY || '',\n        model: 'gpt-4-turbo-preview',\n      },\n    },\n    historyDbPath: 'memory.db',\n  });\n```\n\n----------------------------------------\n\nTITLE: Documenting API Server Setup with embedchain in Markdown\nDESCRIPTION: This markdown snippet provides a brief introduction to the API Server docker template using the embedchain package. It includes a link to the official documentation for more detailed information on usage and implementation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/api_server/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# API Server\n\nThis is a docker template to create your own API Server using the embedchain package. To know more about the API Server and how to use it, go [here](https://docs.embedchain.ai/examples/api_server).\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Update Memory Prompt in Python for Mem0 System\nDESCRIPTION: This snippet defines a custom update memory prompt for the Mem0 system. It includes detailed instructions for comparing newly retrieved facts with existing memory and determining the appropriate action (Add, Update, Delete, or No Change). The prompt provides examples and guidelines for each action type.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-update-memory-prompt.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nUPDATE_MEMORY_PROMPT = \"\"\"You are a smart memory manager which controls the memory of a system.\nYou can perform four operations: (1) add into the memory, (2) update the memory, (3) delete from the memory, and (4) no change.\n\nBased on the above four operations, the memory will change.\n\nCompare newly retrieved facts with the existing memory. For each new fact, decide whether to:\n- ADD: Add it to the memory as a new element\n- UPDATE: Update an existing memory element\n- DELETE: Delete an existing memory element\n- NONE: Make no change (if the fact is already present or irrelevant)\n\nThere are specific guidelines to select which operation to perform:\n\n1. **Add**: If the retrieved facts contain new information not present in the memory, then you have to add it by generating a new ID in the id field.\n- **Example**:\n    - Old Memory:\n        [\n            {\n                \"id\" : \"0\",\n                \"text\" : \"User is a software engineer\"\n            }\n        ]\n    - Retrieved facts: [\"Name is John\"]\n    - New Memory:\n        {\n            \"memory\" : [\n                {\n                    \"id\" : \"0\",\n                    \"text\" : \"User is a software engineer\",\n                    \"event\" : \"NONE\"\n                },\n                {\n                    \"id\" : \"1\",\n                    \"text\" : \"Name is John\",\n                    \"event\" : \"ADD\"\n                }\n            ]\n\n        }\n\n2. **Update**: If the retrieved facts contain information that is already present in the memory but the information is totally different, then you have to update it. \nIf the retrieved fact contains information that conveys the same thing as the elements present in the memory, then you have to keep the fact which has the most information. \nExample (a) -- if the memory contains \"User likes to play cricket\" and the retrieved fact is \"Loves to play cricket with friends\", then update the memory with the retrieved facts.\nExample (b) -- if the memory contains \"Likes cheese pizza\" and the retrieved fact is \"Loves cheese pizza\", then you do not need to update it because they convey the same information.\nIf the direction is to update the memory, then you have to update it.\nPlease keep in mind while updating you have to keep the same ID.\nPlease note to return the IDs in the output from the input IDs only and do not generate any new ID.\n- **Example**:\n    - Old Memory:\n        [\n            {\n                \"id\" : \"0\",\n                \"text\" : \"I really like cheese pizza\"\n            },\n            {\n                \"id\" : \"1\",\n                \"text\" : \"User is a software engineer\"\n            },\n            {\n                \"id\" : \"2\",\n                \"text\" : \"User likes to play cricket\"\n            }\n        ]\n    - Retrieved facts: [\"Loves chicken pizza\", \"Loves to play cricket with friends\"]\n    - New Memory:\n        {\n        \"memory\" : [\n                {\n                    \"id\" : \"0\",\n                    \"text\" : \"Loves cheese and chicken pizza\",\n                    \"event\" : \"UPDATE\",\n                    \"old_memory\" : \"I really like cheese pizza\"\n                },\n                {\n                    \"id\" : \"1\",\n                    \"text\" : \"User is a software engineer\",\n                    \"event\" : \"NONE\"\n                },\n                {\n                    \"id\" : \"2\",\n                    \"text\" : \"Loves to play cricket with friends\",\n                    \"event\" : \"UPDATE\",\n                    \"old_memory\" : \"User likes to play cricket\"\n                }\n            ]\n        }\n\n\n3. **Delete**: If the retrieved facts contain information that contradicts the information present in the memory, then you have to delete it. Or if the direction is to delete the memory, then you have to delete it.\nPlease note to return the IDs in the output from the input IDs only and do not generate any new ID.\n- **Example**:\n    - Old Memory:\n        [\n            {\n                \"id\" : \"0\",\n                \"text\" : \"Name is John\"\n            },\n            {\n                \"id\" : \"1\",\n                \"text\" : \"Loves cheese pizza\"\n            }\n        ]\n    - Retrieved facts: [\"Dislikes cheese pizza\"]\n    - New Memory:\n        {\n        \"memory\" : [\n                {\n                    \"id\" : \"0\",\n                    \"text\" : \"Name is John\",\n                    \"event\" : \"NONE\"\n                },\n                {\n                    \"id\" : \"1\",\n                    \"text\" : \"Loves cheese pizza\",\n                    \"event\" : \"DELETE\"\n                }\n        ]\n        }\n\n4. **No Change**: If the retrieved facts contain information that is already present in the memory, then you do not need to make any changes.\n- **Example**:\n    - Old Memory:\n        [\n            {\n                \"id\" : \"0\",\n                \"text\" : \"Name is John\"\n            },\n            {\n                \"id\" : \"1\",\n                \"text\" : \"Loves cheese pizza\"\n            }\n        ]\n    - Retrieved facts: [\"Name is John\"]\n    - New Memory:\n        {\n        \"memory\" : [\n                {\n                    \"id\" : \"0\",\n                    \"text\" : \"Name is John\",\n                    \"event\" : \"NONE\"\n                },\n                {\n                    \"id\" : \"1\",\n                    \"text\" : \"Loves cheese pizza\",\n                    \"event\" : \"NONE\"\n                }\n            ]\n        }\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Upstash Vector and External Embeddings in Python\nDESCRIPTION: This example shows how to set up a Memory object using Upstash Vector as the vector store and OpenAI as an external embedding provider. It requires environment variables for both Upstash Vector and OpenAI credentials, and specifies the OpenAI embedding model in the configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/upstash-vector.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\nos.environ[\"UPSTASH_VECTOR_REST_URL\"] = \"...\"\nos.environ[\"UPSTASH_VECTOR_REST_TOKEN\"] = \"...\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"upstash_vector\",\n    },\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"text-embedding-3-large\"\n        },\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Likes to play cricket on weekends\", user_id=\"alice\", metadata={\"category\": \"hobbies\"})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AutoGen and Mem0 Integration using Bash\nDESCRIPTION: This snippet uses pip, the Python package installer, to install the necessary libraries: `pyautogen` for creating AI agents, `mem0ai` for memory management, and `openai` for interacting with the OpenAI API. These libraries are required to run the subsequent Python code.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/autogen.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pyautogen mem0ai openai\n```\n\n----------------------------------------\n\nTITLE: Encoding and Sending Local Images via Base64 in Mem0 (Python)\nDESCRIPTION: Shows how to load a local image file, encode it to Base64, and construct a message for Mem0 ingestion. Requires the base64 standard library and access to the local file system. The resulting message dictionary contains a 'data:' URL suitable for memory addition.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Path to the image file\nimage_path = \"path/to/your/image.jpg\"\n\n# Encode the image in Base64\nwith open(image_path, \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n# Create the message dictionary with the Base64-encoded image\nimage_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"image_url\",\n        \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Memory History (Node.js, JavaScript)\nDESCRIPTION: Retrieves the change history for a specific memory by providing memory_id, typically after newly adding a memory. This enables tracking modifications over time. Requires the Memory client and the memory_id returned from previous operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Get memory history\nconst history = await m.getHistory({ memory_id: \"mem456\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Youtube Videos to an Embedchain App\nDESCRIPTION: Python code that demonstrates how to add a Youtube video to an Embedchain app. It imports the App class, creates an instance, and adds a Youtube video URL specifying 'youtube_video' as the data type.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/youtube-video.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add('a_valid_youtube_url_here', data_type='youtube_video')\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Mem0\nDESCRIPTION: Instructions for disabling telemetry in Mem0 using environment variables or programmatically in Python code.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/faqs.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMEM0_TELEMETRY=False\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"MEM0_TELEMETRY\"] = \"False\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies in requirements.txt\nDESCRIPTION: Lists the specific Python package dependencies required for the mem0ai/mem0 project. Includes FastAPI 0.104.0 for API development, Uvicorn 0.23.2 for serving the application, embedchain for handling embeddings, and BeautifulSoup4 for HTML parsing and scraping.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/nextjs_discord/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain\nbeautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Defining Structured Response Schemas with Zod for OpenAI\nDESCRIPTION: This snippet demonstrates how to use Zod to define structured response schemas for OpenAI. It creates a schema for car recommendations and a function tool based on this schema for use in OpenAI requests.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst CarSchema = z.object({\n  car_name: z.string(),\n  car_price: z.string(),\n  car_url: z.string(),\n  car_image: z.string(),\n  car_description: z.string(),\n});\n\nconst Cars = z.object({\n  cars: z.array(CarSchema),\n});\n\nconst carRecommendationTool = zodResponsesFunction({ \n    name: \"carRecommendations\", \n    parameters: Cars \n});\n\nconst response = await openAIClient.responses.create({\n    model: \"gpt-4o\",\n    tools: [{ type: \"web_search_preview\" }, carRecommendationTool],\n    input: `${getMemoryString(relevantMemories)}\\n${userInput}`,\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Embedding Model Configuration in Python\nDESCRIPTION: This snippet demonstrates how to load an embedding model configuration from a YAML file in a Python script using the Embedchain App class.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying Audio Data with Deepgram in Python\nDESCRIPTION: This Python snippet demonstrates setting up an Embedchain App to use an audio file as a data source, leveraging Deepgram for audio transcription. It sets the required DEEPGRAM_API_KEY in the environment and shows how to add an audio file ('introduction.wav') by specifying 'data_type' as 'audio'. With the transcribed text automatically added as context, the app is queried for specific information contained within the audio. Dependencies include the Embedchain Python library and a valid Deepgram API key. The main inputs are the API key and path to the audio file; the output is a string answer to the query based on the transcribed content. Limitations include dependency on accurate audio transcription and requirement for audio files accessible from the given path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/audio.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"DEEPGRAM_API_KEY\"] = \"153xxx\"\n\napp = App()\napp.add(\"introduction.wav\", data_type=\"audio\")\nresponse = app.query(\"What is my name and how old am I?\")\nprint(response)\n# Answer: Your name is Dave and you are 21 years old.\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Specification for Creating Project\nDESCRIPTION: YAML definition for the POST endpoint to create a new project within an organization. It specifies the path, request body schema, and expected responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/project/create-project.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npost:\n  summary: Create a new project\n  operationId: createProject\n  parameters:\n    - name: org_id\n      in: path\n      required: true\n      schema:\n        type: string\n  requestBody:\n    required: true\n    content:\n      application/json:\n        schema:\n          type: object\n          properties:\n            name:\n              type: string\n            description:\n              type: string\n  responses:\n    '201':\n      description: Project created successfully\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Project'\n    '400':\n      description: Bad request\n    '401':\n      description: Unauthorized\n    '403':\n      description: Forbidden\n    '404':\n      description: Organization not found\n    '409':\n      description: Project with the same name already exists\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with Azure OpenAI Configuration\nDESCRIPTION: Creates an Embedchain App instance using the previously defined configuration file. This step initializes the application with the specified Azure OpenAI models and settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config_path=\"azure_openai.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Adding Memories via Mem0 REST API - cURL\nDESCRIPTION: Demonstrates how to add memory data to the Mem0 managed platform via a REST POST request using cURL. Requires supplying an API key for authentication, appropriate headers, and a JSON payload containing messages (conversation turns) and user_id. Useful for shell scripting and integrating with systems outside standard SDKs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \\\"https://api.mem0.ai/v1/memories/\\\" \\\\n     -H \\\"Authorization: Token your-api-key\\\" \\\\n     -H \\\"Content-Type: application/json\\\" \\\\n     -d '{\\n         \\\"messages\\\": [\\n             {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"I live in San Francisco. Thinking of making a sandwich. What do you recommend?\\\"},\\n             {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"How about adding some cheese for extra flavor?\\\"},\\n             {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Actually, I don't like cheese.\\\"},\\n             {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"I'll remember that you don't like cheese for future recommendations.\\\"}\\n         ],\\n         \\\"user_id\\\": \\\"alex\\\"\\n     }'\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Evaluation Metric in Embedchain\nDESCRIPTION: This code snippet demonstrates how to create a custom evaluation metric by extending the BaseMetric class. The custom metric requires implementing an __init__ method that specifies the metric name and an evaluate method that calculates the score based on a dataset of EvalData items.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom embedchain.config.base_config import BaseConfig\nfrom embedchain.evaluation.metrics import BaseMetric\nfrom embedchain.utils.eval import EvalData\n\nclass MyCustomMetric(BaseMetric):\n    def __init__(self, config: Optional[BaseConfig] = None):\n        super().__init__(name=\"my_custom_metric\")\n\n    def evaluate(self, dataset: list[EvalData]):\n        score = 0.0\n        # write your evaluation logic here\n        return score\n```\n\n----------------------------------------\n\nTITLE: Deploying to Embedchain Platform with cURL\nDESCRIPTION: POST request to deploy an Embedchain application to the Embedchain Platform using cURL. This requires an API key for authentication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/my-app/deploy \\\n  -d \"api_key=ec-xxxx\"\n```\n\n----------------------------------------\n\nTITLE: Examining Mem0 Webhook Payload Structure\nDESCRIPTION: This JSON snippet illustrates the structure of the payload sent via HTTP POST to a configured webhook URL when a subscribed memory event occurs in Mem0. The payload contains an `event_details` object with the event `id`, the event type (`event`: e.g., 'ADD', 'UPDATE', 'DELETE'), and the relevant `data` (e.g., the memory content).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n    \"event_details\": {\n        \"id\": \"a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5\",\n            \"data\": {\n            \"memory\": \"Name is Alex\"\n            },\n        \"event\": \"ADD\"\n    }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Inference Endpoint\nDESCRIPTION: This code snippet demonstrates how to use a Hugging Face Inference Endpoint as the LLM provider for Embedchain. It includes specifying the endpoint URL and model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nconfig = {\n  \"app\": {\"config\": {\"id\": \"my-app\"}},\n  \"llm\": {\n      \"provider\": \"huggingface\",\n      \"config\": {\n        \"endpoint\": \"https://api-inference.huggingface.co/models/gpt2\",\n        \"model_params\": {\"temprature\": 0.1, \"max_new_tokens\": 100}\n      },\n  },\n}\napp = App.from_config(config=config)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Specific Document in Embedchain (Python)\nDESCRIPTION: This snippet demonstrates how to remove a specific document from an `embedchain` application using its unique ID. It first initializes the `App`, adds two documents (retrieving their IDs), and then calls the `app.delete()` method with the ID of the document to be removed. A note indicates that if the document ID is unknown, it can be retrieved from the `hash` key within the document's metadata using `app.db.get()`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/delete.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\nforbes_doc_id = app.add(\"https://www.forbes.com/profile/elon-musk\")\nwiki_doc_id = app.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\n\napp.delete(forbes_doc_id)   # deletes the forbes document\n```\n\n----------------------------------------\n\nTITLE: Delete Memory Using Add with Mem0\nDESCRIPTION: Demonstrates deleting memory using the `add()` method in Mem0 by passing a natural language command. Examples are provided in Python, JavaScript, and cURL.  The cURL example includes specifying the message content and user ID in the request body.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\nclient.add(\"Delete all of my food preferences\", user_id=\"alex\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Web Page to an Embedchain App in Python\nDESCRIPTION: This code snippet demonstrates how to initialize an Embedchain App and add a web page to it. The add() method is used with 'web_page' as the data_type parameter to specify that the URL refers to a web page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/web-page.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add('a_valid_web_page_url', data_type='web_page')\n```\n\n----------------------------------------\n\nTITLE: Defining Mastra Tools for Mem0 Memory Management in TypeScript\nDESCRIPTION: Defines two tools, `mem0RememberTool` and `mem0MemorizeTool`, using `@mastra/core`'s `createTool` function. `mem0RememberTool` searches for memories based on a question using `mem0.searchMemory`, while `mem0MemorizeTool` saves a statement to memory using `mem0.createMemory` asynchronously. Both tools use `zod` for input validation and leverage the initialized `mem0` integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-mastra.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n tools/index.ts\nimport { createTool } from \"@mastra/core\";\nimport { z } from \"zod\";\nimport { mem0 } from \"../integrations\";\n\nexport const mem0RememberTool = createTool({\n  id: \"Mem0-remember\",\n  description:\n    \"Remember your agent memories that you've previously saved using the Mem0-memorize tool.\",\n  inputSchema: z.object({\n    question: z\n      .string()\n      .describe(\"Question used to look up the answer in saved memories.\"),\n  }),\n  outputSchema: z.object({\n    answer: z.string().describe(\"Remembered answer\"),\n  }),\n  execute: async ({ context }) => {\n    console.log(`Searching memory \"${context.question}\"`);\n    const memory = await mem0.searchMemory(context.question);\n    console.log(`\\nFound memory \"${memory}\"\\n`);\n\n    return {\n      answer: memory,\n    };\n  },\n});\n\nexport const mem0MemorizeTool = createTool({\n  id: \"Mem0-memorize\",\n  description:\n    \"Save information to mem0 so you can remember it later using the Mem0-remember tool.\",\n  inputSchema: z.object({\n    statement: z.string().describe(\"A statement to save into memory\"),\n  }),\n  execute: async ({ context }) => {\n    console.log(`\\nCreating memory \"${context.statement}\"\\n`);\n    // to reduce latency memories can be saved async without blocking tool execution\n    void mem0.createMemory(context.statement).then(() => {\n      console.log(`\\nMemory \"${context.statement}\" saved.\\n`);\n    });\n    return { success: true };\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Deploying the RAG Application to Gradio.app\nDESCRIPTION: Deploys the application to HuggingFace Spaces using Embedchain's deployment command which runs Gradio's deployment process.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/gradio_app.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain YouTube Dependencies\nDESCRIPTION: Command to install the necessary packages for working with YouTube data in Embedchain using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/youtube-channel.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"embedchain[youtube]\"\n```\n\n----------------------------------------\n\nTITLE: Adding an Image via URL to Mem0 using Python\nDESCRIPTION: This Python snippet shows how to add an image to Mem0 by providing its direct URL. It defines the URL, creates a message dictionary with the `type` as `image_url` and the URL nested within `image_url`, and then uses the `client.add` method to ingest this single image message for a specific user. Assumes `client` is an initialized `MemoryClient` instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the image URL\nimage_url = \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n\n# Create the message dictionary with the image URL\nimage_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"image_url\",\n        \"image_url\": {\n            \"url\": image_url\n        }\n    }\n}\nclient.add([image_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Installing the Mem0 Python Library using Pip\nDESCRIPTION: Installs the `mem0ai` package using the Python package installer `pip`. This command is executed in a shell or terminal and is the prerequisite for using the Mem0 OSS Python client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncMemoryClient in Python\nDESCRIPTION: This snippet demonstrates how to initialize the AsyncMemoryClient in Python. It requires setting the MEM0_API_KEY environment variable.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import AsyncMemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = AsyncMemoryClient()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Mem0 Webhooks in Python\nDESCRIPTION: This Python snippet demonstrates retrieving all webhooks associated with a specific Mem0 project using the `MemoryClient`. It calls the `get_webhooks` method, passing the `project_id` as an argument. The client must be initialized with a valid API key. The output is a list of webhook objects.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python Python\n# Get webhooks for a specific project\nwebhooks = client.get_webhooks(project_id=\"proj_123\")\nprint(webhooks)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing AI Companion Dependencies using Pip (Bash)\nDESCRIPTION: Installs the necessary Python libraries, `openai` for interacting with the OpenAI API and `mem0ai` for managing memory, using the pip package manager. This is a prerequisite for running the AI companion code.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Displaying Bot Response in Markdown\nDESCRIPTION: Formats and displays the bot's response using IPython's Markdown display functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-docs-site-example.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Markdown\n# Create a Markdown object and display it\nmarkdown_answer = Markdown(answer)\ndisplay(markdown_answer)\n```\n\n----------------------------------------\n\nTITLE: Simplified Mem0 Client Initialization\nDESCRIPTION: A simplified version of initializing the Mem0 client without explicitly specifying the provider, using OpenAI as the default provider.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst mem0 = createMem0();\n```\n\n----------------------------------------\n\nTITLE: Searching with Graph Memory in Python\nDESCRIPTION: This snippet demonstrates how to search memories with Graph Memory enabled using the Mem0 Python client. It shows how to make a search API call with the necessary parameters for Graph Memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Search with graph memory enabled\nresults = client.search(\n    \"what is my name?\", \n    user_id=\"joseph\", \n    enable_graph=True, \n    output_format=\"v1.1\"\n)\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 with npm\nDESCRIPTION: Installs the Mem0 client library using npm, the Node.js package manager. This allows you to integrate Mem0 into your JavaScript applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using OpenAI Embeddings with mem0 in TypeScript\nDESCRIPTION: This example shows how to use OpenAI embeddings with mem0 in TypeScript, including setting up the configuration object with API key and model, and adding a memory entry.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  embedder: {\n    provider: 'openai',\n    config: {\n      apiKey: 'your-openai-api-key',\n      model: 'text-embedding-3-large',\n    },\n  },\n};\n\nconst memory = new Memory(config);\nawait memory.add(\"I'm visiting Paris\", { userId: \"john\" });\n```\n\n----------------------------------------\n\nTITLE: Setting up Together AI with Embedchain\nDESCRIPTION: Example showing how to configure Together AI as the LLM provider with Embedchain. This requires installing dependencies, setting the TOGETHER_API_KEY environment variable, and using a YAML config file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"TOGETHER_API_KEY\"] = \"xxx\"\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: together\n  config:\n    model: togethercomputer/RedPajama-INCITE-7B-Base\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Mem0 Email Processing (Bash)\nDESCRIPTION: This command installs the necessary Python libraries, `mem0ai` for interacting with the Mem0 service and `openai` (likely for underlying AI features, although not explicitly used in the provided Python code), using the pip package manager. These are prerequisites for running the subsequent Python email processing script.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/email_processing.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai openai\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Environment Variables\nDESCRIPTION: Imports required modules and sets the OpenAI API key as an environment variable, which is necessary for using embedchain with OpenAI models.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/elasticsearch.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks for Mem0 Project\nDESCRIPTION: This command installs pre-commit hooks to ensure code quality checks are run before committing changes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/development.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Delete Specific User with Mem0\nDESCRIPTION: Deletes a specific user (or agent, app, or run) from the Mem0 system. The examples show how to delete a user by ID using Python, JavaScript, and a cURL request. The cURL request requires an API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_63\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.delete_users({ user_id: \"alex\" })\n    .then(result => console.log(result))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Adding Web Pages to Embedchain Knowledge Base\nDESCRIPTION: This snippet demonstrates how to add web pages to the Embedchain knowledge base. It uses the 'add' method of the App instance to embed information from specified URLs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-chromadb-server.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Embed Online Resources\nelon_bot.add(\"web_page\", \"https://en.wikipedia.org/wiki/Elon_Musk\")\nelon_bot.add(\"web_page\", \"https://www.tesla.com/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Date Filters in cURL\nDESCRIPTION: This cURL command searches memories using a date range for the 'created_at' field. It makes a POST request to the /memories/search endpoint with a JSON payload containing the query and filters. The 'gte' and 'lte' operators are used within the 'created_at' filter to define the date range. A valid API token is required in the Authorization header.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_31\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/search/?version=v2\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"query\": \"What do you know about me?\",\n         \"filters\": {\n             \"AND\": [\n                 {\n                     \"created_at\": {\n                         \"gte\": \"2024-07-20\",\n                         \"lte\": \"2024-07-10\"\n                     }\n                 },\n                 {\n                     \"user_id\": \"alex\"\n                 }\n             ]\n         }\n     }'\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Schema for Getting All Memories in Python\nDESCRIPTION: Defines the Pydantic model `GetAllMemoryInput` for the get all memories tool. It includes fields for version, filters, and optional pagination parameters (page, page_size), along with example usage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass GetAllMemoryInput(BaseModel):\n    version: str = Field(description=\"Version of the memory to retrieve\")\n    filters: Dict[str, Any] = Field(description=\"Filters to apply to the retrieval\")\n    page: Optional[int] = Field(description=\"Page number for pagination\", default=1)\n    page_size: Optional[int] = Field(description=\"Number of items per page\", default=50)\n\n    class Config:\n        json_schema_extra = {\n            \"examples\": [{\n                \"version\": \"v2\",\n                \"filters\": {\n                    \"AND\": [\n                        {\"user_id\": \"alex\"},\n                        {\"created_at\": {\"gte\": \"2024-07-01\", \"lte\": \"2024-07-31\"}},\n                        {\"categories\": {\"contains\": \"food_preferences\"}}\n                    ]\n                },\n                \"page\": 1,\n                \"page_size\": 50\n            }]\n        }\n```\n\n----------------------------------------\n\nTITLE: Embedchain App Configuration in JSON\nDESCRIPTION: This JSON configuration file provides an alternative format for configuring an Embedchain application. It mirrors the YAML configuration, defining settings for the LLM, vector database, embedder, chunker, cache, and memory.  The configuration details include model specifications, API keys, and proxy settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/advanced/configuration.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"app\": {\n    \"config\": {\n      \"name\": \"full-stack-app\"\n    }\n  },\n  \"llm\": {\n    \"provider\": \"openai\",\n    \"config\": {\n      \"model\": \"gpt-4o-mini\",\n      \"temperature\": 0.5,\n      \"max_tokens\": 1000,\n      \"top_p\": 1,\n      \"stream\": false,\n      \"prompt\": \"Use the following pieces of context to answer the query at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n$context\\n\\nQuery: $query\\n\\nHelpful Answer:\",\n      \"system_prompt\": \"Act as William Shakespeare. Answer the following questions in the style of William Shakespeare.\",\n      \"api_key\": \"sk-xxx\",\n      \"model_kwargs\": {\"response_format\": {\"type\": \"json_object\"}},\n      \"api_version\": \"2024-02-01\",\n      \"http_client_proxies\": \"http://testproxy.mem0.net:8000\"\n    }\n  },\n  \"vectordb\": {\n    \"provider\": \"chroma\",\n    \"config\": {\n      \"collection_name\": \"full-stack-app\",\n      \"dir\": \"db\",\n      \"allow_reset\": true\n    }\n  },\n  \"embedder\": {\n    \"provider\": \"openai\",\n    \"config\": {\n      \"model\": \"text-embedding-ada-002\",\n      \"api_key\": \"sk-xxx\",\n      \"http_client_proxies\": \"http://testproxy.mem0.net:8000\"\n    }\n  },\n  \"chunker\": {\n    \"chunk_size\": 2000,\n    \"chunk_overlap\": 100,\n    \"length_function\": \"len\",\n    \"min_chunk_size\": 0\n  },\n  \"cache\": {\n    \"similarity_evaluation\": {\n      \"strategy\": \"distance\",\n      \"max_distance\": 1.0\n    },\n    \"config\": {\n      \"similarity_threshold\": 0.8,\n      \"auto_flush\": 50\n    }\n  },\n  \"memory\": {\n    \"top_k\": 10\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 Storage Directory for AWS Lambda\nDESCRIPTION: Code snippet showing how to modify the Mem0 storage directory configuration for AWS Lambda, which only allows writing to the /tmp directory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/faqs.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMEM0_DIR=/tmp/.mem0\n```\n\nLANGUAGE: python\nCODE:\n```\n# Change from\nhome_dir = os.path.expanduser(\"~\")\nmem0_dir = os.environ.get(\"MEM0_DIR\") or os.path.join(home_dir, \".mem0\")\n\n# To\nmem0_dir = os.environ.get(\"MEM0_DIR\", \"/tmp/.mem0\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Chunking for Discourse Data in Python\nDESCRIPTION: This example illustrates how to customize the data chunking process for Discourse content within the `embedchain` framework. It imports `DiscourseChunker` and `ChunkerConfig`, defines specific chunking parameters (chunk size, overlap, length function), creates a `DiscourseChunker` instance with this configuration, and passes it to the `app.add` method alongside the loader. This allows for fine-grained control over how Discourse data is segmented before embedding.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/discourse.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```Python\n\nfrom embedchain.chunkers.discourse import DiscourseChunker\nfrom embedchain.config.add_config import ChunkerConfig\n\ndiscourse_chunker_config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)\ndiscourse_chunker = DiscourseChunker(config=discourse_chunker_config)\n\napp.add(\"openai\", data_type='discourse', loader=dicourse_loader, chunker=discourse_chunker)\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: Adds a URL as a data source to the Embedchain app for processing and embedding.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Creating Long-term Memory with cURL\nDESCRIPTION: This cURL command demonstrates how to create long-term memory for a user via the Mem0 API. It sends a POST request with the messages, user ID, and metadata in the request body.  It requires an API key in the Authorization header.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_7\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [\n             {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n             {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n         ],\n         \"user_id\": \"alex\",\n         \"metadata\": {\n             \"food\": \"vegan\"\n         }\n     }'\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core and Mem0 Packages in Bash\nDESCRIPTION: Installs the required Python packages 'llama-index-core' and 'llama-index-memory-mem0' via pip. Ensure Python and pip are installed on your system before running. This command prepares the environment for using Mem0 memory integration with LlamaIndex.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install llama-index-core llama-index-memory-mem0\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Question-Answering Loop\nDESCRIPTION: Implements an interactive loop that takes user questions as input and queries the Embedchain app. The loop continues until the user enters 'q', 'exit', or 'quit', and prints the model's responses to each query.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/anthropic.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Adding Messages with Automatic Context in JavaScript (v2)\nDESCRIPTION: Demonstrates how to add messages using version 2 in JavaScript. This approach automatically manages conversation context, simplifying the process of adding new messages.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// First interaction\nconst messages1 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"}\n];\nclient.add(messages1, { user_id: \"alex\", version: \"v2\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n\n// Second interaction - only need to send new messages\nconst messages2 = [\n    {\"role\": \"user\", \"content\": \"I like to eat sushi, and yesterday I went to Sunnyvale to eat sushi with my friends.\"},\n    {\"role\": \"assistant\", \"content\": \"Sushi is really a tasty choice. What did you do this weekend?\"}\n];\nclient.add(messages2, { user_id: \"alex\", version: \"v2\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Initializing Memgraph as Graph Memory Store\nDESCRIPTION: This snippet initializes Memgraph as a Graph Memory store using the previously defined configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nm = Memory.from_config(config_dict=config)\n```\n\n----------------------------------------\n\nTITLE: Running Embedchain App in Development\nDESCRIPTION: Command to run the Embedchain app in a development environment for testing purposes before deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nec dev  #To run the app in development environment\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataset for RAG Applications\nDESCRIPTION: Shows how to create a dataset for RAG evaluation by defining data points with questions, contexts, and expected answers using the EvalData class from Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.utils.eval import EvalData\n\ndata = [\n    {\n        \"question\": \"What is the net worth of Elon Musk?\",\n        \"contexts\": [\n            \"Elon Musk PROFILEElon MuskCEO, ...\",\n            \"a Twitter poll on whether the journalists' ...\",\n            \"2016 and run by Jared Birchall.[335]...\",\n        ],\n        \"answer\": \"As of the information provided, Elon Musk's net worth is $241.6 billion.\",\n    },\n    {\n        \"question\": \"which companies does Elon Musk own?\",\n        \"contexts\": [\n            \"of December 2023[update], ...\",\n            \"ThielCofounderView ProfileTeslaHolds ...\",\n            \"Elon Musk PROFILEElon MuskCEO, ...\",\n        ],\n        \"answer\": \"Elon Musk owns several companies, including Tesla, SpaceX, Neuralink, and The Boring Company.\",\n    },\n]\n\ndataset = []\n\nfor d in data:\n    eval_data = EvalData(question=d[\"question\"], contexts=d[\"contexts\"], answer=d[\"answer\"])\n    dataset.append(eval_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Embeddings with LangChain\nDESCRIPTION: Shows how to initialize and configure Ollama embeddings using LangChain integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/langchain.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_ollama import OllamaEmbeddings\n\n# Initialize an Ollama embeddings model\nollama_embeddings = OllamaEmbeddings(\n    model=\"nomic-embed-text\"\n)\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": ollama_embeddings\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using LlamaIndex ReAct Agent Without Memory\nDESCRIPTION: Creates and interacts with a FunctionCallingAgent without memory, demonstrating its inability to remember past preferences shared by the user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nagent = FunctionCallingAgent.from_tools(\n    [call_tool, email_tool, order_food_tool],\n    # memory is not provided\n    llm=llm,\n    verbose=True,\n)\nresponse = agent.chat(\"I am feeling hungry, order me something and send me the bill\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring API and Agent Credentials in Environment (.env, bash)\nDESCRIPTION: Specifies environment variables necessary for authenticating with ElevenLabs and Mem0. These values must be placed in a .env file or set in the process environment to enable the application's access to external APIs for agent identification, user tracking, and secure API communication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create a .env file with these variables\nAGENT_ID=your-agent-id\nUSER_ID=unique-user-identifier\nELEVENLABS_API_KEY=your-elevenlabs-api-key\nMEM0_API_KEY=your-mem0-api-key\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources with cURL\nDESCRIPTION: POST request to add a web page data source to an Embedchain application using cURL. This example adds Elon Musk's Forbes profile page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/my-app/add \\\n  -d \"source=https://www.forbes.com/profile/elon-musk\" \\\n  -d \"data_type=web_page\"\n```\n\n----------------------------------------\n\nTITLE: Using a Custom PostgresChunker with Embedchain in Python\nDESCRIPTION: This snippet illustrates how to customize the data chunking process for PostgreSQL data added to an Embedchain app. It involves importing `PostgresChunker` and `ChunkerConfig`, creating a `ChunkerConfig` instance with desired parameters like `chunk_size` and `chunk_overlap`, initializing a `PostgresChunker` with this configuration, and finally passing this custom `chunker` instance to the `app.add()` method along with the loader and data type.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/postgres.mdx#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\nfrom embedchain.chunkers.postgres import PostgresChunker\nfrom embedchain.config.add_config import ChunkerConfig\n\npostgres_chunker_config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)\npostgres_chunker = PostgresChunker(config=postgres_chunker_config)\n\napp.add(\"SELECT * FROM table_name;\", data_type='postgres', loader=postgres_loader, chunker=postgres_chunker)\n\n```\n\n----------------------------------------\n\nTITLE: Fly.io Authentication Commands\nDESCRIPTION: Commands for signing up or logging into Fly.io platform using the CLI tool.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfly auth signup\n```\n\nLANGUAGE: bash\nCODE:\n```\nfly auth login\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App\nDESCRIPTION: Creates an Embedchain app instance with Together provider and Mixtral model configuration\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/together.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"together\",\n    \"config\": {\n        \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 1000\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Redis and redisvl Libraries (bash)\nDESCRIPTION: This snippet installs the required Python libraries to interact with Redis including the redisvl vector store extension. 'redis' and 'redisvl' must be present in your Python environment to enable memory vector storage features with mem0. Run this command in a terminal or a shell prompt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/redis.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install redis redisvl\n```\n\n----------------------------------------\n\nTITLE: Running Unacademy UPSC AI Locally with Streamlit\nDESCRIPTION: Commands to set up and run the Unacademy UPSC AI application locally using Streamlit. Requires setting the OpenAI API key as an environment variable and installing dependencies from requirements.txt.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/unacademy-ai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=sk-xxx\npip install -r requirements.txt\nstreamlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Requirements\nDESCRIPTION: Lists the exact package versions required for the Mem0 project. It specifies chainlit version 0.7.700 and embedchain version 0.1.31, which are likely used for building chat interfaces and handling embedding operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chainlit/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nchainlit==0.7.700\nembedchain==0.1.31\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Memory System\nDESCRIPTION: Setting up Mem0 memory system with user preferences and coding conventions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nMEM0_MEMORY_CLIENT = Memory()\n\nUSER_ID = \"chicory.ai.user\"\nMEMORY_DATA = \"\"\"\n* Preference for readability: The user prefers code to be explicitly written with clear variable names.\n* Preference for comments: The user prefers comments explaining each step.\n* Naming convention: The user prefers camelCase for variable names.\n* Docstrings: The user prefers functions to have a descriptive docstring.\n\"\"\"\nAGENT_ID = \"chicory.ai\"\n\nMEM0_MEMORY_CLIENT.add(MEMORY_DATA, user_id=USER_ID)\nMEM0_MEMORY_CLIENT.add(MEMORY_DATA, agent_id=AGENT_ID)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with VertexAI\nDESCRIPTION: Creation and configuration of Embedchain app with VertexAI settings for both LLM and embedder, specifying models and parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/vertex_ai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"vertexai\",\n        \"config\": {\n            \"model\": \"chat-bison\",\n            \"temperature\": 0.5,\n            \"max_tokens\": 1000,\n            \"stream\": False\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"vertexai\",\n        \"config\": {\n            \"model\": \"textembedding-gecko\"\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Defining addMemories Tool for ElevenLabs Agent in JSON\nDESCRIPTION: This JSON snippet defines the schema for the addMemories tool, which allows an agent to store important conversational information for future retrieval. The tool is specified with a name, a description, and a parameters object, which requires a single string parameter called 'message'. This schema must be integrated in the ElevenLabs agent configuration for memory storage functionality to be enabled.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_11\n\nLANGUAGE: JSON\nCODE:\n```\n{\\n  \\\"name\\\": \\\"addMemories\\\",\\n  \\\"description\\\": \\\"Stores important information from the conversation to remember for future interactions\\\",\\n  \\\"parameters\\\": {\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n      \\\"message\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"The important information to remember\\\"\\n      }\\n    },\\n    \\\"required\\\": [\\\"message\\\"]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Response after searching with filters in Mem0 (JSON)\nDESCRIPTION: This JSON response illustrates the expected output after successfully searching for memories in Mem0 with category and metadata filters. The response contains a list of results, each representing a relevant memory found. Each memory includes an ID, the memory content, user ID, metadata, categories, and creation/update timestamps.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_25\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"7f165f7e-b411-4afe-b7e5-35789b72c4a5\",\n            \"memory\": \"Name: Alex. Vegetarian. Allergic to nuts.\",\n            \"user_id\": \"alex\",\n            \"metadata\": {\"food\": \"vegan\"},\n            \"categories\": [\"food_preferences\"],\n            \"immutable\": false,\n            \"expiration_date\": null,\n            \"created_at\": \"2024-07-20T01:30:36.275141-07:00\",\n            \"updated_at\": \"2024-07-20T01:30:36.275172-07:00\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Embedchain App\nDESCRIPTION: This code snippet demonstrates how to query the data that has been added to the Embedchain app. It shows a simple query asking about Elon Musk, which will retrieve relevant information from the added data sources.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/ollama.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nanswer = app.query(\"who is elon musk?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain PostgresLoader in Python\nDESCRIPTION: This snippet shows how to import and initialize the `PostgresLoader` from Embedchain. It demonstrates two configuration methods: using a dictionary with individual connection parameters (host, port, dbname, user, password) or providing a single connection URL string. The loader requires either the dictionary or the URL in the `config` parameter. If the `url` key is present in the config dictionary, it takes precedence over other keys.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/postgres.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain.loaders.postgres import PostgresLoader\n\nconfig = {\n    \"host\": \"host_address\",\n    \"port\": \"port_number\",\n    \"dbname\": \"database_name\",\n    \"user\": \"username\",\n    \"password\": \"password\",\n}\n\n\"\"\"\nconfig = {\n    \"url\": \"your_postgres_url\"\n}\n\"\"\"\n\npostgres_loader = PostgresLoader(config=config)\n\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Full Stack App with Docker Compose\nDESCRIPTION: Command to build and start the full stack application using Docker Compose. This sets up all required services and dependencies in containerized environments.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/full_stack/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Required Discord Bot Permissions\nDESCRIPTION: List of recommended permissions for the Discord bot to function properly, including general and text-specific permissions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n(General Permissions)\nRead Message/View Channels\n\n(Text Permissions)\nSend Messages\nCreate Public Thread\nCreate Private Thread\nSend Messages in Thread\nManage Threads\nEmbed Links\nRead Message History\n```\n\n----------------------------------------\n\nTITLE: Querying Embedded Data with Embedchain in Python\nDESCRIPTION: Sets up an interactive loop for querying the embedded data. Users can input questions, and the app will return answers based on the embedded information. The loop continues until the user chooses to exit.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/chromadb.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Installing Pipecat with Mem0 Extras (Bash)\nDESCRIPTION: Installs the Pipecat AI package along with Mem0 support using pip. This command is a prerequisite for all further steps, ensuring all necessary dependencies for memory integration are present before configuring your Python application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pipecat-ai[mem0]\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Query Loop for Embedchain in Python\nDESCRIPTION: Implements an interactive loop for querying the Embedchain app. This snippet allows users to continuously ask questions against the added data sources until they choose to exit the program.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/cohere.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Implementing the Main User Interaction Loop in Python\nDESCRIPTION: Sets up the main execution block that runs when the script is executed directly. It prints a welcome message, assigns a static `mem0_user_id` (in a real application, this would be dynamically generated or retrieved), and enters an infinite loop to continuously prompt the user for input. Inside the loop, it reads the user's input, checks for exit commands ('quit', 'exit', 'bye'), and if it's not an exit command, it calls the `run_conversation` function to process the input and get the agent's response.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    print(\"Welcome to Customer Support! How can I assist you today?\")\n    mem0_user_id = \"customer_123\"  # You can generate or retrieve this based on your user management system\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in ['quit', 'exit', 'bye']:\n            print(\"Customer Support: Thank you for contacting us. Have a great day!\")\n            break\n        run_conversation(user_input, mem0_user_id)\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependencies\nDESCRIPTION: Requirements.txt file content specifying all necessary Python packages and their versions for the RAG application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/railway.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython-dotenv\nembedchain\nfastapi==0.108.0\nuvicorn==0.25.0\nembedchain\nbeautifulsoup4\nsentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App in Python\nDESCRIPTION: Adds a web URL as a data source to the Embedchain app. This step ingests data from the specified URL for later querying and retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/cohere.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Mem0 Webhooks in JavaScript\nDESCRIPTION: This JavaScript snippet shows how to fetch all webhooks for a given Mem0 project using the `mem0ai` library. It calls the asynchronous `getWebhooks` method on an initialized `MemoryClient`, providing the `projectId`. Requires the `mem0ai` package and a configured client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n```javascript JavaScript\n// Get webhooks for a specific project\nconst webhooks = await client.getWebhooks({projectId: \"proj_123\"});\nconsole.log(webhooks);\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Mem0 and OpenAI Integration\nDESCRIPTION: This snippet shows how to install the necessary npm packages for integrating Mem0 with OpenAI. It includes mem0ai for memory management, openai for OpenAI API access, and zod for schema validation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai openai zod\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Services\nDESCRIPTION: Command to build and start the Docker services defined in the docker-compose.yml file. This will build the containers for both frontend and backend services and start them according to the specified configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/full_stack.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Expected JSON Output for Get All Memory Tool Execution\nDESCRIPTION: Shows the expected paginated JSON output structure from `get_all_tool`. It includes the count, next/previous page links (if applicable), and a list of retrieved memory objects matching the filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"count\": 3,\n  \"next\": null,\n  \"previous\": null,\n  \"results\": [\n    {\n      \"id\": \"1a75e827-7eca-45ea-8c5c-cfd43299f061\",\n      \"memory\": \"Name is Alex\",\n      \"user_id\": \"alex\", \n      \"hash\": \"d0fccc8fa47f7a149ee95750c37bb0ca\",\n      \"metadata\": {\n        \"food\": \"vegan\"\n      },\n      \"categories\": [\n        \"personal_details\"\n      ],\n      \"created_at\": \"2024-11-27T16:53:43.276872-08:00\",\n      \"updated_at\": \"2024-11-27T16:53:43.276885-08:00\"\n    },\n    {\n      \"id\": \"91509588-0b39-408a-8df3-84b3bce8c521\",\n      \"memory\": \"Is a vegetarian\",\n      \"user_id\": \"alex\",\n      \"hash\": \"ce6b1c84586772ab9995a9477032df99\", \n      \"metadata\": {\n        \"food\": \"vegan\"\n      },\n      \"categories\": [\n        \"user_preferences\",\n        \"food\"\n      ],\n      \"created_at\": \"2024-11-27T16:53:43.308027-08:00\",\n      \"updated_at\": \"2024-11-27T16:53:43.308037-08:00\"\n    },\n    {\n      \"id\": \"8d74f7a0-6107-4589-bd6f-210f6bf4fbbb\",\n      \"memory\": \"Is allergic to nuts\",\n      \"user_id\": \"alex\",\n      \"hash\": \"7873cd0e5a29c513253d9fad038e758b\",\n      \"metadata\": {\n        \"food\": \"vegan\"\n      },\n      \"categories\": [\n        \"health\"\n      ],\n      \"created_at\": \"2024-11-27T16:53:43.337253-08:00\",\n      \"updated_at\": \"2024-11-27T16:53:43.337262-08:00\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Searching Memories in Mem0 (cURL)\nDESCRIPTION: This cURL command demonstrates how to search memories in Mem0 via an HTTP POST request. The request includes an API key in the Authorization header and sends a JSON payload containing the search query, user ID, and desired output format. The API endpoint is `https://api.mem0.ai/v1/memories/search/`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_20\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/search/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type\": \"application/json\" \\\n     -d '{\n         \"query\": \"What should I cook for dinner today?\",\n         \"user_id\": \"alex\",\n         \"output_format\": \"v1.1\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Making DELETE Request to Delete an App\nDESCRIPTION: Example of how to delete an app using a curl DELETE request to the specified endpoint. The app_id should be provided in the URL path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/delete.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request DELETE \\\n  --url http://localhost:8080/{app_id}/delete\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package for Gradio.app Deployment\nDESCRIPTION: Installs the Embedchain Python package which provides the necessary tools and templates for creating RAG applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/gradio_app.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying Remote PDF Files with Embedchain in Python\nDESCRIPTION: This snippet shows how to load a PDF file from a URL using `embedchain.App`. It initializes the app, adds the PDF via its URL with `data_type='pdf_file'`, and then demonstrates querying the content using the `query` method. The example query includes `citations=True` and shows the expected answer format along with context metadata, including page numbers. Note: Password-protected PDFs are not supported.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/pdf-file.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\napp.add('https://arxiv.org/pdf/1706.03762.pdf', data_type='pdf_file')\napp.query(\"What is the paper 'attention is all you need' about?\", citations=True)\n# Answer: The paper \"Attention Is All You Need\" proposes a new network architecture called the Transformer, which is based solely on attention mechanisms. It suggests that complex recurrent or convolutional neural networks can be replaced with a simpler architecture that connects the encoder and decoder through attention. The paper discusses how this approach can improve sequence transduction models, such as neural machine translation.\n# Contexts:\n# [\n#     (\n#         'Provided proper attribution is ...',\n#         {\n#             'page': 0,\n#             'url': 'https://arxiv.org/pdf/1706.03762.pdf',\n#             'score': 0.3676220203221626,\n#             ...\n#         }\n#     ),\n# ]\n```\n\n----------------------------------------\n\nTITLE: Rendering Community Connection Cards with React (JSX)\nDESCRIPTION: This snippet demonstrates the use of React components, specifically CardGroup and Card, to display a list of community channels through a grid interface. Each Card component receives properties including title, icon, href, and optionally color, defining its display and link behavior. Usage assumes the existence and import of CardGroup and Card components, possibly from a UI library or local project; these components arrange provided cards in a multi-column format. No parameters are required other than the provided component props. Inputs are React elements, and the output is the rendered JSX output for a connections page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/community/connect-with-us.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={3}>\n  <Card title=\"Twitter\" icon=\"twitter\" href=\"https://twitter.com/embedchain\">\n    Follow us on Twitter\n  </Card>\n  <Card title=\"Slack\" icon=\"slack\" href=\"https://embedchain.ai/slack\" color=\"#4A154B\">\n    Join our slack community\n  </Card>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://discord.gg/6PzXDgEjG5\" color=\"#7289DA\">\n    Join our discord community\n  </Card>\n  <Card title=\"LinkedIn\" icon=\"linkedin\" href=\"https://www.linkedin.com/company/embedchain/\">\n  Connect with us on LinkedIn\n  </Card>\n  <Card title=\"Schedule a call\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/ec\">\n  Schedule a call with Embedchain founder\n  </Card>\n  <Card title=\"Newsletter\" icon=\"message\" href=\"https://embedchain.substack.com/\">\n  Subscribe to our newsletter\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with Default Categories in Python\nDESCRIPTION: This snippet demonstrates how to add memories using default categories in Mem0. It includes setting up the client, defining a conversation, and adding memories without specifying custom categories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient()\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi, my name is Alice.\"},\n    {\"role\": \"assistant\", \"content\": \"Hi Alice, what sports do you like to play?\"},\n    {\"role\": \"user\", \"content\": \"I love playing badminton, football, and basketball. I'm quite athletic!\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! Alice seems to enjoy both individual sports like badminton and team sports like football and basketball.\"},\n    {\"role\": \"user\", \"content\": \"Sometimes, I also draw and sketch in my free time.\"},\n    {\"role\": \"assistant\", \"content\": \"That's cool! I'm sure you're good at it.\"}\n]\n\n# Add memories with default categories\nclient.add(messages, user_id='alice')\n```\n\n----------------------------------------\n\nTITLE: Configuring Required Discord Bot Permissions\nDESCRIPTION: This code shows the text permissions required for the bot to function properly in Discord. The essential permission is 'Send Messages' which allows the bot to respond to user commands.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nSend Messages (under Text Permissions)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Ingesting Multimodal Messages in Mem0 (TypeScript)\nDESCRIPTION: Illustrates the creation of a Mem0 Memory client and ingestion of a conversation consisting of text and image messages using TypeScript. Relies on the `mem0ai/oss` package and assumes appropriate configuration for the backend. The `add` method is invoked with message objects and a user ID, and messages are constructed with type-annotated content for both text and image inputs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Message } from \"mem0ai/oss\";\n\nconst client = new Memory();\n\nconst messages: Message[] = [\n    {\n        role: \"user\",\n        content: \"Hi, my name is Alice.\"\n    },\n    {\n        role: \"assistant\",\n        content: \"Nice to meet you, Alice! What do you like to eat?\"\n    },\n    {\n        role: \"user\",\n        content: {\n            type: \"image_url\",\n            image_url: {\n                url: \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n            }\n        }\n    },\n]\n\nawait client.add(messages, { userId: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package with OpenSearch Support\nDESCRIPTION: Installs the Embedchain package with OpenSearch integration using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/opensearch.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[opensearch]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Memory with Anthropic in TypeScript\nDESCRIPTION: This snippet shows how to set up and use the Memory class with Anthropic's models in TypeScript. It includes configuring the Memory object and adding messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/anthropic.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'anthropic',\n    config: {\n      apiKey: process.env.ANTHROPIC_API_KEY || '',\n      model: 'claude-3-7-sonnet-latest',\n      temperature: 0.1,\n      maxTokens: 2000,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Adding to Specific Conversation Session in Python (v2)\nDESCRIPTION: Illustrates how to add memories to a specific conversation session using both user_id and run_id parameters in Python. This method helps organize memories into logical sessions or tasks.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Adding to a specific conversation session\nmessages = [\n    {\"role\": \"user\", \"content\": \"For this trip to Paris, I want to focus on art museums.\"},\n    {\"role\": \"assistant\", \"content\": \"Great! I'll help you plan your Paris trip with a focus on art museums.\"}\n]\nclient.add(messages, user_id=\"alex\", run_id=\"paris-trip-2024\", version=\"v2\")\n\n# Later in the same conversation session\nmessages2 = [\n    {\"role\": \"user\", \"content\": \"I'd like to visit the Louvre on Monday.\"},\n    {\"role\": \"assistant\", \"content\": \"The Louvre is a great choice for Monday. Would you like information about opening hours?\"}\n]\nclient.add(messages2, user_id=\"alex\", run_id=\"paris-trip-2024\", version=\"v2\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Relevancy in RAG Applications\nDESCRIPTION: Example of how to use the AnswerRelevance metric to assess how well the generated answers address the original questions using cosine similarity.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.evaluation.metrics import AnswerRelevance\n\nmetric = AnswerRelevance()\nscore = metric.evaluate(dataset)\nprint(score)\n# 0.9505334177461916\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to retrieve all memories for a user asynchronously using the MemoryClient in JavaScript. It takes an options object with a user ID.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.getAll({ user_id: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Question-Answering Loop\nDESCRIPTION: Creates an interactive loop for asking questions about the ingested data, with exit commands and answer generation functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/pinecone.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with Hugging Face Embeddings in Python\nDESCRIPTION: Example showing how to configure and initialize Mem0 with Hugging Face embedding models. The code demonstrates setting up the environment, configuring the embedder, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/huggingface.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"huggingface\",\n        \"config\": {\n            \"model\": \"multi-qa-MiniLM-L6-cos-v1\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Creating Short-term Memory in JavaScript\nDESCRIPTION: This JavaScript snippet showcases how to create short-term memory using the `run_id`. The MemoryClient's `add` method is used to persist the provided messages, which can later be used for contextualization within that specific session. The `.then` and `.catch` are included for asynchronous handling.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning a trip to Japan next month.\"},\n    {\"role\": \"assistant\", \"content\": \"That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?\"},\n    {\"role\": \"user\", \"content\": \"Yes, please! Especially in Tokyo.\"},\n    {\"role\": \"assistant\", \"content\": \"Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction.\"}\n];\nclient.add(messages, { user_id: \"alex\", run_id: \"trip-planning-2024\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Memory LangChain Tool Function in Python\nDESCRIPTION: Defines the `search_memory` function which takes a query, filters, and version, and uses the `MemoryClient.search` method to find relevant memories. It wraps this function in a LangChain `StructuredTool` named 'search_memory'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef search_memory(query: str, filters: Dict[str, Any], version: str) -> Any:\n    \"\"\"Search memory with the given query and filters.\"\"\"\n    return client.search(query=query, version=version, filters=filters)\n\nsearch_tool = StructuredTool(\n    name=\"search_memory\",\n    description=\"Search through memories with a query and filters\",\n    func=search_memory,\n    args_schema=SearchMemoryInput\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Chainlit App with Embedchain Integration\nDESCRIPTION: Python script to create a Chainlit app that uses Embedchain for chat functionality. It sets up the OpenAI API key, initializes the Embedchain App, adds data, and defines Chainlit event handlers for chat interactions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/chainlit.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport chainlit as cl\nfrom embedchain import App\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\n@cl.on_chat_start\nasync def on_chat_start():\n    app = App.from_config(config={\n        'app': {\n            'config': {\n                'name': 'chainlit-app'\n            }\n        },\n        'llm': {\n            'config': {\n                'stream': True,\n            }\n        }\n    })\n    # import your data here\n    app.add(\"https://www.forbes.com/profile/elon-musk/\")\n    app.collect_metrics = False\n    cl.user_session.set(\"app\", app)\n\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    app = cl.user_session.get(\"app\")\n    msg = cl.Message(content=\"\")\n    for chunk in await cl.make_async(app.chat)(message.content):\n        await msg.stream_token(chunk)\n    \n    await msg.send()\n```\n\n----------------------------------------\n\nTITLE: Example Mem0 Create Webhook API Response\nDESCRIPTION: This JSON object shows the typical response structure returned by the Mem0 API after successfully creating a webhook. It includes the unique `webhook_id`, the provided `name`, `url`, subscribed `event_types`, the associated `project`, its activation status `is_active`, and timestamps for creation and last update.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n```json Output\n{\n  \"webhook_id\": \"wh_123\",\n  \"name\": \"Memory Logger\",\n  \"url\": \"https://your-app.com/webhook\",\n  \"event_types\": [\"memory_add\"],\n  \"project\": \"default-project\",\n  \"is_active\": true,\n  \"created_at\": \"2025-02-18T22:59:56.804993-08:00\",\n  \"updated_at\": \"2025-02-18T23:06:41.479361-08:00\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Example Response: Adding Memory (Open Source) - JSON Output\nDESCRIPTION: Presents a sample response from adding text memory in the open-source context. Each memory chunk includes an ID, condensed memory summary, and the event. The design mirrors both managed and open-source output for compatibility.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n[\\n    {\\n        \\\"id\\\": \\\"3dc6f65f-fb3f-4e91-89a8-ed1a22f8898a\\\",\\n        \\\"data\\\": {\\\"memory\\\": \\\"Likes to drink coffee in the morning\\\"},\\n        \\\"event\\\": \\\"ADD\\\"\\n    },\\n    {\\n        \\\"id\\\": \\\"f1673706-e3d6-4f12-a767-0384c7697d53\\\",\\n        \\\"data\\\": {\\\"memory\\\": \\\"Likes to go for a walk\\\"},\\n        \\\"event\\\": \\\"ADD\\\"\\n    }\\n]\n```\n\n----------------------------------------\n\nTITLE: Adding Data and Querying Embedchain App\nDESCRIPTION: This code snippet shows how to add data to the Embedchain App and query it. It adds a Forbes profile URL and then queries the app about Elon Musk's college degrees.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#Now let's add some data.\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n#Query the app\nresponse = app.query(\"what college degrees does elon musk have?\")\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 via npm\nDESCRIPTION: Command to install the Mem0 JavaScript package using npm package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify using npm or yarn\nDESCRIPTION: Commands to install Mintlify globally using either npm or yarn package managers. This is the first step in setting up the documentation environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/documentation.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn global add mintlify\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Embedchain\nDESCRIPTION: Sets up the OpenAI API key as an environment variable, which is required for Embedchain to generate embeddings. The key should be obtained from the OpenAI platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/lancedb.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Bot\nDESCRIPTION: Command format for adding new data sources or text to the WhatsApp bot.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/whatsapp_bot.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nadd <url_or_text>\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Prompt with Mem0 Graph Memory in Python\nDESCRIPTION: This snippet demonstrates configuring the Mem0 Memory module to use a Neo4j graph store with a customized prompt for extracting only sports-related entity relationships. It imports the Memory class, creates a configuration dictionary with authentication parameters and the custom prompt, and initializes a Memory instance using this config. Dependencies include the mem0 package and a valid Neo4j database. The config expects graph store connection details and a string prompt that instructs the extraction behavior. The result is a Memory instance ready for domain-targeted entity extraction.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/features.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nconfig = {\n    \"graph_store\": {\n        \"provider\": \"neo4j\",\n        \"config\": {\n            \"url\": \"neo4j+s://xxx\",\n            \"username\": \"neo4j\",\n            \"password\": \"xxx\"\n        },\n        \"custom_prompt\": \"Please only extract entities containing sports related relationships and nothing else.\",\n    }\n}\n\nm = Memory.from_config(config_dict=config)\n\n```\n\n----------------------------------------\n\nTITLE: Querying Embedchain App with User Input\nDESCRIPTION: Implements a loop to continuously accept user questions, query the Embedchain app, and print answers until the user exits.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/opensearch.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories - Result Sample - JSON\nDESCRIPTION: Sample output from Memory.getAll, listing memories for the 'alice' user. Each object includes memory text, hash, timestamps, metadata, and userId. Used for result reference and not directly executed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n      \"memory\": \"User is planning to watch a movie tonight.\",\n      \"hash\": \"1a271c007316c94377175ee80e746a19\",\n      \"createdAt\": \"2025-02-27T16:33:20.557Z\",\n      \"updatedAt\": \"2025-02-27T16:33:27.051Z\",\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    },\n    {\n      \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n      \"memory\": \"User loves sci-fi movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"createdAt\": \"2025-02-27T16:33:20.560Z\",\n      \"updatedAt\": undefined,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    },\n    {\n      \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n      \"memory\": \"User is not a big fan of thriller movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"createdAt\": \"2025-02-27T16:33:20.560Z\",\n      \"updatedAt\": undefined,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Mem0 OSS Configuration Dictionary - Python\nDESCRIPTION: Defines a configuration dictionary for Mem0 OSS setup, including vector store, LLM, embedder, and version details. This must be customized to match your local setup, such as setting the correct collection, host, and embedding dimensions for your vector store and LLM details.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test_9\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 1536,  # Change this according to your local model's dimensions\n        },\n    },\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        },\n    },\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\"model\": \"text-embedding-3-small\"},\n    },\n    \"version\": \"v1.1\",\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Logging for Memory Voice Agent\nDESCRIPTION: Python code to configure the logging module for debugging memory-enabled voice agents. Sets up DEBUG level logging with timestamp, logger name, and log level in the output format.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(\"memory_voice_agent\")\n```\n\n----------------------------------------\n\nTITLE: Set Expiration Date in Mem0 - cURL\nDESCRIPTION: This cURL command demonstrates how to add a memory to Mem0 with an expiration date using the Mem0 API. It sends a POST request to the /memories/ endpoint with the message, user ID, and expiration date in the request body. Requires an API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/expiration-date.mdx#_snippet_2\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [\n             {\n                \"role\": \"user\", \n                \"content\": \"I\\'ll be in San Francisco until end of this month.\"\n            }\n         ],\n         \"user_id\": \"alex\",\n         \"expiration_date\": \"2023-08-31\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Answer Relevancy Evaluation\nDESCRIPTION: Shows how to customize answer relevancy evaluation by configuring the model, embedder, API key, and number of generated questions using the AnswerRelevanceConfig class.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.config.evaluation.base import AnswerRelevanceConfig\nfrom embedchain.evaluation.metrics import AnswerRelevance\n\neval_config = AnswerRelevanceConfig(\n    model='gpt-4',\n    embedder=\"text-embedding-ada-002\",\n    api_key=\"sk-xxx\",\n    num_gen_questions=2\n)\nmetric = AnswerRelevance(config=eval_config)\nscore = metric.evaluate(dataset)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Client and Adding Multimodal Messages in TypeScript\nDESCRIPTION: This TypeScript snippet shows how to initialize the `MemoryClient` from the `mem0ai` package, define an array of messages containing both text and an image specified by a URL, and add these messages to a user's memory using the asynchronous `client.add` method. It requires the `mem0ai` library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport MemoryClient from \"mem0ai\";\n\nconst client = new MemoryClient();\n\nconst messages = [\n    {\n        role: \"user\",\n        content: \"Hi, my name is Alice.\"\n    },\n    {\n        role: \"assistant\",\n        content: \"Nice to meet you, Alice! What do you like to eat?\"\n    },\n    {\n        role: \"user\",\n        content: {\n            type: \"image_url\",\n            image_url: {\n                url: \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n            }\n        }\n    },\n]\n\nawait client.add(messages, { user_id: \"alice\" })\n```\n\n----------------------------------------\n\nTITLE: Initializing Memgraph Graph Memory - Basic (Python)\nDESCRIPTION: This Python code initializes a Mem0 Memory object using Memgraph as the graph backend. It provides connection parameters in the config dictionary, with the 'provider' set to 'memgraph'. Mem0 and Memgraph must be installed and running; the Memory.from_config method is used for instantiation. Credentials and Bolt port must be specified for successful connection.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nconfig = {\n    \"graph_store\": {\n        \"provider\": \"memgraph\",\n        \"config\": {\n            \"url\": \"bolt://localhost:7687\",\n            \"username\": \"memgraph\",\n            \"password\": \"xxx\",\n        },\n    },\n}\n\nm = Memory.from_config(config_dict=config)\n\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Semantic Search with Embedchain in Python\nDESCRIPTION: This snippet demonstrates performing a basic semantic search using the Embedchain Python API. It creates an instance of the `App` class, indexes a publicly available web page, and queries the semantic search API with a user input, limiting the number of returned documents. The `add` and `search` methods are used, and the output is printed to standard output. Requires the `embedchain` library and an accessible internet connection to retrieve documents.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/search.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\napp = App()\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\ncontext = app.search(\\\"What is the net worth of Elon?\\\", num_documents=2)\\nprint(context)\n```\n\n----------------------------------------\n\nTITLE: Getting All Users in JavaScript\nDESCRIPTION: This JavaScript code retrieves all users from the Mem0 API using the `client.users()` method. It uses a Promise to handle the asynchronous request, logging the users to the console upon success or any error to the console if the request fails.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_36\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.users()\n    .then(users => console.log(users))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Creating and Starting Embedchain App without Docker\nDESCRIPTION: Commands to create a new Embedchain application named 'my-app', navigate to its directory, and start the application without using Docker.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/full-stack.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nec create-app my-app\ncd my-app\nec start\n```\n\n----------------------------------------\n\nTITLE: Searching Memories Asynchronously in Python\nDESCRIPTION: This code demonstrates how to search for memories asynchronously based on a query using the AsyncMemoryClient in Python. It takes a search query and a user ID as parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait client.search(\"What is Alice's favorite sport?\", user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for JinaChat and OpenAI\nDESCRIPTION: This code sets the necessary environment variables for JinaChat and OpenAI. It requires API keys from both services, which should be obtained from their respective dashboards.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/jina.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nos.environ[\"JINACHAT_API_KEY\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Delete All User Memories with Mem0\nDESCRIPTION: Deletes all memories associated with a specific user from the Mem0 system. This is achieved using Python, JavaScript, and a cURL request.  The cURL request requires an API key for authorization and specifies the user ID as a query parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_58\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.deleteAll({ user_id: \"alex\" })\n    .then(result => console.log(result))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Path Icon for LangChain Tools Integration\nDESCRIPTION: SVG path definition for the LangChain Tools icon, using the path element with complex d-attribute instructions and currentColor fill to match theme colors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_2\n\nLANGUAGE: SVG\nCODE:\n```\nd=\"M6.0988 5.9175C2.7359 5.9175 0 8.6462 0 12s2.736 6.0825 6.0988 6.0825h11.8024C21.2641 18.0825 24 15.3538 24 12s-2.736 -6.0825 -6.0988 -6.0825ZM5.9774 7.851c0.493 0.0124 1.02 0.2496 1.273 0.6228 0.3673 0.4592 0.4778 1.0668 0.8944 1.4932 0.5604 0.6118 1.199 1.1505 1.7161 1.802 0.4892 0.5954 0.8386 1.2937 1.1436 1.9975 0.1244 0.2335 0.1257 0.5202 0.31 0.7197 0.0908 0.1204 0.5346 0.4483 0.4383 0.5645 0.0555 0.1204 0.4702 0.286 0.3263 0.4027 -0.1944 0.04 -0.4129 0.0476 -0.5616 -0.1074 -0.0549 0.126 -0.183 0.0596 -0.2819 0.0432a4 4 0 0 0 -0.025 0.0736c-0.3288 0.0219 -0.5754 -0.3126 -0.732 -0.565 -0.3111 -0.168 -0.6642 -0.2702 -0.982 -0.446 -0.0182 0.2895 0.0452 0.6485 -0.231 0.8353 -0.014 0.5565 0.8436 0.0656 0.9222 0.4804 -0.061 0.0067 -0.1286 -0.0095 -0.1774 0.0373 -0.2239 0.2172 -0.4805 -0.1645 -0.7385 -0.007 -0.3464 0.174 -0.3808 0.3161 -0.8096 0.352 -0.0237 -0.0359 -0.0143 -0.0592 0.0059 -0.0811 0.1207 -0.1399 0.1295 -0.3046 0.3356 -0.3643 -0.2122 -0.0334 -0.3899 0.0833 -0.5686 0.1757 -0.2323 0.095 -0.2304 -0.2141 -0.5878 0.0164 -0.0396 -0.0322 -0.0208 -0.0615 0.0018 -0.0864 0.0908 -0.1107 0.2102 -0.127 0.345 -0.1208 -0.663 -0.3686 -0.9751 0.4507 -1.2813 0.0432 -0.092 0.0243 -0.1265 0.1068 -0.1845 0.1652 -0.05 -0.0548 -0.0123 -0.1212 -0.0099 -0.1857 -0.0598 -0.028 -0.1356 -0.041 -0.1179 -0.1366 -0.1171 -0.0395 -0.1988 0.0295 -0.286 0.0952 -0.0787 -0.0608 0.0532 -0.1492 0.0776 -0.2125 0.0702 -0.1216 0.23 -0.025 0.3111 -0.1126 0.2306 -0.1308 0.552 0.0814 0.8155 0.0455 0.203 0.0255 0.4544 -0.1825 0.3526 -0.39 -0.2171 -0.2767 -0.179 -0.6386 -0.1839 -0.9695 -0.0268 -0.1929 -0.491 -0.4382 -0.6252 -0.6462 -0.1659 -0.1873 -0.295 -0.4047 -0.4243 -0.6182 -0.4666 -0.9008 -0.3198 -2.0584 -0.9077 -2.8947 -0.266 0.1466 -0.6125 0.0774 -0.8418 -0.119 -0.1238 0.1125 -0.1292 0.2598 -0.139 0.4161 -0.297 -0.2962 -0.2593 -0.8559 -0.022 -1.1855 0.0969 -0.1302 0.2127 -0.2373 0.342 -0.3316 0.0292 -0.0213 0.0391 -0.0419 0.0385 -0.0747 0.1174 -0.5267 0.5764 -0.7391 1.0694 -0.7267m12.4071 0.46c0.5575 0 1.0806 0.2159 1.474 0.6082s0.61 0.9145 0.61 1.4704c0 0.556 -0.2167 1.078 -0.61 1.4698v0.0006l-0.902 0.8995a2.08 2.08 0 0 1 -0.8597 0.5166l-0.0164 0.0047 -0.0058 0.0164a2.05 2.05 0 0 1 -0.474 0.7308l-0.9018 0.8995c-0.3934 0.3924 -0.917 0.6083 -1.4745 0.6083s-1.0806 -0.216 -1.474 -0.6083c-0.813 -0.8107 -0.813 -2.1294 0 -2.9402l0.9019 -0.8995a2.056 2.056 0 0 1 0.858 -0.5143l0.017 -0.0053 0.0058 -0.0158a2.07 2.07 0 0 1 0.4752 -0.7337l0.9018 -0.8995c0.3934 -0.3924 0.9171 -0.6083 1.4745 -0.6083zm0 0.8965a1.18 1.18 0 0 0 -0.8388 0.3462l-0.9018 0.8995a1.181 1.181 0 0 0 -0.3427 0.9252l0.0053 0.0572c0.0323 0.2652 0.149 0.5044 0.3374 0.6917 0.13 0.1296 0.2733 0.2114 0.4471 0.2686a0.9 0.9 0 0 1 0.014 0.1582 0.884 0.884 0 0 1 -0.2609 0.6304l-0.0554 0.0554c-0.3013 -0.1028 -0.5525 -0.253 -0.7794 -0.4792a2.06 2.06 0 0 1 -0.5761 -1.0968l-0.0099 -0.0578 -0.0461 0.0368a1.1 1.1 0 0 0 -0.0876 0.0794l-0.9024 0.8995c-0.4623 0.461 -0.4623 1.212 0 1.673 0.2311 0.2305 0.535 0.346 0.8394 0.3461 0.3043 0 0.6077 -0.1156 0.8388 -0.3462l0.9019 -0.8995c0.4623 -0.461 0.4623 -1.2113 0 -1.673a1.17 1.17 0 0 0 -0.4367 -0.2749 1 1 0 0 1 -0.014 -0.1611c0 -0.2591 0.1023 -0.505 0.2901 -0.6923 0.3019 0.1028 0.57 0.2694 0.7962 0.495 0.3007 0.2999 0.4994 0.679 0.5756 1.0968l0.0105 0.0578 0.0455 -0.0373a1.1 1.1 0 0 0 0.0887 -0.0794l0.902 -0.8996c0.4622 -0.461 0.4628 -1.2124 0 -1.6735a1.18 1.18 0 0 0 -0.8395 -0.3462Zm-9.973 5.1567 -0.0006 0.0006c-0.0793 0.3078 -0.1048 0.8318 -0.506 0.847 -0.033 0.1776 0.1228 0.2445 0.2655 0.1874 0.141 -0.0645 0.2081 0.0508 0.2557 0.1657 0.2177 0.0317 0.5394 -0.0725 0.5516 -0.3298 -0.325 -0.1867 -0.4253 -0.5418 -0.5662 -0.8709\"\n```\n\n----------------------------------------\n\nTITLE: Configuring the SlackLoader for Embedchain - Python\nDESCRIPTION: This snippet demonstrates initializing the SlackLoader by providing custom configuration options, including base_url, headers, and team_id. It expects a valid Slack token set as SLACK_USER_TOKEN in environment variables. Dependencies: 'embedchain' library and a properly structured config dictionary containing Slack API details. Used as a custom data loader for fetching Slack data in Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/slack.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.loaders.slack import SlackLoader\\n\\nos.environ[\\\"SLACK_USER_TOKEN\\\"] = \\\"xoxp-*\\\"\\n\\nconfig = {\\n    'base_url': slack_app_url,\\n    'headers': web_headers,\\n    'team_id': slack_team_id,\\n}\\n\\nloader = SlackLoader(config)\n```\n\n----------------------------------------\n\nTITLE: Debugging Function Tools with Logging - Python\nDESCRIPTION: This example demonstrates how to configure and use Python's logging module for debug output in function tools decorated with `@function_tool`. It sets up a logger named 'memory_voice_agent' and replaces print statements inside async tools with logger calls to ensure visibility when OpenAI Agents SDK captures standard output. Essential for troubleshooting in agent tool functions, requiring Python's standard logging library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport logging\n\n# Set up logging at the top of your file\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    force=True\n)\nlogger = logging.getLogger(\"memory_voice_agent\")\n\n# Then use logger in your function tools\n@function_tool\nasync def save_memories(\n    memory: str\n) -> str:\n    \"\"\"Store a user memory in memory.\"\"\"\n    # This will be visible in your console\n    logger.debug(f\"Saving memory: {memory} for user {USER_ID}\")\n    \n    # Rest of your function...\n\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package\nDESCRIPTION: Command to install the embedchain package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/railway.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Loading Data from MySQL and Querying with Embedchain App in Python\nDESCRIPTION: This snippet demonstrates creating an Embedchain Pipeline (aliased as App), adding data using a MySQL loader via an SQL SELECT query, and querying the app for answers. Dependencies include a valid MySQLLoader instance and a working Embedchain installation. The app.add function takes a SQL SELECT statement, loads the returned data via the loader, and makes it queryable. Do not use this method for CREATE or INSERT queries; use only executable queries like SELECT. Output shows a sample response to a query against the loaded data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/mysql.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain.pipeline import Pipeline as App\\n\\napp = App()\\n\\napp.add(\"SELECT * FROM table_name;\", data_type='mysql', loader=mysql_loader)\\n# Adds `(1, 'What is your net worth, Elon Musk?', \"As of October 2023, Elon Musk's net worth is $255.2 billion.\")`\\n\\nresponse = app.query(question)\\n# Answer: As of October 2023, Elon Musk's net worth is $255.2 billion.\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Embedding Model in YAML\nDESCRIPTION: This YAML configuration specifies the use of Cohere's 'embed-english-light-v3.0' model as the embedding provider in Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-english-light-v3.0'\n```\n\n----------------------------------------\n\nTITLE: Adding Postgres Data and Querying Embedchain App in Python\nDESCRIPTION: This snippet demonstrates how to create an Embedchain `App`, add data from a PostgreSQL database using a `SELECT` query and the previously configured `PostgresLoader`, and then query the app. It requires setting the `OPENAI_API_KEY` environment variable. The `app.add()` method takes the SQL query, specifies `data_type='postgres'`, and uses the initialized `postgres_loader`. It's important to only use executable `SELECT` queries for adding data, as `CREATE` or `INSERT` will not add relevant content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/postgres.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom embedchain.pipeline import Pipeline as App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\napp = App()\n\nquestion = \"What is Elon Musk's networth?\"\nresponse = app.query(question)\n# Answer: As of September 2021, Elon Musk's net worth is estimated to be around $250 billion, making him one of the wealthiest individuals in the world. However, please note that net worth can fluctuate over time due to various factors such as stock market changes and business ventures.\n\napp.add(\"SELECT * FROM table_name;\", data_type='postgres', loader=postgres_loader)\n# Adds `(1, 'What is your net worth, Elon Musk?', \"As of October 2023, Elon Musk's net worth is $255.2 billion.\")`\n\nresponse = app.query(question)\n# Answer: As of October 2023, Elon Musk's net worth is $255.2 billion.\n\n```\n\n----------------------------------------\n\nTITLE: Reset Mem0 Client\nDESCRIPTION: Resets the Mem0 client, deleting all users and memories. This is demonstrated using both Python and JavaScript.  The expected result is logged to the console in the JavaScript example.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_66\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.reset()\n    .then(result => console.log(result))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Searching Memories using Metadata and Categories Filters in cURL\nDESCRIPTION: This cURL command searches memories using metadata and categories filters. It sends a POST request to the /memories/search endpoint with a JSON payload containing the query and filters. The 'contains' operator is used within the 'categories' filter. A valid API token is required in the Authorization header.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_34\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/search/?version=v2\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"query\": \"What do you know about me?\",\n         \"filters\": {\n             \"AND\": [\n                 {\n                     \"metadata\": {\n                         \"food\": \"vegan\"\n                     }\n                 },\n                 {\n                     \"categories\": {\n                         \"contains\": \"food_preferences\"\n                     }\n                 }\n             ]\n         }\n     }'\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Private AI\nDESCRIPTION: Installs the required Python packages for the Private AI project from the requirements.txt file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/private-ai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Viewing Memory Addition Results (JSON)\nDESCRIPTION: Presents a sample JSON output produced when images and text are ingested into Mem0. Each entry documents the resulting memory, event type, and unique identifier. No direct dependencies; used as a reference to expected response structure from a successful ingestion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"memory\": \"Name is Alice\",\n      \"event\": \"ADD\",\n      \"id\": \"7ae113a3-3cb5-46e9-b6f7-486c36391847\"\n    },\n    {\n      \"memory\": \"Likes large pizza with toppings including cherry tomatoes, black olives, green spinach, yellow bell peppers, diced ham, and sliced mushrooms\",\n      \"event\": \"ADD\",\n      \"id\": \"56545065-7dee-4acf-8bf2-a5b2535aabb3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Agent Memories - JavaScript\nDESCRIPTION: This snippet retrieves all memories associated with a specific AI agent using JavaScript. It uses the `client.getAll` method, with a promise to handle the response or any errors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_42\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.getAll({ agent_id: \"ai-tutor\", page: 1, page_size: 50 })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with GPT4All Models\nDESCRIPTION: Creates an Embedchain app with GPT4All as both the LLM provider and embedder. Uses the orca-mini-3b model for text generation and all-MiniLM-L6-v2 for embeddings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/gpt4all.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"gpt4all\",\n        \"config\": {\n            \"model\": \"orca-mini-3b-gguf2-q4_0.gguf\",\n            \"temperature\": 0.5,\n            \"max_tokens\": 1000,\n            \"top_p\": 1,\n            \"stream\": False\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"gpt4all\",\n        \"config\": {\n            \"model\": \"all-MiniLM-L6-v2\"\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Custom Memory-Enabled UserProxyAgent Implementation\nDESCRIPTION: Definition of a custom UserProxyAgent class with integrated memory capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Mem0ProxyCoderAgent(UserProxyAgent):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.memory = MEM0_MEMORY_CLIENT\n        self.agent_id = kwargs.get(\"name\")\n\n    def initiate_chat(self, assistant, message):\n        agent_memories = self.memory.search(message, agent_id=self.agent_id, limit=3)\n        agent_memories_txt = '\\n'.join(mem['memory'] for mem in agent_memories)\n        prompt = f\"{message}\\n Coding Preferences: \\n{str(agent_memories_txt)}\"\n        response = super().initiate_chat(assistant, message=prompt)\n        response_dist = response.__dict__ if not isinstance(response, dict) else response\n        MEMORY_DATA = [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": response_dist}]\n        self.memory.add(MEMORY_DATA, agent_id=self.agent_id)\n        return response\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry\nDESCRIPTION: Commands to install project dependencies using Poetry package manager and activate the virtual environment\nSOURCE: https://github.com/mem0ai/mem0/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake install_all\n\n#activate\n\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Adding YouTube Channel Content to Embedchain App\nDESCRIPTION: Python code to create an Embedchain App instance and add all videos from a YouTube channel using the '@channel_name' syntax with the 'youtube_channel' data type.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/youtube-channel.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add(\"@channel_name\", data_type=\"youtube_channel\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Pinecone and OpenAI\nDESCRIPTION: Configures required environment variables for OpenAI API key, Pinecone API key, and Pinecone environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/pinecone.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nos.environ[\"PINECONE_API_KEY\"] = \"xxx\"\nos.environ[\"PINECONE_ENV\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Local Python Development Setup\nDESCRIPTION: Command to run the application locally in development mode with auto-reload functionality enabled.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/rest-api/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDEVELOPMENT=True && python -m main\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain and Creating RAG Application\nDESCRIPTION: Commands to install Embedchain and create a new RAG application using the Fly.io template.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-rag-app\nec create --template=fly.io\n```\n\n----------------------------------------\n\nTITLE: Delete All Users with Mem0\nDESCRIPTION: Deletes all users, agents and runs from the Mem0 system using Python and JavaScript. It demonstrates the basic function calls. The expected response is logged to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_61\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.delete_users()\n    .then(users => console.log(users))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: Adds a web page as a data source to the Embedchain app for processing and indexing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/opensearch.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Query Loop with GPT4All\nDESCRIPTION: Sets up an interactive loop that continuously prompts the user for questions and returns answers from the Embedchain app using GPT4All. The loop exits when the user enters 'q', 'exit', or 'quit'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/gpt4all.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 SDK with npm - Bash\nDESCRIPTION: Installs the Mem0 Node SDK via npm for use with TypeScript or JavaScript. You must have Node.js and npm pre-installed. This command should be run in your project directory to add 'mem0ai' to your dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Chatting with an AI Assistant\nDESCRIPTION: Demonstrates how to interact with the AI Assistant by sending a question and receiving a response. The assistant processes the query using the data sources added to it.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/opensource-assistant.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nassistant.chat(\"How much OpenAI credits were offered to attendees during OpenAI DevDay?\")\n# Response: 'Every attendee of OpenAI DevDay 2023 was offered $500 in OpenAI credits.'\n```\n\n----------------------------------------\n\nTITLE: Delete Memory by ID with Mem0\nDESCRIPTION: Deletes a specific memory from the Mem0 system using its ID. The examples demonstrate how to perform this operation using Python, JavaScript, and a cURL request. The cURL request requires an API key for authorization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_55\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.delete(\"memory-id-here\")\n    .then(result => console.log(result))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Running Sadhguru AI Locally with Streamlit\nDESCRIPTION: This snippet shows the commands to set up and run Sadhguru AI as a local Streamlit application. It includes setting the OpenAI API key, installing dependencies, and starting the app.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/sadhguru-ai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=sk-xxx\npip install -r requirements.txt\nstreamlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Query Loop with Embedchain\nDESCRIPTION: Creates an interactive loop that allows continuous querying of the app with a simple exit mechanism. User inputs questions, and the app responds with answers based on indexed data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/hugging_face_hub.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Deleting Specific Memory Asynchronously in Python\nDESCRIPTION: This code demonstrates how to delete a specific memory asynchronously using the AsyncMemoryClient in Python. It takes a memory ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait client.delete(memory_id=\"memory-id-here\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Embedchain App on Fly.io\nDESCRIPTION: Command to initialize a Fly.io application without deploying it immediately. This prepares the configuration for deployment later.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfly launch --no-deploy\n```\n\n----------------------------------------\n\nTITLE: Initializing the Basic Mem0 OSS Python Client\nDESCRIPTION: Imports the `Memory` class from the `mem0` library and creates a default instance named `m`. This basic initialization typically requires the OpenAI API key to be configured in the environment for default LLM and embedding operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory; m = Memory()\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify on Custom Port\nDESCRIPTION: Command to start Mintlify on a custom port using the --port flag.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/docs.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev --port 3333\n```\n\n----------------------------------------\n\nTITLE: Loading Local CSV File with Embedchain in Python\nDESCRIPTION: This snippet demonstrates how to initialize an `embedchain` App and add data from a CSV file located on the local filesystem. The `data_type='csv'` argument specifies the format. Column headers are automatically prepended to the corresponding data values.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/csv.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\napp.add('/path/to/file.csv', data_type='csv')\n```\n\n----------------------------------------\n\nTITLE: Querying the Bot\nDESCRIPTION: Format for asking questions to the WhatsApp bot.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/whatsapp_bot.mdx#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n<your-question-here>\n```\n\n----------------------------------------\n\nTITLE: Deploying to Embedchain Platform with Python\nDESCRIPTION: POST request to deploy an Embedchain application to the Embedchain Platform using Python requests library. This requires an API key for authentication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://localhost:8080/my-app/deploy\"\n\npayload = \"api_key=ec-xxxx\"\n\nresponse = requests.request(\"POST\", url, data=payload)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Loading CSV File from URL with Embedchain in Python\nDESCRIPTION: This snippet shows how to initialize an `embedchain` App and add data from a CSV file hosted at a specified URL. The `data_type=\"csv\"` argument is necessary to indicate the data format. The library handles fetching and processing the CSV data, prepending headers to values.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/csv.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\napp.add('https://people.sc.fsu.edu/~jburkardt/data/csv/airtravel.csv', data_type=\"csv\")\n```\n\n----------------------------------------\n\nTITLE: Running Telegram Bot with Docker\nDESCRIPTION: Docker command to run a Telegram bot container with environment variables for OpenAI API key and Telegram bot token. Exposes port 8000 for webhook access.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/telegram_bot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name telegram-bot -e OPENAI_API_KEY=sk-xxx -e TELEGRAM_BOT_TOKEN=xxx -p 8000:8000 embedchain/telegram-bot\n```\n\n----------------------------------------\n\nTITLE: Importing Embedchain App for GPT4All Integration\nDESCRIPTION: Imports the App class from embedchain which will be used to create a retrieval-augmented generation system with GPT4All models.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/gpt4all.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n```\n\n----------------------------------------\n\nTITLE: Deleting a Memory (Node.js, JavaScript)\nDESCRIPTION: Removes an existing memory from Mem0 by providing its memory_id. Useful for managing or cleaning up memory records. Requires the memory_id obtained via previous add or query operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Delete a memory\nawait m.delete({ memory_id: \"mem456\" });\n```\n\n----------------------------------------\n\nTITLE: Example Mem0 Update Webhook API Response\nDESCRIPTION: This JSON object represents the successful response from the Mem0 API after updating a webhook. It typically contains a simple confirmation message indicating the update was successful.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n```json Output\n{\n  \"message\": \"Webhook updated successfully\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Specific Date - JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to add a memory to Mem0 with a specific date (January 1, 2023) as a Unix timestamp. It directly uses the pre-defined timestamp value and adds the memory using the `client.add` method. The result or error is logged to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n// January 1, 2023 timestamp\nconst january2023Timestamp = 1672531200;  // Unix timestamp for 2023-01-01 00:00:00 UTC\n\nclient.add(\"Important historical information\", { user_id: \"user1\", timestamp: january2023Timestamp })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Get All Memories for a User - Python\nDESCRIPTION: Retrieves all memories of a specified user with pagination options using the Python SDK. Parameters include filters object, version, page, and page_size. Returns a list of all found memories for the filtered user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfilters = {\\n   \\\"AND\\\": [\\n      {\\n         \\\"user_id\\\": \\\"alex\\\"\\n      }\\n   ]\\n}\\n\\nall_memories = client.get_all(version=\\\"v2\\\", filters=filters, page=1, page_size=50)\n```\n\n----------------------------------------\n\nTITLE: Running Gradio App Locally for Testing\nDESCRIPTION: Commands to install dependencies and run the Gradio application locally for testing before deployment to Huggingface spaces.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Running Streamlit App Locally\nDESCRIPTION: Command to run the Streamlit app locally using the Python script.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/streamlit-mistral.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nstreamlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Adding Memory Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to add a new memory asynchronously using the MemoryClient in JavaScript. It takes an array of messages and an options object with a user ID.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst messages = [\n    {\"role\": \"user\", \"content\": \"Alice loves playing badminton\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! Alice is a fitness freak\"},\n];\nawait client.add(messages, { user_id: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Using Mem0 with In-Memory Vector Database in TypeScript\nDESCRIPTION: Example of initializing Mem0 with an in-memory vector database in TypeScript, which is only supported in the TypeScript implementation. This configuration sets up a memory provider with a specified collection name and embedding dimension.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/config.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Example for in-memory vector database (Only supported in TypeScript)\nimport { Memory } from 'mem0ai/oss';\n\nconst configMemory = {\n  vector_store: {\n    provider: 'memory',\n    config: {\n      collectionName: 'memories',\n      dimension: 1536,\n    },\n  },\n};\n\nconst memory = new Memory(configMemory);\nawait memory.add(\"Your text here\", { userId: \"user\", metadata: { category: \"example\" } });\n```\n\n----------------------------------------\n\nTITLE: Custom Elasticsearch Search Query Implementation\nDESCRIPTION: Implementation of a custom search query function for Elasticsearch, demonstrating how to customize the vector search behavior with additional parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/elasticsearch.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Optional, Dict\nfrom mem0 import Memory\n\ndef custom_search_query(query: List[float], limit: int, filters: Optional[Dict]) -> Dict:\n    return {\n        \"knn\": {\n            \"field\": \"vector\", \n            \"query_vector\": query, \n            \"k\": limit, \n            \"num_candidates\": limit * 2\n        }\n    }\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"elasticsearch\",\n        \"config\": {\n            \"collection_name\": \"mem0\",\n            \"host\": \"localhost\",\n            \"port\": 9200,\n            \"embedding_model_dims\": 1536,\n            \"custom_search_query\": custom_search_query\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Bot Script\nDESCRIPTION: Command to execute the Python script that runs the Poe bot.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/poe_bot.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython your_script.py\n```\n\n----------------------------------------\n\nTITLE: Adding Sitemap URLs Using embedchain in Python\nDESCRIPTION: This Python code demonstrates how to initialize an embedchain App instance and add all text web pages from a given XML sitemap by specifying the data_type as 'sitemap'. The example requires the embedchain Python package to be installed. The app.add() method takes the sitemap URL and filters out non-text files, only importing pages containing textual content. The primary input is a valid sitemap.xml URL, and the output is that relevant pages are loaded into the application context; non-webpage files are ignored.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/sitemap.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add('https://example.com/sitemap.xml', data_type='sitemap')\n```\n\n----------------------------------------\n\nTITLE: Setting Mem0 API Key as Environment Variable (Bash)\nDESCRIPTION: Sets the MEM0 API key in your environment so it can be accessed by Python scripts through os.getenv(). The Mem0MemoryService requires you to export this variable; replace 'your_mem0_api_key' with your actual API key obtained from mem0.ai. This must be done before running any dependent Python code.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/pipecat.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MEM0_API_KEY=your_mem0_api_key\n```\n\n----------------------------------------\n\nTITLE: Delete Entity OpenAPI Path Definition\nDESCRIPTION: OpenAPI path specification for deleting entities by type and ID. Defines the URL path parameters for entity_type and entity_id.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/entities/delete-user.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndelete /v1/entities/{entity_type}/{entity_id}/\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4 as LLM Model in Embedchain\nDESCRIPTION: This snippet illustrates how to set up Embedchain to use GPT-4 as the language model. It demonstrates setting the OpenAI API key and loading the configuration from a YAML file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'xxx'\n\n# load llm configuration from gpt4.yaml file\napp = App.from_config(config_path=\"gpt4.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: 'gpt-4'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Running Slack AI without Docker\nDESCRIPTION: Commands to install requirements and start the Slack AI application directly on the host machine without containerization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack-AI.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nec install-reqs\nec start\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App\nDESCRIPTION: Adds a web URL as a data source to the Embedchain application. This example adds Elon Musk's Forbes profile which will be used as context for answering questions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/anthropic.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Ingesting Multimodal Messages in Mem0 (Python)\nDESCRIPTION: Demonstrates how to enable vision capabilities in Mem0 using a configuration dictionary, instantiate a Memory client, and ingest a conversation containing both text and image messages. Requires the `mem0` Python package and proper OpenAI provider setup. The main parameters are `enable_vision` (activates image analysis) and `vision_details` (controls detail level). The `add()` method accepts a list of messages and a user identifier, processing both text and image content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import Memory\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"enable_vision\": True,\n            \"vision_details\": \"high\"\n        }\n    }\n}\n\nclient = Memory.from_config(config=config)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi, my name is Alice.\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Nice to meet you, Alice! What do you like to eat?\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n            }\n        }\n    },\n]\n\n# Calling the add method to ingest messages into the memory system\nclient.add(messages, user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Customizing SQL Data Chunking with MySQLChunker in Embedchain (Python)\nDESCRIPTION: This code shows how to instantiate a custom MySQLChunker for use with Embedchain, specifying chunking parameters (size, overlap, length function) via ChunkerConfig. The chunker divides loaded SQL data into manageable pieces before insertion or analysis. The provided chunker is then passed to app.add, enabling customized data chunking for the loader. Ensure that 'embedchain' and its chunker modules are installed and configured, and adjust chunking parameters according to data size and analysis requirements.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/mysql.mdx#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom embedchain.chunkers.mysql import MySQLChunker\\nfrom embedchain.config.add_config import ChunkerConfig\\n\\nmysql_chunker_config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)\\nmysql_chunker = MySQLChunker(config=mysql_chunker_config)\\n\\napp.add(\"SELECT * FROM table_name;\", data_type='mysql', loader=mysql_loader, chunker=mysql_chunker)\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit Hooks (Bash)\nDESCRIPTION: This snippet shows how to install pre-commit hooks to maintain code quality and enforce project standards. The command should be run once after cloning the repository to ensure all contributors have the pre-commit hooks installed. Pre-commit must be installed in the environment prior to use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 Memory with Custom Embedder in Python\nDESCRIPTION: This snippet demonstrates how to set up a configuration object for Mem0, initialize a Memory instance with the config, and add text to the memory. It includes setting an environment variable for the OpenAI API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/config.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"your_chosen_provider\",\n        \"config\": {\n            # Provider-specific settings go here\n        }\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Your text here\", user_id=\"user\", metadata={\"category\": \"example\"})\n```\n\n----------------------------------------\n\nTITLE: Weaviate Vector Database YAML Configuration\nDESCRIPTION: YAML configuration file that specifies Weaviate as the vector database provider and sets the collection name for storing vectors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/weaviate.mdx#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: weaviate\n  config:\n    collection_name: my_weaviate_index\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App in Python\nDESCRIPTION: Demonstrates how to add a data source to the Embedchain app. In this example, a Forbes profile URL is added, which will be processed and embedded by the app.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/chromadb.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Clarifai for Embedchain in Python\nDESCRIPTION: This Python script demonstrates how to set up and use Clarifai with Embedchain, including environment variable setup, configuration loading, and basic usage examples.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"CLARIFAI_PAT\"] = \"XXX\"\n\n# load llm and embedder configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n\n#Now let's add some data.\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n#Query the app\nresponse = app.query(\"what college degrees does elon musk have?\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Embedding Model in Python and YAML\nDESCRIPTION: This example shows how to configure and use an Ollama embedding model locally in Embedchain, combining Python code for loading the configuration and YAML for specifying the model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: ollama\n  config:\n    model: 'all-minilm:latest'\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Configures necessary API keys for OpenAI and Together platforms by setting environment variables\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/together.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"\"\nos.environ[\"TOGETHER_API_KEY\"] = \"\"\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Streamlit RAG Application\nDESCRIPTION: Shows the directory structure created by the template, including app.py, configuration files, and dependency specifications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/streamlit_io.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n├── .streamlit\n│   └── secrets.toml\n├── app.py\n├── embedchain.json\n└── requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 (Basic) - TypeScript\nDESCRIPTION: Demonstrates a basic setup for initializing the Mem0 Memory class in TypeScript. Only the default configuration is used; no custom options are passed. Requires importing Memory from 'mem0ai/oss' and is suitable for development or experimentation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst memory = new Memory();\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 Vercel AI Provider via npm\nDESCRIPTION: Command to install the Mem0 AI SDK Provider package using npm. This is the first step to integrate Mem0's memory capabilities with the Vercel AI SDK.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mem0/vercel-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Running Streamlit Chat Bot App Locally\nDESCRIPTION: Command to execute a Streamlit application for a chat bot that uses Embedchain and Mistral AI. The command launches the app.py file using Streamlit's run command.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/mistral-streamlit/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nstreamlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Adding a Local Text Document via Base64 Encoding to Mem0 using Python\nDESCRIPTION: This Python snippet explains how to add a local text document (TXT or MDX) to Mem0 using Base64 encoding. It defines a helper function `file_to_base64` to read and encode the file, calls this function with the document path, creates a message dictionary with `type` `mdx_url` and the Base64 string as the `url`, and then uses `client.add`. Requires the `base64` module and assumes `client` is initialized.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Path to the document file\ndocument_path = \"path/to/your/document.txt\"\n\n# Function to convert file to Base64\ndef file_to_base64(file_path):\n    with open(file_path, \"rb\") as file:\n        return base64.b64encode(file.read()).decode('utf-8')\n\n# Encode the document in Base64\nbase64_document = file_to_base64(document_path)\n\n# Create the message dictionary with the Base64-encoded document\ndocument_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"mdx_url\",\n        \"mdx_url\": {\n            \"url\": base64_document\n        }\n    }\n}\nclient.add([document_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Fact Extraction Prompt in TypeScript\nDESCRIPTION: This snippet shows how to define a custom fact extraction prompt in TypeScript. The prompt includes instructions and few-shot examples for extracting customer support information, order details, and user information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst customPrompt = `\nPlease only extract entities containing customer support information, order details, and user information. \nHere are some few shot examples:\n\nInput: Hi.\nOutput: {\"facts\" : []}\n\nInput: The weather is nice today.\nOutput: {\"facts\" : []}\n\nInput: My order #12345 hasn't arrived yet.\nOutput: {\"facts\" : [\"Order #12345 not received\"]}\n\nInput: I am John Doe, and I would like to return the shoes I bought last week.\nOutput: {\"facts\" : [\"Customer name: John Doe\", \"Wants to return shoes\", \"Purchase made last week\"]}\n\nInput: I ordered a red shirt, size medium, but received a blue one instead.\nOutput: {\"facts\" : [\"Ordered red shirt, size medium\", \"Received blue shirt instead\"]}\n\nReturn the facts and customer information in a json format as shown above.\n`;\n```\n\n----------------------------------------\n\nTITLE: Querying the Slack Bot\nDESCRIPTION: Command format for asking questions to the Slack Bot in a channel.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack_bot.mdx#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nquery <question>\n```\n\n----------------------------------------\n\nTITLE: Setting Hugging Face Hub Environment Variables\nDESCRIPTION: Sets up the necessary environment variables for Hugging Face Hub authentication. Requires an access token from the Hugging Face Hub dashboard.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/hugging_face_hub.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = \"hf_xxx\"\n```\n\n----------------------------------------\n\nTITLE: Persisting Data Across Multiple App Sessions in Embedchain\nDESCRIPTION: This example demonstrates how to persist data across multiple app sessions in Embedchain by setting an app ID in the configuration. It includes two separate scripts showing how data is saved and then loaded in a subsequent session.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\napp1 = App.from_config(config={\n  \"app\": {\n    \"config\": {\n      \"id\": \"your-app-id\",\n    }\n  }\n})\n\napp1.add(\"https://www.forbes.com/profile/elon-musk\")\n\nresponse = app1.query(\"What is the net worth of Elon Musk?\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\napp2 = App.from_config(config={\n  \"app\": {\n    \"config\": {\n      # this will persist and load data from app1 session\n      \"id\": \"your-app-id\",\n    }\n  }\n})\n\nresponse = app2.query(\"What is the net worth of Elon Musk?\")\n```\n\n----------------------------------------\n\nTITLE: Registering Memory Functions as Client Tools in ElevenLabs Agent (Python)\nDESCRIPTION: Associates the custom asynchronous memory functions with tool names 'addMemories' and 'retrieveMemories' in ElevenLabs' ClientTools system. This enables the conversational agent to call these functions dynamically during conversations, supporting asynchronous memory operations via tool invocation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    # Register the memory functions as tools for the agent\n    client_tools.register(\"addMemories\", add_memories, is_async=True)\n    client_tools.register(\"retrieveMemories\", retrieve_memories, is_async=True)\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with Go\nDESCRIPTION: POST request to create a new Embedchain application with a specified app ID using Go's http package.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"net/http\"\n  \"io/ioutil\"\n)\n\nfunc main() {\n\n  url := \"http://localhost:8080/create?app_id=my-app\"\n\n  payload := strings.NewReader(\"\")\n\n  req, _ := http.NewRequest(\"POST\", url, payload)\n\n  req.Header.Add(\"Content-Type\", \"application/json\")\n\n  res, _ := http.DefaultClient.Do(req)\n\n  defer res.Body.Close()\n  body, _ := ioutil.ReadAll(res.Body)\n\n  fmt.Println(res)\n  fmt.Println(string(body))\n\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 with Graph Support (Python and TypeScript)\nDESCRIPTION: These commands install the Mem0 library and its graph memory dependencies. The Python command uses pip to install the 'graph' extra, while the TypeScript command uses npm for installation. Both commands must be run from a terminal before using Mem0's graph features. No arguments other than library names are required. Network access is required to fetch dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \\\"mem0ai[graph]\\\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Setting up Anthropic LLM with Embedchain\nDESCRIPTION: Example showing how to configure Anthropic (Claude) as the LLM provider with Embedchain. This requires setting the ANTHROPIC_API_KEY environment variable and using a YAML config file for model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"xxx\"\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: anthropic\n  config:\n    model: 'claude-instant-1'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Running the Mem0 LiveKit Voice Agent\nDESCRIPTION: Command to start the LiveKit voice agent in standard mode. This launches the script that allows interaction through LiveKit's Agent Platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython mem0-livekit-voice-agent.py start\n```\n\n----------------------------------------\n\nTITLE: Starting and Managing the ElevenLabs-Agent Conversation Session (Python)\nDESCRIPTION: Begins the conversational session, provides user feedback, configures Ctrl+C signal for graceful shutdown, waits for session completion, and outputs the resulting conversation ID. This main loop is necessary to support live, user-in-the-loop voice and memory-augmented interaction, ensuring proper session lifecycle management.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    # Start the conversation\n    print(f\"Starting conversation with user_id: {USER_ID}\")\n    conversation.start_session()\n\n    # Handle Ctrl+C to gracefully end the session\n    signal.signal(signal.SIGINT, lambda sig, frame: conversation.end_session())\n\n    # Wait for the conversation to end and get the conversation ID\n    conversation_id = conversation.wait_for_session_end()\n    print(f\"Conversation ID: {conversation_id}\")\n\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: Demonstrates how to add a web URL as a data source to the Embedchain app for processing and embedding.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/pinecone.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Single Memory by ID with Mem0 in Python\nDESCRIPTION: Fetches an individual memory by its ID using the get() method. Requires the memory_id and returns detailed memory info, including hash, timestamps, metadata, and user association. Useful for targeted data lookup and verification.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Get a single memory by ID\nspecific_memory = m.get(\"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\")\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n  \"memory\": \"User is planning to watch a movie tonight.\",\n  \"hash\": \"1a271c007316c94377175ee80e746a19\",\n  \"created_at\": \"2025-02-27T16:33:20.557Z\",\n  \"updated_at\": None,\n  \"metadata\": {\n    \"category\": \"movie_recommendations\"\n  },\n  \"user_id\": \"alice\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Question-Answer Loop\nDESCRIPTION: Interactive loop for handling user questions and retrieving answers from the configured Embedchain application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/vertex_ai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package in Python\nDESCRIPTION: This snippet shows how to install the Embedchain package using pip. It's the first step in setting up the environment for using Azure OpenAI with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with Pinecone Config\nDESCRIPTION: Python code demonstrating how to initialize an Embedchain App using Pinecone configuration from YAML files.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/pinecone.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# Load pinecone configuration from yaml file\napp = App.from_config(config_path=\"pod_config.yaml\")\n# Or\napp = App.from_config(config_path=\"serverless_config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Running RAG Application Locally\nDESCRIPTION: Commands to install dependencies and run the RAG application in local development mode using Embedchain's dev command.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/render_com.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying Excel Files with Embedchain - Python\nDESCRIPTION: This snippet showcases how to use the Embedchain Python library to import Excel files (either from a remote URL or a local file path) by specifying the `data_type` parameter as `excel_file`. The example requires the `embedchain` library and valid paths or URLs to Excel files. It demonstrates adding data and querying it using the `query()` method. Inputs are the Excel file path or URL; output is the response to the data query. Ensure the Excel file exists at the given path or URL, and the Embedchain library is installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/excel-file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\napp = App()\\napp.add('https://example.com/content/intro.xlsx', data_type=\\\"excel_file\\\")\\n# Or add file using the local file path on your system\\n# app.add('content/intro.xls', data_type=\\\"excel_file\\\")\\n\\napp.query(\\\"Give brief information about data.\\\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Application to Railway\nDESCRIPTION: Command to deploy the application to Railway using the CLI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/railway.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrailway up\n```\n\n----------------------------------------\n\nTITLE: Retrieving Exported Memory Data in Python\nDESCRIPTION: This Python code demonstrates how to retrieve the exported memory data using the Mem0 API client. It includes an example of applying filters to the retrieval request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/memory-export.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Corrected date range (assuming you meant July 10 to July 20)\nfilters = {\n    \"AND\": [\n        {\"created_at\": {\"gte\": \"2024-07-10\", \"lte\": \"2024-07-20\"}},\n        {\"user_id\": \"alex\"}\n    ]\n}\n\nresponse = client.get_memory_export(filters=filters)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Starting Mem0 API Server with Docker Compose (bash)\nDESCRIPTION: Provides the shell commands to navigate into the `server` directory and start the Mem0 API server along with its pre-configured dependencies (Postgres, Neo4j) using Docker Compose. The server will be accessible at `http://localhost:8888` and supports auto-reload on code changes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd server\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for ElevenLabs and Mem0 Integration (bash)\nDESCRIPTION: Installs the required Python packages: elevenlabs, mem0, and python-dotenv. Must be run in a terminal/shell before executing the main application code. Ensures that the ElevenLabs and Mem0 APIs as well as environment variable loading are available.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs mem0 python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Path Icon for Dify Integration\nDESCRIPTION: SVG path definition for the Dify icon, using a simplified path element with d-attribute to create a curved shape and currentColor fill to match theme colors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_3\n\nLANGUAGE: SVG\nCODE:\n```\n<path\n  d=\"M40 20 H120 C160 20, 160 180, 120 180 H40 V20\"\n  fill=\"currentColor\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Updating a Mem0 Webhook in Python\nDESCRIPTION: This Python snippet shows how to update an existing Mem0 webhook's configuration. It uses the `MemoryClient`'s `update_webhook` method, identifying the webhook by its `webhook_id`. The `name`, `url`, and `event_types` can be modified. The client must be initialized with a valid API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python Python\n# Update webhook for a specific project\nupdated_webhook = client.update_webhook(\n    name=\"Updated Logger\",\n    url=\"https://your-app.com/new-webhook\",\n    event_types=[\"memory_update\", \"memory_add\"],\n    webhook_id=\"wh_123\"\n)\nprint(updated_webhook)\n```\n```\n\n----------------------------------------\n\nTITLE: Running Memgraph with Docker for Graph Memory\nDESCRIPTION: This command runs Memgraph using Docker, exposing the necessary port and enabling schema info for more performant schema generation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 7687:7687 memgraph/memgraph-mage:latest --schema-info-enabled=True\n```\n\n----------------------------------------\n\nTITLE: Installing the Mastra Mem0 Integration using npm\nDESCRIPTION: Installs the necessary Node.js package `@mastra/mem0` using the npm package manager. This package provides the bindings and classes needed to integrate Mem0 memory functionalities within a Mastra project.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-mastra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mastra/mem0\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories with Graph Memory in JavaScript\nDESCRIPTION: This snippet shows how to retrieve all memories with Graph Memory context using the Mem0 JavaScript client. It demonstrates making an API call to get all memories with Graph Memory enabled.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Get all memories with graph context\nconst memories = await client.getAll({\n  userId: \"joseph\",\n  enableGraph: true,\n  outputFormat: \"v1.1\"\n});\n\nconsole.log(memories);\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedder and Graph Store for Mem0\nDESCRIPTION: This snippet sets up the configuration for the embedder model (OpenAI) and Memgraph as the graph store, including connection details.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\"model\": \"text-embedding-3-large\", \"embedding_dims\": 1536},\n    },\n    \"graph_store\": {\n        \"provider\": \"memgraph\",\n        \"config\": {\n            \"url\": \"bolt://localhost:7687\",\n            \"username\": \"memgraph\",\n            \"password\": \"mem0graph\",\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in .env for Docker Compose (txt)\nDESCRIPTION: Demonstrates how to configure the required `OPENAI_API_KEY` environment variable within a `.env` file located in the `server/` directory when running the Mem0 API server using Docker Compose. This key is essential for the server's functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nOPENAI_API_KEY=your-openai-api-key\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Installing necessary Python packages including mem0ai, pyautogen, and flaml using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade pip\n%pip install mem0ai pyautogen flaml\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Environment Variables in Python\nDESCRIPTION: Sets up the OpenAI API key as an environment variable. This step is necessary for authenticating with OpenAI services. The API key can be found on the OpenAI dashboard.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/chromadb.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Installing Agno and Mem0AI Dependencies (bash)\nDESCRIPTION: Installs the required Python dependencies for Mem0 and Agno using pip. The command makes both packages available for import and integration. Make sure to run this in your Python environment before attempting to execute or develop agent code that relies on these frameworks.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/agno.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install agno-ai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Running Chainlit Application\nDESCRIPTION: Command to launch the Chainlit application locally by executing app.py file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chainlit/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nchainlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying Notion Documents with Embedchain in Python\nDESCRIPTION: This snippet demonstrates how to use the Embedchain Python library to load Notion documents by specifying the 'notion' data type with different formats of Notion page IDs. The user initializes an App object, adds Notion documents via direct ID, partial name and ID, or full URL, and then runs a query on the loaded documents. Dependencies include the Embedchain package with the 'community' extras (`pip install --upgrade embedchain[community]`). Inputs are 32-character Notion page IDs (or URLs ending with them); output is the result of the query on added Notion docs. The key limitation is that Notion page ID format must be strictly followed and manual data_type specification is required.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/notion.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add(\"cfbc134ca6464fc980d0391613959196\", data_type=\"notion\")\napp.add(\"my-page-cfbc134ca6464fc980d0391613959196\", data_type=\"notion\")\napp.add(\"https://www.notion.so/my-page-cfbc134ca6464fc980d0391613959196\", data_type=\"notion\")\n\napp.query(\"Summarize the notion doc\")\n```\n\n----------------------------------------\n\nTITLE: Adding Messages with Context in JavaScript (v1)\nDESCRIPTION: Shows how to add messages with context using version 1 (legacy) in JavaScript. This approach requires manually including previous messages for maintaining context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// First interaction\nconst messages1 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"}\n];\nclient.add(messages1, { user_id: \"alex\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n\n// Second interaction - must include previous messages for context\nconst messages2 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"},\n    {\"role\": \"user\", \"content\": \"I like to eat sushi, and yesterday I went to Sunnyvale to eat sushi with my friends.\"},\n    {\"role\": \"assistant\", \"content\": \"Sushi is really a tasty choice. What did you do this weekend?\"}\n];\nclient.add(messages2, { user_id: \"alex\" })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Azure OpenAI Embeddings with Mem0 in Python\nDESCRIPTION: This snippet demonstrates how to set up environment variables, configure the Mem0 Memory object with Azure OpenAI embeddings, and add messages to memory. It includes setting up the embedder provider and custom configuration options.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/azure_openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"EMBEDDING_AZURE_OPENAI_API_KEY\"] = \"your-api-key\"\nos.environ[\"EMBEDDING_AZURE_DEPLOYMENT\"] = \"your-deployment-name\"\nos.environ[\"EMBEDDING_AZURE_ENDPOINT\"] = \"your-api-base-url\"\nos.environ[\"EMBEDDING_AZURE_API_VERSION\"] = \"version-to-use\"\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"azure_openai\",\n        \"config\": {\n            \"model\": \"text-embedding-3-large\"\n            \"azure_kwargs\": {\n                  \"api_version\": \"\",\n                  \"azure_deployment\": \"\",\n                  \"azure_endpoint\": \"\",\n                  \"api_key\": \"\",\n                  \"default_headers\": {\n                    \"CustomHeader\": \"your-custom-header\",\n                  }\n              }\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Remote ChromaDB Configuration\nDESCRIPTION: YAML configuration for connecting to a remote ChromaDB instance running on a specific host and port. Includes collection name and reset permissions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/chromadb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: chroma\n  config:\n    collection_name: 'my-collection'\n    host: localhost\n    port: 5200\n    allow_reset: true\n```\n\n----------------------------------------\n\nTITLE: Embedchain App Configuration in YAML\nDESCRIPTION: This YAML configuration file demonstrates a full-stack Embedchain application configuration. It sets parameters for the LLM (OpenAI), vector database (Chroma), embedder (OpenAI), chunker, cache, and memory components. The configuration includes API keys, model names, and specific settings for each component, such as the temperature and max_tokens for the LLM.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/advanced/configuration.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napp:\n  config:\n    name: 'full-stack-app'\n\nllm:\n  provider: openai\n  config:\n    model: 'gpt-4o-mini'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n    api_key: sk-xxx\n    model_kwargs:\n      response_format: \n        type: json_object\n    api_version: 2024-02-01\n    http_client_proxies: http://testproxy.mem0.net:8000\n    prompt: |\n      Use the following pieces of context to answer the query at the end.\n      If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n      $context\n\n      Query: $query\n\n      Helpful Answer:\n    system_prompt: |\n      Act as William Shakespeare. Answer the following questions in the style of William Shakespeare.\n\nvectordb:\n  provider: chroma\n  config:\n    collection_name: 'full-stack-app'\n    dir: db\n    allow_reset: true\n\nembedder:\n  provider: openai\n  config:\n    model: 'text-embedding-ada-002'\n    api_key: sk-xxx\n    http_client_proxies: http://testproxy.mem0.net:8000\n\nchunker:\n  chunk_size: 2000\n  chunk_overlap: 100\n  length_function: 'len'\n  min_chunk_size: 0\n\ncache:\n  similarity_evaluation:\n    strategy: distance\n    max_distance: 1.0\n  config:\n    similarity_threshold: 0.8\n    auto_flush: 50\n\nmemory:\n  top_k: 10\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Application with Embedchain for Modal.com\nDESCRIPTION: Installs Embedchain with Modal support, creates a directory for the RAG application, and uses the Embedchain CLI to create an application from the Modal.com template. This will open a browser for Modal.com login and generate the necessary project structure.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/modal_com.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain[modal]\nmkdir my-rag-app\nec create --template=modal.com\n```\n\n----------------------------------------\n\nTITLE: Setting up Railway CLI\nDESCRIPTION: Commands to install Railway CLI, login to Railway account, and link project using npm.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/railway.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g @railway/cli\nrailway login\nrailway link [projectID]\n```\n\n----------------------------------------\n\nTITLE: Initializing a FunctionCallingAgent with Tools and Memory - Python\nDESCRIPTION: Initializes a FunctionCallingAgent using a list of FunctionTool objects, an LLM, and optional memory. 'verbose=True' enables additional logging. The agent responds to a chat query and prints the result. Requires LlamaIndex core agent functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.agent import FunctionCallingAgent\n\nagent = FunctionCallingAgent.from_tools(\n    [call_tool, email_tool],\n    llm=llm,\n    memory=memory_from_client,  # or memory_from_config\n    verbose=True,\n)\n\n# Start the chat\nresponse = agent.chat(\"Hi, My name is Mayank\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with ElasticSearch Configuration\nDESCRIPTION: Initializes an Embedchain app with ElasticSearch as the vector database provider. Configures the collection name, ElasticSearch URL, and API key settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/elasticsearch.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"elasticsearch\",\n    \"config\": {\n        \"collection_name\": \"es-index\",\n        \"es_url\": \"your-elasticsearch-url.com\",\n        \"allow_reset\": True,\n        \"api_key\": \"xxx\"\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 Vercel AI Provider via npm (bash)\nDESCRIPTION: Installs the @mem0/vercel-ai-provider npm package required to use Mem0's memory features with the Vercel AI SDK. Run this command in your project's root directory. No parameters are required; outputs the installed package in node_modules. Requires a Node.js/npm environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/vercel-ai-sdk.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mem0/vercel-ai-provider\n```\n\n----------------------------------------\n\nTITLE: Adding Messages with Context in Python (v1)\nDESCRIPTION: Demonstrates how to add messages with context using version 1 (legacy) in Python. This method requires manually including previous messages for context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/contextual-add.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# First interaction\nmessages1 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"}\n]\nclient.add(messages1, user_id=\"alex\")\n\n# Second interaction - must include previous messages for context\nmessages2 = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex and I live in San Francisco.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! Nice to meet you. San Francisco is a beautiful city.\"},\n    {\"role\": \"user\", \"content\": \"I like to eat sushi, and yesterday I went to Sunnyvale to eat sushi with my friends.\"},\n    {\"role\": \"assistant\", \"content\": \"Sushi is really a tasty choice. What did you do this weekend?\"}\n]\nclient.add(messages2, user_id=\"alex\")\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT4ALL Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with GPT4ALL's embedding model. This snippet demonstrates how to configure both the LLM and embedding model using a YAML configuration file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: gpt4all\n  config:\n    model: 'orca-mini-3b-gguf2-q4_0.gguf'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: gpt4all\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Chainlit and Embedchain\nDESCRIPTION: Installs required packages from requirements.txt file for setting up the Chainlit and Embedchain environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chainlit/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Response after searching memories in Mem0 (JSON)\nDESCRIPTION: This JSON response illustrates the expected output after successfully searching for memories in Mem0. The response contains a list of results, each representing a relevant memory found. Each memory includes an ID, the memory content, user ID, metadata, categories, and creation/update timestamps.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_21\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"7f165f7e-b411-4afe-b7e5-35789b72c4a5\",\n            \"memory\": \"Vegetarian. Allergic to nuts.\",\n            \"user_id\": \"alex\",\n            \"metadata\": {\"food\": \"vegan\"},\n            \"categories\": [\"food_preferences\"],\n            \"immutable\": false,\n            \"expiration_date\": null,\n            \"created_at\": \"2024-07-20T01:30:36.275141-07:00\",\n            \"updated_at\": \"2024-07-20T01:30:36.275172-07:00\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running the RAG Application Locally\nDESCRIPTION: Commands to install dependencies and launch the Streamlit application locally for testing before deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/streamlit_io.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Launching WhatsApp Bot with Docker\nDESCRIPTION: Docker command to run the WhatsApp bot container with OpenAI API key configuration and port mapping.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/whatsapp_bot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name whatsapp-bot -e OPENAI_API_KEY=sk-xxx -p 8000:8000 embedchain/whatsapp-bot\n```\n\n----------------------------------------\n\nTITLE: Deploying Gradio App to Huggingface Spaces\nDESCRIPTION: Command to deploy the Gradio RAG application to Huggingface spaces. This creates a public URL where your application can be accessed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Retrieving Filtered Memories with Python using the Mem0 v2 API\nDESCRIPTION: This code demonstrates how to retrieve memories using the v2 API with complex filters. It applies an AND logical operation to filter memories by user_id ('alex') and a date range (July 2024) using the greater than or equal to (gte) and less than or equal to (lte) operators.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/v2-get-memories.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmemories = m.get_all(\n    filters={\n        \"AND\": [\n            {\n                \"user_id\": \"alex\"\n            },\n            {\n                \"created_at\": {\"gte\": \"2024-07-01\", \"lte\": \"2024-07-31\"}\n            }\n        ]\n    },\n    version=\"v2\"\n)\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n{\n    \"id\":\"f38b689d-6b24-45b7-bced-17fbb4d8bac7\",\n    \"memory\":\"Name: Alex. Vegetarian. Allergic to nuts.\",\n    \"user_id\":\"alex\",\n    \"hash\":\"62bc074f56d1f909f1b4c2b639f56f6a\",\n    \"metadata\":null,\n    \"created_at\":\"2024-07-25T23:57:00.108347-07:00\",\n    \"updated_at\":\"2024-07-25T23:57:00.108367-07:00\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0, MultiOn, and OpenAI Clients in Python\nDESCRIPTION: Initializes and configures the Mem0, MultiOn, and OpenAI clients in Python. Sets environment variables for API keys and prepares memory and browser automation clients for use in later code. Requires 'mem0ai', 'multion', 'openai', and a valid set of API keys for your accounts. Expects the environment to already have the above dependencies installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory, MemoryClient\nfrom multion.client import MultiOn\nfrom openai import OpenAI\n\n# Configuration\nOPENAI_API_KEY = 'sk-xxx'  # Replace with your actual OpenAI API key\nMULTION_API_KEY = 'your-multion-key'  # Replace with your actual MultiOn API key\nMEM0_API_KEY = 'your-mem0-key'  # Replace with your actual Mem0 API key\nUSER_ID = \"your-user-id\"\n\n# Set up OpenAI API key\nos.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\nos.environ['MEM0_API_KEY'] = MEM0_API_KEY\n\n# Initialize Mem0 and MultiOn\nmemory = Memory()  # For local usage\nmemory_client = MemoryClient()  # For API usage\nmultion = MultiOn(api_key=MULTION_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Installing Render CLI on Different Operating Systems\nDESCRIPTION: Commands for installing the Render command-line interface on OSX, Linux, and Windows platforms. Linux installation requires Deno as a prerequisite.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/render_com.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew tap render-oss/render\nbrew install render\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/render-oss/render-cli\ncd render-cli\nmake deps\ndeno task run\ndeno compile\n```\n\nLANGUAGE: bash\nCODE:\n```\nchoco install rendercli\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Environment Variable\nDESCRIPTION: Configuration of OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['OPENAI_API_KEY'] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Installing Youtube Dependencies for Embedchain\nDESCRIPTION: Command to install the necessary packages for working with Youtube videos in Embedchain. The command installs the youtube-specific dependencies using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/youtube-video.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"embedchain[youtube]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Embedding Models in YAML\nDESCRIPTION: These YAML configurations demonstrate how to set up different Cohere embedding models in Embedchain, including model name and vector dimension specifications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-english-v3.0'\n    vector_dimension: 1024\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-multilingual-v3.0'\n    vector_dimension: 1024\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-multilingual-light-v3.0'\n    vector_dimension: 384\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-english-v2.0'\n    vector_dimension: 4096\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-english-light-v2.0'\n    vector_dimension: 1024\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: cohere\n  config:\n    model: 'embed-multilingual-v2.0'\n    vector_dimension: 768\n```\n\n----------------------------------------\n\nTITLE: Running Slack AI with Docker\nDESCRIPTION: Commands to build and start the Slack AI application using Docker. This approach containerizes the application for consistent deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack-AI.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose build\nec start --docker\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App and Adding Beehiiv Source in Python\nDESCRIPTION: This Python code snippet demonstrates initializing an `Embedchain` App, adding a Beehiiv newsletter source using its base URL and specifying `data_type='beehiiv'`, and finally executing a query against the ingested data. It relies on the `embedchain` library. The example shows adding 'https://aibreakfast.beehiiv.com' and querying about OpenAI developer payments.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/beehiiv.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom embedchain import App\n\napp = App()\n\n# source: just add the base url and set the data_type to 'beehiiv'\napp.add('https://aibreakfast.beehiiv.com', data_type='beehiiv')\napp.query(\"How much is OpenAI paying developers?\")\n# Answer: OpenAI is aggressively recruiting Google's top AI researchers with offers ranging between $5 to $10 million annually, primarily in stock options.\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package with Hugging Face Hub Support\nDESCRIPTION: Installation command for the embedchain package with huggingface_hub and opensource extensions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/hugging_face_hub.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[huggingface_hub,opensource]\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: Lists required Python packages and their versions. Includes Flask web framework v2.3.2, Flask-SQLAlchemy ORM extension v3.0.5, and Embedchain v0.0.58.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/full_stack/backend/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\nflask==2.3.2\nflask_sqlalchemy==3.0.5\nembedchain==0.0.58\n```\n\n----------------------------------------\n\nTITLE: Sample Agent-User Troubleshooting Scenario (Markdown)\nDESCRIPTION: This Markdown snippet demonstrates a conversational pattern for troubleshooting in the context of memory retrieval. The agent references past problem reports by querying stored memories and providing relevant context to the user, a pattern recommended for customer support applications where issue history must be recalled.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_17\n\nLANGUAGE: Markdown\nCODE:\n```\nUser: \\\"I'm having that same problem again!\\\"\\nAgent: *retrieves memories* \\\"Is this related to the login issue you reported last week?\\\"\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify on a Custom Port\nDESCRIPTION: Command to start the Mintlify development server on a custom port (3333) instead of the default port 3000. Useful when port 3000 is already in use by another application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev --port 3333\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package in Python\nDESCRIPTION: Installs the Embedchain Python package using pip. This is the first step required before using Embedchain with Azure OpenAI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/azure-openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Cat Face Icon for Pipecat Integration\nDESCRIPTION: SVG path and circle elements to create a cat face icon for the Pipecat integration, with paths and circles defining the face, eyes, mouth, and whiskers.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_7\n\nLANGUAGE: SVG\nCODE:\n```\n<path d=\"M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z\" fill=\"currentColor\"/>\n<circle cx=\"8.5\" cy=\"9\" r=\"1.5\" fill=\"currentColor\"/>\n<circle cx=\"15.5\" cy=\"9\" r=\"1.5\" fill=\"currentColor\"/>\n<path d=\"M12 16c1.66 0 3-1.34 3-3H9c0 1.66 1.34 3 3 3z\" fill=\"currentColor\"/>\n<path d=\"M17.5 12c-.83 0-1.5-.67-1.5-1.5s.67-1.5 1.5-1.5 1.5.67 1.5 1.5-.67 1.5-1.5 1.5z\" fill=\"currentColor\"/>\n<path d=\"M6.5 12c-.83 0-1.5-.67-1.5-1.5S5.67 9 6.5 9s1.5.67 1.5 1.5S7.33 12 6.5 12z\" fill=\"currentColor\"/>\n```\n\n----------------------------------------\n\nTITLE: Example Response: Get All Memories - JSON Output\nDESCRIPTION: Represents an example output from requesting all memories for a user. Each memory object contains memory content, id, user_id, timestamps, and a score value; suitable as the output for direct API, Python, or JavaScript SDK methods.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n[\\n    {\\n        \\\"id\\\": \\\"7f165f7e-b411-4afe-b7e5-35789b72c4a5\\\",\\n        \\\"memory\\\": \\\"Does not like cheese\\\",\\n        \\\"user_id\\\": \\\"alex\\\",\\n        \\\"metadata\\\": null,\\n        \\\"created_at\\\": \\\"2024-07-20T01:30:36.275141-07:00\\\",\\n        \\\"updated_at\\\": \\\"2024-07-20T01:30:36.275172-07:00\\\",\\n        \\\"score\\\": 0.92\\n    },\\n    {\\n        \\\"id\\\": \\\"8f165f7e-b411-4afe-b7e5-35789b72c4b6\\\",\\n        \\\"memory\\\": \\\"Lives in San Francisco\\\",\\n        \\\"user_id\\\": \\\"alex\\\",\\n        \\\"metadata\\\": null,\\n        \\\"created_at\\\": \\\"2024-07-20T01:30:36.275141-07:00\\\",\\n        \\\"updated_at\\\": \\\"2024-07-20T01:30:36.275172-07:00\\\",\\n        \\\"score\\\": 0.85\\n    }\\n]\n```\n\n----------------------------------------\n\nTITLE: Customizing Embedchain Chunker for GitHub Data in Python\nDESCRIPTION: Shows how to override the default data chunking mechanism when adding GitHub data to an `embedchain` app. It involves importing `CommonChunker` and `ChunkerConfig`, defining custom chunking parameters (like `chunk_size`, `chunk_overlap`), creating a `CommonChunker` instance with this configuration, and passing this custom `chunker` to the `app.add` method along with the data source query, data type, and loader. This allows for finer control over how GitHub content is segmented before indexing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/github.mdx#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n```Python\nfrom embedchain.chunkers.common_chunker import CommonChunker\nfrom embedchain.config.add_config import ChunkerConfig\n\ngithub_chunker_config = ChunkerConfig(chunk_size=2000, chunk_overlap=0, length_function=len)\ngithub_chunker = CommonChunker(config=github_chunker_config)\n\napp.add(load_query, data_type=\"github\", loader=loader, chunker=github_chunker)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables in Python\nDESCRIPTION: This Python code sets the required environment variables for LangSmith integration using the os module. It configures the same variables as the bash version: LangChain tracing, endpoint, API key, and project name.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/langsmith.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Setting environment variable for LangChain Tracing V2 integration.\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\n\n# Setting the API endpoint for LangChain.\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n\n# Replace '<your-api-key>' with your LangChain API key.\nos.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'\n\n# Replace '<your-project>' with your LangChain project name.\nos.environ['LANGCHAIN_PROJECT'] = '<your-project>'\n```\n\n----------------------------------------\n\nTITLE: Rendering a Card Group with Guide Links Using React in JSX\nDESCRIPTION: This snippet renders a group of two cards using React's JSX syntax to present links to the Python and Node.js SDK guides. Each card includes an icon, a title, and a navigation link. Dependencies include a React environment and the availability of the CardGroup and Card components, which must be defined elsewhere in the codebase. Inputs are props to the Card and CardGroup components, and the output is a rendered UI element displaying the navigation options to the user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={2}>\n<Card title=\"Python SDK Guide\" icon=\"python\" href=\"/open-source/python-quickstart\">\n  Learn more about Mem0 OSS Python SDK\n</Card>\n<Card title=\"Node.js SDK Guide\" icon=\"node\" href=\"/open-source/node-quickstart\">\n  Learn more about Mem0 OSS Node.js SDK\n</Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Adding a PDF Document via URL to Mem0 using Python\nDESCRIPTION: This Python snippet demonstrates adding an online PDF document to Mem0 by providing its URL. It defines the PDF URL, creates a message dictionary specifying the `type` as `pdf_url` and includes the URL, and then uses `client.add` to ingest the PDF content for a specific user. Assumes `client` is an initialized `MemoryClient` instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define the PDF URL\npdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n\n# Create the message dictionary with the PDF URL\npdf_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"pdf_url\",\n        \"pdf_url\": {\n            \"url\": pdf_url\n        }\n    }\n}\nclient.add([pdf_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dropbox Package with pip (Bash)\nDESCRIPTION: This command uses pip, the Python package installer, to install the official Dropbox SDK library. This library is a required dependency for the Python script that interacts with the Dropbox API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/dropbox.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dropbox\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App\nDESCRIPTION: Demonstrates how to add a web URL as a data source to the Embedchain app for indexing and querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/hugging_face_hub.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for mem0ai/mem0\nDESCRIPTION: A requirements file listing Python packages needed for the project. The dependencies include Streamlit for building web interfaces, Embedchain for embedding-based retrieval, LangChain text splitters for document processing, and a specific SQLite binary implementation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chat-pdf/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstreamlit\nembedchain\nlangchain-text-splitters\npysqlite3-binary\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryClient with Organization and Project IDs in Node.js\nDESCRIPTION: This snippet shows how to initialize the MemoryClient from the mem0ai Node.js package with organization and project IDs. It serves the same purpose as the Python version, enabling multi-org/project support and data isolation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemoryClient } from \"mem0ai\";\nconst client = new MemoryClient({organizationId: \"YOUR_ORG_ID\", projectId: \"YOUR_PROJECT_ID\"});\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Chat Endpoint Path\nDESCRIPTION: OpenAPI specification defining a POST endpoint for chat functionality that requires an application ID parameter in the path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/chat.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: post /{app_id}/chat\n```\n\n----------------------------------------\n\nTITLE: Creating a Gradio RAG Application for Huggingface\nDESCRIPTION: Command to create a new RAG application with the Gradio.app template. This creates the necessary files and structure for a Huggingface-compatible Gradio application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-rag-app\nec create --template=hf/gradio.app # inside my-rag-app directory\n```\n\n----------------------------------------\n\nTITLE: Defining retrieveMemories Tool for ElevenLabs Agent in JSON\nDESCRIPTION: This JSON snippet defines the schema for the retrieveMemories tool, which allows the agent to retrieve relevant information from past conversations. The tool requires a single string parameter named 'message', which is used as the query for searching previously stored memories. This configuration ensures the agent can perform context-based memory retrieval during conversations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_12\n\nLANGUAGE: JSON\nCODE:\n```\n{\\n  \\\"name\\\": \\\"retrieveMemories\\\",\\n  \\\"description\\\": \\\"Retrieves relevant information from past conversations\\\",\\n  \\\"parameters\\\": {\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n      \\\"message\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"The query to search for in past memories\\\"\\n      }\\n    },\\n    \\\"required\\\": [\\\"message\\\"]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain\nDESCRIPTION: Adding a URL data source to the configured Embedchain application for processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/vertex_ai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Output of Memory Creation - JSON\nDESCRIPTION: This JSON snippet shows the expected output when successfully adding a memory to Mem0. It includes the ID of the newly created memory and other related data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5\",\n            \"data\": {\"memory\": \"Travelling to SF\"},\n            \"event\": \"ADD\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Example Application\nDESCRIPTION: Command to execute the example script that demonstrates the memory system's features.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run example\n```\n\n----------------------------------------\n\nTITLE: Example Agent Prompt for Utilizing Memory Tools (Markdown)\nDESCRIPTION: This Markdown-formatted snippet provides a sample agent prompt that guides the ElevenLabs agent on when and how to use the retrieveMemories and addMemories tools during user interactions. It describes the flow for recalling context, storing key details, and integrating retrieved memories into responses. This prompt is intended to be incorporated into the agent configuration to improve memory handling consistency.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_15\n\nLANGUAGE: Markdown\nCODE:\n```\nYou are a helpful voice assistant that remembers past conversations with the user.\\n\\nYou have access to memory tools that allow you to remember important information:\\n- Use retrieveMemories at the beginning of the conversation to recall relevant context from prior conversations\\n- Use addMemories to store new important information such as:\\n  * User preferences\\n  * Personal details the user shares\\n  * Important decisions made\\n  * Tasks or follow-ups promised to the user\\n\\nBefore responding to complex questions, always check for relevant memories first.\\nWhen the user shares important information, make sure to store it for future reference.\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with Pinecone\nDESCRIPTION: Creates an Embedchain app instance with Pinecone configuration including cosine similarity metric, vector dimensions, and collection name.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/pinecone.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"pinecone\",\n    \"config\": {\n        \"metric\": \"cosine\",\n        \"vector_dimension\": 768,\n        \"collection_name\": \"pc-index\"\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Rendering Community Contact CardGroup Using React JSX\nDESCRIPTION: This snippet creates a CardGroup component containing multiple Card elements, each representing a contact method (Slack, Discord, GitHub, Scheduling a call) and styled with appropriate properties. Dependencies include React, and specifically the CardGroup and Card components, which are assumed to be imported or defined elsewhere in the codebase. The code expects each Card to accept props such as title, icon, href, and color for configurable display and links; outputs a styled set of cards for user interaction, and constraints include the requirement that supporting Card and CardGroup components exist with relevant props.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/_snippets/missing-llm-tip.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={2}>\n  <Card title=\"Slack\" icon=\"slack\" href=\"https://embedchain.ai/slack\" color=\"#4A154B\">\n    Let us know on our slack community\n  </Card>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://discord.gg/6PzXDgEjG5\" color=\"#7289DA\">\n    Let us know on discord community\n  </Card>\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/embedchain/embedchain/issues/new?assignees=&labels=&projects=&template=feature_request.yml\" color=\"#181717\">\n  Open an issue on our GitHub\n  </Card>\n  <Card title=\"Schedule a call\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/ec\">\n  Schedule a call with Embedchain founder\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Dependencies for Embedchain\nDESCRIPTION: Installs the Elasticsearch integration for Embedchain using pip. This command upgrades the existing installation if present.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/elasticsearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[elasticsearch]'\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 and Keywords AI Libraries (Bash)\nDESCRIPTION: This command uses pip, the Python package installer, to install the necessary client libraries `mem0` and `keywordsai-sdk`. These libraries are required to interact with the Mem0 and Keywords AI services from a Python application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/keywords.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0 keywordsai-sdk\n```\n\n----------------------------------------\n\nTITLE: Example Educational Memory Use Case (Markdown)\nDESCRIPTION: This Markdown block presents a conversation where an educational AI uses the memory retrieval tool to recall a student\\'s previous progress. The agent uses stored context to suggest a seamless continuation of the lesson, illustrating typical behavior for personalized tutoring scenarios leveraging memory integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/elevenlabs.mdx#2025-04-22_snippet_18\n\nLANGUAGE: Markdown\nCODE:\n```\nUser: \\\"Let's continue our math lesson.\\\"\\nAgent: *retrieves memories* \\\"Last time we were working on quadratic equations. Would you like to continue with that?\\\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain JSON for Gradio Deployment\nDESCRIPTION: JSON configuration for the Embedchain app using Gradio, specifying the name and provider. This configuration file tells Embedchain how to deploy the Gradio application to Huggingface spaces.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-rag-app\",\n    \"provider\": \"hf/gradio.app\"\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Memory Extraction in Mem0\nDESCRIPTION: Examples demonstrating when Mem0 extracts memories from input text. It shows that definitional questions don't generate memories, while personal experiences do.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/faqs.mdx#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nInput: \"What is machine learning?\"\nNo memories extracted - Content is definitional and does not meet memory classification criteria.\n\nInput: \"Yesterday I learned about machine learning in class\"\nMemory extracted - Contains personal experience and temporal context.\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies in requirements.txt\nDESCRIPTION: This requirements.txt file lists the necessary Python packages with version constraints. It includes FastAPI 0.104.0 for building APIs, Uvicorn 0.23.2 as the ASGI server, Embedchain for vector embeddings, and BeautifulSoup4 for HTML parsing functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/embedchain/deployment/render.com/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain\nbeautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Example Mem0 Get Webhooks API Response\nDESCRIPTION: This JSON structure represents the response from the Mem0 API when retrieving webhooks. It returns an array of webhook objects, each containing details like `webhook_id`, `url`, `name`, `owner`, subscribed `event_types`, associated `project`, activation status `is_active`, and timestamps.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n```json Output\n[\n    {\n        \"webhook_id\": \"wh_123\",\n        \"url\": \"https://mem0.ai\",\n        \"name\": \"mem0\",\n        \"owner\": \"john\",\n        \"event_types\": [\"memory_add\"],\n        \"project\": \"default-project\",\n        \"is_active\": true,\n        \"created_at\": \"2025-02-18T22:59:56.804993-08:00\",\n        \"updated_at\": \"2025-02-18T23:06:41.479361-08:00\"\n    }\n]\n\n```\n```\n\n----------------------------------------\n\nTITLE: Apps GET Response (JSON)\nDESCRIPTION: Sample JSON response showing an array of application objects, each containing config filename, ID and app_id.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/get-all-apps.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"config\": \"config1.yaml\",\n      \"id\": 1,\n      \"app_id\": \"app1\"\n    },\n    {\n      \"config\": \"config2.yaml\",\n      \"id\": 2,\n      \"app_id\": \"app2\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Embedchain App in Development Mode\nDESCRIPTION: Command to run the Embedchain application in development mode using the Embedchain CLI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nec dev\n```\n\n----------------------------------------\n\nTITLE: Adding Image Data Source to Embedchain App (Python)\nDESCRIPTION: Demonstrates the basic process of adding an image file (local or hosted, here './Elon-Musk.webp') as a data source to an `embedchain` App. It requires setting the `OPENAI_API_KEY` environment variable, initializes the `App`, adds the image specifying `data_type='image'`, and performs a query based on the image content processed by the default GPT-4 Vision configuration. The output is the textual description generated from the image.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/image.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\napp = App()\napp.add(\"./Elon-Musk.webp\", data_type=\"image\")\nresponse = app.query(\"Describe the man in the image.\")\nprint(response)\n# Answer: The man in the image is dressed in formal attire, wearing a dark suit jacket and a white collared shirt. He has short hair and is standing. He appears to be gazing off to the side with a reflective expression. The background is dark with faint, warm-toned vertical lines, possibly from a lit environment behind the individual or reflections. The overall atmosphere is somewhat moody and introspective.\n```\n\n----------------------------------------\n\nTITLE: Deploying Streamlit App to Huggingface Spaces\nDESCRIPTION: Command to deploy the Streamlit RAG application to Huggingface spaces. This creates a public URL where your application can be accessed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Expected JSON Output for Add Memory Tool Execution\nDESCRIPTION: Shows the expected JSON output structure after successfully invoking the `add_tool`. The output contains a list of results, each indicating the memory added and the 'ADD' event.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langchain-tools.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"memory\": \"Name is Alex\",\n      \"event\": \"ADD\"\n    },\n    {\n      \"memory\": \"Is a vegetarian\", \n      \"event\": \"ADD\"\n    },\n    {\n      \"memory\": \"Is allergic to nuts\",\n      \"event\": \"ADD\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package\nDESCRIPTION: Installs the embedchain package with together integration support using pip\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/together.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[together]\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with LangChain OpenAI Model in Python\nDESCRIPTION: This snippet demonstrates how to set up a Mem0 Memory instance using LangChain's ChatOpenAI model as the provider. It shows environment variable configuration, model initialization with parameters, and adding conversation messages to memory with user ID and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/langchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\nfrom langchain_openai import ChatOpenAI\n\n# Set necessary environment variables for your chosen LangChain provider\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Initialize a LangChain model directly\nopenai_model = ChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0.2,\n    max_tokens=2000\n)\n\n# Pass the initialized model to the config\nconfig = {\n    \"llm\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": openai_model\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App\nDESCRIPTION: This code snippet shows how to add a data source to the Embedchain app. The example uses a URL to Elon Musk's Forbes profile, which will be processed and stored in the app's database for querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/ollama.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Running Streamlit App Locally for Testing\nDESCRIPTION: Commands to install dependencies and run the Streamlit application locally for testing before deployment to Huggingface spaces.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Agents SDK with Voice Dependencies\nDESCRIPTION: Command to install OpenAI Agents SDK with voice-specific dependencies using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'openai-agents[voice]'\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with LM Studio Embedding Models\nDESCRIPTION: This code demonstrates how to configure and initialize Mem0 with a LM Studio embedding model for local operation. It sets up the environment, configures the embedder to use the nomic-embed-text model, and adds conversation messages to memory for a user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/lmstudio.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"lmstudio\",\n        \"config\": {\n            \"model\": \"nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package\nDESCRIPTION: Installs the embedchain package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Interactive Query Loop\nDESCRIPTION: Implements an interactive question-answering loop that processes user input and generates responses until exit command is received\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/together.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Pinecone Pod-based Configuration\nDESCRIPTION: YAML configuration for pod-based Pinecone setup, including vector dimensions, index settings, and metadata configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/pinecone.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: pinecone\n  config:\n    metric: cosine\n    vector_dimension: 1536\n    index_name: my-pinecone-index\n    pod_config:\n      environment: gcp-starter\n      metadata_config:\n        indexed:\n          - \"url\"\n          - \"hash\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT4ALL\nDESCRIPTION: This bash command installs the necessary dependencies for using GPT4ALL with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[opensource]'\n```\n\n----------------------------------------\n\nTITLE: Required Slack Bot Permissions\nDESCRIPTION: List of required OAuth scopes for the Slack bot to properly function and interact with the Slack workspace.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\napp_mentions:read\nchannels:history\nchannels:read\nchat:write\nemoji:read\nreactions:write\nreactions:read\n```\n\n----------------------------------------\n\nTITLE: Querying the Bot\nDESCRIPTION: Example format for asking questions to the bot through the Poe interface.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/poe_bot.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n<your-question-here>\n```\n\n----------------------------------------\n\nTITLE: Creating Short-term Memory in Python\nDESCRIPTION: This snippet demonstrates how to create short-term memory for a user session in Python. It utilizes the `run_id` parameter to associate the messages with a specific session, and calls the `add` method to store the provided messages for future contextualization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning a trip to Japan next month.\"},\n    {\"role\": \"assistant\", \"content\": \"That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?\"},\n    {\"role\": \"user\", \"content\": \"Yes, please! Especially in Tokyo.\"},\n    {\"role\": \"assistant\", \"content\": \"Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction.\"}\n]\n\nclient.add(messages, user_id=\"alex\", run_id=\"trip-planning-2024\")\n```\n\n----------------------------------------\n\nTITLE: Resetting Client Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to reset the client, deleting all users and memories asynchronously using the MemoryClient in JavaScript.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.reset();\n```\n\n----------------------------------------\n\nTITLE: Defining Memory Context for Mem0 Integration\nDESCRIPTION: This Python class defines the memory context structure using Pydantic, allowing for user-specific memory operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Mem0Context(BaseModel):\n    user_id: str | None = None\n```\n\n----------------------------------------\n\nTITLE: OpenAI Function Calling with Pydantic Model in Embedchain\nDESCRIPTION: Code snippet showing how to define a Pydantic model for OpenAI function calling. This approach allows defining a structured function with typed parameters for the LLM to interact with.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers together.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify documentation server\nDESCRIPTION: Command to start the Mintlify development server in the docs/ directory. This will make the documentation website available at http://localhost:3000.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/documentation.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Starting Development Mode for Mem0 Extension\nDESCRIPTION: Command to start the development watch mode for real-time code changes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/yt-assistant-chrome/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run watch\n```\n\n----------------------------------------\n\nTITLE: Listing All Memories for a User with AsyncMemory - Python\nDESCRIPTION: This snippet lists all memory entries stored for a given user, agent, or run ID using AsyncMemory's get_all method. Requires at least one filter parameter (e.g., user_id). The function returns an iterable of matching memory objects. Designed for use in asynchronous event loops for fast, non-blocking data retrieval.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.get_all(user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain JSON for Streamlit Deployment\nDESCRIPTION: JSON configuration for the Embedchain app, specifying the name and provider. This configuration file tells Embedchain how to deploy the application to Huggingface spaces.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/huggingface_spaces.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-rag-app\",\n    \"provider\": \"hf/streamlit.io\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding a Text Document (MDX/TXT) via URL to Mem0 using Python\nDESCRIPTION: This Python snippet demonstrates adding an online text document (MDX or TXT) to Mem0 by providing its URL. It defines the document URL, creates a message dictionary specifying the `type` as `mdx_url` and includes the URL, and then uses `client.add` to ingest the document content for a specific user. Assumes `client` is an initialized `MemoryClient` instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the document URL\ndocument_url = \"https://www.w3.org/TR/2003/REC-PNG-20031110/iso_8859-1.txt\"\n\n# Create the message dictionary with the document URL\ndocument_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"mdx_url\",\n        \"mdx_url\": {\n            \"url\": document_url\n        }\n    }\n}\nclient.add([document_message], user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Including Help Information using MDX Snippet\nDESCRIPTION: This MDX snippet includes content from the 'get-help.mdx' file. It's used within the documentation system (likely Mintlify) to dynamically insert reusable content, such as contact information or support links, ensuring consistency across the documentation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/overview.mdx#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n<Snippet file=\"get-help.mdx\"/>\n```\n\n----------------------------------------\n\nTITLE: Embedding Demo Video in HTML\nDESCRIPTION: This HTML snippet embeds a demo video of the YouTube Assistant extension. It uses the <video> tag with autoplay, mute, loop, and inline playback attributes. The video dimensions are set to 700x400 pixels.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/youtube-assistant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<video\n  autoPlay\n  muted\n  loop\n  playsInline\n  width=\"700\"\n  height=\"400\"\n  src=\"https://github.com/user-attachments/assets/c0334ccd-311b-4dd7-8034-ef88204fc751\"\n></video>\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI Globally using npm\nDESCRIPTION: This command uses the Node Package Manager (npm) to install the Mintlify command-line interface (CLI) globally (`-g`). This makes the `mintlify` command available system-wide for previewing documentation changes locally. Requires npm (Node Package Manager) to be installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with Cohere Configuration in Python\nDESCRIPTION: Initializes an Embedchain App with a custom Cohere configuration. This configuration specifies the Cohere model to use and various generation parameters like temperature and token limits.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/cohere.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"cohere\",\n    \"config\": {\n        \"model\": \"gptd-instruct-tft\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 1000,\n        \"top_p\": 1,\n        \"stream\": False\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App from Python Dictionary in Python\nDESCRIPTION: This example demonstrates creating an Embedchain `App` by providing configuration parameters through a Python dictionary. The dictionary specifies the LLM provider ('gpt4all') and its settings (model, temperature, etc.), along with the embedder provider. The `App.from_config()` class method is used with the `config` argument to load these settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nconfig_dict = {\n  'llm': {\n    'provider': 'gpt4all',\n    'config': {\n      'model': 'orca-mini-3b-gguf2-q4_0.gguf',\n      'temperature': 0.5,\n      'max_tokens': 1000,\n      'top_p': 1,\n      'stream': False\n    }\n  },\n  'embedder': {\n    'provider': 'gpt4all'\n  }\n}\n\n# load llm configuration from config dict\napp = App.from_config(config=config_dict)\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Clarifai Package in Python\nDESCRIPTION: This snippet shows how to install the embedchain[clarifai] package using pip. This is the first step in setting up the environment for using Clarifai with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/clarifai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[clarifai]\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA AI Embedding and LLM Models with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with NVIDIA AI's embedding model and LLM. This snippet shows how to set the NVIDIA API key as an environment variable and configure both models using a Python dictionary.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['NVIDIA_API_KEY'] = 'nvapi-xxxx'\n\nconfig = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"my-app\",\n        },\n    },\n    \"llm\": {\n        \"provider\": \"nvidia\",\n        \"config\": {\n            \"model\": \"nemotron_steerlm_8b\",\n        },\n    },\n    \"embedder\": {\n        \"provider\": \"nvidia\",\n        \"config\": {\n            \"model\": \"nvolveqa_40k\",\n            \"vector_dimension\": 1024,\n        },\n    },\n}\n\napp = App.from_config(config=config)\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\nanswer = app.query(\"What is the net worth of Elon Musk today?\")\n# Answer: The net worth of Elon Musk is subject to fluctuations based on the market value of his holdings in various companies.\n# As of March 1, 2024, his net worth is estimated to be approximately $210 billion. However, this figure can change rapidly due to stock market fluctuations and other factors.\n# Additionally, his net worth may include other assets such as real estate and art, which are not reflected in his stock portfolio.\n```\n\n----------------------------------------\n\nTITLE: Setting LLAMA2 Environment Variables\nDESCRIPTION: Sets the necessary environment variables for OpenAI and Replicate API keys. These keys are required for using LLAMA2 with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/llama2.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nos.environ[\"REPLICATE_API_TOKEN\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Helicone with Embedchain in Python\nDESCRIPTION: Example showing how to set up Helicone integration with Embedchain by configuring the OpenAI base URL and API key. The code demonstrates creating an Embedchain App instance, adding data sources, and making queries while routing requests through Helicone for monitoring.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/helicone.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# Modify the base path and add a Helicone URL\nos.environ[\"OPENAI_API_BASE\"] = \"https://oai.helicone.ai/{YOUR_HELICONE_API_KEY}/v1\"\n# Add your OpenAI API Key\nos.environ[\"OPENAI_API_KEY\"] = \"{YOUR_OPENAI_API_KEY}\"\n\napp = App()\n\n# Add data to your app\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\n\n# Query your app\nprint(app.query(\"How many companies did Elon found? Which companies?\"))\n```\n\n----------------------------------------\n\nTITLE: Running Memgraph with Docker (Bash Command)\nDESCRIPTION: This command launches Memgraph with the Mage extension using Docker, mapping the default Bolt port and enabling schema info for better performance. No further parameters are needed; Docker must be installed. The command enables the graph database to be available for Mem0 graph store connections.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 7687:7687 memgraph/memgraph-mage:latest --schema-info-enabled=True\n```\n\n----------------------------------------\n\nTITLE: Loading Local PDF Files with Embedchain in Python\nDESCRIPTION: This snippet demonstrates how to initialize an `embedchain.App` instance and add a PDF file from the local filesystem. It requires the `embedchain` library. The `add` method takes the file path and specifies the data type as 'pdf_file'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/pdf-file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\napp.add('/path/to/file.pdf', data_type='pdf_file')\n```\n\n----------------------------------------\n\nTITLE: OpenAPI GET Endpoint for Organization Members\nDESCRIPTION: API endpoint definition for retrieving organization members. The endpoint requires an org_id parameter in the URL path to identify the organization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/organization/get-org-members.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nget /api/v1/orgs/organizations/{org_id}/members/\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndex ReAct Agent with Tools and Memory\nDESCRIPTION: Creates a FunctionCallingAgent with the defined tools, language model, and Mem0 memory, enabling the agent to perform actions and retain context.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.agent import FunctionCallingAgent\n\nagent = FunctionCallingAgent.from_tools(\n    [call_tool, email_tool, order_food_tool],\n    llm=llm,\n    memory=memory_from_client,  # or memory_from_config\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning Mem0 Repository\nDESCRIPTION: Command to clone the Mem0 repository from GitHub. This is the first step in setting up the demo application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-demo.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/mem0ai/mem0.git\n```\n\n----------------------------------------\n\nTITLE: Updating Custom Categories at Project Level in Python\nDESCRIPTION: This snippet demonstrates how to update custom categories at the project level using the Mem0 API. It sets new categories for lifestyle management, structure seeking, and personal information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient()\n\n# Update custom categories\nnew_categories = [\n    {\"lifestyle_management_concerns\": \"Tracks daily routines, habits, hobbies and interests including cooking, time management and work-life balance\"},\n    {\"seeking_structure\": \"Documents goals around creating routines, schedules, and organized systems in various life areas\"},\n    {\"personal_information\": \"Basic information about the user including name, preferences, and personality traits\"}\n]\n\nresponse = client.update_project(custom_categories = new_categories)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Updating a Memory (Node.js, JavaScript)\nDESCRIPTION: Updates an existing memory using its memory_id, allowing modification to content or associated properties. Used after memories have been added. Requires the memory_id and a properly initialized Memory client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Update a memory\nawait m.update({ memory_id: \"mem456\", text: \"Updated content.\" });\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package in Python\nDESCRIPTION: Installs the Embedchain package using pip. This is the first step in setting up the environment for using ChromaDB with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/chromadb.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Running Redis Stack via Docker (bash)\nDESCRIPTION: This code runs a Redis Stack instance in a Docker container, exposing ports 6379 for Redis and 8001 for the Redis Stack GUI. Docker must be installed on your system. The command starts the container in detached mode and names it \"redis-stack\". This provides a development-ready Redis environment usable by both Python and TypeScript code examples.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/redis.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with OpenAI Embeddings via LangChain\nDESCRIPTION: Shows how to set up Mem0 with OpenAI embeddings through LangChain in Python. Demonstrates environment setup, model initialization, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/langchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\nfrom langchain_openai import OpenAIEmbeddings\n\n# Set necessary environment variables for your chosen LangChain provider\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Initialize a LangChain embeddings model directly\nopenai_embeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    dimensions=1536\n)\n\n# Pass the initialized model to the config\nconfig = {\n    \"embedder\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": openai_embeddings\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Integration Card Component\nDESCRIPTION: JSX Card components that link to different integration pages. Each card includes a title, SVG icon, and descriptive text about the integration's functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Card\n  title=\"CrewAI\"\n  icon={<svg>...</svg>}\n  href=\"/integrations/crewai\"\n>\n  Develop collaborative AI agents with shared memory using CrewAI and Mem0.\n</Card>\n```\n\n----------------------------------------\n\nTITLE: Deploying to Embedchain Platform with JavaScript\nDESCRIPTION: POST request to deploy an Embedchain application to the Embedchain Platform using JavaScript fetch API. This requires an API key for authentication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = fetch(\"http://localhost:8080/my-app/deploy\", {\n  method: \"POST\",\n  body: \"api_key=ec-xxxx\",\n}).then((res) => res.json());\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Storing Pre-defined Memories using Direct Import in Python\nDESCRIPTION: This snippet demonstrates how to use the Direct Import feature to store pre-defined memories for a user. It sets the 'infer' parameter to False in the 'add' method to bypass the memory deduction phase.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/direct-import.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"Alice loves playing badminton\"},\n    {\"role\": \"assistant\", \"content\": \"That's great! Alice is a fitness freak\"},\n    {\"role\": \"user\", \"content\": \"Alice mostly cook at home because of gym plan\"},\n]\n\n\nclient.add(messages, user_id=\"alice\", infer=False)\n```\n\n----------------------------------------\n\nTITLE: Example Response: Adding Memories - JSON Output\nDESCRIPTION: Shows a typical JSON response from the Mem0 API after adding memories. Each object contains a generated memory id, the extracted summary memory, and an event type, usually 'ADD'. The semantic conversion of chat messages into persistent memories is shown. The results reflect successful processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[\\n  {\\n    \\\"id\\\": \\\"24e466b5-e1c6-4bde-8a92-f09a327ffa60\\\",\\n    \\\"memory\\\": \\\"Does not like cheese\\\",\\n    \\\"event\\\": \\\"ADD\\\"\\n  },\\n  {\\n    \\\"id\\\": \\\"e8d78459-fadd-4c5a-bece-abb8c3dc7ed7\\\",\\n    \\\"memory\\\": \\\"Lives in San Francisco\\\",\\n    \\\"event\\\": \\\"ADD\\\"\\n  }\\n]\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a Persistent Embedchain Vector Database (Python)\nDESCRIPTION: This snippet demonstrates initializing an Embedchain application (`App`) with a specific configuration ID (`app-1`). This setup implicitly creates a persistent vector database in the './db' directory. It then adds data sources (a YouTube video URL and a PDF URL) to the application, which are processed and stored in the vector database. Requires the `embedchain` library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/data-type-handling.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nconfig = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"app-1\"\n        }\n    }\n}\nnaval_chat_bot = App.from_config(config=config)\nnaval_chat_bot.add(\"https://www.youtube.com/watch?v=3qHkcs3kG44\")\nnaval_chat_bot.add(\"https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Memory with Litellm Configuration and Adding Conversation History\nDESCRIPTION: This code demonstrates how to initialize a Memory object from Mem0 using Litellm as the LLM provider. It configures the memory with GPT-4o-mini model settings and adds a conversation about movie preferences to the memory store with a user ID and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/litellm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"litellm\",\n        \"config\": {\n            \"model\": \"gpt-4o-mini\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Ping Response Format\nDESCRIPTION: The expected JSON response from the ping endpoint showing a simple ping-pong response format.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/check-status.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"ping\": \"pong\" }\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Mem0 and OpenAI Agents Integration\nDESCRIPTION: This Python code imports the necessary modules and classes for the Mem0 and OpenAI Agents integration, including error handling for the Mem0 import.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\nimport os\nimport asyncio\nfrom pydantic import BaseModel\ntry:\n    from mem0 import AsyncMemoryClient\nexcept ImportError:\n    raise ImportError(\"mem0 is not installed. Please install it using 'pip install mem0ai'.\")\nfrom agents import (\n    Agent,\n    ItemHelpers,\n    MessageOutputItem,\n    RunContextWrapper,\n    Runner,\n    ToolCallItem,\n    ToolCallOutputItem,\n    TResponseInputItem,\n    function_tool,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with Ollama for Local Inference\nDESCRIPTION: This code snippet demonstrates how to initialize an Embedchain App with Ollama as the LLM provider and HuggingFace as the embedder provider. It configures the llama2 model with specific parameters for temperature, top_p, and streaming capability.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/ollama.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"ollama\",\n        \"config\": {\n            \"model\": \"llama2\",\n            \"temperature\": 0.5,\n            \"top_p\": 1,\n            \"stream\": True\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"huggingface\",\n        \"config\": {\n            \"model\": \"BAAI/bge-small-en-v1.5\"\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Cohere and OpenAI in Python\nDESCRIPTION: Configures environment variables for both Cohere and OpenAI API keys. These keys are required for authentication with their respective services when using Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/cohere.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nos.environ[\"COHERE_API_KEY\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Rectangles Icon for ElevenLabs Integration\nDESCRIPTION: SVG rectangle elements to create a simplified icon for ElevenLabs integration, consisting of a white background and two black rectangular bars.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_6\n\nLANGUAGE: SVG\nCODE:\n```\n<rect width=\"24\" height=\"24\" fill=\"white\"/>\n<rect x=\"8\" y=\"4\" width=\"2\" height=\"16\" fill=\"black\"/>\n<rect x=\"14\" y=\"4\" width=\"2\" height=\"16\" fill=\"black\"/>\n```\n\n----------------------------------------\n\nTITLE: Creating Short-term Memory with cURL\nDESCRIPTION: This cURL command sends a request to the Mem0 API to create short-term memory for a user session. It includes the messages, user ID, and run ID in the request body and sets the Authorization header with the API key. The `run_id` parameter associates the memory with a specific user session.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_10\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [\n             {\"role\": \"user\", \"content\": \"I'm planning a trip to Japan next month.\"},\n             {\"role\": \"assistant\", \"content\": \"That's exciting, Alex! A trip to Japan next month sounds wonderful. Would you like some recommendations for vegetarian-friendly restaurants in Japan?\"},\n             {\"role\": \"user\", \"content\": \"Yes, please! Especially in Tokyo.\"},\n             {\"role\": \"assistant\", \"content\": \"Great! I'll remember that you're interested in vegetarian restaurants in Tokyo for your upcoming trip. I'll prepare a list for you in our next interaction.\"}\n         ],\n         \"user_id\": \"alex\",\n         \"run_id\": \"trip-planning-2024\"\n     }'\n```\n\n----------------------------------------\n\nTITLE: Adding and Querying MDX Files Using Embedchain in Python\nDESCRIPTION: This code snippet demonstrates initializing an Embedchain App instance, adding a locally stored MDX file using the data_type parameter set to 'mdx', and querying indexed content. Dependencies include the Embedchain Python library. The 'add' method expects a valid file path to a local MDX file, and 'query' takes a string question to retrieve relevant information. The main limitation is that only MDX files present on disk are supported, with file-path-based ingestion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/mdx.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\napp.add('path/to/file.mdx', data_type='mdx')\n\napp.query(\"What are the docs about?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App for Question Answering Pipeline\nDESCRIPTION: This snippet creates an instance of the Embedchain App, which serves as the foundation for the RAG pipeline. It initializes the application for further configuration and data ingestion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/use-cases/question-answering.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\napp = App()\n```\n\n----------------------------------------\n\nTITLE: Editing Documents with Mem0 in Python\nDESCRIPTION: This function edits a document using Mem0-based stored preferences. It retrieves the stored preferences, extracts them, and applies them to refine the original document content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/document-writing.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef edit_document_based_on_preferences(original_content):\n    \"\"\"Edit a document using Mem0-based stored preferences.\"\"\"\n    \n    # Retrieve stored preferences\n    query = \"What are my writing style preferences?\"\n    preferences_results = client.search(query, user_id=USER_ID, run_id=RUN_ID)\n    \n    if not preferences_results:\n        print(\"No writing preferences found.\")\n        return None\n    \n    # Extract preferences\n    preferences = ' '.join(memory[\"memory\"] for memory in preferences_results)\n    \n    # Apply stored preferences to refine the document\n    edited_content = f\"Applying stored preferences:\\n{preferences}\\n\\nEdited Document:\\n{original_content}\"\n    \n    return edited_content\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify Development Server\nDESCRIPTION: Command to start the Mintlify development server locally from the documentation root directory where mint.json is located.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Customizing Data Loading and Chunking with Embedchain in Python\nDESCRIPTION: This code initializes an Embedchain App instance and demonstrates how to use custom loader and chunker classes for data ingestion. The custom loader and chunker must inherit from BaseLoader and BaseChunker, respectively, ensuring compatibility with Embedchain's data management workflow. Dependencies include the embedchain package and user-defined modules providing the custom classes. The key input is a data source string and the code expects the supplied classes to properly handle data reading and partitioning.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/custom.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\nimport your_loader\\nfrom my_module import CustomLoader\\nfrom my_module import CustomChunker\\n\\napp = App()\\nloader = CustomLoader()\\nchunker = CustomChunker()\\n\\napp.add(\"source\", data_type=\"custom\", loader=loader, chunker=chunker)\n```\n\n----------------------------------------\n\nTITLE: Creating and Sending Image URL Messages to Mem0 (Python)\nDESCRIPTION: Shows how to compose a message with a direct image URL for ingestion. Constructs a message dictionary with the appropriate 'type' and 'image_url' fields, which can then be incorporated into the Mem0 ingestion workflow. Dependencies: string URL value and Python dictionary syntax. Suitable for online images.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/multimodal-support.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the image URL\nimage_url = \"https://www.superhealthykids.com/wp-content/uploads/2021/10/best-veggie-pizza-featured-image-square-2.jpg\"\n\n# Create the message dictionary with the image URL\nimage_message = {\n    \"role\": \"user\",\n    \"content\": {\n        \"type\": \"image_url\",\n        \"image_url\": {\n            \"url\": image_url\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating Memory-Enabled UserProxyAgent\nDESCRIPTION: Creation and configuration of the custom memory-enabled UserProxyAgent instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmem0_user_proxy = Mem0ProxyCoderAgent(\n    name=AGENT_ID,\n    code_execution_config={\n        \"work_dir\": \"coding\",\n        \"use_docker\": False,\n    },\n    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=1,\n)\ncode_result = mem0_user_proxy.initiate_chat(gpt_assistant, message=user_query)\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables in Bash\nDESCRIPTION: These bash commands set the required environment variables for integrating with LangSmith. This includes enabling LangChain tracing, setting the LangChain endpoint, API key, and project name.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/langsmith.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Setting environment variable for LangChain Tracing V2 integration.\nexport LANGCHAIN_TRACING_V2=true\n\n# Setting the API endpoint for LangChain.\nexport LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n\n# Replace '<your-api-key>' with your LangChain API key.\nexport LANGCHAIN_API_KEY=<your-api-key>\n\n# Replace '<your-project>' with your LangChain project name, or it defaults to \"default\".\nexport LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to \"default\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Graph Memory with Neo4j Backend in Python\nDESCRIPTION: Configures Mem0 Memory instance to use Neo4j as the underlying graph store, passing full connection details as a config dictionary. Credentials and URLs must be provided. Useful for graph-based memory representations. Requires Neo4j server and compatible credentials.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"graph_store\": {\n        \"provider\": \"neo4j\",\n        \"config\": {\n            \"url\": \"neo4j+s://---\",\n            \"username\": \"neo4j\",\n            \"password\": \"---\"\n        }\n    }\n}\n\nm = Memory.from_config(config_dict=config)\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Custom Prompt in Python\nDESCRIPTION: This example demonstrates adding a memory using the custom fact extraction prompt in Python. It shows how the prompt extracts relevant information from the user's message about ordering a laptop.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"Yesterday, I ordered a laptop, the order id is 12345\", user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory with Supabase History Store - TypeScript\nDESCRIPTION: Configures the Memory class to use Supabase for external persistent history storage, suitable for serverless environments. Requires valid Supabase URL and key in environment variables. Connects the SDK to a Supabase table for history tracking.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst memory = new Memory({\n  historyStore: {\n    provider: 'supabase',\n    config: {\n      supabaseUrl: process.env.SUPABASE_URL || '',\n      supabaseKey: process.env.SUPABASE_KEY || '',\n      tableName: 'memory_history',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Applying Filtering in Mem0 Search using Python\nDESCRIPTION: Illustrates how to use the filtering feature in Mem0. This mode narrows down search results by applying specific criteria, enhancing search precision by removing irrelevant memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/advanced-retrieval.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient.search(query, filter_memories=True, user_id='alex')\n```\n\nLANGUAGE: python\nCODE:\n```\n# Search for dietary restrictions with filtering enabled\nquery = \"What are my dietary restrictions?\"\nresults = client.search(query, filter_memories=True, user_id='alex')\n\n# Without filtering, results might include:\n# - \"Vegetarian. Allergic to nuts.\" (directly relevant)\n# - \"I enjoy cooking Italian food on weekends\" (somewhat related to food)\n# - \"Mentioned disliking seafood during restaurant discussion\" (food-related)\n# - \"Prefers to eat dinner at 7pm\" (tangentially food-related)\n\n# With filtering enabled, results would be focused:\n# - \"Vegetarian. Allergic to nuts.\" (directly relevant)\n# - \"Mentioned disliking seafood during restaurant discussion\" (relevant restriction)\n#\n# The filtering process removes memories that are about food preferences\n# but not specifically about dietary restrictions\n```\n\n----------------------------------------\n\nTITLE: Using NVIDIA AI Models with Embedchain\nDESCRIPTION: Example of configuring and using both LLM and embedding models from NVIDIA AI in an Embedchain application. Requires an NVIDIA API key set as an environment variable to access their foundation models.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['NVIDIA_API_KEY'] = 'nvapi-xxxx'\n\nconfig = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"my-app\",\n        },\n    },\n    \"llm\": {\n        \"provider\": \"nvidia\",\n        \"config\": {\n            \"model\": \"nemotron_steerlm_8b\",\n        },\n    },\n    \"embedder\": {\n        \"provider\": \"nvidia\",\n        \"config\": {\n            \"model\": \"nvolveqa_40k\",\n            \"vector_dimension\": 1024,\n        },\n    },\n}\n\napp = App.from_config(config=config)\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\nanswer = app.query(\"What is the net worth of Elon Musk today?\")\n# Answer: The net worth of Elon Musk is subject to fluctuations based on the market value of his holdings in various companies.\n# As of March 1, 2024, his net worth is estimated to be approximately $210 billion. However, this figure can change rapidly due to stock market fluctuations and other factors.\n# Additionally, his net worth may include other assets such as real estate and art, which are not reflected in his stock portfolio.\n```\n\n----------------------------------------\n\nTITLE: Defining Release Notes Title and URL in Markdown\nDESCRIPTION: Markdown frontmatter defining a release notes page with a title using an emoji and a GitHub releases URL reference.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/product/release-notes.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: ' 📜 Release Notes'\nurl: https://github.com/embedchain/embedchain/releases\n---\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from Mem0 `add` Operation\nDESCRIPTION: This JSON object illustrates the typical response structure from the Mem0 `add` operation when processing multimodal messages. It contains an array of results, each representing a memory extracted from the input, along with its event type ('ADD') and unique ID.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/multimodal-support.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"memory\": \"Name is Alice\",\n      \"event\": \"ADD\",\n      \"id\": \"7ae113a3-3cb5-46e9-b6f7-486c36391847\"\n    },\n    {\n      \"memory\": \"Likes large pizza with toppings including cherry tomatoes, black olives, green spinach, yellow bell peppers, diced ham, and sliced mushrooms\",\n      \"event\": \"ADD\",\n      \"id\": \"56545065-7dee-4acf-8bf2-a5b2535aabb3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Memory with Anthropic in Python\nDESCRIPTION: This snippet demonstrates how to set up and use the Memory class with Anthropic's models in Python. It includes setting environment variables, configuring the Memory object, and adding messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/anthropic.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"anthropic\",\n        \"config\": {\n            \"model\": \"claude-3-7-sonnet-latest\",\n            \"temperature\": 0.1,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App for LLAMA2\nDESCRIPTION: Creates an Embedchain app with LLAMA2 configuration. Specifies the LLAMA2 model, temperature, max tokens, and other parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/llama2.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"llama2\",\n    \"config\": {\n        \"model\": \"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n        \"temperature\": 0.5,\n        \"max_tokens\": 1000,\n        \"top_p\": 0.5,\n        \"stream\": False\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Mem0 API Key as Environment Variable\nDESCRIPTION: These snippets show how to set the Mem0 API key as an environment variable, either through the command line or within a Python script.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"MEM0_API_KEY\"] = \"your_mem0_api_key\"\n```\n\n----------------------------------------\n\nTITLE: Adding QnA Pair Using Embedchain in Python\nDESCRIPTION: This snippet illustrates how to instantiate an Embedchain App and add a custom question and answer pair as a tuple with the `data_type` parameter set to `qna_pair`. It requires the `embedchain` library to be installed. The primary parameters are the QnA tuple (e.g., (\"Question\", \"Answer\")) and `data_type`, which must be set to `qna_pair` for this operation. Input is a tuple containing the question and answer strings; output is the QnA pair stored within the application context. This example does not include validation, so malformed tuples may cause errors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/qna.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add((\"Question\", \"Answer\"), data_type=\"qna_pair\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Clarifai LLM Provider\nDESCRIPTION: This YAML configuration demonstrates how to set up Clarifai as the LLM provider for Embedchain. It includes model URLs and parameters for both the LLM and embedder.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n provider: clarifai\n config:\n   model: \"https://clarifai.com/mistralai/completion/models/mistral-7B-Instruct\"\n   model_kwargs:\n     temperature: 0.5\n     max_tokens: 1000  \nembedder:\n provider: clarifai\n config:\n   model: \"https://clarifai.com/clarifai/main/models/BAAI-bge-base-en-v15\"\n```\n\n----------------------------------------\n\nTITLE: Searching Memories with Filters in Mem0 (cURL)\nDESCRIPTION: This cURL command demonstrates how to search memories in Mem0 via an HTTP POST request, filtering by categories and metadata. The request includes an API key in the Authorization header and sends a JSON payload containing the search query, categories, and metadata. Note the `version=v2` parameter in the URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_24\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/search/?version=v2\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type\": \"application/json\" \\\n     -d '{\n         \"query\": \"What do you know about me?\",\n         \"categories\": [\"food_preferences\"],\n         \"metadata\": {\"food\": \"vegan\"}\n     }'\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Single Memory by ID - Result Sample - JSON\nDESCRIPTION: Example output from Memory.get when fetching a specific memory by ID. Includes memory text, hash, timestamps, metadata, and userId. Used for output illustration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n  \"memory\": \"User is planning to watch a movie tonight.\",\n  \"hash\": \"1a271c007316c94377175ee80e746a19\",\n  \"createdAt\": \"2025-02-27T16:33:20.557Z\",\n  \"updatedAt\": undefined,\n  \"metadata\": {\n    \"category\": \"movie_recommendations\"\n  },\n  \"userId\": \"alice\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pinecone and Embedchain\nDESCRIPTION: Installs the necessary Python packages including embedchain, pinecone-client, and pinecone-text using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/pinecone.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain pinecone-client pinecone-text\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Personalized AI Tutor\nDESCRIPTION: This command installs the necessary Python packages (openai and mem0ai) using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-ai-tutor.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain App\nDESCRIPTION: Adds a web URL as a data source to the Embedchain app, which will be processed, embedded, and stored in ElasticSearch.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/elasticsearch.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Giving Feedback on Memories using Mem0 API in Python\nDESCRIPTION: This code snippet demonstrates how to use the Mem0 client in Python to provide feedback on a generated memory. It initializes the MemoryClient with an API key and calls the feedback method with the necessary parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/feedback-mechanism.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mem0 import MemoryClient\n\nclient = MemoryClient(api_key=\"your_api_key\")\n\nclient.feedback(memory_id=\"your-memory-id\", feedback=\"NEGATIVE\", feedback_reason=\"I don't like this memory because it is not relevant.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App using YAML\nDESCRIPTION: This YAML configuration file (`config.yaml`) defines settings for an Embedchain `App`. It specifies the LLM provider ('gpt4all') and its parameters (model, temperature, max_tokens, top_p, stream), as well as the embedder provider ('gpt4all'). This file is intended to be loaded by the `App.from_config(config_path=...)` method in Python.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: gpt4all\n  config:\n    model: 'orca-mini-3b-gguf2-q4_0.gguf'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: gpt4all\n```\n\n----------------------------------------\n\nTITLE: Configuring Mem0 API Credentials in Flowise\nDESCRIPTION: This code block shows the JSON-like structure (represented in TypeScript syntax context) for configuring Mem0 credentials within the Flowise UI. It includes the mandatory 'apiKey' field and optional fields like 'userId', 'projectId', and 'orgId' for associating memories with specific users, projects, or organizations. These credentials authenticate requests to the Mem0 API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/flowise.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  \"apiKey\": \"m0-xxx\",\n  \"userId\": \"user-123\",  // Optional: Specify user ID\n  \"projectId\": \"proj-xxx\",  // Optional: Specify project ID\n  \"orgId\": \"org-xxx\"  // Optional: Specify organization ID\n}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Environment Variables\nDESCRIPTION: Configures OpenAI API key as an environment variable and imports required dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 with OpenAI in TypeScript\nDESCRIPTION: Sets up a Memory instance using OpenAI's LLM models in TypeScript, configuring the API key and model parameters, then adds conversation messages to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'openai',\n    config: {\n      apiKey: process.env.OPENAI_API_KEY || '',\n      model: 'gpt-4-turbo-preview',\n      temperature: 0.2,\n      maxTokens: 1500,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with ElasticSearch Support\nDESCRIPTION: Installs the embedchain package with ElasticSearch integration using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/elasticsearch.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[elasticsearch]\n```\n\n----------------------------------------\n\nTITLE: Creating Basic App with REST API\nDESCRIPTION: Example of creating an app using the REST API endpoint with a basic curl request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/create.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/create?app_id=app1 \\\n  -F \"config=@/path/to/config.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Using Open Source Models in Embedchain\nDESCRIPTION: This example demonstrates how to use open source models in Embedchain for users without OpenAI credits. It configures the app to use GPT4All for both the language model and embedder.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load llm configuration from opensource.yaml file\napp = App.from_config(config_path=\"opensource.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: gpt4all\n  config:\n    model: 'orca-mini-3b-gguf2-q4_0.gguf'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: gpt4all\n  config:\n    model: 'all-MiniLM-L6-v2'\n```\n\n----------------------------------------\n\nTITLE: Configuring JinaChat LLM Provider\nDESCRIPTION: This code snippet and YAML configuration show how to set up JinaChat as the LLM provider for Embedchain. It includes setting the API key and configuring model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"JINACHAT_API_KEY\"] = \"xxx\"\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: jina\n  config:\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Retrieving Added Data Sources in Embedchain (Python)\nDESCRIPTION: This Python snippet demonstrates initializing an `embedchain` App, adding two web page data sources using `app.add()`, and then retrieving the list of all added data sources with `app.get_data_sources()`. The output is a list of dictionaries, each containing details about a data source like its type and value. Requires the `embedchain` library to be installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/get.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom embedchain import App\n\napp = App()\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\n\ndata_sources = app.get_data_sources()\n# [\n#   {\n#       'data_type': 'web_page',\n#       'data_value': 'https://en.wikipedia.org/wiki/Elon_Musk',\n#       'metadata': 'null'\n#   },\n#   {\n#       'data_type': 'web_page',\n#       'data_value': 'https://www.forbes.com/profile/elon-musk',\n#       'metadata': 'null'\n#   }\n# ]\n```\n```\n\n----------------------------------------\n\nTITLE: Customizing Slack Chunking with SlackChunker - Python\nDESCRIPTION: This snippet illustrates how to customize Slack data chunking in Embedchain by providing a bespoke SlackChunker instance and configuration. Dependencies: 'embedchain.chunkers.slack', 'embedchain.config.add_config', a previously set up loader, and valid chunking parameters. It creates a chunker with specific chunk size and overlap, then applies it when adding Slack data. Chunker customization helps control document segmentation for better downstream processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/slack.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.chunkers.slack import SlackChunker\\nfrom embedchain.config.add_config import ChunkerConfig\\n\\nslack_chunker_config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)\\nslack_chunker = SlackChunker(config=slack_chunker_config)\\n\\napp.add(slack_chunker, data_type=\\\"slack\\\", loader=loader, chunker=slack_chunker)\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama2 LLM Provider\nDESCRIPTION: This code snippet and YAML configuration demonstrate how to set up Llama2 as the LLM provider for Embedchain using Replicate. It includes setting the API token and configuring model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"xxx\"\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: llama2\n  config:\n    model: 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 0.5\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for CrewAI and Mem0 (bash)\nDESCRIPTION: Installs the required Python libraries: crewai for agents, crewai-tools for agent tools, and mem0ai for handling persistent memory. These libraries are prerequisites for the subsequent code and must be installed in your Python environment before running any scripts. Run this command in a terminal with pip configured.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/crewai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install crewai crewai-tools mem0ai\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Embedchain\nDESCRIPTION: Adds a web page (Elon Musk's Forbes profile) as a data source to the Embedchain app. The content will be processed, embedded, and stored in the LanceDB collection.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/lancedb.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Deleting a Specific Memory by ID using AsyncMemory - Python\nDESCRIPTION: This snippet deletes a memory record using AsyncMemory's delete method by providing the memory_id. The method requires the memory_id to locate the entry. Intended for use inside async code blocks for non-blocking removal of specific memory records.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.delete(memory_id=\"memory-id-here\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App using JSON\nDESCRIPTION: This JSON configuration file (`config.json`) provides settings for an Embedchain `App`. It defines the LLM provider as 'gpt4all' with specific configuration details (model, temperature, etc.) and sets the embedder provider also to 'gpt4all'. This file can be loaded using `App.from_config(config_path=...)` in Python to configure the app instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/overview.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"llm\": {\n    \"provider\": \"gpt4all\",\n    \"config\": {\n      \"model\": \"orca-mini-3b-gguf2-q4_0.gguf\",\n      \"temperature\": 0.5,\n      \"max_tokens\": 1000,\n      \"top_p\": 1,\n      \"stream\": false\n    }\n  },\n  \"embedder\": {\n    \"provider\": \"gpt4all\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing FAISS Memory Store in Python\nDESCRIPTION: Example demonstrating how to configure and initialize a FAISS vector store in mem0, including setting up OpenAI credentials and adding conversation messages to memory with metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/faiss.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"faiss\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"path\": \"/tmp/faiss_memories\",\n            \"distance_strategy\": \"euclidean\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Integrating Gemini Model with mem0 Memory System\nDESCRIPTION: This code demonstrates how to configure and use Google's Gemini model with the mem0 Memory system. It requires setting both OpenAI API key (for embeddings) and Gemini API key, then configuring the LLM provider with specific parameters before adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/gemini.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"gemini\",\n        \"config\": {\n            \"model\": \"gemini-1.5-flash-latest\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with Hugging Face Models\nDESCRIPTION: Creates an Embedchain app with a custom configuration using Hugging Face for both the language model and embedder. Specifies models and parameters for generation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/hugging_face_hub.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"huggingface\",\n        \"config\": {\n            \"model\": \"google/flan-t5-xxl\",\n            \"temperature\": 0.5,\n            \"max_tokens\": 1000,\n            \"top_p\": 0.8,\n            \"stream\": False\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"huggingface\",\n        \"config\": {\n            \"model\": \"sentence-transformers/all-mpnet-base-v2\"\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Context Relevancy in RAG Applications\nDESCRIPTION: Example of how to use the ContextRelevance metric to evaluate how relevant retrieved contexts are to the user's question, with both basic and advanced configuration options.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.evaluation.metrics import ContextRelevance\nmetric = ContextRelevance()\nscore = metric.evaluate(dataset)\nprint(score)\n# 0.27975528364849833\n```\n\n----------------------------------------\n\nTITLE: Defining GET Endpoint for Retrieving Memory in OpenAPI\nDESCRIPTION: This OpenAPI specification defines the GET endpoint for retrieving a specific memory by its ID. The endpoint is /v1/memories/{memory_id}/, where {memory_id} is a path parameter representing the unique identifier of the memory to be retrieved.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/get-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: openapi\nCODE:\n```\nget /v1/memories/{memory_id}/\n```\n\n----------------------------------------\n\nTITLE: Setting Clarifai PAT as Environment Variable in Python\nDESCRIPTION: This code sets the Clarifai Personal Access Token (PAT) as an environment variable. It also imports the necessary App class from embedchain. The PAT is required for authentication with Clarifai's services.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/clarifai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"CLARIFAI_PAT\"]=\"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Recording Audio from Microphone - Python\nDESCRIPTION: This async function, `record_from_microphone`, records audio from the user's microphone for a given duration and sample rate, storing audio frames into a buffer and combining them into a numpy array. It uses the sounddevice (`sd`) library and numpy, requires the asyncio event loop, and assumes single-channel (mono) int16 audio. Parameters include duration and sample rate. The output is a numpy array containing the recorded audio data for further processing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nasync def record_from_microphone(duration=5, samplerate=24000):\n    \"\"\"Record audio from the microphone for a specified duration.\"\"\"\n    print(f\"Recording for {duration} seconds...\")\n    \n    # Create a buffer to store the recorded audio\n    frames = []\n    \n    # Callback function to store audio data\n    def callback(indata, frames_count, time_info, status):\n        frames.append(indata.copy())\n    \n    # Start recording\n    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback, dtype=np.int16):\n        await asyncio.sleep(duration)\n    \n    # Combine all frames into a single numpy array\n    audio_data = np.concatenate(frames)\n    return audio_data\n\n```\n\n----------------------------------------\n\nTITLE: Adding Slack Data Using Custom SlackLoader - Python\nDESCRIPTION: This example shows how to use a previously configured SlackLoader to add Slack channel data to an Embedchain pipeline. It instantiates the App, loads data from the 'random' channel with the custom loader, and defines a query. It requires prior SlackLoader configuration and proper environment setup. The query demonstrates how to interrogate available workspace bots.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/slack.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom embedchain.pipeline import Pipeline as App\\n\\napp = App()\\n\\napp.add(\\\"in:random\\\", data_type=\\\"slack\\\", loader=loader)\\nquestion = \\\"Which bots are available in the slack workspace's random channel?\\\"\\n# Answer: The available bot in the slack workspace's random channel is the Embedchain bot.\n```\n\n----------------------------------------\n\nTITLE: Implementing Microphone Recording Function\nDESCRIPTION: Asynchronous function to record audio from the microphone for a specified duration using the sounddevice library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def record_from_microphone(duration=5, samplerate=24000):\n    \"\"\"Record audio from the microphone for a specified duration.\"\"\"\n    print(f\"Recording for {duration} seconds...\")\n    \n    # Create a buffer to store the recorded audio\n    frames = []\n    \n    # Callback function to store audio data\n    def callback(indata, frames_count, time_info, status):\n        frames.append(indata.copy())\n    \n    # Start recording\n    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback, dtype=np.int16):\n        await asyncio.sleep(duration)\n    \n    # Combine all frames into a single numpy array\n    audio_data = np.concatenate(frames)\n    return audio_data\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Vector DB with Docker in Bash\nDESCRIPTION: Provides bash commands to pull and run the Qdrant vector database required for Mem0 production use. Mounts local disk for persistent storage. Must be executed prior to initializing Memory with Qdrant as the backend.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull qdrant/qdrant\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    qdrant/qdrant\n```\n\n----------------------------------------\n\nTITLE: Deploying Slack Bot with Python\nDESCRIPTION: Commands to install the Embedchain package with Slack integration and run the Slack Bot using Python.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack_bot.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade \"embedchain[slack]\"\npython3 -m embedchain.bots.slack --port 8000\n```\n\n----------------------------------------\n\nTITLE: Chatting with OpenAI Assistant via Embedchain\nDESCRIPTION: Demonstrates how to interact with the assistant by sending a query and receiving a response. The assistant uses the previously added data to provide relevant answers.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/openai-assistant.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nassistant.chat(\"How much OpenAI credits were offered to attendees during OpenAI DevDay?\")\n# Response: 'Every attendee of OpenAI DevDay 2023 was offered $500 in OpenAI credits.'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Supabase Database for Mem0 (TypeScript Implementation) using SQL\nDESCRIPTION: This SQL script provides the necessary database migrations for setting up Supabase to work with the Mem0 TypeScript implementation. It first ensures the `vector` extension (pgvector) is enabled. Then, it creates the `memories` table with columns for ID, vector embeddings (1536 dimensions), metadata (JSONB), and timestamps. Finally, it defines a PostgreSQL function `match_vectors` to perform vector similarity searches based on cosine distance, allowing filtering by metadata. These migrations should be run in the Supabase SQL Editor.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/supabase.mdx#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n```sql\n-- Enable the vector extension\ncreate extension if not exists vector;\n\n-- Create the memories table\ncreate table if not exists memories (\n  id text primary key,\n  embedding vector(1536),\n  metadata jsonb,\n  created_at timestamp with time zone default timezone('utc', now()),\n  updated_at timestamp with time zone default timezone('utc', now())\n);\n\n-- Create the vector similarity search function\ncreate or replace function match_vectors(\n  query_embedding vector(1536),\n  match_count int,\n  filter jsonb default '{}'::jsonb\n)\nreturns table (\n  id text,\n  similarity float,\n  metadata jsonb\n)\nlanguage plpgsql\nas $$\nbegin\n  return query\n  select\n    t.id::text,\n    1 - (t.embedding <=> query_embedding) as similarity,\n    t.metadata\n  from memories t\n  where case\n    when filter::text = '{}'::text then true\n    else t.metadata @> filter\n  end\n  order by t.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Timestamp - cURL\nDESCRIPTION: This cURL command demonstrates how to add a memory to Mem0 with a custom timestamp using a POST request. It includes the API key in the Authorization header and the memory content, user ID, and timestamp in the JSON payload.  Requires a valid API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_2\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X POST \"https://api.mem0.ai/v1/memories/\" \\\n     -H \"Authorization: Token your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"messages\": [{\"role\": \"user\", \"content\": \"I'm travelling to SF\"}],\n         \"user_id\": \"user1\",\n         \"timestamp\": 1721577600\n     }'\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 SDK (Bash)\nDESCRIPTION: This bash command installs the Mem0 SDK for enabling memory storage and retrieval in the assistant. Mem0 is necessary for storing user interactions and performing memory-aware LLM responses. The command should be run in the Python environment where the project resides. There are no additional parameters beyond package installation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Storage Function Tool\nDESCRIPTION: Defines a function tool for storing user memories using Mem0. It takes a memory string, formats it, and stores it in Mem0 with metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\n# Set up logging at the top of your file\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    force=True\n)\nlogger = logging.getLogger(\"memory_voice_agent\")\n\n# Then use logger in your function tools\n@function_tool\nasync def save_memories(\n    memory: str\n) -> str:\n    \"\"\"Store a user memory in memory.\"\"\"\n    # This will be visible in your console\n    logger.debug(f\"Saving memory: {memory} for user {USER_ID}\")\n    \n    # Store the preference in Mem0\n    memory_content = f\"User memory - {memory}\"\n    await mem0_client.add(\n        memory_content,\n        user_id=USER_ID,\n    )\n\n    return f\"I've saved your memory: {memory}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Context Relevancy Evaluation\nDESCRIPTION: Demonstrates how to customize context relevancy evaluation by specifying model, API key, and language parameters using the ContextRelevanceConfig class.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain.config.evaluation.base import ContextRelevanceConfig\nfrom embedchain.evaluation.metrics import ContextRelevance\n\neval_config = ContextRelevanceConfig(model=\"gpt-4\", api_key=\"sk-xxx\", language=\"en\")\nmetric = ContextRelevance(config=eval_config)\nmetric.evaluate(dataset)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Memories - TypeScript\nDESCRIPTION: Demonstrates how to retrieve all stored memories for a specific user ID using Memory.getAll. The function returns an array of memory objects with full details including creation timestamps, hashes, userId, and metadata. Suitable for listing all data associated with a user.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Get all memories\nconst allMemories = await memory.getAll({ userId: \"alice\" });\nconsole.log(allMemories)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI PUT Endpoint for Updating Memory in YAML\nDESCRIPTION: This YAML snippet defines the OpenAPI specification for the PUT /v1/memories/{memory_id}/ endpoint. It specifies the path, HTTP method, and other metadata for updating an existing memory in the mem0ai system.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/update-memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: 'Update Memory'\nopenapi: put /v1/memories/{memory_id}/\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses with OpenAI Model in Embedchain\nDESCRIPTION: This snippet shows how to enable streaming responses when using an OpenAI model in Embedchain. It includes the configuration setup and a usage example for querying information.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: 'gpt-4o-mini'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: true\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'sk-xxx'\n\napp = App.from_config(config_path=\"openai.yaml\")\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\nresponse = app.query(\"What is the net worth of Elon Musk?\")\n# response will be streamed in stdout as it is generated.\n```\n\n----------------------------------------\n\nTITLE: Setting Python Version for Railway\nDESCRIPTION: Content for .python-version file to specify Python 3.10 as the required version for Railway deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/railway.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n3.10\n```\n\n----------------------------------------\n\nTITLE: Searching for Character Information in Python and TypeScript\nDESCRIPTION: Shows how to search for information about entities other than the user, retrieving the relationships between characters in the memory graph.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nm.search(\"Who is spiderman?\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.search(\"Who is spiderman?\", { userId: \"alice123\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    'memories': [...],\n    'entities': [\n        {'source': 'peter', 'relation': 'identity','destination': 'spiderman'}\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory History Asynchronously in Python\nDESCRIPTION: This code demonstrates how to get the history of a specific memory asynchronously using the AsyncMemoryClient in Python. It takes a memory ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nawait client.history(memory_id=\"memory-id-here\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Streamlit RAG Application from Template\nDESCRIPTION: Commands to create a directory for the RAG application and generate starter files using Embedchain's template for Streamlit.io.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/streamlit_io.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-rag-app\nec create --template=streamlit.io\n```\n\n----------------------------------------\n\nTITLE: Searching Memories - Result Sample - JSON\nDESCRIPTION: Illustrates the output structure for a Memory.search operation. Lists matched memories with relevance scores, hashes, user and category info. Intended for sample reference.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n      \"memory\": \"User is planning to watch a movie tonight.\",\n      \"hash\": \"1a271c007316c94377175ee80e746a19\",\n      \"createdAt\": \"2025-02-27T16:33:20.557Z\",\n      \"updatedAt\": undefined,\n      \"score\": 0.38920719231944799,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    },\n    {\n      \"id\": \"475bde34-21e6-42ab-8bef-0ab84474f156\",\n      \"memory\": \"User loves sci-fi movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"createdAt\": \"2025-02-27T16:33:20.560Z\",\n      \"updatedAt\": undefined,\n      \"score\": 0.36869761478135689,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    },\n    {\n      \"id\": \"cbb1fe73-0bf1-4067-8c1f-63aa53e7b1a4\",\n      \"memory\": \"User is not a big fan of thriller movies.\",\n      \"hash\": \"285d07801ae42054732314853e9eadd7\",\n      \"createdAt\": \"2025-02-27T16:33:20.560Z\",\n      \"updatedAt\": undefined,\n      \"score\": 0.33855272141248272,\n      \"metadata\": {\n        \"category\": \"movie_recommendations\"\n      },\n      \"userId\": \"alice\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Custom Prompt in TypeScript\nDESCRIPTION: This example shows adding a memory using the custom fact extraction prompt in TypeScript. It demonstrates how the prompt extracts relevant information from the user's message about ordering a laptop.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-fact-extraction-prompt.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait memory.add('Yesterday, I ordered a laptop, the order id is 12345', { userId: \"user123\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Memories with Graph Memory in JavaScript\nDESCRIPTION: This snippet shows how to add new memories with Graph Memory enabled using the Mem0 JavaScript client. It includes setting up the client and making an API call with the necessary parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemoryClient } from \"mem0\";\n\nconst client = new MemoryClient({\n  apiKey: \"your-api-key\",\n  orgId: \"your-org-id\",\n  projectId: \"your-project-id\"\n});\n\nconst messages = [\n  { role: \"user\", content: \"My name is Joseph\" },\n  { role: \"assistant\", content: \"Hello Joseph, it's nice to meet you!\" },\n  { role: \"user\", content: \"I'm from Seattle and I work as a software engineer\" }\n];\n\n// Enable graph memory when adding\nawait client.add({\n  messages,\n  userId: \"joseph\",\n  version: \"v1\",\n  enableGraph: true,\n  outputFormat: \"v1.1\"\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Query Loop for Embedchain App\nDESCRIPTION: Creates a continuous loop that accepts user questions, queries the Embedchain app for answers based on the added data sources, and displays the results. The loop exits when the user inputs 'q', 'exit', or 'quit'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/elasticsearch.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Deploying Embedchain App with Fly.io\nDESCRIPTION: Command to launch the Embedchain app on Fly.io without immediate deployment. This prepares the app for deployment configuration while ensuring a unique app name is set in the fly.toml file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfly launch --no-deploy\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Local Pipelines\nDESCRIPTION: This code snippet shows how to use a locally downloaded Hugging Face model as the LLM provider for Embedchain. It includes specifying the local model and its parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nconfig = {\n  \"app\": {\"config\": {\"id\": \"my-app\"}},\n  \"llm\": {\n      \"provider\": \"huggingface\",\n      \"config\": {\n          \"model\": \"Trendyol/Trendyol-LLM-7b-chat-v0.1\",\n          \"local\": True,  # Necessary if you want to run model locally\n          \"top_p\": 0.5,\n          \"max_tokens\": 1000,\n          \"temperature\": 0.1,\n      },\n  }\n}\napp = App.from_config(config=config)\n```\n\n----------------------------------------\n\nTITLE: Creating Long-term Memory in Python\nDESCRIPTION: This snippet demonstrates how to create long-term memory for a user in Python using the Mem0 Platform. It defines a list of messages, then calls the `add` method on the MemoryClient instance, passing in the messages, user ID, and optional metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n]\n\nclient.add(messages, user_id=\"alex\", metadata={\"food\": \"vegan\"})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memories by Categories - Python\nDESCRIPTION: These Python snippets demonstrate how to retrieve memories based on specific categories using the `client.get_all` method. They illustrate retrieving memories with a single category, multiple categories, custom pagination combined with categories and using specific keywords.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\n# Get memories with specific categories\nmemories = client.get_all(user_id=\"alex\", categories=[\"likes\"])\n\n# Get memories with multiple categories\nmemories = client.get_all(user_id=\"alex\", categories=[\"likes\", \"food_preferences\"])\n\n# Custom pagination with categories\nmemories = client.get_all(user_id=\"alex\", categories=[\"likes\"], page=1, page_size=50)\n\n# Get memories with specific keywords\nmemories = client.get_all(user_id=\"alex\", keywords=\"to play\", page=1, page_size=50)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with OpenSearch\nDESCRIPTION: Creates an Embedchain App instance with OpenSearch configuration, including connection details and vector dimensions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/opensearch.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"opensearch\",\n    \"config\": {\n        \"opensearch_url\": \"your-opensearch-url.com\",\n        \"http_auth\": [\"admin\", \"admin\"],\n        \"vector_dimension\": 1536,\n        \"collection_name\": \"my-app\",\n        \"use_ssl\": False,\n        \"verify_certs\": False\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Vertex AI LLM Provider\nDESCRIPTION: This code snippet and YAML configuration show how to set up Vertex AI as the LLM provider for Embedchain. It includes specifying the model and its parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: vertexai\n  config:\n    model: 'chat-bison'\n    temperature: 0.5\n    top_p: 0.5\n```\n\n----------------------------------------\n\nTITLE: Running the Mem0 LiveKit Voice Agent in Debug Mode\nDESCRIPTION: Command to start the LiveKit voice agent in development/debug mode for testing and troubleshooting.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/livekit.mdx#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython mem0-livekit-voice-agent.py dev\n```\n\n----------------------------------------\n\nTITLE: Delete All Users with Mem0\nDESCRIPTION: Deletes all users, agents and runs from the Mem0 system using Python and JavaScript. It demonstrates the basic function calls. The expected response is logged to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\nclient.delete_users()\n```\n\n----------------------------------------\n\nTITLE: Get All Memories for a User - cURL\nDESCRIPTION: Illustrates how to send a GET request via cURL with filters to list all memories for a user. Includes the Mem0 API endpoint, versioning, pagination, and a user filter in the request body. Used for bulk data retrieval for account-specific memory histories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \\\"https://api.mem0.ai/v1/memories/?version=v2&page=1&page_size=50\\\" \\\\n     -H \\\"Authorization: Token your-api-key\\\" \\\\n     -H \\\"Content-Type: application/json\\\" \\\\n     -d '{\\n         \\\"filters\\\": {\\n             \\\"AND\\\": [\\n                 {\\n                     \\\"user_id\\\": \\\"alice\\\"\\n                 }\\n             ]\\n         }\\n     }'\n```\n\n----------------------------------------\n\nTITLE: Deploying RAG Application to Modal.com\nDESCRIPTION: Command to deploy the RAG application to Modal.com using Embedchain's deployment utility, which will provide a deployment endpoint upon completion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/modal_com.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with YAML\nDESCRIPTION: YAML configuration file for the Embedchain app, specifying the LLM provider (Hugging Face), model (Mistral), and embedder settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/streamlit-mistral.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napp:\n    config:\n        name: 'mistral-streamlit-app'\n\nllm:\n    provider: huggingface\n    config:\n        model: 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n        temperature: 0.1\n        max_tokens: 250\n        top_p: 0.1\n        stream: true\n\nembedder:\n    provider: huggingface\n    config:\n        model: 'sentence-transformers/all-mpnet-base-v2'\n```\n\n----------------------------------------\n\nTITLE: Adding a New Memory Entry Asynchronously with AsyncMemory - Python\nDESCRIPTION: This snippet adds a new set of messages to the memory for a specified user asynchronously using the add method of AsyncMemory. The method requires a messages parameter (list of chat dicts) and a user_id. Outputs a memory record or acknowledgement of insertion. The operation is non-blocking and intended for asyncio event loops.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.add(\\n    messages=[\\n        {\"role\": \"user\", \"content\": \"I'm travelling to SF\"},\\n        {\"role\": \"assistant\", \"content\": \"That's great to hear!\"}\\n    ],\\n    user_id=\"alice\"\\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with Milvus Support for Zilliz Integration\nDESCRIPTION: Command to install the necessary Python dependencies for using Embedchain with Zilliz/Milvus vector database. This installs the Embedchain package with the Milvus extra dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/zilliz.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[milvus]'\n```\n\n----------------------------------------\n\nTITLE: Memory Search API Response - JSON\nDESCRIPTION: This JSON snippet shows the expected response format from the memory search API. It includes an array of memory objects, each containing the memory's ID, content, metadata, score, creation and update timestamps, user ID, and agent ID. The metadata contains category of the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/v2-search-memories.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"memories\": [\n    {\n      \"id\": \"ea925981-272f-40dd-b576-be64e4871429\",\n      \"memory\": \"Likes to play cricket and plays cricket on weekends.\",\n      \"metadata\": {\n        \"category\": \"hobbies\"\n      },\n      \"score\": 0.32116443111457704,\n      \"created_at\": \"2024-07-26T10:29:36.630547-07:00\",\n      \"updated_at\": null,\n      \"user_id\": \"alice\",\n      \"agent_id\": \"sports-agent\"\n    }\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0Memory via from_client with LlamaIndex - Python\nDESCRIPTION: Demonstrates importing Mem0Memory and creating a new instance using 'from_client', providing a context dictionary and an API key. 'search_msg_limit' is optional and determines how many messages to use for memory retrieval; more messages increase context but may slow retrieval. Requires LlamaIndex and Mem0Memory dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/llama-index.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.memory.mem0 import Mem0Memory\n\ncontext = {\"user_id\": \"user_1\"}\nmemory_from_client = Mem0Memory.from_client(\n    context=context,\n    api_key=\"<your-mem0-api-key>\",\n    search_msg_limit=4,  # optional, default is 5\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Hub LLM Provider\nDESCRIPTION: This code snippet demonstrates how to configure and use a Hugging Face Hub model as the LLM provider for Embedchain. It includes setting the access token and specifying model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = \"xxx\"\n\nconfig = {\n  \"app\": {\"config\": {\"id\": \"my-app\"}},\n  \"llm\": {\n      \"provider\": \"huggingface\",\n      \"config\": {\n          \"model\": \"bigscience/bloom-1b7\",\n          \"top_p\": 0.5,\n          \"max_length\": 200,\n          \"temperature\": 0.1,\n      },\n  },\n}\n\napp = App.from_config(config=config)\n```\n\n----------------------------------------\n\nTITLE: Updating an Existing Memory by ID using AsyncMemory - Python\nDESCRIPTION: This snippet shows how to update the data field of an existing memory record by specifying its memory_id and the new data via AsyncMemory's update method. Both memory_id and new data are required parameters. Operation returns success/status and is performed asynchronously for efficiency.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.update(\\n    memory_id=\"memory-id-here\",\\n    data=\"I'm travelling to Seattle\"\\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a Memory with Metadata - TypeScript (Open Source)\nDESCRIPTION: Persists a memory string into the open-source Memory instance via TypeScript, supplying userId and metadata parameters. The returned result reflects the outcome and processing of the input memory, with output objects for each deduced fact.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_18\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst result = memory.add(\\\"I like to drink coffee in the morning and go for a walk.\\\", { userId: \\\"alice\\\", metadata: { category: \\\"preferences\\\" } });\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph State Structure in Python\nDESCRIPTION: Defines the state structure (`State`) for the LangGraph using `TypedDict`. The state manages the conversation history (`messages`, automatically updated using `add_messages`) and the unique identifier for the user in Mem0 (`mem0_user_id`). Initializes the `StateGraph` instance with this defined state schema.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/langgraph.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass State(TypedDict):\n    messages: Annotated[List[HumanMessage | AIMessage], add_messages]\n    mem0_user_id: str\n\ngraph = StateGraph(State)\n```\n\n----------------------------------------\n\nTITLE: Running Basic RAG Evaluation with Embedchain\nDESCRIPTION: Demonstrates how to create an Embedchain App, add a data source, and run evaluation on multiple questions, returning scores for answer relevancy, groundedness, and context relevancy.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/evaluation.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\n# Add data sources\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\n# Run evaluation\napp.evaluate([\"What is the net worth of Elon Musk?\", \"How many companies Elon Musk owns?\"])\n# {'answer_relevancy': 0.9987286412340826, 'groundedness': 1.0, 'context_relevancy': 0.3571428571428571}\n```\n\n----------------------------------------\n\nTITLE: OpenAI Function Calling with Tool Dictionary in Embedchain\nDESCRIPTION: Snippet showing how to define a tool dictionary for OpenAI function calling. This approach allows for detailed specification of function parameters, types, and requirements in a structured format.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmultiply = {\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"multiply\",\n    \"description\": \"Multiply two integers together.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"a\": {\n          \"description\": \"First integer\",\n          \"type\": \"integer\"\n        },\n        \"b\": {\n          \"description\": \"Second integer\",\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\n        \"a\",\n        \"b\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Searching for Memories with Mem0 SDKs\nDESCRIPTION: Demonstrates how to search for relevant memories using the Mem0 OSS SDK in Python and TypeScript. The search function takes a query string and user ID to retrieve memories that match the search criteria, returning an array of memory objects with relevance scores.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nrelated_memories = m.search(\"Should I drink coffee or tea?\", user_id=\"alice\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst relatedMemories = memory.search(\"Should I drink coffee or tea?\", { userId: \"alice\" });\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"id\": \"3dc6f65f-fb3f-4e91-89a8-ed1a22f8898a\",\n        \"memory\": \"Likes to drink coffee in the morning\",\n        \"user_id\": \"alice\",\n        \"metadata\": {\"category\": \"preferences\"},\n        \"categories\": [\"user_preferences\", \"food\"],\n        \"immutable\": false,\n        \"created_at\": \"2025-02-24T20:11:39.010261-08:00\",\n        \"updated_at\": \"2025-02-24T20:11:39.010274-08:00\",\n        \"score\": 0.5915589089130715\n    },\n    {\n        \"id\": \"e8d78459-fadd-4c5a-bece-abb8c3dc7ed7\",\n        \"memory\": \"Likes to go for a walk\",\n        \"user_id\": \"alice\",\n        \"metadata\": {\"category\": \"preferences\"},\n        \"categories\": [\"hobby\", \"food\"],\n        \"immutable\": false,\n        \"created_at\": \"2025-02-24T11:47:52.893038-08:00\",\n        \"updated_at\": \"2025-02-24T11:47:52.893048-08:00\",\n        \"score\": 0.43263634637810866\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory Change History with AsyncMemory - Python\nDESCRIPTION: This snippet illustrates fetching the modification history of a particular memory entry using AsyncMemory's history method. The required parameter is memory_id. Returns an event or audit log of changes for that memory, asynchronously, suitable for use with async event loops.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/async-memory.mdx#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nawait memory.history(memory_id=\"memory-id-here\")\n```\n\n----------------------------------------\n\nTITLE: Starting Mintlify Local Development Server\nDESCRIPTION: Command to start the Mintlify documentation preview server locally. This should be run in the directory where your mint.json file is located.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Querying Embedchain App with User Input in Python\nDESCRIPTION: This snippet sets up an interactive loop for querying the Embedchain app. It continuously prompts the user for questions, processes them through the app, and prints the answers. The loop exits when the user enters 'q', 'exit', or 'quit'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/clarifai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Agent Memories - cURL\nDESCRIPTION: This cURL command retrieves all memories associated with a specific AI agent, paginated with a page size of 50.  It uses a GET request with parameters for agent_id, page, and page_size, and includes an authorization header with the API token.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_43\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/memories/?agent_id=ai-tutor&page=1&page_size=50\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Google AI with Embedchain\nDESCRIPTION: Example showing how to configure Google AI as the LLM provider with Embedchain. This requires setting the GOOGLE_API_KEY environment variable and using a YAML config file to set model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"GOOGLE_API_KEY\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n\nresponse = app.query(\"What is the net worth of Elon Musk?\")\nif app.llm.config.stream: # if stream is enabled, response is a generator\n    for chunk in response:\n        print(chunk)\nelse:\n    print(response)\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: google\n  config:\n    model: gemini-pro\n    max_tokens: 1000\n    temperature: 0.5\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: google\n  config:\n    model: 'models/embedding-001'\n    task_type: \"retrieval_document\"\n    title: \"Embeddings for Embedchain\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSearch Dependencies for mem0\nDESCRIPTION: Command to install the required OpenSearch dependencies for mem0 using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/opensearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install opensearch>=2.8.0\n```\n\n----------------------------------------\n\nTITLE: Set Expiration Date in Mem0 - JavaScript\nDESCRIPTION: This code snippet demonstrates how to add a memory to Mem0 with an expiration date using the JavaScript client. It calculates a date 30 days from the current date and also shows how to use an explicit date string. Requires the 'mem0ai' package.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/expiration-date.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport MemoryClient from 'mem0ai';\nconst client = new MemoryClient({ apiKey: 'your-api-key' });\n\nconst messages = [\n    {\n        \"role\": \"user\", \n        \"content\": \"I'll be in San Francisco until end of this month.\"\n    }\n];\n\n// Set an expiration date 30 days from now\nconst expirationDate = new Date();\nexpirationDate.setDate(expirationDate.getDate() + 30);\nclient.add(messages, { \n    user_id: \"alex\", \n    expiration_date: expirationDate.toISOString().split('T')[0] \n})\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n\n// You can also use an explicit date string\nclient.add(messages, { \n    user_id: \"alex\", \n    expiration_date: \"2023-08-31\" \n})\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Defining GET Memory Export API Endpoint in OpenAPI\nDESCRIPTION: YAML snippet defining the OpenAPI specification for the POST /v1/exports/get endpoint. This endpoint is used to retrieve the latest structured memory export, with options to filter by user_id, run_id, session_id, or app_id.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/memory/get-memory-export.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npost /v1/exports/get\n```\n\n----------------------------------------\n\nTITLE: Defining LLM and Embedding Model Config in YAML\nDESCRIPTION: This snippet defines the configuration for the LLM and embedding models using AWS Bedrock. It specifies the model providers, names, and various parameters. The configuration is written to a YAML file for later use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = \"\"\"\nllm:\n  provider: aws_bedrock\n  config:\n    model: 'amazon.titan-text-express-v1'\n    deployment_name: ec_titan_express_v1\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: aws_bedrock\n  config:\n    model: amazon.titan-embed-text-v2:0\n    deployment_name: ec_embeddings_titan_v2\n\"\"\"\n\n# Write the multi-line string to a YAML file\nwith open('aws_bedrock.yaml', 'w') as file:\n    file.write(config)\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq LLM Provider\nDESCRIPTION: This code snippet demonstrates how to set up Groq as the LLM provider for Embedchain. It includes setting the API key, configuring the model, adding a data source, and querying the app.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# Set your API key here or pass as the environment variable\ngroq_api_key = \"gsk_xxxx\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"groq\",\n        \"config\": {\n            \"model\": \"mixtral-8x7b-32768\",\n            \"api_key\": groq_api_key,\n            \"stream\": True\n        }\n    }\n}\n\napp = App.from_config(config=config)\n# Add your data source here\napp.add(\"https://docs.embedchain.ai/sitemap.xml\", data_type=\"sitemap\")\napp.query(\"Write a poem about Embedchain\")\n\n# In the realm of data, vast and wide,\n# Embedchain stands with knowledge as its guide.\n# A platform open, for all to try,\n# Building bots that can truly fly.\n\n# With REST API, data in reach,\n# Deployment a breeze, as easy as a speech.\n# Updating data sources, anytime, anyday,\n# Embedchain's power, never sway.\n\n# A knowledge base, an assistant so grand,\n# Connecting to platforms, near and far.\n# Discord, WhatsApp, Slack, and more,\n# Embedchain's potential, never a bore.\n```\n\n----------------------------------------\n\nTITLE: Getting All Users in Python\nDESCRIPTION: This Python code retrieves all users from the Mem0 API using the `client.users()` method. It returns a list of user objects.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nclient.users()\n```\n\n----------------------------------------\n\nTITLE: Batch Update Memories with Mem0\nDESCRIPTION: Updates multiple memories in a single API call using Mem0.  The examples show how to construct the list of memory updates and perform the API call in Python, JavaScript, and using a cURL request. The cURL request requires an API key and the request body contains the list of memories to update.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_71\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst updateMemories = [{\\\"memory_id\\\": \\\"285ed74b-6e05-4043-b16b-3abd5b533496\\\",\n        text: \"Watches football\"\n    },\n    {\\\"memory_id\\\": \\\"2c9bd859-d1b7-4d33-a6b8-94e0147c4f07\\\",\n        text: \"Loves to travel\"\n    }\n];\n\nclient.batchUpdate(updateMemories)\n    .then(response => console.log('Batch update response:', response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Environment Variables in Python\nDESCRIPTION: This code sets up the necessary AWS environment variables for authentication and region selection. It uses placeholder values that need to be replaced with actual AWS credentials from the AWS Management Console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAIOSFODNN7EXAMPLE\" # replace with your AWS_ACCESS_KEY_ID\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" # replace with your AWS_SECRET_ACCESS_KEY\nos.environ[\"AWS_SESSION_TOKEN\"] = \"IQoJb3JpZ2luX2VjEJr...==\" # replace with your AWS_SESSION_TOKEN\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\" # replace with your AWS_DEFAULT_REGION\n\nfrom embedchain import App\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry for Mem0 Project\nDESCRIPTION: This snippet demonstrates how to install project dependencies using Poetry and activate the virtual environment. It emphasizes not to use pip or conda for dependency management.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/development.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake install_all\n\n# Activate virtual environment\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT4ALL LLM Provider\nDESCRIPTION: This code snippet and YAML configuration demonstrate how to set up GPT4ALL as the LLM provider for Embedchain. It includes model selection and parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: gpt4all\n  config:\n    model: 'orca-mini-3b-gguf2-q4_0.gguf'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: gpt4all\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DeepSeek API Endpoint\nDESCRIPTION: This snippet shows how to configure a custom API endpoint for DeepSeek in the mem0 configuration. It allows specifying a custom base URL and directly setting the API key in the configuration rather than using environment variables.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/deepseek.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"llm\": {\n        \"provider\": \"deepseek\",\n        \"config\": {\n            \"model\": \"deepseek-chat\",\n            \"deepseek_base_url\": \"https://your-custom-endpoint.com\",\n            \"api_key\": \"your-api-key\"  # alternatively to using environment variable\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 SDK via pip and npm - bash\nDESCRIPTION: Demonstrates installation of the Mem0 client package for Python (using pip) and JavaScript (using npm). These commands are prerequisites for any Python, JavaScript, or TypeScript Mem0 integration, and should be run in your shell to make the SDK available in your environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Adding User Preferences to Mem0 Memory in JavaScript\nDESCRIPTION: This function demonstrates how to add user preferences to Mem0's memory. It creates a MemoryClient instance and uses it to store a string of user preferences related to car preferences and budget.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nasync function addUserPreferences() {\n    const mem0Client = new MemoryClient(mem0Config);\n    \n    const userPreferences = \"I Love BMW, Audi and Porsche. I Hate Mercedes. I love Red cars and Maroon cars. I have a budget of 120K to 150K USD. I like Audi the most.\";\n    \n    await mem0Client.add([{\n        role: \"user\",\n        content: userPreferences,\n    }], mem0Config);\n}\n\nawait addUserPreferences();\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Go\nDESCRIPTION: POST request to query an Embedchain application for information about Elon Musk using Go's http package. The query is sent as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_12\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"strings\"\n  \"net/http\"\n  \"io/ioutil\"\n)\n\nfunc main() {\n\n  url := \"http://localhost:8080/my-app/query\"\n\n  payload := strings.NewReader(\"query=Who is Elon Musk?\")\n\n  req, _ := http.NewRequest(\"POST\", url, payload)\n\n  req.Header.Add(\"Content-Type\", \"application/x-www-form-urlencoded\")\n\n  res, _ := http.DefaultClient.Do(req)\n\n  defer res.Body.Close()\n  body, _ := ioutil.ReadAll(res.Body)\n\n  fmt.Println(res)\n  fmt.Println(string(body))\n\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pinecone with Serverless Deployment (Python)\nDESCRIPTION: This snippet shows how to configure Pinecone for serverless deployment using AWS. It specifies the collection name, embedding model dimensions, and serverless configuration details.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/pinecone.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"pinecone\",\n        \"config\": {\n            \"collection_name\": \"memory_index\",\n            \"embedding_model_dims\": 1536,  # For OpenAI's text-embedding-3-small\n            \"serverless_config\": {\n                \"cloud\": \"aws\",  # or \"gcp\" or \"azure\"\n                \"region\": \"us-east-1\"  # Choose appropriate region\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Mem0 Project\nDESCRIPTION: This code snippet lists the required Python packages and their specific versions (where applicable) for the Mem0 project. It includes FastAPI for building APIs, Uvicorn as the ASGI server, Embedchain for embeddings, and BeautifulSoup4 for web scraping.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/embedchain/deployment/fly.io/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain\nbeautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with Python\nDESCRIPTION: POST request to create a new Embedchain application with a specified app ID using Python requests library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://localhost:8080/create?app_id=my-app\"\n\npayload={}\n\nresponse = requests.request(\"POST\", url, data=payload)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Inserting User Data into Mem0 Memory in Python\nDESCRIPTION: Adds structured user profile data to the Mem0 memory for subsequent retrieval and personalization. This data can include interests, history, and other relevant facts, which are associated with a user ID. The user must have initialized the Mem0 'memory' instance and set USER_ID. Inputs: user data string and user_id. Output: a confirmation message to indicate successful addition.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations/multion.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nUSER_DATA = \"\"\"\nAbout me\n- I'm Deshraj Yadav, Co-founder and CTO at Mem0, interested in AI and ML Infrastructure.\n- Previously, I was a Senior Autopilot Engineer at Tesla, leading the AI Platform for Autopilot.\n- I built EvalAI at Georgia Tech, an open-source platform for evaluating ML algorithms.\n- Outside of work, I enjoy playing cricket in two leagues in the San Francisco.\n\"\"\"\n\nmemory.add(USER_DATA, user_id=USER_ID)\nprint(\"User data added to memory.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with OpenAI's embedding model. This snippet demonstrates how to set the OpenAI API key as an environment variable and load the embedding model configuration from a YAML file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'xxx'\n\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n\napp.add(\"https://en.wikipedia.org/wiki/OpenAI\")\napp.query(\"What is OpenAI?\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: openai\n  config:\n    model: 'text-embedding-3-small'\n```\n\n----------------------------------------\n\nTITLE: Creating a Mem0 Webhook in Python\nDESCRIPTION: This Python snippet demonstrates how to create a new webhook associated with a specific Mem0 project. It initializes the `MemoryClient`, sets the API key environment variable, and calls the `create_webhook` method with the target URL, a name for the webhook, the project ID, and the list of event types to subscribe to (e.g., 'memory_add'). Requires the `mem0` Python library and a valid API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python Python\nimport os\nfrom mem0 import MemoryClient\n\nos.environ[\"MEM0_API_KEY\"] = \"your-api-key\"\n\nclient = MemoryClient()\n\n# Create webhook in a specific project\nwebhook = client.create_webhook(\n    url=\"https://your-app.com/webhook\",\n    name=\"Memory Logger\",\n    project_id=\"proj_123\",\n    event_types=[\"memory_add\"]\n)\nprint(webhook)\n```\n```\n\n----------------------------------------\n\nTITLE: Local ChromaDB Configuration\nDESCRIPTION: YAML configuration for setting up a local ChromaDB instance with persistent storage in a directory. Specifies collection name and allows database reset functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/chromadb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: chroma\n  config:\n    collection_name: 'my-collection'\n    dir: db\n    allow_reset: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedchain App with OpenAI\nDESCRIPTION: Creates an Embedchain app instance with specific OpenAI configuration including model selection, temperature, and token limits for both LLM and embedder.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4o-mini\",\n            \"temperature\": 0.5,\n            \"max_tokens\": 1000,\n            \"top_p\": 1,\n            \"stream\": False\n        }\n    },\n    \"embedder\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"text-embedding-ada-002\"\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryClient in JavaScript\nDESCRIPTION: This snippet shows how to initialize the MemoryClient in JavaScript. It requires passing the API key as an option.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { MemoryClient } = require('mem0ai');\nconst client = new MemoryClient({ apiKey: 'your-api-key'});\n```\n\n----------------------------------------\n\nTITLE: Zilliz Vector Database YAML Configuration for Embedchain\nDESCRIPTION: YAML configuration file that defines the vector database settings for Zilliz. It specifies the provider, collection name, connection URI and token, vector dimension, and distance metric type to use for similarity search.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/zilliz.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: zilliz\n  config:\n    collection_name: 'zilliz_app'\n    uri: https://xxxx.api.gcp-region.zillizcloud.com\n    token: xxx\n    vector_dim: 1536\n    metric_type: L2\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables\nDESCRIPTION: Configuration of environment variables including OpenAI API key required for the application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/vertex_ai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Adding Memory with Timestamp - JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to add a memory to Mem0 with a custom timestamp. It calculates a timestamp for 5 days ago, converts it to a Unix timestamp, and then adds the memory using the `client.add` method. The result or error is logged to the console.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/timestamp.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Get the current time\nconst currentTime = new Date();\n\n// Calculate 5 days ago\nconst fiveDaysAgo = new Date();\nfiveDaysAgo.setDate(currentTime.getDate() - 5);\n\n// Convert to Unix timestamp (seconds since epoch)\nconst unixTimestamp = Math.floor(fiveDaysAgo.getTime() / 1000);\n\n// Add memory with custom timestamp\nclient.add(\"I'm travelling to SF\", { user_id: \"user1\", timestamp: unixTimestamp })\n    .then(response => console.log(response))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory Revision History with Mem0 in Python\nDESCRIPTION: Demonstrates use of the history() function to fetch all change events for a memory by its ID. Each record details previous and updated values, actions (ADD/UPDATE), timestamps, and deletion status. Useful for auditing and change tracking.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nhistory = m.history(memory_id=\"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\")\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 39,\n    \"memory_id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n    \"previous_value\": \"User is planning to watch a movie tonight.\",\n    \"new_value\": \"I love India, it is my favorite country.\",\n    \"action\": \"UPDATE\",\n    \"created_at\": \"2025-02-27T16:33:20.557Z\",\n    \"updated_at\": \"2025-02-27T16:33:27.051Z\",\n    \"is_deleted\": 0\n  },\n  {\n    \"id\": 37,\n    \"memory_id\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n    \"previous_value\": null,\n    \"new_value\": \"User is planning to watch a movie tonight.\",\n    \"action\": \"ADD\",\n    \"created_at\": \"2025-02-27T16:33:20.557Z\",\n    \"updated_at\": null,\n    \"is_deleted\": 0\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Deploying to Embedchain Platform with Go\nDESCRIPTION: POST request to deploy an Embedchain application to the Embedchain Platform using Go's http package. This requires an API key for authentication.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_16\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"strings\"\n  \"net/http\"\n  \"io/ioutil\"\n)\n\nfunc main() {\n\n  url := \"http://localhost:8080/my-app/deploy\"\n\n  payload := strings.NewReader(\"api_key=ec-xxxx\")\n\n  req, _ := http.NewRequest(\"POST\", url, payload)\n\n  req.Header.Add(\"Content-Type\", \"application/x-www-form-urlencoded\")\n\n  res, _ := http.DefaultClient.Do(req)\n\n  defer res.Body.Close()\n  body, _ := ioutil.ReadAll(res.Body)\n\n  fmt.Println(res)\n  fmt.Println(string(body))\n\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Activating Environment with Poetry (Bash)\nDESCRIPTION: This snippet describes how to install all required project dependencies and activate the poetry-managed virtual environment. It uses a Makefile task (make install_all) for installation and then enters the Poetry-managed shell. Poetry must be installed prior to running these commands. The first command sets up all dependencies, while the subsequent one activates the shell for running project commands in an isolated environment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake install_all\n\n#activate\n\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Configuring Clarifai LLM and Embedder in YAML\nDESCRIPTION: This YAML configuration specifies the use of Clarifai for both the LLM and embedder in Embedchain, including model URLs and additional parameters for the LLM.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n provider: clarifai\n config:\n   model: \"https://clarifai.com/mistralai/completion/models/mistral-7B-Instruct\"\n   model_kwargs:\n     temperature: 0.5\n     max_tokens: 1000  \nembedder:\n provider: clarifai\n config:\n   model: \"https://clarifai.com/clarifai/main/models/BAAI-bge-base-en-v15\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating Mem0 Client - JavaScript\nDESCRIPTION: Shows how to construct a MemoryClient instance in JavaScript using the mem0ai npm package, providing an API key for authentication. Enables subsequent API calls for memory management via the managed platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport MemoryClient from 'mem0ai';\\nconst client = new MemoryClient({ apiKey: 'your-api-key' });\n```\n\n----------------------------------------\n\nTITLE: Delete All User Memories with Mem0\nDESCRIPTION: Deletes all memories associated with a specific user from the Mem0 system. This is achieved using Python, JavaScript, and a cURL request.  The cURL request requires an API key for authorization and specifies the user ID as a query parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_59\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X DELETE \"https://api.mem0.ai/v1/memories/?user_id=alex\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Creating Embedchain App with ChromaDB Configuration in Python\nDESCRIPTION: Initializes an Embedchain App with a custom configuration for ChromaDB. This setup includes specifying the vector database provider, collection name, host, port, and reset permissions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/chromadb.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n     \"vectordb\": {\n        \"provider\": \"chroma\",\n        \"config\": {\n            \"collection_name\": \"my-collection\",\n            \"host\": \"your-chromadb-url.com\",\n            \"port\": 5200,\n            \"allow_reset\": True\n        }\n     }\n})\n```\n\n----------------------------------------\n\nTITLE: Delete Memory by ID with Mem0\nDESCRIPTION: Deletes a specific memory from the Mem0 system using its ID. The examples demonstrate how to perform this operation using Python, JavaScript, and a cURL request. The cURL request requires an API key for authorization.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_56\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X DELETE \"https://api.mem0.ai/v1/memories/memory-id-here\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Bedrock Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with AWS Bedrock's embedding model. This snippet demonstrates how to set the AWS credentials as environment variables and configure the embedding model with specific parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"xxx\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxx\"\nos.environ[\"AWS_REGION\"] = \"us-west-2\"\n\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: aws_bedrock\n  config:\n    model: 'amazon.titan-embed-text-v2:0'\n    vector_dimension: 1024\n    task_type: \"retrieval_document\"\n    title: \"Embeddings for Embedchain\"\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure for Modal.com RAG Application\nDESCRIPTION: Shows the directory structure generated by the Embedchain template for Modal.com applications, including the main app file, environment configurations, and requirements.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/modal_com.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n├── app.py\n├── .env\n├── .env.example\n├── embedchain.json\n└── requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenLIT Monitoring with Embedchain\nDESCRIPTION: Python code example showing how to initialize OpenLIT monitoring with an Embedchain application, including data addition and querying functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/openlit.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\nimport OpenLIT\n\n# Initialize OpenLIT Auto Instrumentation for monitoring.\nopenlit.init()\n\n# Initialize EmbedChain application.\napp = App()\n\n# Add data to your app\napp.add(\"https://en.wikipedia.org/wiki/Elon_Musk\")\n\n# Query your app\napp.query(\"How many companies did Elon found?\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack User Token Scopes\nDESCRIPTION: Required OAuth scopes that need to be added to the Slack app for reading channel history and fetching lists of channels. These permissions allow the AI to access message data from various channel types.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack-AI.mdx#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Following scopes are needed for reading channel history\nchannels:history\nchannels:read\n\n# Following scopes are needed to fetch list of channels from slack\ngroups:read\nmpim:read\nim:read\n```\n\n----------------------------------------\n\nTITLE: JSON Output for Retrieving Custom Instructions\nDESCRIPTION: This JSON snippet shows the expected output when retrieving custom instructions for a project. It includes the full text of the custom instructions, focusing on health-related information extraction guidelines.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-instructions.mdx#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"custom_instructions\": \"Your Task: Extract ONLY health-related information from conversations, focusing on the following areas:\\n1. Medical Conditions, Symptoms, and Diagnoses - illnesses, disorders, or symptoms (e.g., fever, diabetes), confirmed or suspected diagnoses.\\n2. Medications, Treatments, and Procedures - prescription or OTC medications (names, dosages), treatments, therapies, or medical procedures.\\n3. Diet, Exercise, and Sleep - dietary habits, fitness routines, and sleep patterns.\\n4. Doctor Visits and Appointments - past, upcoming, or regular medical visits.\\n5. Health Metrics - data like weight, BP, cholesterol, or sugar levels.\\n\\nGuidelines: Focus solely on health-related content. Maintain clarity and context accuracy while recording.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Multi-User Chat Sessions using session_id in Embedchain Python\nDESCRIPTION: This snippet illustrates maintaining multiple chat sessions for different users by utilizing the `session_id` keyword argument in the `chat()` method. This requires the `embedchain` package. The `session_id` parameter isolates session-specific history, enabling personalized conversation tracking. Each user’s queries and history are kept distinct, allowing context-aware responses tailored to individual users.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/chat.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\napp = App()\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\n# Chat on your data using `.chat()`\\napp.chat(\\\"What is the net worth of Elon Musk?\\\", session_id=\\\"user1\\\")\\n# 'The net worth of Elon Musk is $250.8 billion.'\\napp.chat(\\\"What is the net worth of Bill Gates?\\\", session_id=\\\"user2\\\")\\n# \\\"I don't know the current net worth of Bill Gates.\\\"\\napp.chat(\\\"What was my last question\\\", session_id=\\\"user1\\\")\\n# 'Your last question was \\\"What is the net worth of Elon Musk?\\\"'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Memory - JavaScript\nDESCRIPTION: This JavaScript snippet retrieves a specific memory by its ID. It utilizes `client.get` and handles the promise returned by the method.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_48\n\nLANGUAGE: JavaScript\nCODE:\n```\nclient.get(\"582bbe6d-506b-48c6-a4c6-5df3b1e63428\")\n    .then(memory => console.log(memory))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Deleting Memories (Single and Bulk) with Mem0 in Python\nDESCRIPTION: Illustrates how to delete either a specific memory using its ID or all memories for a particular user. Accepts either memory_id or user_id as parameters for m.delete() and m.delete_all(). Designed for cleanup or data removal tasks.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/python-quickstart.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Delete a memory by id\nm.delete(memory_id=\"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\")\n# Delete all memories for a user\nm.delete_all(user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Adding Raw Text Data Using embedchain App\nDESCRIPTION: Shows how to initialize an embedchain App instance and add raw text data using the 'text' data type. The example demonstrates adding a simple string quote without any preprocessing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/text.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add('Seek wealth, not money or status. Wealth is having assets that earn while you sleep. Money is how we transfer time and wealth. Status is your place in the social hierarchy.', data_type='text')\n```\n\n----------------------------------------\n\nTITLE: Using GPT-4 Turbo Model in Embedchain\nDESCRIPTION: This example shows how to configure Embedchain to use the GPT-4 Turbo model released by OpenAI. It sets up the environment variables and loads the configuration from a YAML file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/faq.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['OPENAI_API_KEY'] = 'xxx'\n\n# load llm configuration from gpt4_turbo.yaml file\napp = App.from_config(config_path=\"gpt4_turbo.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: 'gpt-4-turbo'\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data File for Embedchain (JSON)\nDESCRIPTION: This JSON snippet (`temp.json`) provides sample data structured as a key-value pair. It contains a question about Elon Musk's net worth and the corresponding answer. This file serves as the input data source for the accompanying Python example demonstrating how Embedchain processes local JSON files.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/json.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n```json temp.json\n{\n    \"question\": \"What is your net worth, Elon Musk?\",\n    \"answer\": \"As of October 2023, Elon Musk's net worth is $255.2 billion, making him one of the wealthiest individuals in the world.\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Navigating to Chat-PDF Example App in Embedchain\nDESCRIPTION: Command to navigate to the chat-pdf example directory in the forked Embedchain repository. This is the first step after forking the repository to access the example code.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/chat-with-PDF.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd <your_fork_repo>/examples/chat-pdf\n```\n\n----------------------------------------\n\nTITLE: Structured Message Format with Memory\nDESCRIPTION: Demonstrates using structured messages with multiple content parts while maintaining memory context. This allows for more complex and detailed prompts with persistent memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/vercel-ai-sdk/README.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { generateText } from \"ai\";\nimport { createMem0 } from \"@mem0/vercel-ai-provider\";\n\nconst mem0 = createMem0();\n\nconst { text } = await generateText({\n  model: mem0(\"gpt-4-turbo\", {\n    user_id: \"borat\",\n  }),\n  messages: [\n    {\n      role: \"user\",\n      content: [\n        { type: \"text\", text: \"Suggest me a good car to buy.\" },\n        { type: \"text\", text: \"Why is it better than the other cars for me?\" },\n        { type: \"text\", text: \"Give options for every price range.\" },\n      ],\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 Memory for LlamaIndex\nDESCRIPTION: Sets up the Mem0 API key and initializes the Mem0 memory client for use with LlamaIndex, including context and search message limit configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"MEM0_API_KEY\"] = \"<your-mem0-api-key>\"\n\nfrom llama_index.memory.mem0 import Mem0Memory\n\ncontext = {\"user_id\": \"david\"}\nmemory_from_client = Mem0Memory.from_client(\n    context=context,\n    api_key=os.environ[\"MEM0_API_KEY\"],\n    search_msg_limit=4,  # optional, default is 5\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Binary Compression for Large Vector Collections in Azure AI Search\nDESCRIPTION: This snippet shows how to configure binary compression for large vector collections in Azure AI Search. It uses half precision for storage efficiency.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/azure.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"azure_ai_search\",\n        \"config\": {\n            \"service_name\": \"ai-search-test\",\n            \"api_key\": \"*****\",\n            \"collection_name\": \"mem0\", \n            \"embedding_model_dims\": 1536,\n            \"compression_type\": \"binary\",\n            \"use_float16\": True  # Use half precision for storage efficiency\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Getting All Memories for a User in cURL\nDESCRIPTION: This cURL command retrieves all memories for a specified user using the GET method with query parameters for pagination (`page=1`, `page_size=50`) and filtering by user ID (`user_id=alex`).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_40\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&page=1&page_size=50\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Building Mem0 API Server Docker Image Locally (bash)\nDESCRIPTION: Command to build the Docker image for the Mem0 API server locally using the Dockerfile present in the current directory. The built image will be tagged as `mem0-api-server`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t mem0-api-server .\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Azure OpenAI Embeddings with Mem0 in TypeScript\nDESCRIPTION: This snippet shows how to configure the Mem0 Memory object with Azure OpenAI embeddings in TypeScript. It includes setting up the embedder provider, specifying model properties, and adding messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/azure_openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n    embedder: {\n        provider: \"azure_openai\",\n        config: {\n            model: \"text-embedding-3-large\",\n            modelProperties: {\n                endpoint: \"your-api-base-url\",\n                deployment: \"your-deployment-name\",\n                apiVersion: \"version-to-use\",\n            }\n        }\n    }\n}\n\nconst memory = new Memory(config);\n\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\n\nawait memory.add(messages, { userId: \"john\" });\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: This command sets the necessary environment variable `OPENAI_API_KEY` required by the OpenAI Node.js library to authenticate API requests. Replace 'your_api_key' with your actual OpenAI API key. This variable needs to be accessible in the environment where the Node.js script is executed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion_js.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key\n```\n\n----------------------------------------\n\nTITLE: JSON Output Format for Delete Action in Mem0 System\nDESCRIPTION: This snippet shows the JSON output format for the Delete action in the Mem0 memory system. It demonstrates how to structure the response when deleting information from the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-update-memory-prompt.mdx#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"memory\": [\n        {\n            \"id\" : \"0\",\n            \"text\" : \"This information will be deleted\",\n            \"event\" : \"DELETE\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Exported Memory Data using cURL\nDESCRIPTION: This cURL command shows how to retrieve the exported memory data from the Mem0 API. It includes a user ID filter in the query parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/memory-export.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/memories/export/?user_id=alice\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Initializing ChromaDB App from Config\nDESCRIPTION: Demonstrates how to initialize a ChromaDB application by loading configuration from a YAML file using the embedchain library.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/chromadb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load chroma configuration from yaml file\napp = App.from_config(config_path=\"config1.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Short-Term Memories - cURL\nDESCRIPTION: This cURL command retrieves short-term memories associated with a specific user and run ID.  It uses a GET request with the user_id, run_id, page, and page_size as parameters, along with the authorization token in the header.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_46\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/memories/?user_id=alex&run_id=trip-planning-2024&page=1&page_size=50\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Loading Discourse Data and Querying with embedchain App in Python\nDESCRIPTION: This snippet shows how to set up an `embedchain` Pipeline (App), configure the necessary environment variable (e.g., `OPENAI_API_KEY`), and use the previously initialized `DiscourseLoader` to add data. The `app.add` method takes a Discourse search query string ('openai after:2023-10-1') and the loader instance. Finally, it demonstrates how to query the loaded data using `app.query`. Dependencies include the `os` module and `embedchain.pipeline.Pipeline`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/discourse.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```Python\nimport os\nfrom embedchain.pipeline import Pipeline as App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\napp = App()\n\napp.add(\"openai after:2023-10-1\", data_type=\"discourse\", loader=dicourse_loader)\n\nquestion = \"Where can I find the OpenAI API status page?\"\napp.query(question)\n# Answer: You can find the OpenAI API status page at https:/status.openai.com/.\n```\n```\n\n----------------------------------------\n\nTITLE: Enabling Hybrid Search in Azure AI Search Configuration\nDESCRIPTION: This snippet demonstrates how to enable hybrid search in the Azure AI Search configuration for Mem0. It sets the hybrid_search flag to True and specifies the vector_filter_mode.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/azure.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"azure_ai_search\",\n        \"config\": {\n            \"service_name\": \"ai-search-test\",\n            \"api_key\": \"*****\",\n            \"collection_name\": \"mem0\", \n            \"embedding_model_dims\": 1536,\n            \"hybrid_search\": True,\n            \"vector_filter_mode\": \"postFilter\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Memories from Mem0 in JavaScript\nDESCRIPTION: This code snippet shows how to search for relevant memories in Mem0 based on the current user input. It uses the MemoryClient's search method to find memories that match the given input.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst relevantMemories = await mem0Client.search(userInput, mem0Config);\n```\n\n----------------------------------------\n\nTITLE: Initializing AWS Bedrock with mem0 Library\nDESCRIPTION: Demonstrates how to configure and initialize AWS Bedrock integration with the mem0 library. Includes setting up environment variables for AWS credentials and OpenAI API key, configuring the LLM provider settings, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/llms/models/aws_bedrock.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ['AWS_REGION'] = 'us-east-1'\nos.environ[\"AWS_ACCESS_KEY\"] = \"xx\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xx\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"aws_bedrock\",\n        \"config\": {\n            \"model\": \"arn:aws:bedrock:us-east-1:123456789012:model/your-model-name\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Short-Term Memories - Python\nDESCRIPTION: This Python code retrieves short-term memories associated with a specific user and run ID, paginated with a page size of 50. It employs `client.get_all` to filter memories by `user_id` and `run_id`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nshort_term_memories = client.get_all(user_id=\"alex\", run_id=\"trip-planning-2024\", page=1, page_size=50)\n```\n\n----------------------------------------\n\nTITLE: Configuring Token Usage Tracking in YAML\nDESCRIPTION: YAML configuration example for enabling token usage tracking with OpenAI. Sets the LLM provider, model, and additional parameters including the token_usage flag.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: openai\n  config:\n    model: gpt-4o-mini\n    temperature: 0.5\n    max_tokens: 1000\n    token_usage: true\n```\n\n----------------------------------------\n\nTITLE: Adding XML Files to embedchain App in Python\nDESCRIPTION: Shows how to import the embedchain library and add an XML file to an App instance. The example demonstrates loading XML content where only the text content will be processed while tags are ignored.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/xml.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\napp = App()\n\napp.add('content/data.xml')\n```\n\n----------------------------------------\n\nTITLE: Configuring Vertex AI Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with Google's Vertex AI embedding model. This snippet demonstrates how to configure both the LLM and embedding model using a YAML configuration file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load embedding model configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: vertexai\n  config:\n    model: 'chat-bison'\n    temperature: 0.5\n    top_p: 0.5\n\nembedder:\n  provider: vertexai\n  config:\n    model: 'textembedding-gecko'\n```\n\n----------------------------------------\n\nTITLE: Setting up vLLM with Embedchain\nDESCRIPTION: Example showing how to configure vLLM as the LLM provider with Embedchain. This requires setting up vLLM separately and using a YAML config file to configure model parameters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\n# load llm configuration from config.yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: vllm\n  config:\n    model: 'meta-llama/Llama-2-70b-hf'\n    temperature: 0.5\n    top_p: 1\n    top_k: 10\n    stream: true\n    trust_remote_code: true\n```\n\n----------------------------------------\n\nTITLE: Adding Web Page Content via POST Request\nDESCRIPTION: Makes a POST request to add a web page URL to the system. Requires app_id parameter and accepts source URL and data_type as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/add-data.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/{app_id}/add \\\n  -d \"source=https://www.forbes.com/profile/elon-musk\" \\\n  -d \"data_type=web_page\"\n```\n\n----------------------------------------\n\nTITLE: Querying Embedded Data with Embedchain\nDESCRIPTION: This code snippet shows how to query the embedded data using the Embedchain App. It uses the 'query' method to ask a question about the information stored in the knowledge base.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-chromadb-server.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nelon_bot.query(\"How many companies does Elon Musk run?\")\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Lists required Python packages and their versions for the project. Includes FastAPI web framework, Uvicorn ASGI server, Embedchain library, and BeautifulSoup4 for HTML parsing.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/ec_app/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain\nbeautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Chatting with the Discord Bot\nDESCRIPTION: Slash command format for initiating a chat interaction with the Discord bot. This allows for a more conversational interface compared to the query command, supporting back-and-forth dialogue.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n/ec chat <question>\n```\n\n----------------------------------------\n\nTITLE: Using Mem0 with Custom Configuration in Python\nDESCRIPTION: Example of how to initialize and use Mem0's Memory class with a custom vector store configuration in Python. This snippet demonstrates setting the OpenAI API key as an environment variable and creating a Memory instance with a configurable vector store provider.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/config.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\"\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"your_chosen_provider\",\n        \"config\": {\n            # Provider-specific settings go here\n        }\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Your text here\", user_id=\"user\", metadata={\"category\": \"example\"})\n```\n\n----------------------------------------\n\nTITLE: Deleting Organization Member - OpenAPI Endpoint Definition\nDESCRIPTION: OpenAPI endpoint specification for removing a member from an organization. The endpoint accepts an organization ID parameter to identify the organization from which to remove the member.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/organization/delete-org-member.mdx#2025-04-22_snippet_0\n\nLANGUAGE: openapi\nCODE:\n```\ndelete /api/v1/orgs/organizations/{org_id}/members/\n```\n\n----------------------------------------\n\nTITLE: Example Output - JSON\nDESCRIPTION: This JSON snippet shows an example of the response received after successfully adding a memory to Mem0.  It includes the memory ID, the memory content, and the event type (ADD).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/expiration-date.mdx#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"results\": [\n        {\n            \"id\": \"a1b2c3d4-e5f6-4g7h-8i9j-k0l1m2n3o4p5\",\n            \"data\": {\n                \"memory\": \"In San Francisco until end of this month\"\n            },\n            \"event\": \"ADD\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Project Tests\nDESCRIPTION: Commands to run project tests using pytest through Poetry or make command\nSOURCE: https://github.com/mem0ai/mem0/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n\n# or\n\nmake test\n```\n\n----------------------------------------\n\nTITLE: Retrieving Custom Instructions in Python\nDESCRIPTION: This code snippet shows how to retrieve the current custom instructions for a project using Python. It uses the client.get_project() method with a specific field parameter to fetch only the custom instructions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-instructions.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve current custom instructions\nresponse = client.get_project(fields=[\"custom_instructions\"])\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for Full Stack Application\nDESCRIPTION: Docker Compose configuration that defines two services: a backend container exposing port 8000 and a frontend container exposing port 3000. The frontend depends on the backend service and both services are configured to restart automatically unless manually stopped.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/full_stack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.9\"\n\nservices:\n  backend:\n    container_name: embedchain-backend\n    restart: unless-stopped\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    image: embedchain/backend\n    ports:\n      - \"8000:8000\"\n\n  frontend:\n    container_name: embedchain-frontend\n    restart: unless-stopped\n    build:\n      context: frontend\n      dockerfile: Dockerfile\n    image: embedchain/frontend\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - \"backend\"\n```\n\n----------------------------------------\n\nTITLE: Integrating Teachability with Mem0\nDESCRIPTION: Setup of Teachability feature with Mem0 memory system for enhanced learning capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom cookbooks.helper.mem0_teachability import Mem0Teachability\n\nteachability = Mem0Teachability(\n                verbosity=2,\n                recall_threshold=0.5,\n                reset_db=False,\n                agent_id=AGENT_ID,\n                memory_client = MEM0_MEMORY_CLIENT,\n            )\nteachability.add_to_agent(user_proxy)\n```\n\n----------------------------------------\n\nTITLE: Creating Memory-Enabled Voice Agent\nDESCRIPTION: Function to create an OpenAI Agent with memory-related tools and specific instructions for voice interaction.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef create_memory_voice_agent():\n    # Create the agent with memory-enabled tools\n    agent = Agent(\n        name=\"Memory Assistant\",\n        instructions=prompt_with_handoff_instructions(\n            \"\"\"You're speaking to a human, so be polite and concise.\n            Always respond in clear, natural English.\n            You have the ability to remember information about the user.\n            Use the save_memories tool when the user shares an important information worth remembering.\n            Use the search_memories tool when you need context from past conversations or user asks you to recall something.\n            \"\"\",\n        ),\n        model=\"gpt-4o\",\n        tools=[save_memories, search_memories],\n    )\n    \n    return agent\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 Package via NPM\nDESCRIPTION: Command to install the Mem0 package using npm for the open-source version of the library. This is the first step in setting up Mem0 for use in your application.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i mem0ai\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Embedchain Chat-PDF App in Development Mode\nDESCRIPTION: Commands to install required dependencies and run the chat-pdf application in development mode. This uses the Embedchain CLI tool to start a local development server.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/chat-with-PDF.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with Anthropic Configuration\nDESCRIPTION: Initializes an Embedchain application with Anthropic as the provider. The configuration specifies the Claude model to use along with parameters like temperature and top_p that control the model's response generation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/anthropic.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config={\n    \"provider\": \"anthropic\",\n    \"config\": {\n        \"model\": \"claude-instant-1\",\n        \"temperature\": 0.5,\n        \"top_p\": 1,\n        \"stream\": False\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embedding Models (text-embedding-3-small and text-embedding-3-large)\nDESCRIPTION: YAML configurations for OpenAI's two embedding models: text-embedding-3-small and text-embedding-3-large. These configurations can be used with Embedchain to specify which OpenAI embedding model to use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: openai\n  config:\n    model: 'text-embedding-3-small'\n```\n\nLANGUAGE: yaml\nCODE:\n```\nembedder:\n  provider: openai\n  config:\n    model: 'text-embedding-3-large'\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Embedding Model with Embedchain\nDESCRIPTION: Beginning of an example for setting up an Embedchain app with Cohere's embedding model. This snippet shows how to set the Cohere API key as an environment variable.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['COHERE_API_KEY'] = 'xxx'\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Mem0 API Server (bash)\nDESCRIPTION: Command to install all necessary Python packages listed in the `requirements.txt` file using pip. This step is a prerequisite for running the server directly without Docker.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Querying Embedchain Bot\nDESCRIPTION: Sends a query to the bot requesting Flask API implementation details.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-docs-site-example.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nanswer = embedchain_docs_bot.query(\"Write a flask API for embedchain bot\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Mem0 OSS Python Client with Graph Memory\nDESCRIPTION: Creates an instance of the `Memory` class, explicitly enabling the graph memory feature by setting the `enable_graph` parameter to `True`. This configuration allows Mem0 to build and utilize entity relationships stored in a graph database alongside vector storage.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/llms.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMemory(enable_graph=True)\n```\n\n----------------------------------------\n\nTITLE: OpenSearch Configuration YAML\nDESCRIPTION: YAML configuration file specifying OpenSearch connection parameters including collection name, URL, authentication, vector dimensions, and SSL settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/opensearch.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: opensearch\n  config:\n    collection_name: 'my-app'\n    opensearch_url: 'https://localhost:9200'\n    http_auth:\n      - admin\n      - admin\n    vector_dimension: 1536\n    use_ssl: false\n    verify_certs: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Required Slack Bot Permission Scopes\nDESCRIPTION: List of required OAuth scopes that need to be added to the Slack Bot for proper functionality, including reading messages and writing responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack_bot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\napp_mentions:read\nchannels:history\nchannels:read\nchannels:write\n```\n\n----------------------------------------\n\nTITLE: Making a POST Request to Deploy App Endpoint in Bash\nDESCRIPTION: Example of how to use curl to send a POST request to deploy an app with a specific app_id. The request requires an API key passed as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/deploy.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/{app_id}/deploy \\\n  -d \"api_key=ec-xxxx\"\n```\n\n----------------------------------------\n\nTITLE: Pulling Mem0 API Server Docker Image from Hub (bash)\nDESCRIPTION: Command to download the official pre-built Docker image for the Mem0 API server from Docker Hub. This is an alternative to building the image locally.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull mem0/mem0-api-server\n```\n\n----------------------------------------\n\nTITLE: JSON Response from Mem0 API Query\nDESCRIPTION: Example of the JSON response returned by the mem0 API when querying information about Elon Musk. The response is a simple JSON object containing a single field with the answer to the query.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/query.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"response\": \"Net worth of Elon Musk is $218 Billion.\" }\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Anthropic and OpenAI\nDESCRIPTION: Sets up the necessary environment variables for Anthropic and OpenAI API keys. These keys are required for authenticating with their respective services.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/anthropic.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Mem0 and OpenAI\nDESCRIPTION: This snippet demonstrates how to set up environment variables for Mem0 and OpenAI API keys. These keys are essential for authenticating requests to both services.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/openai-inbuilt-tools.mdx#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nMEM0_API_KEY=your_mem0_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App in Python\nDESCRIPTION: This code shows how to add a data source to the configured Embedchain app. In this example, it's adding a URL to Elon Musk's Forbes profile as a data source for the app to process and learn from.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/clarifai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Running Slack Bot in Development\nDESCRIPTION: Command to run the Slack bot in development environment for testing connectivity with the Embedchain app before deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython app.py  #To run the app in development environment\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Mem0 Memory with OpenAI Embedder in TypeScript\nDESCRIPTION: This snippet shows how to create a configuration object for Mem0 using OpenAI as the embedder provider, initialize a Memory instance, and add text to the memory in TypeScript. It uses environment variables for the API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/config.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  embedder: {\n    provider: 'openai',\n    config: {\n      apiKey: process.env.OPENAI_API_KEY || '',\n      model: 'text-embedding-3-small',\n      // Provider-specific settings go here\n    },\n  },\n};\n\nconst memory = new Memory(config);\nawait memory.add(\"Your text here\", { userId: \"user\", metadata: { category: \"example\" } });\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with npm (Bash)\nDESCRIPTION: This command uses npm (Node Package Manager) to install the necessary dependencies for the AI companion project: the 'openai' library for interacting with the OpenAI API and the 'mem0ai' library for using Mem0's memory capabilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/ai_companion_js.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Required Python Packages for Project Dependencies\nDESCRIPTION: A list of external Python packages required for the project. The packages include python-dotenv for environment variable management, slack-sdk and slack_bolt for Slack integration, and embedchain for handling embeddings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/nextjs/nextjs_slack/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npython-dotenv\nslack-sdk\nslack_bolt\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory History Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to get the history of a specific memory asynchronously using the MemoryClient in JavaScript. It takes a memory ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.history(\"memory-id-here\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Streamlit RAG Chatbot with Embedchain\nDESCRIPTION: Python script for a Streamlit app that creates a chatbot using Embedchain and Mistral AI. It allows users to ask questions, add knowledge, and interact with the AI model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/streamlit-mistral.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\nimport streamlit as st\n\nwith st.sidebar:\n    huggingface_access_token = st.text_input(\"Hugging face Token\", key=\"chatbot_api_key\", type=\"password\")\n    \"[Get Hugging Face Access Token](https://huggingface.co/settings/tokens)\"\n    \"[View the source code](https://github.com/embedchain/examples/mistral-streamlit)\"\n\n\nst.title(\"💬 Chatbot\")\nst.caption(\"🚀 An Embedchain app powered by Mistral!\")\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [\n        {\n            \"role\": \"assistant\",\n            \"content\": \"\"\"\n        Hi! I'm a chatbot. I can answer questions and learn new things!\\n\n        Ask me anything and if you want me to learn something do `/add <source>`.\\n\n        I can learn mostly everything. :)\n        \"\"\",\n        }\n    ]\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):\n    if not st.session_state.chatbot_api_key:\n        st.error(\"Please enter your Hugging Face Access Token\")\n        st.stop()\n\n    os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = st.session_state.chatbot_api_key\n    app = App.from_config(config_path=\"config.yaml\")\n\n    if prompt.startswith(\"/add\"):\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        prompt = prompt.replace(\"/add\", \"\").strip()\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n            message_placeholder.markdown(\"Adding to knowledge base...\")\n            app.add(prompt)\n            message_placeholder.markdown(f\"Added {prompt} to knowledge base!\")\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"Added {prompt} to knowledge base!\"})\n            st.stop()\n\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    with st.chat_message(\"assistant\"):\n        msg_placeholder = st.empty()\n        msg_placeholder.markdown(\"Thinking...\")\n        full_response = \"\"\n\n        for response in app.chat(prompt):\n            msg_placeholder.empty()\n            full_response += response\n\n        msg_placeholder.markdown(full_response)\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n```\n\n----------------------------------------\n\nTITLE: Updating Mintlify CLI with npm\nDESCRIPTION: Command to update the Mintlify CLI to the latest version using npm. This ensures you're using the most recent features and fixes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify@latest\n```\n\n----------------------------------------\n\nTITLE: Fetching Memories for Customer Support AI Agent in Python\nDESCRIPTION: This code snippet shows how to retrieve all memories associated with a specific customer ID using the CustomerSupportAIAgent class.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/customer-support-agent.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmemories = support_agent.get_memories(user_id=customer_id)\nfor m in memories['results']:\n    print(m['memory'])\n```\n\n----------------------------------------\n\nTITLE: Setting Telegram Bot Webhook URL\nDESCRIPTION: URL format for setting a webhook for your Telegram bot. This connects your bot to your server endpoint, allowing it to receive updates from Telegram.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/telegram_bot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: url\nCODE:\n```\nhttps://api.telegram.org/bot<Your_Telegram_Bot_Token>/setWebhook?url=<Replit_Generated_URL>\n```\n\n----------------------------------------\n\nTITLE: Deploying Slack Bot with Docker\nDESCRIPTION: Command to run the Slack Bot using Docker, specifying environment variables for OpenAI API key and Slack Bot token.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack_bot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name slack-bot -e OPENAI_API_KEY=sk-xxx -e SLACK_BOT_TOKEN=xxx -p 8000:8000 embedchain/slack-bot\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Header for Telegram Bot Template\nDESCRIPTION: This code snippet shows the main header of the README file, introducing the Telegram bot template.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/telegram_bot/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Telegram Bot\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet defines the required Python packages for the project. It specifies streamlit version 1.29.0 and embedchain without a version constraint.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/embedchain/deployment/streamlit.io/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nstreamlit==1.29.0\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages for the project. It specifies Gradio version 4.11.0 and the embedchain package without a version constraint.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/embedchain/deployment/gradio.app/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngradio==4.11.0\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running Development Environment in Bash\nDESCRIPTION: These commands install the required dependencies from the requirements.txt file and start the development environment for the Chat-PDF app using Embedchain's CLI tool.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chat-pdf/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Starting Mem0 FastAPI Server with Uvicorn (bash)\nDESCRIPTION: Command to start the Mem0 FastAPI application (defined as `app` in `main.py`) using the Uvicorn ASGI server. The `--reload` flag enables auto-reloading upon code changes, which is useful during development. This is used when running the server without Docker. The API will be accessible at `http://localhost:8000`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Searching with Graph Memory in JavaScript\nDESCRIPTION: This snippet shows how to search memories with Graph Memory enabled using the Mem0 JavaScript client. It demonstrates making a search API call with the necessary parameters for Graph Memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/graph-memory.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Search with graph memory enabled\nconst results = await client.search({\n  query: \"what is my name?\",\n  userId: \"joseph\",\n  enableGraph: true,\n  outputFormat: \"v1.1\"\n});\n\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Updating a Memory - Result Sample - JSON\nDESCRIPTION: Sample output after updating a memory, showing a textual confirmation. Typically returned by the Memory.update function after a successful update. For output example purposes only.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Memory updated successfully!\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Clarifai with Embedchain\nDESCRIPTION: Example showing how to install dependencies and set up Clarifai as a provider with Embedchain. This requires setting the CLARIFAI_PAT environment variable after installing the required packages.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"CLARIFAI_PAT\"] = \"XXX\"\n```\n\n----------------------------------------\n\nTITLE: Code Formatting with Black (Bash)\nDESCRIPTION: This snippet demonstrates how to auto-format all project code using black, triggered via a Makefile command. Black enforces a consistent code style throughout the codebase. Run this command before submitting changes to ensure formatting consistency.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake format\n```\n\n----------------------------------------\n\nTITLE: Deleting Webhook via OpenAPI Endpoint\nDESCRIPTION: DELETE endpoint specification for removing a webhook using its unique identifier. The endpoint requires the webhook_id parameter in the URL path.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/webhook/delete-webhook.mdx#2025-04-22_snippet_0\n\nLANGUAGE: openapi\nCODE:\n```\ndelete /api/v1/webhooks/{webhook_id}/\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Mem0 Support Chatbot\nDESCRIPTION: Imports necessary libraries and sets up environment variables for OpenAI (used for embeddings) and Anthropic API keys required by the chatbot.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/customer-support-chatbot.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Dict\nfrom mem0 import Memory\nfrom datetime import datetime\nimport anthropic\n\n# Set up environment variables\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\" # needed for embedding model\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your_anthropic_api_key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Command to install all required Python dependencies for the NextJS project using pip and the requirements.txt file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Retrieving User Data Asynchronously in Python\nDESCRIPTION: This code demonstrates how to get all users, agents, and runs which have memories associated with them asynchronously using the AsyncMemoryClient in Python.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nawait client.users()\n```\n\n----------------------------------------\n\nTITLE: Storing Memories in Memgraph\nDESCRIPTION: This snippet demonstrates how to store inferred memories from the conversation in Memgraph, associated with a user ID and metadata.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresult = m.add(\n    messages, user_id=\"alice\", metadata={\"category\": \"movie_recommendations\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest for Mem0 Project\nDESCRIPTION: This command runs the project's test suite using pytest to verify functionality before submitting a pull request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/development.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Customer Support AI Agent\nDESCRIPTION: This snippet shows how to install the necessary Python packages (openai and mem0ai) using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/customer-support-agent.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Dependencies\nDESCRIPTION: Command to install other required dependencies such as numpy, sounddevice, and pydantic.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install numpy sounddevice pydantic\n```\n\n----------------------------------------\n\nTITLE: Making a Ping Request\nDESCRIPTION: Example of how to make a GET request to the ping endpoint using curl. This endpoint is used to verify the service is up and running.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/check-status.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET \\\n    --url http://localhost:8080/ping\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Application Locally with Embedchain\nDESCRIPTION: Commands to install dependencies and run the RAG application locally for testing before deployment to Modal.com.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/modal_com.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Querying and Updating Embedchain App Context in Python\nDESCRIPTION: This code snippet shows how to query the Embedchain app, print the context, add a new data source, and then re-query to see the updated context. It demonstrates how the app's underlying context changes with new data sources.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"Who is Elon Musk?\"\ncontext = \" \".join([a['context'] for a in app.search(question)])\nprint(\"Context:\", context)\napp.add(\"https://www.forbes.com/profile/elon-musk\")\ncontext = \" \".join([a['context'] for a in app.search(question)])\nprint(\"Context with updated memory:\", context)\n```\n\n----------------------------------------\n\nTITLE: Docker Run with API Key\nDESCRIPTION: Example of running the Docker container with an OpenAI API key environment variable.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/create.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name embedchain -p 8080:8080 -e OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> embedchain/rest-api:latest\n```\n\n----------------------------------------\n\nTITLE: Deleting Specific Memory Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to delete a specific memory asynchronously using the MemoryClient in JavaScript. It takes a memory ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.delete(\"memory-id-here\");\n```\n\n----------------------------------------\n\nTITLE: Initializing App with Weaviate Configuration in Python\nDESCRIPTION: Creates an App instance using configuration loaded from a YAML file that specifies Weaviate as the vector database provider.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/weaviate.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load weaviate configuration from yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Embedchain App with cURL\nDESCRIPTION: POST request to create a new Embedchain application with a specified app ID using cURL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \"http://localhost:8080/create?app_id=my-app\" \\\n -H \"accept: application/json\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Apps via GET Request (Bash)\nDESCRIPTION: Example curl command to make a GET request to the /apps endpoint running on localhost port 8080.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/get-all-apps.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET \\\n  --url http://localhost:8080/apps\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Chainlit and Embedchain\nDESCRIPTION: Install the necessary Python packages (embedchain and chainlit) using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/chainlit.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain chainlit\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Application with Embedchain's Template\nDESCRIPTION: Creates a new directory for your RAG application and initializes it with the Gradio.app template using Embedchain's command-line utility.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/gradio_app.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-rag-app\nec create --template=gradio.app\n```\n\n----------------------------------------\n\nTITLE: Initializing Together Embedder with Memory Configuration in Python\nDESCRIPTION: Example showing how to configure and initialize a Memory instance using Together embedder. Demonstrates setting up environment variables, creating configuration, and adding conversation messages to memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/together.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mem0 import Memory\n\nos.environ[\"TOGETHER_API_KEY\"] = \"your_api_key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"together\",\n        \"config\": {\n            \"model\": \"togethercomputer/m2-bert-80M-8k-retrieval\"\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"john\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Appending to Struct Vector in C\nDESCRIPTION: Functions to create a new vector and append elements to an existing vector. The new function initializes an empty vector with specified element size and capacity, while append adds an element and handles capacity expansion.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/retrieval-methods.mdx#2025-04-22_snippet_2\n\nLANGUAGE: C\nCODE:\n```\nvec_t *vec_new(size_t elem_size, size_t cap_count) {\\n    return vec_load(elem_size, cap_count, 0, NULL);\\n}\\n\\nbool vec_append(vec_t *vec, const void *elem) {\\n    if (!vec_validate(vec) || !elem)\\n        return false;\\n    if (vec->elem_count >= vec->cap_count) {\\n        size_t new_count = vec->cap_count ? (vec->cap_count * 2) : 64;\\n        void *new_data = calloc(new_count, vec->elem_size);\\n        if (!new_data)\\n            return false;\\n        if (vec->elem) {\\n            if (vec->elem_count)\\n                memcpy(new_data, vec->elem, vec->elem_count * vec->elem_size);\\n            free(vec->elem);\\n        }\\n        vec->elem = new_data;\\n        vec->cap_count = new_count;\\n    }\\n    memcpy((char *)vec->elem + (vec->elem_count * vec->elem_size), elem, vec->elem_size);\\n    vec->elem_count++;\\n    return true;\\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response: Memory Search - JSON Output\nDESCRIPTION: A sample JSON result from searching for relevant memories. Each memory object contains IDs, user and memory content, metadata, timestamps, and a score indicating relevance. The format matches outputs for both Python and JavaScript SDKs, and direct API calls.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\\n    {\\n        \\\"id\\\": \\\"7f165f7e-b411-4afe-b7e5-35789b72c4a5\\\",\\n        \\\"memory\\\": \\\"Does not like cheese\\\",\\n        \\\"user_id\\\": \\\"alex\\\",\\n        \\\"metadata\\\": null,\\n        \\\"created_at\\\": \\\"2024-07-20T01:30:36.275141-07:00\\\",\\n        \\\"updated_at\\\": \\\"2024-07-20T01:30:36.275172-07:00\\\",\\n        \\\"score\\\": 0.92\\n    },\\n    {\\n        \\\"id\\\": \\\"8f165f7e-b411-4afe-b7e5-35789b72c4b6\\\",\\n        \\\"memory\\\": \\\"Lives in San Francisco\\\",\\n        \\\"user_id\\\": \\\"alex\\\",\\n        \\\"metadata\\\": null,\\n        \\\"created_at\\\": \\\"2024-07-20T01:30:36.275141-07:00\\\",\\n        \\\"updated_at\\\": \\\"2024-07-20T01:30:36.275172-07:00\\\",\\n        \\\"score\\\": 0.85\\n    }\\n]\n```\n\n----------------------------------------\n\nTITLE: Running Mem0 API Server Docker Container (bash)\nDESCRIPTION: Command to run the Mem0 API server Docker container (using the image `mem0-api-server`, either pulled or locally built). It maps port 8000 of the host to port 8000 of the container and loads environment variables from the `.env` file in the current directory. The API will be accessible at `http://localhost:8000`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8000:8000 mem0-api-server --env-file .env\n```\n\n----------------------------------------\n\nTITLE: Deleting All Memories Asynchronously in JavaScript\nDESCRIPTION: This snippet shows how to delete all memories for a user asynchronously using the MemoryClient in JavaScript. It takes an options object with a user ID.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait client.deleteAll({ user_id: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources to Slack Bot\nDESCRIPTION: Command format for adding data sources to the Slack Bot, requiring data type and URL or text content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/slack_bot.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nadd <data_type> <url_or_text>\n```\n\n----------------------------------------\n\nTITLE: Formatting Code with Black for Mem0 Project\nDESCRIPTION: This command uses the Black formatter to ensure consistent code style across the project.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/development.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake format\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI with yarn\nDESCRIPTION: Command to install the Mintlify CLI globally using yarn package manager. This is an alternative to the npm installation method.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn global add mintlify\n```\n\n----------------------------------------\n\nTITLE: Updating a Mem0 Webhook in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates updating a Mem0 webhook using the `mem0ai` library. It calls the asynchronous `updateWebhook` method on an initialized `MemoryClient`, specifying the `webhookId` and providing the new values for `name`, `url`, and `eventTypes`. Requires the `mem0ai` package and a configured client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n```javascript JavaScript\n// Update webhook for a specific project\nconst updatedWebhook = await client.updateWebhook({\n    name: \"Updated Logger\",\n    url: \"https://your-app.com/new-webhook\",\n    eventTypes: [\"memory_update\", \"memory_add\"],\n    webhookId: \"wh_123\"\n});\nconsole.log(updatedWebhook);\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 with Graph Memory Support using pip\nDESCRIPTION: This command installs Mem0 along with the necessary dependencies for graph functionality using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/graph-db-demo/memgraph-example.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mem0ai[graph]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LlamaIndex and Mem0\nDESCRIPTION: Installs the necessary Python packages for using LlamaIndex core and Mem0 memory integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install llama-index-core llama-index-memory-mem0\n```\n\n----------------------------------------\n\nTITLE: Deploying to Fly.io\nDESCRIPTION: Command to deploy the RAG application to Fly.io platform.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages that need to be installed. Includes embedchain for embedding functionality, streamlit for web interface, and pysqlite3-binary for SQLite database support.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/unacademy-ai/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nembedchain\nstreamlit\npysqlite3-binary\n```\n\n----------------------------------------\n\nTITLE: Creating App with Custom Config\nDESCRIPTION: Example of creating an app with a custom configuration file using curl.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/create.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/create?app_id=my-app \\\n  -F \"config=@/path/to/config.yaml\"\n```\n\n----------------------------------------\n\nTITLE: API Response Format\nDESCRIPTION: Example response format when successfully creating an app through the API.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/create.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"response\": \"App created successfully. App ID: app1\" }\n```\n\n----------------------------------------\n\nTITLE: Linking to WhatsApp Bot Documentation in Markdown\nDESCRIPTION: This Markdown snippet provides a hyperlink to the detailed documentation for the WhatsApp bot implementation using Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/whatsapp_bot/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n[here](https://docs.embedchain.ai/examples/whatsapp_bot)\n```\n\n----------------------------------------\n\nTITLE: Deploying Embedchain App\nDESCRIPTION: Command to deploy the Embedchain application to Fly.io using the Embedchain CLI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies with Version Requirements\nDESCRIPTION: This code snippet defines the required Python packages with their specific version constraints for the mem0ai/mem0 project. It includes Modal version 0.56.4329, FastAPI version 0.104.0, Uvicorn version 0.23.2, and Embedchain without a version specification.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/embedchain/deployment/modal.com/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmodal==0.56.4329\nfastapi==0.104.0\nuvicorn==0.23.2\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Installing Clarifai Dependencies for Embedchain\nDESCRIPTION: This bash command installs the necessary dependencies for using Clarifai with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[clarifai]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Zilliz with Environment Variables in Python\nDESCRIPTION: Python code that sets up Zilliz connection by configuring environment variables and initializing an Embedchain App using a YAML configuration file. This approach loads the connection details from a separate configuration file.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/zilliz.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ['ZILLIZ_CLOUD_URI'] = 'https://xxx.zillizcloud.com'\nos.environ['ZILLIZ_CLOUD_TOKEN'] = 'xxx'\n\n# load zilliz configuration from yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Successful Response from App Deletion Endpoint\nDESCRIPTION: Example JSON response when an app is successfully deleted. The response includes a confirmation message with the app_id that was deleted.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/delete.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"response\": \"App with id {app_id} deleted successfully.\" }\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with LanceDB Dependencies\nDESCRIPTION: Command to install Embedchain with LanceDB and related dependencies using pip. This installation provides everything needed to use LanceDB as a vector database with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/lancedb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"embedchain[lancedb]\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Discord Bot with Python\nDESCRIPTION: Commands to install the required Python package with Discord support and run the bot. The optional flag '--include-question' can be used to display both the question and answer in responses.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/discord_bot.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade \"embedchain[discord]\"\n\npython -m embedchain.bots.discord\n\n# or if you prefer to see the question and not only the answer, run it with\npython -m embedchain.bots.discord --include-question\n```\n\n----------------------------------------\n\nTITLE: Example Response from GET /{app_id}/data Endpoint\nDESCRIPTION: Sample JSON response from the endpoint showing the structure of returned data. The response contains an array of results, each with data_type, data_value, and metadata fields.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/get-data.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"results\": [\n    {\n      \"data_type\": \"web_page\",\n      \"data_value\": \"https://www.forbes.com/profile/elon-musk/\",\n      \"metadata\": \"null\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Answers without Citations using Embedchain Python\nDESCRIPTION: This code demonstrates the standard usage of the Embedchain `chat()` function to obtain an answer from a provided data source without returning supporting citations (the default behavior). Dependencies include the `embedchain` package. The main parameter is the user query string passed to `.chat()`, with the output being a plain answer string. This form is useful when only direct answers are needed and source transparency is not required.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/api-reference/app/chat.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\\n\\n# Initialize app\\napp = App()\\n\\n# Add data source\\napp.add(\\\"https://www.forbes.com/profile/elon-musk\\\")\\n\\n# Chat on your data using `.chat()`\\nanswer = app.chat(\\\"What is the net worth of Elon?\\\")\\nprint(answer)\\n# Answer: The net worth of Elon Musk is $221.9 billion.\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI\nDESCRIPTION: Command to install the Mintlify CLI globally using npm package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Relationship Memories in Python and TypeScript\nDESCRIPTION: Illustrates adding memories that establish multiple relationships simultaneously, in this case connecting two different people to the same activity.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"John loves to hike and Harry loves to hike as well\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"John loves to hike and Harry loves to hike as well\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Professional Profile Export\nDESCRIPTION: This JSON schema defines the structure for exporting professional profile information. It includes properties like full name, current role, education level, and employment status with their respective constraints and descriptions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/memory-export.mdx#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"$defs\": {\n        \"EducationLevel\": {\n            \"enum\": [\"high_school\", \"bachelors\", \"masters\"],\n            \"title\": \"EducationLevel\",\n            \"type\": \"string\"\n        },\n        \"EmploymentStatus\": {\n            \"enum\": [\"full_time\", \"part_time\", \"student\"],\n            \"title\": \"EmploymentStatus\", \n            \"type\": \"string\"\n        }\n    },\n    \"properties\": {\n        \"full_name\": {\n            \"anyOf\": [\n                {\n                    \"maxLength\": 100,\n                    \"minLength\": 2,\n                    \"type\": \"string\"\n                },\n                {\n                    \"type\": \"null\"\n                }\n            ],\n            \"default\": null,\n            \"description\": \"The professional's full name\",\n            \"title\": \"Full Name\"\n        },\n        \"current_role\": {\n            \"anyOf\": [\n                {\n                    \"type\": \"string\"\n                },\n                {\n                    \"type\": \"null\"\n                }\n            ],\n            \"default\": null,\n            \"description\": \"Current job title or role\",\n            \"title\": \"Current Role\"\n        }\n    },\n    \"title\": \"ProfessionalProfile\",\n    \"type\": \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing FastAPI Poe Package\nDESCRIPTION: Command to install the required FastAPI Poe package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/poe_bot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install fastapi-poe==0.0.16 \n```\n\n----------------------------------------\n\nTITLE: Testing the RAG Application Locally\nDESCRIPTION: Installs the required dependencies and runs the application locally for testing before deployment.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/gradio_app.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Deploying RAG Application to Render.com\nDESCRIPTION: Command to deploy the RAG application to render.com platform after configuring the repository URL in render.yaml.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/render_com.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nec deploy\n```\n\n----------------------------------------\n\nTITLE: Example Mem0 Delete Webhook API Response\nDESCRIPTION: This JSON object shows the typical response from the Mem0 API upon successful deletion of a webhook. It usually consists of a confirmation message.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n```json Output\n{\n  \"message\": \"Webhook deleted successfully\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Deleting All Memories Asynchronously in Python\nDESCRIPTION: This code demonstrates how to delete all memories for a user asynchronously using the AsyncMemoryClient in Python. It takes a user ID as a parameter.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait client.delete_all(user_id=\"alice\")\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 via pip\nDESCRIPTION: Command to install the Mem0 Python package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 SDK\nDESCRIPTION: Command to install the Mem0 SDK using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-openai-voice-demo.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: Querying Data with JavaScript\nDESCRIPTION: POST request to query an Embedchain application for information about Elon Musk using JavaScript fetch API. The query is sent as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = fetch(\"http://localhost:8080/my-app/query\", {\n  method: \"POST\",\n  body: \"query=Who is Elon Musk?\",\n}).then((res) => res.json());\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: JSON Output Format for Add Action in Mem0 System\nDESCRIPTION: This snippet demonstrates the JSON output format for the Add action in the Mem0 memory system. It shows how to structure the response when adding new information to the memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-update-memory-prompt.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"memory\": [\n        {\n            \"id\" : \"0\",\n            \"text\" : \"This information is new\",\n            \"event\" : \"ADD\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up the mem0-ts Repository\nDESCRIPTION: Instructions for cloning the repository and installing dependencies for the mem0-ts project.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <repository-url>\ncd mem0-ts\n```\n\n----------------------------------------\n\nTITLE: Updating Mintlify CLI\nDESCRIPTION: Commands for updating the Mintlify CLI to the latest version using either npm or yarn package managers.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/docs.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify@latest\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn global upgrade mintlify\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Memory - cURL\nDESCRIPTION: This cURL command retrieves a specific memory by its ID. It sends a GET request to the memories endpoint with the memory ID as a path parameter and includes the authorization token in the header.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_49\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/memories/582bbe6d-506b-48c6-a4c6-5df3b1e63428\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest via Poetry (Bash)\nDESCRIPTION: This snippet uses poetry to execute tests with pytest, ensuring all project tests pass in the poetry-managed environment. Pytest and all test dependencies must be installed in the poetry environment beforehand. Regularly running this command helps maintain code quality and catch regressions before submitting a pull request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Hugging Face\nDESCRIPTION: This bash command installs the necessary dependencies for using Hugging Face models with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/llms.mdx#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[huggingface-hub]'\n```\n\n----------------------------------------\n\nTITLE: Getting All Users in cURL\nDESCRIPTION: This cURL command retrieves all users from the Mem0 API by sending a GET request to the `/v1/entities/` endpoint. The `Authorization` header is required and must contain a valid API token.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_37\n\nLANGUAGE: cURL\nCODE:\n```\ncurl -X GET \"https://api.mem0.ai/v1/entities/\" \\\n     -H \"Authorization: Token your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Library via pip\nDESCRIPTION: Command to install the Embedchain library, which provides tools for building RAG applications.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/streamlit_io.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Adding a Contradictory Memory about Badminton in Python and TypeScript\nDESCRIPTION: Illustrates adding a memory that contradicts a previous statement, showing how Mem0 handles evolving or conflicting memories.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"I hate playing badminton\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"I hate playing badminton\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Environment Variables\nDESCRIPTION: Sets up the OpenAI API key as an environment variable and imports necessary modules.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/opensearch.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package\nDESCRIPTION: Command to install the embedchain Python package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/whatsapp_bot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade embedchain\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Mem0 Chrome Extension\nDESCRIPTION: Command to install required npm dependencies for the extension development.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/yt-assistant-chrome/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSearch Dependencies\nDESCRIPTION: Installs the required OpenSearch dependencies for Embedchain using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/opensearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'embedchain[opensearch]'\n```\n\n----------------------------------------\n\nTITLE: Running the Chainlit App\nDESCRIPTION: Command to run the Chainlit app using the chainlit CLI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/chainlit.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nchainlit run app.py\n```\n\n----------------------------------------\n\nTITLE: Port Conflict Error Message\nDESCRIPTION: Example error message when attempting to run Mintlify on a port that is already in use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/docs.mdx#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nError: listen EADDRINUSE: address already in use :::3000\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Python\nDESCRIPTION: POST request to query an Embedchain application for information about Elon Musk using Python requests library. The query is sent as form data.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://localhost:8080/my-app/query\"\n\npayload = \"query=Who is Elon Musk?\"\nheaders = {}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Path Icon for MCP Server Integration\nDESCRIPTION: SVG path definition for the MCP Server icon, using a path element to create three horizontal lines with stroke attributes to control appearance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_4\n\nLANGUAGE: SVG\nCODE:\n```\n<path\n  d=\"M45 45 L135 45 M45 90 L135 90 M45 135 L135 135\"\n  stroke=\"currentColor\"\n  strokeWidth=\"12\"\n  strokeLinecap=\"round\"\n  fill=\"none\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Starting Mintlify Development Server\nDESCRIPTION: Command to start the Mintlify development server in the docs directory where mint.json is located. Runs on port 3000 by default.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/docs.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Installing Pinecone Dependencies\nDESCRIPTION: Command to install required Pinecone client libraries and dependencies.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/pinecone.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'pinecone-client pinecone-text'\n```\n\n----------------------------------------\n\nTITLE: Managing Vector Memory and Serialization in C\nDESCRIPTION: Functions for freeing vector memory and saving vector contents to a file. The free function properly deallocates all resources, while save ensures data persistence by writing the vector data to a destination.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/retrieval-methods.mdx#2025-04-22_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nvoid vec_free(vec_t **vecp) {\\n    if (vecp && *vecp) {\\n        if ((*vecp)->elem)\\n            free((*vecp)->elem);\\n        free(*vecp);\\n        *vecp = NULL;\\n    }\\n}\\n\\nbool vec_save(const vec_t *vec, void *dst) {\\n    if (!vec_validate(vec) || !dst)\\n        return false;\\n    if (vec->elem_count)\\n        memcpy(dst, vec->elem, vec->elem_count * vec->elem_size);\\n    return true;\\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Weaviate Client Libraries in Bash\nDESCRIPTION: This snippet installs the core Weaviate engine and its Python client using pip. Both 'weaviate' and 'weaviate-client' packages are required for interacting with a Weaviate server from Python code. The user must have Python and pip already installed. The command does not handle version specifications or virtual environment setup.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/weaviate.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install weaviate weaviate-client\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch in YAML for Embedchain\nDESCRIPTION: YAML configuration file that specifies Elasticsearch as the vector database provider with connection details including cloud ID and basic authentication credentials.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/elasticsearch.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: elasticsearch\n  config:\n    collection_name: 'es-index'\n    cloud_id: 'deployment-name:xxxx'\n    basic_auth:\n      - elastic\n      - <your_password>\n    verify_certs: false\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables\nDESCRIPTION: Example content for the .env file, which includes the necessary API keys for OpenAI and Mem0. These keys are required for the application to function properly.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-demo.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your_openai_api_key\nMEM0_API_KEY=your_mem0_api_key\n```\n\n----------------------------------------\n\nTITLE: Successful Deploy App Response in JSON\nDESCRIPTION: Example of the JSON response received after successfully deploying an app. The response provides a simple success message.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/deploy.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"response\": \"App deployed successfully.\" }\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with ChromaDB Configuration\nDESCRIPTION: This code initializes an Embedchain App with ChromaDB server configuration. It sets the host and port for the ChromaDB server and creates an App instance with the specified configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-chromadb-server.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\nfrom embedchain.config import AppConfig\n\n\nchromadb_host = \"localhost\"\nchromadb_port = 8000\n\nconfig = AppConfig(host=chromadb_host, port=chromadb_port)\nelon_bot = App(config)\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain and Streamlit\nDESCRIPTION: Installs the required Python packages Embedchain and Streamlit using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/streamlit-mistral.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain streamlit\n```\n\n----------------------------------------\n\nTITLE: Displaying Community Links with Card Components in JSX\nDESCRIPTION: This JSX snippet utilizes `CardGroup` and `Card` components to render a row of three cards. Each `Card` displays a title, an icon, and serves as a hyperlink (`href`) to an external resource: a Calendly scheduling link, a Slack community invite, and a Discord community invite. The `cols={3}` prop on `CardGroup` arranges the cards into three columns. Specific colors are applied to the Slack and Discord cards. This component likely relies on a pre-defined component library providing `CardGroup` and `Card`.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/_snippets/get-help.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={3}>\n  <Card title=\"Talk to founders\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/ec\">\n  Schedule a call\n  </Card>\n  <Card title=\"Slack\" icon=\"slack\" href=\"https://embedchain.ai/slack\" color=\"#4A154B\">\n    Join our slack community\n  </Card>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://discord.gg/6PzXDgEjG5\" color=\"#7289DA\">\n    Join our discord community\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources with Go\nDESCRIPTION: POST request to add a web page data source to an Embedchain application using Go's http package. This example adds Elon Musk's Forbes profile page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_8\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"strings\"\n  \"net/http\"\n  \"io/ioutil\"\n)\n\nfunc main() {\n\n  url := \"http://localhost:8080/my-app/add\"\n\n  payload := strings.NewReader(\"source=https://www.forbes.com/profile/elon-musk&data_type=web_page\")\n\n  req, _ := http.NewRequest(\"POST\", url, payload)\n\n  req.Header.Add(\"Content-Type\", \"application/x-www-form-urlencoded\")\n\n  res, _ := http.DefaultClient.Do(req)\n\n  defer res.Body.Close()\n  body, _ := ioutil.ReadAll(res.Body)\n\n  fmt.Println(res)\n  fmt.Println(string(body))\n\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting and Linting Embedchain Code\nDESCRIPTION: Command to run linting and formatting tools on the codebase to ensure it adheres to project standards before creating a pull request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/dev.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake lint format\n```\n\n----------------------------------------\n\nTITLE: Organization Member Roles\nDESCRIPTION: Defines the two available role types for organization members: READER for view-only access and OWNER for full administrative privileges.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/organization/update-org-member.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nREADER: Allows viewing of organization resources.\nOWNER: Grants full administrative access to manage the organization and its resources.\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Using pip (Bash)\nDESCRIPTION: Demonstrates the installation of the Embedchain package using pip, Python's package manager. This single-line command is intended for setup on any Unix-like system with Python and pip already available. It installs the latest release of Embedchain and any required dependencies from PyPI.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Citing Embedchain in Academic Works (BibTeX)\nDESCRIPTION: Provides a BibTeX-formatted citation for referencing Embedchain in academic or technical works. Users should copy this to a .bib file for inclusion in LaTeX documents. No dependencies are required besides LaTeX/BibTeX toolchains. Fields include authors, title, year, publisher, journal, and howpublished with a GitHub URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{embedchain,\n  author = {Taranjeet Singh, Deshraj Yadav},\n  title = {Embedchain: The Open Source RAG Framework},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/embedchain/embedchain}},\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Vector Elements by Index in C\nDESCRIPTION: Function to safely retrieve an element from a vector by its index. It performs validation to ensure the index is within bounds and the vector is properly initialized.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/retrieval-methods.mdx#2025-04-22_snippet_4\n\nLANGUAGE: C\nCODE:\n```\nvoid *vec_get(const vec_t *vec, size_t idx) {\\n    if (!vec_validate(vec) || idx >= vec->elem_count)\\n        return NULL;\\n    return (void *)((char *)vec->elem + (idx * vec->elem_size));\\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration YAML\nDESCRIPTION: Example YAML configuration file defining app settings, LLM provider, vector database, and embedder configurations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/create.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napp:\n  config:\n    id: \"default-app\"\n\nllm:\n  provider: openai\n  config:\n    model: \"gpt-4o-mini\"\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n    prompt: |\n      Use the following pieces of context to answer the query at the end.\n      If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n      $context\n\n      Query: $query\n\n      Helpful Answer:\n\nvectordb:\n  provider: chroma\n  config:\n    collection_name: \"rest-api-app\"\n    dir: db\n    allow_reset: true\n\nembedder:\n  provider: openai\n  config:\n    model: \"text-embedding-ada-002\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with Elasticsearch Configuration\nDESCRIPTION: Creates an Embedchain App instance by loading configuration from a YAML file that contains Elasticsearch settings.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/elasticsearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load elasticsearch configuration from yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\n----------------------------------------\n\nTITLE: JSON Output for Updating Custom Instructions\nDESCRIPTION: This JSON snippet represents the expected output when successfully updating custom instructions for a project. It contains a simple message confirming the update.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-instructions.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"message\": \"Updated custom instructions\"\n}\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A comprehensive list of Python package dependencies with pinned versions required for the project. Notable dependencies include FastAPI, Streamlit, embedchain with various integrations, and multiple packages for handling different data sources and APIs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/rest-api/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.104.0\nuvicorn==0.23.2\nstreamlit==1.29.0\nembedchain==0.1.3\nslack-sdk==3.21.3 \nflask==2.3.3\nfastapi-poe==0.0.16\ndiscord==2.3.2\ntwilio==8.5.0\nhuggingface-hub==0.17.3\nembedchain[community, opensource, elasticsearch, opensearch, weaviate, pinecone, qdrant, images, cohere, together, milvus, vertexai, llama2, gmail, json]==0.1.3\nsqlalchemy==2.0.22\npython-multipart==0.0.6\nyoutube-transcript-api==0.6.1 \npytube==15.0.0 \nbeautifulsoup4==4.12.3\nslack-sdk==3.21.3\nhuggingface_hub==0.23.0\ngitpython==3.1.38\nyt_dlp==2023.11.14\nPyGithub==1.59.1\nfeedparser==6.0.10\nnewspaper3k==0.2.8\nlistparser==0.19\n```\n\n----------------------------------------\n\nTITLE: Initializing DiscourseLoader in Python\nDESCRIPTION: This snippet demonstrates how to import the `DiscourseLoader` from `embedchain.loaders.discourse` and initialize it. It requires providing the target Discourse community's domain URL within a configuration dictionary. This loader is used to fetch data from the specified Discourse instance.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/data-sources/discourse.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```Python\nfrom embedchain.loaders.discourse import DiscourseLoader\n\ndicourse_loader = DiscourseLoader(config={\"domain\": \"https://community.openai.com\"})\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Mem0 with pip\nDESCRIPTION: Installs the Mem0 client library using pip, the Python package installer. This command allows you to use Mem0 in your Python projects.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai\n```\n\n----------------------------------------\n\nTITLE: JSON Output Format for No Change Action in Mem0 System\nDESCRIPTION: This snippet presents the JSON output format for the No Change action in the Mem0 memory system. It shows how to structure the response when no changes are made to the existing memory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-update-memory-prompt.mdx#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"memory\": [\n        {\n            \"id\" : \"0\",\n            \"text\" : \"No changes for this information\",\n            \"event\" : \"NONE\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Data Sources with Python\nDESCRIPTION: POST request to add a web page data source to an Embedchain application using Python requests library. This example adds Elon Musk's Forbes profile page.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://localhost:8080/my-app/add\"\n\npayload = \"source=https://www.forbes.com/profile/elon-musk&data_type=web_page\"\nheaders = {}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package with pip\nDESCRIPTION: Command to install or update the Embedchain Python package using pip. This is a prerequisite for using the Embedchain CLI tool.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/full-stack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain -U\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify on a custom port\nDESCRIPTION: Command to start the Mintlify development server on a custom port (3333 in this example) using the --port flag. This is useful when the default port 3000 is already in use.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/documentation.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev --port 3333\n```\n\n----------------------------------------\n\nTITLE: Deleting a Memory - TypeScript\nDESCRIPTION: Deletes a memory record by ID or all memories for a given user ID. Memory.delete accepts a single memory ID, whereas Memory.deleteAll removes all records associated with a user. Deletions are asynchronous and irreversible.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n// Delete a memory by id\nawait memory.delete('892db2ae-06d9-49e5-8b3e-585ef9b85b8e');\n\n// Delete all memories for a user\nawait memory.deleteAll({ userId: \"alice\" });\n```\n\n----------------------------------------\n\nTITLE: Setting up ChromaDB Server with Docker\nDESCRIPTION: This snippet shows how to clone the ChromaDB repository and start the server using Docker Compose. It sets up the necessary environment for running ChromaDB as a server.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-chromadb-server.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/chroma-core/chroma\ncd chroma && docker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: Launching WhatsApp Bot with Python\nDESCRIPTION: Python command to start the WhatsApp bot on a specified port using the embedchain module.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/whatsapp_bot.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m embedchain.bots.whatsapp --port 5000\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Embeddings with LangChain\nDESCRIPTION: Example of setting up HuggingFace embeddings through LangChain, showing model initialization and configuration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/embedders/models/langchain.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\n# Initialize a HuggingFace embeddings model\nhf_embeddings = HuggingFaceEmbeddings(\n    model_name=\"BAAI/bge-small-en-v1.5\",\n    encode_kwargs={\"normalize_embeddings\": True}\n)\n\nconfig = {\n    \"embedder\": {\n        \"provider\": \"langchain\",\n        \"config\": {\n            \"model\": hf_embeddings\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App in Python\nDESCRIPTION: This snippet demonstrates how to add a data source to the Embedchain app. In this case, it adds a Lorem Ipsum generator website as an example of an unrelated data source.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.lipsum.com/\")\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install the required npm packages for the mem0-ts project.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Creating Redis Embedding Database in Go\nDESCRIPTION: This snippet demonstrates how to create a Redis embedding database using the redis-stack with integration for vector operations. It includes connecting to the Redis server and setting up the necessary configuration for vector search.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/support/get-help.mdx#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nredisEmbedding, err := goredis.New(\n\tcontext.Background(),\n\tgoredis.Config{\n\t\tAddress: \"redis-stack:6379\",\n\t\tVector: goredis.VectorConfig{\n\t\t\tAlgorithm:     \"flat\",\n\t\t\tDistanceMetric: \"cosine\",\n\t\t},\n\t},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Path Icon for Agno Integration\nDESCRIPTION: SVG path definition for the Agno icon, using a simple path with stroke attributes to create a curved line and no fill.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_8\n\nLANGUAGE: SVG\nCODE:\n```\n<path d=\"M8 4h8v12h8\" stroke=\"currentColor\" strokeWidth=\"2\" fill=\"none\" transform=\"rotate(15, 12, 12)\"/>\n```\n\n----------------------------------------\n\nTITLE: Running Application Locally\nDESCRIPTION: Commands to install dependencies and run the RAG application in local development mode.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nec dev\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source to Embedchain App\nDESCRIPTION: This code demonstrates how to add a data source to the Embedchain app. In this example, it adds a Forbes profile URL as a data source for the app to process and learn from.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/jina.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pinecone Embedding Database in Go\nDESCRIPTION: This code example shows how to initialize a Pinecone embedding database connection. It configures the necessary parameters like API key, environment, and the embedding dimension.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/support/get-help.mdx#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\npineconeEmbedding, err := pinecone.New(\n\tcontext.Background(),\n\tpinecone.Config{\n\t\tAPIKey:     \"PINECONE_API_KEY\",\n\t\tEnvironment: \"ENVIRONMENT\",\n\t\tDimension:   1536,\n\t},\n)\n```\n\n----------------------------------------\n\nTITLE: Running Linter for Mem0 Project\nDESCRIPTION: This command runs the ruff linter to check for code style issues and potential errors.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/contributing/development.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Navigating to Chat-PDF Example App Directory in Bash\nDESCRIPTION: This command changes the current directory to the chat-pdf example app folder in the forked Embedchain repository.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/chat-pdf/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd <your_fork_repo>/examples/chat-pdf\n```\n\n----------------------------------------\n\nTITLE: Development Build and Clean Commands\nDESCRIPTION: Commands for building the TypeScript project and cleaning build artifacts during development.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm run clean\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain VertexAI Package\nDESCRIPTION: Installation command for the Embedchain package with VertexAI support using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/vertex_ai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[vertexai]\n```\n\n----------------------------------------\n\nTITLE: Running Mintlify Local Development Server\nDESCRIPTION: This command starts the Mintlify local development server. It should be executed in the root directory of the documentation project (where the `mint.json` configuration file resides) to preview changes locally before deployment. Requires the Mintlify CLI to be installed and the command to be run in the correct directory.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Starting Development Server\nDESCRIPTION: Command to start the development server for the demo application using pnpm.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-demo.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Defining DELETE Endpoint for Organization Removal in OpenAPI\nDESCRIPTION: This YAML snippet defines the OpenAPI specification for deleting an organization. It specifies the path parameters, security requirements, and possible response codes.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/organization/delete-org.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndelete /api/v1/orgs/organizations/{org_id}/:\n  summary: Delete an organization\n  description: Delete an organization by ID. This action is irreversible.\n  parameters:\n    - name: org_id\n      in: path\n      required: true\n      schema:\n        type: string\n  security:\n    - BearerAuth: []\n  responses:\n    '204':\n      description: Organization successfully deleted\n    '401':\n      $ref: '#/components/responses/UnauthorizedError'\n    '403':\n      $ref: '#/components/responses/ForbiddenError'\n    '404':\n      $ref: '#/components/responses/NotFoundError'\n```\n\n----------------------------------------\n\nTITLE: Building the Project\nDESCRIPTION: Command to compile the TypeScript code for the mem0-ts project.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Creating Function Tools for LlamaIndex ReAct Agent\nDESCRIPTION: Defines and creates function tools for calling, emailing, and ordering food, which will be used by the ReAct agent to perform actions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.tools import FunctionTool\n\ndef call_fn(name: str):\n    \"\"\"Call the provided name.\n    Args:\n        name: str (Name of the person)\n    \"\"\"\n    return f\"Calling... {name}\"\n\ndef email_fn(name: str):\n    \"\"\"Email the provided name.\n    Args:\n        name: str (Name of the person)\n    \"\"\"\n    return f\"Emailing... {name}\"\n\ndef order_food(name: str, dish: str):\n    \"\"\"Order food for the provided name.\n    Args:\n        name: str (Name of the person)\n        dish: str (Name of the dish)\n    \"\"\"\n    return f\"Ordering {dish} for {name}\"\n\ncall_tool = FunctionTool.from_defaults(fn=call_fn)\nemail_tool = FunctionTool.from_defaults(fn=email_fn)\norder_food_tool = FunctionTool.from_defaults(fn=order_food)\n```\n\n----------------------------------------\n\nTITLE: Rendering Contact Channel Cards with CardGroup in React - JavaScript\nDESCRIPTION: This snippet renders a group of contact channels as cards using the CardGroup and Card components in React JSX. It expects both components to be defined and available either in the file scope or imported from a library. Key properties on each Card—title, icon, href, and color—define the display and link behavior. Inputs are not dynamic: the channels and their parameters are hardcoded, and clicking a card navigates the user to the associated channel. No form submissions or backend dependencies are handled inside this snippet; all interactivity is handled via hyperlink navigation.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/_snippets/missing-data-source-tip.mdx#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n<CardGroup cols={2}>\n  <Card title=\"Google Form\" icon=\"file\" href=\"https://forms.gle/NDRCKsRpUHsz2Wcm8\" color=\"#7387d0\">\n    Fill out this form\n  </Card>\n  <Card title=\"Slack\" icon=\"slack\" href=\"https://embedchain.ai/slack\" color=\"#4A154B\">\n    Let us know on our slack community\n  </Card>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://discord.gg/6PzXDgEjG5\" color=\"#7289DA\">\n    Let us know on discord community\n  </Card>\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/embedchain/embedchain/issues/new?assignees=&labels=&projects=&template=feature_request.yml\" color=\"#181717\">\n  Open an issue on our GitHub\n  </Card>\n  <Card title=\"Schedule a call\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/ec\">\n  Schedule a call with Embedchain founder\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memories by Categories - JavaScript\nDESCRIPTION: These JavaScript snippets show how to retrieve memories based on categories using `client.getAll`. They cover cases with single and multiple categories, pagination and filtering by keywords.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/platform/quickstart.mdx#_snippet_51\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Get memories with specific categories\nclient.getAll({ user_id: \"alex\", categories: [\"likes\"] })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n\n// Get memories with multiple categories\nclient.getAll({ user_id: \"alex\", categories: [\"likes\", \"food_preferences\"] })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n\n// Custom pagination with categories\nclient.getAll({ user_id: \"alex\", categories: [\"likes\"], page: 1, page_size: 50 })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n\n// Get memories with specific keywords\nclient.getAll({ user_id: \"alex\", keywords: \"to play\", page: 1, page_size: 50 })\n    .then(memories => console.log(memories))\n    .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Checking for Default Categories Usage in Python\nDESCRIPTION: This code snippet shows how to check if default categories are being used in a Mem0 project. It calls the 'get_project' method to retrieve the custom categories field.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-categories.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclient.get_project([\"custom_categories\"])\n```\n\n----------------------------------------\n\nTITLE: Installing OpenLIT SDK\nDESCRIPTION: Command to install the OpenLIT SDK package using pip package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/openlit.mdx#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install openlit\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Demo Video in HTML\nDESCRIPTION: This HTML code embeds a YouTube video demonstrating Deep Research in action. It sets the video dimensions, source, and various attributes for playback and security.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personalized-deep-research.mdx#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe \n  width=\"700\" \n  height=\"400\" \n  src=\"https://www.youtube.com/embed/8vQlCtXzF60?si=b8iTOgummAVzR7ia\" \n  title=\"YouTube video player\" \n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" \n  referrerpolicy=\"strict-origin-when-cross-origin\" \n  allowfullscreen\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Creating RAG Application with Embedchain\nDESCRIPTION: Commands to install Embedchain and create a new RAG application using the render.com template. This process generates the necessary project structure and configuration files.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/render_com.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\nmkdir my-rag-app\nec create --template=render.com\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Embedchain Development\nDESCRIPTION: Command to install all necessary dependencies for Embedchain development using Make. This ensures your development environment has all required packages before making contributions.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/dev.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure\nDESCRIPTION: Example directory structure showing the files generated by the Fly.io template.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n├── Dockerfile\n├── app.py\n├── fly.toml\n├── .env\n├── .env.example\n├── embedchain.json\n└── requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing FAISS Dependencies\nDESCRIPTION: Commands for installing FAISS packages, offering options for both CPU-only and GPU-enabled (CUDA required) installations via pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/faiss.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# For CPU version\npip install faiss-cpu\n\n# For GPU version (requires CUDA)\npip install faiss-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI with npm\nDESCRIPTION: Command to install the Mintlify CLI globally using npm. This allows you to run Mintlify commands from anywhere in your terminal.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: Lists required Python packages with version specifications. Includes Flask web framework v2.3.2, Twilio SDK v8.5.0, and embedchain package (version unspecified).\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/whatsapp_bot/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nFlask==2.3.2\ntwilio==8.5.0\nembedchain\n```\n\n----------------------------------------\n\nTITLE: Running Code Linting with Ruff (Bash)\nDESCRIPTION: This snippet shows how to use Makefile to trigger code linting via ruff, a performant Python linter. Ensure ruff is installed (typically via poetry dependencies) before running this command. The linter should report no errors or warnings before code is committed or submitted in a pull request.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package using pip\nDESCRIPTION: This snippet shows how to install the Embedchain package using pip. It's a prerequisite for using Embedchain with JinaChat.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/jina.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation Source to Embedchain Bot\nDESCRIPTION: Loads documentation from the Embedchain website into the bot for processing and reference.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-docs-site-example.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nembedchain_docs_bot.add(\"docs_site\", \"https://docs.embedchain.ai/\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in .env for Docker (txt)\nDESCRIPTION: Shows how to set the mandatory `OPENAI_API_KEY` environment variable in a `.env` file in the current directory when running the Mem0 API server using a standalone Docker container. This file will be referenced when starting the container.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/features/rest-api.mdx#2025-04-22_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\nOPENAI_API_KEY=your-openai-api-key\n```\n\n----------------------------------------\n\nTITLE: Setting OpenTelemetry Environment Variables\nDESCRIPTION: Configuration of environment variables for OpenTelemetry endpoint and authentication headers required for OpenLIT integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/integration/openlit.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Setting environment variable for OpenTelemetry destination and authetication.\nexport OTEL_EXPORTER_OTLP_ENDPOINT = \"YOUR_OTEL_ENDPOINT\"\nexport OTEL_EXPORTER_OTLP_HEADERS = \"YOUR_OTEL_ENDPOINT_AUTH\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Embedding Model with Embedchain\nDESCRIPTION: Example of setting up an Embedchain app with Azure OpenAI's embedding model. This snippet shows how to set Azure OpenAI-related environment variables and configure both the LLM and embedding model.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/embedding-models.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom embedchain import App\n\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://xxx.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"xxx\"\nos.environ[\"OPENAI_API_VERSION\"] = \"xxx\"\n\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  provider: azure_openai\n  config:\n    model: gpt-35-turbo\n    deployment_name: your_llm_deployment_name\n    temperature: 0.5\n    max_tokens: 1000\n    top_p: 1\n    stream: false\n\nembedder:\n  provider: azure_openai\n  config:\n    model: text-embedding-ada-002\n    deployment_name: you_embedding_model_deployment_name\n```\n\n----------------------------------------\n\nTITLE: Rendering Contact Cards Using CardGroup Component in JSX\nDESCRIPTION: This code snippet creates a CardGroup component with three Card components inside, providing links for Discord community, GitHub discussions, and founder meetings. Each card has a title, icon, and href attribute.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/_snippets/get-help.mdx#2025-04-22_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<CardGroup cols={3}>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://mem0.dev/DiD\" color=\"#7289DA\">\n    Join our community\n  </Card>\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/mem0ai/mem0/discussions/new?category=q-a\">\n    Ask questions on GitHub\n  </Card>\n  <Card title=\"Support\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/meet\">\n  Talk to founders\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Get Webhook API Endpoint Definition\nDESCRIPTION: OpenAPI endpoint definition for retrieving webhook details by project ID. The endpoint follows REST conventions using GET method.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/api-reference/webhook/get-webhook.mdx#2025-04-22_snippet_0\n\nLANGUAGE: openapi\nCODE:\n```\nget /api/v1/webhooks/projects/{project_id}/\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Mem0AI Project\nDESCRIPTION: This code snippet defines the required Python packages and their versions for the Mem0AI project. It includes web framework FastAPI, ASGI server Uvicorn, data validation library Pydantic, the Mem0AI package itself, and python-dotenv for environment variable management.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/server/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nfastapi==0.115.8\nuvicorn==0.34.0\npydantic==2.10.4\nmem0ai>=0.1.48\npython-dotenv==1.0.1\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Query Loop for Embedchain App\nDESCRIPTION: This snippet sets up an interactive loop for querying the Embedchain app. It continuously prompts the user for questions, processes them through the app, and prints the answers. The loop exits when the user inputs 'q', 'exit', or 'quit'.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/jina.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwhile(True):\n    question = input(\"Enter question: \")\n    if question in ['q', 'exit', 'quit']:\n        break\n    answer = app.query(question)\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Running Embedchain REST API with Docker\nDESCRIPTION: Command to start the Embedchain REST API service using Docker. This exposes the API on port 8080 with Swagger documentation available at localhost.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/getting-started.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name embedchain -p 8080:8080 embedchain/rest-api:latest\n```\n\n----------------------------------------\n\nTITLE: Re-installing Mintlify Dependencies\nDESCRIPTION: This command instructs Mintlify to re-install its necessary dependencies. It's used as a troubleshooting step if the `mintlify dev` command fails to run, potentially resolving issues caused by corrupted or missing dependencies. Requires the Mintlify CLI to be installed.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmintlify install\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Struct Vector Elements in C\nDESCRIPTION: A macro that enables safer iteration through arrays of struct elements. It takes a vector pointer (vp), the struct type, and the variable name to use for each element in the iteration loop.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/retrieval-methods.mdx#2025-04-22_snippet_0\n\nLANGUAGE: C\nCODE:\n```\n#define FOR_VEC(vp, type, val) \\n    for (type *val = (type *)(vp)->elem, *CONCAT(_end, __LINE__) = (type *)((vp)->elem + (vp)->elem_count * (vp)->elem_size); \\n         val < CONCAT(_end, __LINE__); \\n         val = (type *)((char *)(val) + (vp)->elem_size))\n```\n\n----------------------------------------\n\nTITLE: Adding Data Source\nDESCRIPTION: Adds a URL as a data source to the Embedchain app for processing\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/together.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.add(\"https://www.forbes.com/profile/elon-musk\")\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with Cohere Support in Python\nDESCRIPTION: Installs the embedchain package with Cohere integration using pip. This is the first step required to use Cohere with Embedchain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/cohere.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[cohere]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Memory History - Result Sample - JSON\nDESCRIPTION: Sample output for history retrieval, listing each change as an object detailing previous and new values, action type, timestamps, and deletion flags. Used as a reference for what Memory.history returns.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/node-quickstart.mdx#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 39,\n    \"memoryId\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n    \"previousValue\": \"User is planning to watch a movie tonight.\",\n    \"newValue\": \"I love India, it is my favorite country.\",\n    \"action\": \"UPDATE\",\n    \"createdAt\": \"2025-02-27T16:33:20.557Z\",\n    \"updatedAt\": \"2025-02-27T16:33:27.051Z\",\n    \"isDeleted\": 0\n  },\n  {\n    \"id\": 37,\n    \"memoryId\": \"892db2ae-06d9-49e5-8b3e-585ef9b85b8e\",\n    \"previousValue\": null,\n    \"newValue\": \"User is planning to watch a movie tonight.\",\n    \"action\": \"ADD\",\n    \"createdAt\": \"2025-02-27T16:33:20.557Z\",\n    \"updatedAt\": null,\n    \"isDeleted\": 0\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in HTML\nDESCRIPTION: This HTML snippet embeds a YouTube video demonstrating Mem0's multimodal capabilities. The iframe is configured with specific dimensions, source URL, and various attributes for playback and security.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/multimodal-demo.mdx#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/2Md5AEFVpmg?si=rXXupn6CiDUPJsi3\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n```\n\n----------------------------------------\n\nTITLE: Port Already in Use Error Message\nDESCRIPTION: Error message that appears when trying to run Mintlify on a port that's already being used by another process. This indicates you need to use a different port with the --port flag.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_4\n\nLANGUAGE: md\nCODE:\n```\nError: listen EADDRINUSE: address already in use :::3000\n```\n\n----------------------------------------\n\nTITLE: Displaying YouTube Video in Markdown\nDESCRIPTION: This code snippet embeds a YouTube video demonstrating the Mem0 Chrome Extension functionality using an iframe element in Markdown.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/chrome-extension.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/dqenCMMlfwQ?si=zhGVrkq6IS_0Jwyj\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet lists various Python packages with their specific versions. It includes libraries for web development (Flask), API interactions (youtube-transcript-api, slack-sdk), web scraping (beautifulsoup4, newspaper3k), and other utilities.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/examples/api_server/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nflask==2.3.2\nyoutube-transcript-api==0.6.1 \npytube==15.0.0 \nbeautifulsoup4==4.12.3\nslack-sdk==3.21.3\nhuggingface_hub==0.23.0\ngitpython==3.1.38\nyt_dlp==2023.11.14\nPyGithub==1.59.1\nfeedparser==6.0.10\nnewspaper3k==0.2.8\nlistparser==0.19\n```\n\n----------------------------------------\n\nTITLE: Displaying Support Cards Using React JSX Components - JSX\nDESCRIPTION: This code snippet displays a group of React UI cards to direct users to community channels for support. It uses a CardGroup component with a 'cols' property to specify the number of columns and multiple Card components configured with props such as 'title', 'icon', 'href', and 'color'. No additional dependencies beyond React and the Card/CardGroup components are assumed. Inputs are static UI content; outputs are JSX-rendered HTML elements for user interaction. The code assumes those UI components are imported and available.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/_snippets/missing-vector-db-tip.mdx#2025-04-22_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<CardGroup cols={2}>\n  <Card title=\"Slack\" icon=\"slack\" href=\"https://embedchain.ai/slack\" color=\"#4A154B\">\n    Let us know on our slack community\n  </Card>\n  <Card title=\"Discord\" icon=\"discord\" href=\"https://discord.gg/6PzXDgEjG5\" color=\"#7289DA\">\n    Let us know on discord community\n  </Card>\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/embedchain/embedchain/issues/new?assignees=&labels=&projects=&template=feature_request.yml\" color=\"#181717\">\n  Open an issue on our GitHub\n  </Card>\n  <Card title=\"Schedule a call\" icon=\"calendar\" href=\"https://cal.com/taranjeetio/ec\">\n  Schedule a call with Embedchain founder\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package\nDESCRIPTION: Installs the Embedchain package using pip, which is required for building applications with Anthropic integration.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/anthropic.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain\n```\n\n----------------------------------------\n\nTITLE: Configuring Chroma Embedding in Go\nDESCRIPTION: This example shows how to set up a Chroma embedding client. It configures the connection to the Chroma server using the specified host and port, suitable for vector similarity search operations.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/support/get-help.mdx#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nchromaEmbedding, err := chroma.New(\n\tcontext.Background(),\n\tchroma.Config{\n\t\tHost: \"chroma\",\n\t\tPort: \"8000\",\n\t},\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Mintlify CLI with yarn\nDESCRIPTION: Command to update the Mintlify CLI to the latest version using yarn. This is the alternative to updating with npm.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/development.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn global upgrade mintlify\n```\n\n----------------------------------------\n\nTITLE: Database Integration Links Table in JSX/HTML\nDESCRIPTION: Table structure with database options (OpenSearch and Pinecone) featuring clickable links to Colab notebooks and Replit environments. Includes styled badges and external links with proper formatting.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/notebooks-and-replits.mdx#2025-04-22_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<td className=\"align-middle\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/embedchain/embedchain/blob/main/notebooks/opensearch.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" noZoom alt=\"Open In Colab\"/></a></td>\n      <td className=\"align-middle\"><a target=\"_blank\" href=\"https://replit.com/@taranjeetio/opensearchdb#main.py\"><img src=\"https://replit.com/badge?caption=Try%20with%20Replit&amp;variant=small\" noZoom alt=\"Try with Replit Badge\"/></a></td>\n    </tr>\n    <tr>\n      <td className=\"align-middle\">Pinecone</td>\n      <td className=\"align-middle\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/embedchain/embedchain/blob/main/notebooks/pinecone.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" noZoom alt=\"Open In Colab\"/></a></td>\n      <td className=\"align-middle\"><a target=\"_blank\" href=\"https://replit.com/@taranjeetio/pineconedb#main.py\"><img src=\"https://replit.com/badge?caption=Try%20with%20Replit&amp;variant=small\" noZoom alt=\"Try with Replit Badge\"/></a></td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain with Open Source Support for GPT4All\nDESCRIPTION: Installs the embedchain package with open source dependencies required for using GPT4All models.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/gpt4all.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[opensource]\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AI Travel Assistant\nDESCRIPTION: Bash command to install the required Python packages (openai and mem0ai) using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/personal-travel-assistant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai mem0ai\n```\n\n----------------------------------------\n\nTITLE: JSON Output Format for Update Action in Mem0 System\nDESCRIPTION: This snippet illustrates the JSON output format for the Update action in the Mem0 memory system. It shows how to structure the response when updating existing information in the memory, including the old memory content.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/custom-update-memory-prompt.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"memory\": [\n        {\n            \"id\" : \"0\",\n            \"text\" : \"This information replaces the old information\",\n            \"event\" : \"UPDATE\",\n            \"old_memory\" : \"Old information\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting a Mem0 Webhook in JavaScript\nDESCRIPTION: This JavaScript snippet shows how to delete a Mem0 webhook using the `mem0ai` library. It calls the asynchronous `deleteWebhook` method on an initialized `MemoryClient`, providing the `webhookId` of the target webhook. Requires the `mem0ai` package and a configured client.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/webhooks.mdx#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n```javascript JavaScript\n// Delete webhook from a specific project\nconst response = await client.deleteWebhook({webhookId: \"wh_123\"});\nconsole.log(response);\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM for LlamaIndex\nDESCRIPTION: Sets up the OpenAI API key and initializes the OpenAI language model for use with LlamaIndex.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/llama-index-mem0.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom llama_index.llms.openai import OpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables\nDESCRIPTION: Commands to copy the example environment file and configure it with your OpenAI API key.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/mem0-ts/src/oss/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n# Edit .env with your OpenAI API key\n```\n\n----------------------------------------\n\nTITLE: Response Format for Add Content Request\nDESCRIPTION: Success response containing a unique identifier for the added content\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/add-data.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"response\": \"fec7fe91e6b2d732938a2ec2e32bfe3f\" }\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Embedchain\nDESCRIPTION: BibTeX citation format for referencing the Embedchain project in academic or technical publications. Includes author information, title, year, and repository URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/dev.mdx#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n@misc{embedchain,\n  author = {Taranjeet Singh, Deshraj Yadav},\n  title = {Embechain: The Open Source RAG Framework},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/embedchain/embedchain}},\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to Demo Application Folder\nDESCRIPTION: Command to change the current directory to the mem0-demo folder within the cloned repository.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-demo.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd mem0/examples/mem0-demo\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App with Qdrant Configuration\nDESCRIPTION: This snippet demonstrates how to initialize an Embedchain App using a YAML configuration file that specifies Qdrant as the vector database provider.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/components/vector-databases/qdrant.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\n# load qdrant configuration from yaml file\napp = App.from_config(config_path=\"config.yaml\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\nvectordb:\n  provider: qdrant\n  config:\n    collection_name: my_qdrant_index\n```\n\n----------------------------------------\n\nTITLE: Querying for Relevant Memories - Python\nDESCRIPTION: Performs a search for contextually relevant memories using the Python SDK. Requires a MemoryClient instance, a text query, and filter options (such as user_id). The version parameter specifies API versioning; results depend on query semantics and applied filters.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Example showing location and preference-aware recommendations\\nquery = \\\"I'm craving some pizza. Any recommendations?\\\"\\nfilters = {\\n    \\\"AND\\\": [\\n        {\\n            \\\"user_id\\\": \\\"alex\\\"\\n        }\\n    ]\\n}\\nclient.search(query, version=\\\"v2\\\", filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Building Mem0 Extension for Production\nDESCRIPTION: Command to create a production build of the extension.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/examples/yt-assistant-chrome/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Creating Embedchain App from Config in Python\nDESCRIPTION: This code creates an Embedchain app using the configuration file created earlier. It also resets the app to clear the cache and start fresh.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/aws-bedrock.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp = App.from_config(config_path=\"aws_bedrock.yaml\")\napp.reset() # Reset the app to clear the cache and start fresh\n```\n\n----------------------------------------\n\nTITLE: Resetting Client Asynchronously in Python\nDESCRIPTION: This code demonstrates how to reset the client, deleting all users and memories asynchronously using the AsyncMemoryClient in Python.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/features/async-client.mdx#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nawait client.reset()\n```\n\n----------------------------------------\n\nTITLE: Adding a Fictional Character Relationship in Python and TypeScript\nDESCRIPTION: Shows how to add a memory that establishes a relationship between a real person and a fictional identity.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/open-source/graph_memory/overview.mdx#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nm.add(\"My friend peter is the spiderman\", user_id=\"alice123\")\n```\n\nLANGUAGE: typescript\nCODE:\n```\nmemory.add(\"My friend peter is the spiderman\", { userId: \"alice123\" });\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Directory Structure\nDESCRIPTION: Shows the file structure generated by the Embedchain template, including the main app file, configuration, and requirements.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/gradio_app.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n├── app.py\n├── embedchain.json\n└── requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Making a GET Request to Retrieve Application Data\nDESCRIPTION: Example of using curl to make a GET request to retrieve data for a specific application by its ID. The application ID is provided as a path parameter in the URL.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/rest-api/get-data.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET \\\n  --url http://localhost:8080/{app_id}/data\n```\n\n----------------------------------------\n\nTITLE: Installing EmbedChain Package\nDESCRIPTION: Installs the EmbedChain Python package using pip. This is the first step in setting up an AI app with EmbedChain.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embedchain\n```\n\n----------------------------------------\n\nTITLE: SVG Integration Icons\nDESCRIPTION: SVG path definitions and viewBox specifications for integration icons including CrewAI, LangGraph, Vercel AI SDK and LangChain icons. The SVGs use currentColor for dynamic coloring.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_0\n\nLANGUAGE: SVG\nCODE:\n```\n<svg width=\"32\" height=\"32\" viewBox=\"0 0 63 33\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.0556 0.580566H46.6516C55.3698 0.580566 62.4621 7.69777 62.4621 16.4459C62.4621 25.194 55.3698 32.3112 46.6516 32.3112H16.0556C7.16924 32.3112 0.245117 25.194 0.245117 16.4459C0.245117 7.69777 7.16924 0.580566 16.0556 0.580566Z\" fill=\"currentColor\" />\n</svg>\n```\n\n----------------------------------------\n\nTITLE: Installing Embedchain Package for LLAMA2\nDESCRIPTION: Installs the embedchain package with LLAMA2 support using pip.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/llama2.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install embedchain[llama2]\n```\n\n----------------------------------------\n\nTITLE: Installing Fly CLI Across Operating Systems\nDESCRIPTION: Commands to install the Fly.io command line interface (flyctl) on different operating systems including OSX, Linux and Windows.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/deployment/fly_io.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install flyctl\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -L https://fly.io/install.sh | sh\n```\n\nLANGUAGE: bash\nCODE:\n```\npwsh -Command \"iwr https://fly.io/install.ps1 -useb | iex\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Embedchain App\nDESCRIPTION: Creates a new instance of the Embedchain App for document processing and querying.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/notebooks/embedchain-docs-site-example.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom embedchain import App\n\nembedchain_docs_bot = App()\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks\nDESCRIPTION: Command to install pre-commit hooks for maintaining code standards before commits\nSOURCE: https://github.com/mem0ai/mem0/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoGen Agents Configuration\nDESCRIPTION: Setup of GPTAssistantAgent and UserProxyAgent with LLM configuration and basic chat functionality.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/cookbooks/mem0-autogen.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\n\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\n\nassistant_id = os.environ.get(\"ASSISTANT_ID\", None)\n\nCACHE_SEED = 42\nllm_config = {\n    \"config_list\": [\n        {\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n    ],\n    \"cache_seed\": CACHE_SEED,\n    \"timeout\": 120,\n    \"temperature\": 0.0,\n}\n\nassistant_config = {\"assistant_id\": assistant_id}\n\ngpt_assistant = GPTAssistantAgent(\n    name=\"assistant\",\n    instructions=AssistantAgent.DEFAULT_SYSTEM_MESSAGE,\n    llm_config=llm_config,\n    assistant_config=assistant_config,\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    code_execution_config={\n        \"work_dir\": \"coding\",\n        \"use_docker\": False,\n    },\n    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=1,\n    llm_config=llm_config,\n)\n\nuser_query = \"Write a Python function that reverses a string.\"\nuser_proxy.initiate_chat(gpt_assistant, message=user_query)\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Dependencies\nDESCRIPTION: Command to install the required Elasticsearch Python client version 8.0.0 or higher.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/components/vectordbs/dbs/elasticsearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elasticsearch>=8.0.0\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Mem0 and OpenAI Agents Integration\nDESCRIPTION: This bash command installs the necessary Python packages for integrating Mem0 with OpenAI's Agents SDK.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-agentic-tool.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mem0ai pydantic openai-agents\n```\n\n----------------------------------------\n\nTITLE: Creating SVG Path Icon for Keywords AI Integration\nDESCRIPTION: SVG path definition for the Keywords AI icon using fill-rule and clip-rule to create a complex shape with currentColor fill.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/integrations.mdx#2025-04-22_snippet_9\n\nLANGUAGE: SVG\nCODE:\n```\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.07513 1.1863C9.21663 1.07722 9.39144 1.01009 9.56624 1.01009C9.83261 1.01009 10.0823 1.12756 10.2405 1.33734L15.0101 7.4964V12.4136L16.4335 13.8401C16.7582 14.1673 16.7582 14.7043 16.4335 15.0316C16.1089 15.3588 15.5762 15.3588 15.2515 15.0316L13.3453 13.1016V8.07538L8.92529 2.36944V2.36105C8.64228 2.00024 8.70887 1.4716 9.07513 1.1863ZM18.976 14.4133C18.8344 14.3778 18.7003 14.3042 18.5894 14.1925L16.9163 12.5059C16.7249 12.3129 16.6416 12.0528 16.6749 11.8094V6.88385H16.6499L11.8553 0.691225C11.7282 0.529117 11.6716 0.333133 11.6803 0.140562C11.134 0.0481292 10.5726 0 10 0C4.47715 0 0 4.47715 0 10C0 15.5228 4.47715 20 10 20C13.9387 20 17.3456 17.7229 18.976 14.4133Z\" fill=\"currentColor\"></path>\n```\n\n----------------------------------------\n\nTITLE: Installing Mintlify CLI\nDESCRIPTION: Commands for installing Mintlify globally using either npm or yarn package managers. Requires Node.js version 18.10.0 or higher.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/contribution/docs.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn global add mintlify\n```\n\n----------------------------------------\n\nTITLE: Instantiating Mem0 Open Source Memory Class - TypeScript\nDESCRIPTION: Demonstrates construction of the Memory class instance from the open-source distribution of mem0ai for TypeScript. Assumes the mem0ai npm package is installed, and uses 'mem0ai/oss' import path for open-source usage. Enables calling add and other memory-related functions without external APIs.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/quickstart.mdx#2025-04-22_snippet_16\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Memory } from 'mem0ai/oss';\\nconst memory = new Memory();\n```\n\n----------------------------------------\n\nTITLE: Creating and Starting Embedchain App with Docker\nDESCRIPTION: Commands to create a new Embedchain application named 'my-app' with Docker support, navigate to its directory, and start the application using Docker.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/get-started/full-stack.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nec create-app my-app --docker\ncd my-app\nec start --docker\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Command to install the necessary dependencies for the demo application using pnpm package manager.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/docs/examples/mem0-demo.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot in Development\nDESCRIPTION: Command to run the Discord bot locally for development and testing purposes using Python.\nSOURCE: https://github.com/mem0ai/mem0/blob/main/embedchain/docs/examples/nextjs-assistant.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython app.py\n```"
  }
]
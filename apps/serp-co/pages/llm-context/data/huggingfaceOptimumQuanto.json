[
  {
    "owner": "huggingface",
    "repo": "optimum-quanto",
    "content": "TITLE: Low-level Quantization Workflow Example\nDESCRIPTION: Complete example showing the quantization workflow using the low-level API including quantization, calibration, training, freezing and serialization.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.quanto import quantize, qint8\n\nquantize(model, weights=qint8, activations=qint8)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.quanto import Calibration\n\nwith Calibration(momentum=0.9):\n    model(samples)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data, target = data.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model(data).dequantize()\n    loss = torch.nn.functional.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.quanto import freeze\n\nfreeze(model)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom safetensors.torch import save_file\n\nsave_file(model.state_dict(), 'model.safetensors')\n```\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom optimum.quanto import quantization_map\n\nwith open('quantization_map.json', 'w') as f:\n  json.dump(quantization_map(model), f)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom safetensors.torch import load_file\nfrom optimum.quanto import requantize\n\nstate_dict = load_file('model.safetensors')\nwith open('quantization_map.json', 'r') as f:\n  quantization_map = json.load(f)\n\n# Create an empty model from your modeling code and requantize it\nwith torch.device('meta'):\n  new_model = ...\nrequantize(new_model, state_dict, quantization_map, device=torch.device('cuda'))\n```\n\n----------------------------------------\n\nTITLE: Quantizing LLM Models with Optimum Quanto\nDESCRIPTION: Example showing how to quantize a LLaMA model using QuantizedModelForCausalLM with 4-bit quantization, excluding the language model head.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM\nfrom optimum.quanto import QuantizedModelForCausalLM, qint4\n\nmodel = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3-8B')\nqmodel = QuantizedModelForCausalLM.quantize(model, weights=qint4, exclude='lm_head')\n```\n\n----------------------------------------\n\nTITLE: Quantizing Diffusers Models\nDESCRIPTION: Example demonstrating quantization of PixArt transformer model using 8-bit float quantization.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import PixArtTransformer2DModel\nfrom optimum.quanto import QuantizedPixArtTransformer2DModel, qfloat8\n\nmodel = PixArtTransformer2DModel.from_pretrained(\"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\", subfolder=\"transformer\")\nqmodel = QuantizedPixArtTransformer2DModel.quantize(model, weights=qfloat8)\nqmodel.save_pretrained(\"./pixart-sigma-fp8\")\n```\n\n----------------------------------------\n\nTITLE: Supported PyTorch Module Quantization\nDESCRIPTION: Lists the supported PyTorch modules that can be quantized using optimum-quanto. Linear layers use QLinear with quantized weights but unquantized biases, Conv2d uses QConv2D with similar quantization strategy, and LayerNorm supports output quantization only.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.nn.Linear  # Replaced with QLinear\ntorch.nn.Conv2d   # Replaced with QConv2D\ntorch.nn.LayerNorm  # Supports output quantization only\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Quantized LLM Models\nDESCRIPTION: Code showing how to save a quantized model using save_pretrained and reload it using from_pretrained.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nqmodel.save_pretrained('./Llama-3-8B-quantized')\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.quanto import QuantizedModelForCausalLM\n\nqmodel = QuantizedModelForCausalLM.from_pretrained('Llama-3-8B-quantized')\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized Diffusers Model into Pipeline\nDESCRIPTION: Code showing how to load a quantized PixArt transformer and integrate it into a pipeline.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import PixArtTransformer2DModel\nfrom optimum.quanto import QuantizedPixArtTransformer2DModel\n\ntransformer = QuantizedPixArtTransformer2DModel.from_pretrained(\"./pixart-sigma-fp8\")\ntransformer.to(device=\"cuda\")\npipe = PixArtSigmaPipeline.from_pretrained(\n  \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n  transformer=None,\n  torch_dtype=torch.float16,\n).to(\"cuda\")\npipe.transformer = transformer\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation Scripts Overview\nDESCRIPTION: Suite of Python scripts for evaluating quantized models using three key metrics: latency per generated token (latency.py), last token prediction accuracy on Lambada dataset (prediction.py), and perplexity on WikiText dataset (perplexity.py). Includes evaluate_model.py for generating visualizations.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/bench/generation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nlatency.py - evaluates the latency per generated token\\nprediction.py - evaluates accuracy on Lambada dataset\\nperplexity.py - evaluates perplexity on WikiText dataset\\nevaluate_model.py - generates visualizations\n```\n\n----------------------------------------\n\nTITLE: Launching Stable Diffusion Quantization Script\nDESCRIPTION: Command to run the quantization script for Stable Diffusion with specified batch size and torch data type.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython quantize_StableDiffusion.py --batch_size=1 --torch_dtype=\"fp32\"\n```\n\n----------------------------------------\n\nTITLE: Experimental Results: FP16-INT8 Configuration\nDESCRIPTION: Performance metrics for running the quantization script with FP16 precision for torch_dtype and INT8 for unet_dtype.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nfp16-int8\n\nbatch_size: 1, torch_dtype: fp16, unet_dtype: int8  in 3.920 seconds.Memory: 2.634GB.\n```\n\n----------------------------------------\n\nTITLE: Experimental Results: FP16-FP16 Configuration\nDESCRIPTION: Performance metrics for running the quantization script with FP16 precision for both torch_dtype and unet_dtype.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nfp16-fp16\n\nbatch_size: 1, torch_dtype: fp16, unet_dtype: none  in 3.307 seconds.Memory: 3.192GB.\n```\n\n----------------------------------------\n\nTITLE: Experimental Results: BF16-INT8 Configuration\nDESCRIPTION: Performance metrics for running the quantization script with BF16 precision for torch_dtype and INT8 for unet_dtype.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbf16-int8\n\nbatch_size: 1, torch_dtype: bf16, unet_dtype: int8  in 3.918 seconds.Memory: 2.644GB.\n```\n\n----------------------------------------\n\nTITLE: Running SmoothQuant Conversion for OPT Model in Bash\nDESCRIPTION: This command executes the smoothquant.py script to convert a Hugging Face OPT-1.3B model to a smoothed version. The converted model is saved to a specified path for later use in quantization workflows.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/external/smoothquant/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python smoothquant.py --model facebook/opt-1.3b --save-path smoothed-models/facebook/opt-1.3b\n```\n\n----------------------------------------\n\nTITLE: Implementing a New Device-Specific Operation in Quanto Library\nDESCRIPTION: Example showing how to provide an implementation for a newly declared device-specific operation. This snippet shows the implementation signature for the 'gemm_f16i4' operation on CUDA devices.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@torch.library.impl(\"quanto::gemm_f16i4\", [\"CUDA\"])\ndef gemm_f16i4(\n    input: torch.Tensor,\n    other: torch.Tensor,\n    scales: torch.Tensor,\n    shift: torch.Tensor,\n    group_size: int,\n) -> torch.Tensor:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Device-Specific Operations in Quanto Library\nDESCRIPTION: Example showing how to provide a device-specific implementation for an operation that already has a default implementation. This snippet demonstrates implementing the 'unpack' operation for CPU and CUDA devices.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.library.impl(\"quanto::unpack\", [\"CPU\", \"CUDA\"])\ndef unpack(packed: torch.Tensor, bits: int) -> torch.Tensor:\n    return ext.unpack(t, bits)\n```\n\n----------------------------------------\n\nTITLE: Declaring New Device-Specific Operations in Quanto Library\nDESCRIPTION: Example showing how to declare a new device-specific operation in the Quanto library. This snippet defines a new operation called 'gemm_f16i4' with specific parameter requirements.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch.library.define(\n    \"quanto::gemm_f16i4\",\n    \"(Tensor input,\"\n    \" Tensor other,\"\n    \" Tensor other_scale,\"\n    \" Tensor other_shift,\"\n    \" int group_size)\"\n    \" -> Tensor\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Python Wrapper for C++ Bindings\nDESCRIPTION: Instructs to provide an implementation in __init__.py that calls the C++ binding for the new operation.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cpp/README.md#2025-04-08_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n__init__.py\n```\n\n----------------------------------------\n\nTITLE: Referencing PyTorch Operations in C++ Kernels\nDESCRIPTION: Specifies the namespaces that can be used for PyTorch operations in C++ kernel implementations.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cpp/README.md#2025-04-08_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\naten::\nc10::\n```\n\n----------------------------------------\n\nTITLE: Adding New C++ Implementation Files\nDESCRIPTION: Instructs to add new .cpp files to the list of sources in __init__.py for implementing operations defined in library/ops.py.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cpp/README.md#2025-04-08_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n__init__.py\n```\n\n----------------------------------------\n\nTITLE: Adding Bindings for New C++ Implementations\nDESCRIPTION: Directs to add bindings for new C++ implementations in the pybind_module.cpp file.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cpp/README.md#2025-04-08_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\npybind_module.cpp\n```\n\n----------------------------------------\n\nTITLE: Referencing Pytorch Operations in Quanto CUDA Extension\nDESCRIPTION: This snippet demonstrates how to reference Pytorch operations within the Quanto CUDA extension. It specifies that kernels can use both C++ and CUDA syntax, and can utilize any operation defined under the 'aten::' or 'c10::' namespaces.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cuda/README.md#2025-04-08_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\naten::\nc10::\n```\n\n----------------------------------------\n\nTITLE: Adding New Operation Implementation in Quanto CUDA Extension\nDESCRIPTION: This snippet outlines the steps required to add a new implementation for an operation defined in 'library./ops.py'. It involves adding source files, creating bindings, and providing an implementation that calls the binding.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/extensions/cuda/README.md#2025-04-08_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nlibrary./ops.py\n__init__.py\npybind_module.cpp\n```\n\n----------------------------------------\n\nTITLE: Accessing Quanto Operations in Python\nDESCRIPTION: Demonstrates how to access the quanto:: operations in Python using the torch.ops.quanto namespace.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/optimum/quanto/library/README.md#2025-04-08_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.ops.quanto\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Quanto via pip\nDESCRIPTION: Command to install the Optimum Quanto package using pip package manager.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/README.md#2025-04-08_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install optimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Installing Quanto Library from Source\nDESCRIPTION: Instructions for cloning the Quanto repository and installing it from source to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/quanto\ncd quanto\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Example-Specific Requirements\nDESCRIPTION: Command to install the required dependencies for the Stable Diffusion quantization examples.\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/examples/vision/StableDiffusion/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands for running specific tests and all tests in the project\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest test/<TEST_TO_RUN>.py\n```\n\n----------------------------------------\n\nTITLE: Running All Tests\nDESCRIPTION: Command to execute the complete test suite\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test\nDESCRIPTION: Command to run tests from a specific subfolder or test file\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest -sv ./test/<subfolder>/<test>.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment with Git\nDESCRIPTION: Initial setup commands for cloning the repository and configuring git remotes for contribution\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:<your Github handle>/optimum-quanto.git\ncd optimum-quanto\ngit remote add upstream https://github.com/huggingface/optimum-quanto.git\n```\n\n----------------------------------------\n\nTITLE: Creating Development Branch\nDESCRIPTION: Command to create and switch to a new development branch\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install the package in development mode with additional development dependencies\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Code Style Formatting\nDESCRIPTION: Command to apply automatic style corrections using black and ruff\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake style\n```\n\n----------------------------------------\n\nTITLE: Git Commit Commands\nDESCRIPTION: Basic git commands for staging and committing changes\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit add modified_file.py\ngit commit\n```\n\n----------------------------------------\n\nTITLE: Updating from Upstream\nDESCRIPTION: Commands to keep local repository in sync with upstream repository\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch upstream\ngit rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes\nDESCRIPTION: Command to push local changes to remote repository\nSOURCE: https://github.com/huggingface/optimum-quanto/blob/main/CONTRIBUTING.md#2025-04-08_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit push -u origin a-descriptive-name-for-my-changes\n```"
  }
]
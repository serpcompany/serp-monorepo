[
  {
    "owner": "deepspeedai",
    "repo": "deepspeed",
    "content": "TITLE: Checkpointing with DeepSpeed in Python\nDESCRIPTION: Shows how to save and load checkpoints using DeepSpeed. This includes handling custom client state data and advancing the data loader to the correct step after loading a checkpoint.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#load checkpoint\n_, client_sd = model_engine.load_checkpoint(args.load_dir, args.ckpt_id)\nstep = client_sd['step']\n\n#advance data loader to ckpt step\ndataloader_to_step(data_loader, step + 1)\n\nfor step, batch in enumerate(data_loader):\n\n    #forward() method\n    loss = model_engine(batch)\n\n    #runs backpropagation\n    model_engine.backward(loss)\n\n    #weight update\n    model_engine.step()\n\n    #save checkpoint\n    if step % args.save_interval:\n        client_sd['step'] = step\n        ckpt_id = loss.item()\n        model_engine.save_checkpoint(args.save_dir, ckpt_id, client_sd = client_sd)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration in JSON\nDESCRIPTION: Example of a DeepSpeed configuration file in JSON format. This configuration enables mixed precision training, sets the batch size, and configures the optimizer.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 8,\n  \"gradient_accumulation_steps\": 1,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.00015\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true\n  },\n  \"zero_optimization\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Engine in Python\nDESCRIPTION: Sets up the DeepSpeed engine by wrapping a PyTorch model and preparing it for distributed training. This includes handling the optimizer, data loader, and learning rate scheduler.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel_engine, optimizer, _, _ = deepspeed.initialize(args=cmd_args,\n                                                     model=model,\n                                                     model_parameters=params)\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed for Inference with PyTorch Models\nDESCRIPTION: This snippet demonstrates how to initialize DeepSpeed for inference, including setting up model parallelism, loading checkpoints, and injecting optimized kernels.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# create the model\nif args.pre_load_checkpoint:\n    model = model_class.from_pretrained(args.model_name_or_path)\nelse:\n    model = model_class()\n\n# create the tokenizer\ntokenizer = model_class.from_pretrained(args.model_name_or_path)\n...\n\nimport deepspeed\n\n# Initialize the DeepSpeed-Inference engine\nds_engine = deepspeed.init_inference(model,\n                                     tensor_parallel={\"tp_size\": world_size},\n                                     dtype=torch.half,\n                                     checkpoint=None if args.pre_load_checkpoint else args.checkpoint_json,\n                                     replace_with_kernel_inject=True)\nmodel = ds_engine.module\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\noutput = pipe('Input String')\n```\n\n----------------------------------------\n\nTITLE: Initializing Large Language Model with DeepSpeed ZeRO-3\nDESCRIPTION: Demonstrates how to allocate a large language model using DeepSpeed ZeRO-3 for memory-scalable initialization across data parallel groups and remote devices.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith deepspeed.zero.Init(data_parallel_group=mpu.get_data_parallel_group(),\n                             remote_device=get_args().remote_device,\n                             enabled=get_args().zero_stage==3):\n    model = GPT2Model(num_tokentypes=0, parallel_output=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO-Infinity with CPU Offloading\nDESCRIPTION: JSON configuration to enable ZeRO-Infinity (ZeRO stage 3) with CPU offloading in DeepSpeed, allowing training of trillion-scale models by offloading model states to CPU memory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"contiguous_gradients\": true,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_prefetch_bucket_size\": 1e7,\n        \"stage3_param_persistence_threshold\": 1e5,\n        \"reduce_bucket_size\": 1e7,\n        \"sub_group_size\": 1e9,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n         },\n        \"offload_param\": {\n            \"device\": \"cpu\"\n       }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: End-to-End GPT-NEO-2.7B Inference with DeepSpeed and HuggingFace\nDESCRIPTION: Complete example of using DeepSpeed inference with HuggingFace pipeline for text generation using the GPT-NEO-2.7B model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Filename: gpt-neo-2.7b-generation.py\nimport os\nimport deepspeed\nimport torch\nfrom transformers import pipeline\n\nlocal_rank = int(os.getenv('LOCAL_RANK', '0'))\nworld_size = int(os.getenv('WORLD_SIZE', '1'))\ngenerator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B',\n                     device=local_rank)\n\n\n\ngenerator.model = deepspeed.init_inference(generator.model,\n                                           tensor_parallel={\"tp_size\": world_size},\n                                           dtype=torch.float,\n                                           replace_with_kernel_inject=True)\n\nstring = generator(\"DeepSpeed is\", do_sample=True, min_length=50)\nif not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n    print(string)\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed Environment with DeepSpeed\nDESCRIPTION: Replaces the standard PyTorch distributed initialization with DeepSpeed's initialization function. This sets up the distributed environment for training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.init_distributed()\n```\n\n----------------------------------------\n\nTITLE: Training Loop with DeepSpeed in Python\nDESCRIPTION: Demonstrates a basic training loop using DeepSpeed, including forward pass, backward propagation, and weight updates. DeepSpeed automatically handles gradient averaging and loss scaling for mixed precision training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor step, batch in enumerate(data_loader):\n    #forward() method\n    loss = model_engine(batch)\n\n    #runs backpropagation\n    model_engine.backward(loss)\n\n    #weight update\n    model_engine.step()\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Training Engine\nDESCRIPTION: Shows how to initialize the DeepSpeed training engine with a model and its parameters. Returns the model engine and optimizer for training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/initialize.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_engine, optimizer, _, _ = deepspeed.initialize(args=cmd_args,\n                                                     model=net,\n                                                     model_parameters=net.parameters())\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Model and Optimizer Initialization\nDESCRIPTION: Python function to initialize DeepSpeed with model and optimizer parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_model_optimizer(args):\n    # Loading Model\n    model = BertMultiTask(args)\n\n    # Optimizer parameters\n    optimizer_parameters = prepare_optimizer_parameters(args, model)\n    model.network, optimizer, _, _ = deepspeed.initialize(args=args,\n                                         model=model.network,\n                                         model_parameters=optimizer_parameters,\n                                         dist_init_required=False)\n    return model, optimizer\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Mixed Precision (AMP) in DeepSpeed JSON\nDESCRIPTION: This snippet shows how to configure AMP training in DeepSpeed. It enables AMP and sets the optimization level. Additional parameters can be passed to AMP's initialize call.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"amp\": {\n    \"enabled\": true,\n    ...\n    \"opt_level\": \"O1\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing PipelineModule with Sequential Layers (Python)\nDESCRIPTION: Demonstrates how to create a PipelineModule from a sequential network, dividing it into pipeline stages for parallel processing.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnet = nn.Sequential(\n    nn.Linear(in_features, hidden_dim),\n    nn.ReLU(inplace=True),\n    nn.Linear(hidden_dim, out_features)\n)\nfrom deepspeed.pipe import PipelineModule\nnet = PipelineModule(layers=net, num_stages=2)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Environment Variables Configuration\nDESCRIPTION: Example of environment variable configuration in .deepspeed_env file for NCCL settings in cluster environments.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nNCCL_IB_DISABLE=1\nNCCL_SOCKET_IFNAME=eth0\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed RLHF Custom Training Pipeline\nDESCRIPTION: Example showing how to use DeepSpeed's RLHF APIs to create a custom training pipeline. Demonstrates initialization of the RLHF engine, trainer setup, and training loop implementation with experience generation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nengine = DeepSpeedRLHFEngine(\n  actor_model_name_or_path=args.actor_model_name_or_path,\n  critic_model_name_or_path=args.critic_model_name_or_path,\n  tokenizer=tokenizer,\n  num_total_iters=num_total_iters,\n  args=args)\n\ntrainer = DeepSpeedPPOTrainer(engine=engine, args=args)\n\nfor prompt_batch in prompt_train_dataloader:\n  out = trainer.generate_experience(prompt_batch)\n  actor_loss, critic_loss = trainer.train_rlhf(out)\n```\n\n----------------------------------------\n\nTITLE: Basic ZeRO-3 Configuration with AdamW Optimizer\nDESCRIPTION: Configuration to enable ZeRO stage 3 optimization with AdamW optimizer, including FP16 training and parameter settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n        \"lr\": 0.001,\n        \"betas\": [\n            0.8,\n            0.999\n        ],\n        \"eps\": 1e-8,\n        \"weight_decay\": 3e-7\n        }\n    },\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Profiling BERT Model with DeepSpeed\nDESCRIPTION: Example showing how to profile a BERT model using DeepSpeed's get_model_profile function. Includes input construction and model initialization with the HuggingFace transformers library.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nimport torch\nfrom transformers import BertForSequenceClassification, BertTokenizer\nfrom deepspeed.profiling.flops_profiler import get_model_profile\nfrom deepspeed.accelerator import get_accelerator\n\n\ndef bert_input_constructor(batch_size, seq_len, tokenizer):\n    fake_seq = \"\"\n    for _ in range(seq_len - 2):  # ignore the two special tokens [CLS] and [SEP]\n      fake_seq += tokenizer.pad_token\n    inputs = tokenizer([fake_seq] * batch_size,\n                       padding=True,\n                       truncation=True,\n                       return_tensors=\"pt\")\n    labels = torch.tensor([1] * batch_size)\n    inputs = dict(inputs)\n    inputs.update({\"labels\": labels})\n    return inputs\n\n\nwith get_accelerator().device(0):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n    batch_size = 4\n    seq_len = 128\n    enable_profile = True\n    if enable_profile:\n      flops, macs, params = get_model_profile(\n          model,\n          kwargs=bert_input_constructor(batch_size, seq_len, tokenizer),\n          print_profile=True,\n          detailed=True,\n      )\n    else:\n      inputs = bert_input_constructor((batch_size, seq_len), tokenizer)\n      outputs = model(inputs)\n```\n\n----------------------------------------\n\nTITLE: ZeRO-Offload DeepSpeed Configuration\nDESCRIPTION: JSON configuration for enabling ZeRO-Offload optimization in DeepSpeed. Sets ZeRO stage 2, enables CPU offloading for optimizer, and configures performance optimization parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-offload.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        },\n        \"contiguous_gradients\": true,\n        \"overlap_comm\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DeepSpeed-FP6 with LLaMa-2-70B in Python\nDESCRIPTION: This snippet demonstrates how to use DeepSpeed-FP6 for quantization and inference with the LLaMa-2-70B model. It imports the mii library, creates a pipeline with FP6 quantization, and generates responses.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mii\npipe = mii.pipeline(\"NousResearch/Llama-2-70b-hf\", quantization_mode='wf6af16')\nresponse = pipe([\"DeepSpeed is\", \"Seattle is\"], max_new_tokens=128)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Persistent Deployment Server with DeepSpeed-FastGen in Python\nDESCRIPTION: This code snippet shows how to create a persistent deployment server using DeepSpeed-FastGen. This is ideal for long-running and production applications, using a lightweight GRPC server.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mii\nmii.serve(\"mistralai/Mistral-7B-v0.1\")\n```\n\n----------------------------------------\n\nTITLE: Replacing Attention Module with DeepSpeed-Ulysses DistributedAttention in Python\nDESCRIPTION: This snippet shows how to update the attention module in a transformer model to use DeepSpeed-Ulysses DistributedAttention. It replaces the original CoreAttention with a distributed version that enables sequence parallelism.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds-sequence.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.sequence.layer import DistributedAttention\n\ndef __init__():\n    ...\n    self.local_attn = CoreAttention(self.layer_number, config, self.attn_mask_type)\n    self.dist_attn = DistributedAttention(self.local_attn, parallel_state.get_sequence_parallel_group())\n    ...\n\ndef forward():\n    ...\n    context_layer = self.dist_attn(query_layer, key_layer, value_layer, attention_mask)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Adam Optimizer in DeepSpeed\nDESCRIPTION: Configuration example for the Adam optimizer in DeepSpeed, showing standard parameters like learning rate, betas, epsilon, and weight decay. This snippet also notes additional DeepSpeed-specific parameters available for Adam optimization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.001,\n      \"betas\": [\n        0.8,\n        0.999\n      ],\n      \"eps\": 1e-8,\n      \"weight_decay\": 3e-7\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: ZeRO-3 Model Initialization\nDESCRIPTION: Code snippet showing how to initialize a large model with ZeRO-3 optimization using DeepSpeed's initialization context.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith deepspeed.zero.Init():\n    model = MyLargeModel()\n```\n\n----------------------------------------\n\nTITLE: Save DeepSpeed Checkpoint Implementation\nDESCRIPTION: Implementation of save_ds_checkpoint function that collects model states and RNG states before saving them through DeepSpeed's checkpoint system.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef save_ds_checkpoint(iteration, model, args):\n    \"\"\"Save a model checkpoint.\"\"\"\n\n    sd = {}\n    sd['iteration'] = iteration\n    # rng states.\n    if not args.no_save_rng:\n        sd['random_rng_state'] = random.getstate()\n        sd['np_rng_state'] = np.random.get_state()\n        sd['torch_rng_state'] = torch.get_rng_state()\n        sd['cuda_rng_state'] = get_accelerator().get_rng_state()\n        sd['rng_tracker_states'] = mpu.get_cuda_rng_tracker().get_states()\n\n    model.save_checkpoint(args.save, iteration, client_state = sd)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Attention in DeepSpeed\nDESCRIPTION: Sparse attention implementation using DeepSpeed's SparseSelfAttention module to replace dense attention operations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontext_layer = self.sparse_self_attention(query_layer, key_layer, value_layer, key_padding_mask=attention_mask)\n```\n\n----------------------------------------\n\nTITLE: Using DeepSpeed Inference with T5 Model and Custom Injection Policy\nDESCRIPTION: This example shows how to use DeepSpeed-Inference with a T5 model by specifying a custom injection policy for model parallelism.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# create the model\nimport transformers\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nimport deepspeed\n\npipe = pipeline(\"text2text-generation\", model=\"google/t5-v1_1-small\", device=local_rank)\n# Initialize the DeepSpeed-Inference engine\npipe.model = deepspeed.init_inference(\n    pipe.model,\n    tensor_parallel={\"tp_size\": world_size},\n    dtype=torch.float,\n    injection_policy={T5Block: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')}\n)\noutput = pipe('Input String')\n```\n\n----------------------------------------\n\nTITLE: Implementing New Automatic Tensor Parallelism in Python\nDESCRIPTION: Example of implementing the new automatic tensor parallelism method using DeepSpeed. This approach doesn't require an injection policy and automatically handles tensor parallelism for supported models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/automatic-tensor-parallelism.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport transformers\nimport deepspeed\nlocal_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n# create the model pipeline\npipe = transformers.pipeline(task=\"text2text-generation\", model=\"google/t5-v1_1-small\", device=local_rank)\n# Initialize the DeepSpeed-Inference engine\npipe.model = deepspeed.init_inference(\n    pipe.model,\n    mp_size=world_size,\n    dtype=torch.float\n)\noutput = pipe('Input String')\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Autotuning Parameters\nDESCRIPTION: Configuration options for DeepSpeed's autotuning feature. Includes settings for controlling experiment directories, profiling steps, tuning parameters, and performance metrics. Used to automatically optimize training performance.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"autotuning\": {\n    \"enabled\": false,\n    \"results_dir\": \"autotuning_results\",\n    \"exps_dir\": \"autotuning_exps\",\n    \"overwrite\": false,\n    \"metric\": \"throughput\",\n    \"start_profile_step\": 3,\n    \"end_profile_step\": 5,\n    \"fast\": true,\n    \"max_train_batch_size\": null,\n    \"mp_size\": 1,\n    \"num_tuning_micro_batch_sizes\": 3,\n    \"tuner_type\": \"model_based\",\n    \"tuner_early_stopping\": 5,\n    \"tuner_num_trials\": 50,\n    \"arg_mappings\": null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MixZ++ in Training Script\nDESCRIPTION: Python code showing how to initialize DeepSpeed with MixZ++ and properly handle LoRA model conversion. The key addition is calling quantize_nontrainable_params() after converting to LoRA model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixed_precision_zeropp.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer, _, lr_scheduler = deepspeed.initialize(\n    model=model,\n    optimizer=optimizer,\n    args=args,\n    config=ds_config,\n    lr_scheduler=lr_scheduler,\n    dist_init_required=True)\n# ...\n# (the custom code to convert base model to LoRA model)\n# ...\n# call DeepSpeed engine again to identify LoRA frozen parameters\nmodel.optimizer.quantize_nontrainable_params()\n# ...\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed Inference on Multiple GPUs\nDESCRIPTION: Command to launch DeepSpeed inference on multiple GPUs using the DeepSpeed launcher.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_gpus 2 inference.py\n```\n\n----------------------------------------\n\nTITLE: Complete Example of DeepSpeed Autotuning for Language Model Training\nDESCRIPTION: A complete example showing how to use DeepSpeed autotuning with a Hugging Face language model training script. The example demonstrates running autotuning on 8 GPUs across 1 node for GPT-2 language model training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n  deepspeed --autotuning run --num_nodes=1 --num_gpus=8 $HF_PATH/transformers/examples/pytorch/language-modeling/run_clm.py \\\n  --deepspeed $DS_CONFIG_PATH \\\n  --model_name_or_path gpt2 \\\n  --do_train \\\n  --do_eval \\\n  --fp16 \\\n  --per_device_train_batch_size 8 \\\n  --gradient_accumulation_steps 1 \\\n  ...\n```\n\n----------------------------------------\n\nTITLE: Deploying Stable Diffusion Model with MII-Public\nDESCRIPTION: This code snippet demonstrates how to deploy a Stable Diffusion model using MII-Public deployment. It creates a lightweight GRPC server and provides a GRPC inference endpoint for queries.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-10-11-mii.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mii\nmii.deploy(task=\"text-to-image\",\n           model=\"CompVis/stable-diffusion-v1-4\",\n           deployment_name=\"sd-deployment\")\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO Memory Optimizations in DeepSpeed\nDESCRIPTION: JSON configuration structure for ZeRO memory optimizations in DeepSpeed. Includes settings for different optimization stages (0-3), memory management parameters, communication options, and offloading configurations for both parameters and optimizer states.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"zero_optimization\": {\n    \"stage\": [0|1|2|3],\n    \"allgather_partitions\": [true|false],\n    \"allgather_bucket_size\": 5e8,\n    \"overlap_comm\": false,\n    \"reduce_scatter\": [true|false],\n    \"reduce_bucket_size\": 5e8,\n    \"contiguous_gradients\" : [true|false],\n    \"offload_param\": {\n      ...\n    },\n    \"offload_optimizer\": {\n      ...\n    },\n    \"stage3_max_live_parameters\" : 1e9,\n    \"stage3_max_reuse_distance\" : 1e9,\n    \"stage3_prefetch_bucket_size\" : 5e8,\n    \"stage3_param_persistence_threshold\" : 1e6,\n    \"sub_group_size\" : 1e12,\n    \"elastic_checkpoint\" : [true|false],\n    \"stage3_gather_16bit_weights_on_model_save\": [true|false],\n    \"ignore_unused_parameters\": [true|false],\n    \"round_robin_gradients\": [true|false],\n    \"zero_hpz_partition_size\": 1,\n    \"zero_quantized_weights\": [true|false],\n    \"zero_quantized_gradients\": [true|false],\n    \"log_trace_cache_warnings\": [true|false]\n    }\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Transformer Configuration\nDESCRIPTION: Example code showing how to instantiate transformer kernel layers using BERT-Large configuration settings with Pre-LN architecture. Sets up 24 layers with 1024 hidden dimension, sequence length of 128, and batch size of 64.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/transformer_kernel.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = DeepSpeedTransformerConfig(batch_size = 64,\n                                    max_seq_length = 128,\n                                    hidden_size = 1024,\n                                    heads = 16,\n                                    attn_dropout_ratio = 0.1,\n                                    hidden_dropout_ratio = 0.1,\n                                    num_hidden_layers = 24,\n                                    initializer_range = 0.02,\n                                    local_rank = 0,\n                                    seed = 1234,\n                                    fp16 = True,\n                                    pre_layer_norm=True,\n                                    attn_dropout_checkpoint=False,\n                                    normalize_invertible=False,\n                                    gelu_checkpoint=False)\nself.layer = nn.ModuleList([\n    copy.deepcopy(DeepSpeedTransformerLayer(cuda_config))\n    for _ in range(config.num_hidden_layers)\n])\n```\n\n----------------------------------------\n\nTITLE: Enabling ZeRO Stage 1 in DeepSpeed JSON Config\nDESCRIPTION: JSON configuration to enable ZeRO stage 1 optimization in DeepSpeed, partitioning optimizer states across devices.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 1,\n        \"reduce_bucket_size\": 5e8\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling ZeRO Stage 2 in DeepSpeed JSON Config\nDESCRIPTION: JSON configuration to enable ZeRO stage 2 optimization in DeepSpeed, partitioning gradients and optimizer states with additional memory optimizations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"contiguous_gradients\": true,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 5e8,\n        \"allgather_bucket_size\": 5e8\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizer Offloading in DeepSpeed (JSON)\nDESCRIPTION: JSON configuration for enabling and customizing ZeRO optimization offloading in DeepSpeed. It allows offloading optimizer computation to CPU and state to CPU/NVMe, with various options for device selection, memory pinning, and NVMe-specific settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"offload_optimizer\": {\n    \"device\": \"[cpu|nvme]\",\n    \"nvme_path\": \"/local_nvme\",\n    \"pin_memory\": [true|false],\n    \"ratio\": 0.3,\n    \"buffer_count\": 4,\n    \"fast_init\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Loop Implementation\nDESCRIPTION: Python code showing the backward pass implementation with DeepSpeed integration.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif args.deepspeed:\n    model.network.backward(loss)\nelse:\n    if args.fp16:\n        optimizer.backward(loss)\n    else:\n        loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Inference Engine in Python\nDESCRIPTION: This snippet demonstrates how to initialize the DeepSpeed inference engine using the init_inference function. It requires a model and a configuration dictionary as inputs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/inference-init.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nengine = deepspeed.init_inference(model=net, config=config)\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Model Engines for DCGAN in Python\nDESCRIPTION: Creates two DeepSpeed model engines, one for the discriminator network and one for the generator network, along with their respective optimizers. This setup enables distributed training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_engineD, optimizerD, _, _ = deepspeed.initialize(args=args, model=netD, model_parameters=netD.parameters(), optimizer=optimizerD)\nmodel_engineG, optimizerG, _, _ = deepspeed.initialize(args=args, model=netG, model_parameters=netG.parameters(), optimizer=optimizerG)\n```\n\n----------------------------------------\n\nTITLE: Configuring MixZ++ with DeepSpeed JSON Settings\nDESCRIPTION: DeepSpeed configuration snippet showing how to enable Mixed Precision ZeRO++ optimizations including quantized weights and hierarchical partitioning. The zero_hpz_partition_size should match GPUs per node for multi-node training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixed_precision_zeropp.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"...\"\n        \"zero_quantized_nontrainable_weights\": true,\n        \"zero_hpz_partition_size\": 16,\n        \"...\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Model Parallelism in Python with DeepSpeed\nDESCRIPTION: Shows the required Python functions that need to be implemented in a 'model parallelism unit' (mpu) to enable custom model parallelism with DeepSpeed. Includes functions for getting ranks and groups.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmpu.get_model_parallel_rank()\nmpu.get_model_parallel_group()\nmpu.get_model_parallel_world_size()\n\nmpu.get_data_parallel_rank()\nmpu.get_data_parallel_group()\nmpu.get_data_parallel_world_size()\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory-Efficient AlexNet with LayerSpec in DeepSpeed\nDESCRIPTION: Example showing how to implement AlexNet using DeepSpeed's LayerSpec for memory-efficient model construction. LayerSpec delays module construction until layer partitioning, reducing CPU memory requirements from Nx to 1x model size for N GPUs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.pipe import PipelineModule, LayerSpec\nclass AlexNetPipe(PipelineModule):\n    def __init__(self, num_classes=10, **kwargs):\n        self.num_classes = num_classes\n        specs = [\n            LayerSpec(nn.Conv2d, 3, 64, kernel_size=11, stride=4, padding=2),\n            LayerSpec(nn.ReLU, inplace=True),\n            ...\n            LayerSpec(nn.ReLU, inplace=True),\n            LayerSpec(nn.Linear, 4096, self.num_classes),\n        ]\n        super().__init__(layers=specs, loss_fn=nn.CrossEntropyLoss(), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Profiling Model Training Loop with PyTorch Profiler in Python\nDESCRIPTION: This snippet demonstrates how to profile a model training loop using PyTorch Profiler. It sets up a profiling schedule, handles traces, and profiles each step of the training process including forward pass, backpropagation, and weight updates.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pytorch-profiler.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\nwith torch.profiler.profile(\n    schedule=torch.profiler.schedule(\n        wait=5, # During this phase profiler is not active.\n        warmup=2, # During this phase profiler starts tracing, but the results are discarded.\n        active=6, # During this phase profiler traces and records data.\n        repeat=2), # Specifies an upper bound on the number of cycles.\n    on_trace_ready=tensorboard_trace_handler,\n    with_stack=True # Enable stack tracing, adds extra profiling overhead.\n) as profiler:\n    for step, batch in enumerate(data_loader):\n        print(\"step:{}\".format(step))\n\n        #forward() method\n        loss = model_engine(batch)\n\n        #runs backpropagation\n        model_engine.backward(loss)\n\n        #weight update\n        model_engine.step()\n        profiler.step() # Send the signal to the profiler that the next step has started.\n```\n\n----------------------------------------\n\nTITLE: Adding Sequence Parallel Communication Group in Python\nDESCRIPTION: This code snippet demonstrates how to create a sequence parallel communication group for DeepSpeed-Ulysses. It initializes the model parallel environment and sets up the necessary communication groups for sequence parallelism.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds-sequence.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_model_parallel(\n    ...\n    sequence_parallel_size,\n    ...\n):\n    ...\n    num_sequence_parallel_groups: int = world_size // sequence_parallel_size\n    num_sequence_data_parallel_groups: int = world_size // sequence_parallel_size // data_parallel_size\n    ...\n    global _SEQUENCE_PARALLEL_GROUP\n    for i in range(num_sequence_parallel_groups):\n        ranks = range(i * sequence_parallel_size,\n                      (i + 1) * sequence_parallel_size)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _SEQUENCE_PARALLEL_GROUP = group\n\ndef get_sequence_parallel_group():\n    \"\"\"Get the sequence parallel group the caller rank belongs to.\"\"\"\n    return _SEQUENCE_PARALLEL_GROUP\n```\n\n----------------------------------------\n\nTITLE: MPU Transformer Checkpointing Configuration\nDESCRIPTION: Configuration code for replacing standard checkpointing functions with DeepSpeed's versions in the transformer implementation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif deepspeed.checkpointing.is_configured():\n    global get_cuda_rng_tracker, checkpoint\n    get_cuda_rng_tracker = deepspeed.checkpoint.get_cuda_rng_tracker\n    checkpoint = deepspeed.checkpointing.checkpoint\n```\n\n----------------------------------------\n\nTITLE: Saving 16-bit Model Weights with DeepSpeed\nDESCRIPTION: Python code snippet for saving a 16-bit model when using DeepSpeed ZeRO-3 optimization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif self.deepspeed:\n    self.deepspeed.save_16bit_model(output_dir, output_file)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration for BERT Pre-training with 1-bit Adam\nDESCRIPTION: Configuration file for BERT-large pre-training using 1-bit Adam optimizer with sequence length 128. Specifies batch sizes, optimizer parameters, gradient clipping, and FP16 settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"train_micro_batch_size_per_gpu\": 16,\n  \"steps_per_print\": 100,\n  \"prescale_gradients\": false,\n  \"optimizer\": {\n    \"type\": \"OneBitAdam\",\n    \"params\": {\n      \"lr\": 4e-4,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false,\n      \"freeze_step\": 23000,\n      \"comm_backend_name\": \"nccl\"\n    }\n  },\n  \"gradient_clipping\": 1.0,\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Jointly Training Multiple Models with Shared Loss using DeepSpeed in Python\nDESCRIPTION: This snippet demonstrates how to jointly train multiple models on a shared loss value using DeepSpeed. It creates multiple DeepSpeedEngines, calculates a shared loss, and performs backward propagation and optimization steps for all models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/training.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_engines = [engine for engine, _, _, _ in [deepspeed.initialize(m, ...,) for m in models]]\nfor batch in data_loader:\n    losses = [engine(batch[0], batch[1]) for engine in model_engines]\n    loss = sum(l / (i + 1) for i, l in enumerate(losses))\n    loss.backward()\n\n    for engine in model_engines:\n        engine._backward_epilogue()\n\n    for engine in model_engines:\n        engine.step()\n\n    for engine in model_engines:\n        engine.optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Gathering Embeddings for Initialization with DeepSpeed ZeRO\nDESCRIPTION: Shows how to use GatheredParameters to initialize embedding weights in a distributed setting, ensuring consistent initialization across all ranks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nself.position_embeddings = torch.nn.Embedding(...)\nwith deepspeed.zero.GatheredParameters(self.position_embeddings.weight,\n                                           modifier_rank=0):\n    # Initialize the position embeddings.\n    self.init_method(self.position_embeddings.weight)\n\n...\n\nself.tokentype_embeddings = torch.nn.Embedding(...)\nwith deepspeed.zero.GatheredParameters(self.tokentype_embeddings.weight,\n                                    modifier_rank=0):\n    # Initialize the token-type embeddings.\n    self.init_method(self.tokentype_embeddings.weight)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mixed Precision Training in DeepSpeed JSON\nDESCRIPTION: Enables 16-bit (FP16) training by configuring parameters in the DeepSpeed JSON configuration file. Specifies settings like loss scaling and hysteresis.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"consecutive_hysteresis\": false,\n    \"min_loss_scale\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Example Usage of DeepSpeed State Offloading\nDESCRIPTION: Demonstrates a common pattern for offloading FP32 parameters and optimizer states to CPU memory when GPU memory is needed for other operations, then reloading them when required again.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Offload after forward, backward, and step\nds_engine.offload_states(include=[OffloadStateTypeEnum.hp_params, OffloadStateTypeEnum.optim_states])\n\n# Do something requiring a lot of device memory\n...\n# Load states back to device memory\nds_engine.reload_states()\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed-MoE for Inference in Python\nDESCRIPTION: This snippet demonstrates how to initialize a DeepSpeed-MoE model for inference, including setting up expert parallelism, loading checkpoints, and enabling high-performance inference kernels.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-inference.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed\nimport torch.distributed as dist\n\n# Set expert-parallel size\nworld_size = dist.get_world_size()\nexpert_parallel_size = min(world_size, args.num_experts)\n\n# create the MoE model\nmoe_model = get_model(model, ep_size=expert_parallel_size)\n...\n\n# Initialize the DeepSpeed-Inference engine\nds_engine = deepspeed.init_inference(moe_model,\n                                     mp_size=tensor_slicing_size,\n                                     dtype=torch.half,\n                                     moe_experts=args.num_experts,\n                                     checkpoint=args.checkpoint_path,\n                                     replace_with_kernel_inject=True,)\nmodel = ds_engine.module\noutput = model('Input String')\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Inference in Python\nDESCRIPTION: This code block shows an example configuration dictionary for DeepSpeed inference. It includes settings for kernel injection, tensor parallelism, data type, and CUDA graph enablement.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/inference-init.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n\t\"kernel_inject\": True,\n\t\"tensor_parallel\": {\"tp_size\": 4},\n\t\"dtype\": \"fp16\",\n\t\"enable_cuda_graph\": False\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Inference Test With Tensor Parallelism\nDESCRIPTION: Bash command for running inference testing with DeepSpeed tensor parallelism enabled for compatible models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/automatic-tensor-parallelism.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_gpus <num_gpus> DeepSpeedExamples/inference/huggingface/text-generation/inference-test.py --name <model> --batch_size <batch_size> --test_performance --ds_inference\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory-centric Tiling with DeepSpeed ZeRO-Infinity\nDESCRIPTION: Demonstrates how to replace standard Linear layers with TiledLinear layers to reduce memory usage in large Transformer models using ZeRO-Infinity.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nself.dense_h_to_4h = deepspeed.zero.TiledLinearReturnBias(\n    args.hidden_size,\n    4 * args.hidden_size,\n    in_splits=args.tile_factor,\n    out_splits=4*args.tile_factor,\n    linear_cls=mpu.ColumnParallelLinear,\n    gather_output=False,\n    init_method=init_method,\n    skip_bias_add=True)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration JSON\nDESCRIPTION: JSON configuration file for DeepSpeed specifying training parameters, optimizer settings, and FP16 options.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"train_micro_batch_size_per_gpu\": 64,\n  \"steps_per_print\": 1000,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 2e-4,\n      \"max_grad_norm\": 1.0,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Weight Quantization Example in DeepSpeed\nDESCRIPTION: Commands to install requirements and run the weight quantization example script in DeepSpeedExamples. This demonstrates how to use weight quantization in practice.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/quant_weight.sh\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Autotuning Configuration\nDESCRIPTION: JSON configuration file for DeepSpeed showing how to enable autotuning and map configuration parameters to training script arguments.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/autotuning.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"fp16\": {\n    \"enabled\": true\n  },\n  \"autotuning\": {\n    \"enabled\": true,\n    \"arg_mappings\": {\n      \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n      \"gradient_accumulation_steps \": \"--gradient_accumulation_steps\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Flops Profiler in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the DeepSpeed Flops Profiler in the deepspeed_config file. It includes various options such as enabling the profiler, setting the profile step, module depth, and output preferences.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flops_profiler\": {\n    \"enabled\": true,\n    \"recompute_fwd_factor\": 0.0,\n    \"profile_step\": 1,\n    \"module_depth\": -1,\n    \"top_modules\": 1,\n    \"detailed\": true,\n    \"output_file\": null\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running DeepNVMe Performance Tuning\nDESCRIPTION: Example of using ds_nvme_tune utility to automatically discover optimal DeepNVMe configurations for GPU memory and NVMe SSD transfers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ ds_nvme_tune --nvme_dir /local_nvme --gpu\nRunning DeepNVMe performance tuning on ['/local_nvme/']\nBest performance (GB/sec): read =  3.69, write =  3.18\n{\n   \"aio\": {\n      \"single_submit\": \"false\",\n      \"overlap_events\": \"true\",\n      \"intra_op_parallelism\": 8,\n      \"queue_depth\": 32,\n      \"block_size\": 1048576\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed environment report\nDESCRIPTION: Validates the DeepSpeed installation and shows which ops are compatible with the current machine. Useful for debugging installation or compatibility issues.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nds_report\n```\n\n----------------------------------------\n\nTITLE: Pipeline Training Loop with DeepSpeed (Python)\nDESCRIPTION: Demonstrates how to use DeepSpeed's pipeline engine for training, which handles the complexity of interleaved forward and backward passes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_iter = iter(train_loader)\nloss = engine.train_batch(data_iter=train_iter)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Launch Script for 10B GPT-2 Model\nDESCRIPTION: Bash script modifications to configure training of a 10 billion parameter GPT-2 model using DeepSpeed with ZeRO stage 2 optimization and activation checkpointing.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n       --model-parallel-size 1 \\\n       --num-layers 50 \\\n       --hidden-size 4096 \\\n       --num-attention-heads 32 \\\n       --batch-size 1 \\\n       --deepspeed_config ds_zero_stage_2.config \\\n       --checkpoint-activations\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO-Inference with CPU Offloading in DeepSpeed\nDESCRIPTION: JSON configuration snippet for enabling ZeRO-Inference with offloading to CPU memory in DeepSpeed. This sets the ZeRO optimization stage to 3 and configures parameter offloading to the CPU.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-09-10-zero-inference.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            ...\n        },\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed-Ulysses Training Scripts in Bash\nDESCRIPTION: These bash commands demonstrate how to run example scripts for training GPT-3 like models with very long sequences using DeepSpeed-Ulysses. They use pre-configured scripts for 1.3B and 30B parameter models with 32k sequence length.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds-sequence.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMegatron-DeepSpeed/examples_deepspeed/sequence_parallel$ bash ds_pretrain_gpt_1.3B_seq_parallel_32k.sh\nMegatron-DeepSpeed/examples_deepspeed/sequence_parallel$ bash ds_pretrain_gpt_30B_seq_parallel_32k.sh\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed via pip\nDESCRIPTION: Command to install DeepSpeed from PyPI package repository. This is the recommended installation method for most users.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Creating AsyncIO Handle in Python\nDESCRIPTION: Creates an aio_handle for asynchronous I/O operations using DeepSpeed's AsyncIOBuilder. This handle works with both host and device tensors.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.ops.op_builder import AsyncIOBuilder\naio_handle = AsyncIOBuilder().load().aio_handle()\n```\n\n----------------------------------------\n\nTITLE: Implementing MSA Column-wise Attention with DS4Sci_EvoformerAttention in Python\nDESCRIPTION: Example of using DS4Sci_EvoformerAttention for MSA column-wise attention, which allows elements belonging to the same target residue to exchange information.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Q, K, V: [Batch, N_res, N_seq, Head, Dim]\n# res_mask: [Batch, N_seq, 1, 1, N_res]\nout = DS4Sci_EvoformerAttention(Q, K, V, [res_mask])\n```\n\n----------------------------------------\n\nTITLE: Profiling Memory Consumption with PyTorch Profiler in Python\nDESCRIPTION: This snippet illustrates how to profile memory consumption of a model using PyTorch Profiler. It enables memory profiling to record the amount of memory used by the model's tensors during the execution of operators.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pytorch-profiler.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith profile(activities=[ProfilerActivity.CUDA],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed BERT Training with Bash\nDESCRIPTION: Example command for launching DeepSpeed BERT training on multiple nodes with various configuration options, including the DeepSpeed Transformer Kernel.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_nodes 4  \\\n    deepspeed_train.py \\\n    --deepspeed \\\n    --deepspeed_config  deepspeed_bsz4096_adam_config.json \\\n    --cf /path-to-deepspeed/examples/tests/bing_bert/bert_large_adam_seq128.json \\\n    --train_batch_size 4096  \\\n    --max_seq_length 128 \\\n    --gradient_accumulation_steps 4 \\\n    --max_grad_norm 1.0 \\\n    --fp16 \\\n    --loss_scale 0 \\\n    --delay_allreduce \\\n    --max_steps 32 \\\n    --print_steps 1 \\\n    --deepspeed_transformer_kernel \\\n    --output_dir <output_directory>\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Flops Profiler in JSON\nDESCRIPTION: This JSON snippet shows how to configure the DeepSpeed Flops Profiler in the DeepSpeed configuration file. It enables the profiler, sets the profile step, module depth, number of top modules to display, and other options.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flops_profiler\": {\n    \"enabled\": true,\n    \"profile_step\": 1,\n    \"module_depth\": -1,\n    \"top_modules\": 1,\n    \"detailed\": true,\n    \"output_file\": null\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO Offloading in DeepSpeed (Python)\nDESCRIPTION: This snippet shows different configurations for ZeRO offloading in DeepSpeed. It includes options for offloading parameters and optimizer states to different devices.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"offload_param\": {\"device\": \"none\"}, \"offload_optimizer\": {\"device\": \"none\"}\n```\n\nLANGUAGE: python\nCODE:\n```\n\"offload_param\": {\"device\": \"cpu\"}, \"offload_optimizer\": {\"device\": \"cpu\"}\n```\n\nLANGUAGE: python\nCODE:\n```\n\"offload_param\": {\"device\": \"none\"}, \"offload_optimizer\": {\"device\": \"cpu\"}\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed Training with Multi-Node Configuration\nDESCRIPTION: Command-line examples for launching DeepSpeed training jobs across multiple nodes. Shows how to use hostfiles, specify node and GPU counts, and include/exclude specific resources.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --hostfile=myhostfile <client_entry.py> <client args> \\\n  --deepspeed --deepspeed_config ds_config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_nodes=2 \\\n\t<client_entry.py> <client args> \\\n\t--deepspeed --deepspeed_config ds_config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --exclude=\"worker-2:0@worker-3:0,1\" \\\n\t<client_entry.py> <client args> \\\n\t--deepspeed --deepspeed_config ds_config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --include=\"worker-2:0,1\" \\\n\t<client_entry.py> <client args> \\\n\t--deepspeed --deepspeed_config ds_config.json\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Transformer Kernel Configuration\nDESCRIPTION: Code showing how to add and configure DeepSpeed's optimized transformer kernel for improved training throughput.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparser.add_argument(\n    '--deepspeed_transformer_kernel',\n    default=False,\n    action='store_true',\n    help='Use DeepSpeed transformer kernel to accelerate.'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Data Efficiency Settings in JSON\nDESCRIPTION: Complete configuration example for DeepSpeed Data Efficiency Library showing settings for both curriculum learning and random layerwise token dropping (random-LTD). Includes detailed parameters for data routing, sampling schedules, and curriculum metrics.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data_efficiency\": {\n    \"enabled\": true,\n    \"seed\": 1234,\n    \"data_routing\": {\n      \"enabled\": true,\n      \"random_ltd\":{\n        \"enabled\": true,\n        \"total_layer_num\": 24,\n        \"random_ltd_layer_num\": 22,\n        \"random_ltd_layer_id\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n        \"model_mask_name\": \"attention_mask\",\n        \"model_type\": \"decoder\",\n        \"hidden_state_order\": \"seq_batch_dim\",\n        \"random_ltd_schedule\": {\n          \"min_value\": 128,\n          \"max_value\": 2048,\n          \"schedule_type\":\"fixed_linear\",\n          \"schedule_config\": {\n            \"require_steps\": 200000,\n            \"seq_per_step\": 16\n          }\n        }\n      }\n    },\n    \"data_sampling\": {\n      \"enabled\": true,\n      \"num_epochs\": 1,\n      \"num_workers\": 0,\n      \"curriculum_learning\": {\n        \"enabled\": true,\n        \"data_cluster_path\": \"/path/to/data_clusters\",\n        \"curriculum_metrics\": {\n          \"vocabularyrarity\": {\n            \"index_to_sample_path\": \"/path/to/index_to_sample\",\n            \"index_to_metric_path\": \"/path/to/index_to_metric\",\n            \"difficulty_type\": \"percentile\",\n            \"clustering_type\": \"schedule_based\",\n            \"min_difficulty\": 1,\n            \"max_difficulty\": 100,\n            \"schedule_type\": \"fixed_root\",\n            \"schedule_config\": {\n              \"total_curriculum_step\": 110000,\n              \"difficulty_step\": 1,\n              \"root_degree\": 2\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Inference with Quantized Models\nDESCRIPTION: Example of initializing DeepSpeed inference with quantized int8 models, specifying quantization settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed\nmodel = deepspeed.init_inference(model,\n                                 checkpoint='./checkpoint.json',\n                                 dtype=torch.int8,\n                                 quantization_setting=(quantize_groups,\n                                                       mlp_extra_grouping)\n                                )\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed-MoE for Text Generation in Bash\nDESCRIPTION: This bash script shows the configuration options for running a text generation example using an MoE model with DeepSpeed. It specifies model parameters, expert configuration, and enables DeepSpeed inference optimizations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-inference.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngenerate_samples_gpt.py \\\n       --tensor-model-parallel-size 1 \\\n       --num-experts ${experts} \\\n       --num-layers 24 \\\n       --hidden-size 2048 \\\n       --num-attention-heads 32 \\\n       --max-position-embeddings 1024 \\\n       --tokenizer-type GPT2BPETokenizer \\\n       --load $checkpoint_path \\\n       --fp16 \\\n       --ds-inference \\\n```\n\n----------------------------------------\n\nTITLE: Reloading DeepSpeed Engine States from Offload Device\nDESCRIPTION: Function that reloads previously offloaded engine states back to their original device. This function allows for optional asynchronous reloading via the non_blocking parameter.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef reload_states(self, non_blocking: bool = False) -> None:\n    \"\"\"Reload the engine states to the original device.\n\n    Arguments:\n        non_blocking: Optional. Whether to offload the states asynchronously.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Optimizing LLM Inference with Intel Extension for PyTorch\nDESCRIPTION: Python code snippet to optimize a DeepSpeed model using Intel Extension for PyTorch. This enables CPU inference to benefit from both DeepSpeed AutoTP and Intel's LLM optimizations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nipex_model = ipex.llm.optimize(deepspeed_model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Triton Compiler in DeepSpeed for BERT Model Inference\nDESCRIPTION: This snippet demonstrates how to initialize the DeepSpeed inference engine with Triton compiler optimization for a BERT model. It enables float16 precision, kernel injection, CUDA graph, and Triton autotuning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-triton/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipe = pipeline('fill-mask', model='bert-base-cased', framework='pt', device=0)\npipe.model = deepspeed.init_inference(pipe.model,\n                                        dtype=torch.float16,\n                                        replace_with_kernel_inject=True,\n                                        enable_cuda_graph=True,\n                                        use_triton=True,\n                                        triton_autotune=True,\n                                        max_out_tokens=pipe.tokenizer.model_max_length)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running a Model with DeepSpeed Accelerator Interface in Python\nDESCRIPTION: This snippet demonstrates how to use the device-agnostic interface in DeepSpeed to load a model, initialize inference, and prepare it for evaluation. It uses the DeepSpeed accelerator to determine the device name and sets up the model for distributed inference.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/intel-inference/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed\nfrom deepspeed.accelerator import get_accelerator\n...\n# load model checkpoint into model\nmodel = model.eval().to(get_accelerator().device_name())\n\nds_world_size = int(os.getenv('WORLD_SIZE', '0'))\n\nengine = deepspeed.init_inference(model=model, mp_size=ds_world_size, \\\n  dtype=torch.bfloat16, replace_method=\"auto\", \\\n  replace_with_kernel_inject=False)\n\nmodel = engine.module\n...\n# evaluate model\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Engine with Curriculum Learning in Python\nDESCRIPTION: Example Python code for initializing the DeepSpeed engine with curriculum learning. It shows how to provide the train dataset and use the returned dataloader with curriculum learning capability.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# In megatron/training.py, function setup_model_and_optimizer:\nengine, optimizer, _, _ = deepspeed.initialize(\n    model=model,\n    optimizer=optimizer,\n    args=args,\n    lr_scheduler=lr_scheduler,\n    dist_init_required=False,\n    config_params=deepspeed_config,\n    model_parameters=model_params,\n    training_data=train_data  # Provide train dataset here\n)\n# Use the dataloader returned by initialize\ndataloader = engine.deepspeed_io\n```\n\n----------------------------------------\n\nTITLE: MoE Training Configuration Parameters\nDESCRIPTION: Core configuration parameters for standard MoE model training, including expert count, parallelism settings, and capacity factors.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-nlg.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--num-experts\n--moe-expert-parallel-size\n--moe-loss-coeff\n--moe-train-capacity-factor\n--moe-eval-capacity-factor\n--moe-min-capacity\n--disable-moe-token-dropping\n```\n\n----------------------------------------\n\nTITLE: Initialize MoE Layer with PyTorch\nDESCRIPTION: Examples showing how to modify existing model layers to use MoE, comparing original model config with MoE-enabled version.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    self.fc3 = nn.Linear(84, 10)\n\n# Updated with MoE Layers\n    self.fc3 = nn.Linear(84, 84)\n    self.fc3 = deepspeed.moe.layer.MoE(hidden_size=84, expert=self.fc3, num_experts=args.num_experts, ep_size=<desired expert-parallel world size> ...)\n    self.fc4 = nn.Linear(84, 10)\n```\n\n----------------------------------------\n\nTITLE: Complete MoE Implementation Example\nDESCRIPTION: Full example showing MoE layer implementation with specific world size and expert parallel configuration.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport deepspeed\nimport deepspeed.utils.groups as groups\nfrom deepspeed.moe.layer import MoE\n\nWORLD_SIZE = 4\nEP_WORLD_SIZE = 2\nEXPERTS = 8\n\nfc3 = torch.nn.Linear(84, 84)\nfc3 = MoE(hidden_size=84, expert=self.fc3, num_experts=EXPERTS, ep_size=EP_WORLD_SIZE, k=1)\nfc4 = torch.nn.Linear(84, 10)\n```\n\n----------------------------------------\n\nTITLE: Creating a Non-Persistent Pipeline in Python with DeepSpeed-FastGen\nDESCRIPTION: This snippet demonstrates how to create a non-persistent pipeline for text generation using the Mistral-7B model. It's a quick way to get started with DeepSpeed-FastGen for temporary interactive sessions.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mii import pipeline\npipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\noutput = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Launching Multi-Node DeepSpeed Training via Command Line\nDESCRIPTION: Demonstrates how to launch DeepSpeed training across multiple nodes using a hostfile to specify resources. Allows easy switching between single-GPU, multi-GPU, and multi-node execution.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --hostfile=<hostfile> \\\n\t<client_entry.py> <client args> \\\n\t--deepspeed --deepspeed_config ds_config.json\n```\n\n----------------------------------------\n\nTITLE: Integrating DeepSpeed Communication Logging in Python\nDESCRIPTION: Python code snippet demonstrating how to integrate DeepSpeed communication logging into an existing training loop. It shows importing the DeepSpeed communication module, using it for all-reduce operations, and calling the log summary function at the end of each epoch.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/comms-logging.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed.comm as dist\n\n# Note that any communication operations using `import torch.distributed as dist` calls can remain unchanged, and will be automatically logged under deepspeed.comm!\ndist.all_reduce(tensor)\n\nfor epoch in range(2):\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader):\n        pre = time.time()\n        inputs, labels = data[0].to(model_engine.local_rank), data[1].to(\n            model_engine.local_rank)\n        if fp16:\n            inputs = inputs.half()\n        outputs = model_engine(inputs)\n        loss = criterion(outputs, labels)\n\n        model_engine.backward(loss)\n        model_engine.step()\n        post = time.time()\n    # Step 3: Call `deepspeed.comm.log_summary()`\n    dist.log_summary()\n```\n\n----------------------------------------\n\nTITLE: Running BERT Pre-training with DeepSpeed and MPI\nDESCRIPTION: Example command for launching BERT pre-training using mpirun with 32 GPUs across 8 nodes using MVAPICH2 and InfiniBand support.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np 32 -ppn 4 -hostfile hosts -env MV2_USE_CUDA=1 -env MV2_SUPPORT_DL=1 -env MV2_ENABLE_AFFINITY=0 -env MV2_SMP_USE_CMA=0 bash ds_train_bert_onebit_bsz4k_seq128.sh\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Model Initialization\nDESCRIPTION: Code demonstrating how to initialize DeepSpeed by wrapping the model, optimizer, and other components.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer, _, _ = deepspeed.initialize(\n    args=args,\n    model=model,\n    model_parameters=optimizer_grouped_parameters\n)\n```\n\n----------------------------------------\n\nTITLE: Converting AlexNet to PipelineModule (Python)\nDESCRIPTION: Demonstrates how to convert the AlexNet model into a PipelineModule by flattening its submodules into a single sequence of layers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass AlexNetPipe(AlexNet):\n    def to_layers(self):\n        layers = [\n            *self.features,\n            self.avgpool,\n            lambda x: torch.flatten(x, 1),\n            *self.classifier\n        ]\n        return layers\n\nfrom deepspeed.pipe import PipelineModule\nnet = AlexNetPipe()\nnet = PipelineModule(layers=net.to_layers(), num_stages=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Flops Profiler\nDESCRIPTION: Configuration settings for DeepSpeed's FLOPS profiler functionality. Controls profiling steps, module depth, output detail level, and reporting options. Used to analyze model computation requirements and performance.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flops_profiler\": {\n    \"enabled\": false,\n    \"profile_step\": 1,\n    \"module_depth\": -1,\n    \"top_modules\": 1,\n    \"detailed\": true,\n    \"output_file\": null,\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO-Inference with NVMe Offloading in DeepSpeed\nDESCRIPTION: JSON configuration snippet for enabling ZeRO-Inference with offloading to an NVMe device in DeepSpeed. This sets the ZeRO optimization stage to 3 and configures parameter offloading to an NVMe device mounted at '/local_nvme'.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-09-10-zero-inference.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            ...\n        },\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Micro-Batch Size and Gradient Accumulation Steps\nDESCRIPTION: This JSON configuration sets up the DeepSpeed Autotuner to automatically determine the micro-batch size and gradient accumulation steps. It maps these parameters to corresponding arguments in the training script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": \"auto\",\n    \"autotuning\": {\n        \"enabled\": true,\n        \"arg_mappings\": {\n            \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n            \"gradient_accumulation_steps \": \"--gradient_accumulation_steps\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FP16 Training Options in DeepSpeed\nDESCRIPTION: JSON configuration for FP16 mixed precision training using NVIDIA's Apex package. Includes settings for loss scaling, hysteresis, and other FP16-specific parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"enabled\": true,\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"consecutive_hysteresis\": false,\n    \"min_loss_scale\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Implicit Forward Pass in Pipeline Parallel Models (Python)\nDESCRIPTION: Shows the implicit forward pass implementation for pipeline parallel models, where each layer consumes the output of the previous layer.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, inputs):\n    x = inputs\n    for layer in self.layers:\n        x = layer(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Loop Implementation\nDESCRIPTION: Implementation of the training loop using DeepSpeed's Model Engine API\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor i, data in enumerate(trainloader):\n    # Get the inputs; data is a list of [inputs, labels].\n    inputs = data[0].to(model_engine.device)\n    labels = data[1].to(model_engine.device)\n\n    outputs = model_engine(inputs)\n    loss = criterion(outputs, labels)\n\n    model_engine.backward(loss)\n    model_engine.step()\n```\n\n----------------------------------------\n\nTITLE: Loading FP32 Weights from ZeRO Checkpoint in Python\nDESCRIPTION: Python code to load FP32 weights from a ZeRO checkpoint into a model or obtain the state dict.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\nfp32_model = load_state_dict_from_zero_checkpoint(deepspeed.module, checkpoint_dir)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\nstate_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)\n```\n\n----------------------------------------\n\nTITLE: BERT Model Configuration for Fine-tuning\nDESCRIPTION: JSON configuration specifying BERT model parameters for fine-tuning on GLUE tasks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\nbert_model_config = {\n    \"vocab_size_or_config_json_file\": 119547,\n    \"hidden_size\": 768,\n    \"num_hidden_layers\": 12,\n    \"num_attention_heads\": 12,\n    \"intermediate_size\": 3072,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"max_position_embeddings\": 512,\n    \"type_vocab_size\": 2,\n    \"initializer_range\": 0.02\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Checkpoint for Inference\nDESCRIPTION: JSON configuration for loading DeepSpeed-trained model checkpoints for inference.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"ds_model\",\n    \"version\": 0.0,\n    \"checkpoints\": \"path_to_checkpoints\"\n}\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed Training with ZeRO++\nDESCRIPTION: Command to launch GPT-2 training using DeepSpeed with ZeRO++ optimizations. Configures model parameters, parallel processing settings, and enables various optimization flags.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zeropp.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed pretrain_zeropp_gpt.py \\\n       --tensor-model-parallel-size 1 \\\n       --pipeline-model-parallel-size 1 \\\n       --num-layers 40 \\\n       --hidden-size 6144 \\\n       --seq-length 512 \\\n       --num-attention-heads 32 \\\n       --batch-size 1 \\\n       --zero-stage 3 \\\n       --deepspeed_config ds_zeropp_config.json \\\n       --deepspeed-activation-checkpointing \\\n       --fp16 \\\n       --checkpoint-activations\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Type Options in DeepSpeed JSON\nDESCRIPTION: JSON configuration for specifying data types in DeepSpeed, particularly for gradient accumulation operations. This allows setting precision as fp32, fp16, or bf16 based on model requirements.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n\"data_types\": {\n    \"grad_accum_dtype\"=[\"fp32\"|\"fp16\"|\"bf16\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating DeepSpeed Transformer Layer in BERT Encoder\nDESCRIPTION: Configures and instantiates the DeepSpeed Transformer Layer within the BERT Encoder class, based on command-line arguments and DeepSpeed configuration.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif args.deepspeed_transformer_kernel:\n    from deepspeed import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig, DeepSpeedConfig\n\n    if hasattr(args, 'deepspeed_config') and args.deepspeed_config:\n        ds_config = DeepSpeedConfig(args.deepspeed_config)\n    else:\n        raise RuntimeError('deepspeed_config is not found in args.')\n\n    cuda_config = DeepSpeedTransformerConfig(\n        batch_size = ds_config.train_micro_batch_size_per_gpu,\n        max_seq_length = args.max_seq_length,\n        hidden_size = config.hidden_size,\n        heads = config.num_attention_heads,\n        attn_dropout_ratio = config.attention_probs_dropout_prob,\n        hidden_dropout_ratio = config.hidden_dropout_prob,\n        num_hidden_layers = config.num_hidden_layers,\n        initializer_range = config.initializer_range,\n        local_rank = args.local_rank if hasattr(args, 'local_rank') else -1,\n        seed = args.seed,\n        fp16 = ds_config.fp16_enabled,\n        pre_layer_norm=True,\n        attn_dropout_checkpoint=args.attention_dropout_checkpoint,\n        normalize_invertible=args.normalize_invertible,\n        gelu_checkpoint=args.gelu_checkpoint,\n        stochastic_mode=True)\n\n    layer = DeepSpeedTransformerLayer(cuda_config)\nelse:\n    layer = BertLayer(config)\nself.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Model Initialization\nDESCRIPTION: Code showing how to initialize the DeepSpeed engine with model parameters and training data\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparameters = filter(lambda p: p.requires_grad, net.parameters())\nargs=add_argument()\n\n# Initialize DeepSpeed to use the following features\n# 1) Distributed model.\n# 2) Distributed data loader.\n# 3) DeepSpeed optimizer.\nmodel_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=net, model_parameters=parameters, training_data=trainset)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Script with Autotuning\nDESCRIPTION: Example command for running model training with DeepSpeed autotuning enabled, including various configuration parameters and dataset settings for a language model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/autotuning.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --autotuning run --num_nodes=$NNODES --num_gpus=$NGPUS $HF_PATH/transformers/examples/pytorch/language-modeling/run_clm.py --deepspeed $DS_CONFIG\\\n    --model_name_or_path $MODEL_NAME \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --do_train \\\n    --do_eval \\\n    --fp16 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs $NEPOCHS \\\n    --output_dir ${OUTPUT_DIR} \\\n    --overwrite_output_dir\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Argument Parsing Implementation\nDESCRIPTION: Python code that sets up command line argument parsing for the CIFAR-10 model with DeepSpeed integration\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport deepspeed\n\ndef add_argument():\n\n    parser=argparse.ArgumentParser(description='CIFAR')\n\n    # Data.\n    # Cuda.\n    parser.add_argument('--with_cuda', default=False, action='store_true',\n                        help='use CPU in case there\\'s no GPU support')\n    parser.add_argument('--use_ema', default=False, action='store_true',\n                        help='whether use exponential moving average')\n\n    # Train.\n    parser.add_argument('-b', '--batch_size', default=32, type=int,\n                        help='mini-batch size (default: 32)')\n    parser.add_argument('-e', '--epochs', default=30, type=int,\n                        help='number of total epochs (default: 30)')\n    parser.add_argument('--local_rank', type=int, default=-1,\n                       help='local rank passed from distributed launcher')\n\n    # Include DeepSpeed configuration arguments.\n    parser = deepspeed.add_config_arguments(parser)\n\n    args=parser.parse_args()\n\n    return args\n```\n\n----------------------------------------\n\nTITLE: Updating BERT Encoder for Sparse Attention\nDESCRIPTION: Code to update the BERT encoder to use sparse attention when enabled.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif sparse_attention_config is not None:\n    from deepspeed.ops.sparse_attention import BertSparseSelfAttention\n\n    layer.attention.self = BertSparseSelfAttention(config, sparsity_config=sparse_attention_config)\n```\n\n----------------------------------------\n\nTITLE: Configuring MoQ Parameters in DeepSpeed JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure MoQ parameters in the DeepSpeed configuration file. It includes settings for quantization training, bits, schedule, and algorithm.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/MoQ-tutorial.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"optimizer\": {\n      \"type\": \"AdamW\",\n      \"params\": {\n        \"lr\": 2e-5,\n        \"weight_decay\": 0.0,\n        \"bias_correction\": true\n      }\n    },\n    \"gradient_clipping\": 1.0,\n    \"fp16\": {\n      \"initial_scale_power\": 16,\n      \"enabled\": true\n    },\n    \"quantize_training\": {\n      \"enabled\": true,\n      \"quantize_verbose\": true,\n      \"quantizer_kernel\": true,\n      \"quantize-algo\": {\n        \"q_type\": \"symmetric\"\n      },\n      \"quantize_bits\": {\n        \"start_bits\": 16,\n        \"target_bits\": 8\n      },\n      \"quantize_schedule\": {\n        \"quantize_period\": 400,\n        \"schedule_offset\": 0\n      },\n      \"quantize_groups\": 8,\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Distributed Training\nDESCRIPTION: Python code for initializing distributed training environment when using model parallelism or pipeline parallelism with MPI support.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.init_distributed()\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration for BERT with PLD\nDESCRIPTION: Complete DeepSpeed JSON configuration for running BERT pre-training with Progressive Layer Dropping enabled.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"train_micro_batch_size_per_gpu\": 16,\n  \"steps_per_print\": 1000,\n  \"prescale_gradients\": true,\n  \"gradient_predivide_factor\": 8,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 1e-3,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false\n    }\n  },\n  \"gradient_clipping\": 1.0,\n\n  \"wall_clock_breakdown\": false,\n\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0\n  },\n\n  \"progressive_layer_drop\": {\n    \"enabled\": true,\n    \"theta\": 0.5,\n    \"gamma\": 0.001\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PLD in DeepSpeed JSON\nDESCRIPTION: JSON configuration for enabling Progressive Layer Dropping in DeepSpeed, specifying theta and gamma values.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n  \"progressive_layer_drop\": {\n    \"enabled\": true,\n    \"theta\": 0.5,\n    \"gamma\": 0.001\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MSA Row-wise Attention with DS4Sci_EvoformerAttention in Python\nDESCRIPTION: Example of using DS4Sci_EvoformerAttention for MSA row-wise attention, which builds attention weights for residue pairs and integrates information from pair representation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Q, K, V: [Batch, N_seq, N_res, Head, Dim]\n# res_mask: [Batch, N_seq, 1, 1, N_res]\n# pair_bias: [Batch, 1, Head, N_res, N_res]\nout = DS4Sci_EvoformerAttention(Q, K, V, [res_mask, pair_bias])\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Checkpoint API Definitions\nDESCRIPTION: Core DeepSpeed checkpoint saving and loading API method signatures for handling both client model and internal states.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef save_checkpoint(self, save_dir, tag, client_state={})\ndef load_checkpoint(self, load_dir, tag)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Monitoring with DeepSpeed in Python\nDESCRIPTION: Example Python code demonstrating how to set up custom monitoring using DeepSpeed Monitor. It shows importing the monitor, initializing it with a configuration, and logging custom events during training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/monitor.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Step 1: Import monitor (and DeepSpeed config, if needed)\nfrom deepspeed.monitor.monitor import MonitorMaster\nfrom deepspeed.runtime.config import DeepSpeedConfig\n\n# Step 2: Initialized monitor with DeepSpeed config (get DeepSpeed config object, if needed)\nds_config = DeepSpeedConfig(\"ds_config.json\")\nmonitor = MonitorMaster(ds_config.monitor_config)\n\nfor epoch in range(2):\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader):\n        pre = time.time()\n        inputs, labels = data[0].to(model_engine.local_rank), data[1].to(\n            model_engine.local_rank)\n        if fp16:\n            inputs = inputs.half()\n        outputs = model_engine(inputs)\n        loss = criterion(outputs, labels)\n\n        model_engine.backward(loss)\n        model_engine.step()\n        post = time.time()\n        # Step 3: Create list of 3-tuple records (single entry in this case)\n        events = [(\"Time per step\", post-pre, model_engine.global_samples)]\n        # Step 4: Call monitor.write_events on the list from step 3\n        monitor.write_events(events)\n```\n\n----------------------------------------\n\nTITLE: Modifying Discriminator Training for DeepSpeed in Python\nDESCRIPTION: Adjusts the backward pass for the discriminator to include gradients from both real and fake mini-batches in the optimizer update.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_engineD.backward(errD_real)\nmodel_engineD.backward(errD_fake)\n```\n\n----------------------------------------\n\nTITLE: Single-Node Resource Configuration Commands\nDESCRIPTION: Example commands for launching DeepSpeed on specific GPUs in a single-node setup using either include flag or CUDA_VISIBLE_DEVICES.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --include localhost:0,1 ...\n```\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1 deepspeed ...\n```\n\n----------------------------------------\n\nTITLE: Profiling BERT Model with DeepSpeed FLOPS Counter\nDESCRIPTION: Example showing how to profile a BERT model using DeepSpeed's get_model_profile function. Creates fake input data and measures FLOPS, MACs, and parameters for a BERT sequence classification model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nimport torch\nfrom transformers import BertForSequenceClassification, BertTokenizer\nfrom deepspeed.profiling.flops_profiler import get_model_profile\nfrom deepspeed.accelerator import get_accelerator\n\n\ndef bert_input_constructor(batch_size, seq_len, tokenizer):\n    fake_seq = \"\"\n    for _ in range(seq_len - 2):  # ignore the two special tokens [CLS] and [SEP]\n      fake_seq += tokenizer.pad_token\n    inputs = tokenizer([fake_seq] * batch_size,\n                       padding=True,\n                       truncation=True,\n                       return_tensors=\"pt\")\n    labels = torch.tensor([1] * batch_size)\n    inputs = dict(inputs)\n    inputs.update({\"labels\": labels})\n    return inputs\n\n\nwith get_accelerator().device(0):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n    batch_size = 4\n    seq_len = 128\n    enable_profile = True\n    if enable_profile:\n      flops, macs, params = get_model_profile(\n          model,\n          kwargs=bert_input_constructor(batch_size, seq_len, tokenizer),\n          print_profile=True,\n          detailed=True,\n      )\n    else:\n      inputs = bert_input_constructor((batch_size, seq_len), tokenizer)\n      outputs = model(inputs)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DeepSpeed-FP6 Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to initialize a DeepSpeed-FP6 pipeline for the LLaMa-2-70B model and generate responses using FP6 quantization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mii\npipe = mii.pipeline(\"NousResearch/Llama-2-70b-hf\", quantization_mode='wf6af16')\nresponse = pipe([\"DeepSpeed is\", \"Seattle is\"], max_new_tokens=128)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Activation Checkpointing in DeepSpeed\nDESCRIPTION: JSON configuration object that specifies activation checkpointing settings for DeepSpeed. Controls features like partition activation, CPU offloading, memory optimization, checkpoint synchronization, and profiling. Each setting has specific implications for model training performance and memory usage.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"activation_checkpointing\": {\n    \"partition_activations\": false,\n    \"cpu_checkpointing\": false,\n    \"contiguous_memory_optimization\": false,\n    \"number_checkpoints\": null,\n    \"synchronize_checkpoint_boundary\": false,\n    \"profile\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Row Pruning for Feed Forward Networks in DeepSpeed\nDESCRIPTION: JSON configuration for row pruning using the topK method in DeepSpeed. This example prunes intermediate dense layers to 50% density after 20 training steps, with corresponding column pruning in related output dense layers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n  \"row_pruning\":{\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"schedule_offset\": 20,\n      \"method\": \"topk\"\n    },\n    \"different_groups\":{\n      \"rp1\": {\n        \"params\": {\n            \"dense_ratio\": 0.5\n        },\n        \"modules\": [\n          \"intermediate.dense\"\n        ],\n        \"related_modules\":[\n          [\"layer.\\\\w+.output.dense\"]\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding DeepSpeed Arguments\nDESCRIPTION: Modification to arguments.py to include DeepSpeed configuration options in the argument parser\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_args():\n    \"\"\"Parse all the args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='PyTorch BERT Model')\n    parser = add_model_config_args(parser)\n    parser = add_fp16_config_args(parser)\n    parser = add_training_args(parser)\n    parser = add_evaluation_args(parser)\n    parser = add_text_generate_args(parser)\n    parser = add_data_args(parser)\n\n    # Include DeepSpeed configuration arguments\n    parser = deepspeed.add_config_arguments(parser)\n```\n\n----------------------------------------\n\nTITLE: Configuring 1-Cycle Scheduler in PyTorch DeepSpeed\nDESCRIPTION: JSON configuration for implementing a 1-Cycle learning rate and momentum scheduler in PyTorch using DeepSpeed. Defines parameters for cycle phases, learning rates, and momentum values for optimizing model training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/one-cycle.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scheduler\": {\n        \"type\": \"OneCycle\",\n        \"params\": {\n            \"cycle_first_step_size\": 1000,\n            \"cycle_first_stair_count\": 500,\n            \"cycle_second_step_size\": 1000,\n            \"cycle_second_stair_count\": 500,\n            \"decay_step_size\": 1000,\n            \"cycle_min_lr\": 0.0001,\n            \"cycle_max_lr\": 0.0010,\n            \"decay_lr_rate\": 0.001,\n            \"cycle_min_mom\": 0.85,\n            \"cycle_max_mom\": 0.99,\n            \"decay_mom_rate\": 0.0\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: ZeRO-3 with NVMe Offload\nDESCRIPTION: Configuration for ZeRO stage 3 with parameter and optimizer state offloading to NVMe storage.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/nvme_data\"\n        }\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/nvme_data\"\n        }\n    },\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: BERT-Large Performance Profiling Output with DeepSpeed Flops Profiler\nDESCRIPTION: Example shell output showing detailed performance metrics from profiling BERT-Large model on an A100 GPU with batch size 80. Includes summary statistics, aggregated module profiles at different depths, and detailed per-module performance breakdown.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n-------------------------- DeepSpeed Flops Profiler --------------------------\nProfile Summary at step 10:\nNotations:\ndata parallel size (dp_size), model parallel size(mp_size),\nnumber of parameters (params), number of multiply-accumulate operations(MACs),\nnumber of floating-point operations (flops), floating-point operations per second (FLOPS),\nfwd latency (forward propagation latency), bwd latency (backward propagation latency),\nstep (weights update latency), iter latency (sum of fwd, bwd and step latency)\n\nworld size:                                                   1\ndata parallel size:                                           1\nmodel parallel size:                                          1\nbatch size per GPU:                                           80\nparams per gpu:                                               336.23 M\nparams of model = params per GPU * mp_size:                   336.23 M\nfwd MACs per GPU:                                             3139.93 G\nfwd flops per GPU:                                            6279.86 G\nfwd flops of model = fwd flops per GPU * mp_size:             6279.86 G\nfwd latency:                                                  76.67 ms\nbwd latency:                                                  108.02 ms\nfwd FLOPS per GPU = fwd flops per GPU / fwd latency:          81.9 TFLOPS\nbwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:      116.27 TFLOPS\nfwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):   102.0 TFLOPS\nstep latency:                                                 34.09 us\niter latency:                                                 184.73 ms\nsamples/second:                                               433.07\n```\n\n----------------------------------------\n\nTITLE: Configuring Sparse Attention Parameters in DeepSpeed\nDESCRIPTION: Example configuration for sparse attention using the fixed mode with block size 16. The configuration enables different layouts per attention head, sets local and global attention patterns, and configures bidirectional attention with sliding window blocks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n  \"sparse_attention\": {\n    \"mode\": \"fixed\",\n    \"block\": 16,\n    \"different_layout_per_head\": true,\n    \"num_local_blocks\": 4,\n    \"num_global_blocks\": 1,\n    \"attention\": \"bidirectional\",\n    \"horizontal_global_attention\": false,\n    \"num_different_global_patterns\": 4,\n    \"num_random_blocks\": 0,\n    \"local_window_blocks\": [4],\n    \"global_block_indices\": [0],\n    \"global_block_end_indices\": None,\n    \"num_sliding_window_blocks\": 3\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring OneBitLamb Optimizer in DeepSpeed\nDESCRIPTION: Configuration for the OneBitLamb optimizer, which includes both standard LAMB parameters and additional 1-bit compression specific parameters like freeze_step, cuda_aware, and coefficient settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"optimizer\": {\n    \"type\": \"OneBitLamb\",\n    \"params\": {\n      \"lr\": 11e-3,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false,\n      \"max_coeff\": 0.3,\n      \"min_coeff\": 0.01,\n      \"freeze_step\": 1000,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\",\n      \"coeff_beta\": 0.9,\n      \"factor_max\": 4.0,\n      \"factor_min\": 0.5,\n      \"factor_threshold\": 0.1\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring 0/1 Adam Optimizer in DeepSpeed\nDESCRIPTION: JSON configuration for using the 0/1 Adam optimizer in DeepSpeed, including specific parameters for controlling communication efficiency and variance updates.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"train_micro_batch_size_per_gpu\": 16,\n  \"optimizer\": {\n    \"type\": \"ZeroOneAdam\",\n    \"params\": {\n      \"lr\": 1e-3,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false,\n      \"var_freeze_step\": 1000,\n      \"var_update_scaler\": 16,\n      \"local_step_scaler\": 1000,\n      \"local_step_clipper\": 16,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\"\n    }\n  },\n  \"gradient_clipping\": 1.0,\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Megatron-LM Launch Parameters for 10B GPT-2\nDESCRIPTION: Command line parameters for configuring a 10B parameter GPT-2 model with activation checkpointing enabled. Sets model parallel size, layers, hidden size, attention heads, batch size and checkpointing options.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-offload.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n       --model-parallel-size 1 \\\n       --num-layers 50 \\\n       --hidden-size 4096 \\\n       --num-attention-heads 32 \\\n       --batch-size 10 \\\n       --deepspeed_config ds_zero_offload.config \\\n       --checkpoint-activations\n```\n\n----------------------------------------\n\nTITLE: ZeRO-3 with CPU Optimizer Offload\nDESCRIPTION: Configuration to enable ZeRO stage 3 with optimizer state and computation offloading to CPU memory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        }\n    },\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Client for Persistent Deployment in Python with DeepSpeed-FastGen\nDESCRIPTION: This snippet demonstrates how to create a client to interact with a persistent deployment server in DeepSpeed-FastGen. It shows how to generate text and terminate the server when it's no longer needed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclient = mii.client(\"mistralai/Mistral-7B-v0.1\")\noutput = client.generate(\"Deepspeed is\", max_new_tokens=128)\nprint(output)\n\n# Terminating the server\nclient.terminate_server()\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed ZeRO-Offload Configuration\nDESCRIPTION: JSON configuration for enabling ZeRO-Offload (stage 2) with MoE models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"zero_optimization\": {\n      \"stage\": 2,\n      \"allgather_partitions\": true,\n      \"reduce_scatter\": true,\n      \"allgather_bucket_size\": 50000000,\n      \"reduce_bucket_size\": 50000000,\n      \"overlap_comm\": true,\n      \"contiguous_gradients\": true,\n      \"cpu_offload\": true\n  }\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Sparse Attention JSON Configuration\nDESCRIPTION: JSON configuration for sparse attention settings including block size, layout, and attention patterns.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"sparse_attention\": {\n        \"mode\": \"fixed\",\n        \"block\": 16,\n        \"different_layout_per_head\": true,\n        \"num_local_blocks\": 4,\n        \"num_global_blocks\": 1,\n        \"attention\": \"bidirectional\",\n        \"horizontal_global_attention\": false,\n        \"num_different_global_patterns\": 4\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameter Offloading in DeepSpeed ZeRO (JSON)\nDESCRIPTION: JSON configuration for enabling and configuring ZeRO optimization of parameter offloading to CPU/NVMe. This is available only with ZeRO stage 3. The configuration includes options for device selection, NVMe path, memory pinning, buffer settings, and CPU memory limits.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"offload_param\": {\n    \"device\": \"[cpu|nvme]\",\n    \"nvme_path\": \"/local_nvme\",\n    \"pin_memory\": [true|false],\n    \"buffer_count\": 5,\n    \"buffer_size\": 1e8,\n    \"max_in_cpu\": 1e9\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning OPT-125M with DeepSpeed, LoRA, and CPU Offloading\nDESCRIPTION: Command for supervised fine-tuning of the facebook/opt-125m model using DeepSpeed with LoRA parameter-efficient fine-tuning and CPU offloading for memory optimization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed training\\step1_supervised_finetuning\\main.py --model_name_or_path facebook/opt-125m --gradient_accumulation_steps 8 --lora_dim 128 --only_optimize_lora --print_loss --zero_stage 2 --deepspeed --dtype bf16 --offload --output_dir output\n```\n\n----------------------------------------\n\nTITLE: Labeling Code Ranges with PyTorch Profiler in Python\nDESCRIPTION: This snippet shows how to label arbitrary code ranges using the record_function context manager in PyTorch Profiler. It demonstrates labeling a model forward pass with a custom name.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pytorch-profiler.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith profile(record_shapes=True) as prof: # record_shapes indicates whether to record shapes of the operator inputs.\n    with record_function(\"model_forward\"):\n        model_engine(inputs)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Debugging Utilities Usage\nDESCRIPTION: Example showing how to access and debug model states including parameters, gradients, and optimizer states in a training loop.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbackward(loss)\n[...]\nfrom deepspeed.utils import safe_get_full_fp32_param, safe_get_full_grad, safe_get_full_optimizer_state\nfor n, lp in model.named_parameters():\n    # 1. Access the full states\n    #  1.1) gradient lookup\n    # For zero1 and zero2, gradient lookup must be called after `backward` and before `step`\n    # For zero3, gradient lookup must be called after `backward`\n    hp_grad = safe_get_full_grad(lp)\n\n\n    # 1.2) fp32 and optim states can probably be called anywhere in the training loop, but will be updated after `step`\n    hp = safe_get_full_fp32_param(lp)\n    exp_avg = safe_get_full_optimizer_state(lp, \"exp_avg\")\n    exp_avg_sq = safe_get_full_optimizer_state(lp, \"exp_avg_sq\")\n\n    # 2. Access the local states (zero3)\n    # For zero3, all of the parameters, gradients, and optimizer states are partitioned,\n    # and each process can access its corresponding local state.\n    local_hp = safe_get_local_fp32_param(lp)\n    local_hp_grad = safe_get_local_grad(lp)\n    local_exp_avg = safe_get_local_optimizer_state(lp, \"exp_avg\")\n    local_exp_avg_sq = safe_get_local_optimizer_state(lp, \"exp_avg_sq\")\n\n[...]\noptimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Configuring Curriculum Learning in DeepSpeed JSON\nDESCRIPTION: Example DeepSpeed configuration JSON showing how to enable and configure curriculum learning for GPT-2 pre-training. Includes settings for batch size, optimizer, FP16, and curriculum learning parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"gradient_accumulation_steps\": 1,\n  \"steps_per_print\": 1,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.00015,\n      \"max_grad_norm\": 1.0,\n      \"betas\": [0.9, 0.95]\n    }\n  },\n  \"gradient_clipping\": 1.0,\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"consecutive_hysteresis\": false,\n    \"min_loss_scale\": 1\n  },\n  \"curriculum_learning\": {\n    \"enabled\": true,\n    \"curriculum_type\": \"seqlen\",\n    \"min_difficulty\": 8,\n    \"max_difficulty\": 1024,\n    \"schedule_type\": \"fixed_linear\",\n    \"schedule_config\": {\n      \"total_curriculum_step\": 15000,\n      \"difficulty_step\": 8\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Argument Parsing Integration\nDESCRIPTION: Code showing how to add DeepSpeed configuration arguments to the argument parser in the main entry point.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nparser = get_argument_parser()\n# Include DeepSpeed configuration arguments\nparser = deepspeed.add_config_arguments(parser)\nargs = parser.parse_args()\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Checkpoint Management\nDESCRIPTION: Python functions for saving and loading model checkpoints with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef checkpoint_model(PATH, ckpt_id, model, epoch, last_global_step, last_global_data_samples, **kwargs):\n    checkpoint_state_dict = {'epoch': epoch,\n                             'last_global_step': last_global_step,\n                             'last_global_data_samples': last_global_data_samples}\n    checkpoint_state_dict.update(kwargs)\n    success = model.network.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)\n    return\n```\n\n----------------------------------------\n\nTITLE: Running GLUE Task with DeepSpeed MoQ\nDESCRIPTION: This Python script demonstrates how to run a GLUE task (MRPC) using DeepSpeed with MoQ quantization. It sets up the task parameters and executes the training script with the DeepSpeed configuration.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/MoQ-tutorial.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTSK=mrpc\nTEST_JSON=test.json\n\npython text-classification/run_glue.py \\\n  --model_name_or_path bert-base-cased \\\n  --task_name $TSK \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TSK/ \\\n  --fp16 \\\n  --warmup_steps 2 \\\n  --deepspeed test.json\n```\n\n----------------------------------------\n\nTITLE: Running ZeroQuant Example for BERT with DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run the ZeroQuant example for BERT using DeepSpeed. This shows how to apply efficient post-training quantization to transformer models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/ZeroQuant/zero_quant.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring 1-bit LAMB Optimizer in DeepSpeed\nDESCRIPTION: JSON configuration for the 1-bit LAMB optimizer in DeepSpeed, including parameters for learning rate, compression, and mixed precision training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-lamb.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 65536,\n  \"train_micro_batch_size_per_gpu\": 64,\n  \"optimizer\": {\n    \"type\": \"OneBitLamb\",\n    \"params\": {\n      \"lr\": 11e-3,\n      \"max_coeff\": 0.3,\n      \"min_coeff\": 0.01,\n      \"freeze_step\": 1000,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\",\n      \"coeff_beta\": 0.9,\n      \"factor_max\": 4.0,\n      \"factor_min\": 0.5,\n      \"factor_threshold\": 0.1\n    }\n  },\n  \"gradient_clipping\": 1.0,\n  \"fp16\": {\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 16\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: FP16 Optimization Configuration\nDESCRIPTION: JSON configuration for enabling FP16 optimization for large model training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"fp16\": {\n      \"enabled\": true,\n      \"fp16_master_weights_and_grads\": true\n  }\n```\n\n----------------------------------------\n\nTITLE: Modifying TransformerBlock for Pipeline Compatibility (Python)\nDESCRIPTION: Shows how to modify a TransformerBlock to make it compatible with pipeline parallelism by collecting arguments into a tuple and returning both output and mask.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TransformerBlockPipe(TransformerBlock)\n    def forward(self, inputs):\n        hidden, mask = inputs\n        output = super().forward(hidden, mask)\n        return (output, mask)\nstack = [ TransformerBlockPipe() for _ in range(num_layers) ]\n```\n\n----------------------------------------\n\nTITLE: Loading HuggingFace Pretrained Model in DeepSpeed\nDESCRIPTION: Demonstrates the command-line arguments needed to load a HuggingFace pretrained model for fine-tuning with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n--model_file test/huggingface/bert-large-uncased-whole-word-masking-pytorch_model.bin \\\n--ckpt_type HF \\\n--origin_bert_config_file test/huggingface/bert-large-uncased-whole-word-masking-config.json \\\n```\n\n----------------------------------------\n\nTITLE: Modifying Generator Training for DeepSpeed in Python\nDESCRIPTION: Adjusts the backward pass for the generator. Note that for gradient accumulation, the discriminator parameters should have requires_grad set to False before this step.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_engineG.backward(errG)\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed-VisualChat API\nDESCRIPTION: Commands to start the chat API using the trained model checkpoint. Requires configuring variables like checkpoint path before running.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ../chat\nbash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed on Intel CPU with Core Binding\nDESCRIPTION: Command to launch DeepSpeed on Intel Architecture CPU with core binding for improved performance. This binds each worker process to a specific set of CPU cores.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --bind_cores_to_rank <deepspeed-model-script>\n```\n\n----------------------------------------\n\nTITLE: Profiling AlexNet using DeepSpeed Flops Profiler\nDESCRIPTION: This Python code demonstrates how to use the DeepSpeed Flops Profiler to analyze the AlexNet model. It shows the setup for profiling, including setting various parameters such as input shape, batch size, and profiling options.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision.models as models\nimport torch\nfrom deepspeed.profiling.flops_profiler import get_model_profile\nfrom deepspeed.accelerator import get_accelerator\n\nwith get_accelerator().device(0):\n    model = models.alexnet()\n    batch_size = 256\n    flops, macs, params = get_model_profile(model=model, # model\n                                    input_shape=(batch_size, 3, 224, 224), # input shape to the model. If specified, the model takes a tensor with this shape as the only positional argument.\n                                    args=None, # list of positional arguments to the model.\n                                    kwargs=None, # dictionary of keyword arguments to the model.\n                                    print_profile=True, # prints the model graph with the measured profile attached to each module\n                                    detailed=True, # print the detailed profile\n                                    module_depth=-1, # depth into the nested modules, with -1 being the inner most modules\n                                    top_modules=1, # the number of top modules to print aggregated profile\n                                    warm_up=10, # the number of warm-ups before measuring the time of each module\n                                    as_string=True, # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)\n                                    output_file=None, # path to the output file. If None, the profiler prints to stdout.\n                                    ignore_modules=None) # the list of modules to ignore in the profiling\n```\n\n----------------------------------------\n\nTITLE: Implementing Previous Tensor Parallelism Method in Python\nDESCRIPTION: Example of the previous tensor parallelism implementation that required manual injection policy specification for transformer layers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/automatic-tensor-parallelism.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport transformers\nimport deepspeed\nfrom transformers.models.t5.modeling_t5 import T5Block\nlocal_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n# create the model pipeline\npipe = transformers.pipeline(task=\"text2text-generation\", model=\"google/t5-v1_1-small\", device=local_rank)\n# Initialize the DeepSpeed-Inference engine\npipe.model = deepspeed.init_inference(\n    pipe.model,\n    mp_size=world_size,\n    dtype=torch.float,\n    injection_policy={T5Block: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')}\n)\noutput = pipe('Input String')\n```\n\n----------------------------------------\n\nTITLE: Using RepeatingLoader for Continuous Data Streaming (Python)\nDESCRIPTION: Shows how to use DeepSpeed's RepeatingLoader to ensure continuous data supply during pipeline parallel training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_loader = deepspeed.utils.RepeatingLoader(train_loader)\ntrain_iter = iter(train_loader)\nfor step in range(args.steps):\n    loss = engine.train_batch(data_iter=train_iter)\n```\n\n----------------------------------------\n\nTITLE: Launching GLUE Fine-tuning with DeepSpeed and 0/1 Adam (Bash)\nDESCRIPTION: This command demonstrates how to launch GLUE fine-tuning tasks using a bash script. It requires specifying the path to the pre-trained BERT checkpoint as an argument.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash run_glue_bert_base_finetune.sh <path to checkpoint>\n```\n\n----------------------------------------\n\nTITLE: Configuring Activation Checkpointing in DeepSpeed\nDESCRIPTION: Function to configure activation checkpointing settings in DeepSpeed. It allows for various memory optimizations such as activation partitioning across GPUs and CPU checkpointing.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.configure\n```\n\n----------------------------------------\n\nTITLE: Configuring 1-bit Adam in DeepSpeed JSON\nDESCRIPTION: JSON configuration for enabling and configuring the 1-bit Adam optimizer in DeepSpeed, including settings for learning rate, freeze step, and communication backend.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4096,\n  \"train_micro_batch_size_per_gpu\": 16,\n  \"optimizer\": {\n    \"type\": \"OneBitAdam\",\n    \"params\": {\n      \"lr\": 4e-4,\n      \"freeze_step\": 23000,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\"\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Layer Reduction Example for BERT Fine-tuning in Python\nDESCRIPTION: This snippet demonstrates how to run the layer reduction example for BERT fine-tuning using DeepSpeed Compression. It installs requirements and executes a bash script to perform layer reduction.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/layer_reduction.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Data Truncation in GPT-2 Forward Step (Python)\nDESCRIPTION: Modify the forward_step() function in pretrain_gpt2.py to incorporate the curriculum_seqlen parameter for sequence length-based curriculum learning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forward_step(curriculum_seqlen):\n    # Implementation details not provided in the original text\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing and Training DeepSpeed-VisualChat\nDESCRIPTION: Commands to clone the repository, install dependencies, and start training DeepSpeed-VisualChat using the LLaMa-7B model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/deepspeedai/DeepSpeedExamples.git\ncd DeepSpeedExamples/applications/DeepSpeed-VisualChat/\npip install -r requirements.txt\ncd training\nbash training_scripts/run_7b.sh\n```\n\n----------------------------------------\n\nTITLE: Model and Optimizer Setup with DeepSpeed\nDESCRIPTION: Implementation of model and optimizer initialization using DeepSpeed, including handling of FP16 settings\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef setup_model_and_optimizer(args):\n    \"\"\"Setup model and optimizer.\"\"\"\n\n    model = get_model(args)\n    optimizer = get_optimizer(model, args)\n    lr_scheduler = get_learning_rate_scheduler(optimizer, args)\n\n    if args.deepspeed:\n        import deepspeed\n\n        print_rank_0(\"DeepSpeed is enabled.\")\n\n        model, optimizer, _, lr_scheduler = deepspeed.initialize(\n            model=model,\n            optimizer=optimizer,\n            args=args,\n            lr_scheduler=lr_scheduler,\n            mpu=mpu,\n            dist_init_required=False\n       )\n```\n\n----------------------------------------\n\nTITLE: ZeRO-3 with CPU Parameter and Optimizer Offload\nDESCRIPTION: Configuration for ZeRO stage 3 with both parameter and optimizer state offloading to CPU memory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        }\n        \"offload_param\": {\n            \"device\": \"cpu\"\n        }\n    },\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Curriculum Sequence Length for Pipeline Parallelism (Python)\nDESCRIPTION: Create a duplicate of deepspeed.runtime.data_pipeline.curriculum_scheduler on the user side to retrieve the curriculum_seqlen when using pipeline parallelism. This implementation is found in megatron/training.py.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Code snippet not provided in the original text\n# Implementation would be in megatron/training.py\n```\n\n----------------------------------------\n\nTITLE: Gradient Modification Example\nDESCRIPTION: Example showing how to modify gradients in ZeRO training between backward pass and optimizer step.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nbackward(loss)\n[...]\nfrom deepspeed.runtime.zero.utils import is_zero_param\nfrom deepspeed.utils import safe_set_full_grad, safe_set_local_grad\n# Here is an example of how to zero all the gradients.\nfor n, lp in model.named_parameters():\n    # 1. For zero stage 1, 2, or 3 set the full gradient.\n    zero_tensor = torch.zeros(lp.ds_shape) if is_zero_param(lp) else torch.zeros(lp.shape)\n\n    safe_set_full_grad(lp, zero_tensor)\n\n    # 2. For zero stage 3, each process sets its local gradient partition.\n    zero_tensor_local = torch.zeros_like(lp.ds_tensor.shape)\n\n    safe_set_local_grad(lp, zero_tensor_local)\n\n[...]\noptimizer.step()\n```\n\n----------------------------------------\n\nTITLE: MoS Training Configuration Parameters\nDESCRIPTION: Parameters for enabling Mixture-of-Students training with knowledge distillation from teacher models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-nlg.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--mos\n--load-teacher\nnum-layers-teacher\n--hidden-size-teacher\n--hidden-size-teacher\n--num-experts-teacher\n```\n\n----------------------------------------\n\nTITLE: Creating GDS Handle in Python\nDESCRIPTION: Creates a gds_handle for GPU Direct Storage operations using DeepSpeed's GDSBuilder. This handle works specifically with CUDA tensors for optimized performance.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.ops.op_builder import GDSBuilder\ngds_handle = GDSBuilder().load().gds_handle()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sparse Attention in BERT Model\nDESCRIPTION: Setup code for configuring sparse attention in the BERT model initialization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nself.pad_token_id = config.pad_token_id if hasattr(config, 'pad_token_id') and config.pad_token_id is not None else 0\n# set sparse_attention_config if it has been selected\nself.sparse_attention_config = get_sparse_attention_config(args, config.num_attention_heads)\nself.encoder = BertEncoder(config, args, sparse_attention_config=self.sparse_attention_config)\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS for DS4Sci_EvoformerAttention in Shell\nDESCRIPTION: Commands to clone the CUTLASS repository and set the CUTLASS_PATH environment variable, which is required for DS4Sci_EvoformerAttention.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/NVIDIA/cutlass\nexport CUTLASS_PATH=/path/to/cutlass\n```\n\n----------------------------------------\n\nTITLE: Configuring Weight Quantization in DeepSpeed\nDESCRIPTION: JSON configuration for weight quantization compression in DeepSpeed. Defines shared parameters and different quantization groups with their specific settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compression_training\": {\n  \"weight_quantization\": {\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"quantizer_kernel\": false,\n      \"schedule_offset\": 0,\n      \"quantize_groups\": 1,\n      \"quantize_verbose\": false,\n      \"quantization_type\": \"symmetric\",\n      \"rounding\": \"nearest\",\n      \"quantize_weight_in_forward\": false,\n      \"fp16_mixed_quantize\":{\n        \"enabled\": false,\n        \"quantize_change_ratio\": 0.001\n      }\n    },\n    \"different_groups\":{\n      \"wq1\": {\n        \"params\": {\n            \"start_bits\": 8,\n            \"target_bits\": 8,\n            \"quantization_period\": 50\n        },\n        \"modules\": [\n          \"attention.self\",\n          \"intermediate\"\n        ]\n      },\n      \"wq2\": {\n        \"params\": {\n            \"start_bits\": 4,\n            \"target_bits\": 4,\n            \"quantization_period\": 50\n        },\n        \"modules\": [\n          \"attention.output\"\n        ]\n      }\n    }\n  }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Data Post-Processing Function for Curriculum Learning in Python\nDESCRIPTION: Example Python code for setting a data post-processing function for curriculum learning. This is necessary for metrics that require data post-processing, such as truncation-based sequence length.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# In megatron/training.py, pretrain_bert.py, or pretrain_gpt.py:\nengine.set_data_post_process_func(post_process_func)\n```\n\n----------------------------------------\n\nTITLE: MoE Parameter Group Creation\nDESCRIPTION: Function to create parameter groups for MoE layers to be used with optimizers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_moe_param_groups(model):\n    from deepspeed.moe.utils import split_params_into_different_moe_groups_for_optimizer\n\n    parameters = {'params': [p for p in model.parameters()], 'name': 'parameters'}\n\n    return split_params_into_different_moe_groups_for_optimizer(parameters)\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed with Dataset (Python)\nDESCRIPTION: Demonstrates how to initialize DeepSpeed with a dataset, allowing it to handle data loading complexities for pipeline parallel training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nengine, _, _, _ = deepspeed.initialize(\n    args=args,\n    model=net,\n    model_parameters=[p for p in net.parameters() if p.requires_grad],\n    training_data=cifar_trainset())\n\nfor step in range(args.steps):\n    loss = engine.train_batch()\n```\n\n----------------------------------------\n\nTITLE: Configuring OneBitAdam Optimizer in DeepSpeed\nDESCRIPTION: Configuration example for the OneBitAdam optimizer, which includes standard Adam parameters along with 1-bit compression specific parameters like freeze_step, cuda_aware, and communication backend settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"optimizer\": {\n    \"type\": \"OneBitAdam\",\n    \"params\": {\n      \"lr\": 0.001,\n      \"betas\": [\n        0.8,\n        0.999\n      ],\n      \"eps\": 1e-8,\n      \"weight_decay\": 3e-7,\n      \"freeze_step\": 400,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring PR-MoE for DeepSpeed Inference\nDESCRIPTION: This bash script demonstrates how to configure PR-MoE for DeepSpeed inference. It sets up a mixture of 64 and 128 experts for alternating layers, uses the 'residual' MLP type, and configures various model parameters for a 24-layer transformer model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-inference.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexperts=\"64 64 64 64 64 64 64 64 64 64 128 128\"\ngenerate_samples_gpt.py \\\n       --tensor-model-parallel-size 1 \\\n       --num-experts ${experts} \\\n       --mlp_type 'residual' \\\n       --num-layers 24 \\\n       --hidden-size 2048 \\\n       --num-attention-heads 16 \\\n       --max-position-embeddings 1024 \\\n       --tokenizer-type GPT2BPETokenizer \\\n       --load $checkpoint_path \\\n       --fp16 \\\n       --ds-inference \\\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Transformer Layer Implementation\nDESCRIPTION: Implementation of DeepSpeed's transformer layer initialization with configuration settings for the transformer kernel.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif args.deepspeed_transformer_kernel:\n    from deepspeed import DeepSpeedTransformerLayer, \\\n        DeepSpeedTransformerConfig, DeepSpeedConfig\n\n    ds_config = DeepSpeedConfig(args.deepspeed_config)\n\n    cuda_config = DeepSpeedTransformerConfig(\n        batch_size=ds_config.train_micro_batch_size_per_gpu,\n        max_seq_length=args.max_seq_length,\n        hidden_size=config.hidden_size,\n        heads=config.num_attention_heads,\n        attn_dropout_ratio=config.attention_probs_dropout_prob,\n        hidden_dropout_ratio=config.hidden_dropout_prob,\n        num_hidden_layers=config.num_hidden_layers,\n        initializer_range=config.initializer_range,\n        seed=args.seed,\n        fp16=ds_config.fp16_enabled\n    )\n    self.layer = nn.ModuleList([\n        copy.deepcopy(DeepSpeedTransformerLayer(i, cuda_config))\n        for i in range(config.num_hidden_layers)\n    ])\nelse:\n    layer = BertLayer(config)\n    self.layer = nn.ModuleList([\n        copy.deepcopy(layer)\n        for _ in range(config.num_hidden_layers)\n    ])\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepSpeed Repository and Examples\nDESCRIPTION: Shell commands to clone the DeepSpeed repository and checkout the DeepSpeedExamples submodule containing BingBertSQuAD and BERT Pre-training examples.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/deepspeedai/DeepSpeed\ncd DeepSpeed\ngit submodule update --init --recursive\ncd DeepSpeedExamples/\n```\n\n----------------------------------------\n\nTITLE: Enabling synced_gpus for ZeRO Stage 3 in DeepSpeed-Chat\nDESCRIPTION: This code snippet explicitly sets the synced_gpus argument to True when invoking the generate() function with ZeRO Stage 3 enabled. This resolves a desynchronization issue between GPUs during sequence generation in DeepSpeed-Chat Step 3 training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.actor_model.generate(input_ids=input_ids, synced_gpus=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Megatron-LM Checkpoint for DeepSpeed Inference\nDESCRIPTION: JSON configuration for loading Megatron-LM checkpoints trained with model parallelism for DeepSpeed inference.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"Megatron\",\n    \"version\": 0.0,\n    \"checkpoints\": [\n        \"mp_rank_00/model_optim_rng.pt\",\n        \"mp_rank_01/model_optim_rng.pt\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running ViT Finetuning with Random-LTD in DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run ViT finetuning examples with random-LTD using DeepSpeed. It includes commands for both CIFAR and ImageNet datasets.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nDeepSpeedExamples/data_efficiency/vit_finetuning$ pip install -r requirement.txt\nDeepSpeedExamples/data_efficiency/vit_finetuning$ bash ./bash_script/run_cifar.sh\nDeepSpeedExamples/data_efficiency/vit_finetuning$ bash ./bash_script/run_imagenet.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Curriculum Learning Schedule in Python\nDESCRIPTION: Example Python code for setting a custom curriculum learning schedule. This is used to update the maximum accepted difficulty during training based on a custom pacing function.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nengine.set_custom_curriculum_learning_schedule(custom_schedule_func)\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed CPU Adam\nDESCRIPTION: Python import statement for DeepSpeed's CPU-optimized Adam optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.ops.adam.DeepSpeedCPUAdam\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed with Learning Rate Scheduler\nDESCRIPTION: When using a DeepSpeed learning rate scheduler that should execute at every training step, pass it to deepspeed.initialize during engine initialization. This allows DeepSpeed to manage the scheduler for updates and save/restore operations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/schedulers.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.initialize(model=model, optimizer=optimizer, lr_scheduler=scheduler)\n```\n\n----------------------------------------\n\nTITLE: Optimizing DeepSpeed AutoTP Models with Intel Extension for PyTorch\nDESCRIPTION: This Python snippet demonstrates how to further optimize DeepSpeed AutoTP models using Intel Extension for PyTorch. It applies additional optimizations to the model, specifically for transformer architectures, using bfloat16 precision.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/intel-inference/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Use Intel Extension for PyTorch to optimize model\n...\nmodel = engine.module\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize_transformers(model.eval(), dtype=torch.bfloat16, inplace=True)\n...\n```\n\n----------------------------------------\n\nTITLE: Enabling Pinned Memory in ZeRO-3 (Python)\nDESCRIPTION: This configuration enables the use of pinned memory in ZeRO-3. It shows how to set the 'cpu_offload_use_pin_memory' option to true.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"cpu_offload_use_pin_memory\" : true\n```\n\n----------------------------------------\n\nTITLE: Implementing Triangular Self-Attention with DS4Sci_EvoformerAttention in Python\nDESCRIPTION: Example of using DS4Sci_EvoformerAttention for triangular self-attention (around starting node), which updates the pair representation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Q, K, V: [Batch, N_res, N_res, Head, Dim]\n# res_mask: [Batch, N_res, 1, 1, N_res]\n# right_edges: [Batch, 1, Head, N_res, N_res]\nout = DS4Sci_EvoformerAttention(Q, K, V, [res_mask, right_edges])\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed Training with Transformer Kernel\nDESCRIPTION: Command-line example for launching DeepSpeed training with transformer kernel enabled, including memory optimization settings for sequence length 512 and micro-batch size of 16.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/transformer_kernel.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed deepspeed_train.py \\\n--cf bert_large_lamb.json \\\n--max_seq_length 512 \\\n--print_steps 100 \\\n--deepspeed \\\n--deepspeed_transformer_kernel \\\n--deepspeed_config deepspeed_bsz32K_lamb_config_seq512.json \\\n--rewarmup \\\n--lr_schedule \"EE\" \\\n--lr_offset 0.0 \\\n--attention_dropout_checkpoint \\\n--load_training_checkpoint ${CHECKPOINT_BASE_PATH} \\\n--load_checkpoint_id ${CHECKPOINT_EPOCH150_NAME}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Communication Logging in JSON\nDESCRIPTION: JSON configuration for enabling DeepSpeed communication logging. This snippet shows how to set up logging in the DeepSpeed configuration file, including options for verbose output and profiling all operations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/comms-logging.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comms_logger\": {\n    \"enabled\": true,\n    \"verbose\": false,\n    \"prof_all\": true,\n    \"debug\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DeepSpeed Checkpoint for Fine-tuning\nDESCRIPTION: Python code to load a DeepSpeed checkpoint for fine-tuning BERT on GLUE tasks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmodel.load_state_dict(checkpoint_state_dict['module'], strict=False)\n```\n\n----------------------------------------\n\nTITLE: Executing DeepSpeed Flops Profiler on Megatron-LM Model\nDESCRIPTION: This shell output shows the results of running the DeepSpeed Flops Profiler on a 12-layer Megatron-LM model. It provides detailed metrics on model parameters, MACs, latency, and FLOPS for various components of the model architecture.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n-------------------------- DeepSpeed Flops Profiler --------------------------\nProfile Summary at step 10:\nNotations:\ndata parallel size (dp_size), model parallel size(mp_size),\nnumber of parameters (params), number of multiply-accumulate operations(MACs),\nnumber of floating-point operations (flops), floating-point operations per second (FLOPS),\nfwd latency (forward propagation latency), bwd latency (backward propagation latency),\nstep (weights update latency), iter latency (sum of fwd, bwd and step latency)\n\nworld size:                                                   1\ndata parallel size:                                           1\nmodel parallel size:                                          1\nbatch size per GPU:                                           1024\nparams per gpu:                                               1.29 M\nparams of model = params per GPU * mp_size:                   1.29 M\nfwd MACs per GPU:                                             41271.95 G\nfwd flops per GPU:                                            82543.9 G\nfwd flops of model = fwd flops per GPU * mp_size:             82543.9 G\nfwd latency:                                                  1.89 s\nbwd latency:                                                  5.38 s\nfwd FLOPS per GPU = fwd flops per GPU / fwd latency:          43.68 TFLOPS\nbwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:      30.7 TFLOPS\nfwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):   34.07 TFLOPS\nstep latency:                                                 34.12 s\niter latency:                                                 41.39 s\nsamples/second:                                               24.74\n\n----------------------------- Aggregated Profile per GPU -----------------------------\nTop 1 modules in terms of params, MACs or fwd latency at different model depths:\ndepth 0:\n    params      - {'GPT2Model': '1.29 M'}\n    MACs        - {'GPT2Model': '41271.95 GMACs'}\n    fwd latency - {'GPT2Model': '1.84 s'}\ndepth 1:\n    params      - {'TransformerLanguageModel': '1.29 M'}\n    MACs        - {'TransformerLanguageModel': '39584.03 GMACs'}\n    fwd latency - {'TransformerLanguageModel': '1.83 s'}\ndepth 2:\n    params      - {'ParallelTransformer': '1.29 M'}\n    MACs        - {'ParallelTransformer': '39584.03 GMACs'}\n    fwd latency - {'ParallelTransformer': '1.81 s'}\ndepth 3:\n    params      - {'ModuleList': '1.28 M'}\n    MACs        - {'ModuleList': '39584.03 GMACs'}\n    fwd latency - {'ModuleList': '1.3 s'}\ndepth 4:\n    params      - {'ParallelTransformerLayerPart2': '688.15 k'}\n    MACs        - {'ParallelTransformerLayerPart2': '26388.28 GMACs'}\n    fwd latency - {'ParallelTransformerLayerPart2': '865.73 ms'}\ndepth 5:\n    params      - {'ParallelMLP': '491.54 k'}\n    MACs        - {'ParallelMLP': '26388.28 GMACs'}\n    fwd latency - {'ParallelMLP': '849.4 ms'}\n\n------------------------------ Detailed Profile per GPU ------------------------------\nEach module profile is listed after its name in the following order:\nparams, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n\nNote: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs(or latency) and the sum of its submodules'.\n1. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n2. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n\nGPT2Model(\n  1.29 M, 100.00% Params, 41271.95 GMACs, 100.00% MACs, 1.84 s, 100.00% latency, 44.78 TFLOPS,\n  (language_model): TransformerLanguageModel(\n    1.29 M, 100.00% Params, 39584.03 GMACs, 95.91% MACs, 1.83 s, 99.11% latency, 43.34 TFLOPS,\n    (embedding): Embedding(\n      2, 0.00% Params, 0 MACs, 0.00% MACs, 18.1 ms, 0.98% latency, 0.0 FLOPS,\n      (word_embeddings): VocabParallelEmbedding(1, 0.00% Params, 0 MACs, 0.00% MACs, 164.75 us, 0.01% latency, 0.0 FLOPS, )\n      (position_embeddings): Embedding(1, 0.00% Params, 0 MACs, 0.00% MACs, 489.23 us, 0.03% latency, 0.0 FLOPS, 1024, 8192)\n      (embedding_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n    )\n    (transformer): ParallelTransformer(\n      1.29 M, 100.00% Params, 39584.03 GMACs, 95.91% MACs, 1.81 s, 98.11% latency, 43.78 TFLOPS,\n      (layers): ModuleList(\n        1.28 M, 98.73% Params, 39584.03 GMACs, 95.91% MACs, 1.3 s, 70.66% latency, 60.79 TFLOPS,\n        (0): ParallelTransformerLayerPart1(\n          49.15 k, 3.80% Params, 1099.65 GMACs, 2.66% MACs, 23.5 ms, 1.27% latency, 93.6 TFLOPS,\n          (input_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 128.75 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n          (attention): ParallelSelfAttention(\n            32.77 k, 2.53% Params, 1099.65 GMACs, 2.66% MACs, 22.8 ms, 1.24% latency, 96.46 TFLOPS,\n            (query_key_value): ColumnParallelLinear(24.58 k, 1.90% Params, 824.63 GMACs, 2.00% MACs, 8.93 ms, 0.48% latency, 184.7 TFLOPS, )\n            (scale_mask_softmax): FusedScaleMaskSoftmax(0, 0.00% Params, 134.22 MMACs, 0.00% MACs, 151.16 us, 0.01% latency, 1.78 TFLOPS, )\n            (attention_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 79.63 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)\n            (dense): RowParallelLinear(8.19 k, 0.63% Params, 274.88 GMACs, 0.67% MACs, 2.67 ms, 0.14% latency, 205.81 TFLOPS, )\n          )\n        )\n        (1): ParallelTransformerLayerPart2(\n          57.35 k, 4.43% Params, 2199.02 GMACs, 5.33% MACs, 77.53 ms, 4.21% latency, 56.73 TFLOPS,\n          (post_attention_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 116.11 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n          (mlp): ParallelMLP(\n            40.96 k, 3.16% Params, 2199.02 GMACs, 5.33% MACs, 76.19 ms, 4.13% latency, 57.72 TFLOPS,\n            (dense_h_to_4h): ColumnParallelLinear(32.77 k, 2.53% Params, 1099.51 GMACs, 2.66% MACs, 10.79 ms, 0.59% latency, 203.81 TFLOPS, )\n            (dense_4h_to_h): RowParallelLinear(8.19 k, 0.63% Params, 1099.51 GMACs, 2.66% MACs, 14.38 ms, 0.78% latency, 152.95 TFLOPS, )\n          )\n        )\n        ...\n        (23): ParallelTransformerLayerPart2(...)\n      )\n      (final_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 110.86 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)\n------------------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Autotuning\nDESCRIPTION: JSON configuration for DeepSpeed's autotuning feature that automatically optimizes Zero stage, micro batch size and other configurations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"autotuning\": {\n    \"enabled\": true,\n    \"results_dir\": null,\n    \"exps_dir\": null,\n    \"overwrite\": false,\n    \"metric\": \"throughput\",\n    \"num_nodes\": null,\n    \"num_gpus\": null,\n    \"start_profile_step\": 3,\n    \"end_profile_step\": 5,\n    \"fast\": true,\n    \"num_tuning_micro_batch_sizes\": 3,\n    \"tuner_type\": \"model_based\",\n    \"tuner_early_stopping\": 5,\n    \"tuner_num_trials\": 50,\n    \"arg_mappings\": null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Linear Layer with External Parameter\nDESCRIPTION: Implementation of a custom linear layer that returns both output and bias parameter, demonstrating external parameter handling in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass CustomLinear(torch.nn.Linear):\n    def forward(self, *input):\n        output = super().forward(*input)\n        return output, self.bias\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration JSON\nDESCRIPTION: Configuration file specifying DeepSpeed parameters including batch size, optimizer settings, and scheduler configuration\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 4,\n  \"steps_per_print\": 2000,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.001,\n      \"betas\": [\n        0.8,\n        0.999\n      ],\n      \"eps\": 1e-8,\n      \"weight_decay\": 3e-7\n    }\n  },\n  \"scheduler\": {\n    \"type\": \"WarmupLR\",\n    \"params\": {\n      \"warmup_min_lr\": 0,\n      \"warmup_max_lr\": 0.001,\n      \"warmup_num_steps\": 1000\n    }\n  },\n  \"wall_clock_breakdown\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Discrete Schedule for Curriculum Learning\nDESCRIPTION: Example configuration for the fixed_discrete curriculum learning schedule in DeepSpeed. Defines discrete difficulty levels and step thresholds for switching difficulty.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"schedule_type\": \"fixed_discrete\",\n\"schedule_config\": {\n  \"difficulty\": [1,2,3],\n  \"max_step\": [5,10]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU Checkpointing for Activation Memory Reduction\nDESCRIPTION: Conditional configuration to enable CPU checkpointing for reducing activation memory footprint in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ulysses-offload.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nif [ \"${activation_checkpoint}\" = \"true\" ]; then\ndeepspeed_options=\"${deepspeed_options} \\\n    --deepspeed-activation-checkpointing \\\n    --checkpoint-in-cpu\"\nfi\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeedTransformerLayer in Python\nDESCRIPTION: This snippet demonstrates how to import the DeepSpeedTransformerLayer class from the deepspeed module. This class is used to initialize and create the transformer layer module for BERT models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/kernel.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom deepspeed import DeepSpeedTransformerLayer\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard Logging in DeepSpeed\nDESCRIPTION: JSON configuration for enabling TensorBoard logging in DeepSpeed. It specifies the output path and job name for the logs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"tensorboard\": {\n        \"enabled\": true,\n        \"output_path\": \"output/ds_logs/\",\n        \"job_name\": \"train_bert\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Finetuning GPT-2 with Curriculum Learning in Bash\nDESCRIPTION: Example bash script for finetuning GPT-2 model with curriculum learning. It demonstrates how to apply curriculum learning to the finetuning process.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ndata_efficiency/gpt_finetuning/finetune/ds_finetune_gpt2_run.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Flops Profiler\nDESCRIPTION: JSON configuration for the flops profiler that measures time, flops and parameters of PyTorch models to identify bottlenecks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flops_profiler\": {\n    \"enabled\": true,\n    \"profile_step\": 1,\n    \"module_depth\": -1,\n    \"top_modules\": 3,\n    \"detailed\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating ZeRO-3 Memory Usage without Live Model in Python\nDESCRIPTION: This snippet shows how to estimate memory requirements for a 3B parameter model using ZeRO-3 without loading the actual model. It uses the estimate_zero3_model_states_mem_needs_all_cold function from DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_cold; \nestimate_zero3_model_states_mem_needs_all_cold(total_params=2851e6, largest_layer_params=32e6, num_gpus_per_node=8, num_nodes=1)'\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Monitor in JSON\nDESCRIPTION: Example JSON configuration for enabling various monitoring backends in DeepSpeed, including TensorBoard, Weights & Biases, Comet, and CSV logging.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/monitor.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tensorboard\": {\n    \"enabled\": true,\n    \"output_path\": \"output/ds_logs/\",\n    \"job_name\": \"train_bert\"\n  },\n  \"wandb\": {\n    \"enabled\": true,\n    \"team\": \"my_team\",\n    \"group\": \"my_group\",\n    \"project\": \"my_project\"\n  },\n  \"comet\": {\n    \"enabled\": true,\n    \"project\": \"my_project\",\n    \"experiment_name\": \"my_experiment\"\n  },\n  \"csv_monitor\": {\n    \"enabled\": true,\n    \"output_path\": \"output/ds_logs/\",\n    \"job_name\": \"train_bert\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed for DCGAN in JSON\nDESCRIPTION: Creates a JSON configuration file for DeepSpeed, specifying parameters such as batch size, optimizer settings, and print frequency.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\" : 64,\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.0002,\n      \"betas\": [\n        0.5,\n        0.999\n      ],\n      \"eps\": 1e-8\n    }\n  },\n  \"steps_per_print\" : 10\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MoQ with Eigenvalue-based Dynamic Scheduling\nDESCRIPTION: This JSON snippet shows how to configure MoQ with eigenvalue-based dynamic scheduling. It includes settings for quantization training and eigenvalue computation parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/MoQ-tutorial.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"quantize_training\": {\n      \"enabled\": true,\n      \"quantize_verbose\": true,\n      \"quantizer_kernel\": true,\n      \"quantize_type\": \"symmetric\",\n      \"quantize_bits\": {\n        \"start_bits\": 12,\n        \"target_bits\": 8\n      },\n      \"quantize_schedule\": {\n        \"quantize_period\": 10,\n        \"schedule_offset\": 0\n      },\n      \"quantize_groups\": 8,\n      \"fp16_mixed_quantize\": {\n        \"enabled\": false,\n        \"quantize_change_ratio\": 0.001\n      },\n      \"eigenvalue\": {\n        \"enabled\": true,\n        \"verbose\": true,\n        \"max_iter\": 50,\n        \"tol\": 1e-2,\n        \"stability\": 0,\n        \"gas_boundary_resolution\": 1,\n        \"layer_name\": \"bert.encoder.layer\",\n        \"layer_num\": 12\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: PR-MoE Configuration Parameters\nDESCRIPTION: Configuration parameters specific to Pyramid-Residual MoE implementation, including expert distribution and MLP type settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts-nlg.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--num-experts\n--mlp-type\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameter Class for Fused QKV in Transformer Model\nDESCRIPTION: This code snippet defines a Parameter class for Llama model that fuses separate query, key, and value projections into a single larger projection for higher throughput. It inherits from ParameterBase and implements the finalize method.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/inference/v2/model_implementations/AddingAModel.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.inference.module_implementations.parameter_base import ParameterBase\n\nclass UnfusedQKVParameter(ParameterBase):\n    query: torch.Tensor\n    key: torch.Tensor\n    value: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        fused_param = torch.cat([self.query, self.key, self.value], dim=0)\n        return self.inference_model.transform_qkv_param(fused_param)\n```\n\n----------------------------------------\n\nTITLE: Converting ZeRO Checkpoint to Universal Format using DeepSpeed CLI\nDESCRIPTION: Command line script to convert a DeepSpeed ZeRO checkpoint into the Universal checkpoint format. The script takes input and output folder paths as parameters and performs the conversion process.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/universal-checkpointing.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ds_to_universal.py --input_folder /path/to/zero/checkpoint --output_folder /path/to/universal/checkpoint\n```\n\n----------------------------------------\n\nTITLE: Modified Checkpoint Save Function\nDESCRIPTION: Updated save_checkpoint function that integrates DeepSpeed checkpoint saving with existing checkpoint logic.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef save_checkpoint(iteration, model, optimizer,\n                    lr_scheduler, args):\n    \"\"\"Save a model checkpoint.\"\"\"\n    if args.deepspeed:\n        save_ds_checkpoint(iteration, model, args)\n    else:\n        ......\n```\n\n----------------------------------------\n\nTITLE: Avoiding ZeRO Checkpoint Bloat with DeepSpeed in Python\nDESCRIPTION: This snippet demonstrates how to use DeepSpeed's clone_tensors_for_torch_save utility to create a lean HuggingFace model checkpoint, avoiding the bloat caused by ZeRO's tensor flattening and torch's storage management.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/model-checkpointing.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nds_config = {\n ...\n}\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16)\nds_engine, _, _, _ = deepspeed.initialize(model=model, config_params=ds_config)\nlean_state_dict = deepspeed.checkpoint.utils.clone_tensors_for_torch_save(ds_engine.module.state_dict())\nds_engine.module.save_pretrained(\"lean_after\", state_dict=lean_state_dict)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Activation Checkpointing Configuration\nDESCRIPTION: Setup code for enabling DeepSpeed's activation checkpointing features including activation partitioning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Optional DeepSpeed Activation Checkpointing Features\n#\nif args.deepspeed and args.deepspeed_activation_checkpointing:\n    set_deepspeed_activation_checkpointing(args)\n\ndef set_deepspeed_activation_checkpointing(args):\n\n    deepspeed.checkpointing.configure(mpu,\n                            deepspeed_config=args.deepspeed_config,\n                            partition_activation=True)\n\n    mpu.checkpoint = deepspeed.checkpointing.checkpoint\n    mpu.get_cuda_rng_tracker = deepspeed.checkpointing.get_cuda_rng_tracker\n    mpu.model_parallel_cuda_manual_seed =\n                    deepspeed.checkpointing.model_parallel_cuda_manual_seed\n```\n\n----------------------------------------\n\nTITLE: Setting Up WandB Logging in DeepSpeed\nDESCRIPTION: JSON configuration for enabling WandB (Weights & Biases) logging in DeepSpeed. It includes group, team, and project settings for organizing the logs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"wandb\": {\n        \"enabled\": true,\n        \"group\": \"my_group\",\n        \"team\": \"my_team\",\n        \"project\": \"my_project\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepSpeed Arguments for DCGAN in Python\nDESCRIPTION: Adds DeepSpeed configuration arguments to the DCGAN model's argument parser. This is the first step in applying DeepSpeed to the model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed\n\ndef main():\n    parser = get_argument_parser()\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args()\n    train(args)\n```\n\n----------------------------------------\n\nTITLE: Accessing CUDA RNG Tracker in DeepSpeed\nDESCRIPTION: Function to get the CUDA random number generator tracker in DeepSpeed for managing random seeds during checkpointing.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.get_cuda_rng_tracker\n```\n\n----------------------------------------\n\nTITLE: Setting Context Size for GPT Training with Ulysses-Offload\nDESCRIPTION: Configuration for setting the context size to 256K tokens and a mini-batch size of one for training a large language model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ulysses-offload.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n### Main configs\nseq_len=262144 # need to be power of 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Sparse Pruning with L1 Method in DeepSpeed\nDESCRIPTION: JSON configuration for sparse pruning using the L1 method in DeepSpeed. This example prunes self-attention modules to 50% density after 30 training steps.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n  \"sparse_pruning\":{\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"schedule_offset\": 30,\n      \"method\": \"l1\"\n    },\n    \"different_groups\":{\n      \"sp1\": {\n        \"params\": {\n            \"dense_ratio\": 0.5\n        },\n        \"modules\": [\n          \"attention.self\"\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Autotuning Output Summary\nDESCRIPTION: This text snippet shows the output of DeepSpeed autotuning, displaying the results of different tuning spaces, number of experiments, best metric values, and the best experiment configurations. It also includes the total tuning time and number of experiments.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n| tuning_space | num_experiments | best_metric_val | best_exp_name   |\n| :----------- | --------------: | --------------: | :-------------- |\n| z0           |               4 |         59.0229 | z0_gas1_tmbspg2 |\n| z1           |               5 |         87.3017 | z1_gas1_tmbspg3 |\n| z2           |               3 |         77.8338 | z2_gas1_tmbspg3 |\n| z3           |               1 |               0 | z3_gas1_tmbspg3 |\n| global       |              13 |         87.3017 | z1_gas1_tmbspg3 |\n\nTuning completed in 0:27:33.988447. Total number of experiments: 13.\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Argument Parsing Integration\nDESCRIPTION: Python function to add DeepSpeed-specific configuration arguments to the argument parser.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_arguments():\n    parser = get_argument_parser()\n    # Include DeepSpeed configuration arguments\n    parser = deepspeed.add_config_arguments(parser)\n\n    args = parser.parse_args()\n\n    return args\n```\n\n----------------------------------------\n\nTITLE: Installing 1-bit Adam Dependencies\nDESCRIPTION: Pip command to install the necessary dependencies for using 1-bit Adam optimizer in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install deepspeed[1bit_adam]\n```\n\n----------------------------------------\n\nTITLE: Pretraining GPT-3 with Curriculum Learning in Bash\nDESCRIPTION: Example bash script for pretraining GPT-3 model with curriculum learning. It includes configurations for curriculum learning in the DeepSpeed JSON config file.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexamples_deepspeed/data_efficiency/gpt/pretrain/ds_pretrain_gpt_1.3B_dense_run.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Layer Reduction in DeepSpeed\nDESCRIPTION: JSON configuration for layer reduction compression in DeepSpeed. Specifies which layers to keep and reinitialize during model compression.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compression_training\": {\n    \"layer_reduction\": {\n      \"enabled\": true,\n      \"keep_number_layer\": 5,\n      \"module_name_prefix\": \"bert.encoder.layer\",\n      \"teacher_layer\": [\n        2,\n        4,\n        6,\n        8,\n        10\n      ],\n      \"other_module_name\": [\n        \"bert.pooler\",\n        \"bert.embeddings\",\n        \"classifier\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Communication Logging in DeepSpeed\nDESCRIPTION: JSON configuration for DeepSpeed's communication logging system. Controls logging enablement, verbosity, and operation profiling settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comms_logger\": {\n    \"enabled\": true,\n    \"verbose\": false,\n    \"prof_all\": true,\n    \"debug\": false\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comms_logger\": {\n    \"enabled\": true,\n    \"verbose\": false,\n    \"prof_all\": false,\n    \"debug\": false,\n    \"prof_ops\": [\"all_reduce\", \"all_gather\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Elastic Training in DeepSpeed\nDESCRIPTION: JSON configuration for elastic training settings in DeepSpeed versions 0.1 and 0.2. Controls batch sizes, GPU allocation, and model parallelism parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"elasticity\": {\n    \"enabled\": true,\n    \"max_train_batch_size\": \"seqlen\",\n    \"micro_batch_sizes\": 8,\n    \"min_gpus\": 1024,\n    \"max_gpus\": \"fixed_linear\",\n    \"min_time\": \"seqlen\",\n    \"version\": 8,\n    \"ignore_non_elastic_batch_info\": 1024,\n    \"num_gpus_per_node\": \"fixed_linear\",\n    \"model_parallel_size\": MODEL_PARALLEL_SIZE\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Device Information for DeepSpeed Model States\nDESCRIPTION: Function that returns the devices where a specified state type is currently stored. This is useful for diagnostics and for making decisions about state management based on current allocations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_state_devices(model, state: OffloadStateTypeEnum) -> Set[torch.device]:\n    \"\"\"Retrieve the devices of the specified state of the model.\n\n    Args:\n        model (DeepSpeedEngine): The model whose device allocations are to be checked.\n        state (OffloadStateTypeEnum): The specific state for which the devices should be retrieved.\n\n    Returns:\n        Set[torch.device]: A set of devices of the specified state.\n\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pyramid-Residual MoE Layer Configuration\nDESCRIPTION: Implementation of Pyramid-Residual MoE layer with pyramid structure and residual connections.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/mixture-of-experts.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.experts = deepspeed.moe.layer.MoE(hidden_size=input_dim, expert=ExpertModule(), num_experts=[..], ep_size=ep_size, use_residual=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Head Pruning for Transformer Attention in DeepSpeed\nDESCRIPTION: JSON configuration for attention head pruning using the topK method in DeepSpeed. This example prunes a 12-head transformer model to 50% of heads after 10 training steps, with corresponding pruning in query, key, and value matrices.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n  \"head_pruning\":{\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"schedule_offset\": 10,\n      \"method\": \"topk\",\n      \"num_heads\": 12\n    },\n    \"different_groups\":{\n      \"rp1\": {\n        \"params\": {\n            \"dense_ratio\": 0.5\n        },\n        \"modules\": [\n          \"attention.output.dense\"\n        ],\n        \"related_modules\":[\n          [\"self.query\", \"self.key\", \"self.value\"]\n        ]\n      }\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed Flops Profiler with Megatron-LM\nDESCRIPTION: This shell output demonstrates the detailed profiling results of a 12-layer Megatron-LM model using the DeepSpeed Flops Profiler. It shows various metrics including params, MACs, latency, and FLOPS for different modules of the model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n-------------------------- DeepSpeed Flops Profiler --------------------------\nProfile Summary at step 10:\nNotations:\ndata parallel size (dp_size), model parallel size(mp_size),\nnumber of parameters (params), number of multiply-accumulate operations(MACs),\nnumber of floating-point operations (flops), floating-point operations per second (FLOPS),\nfwd latency (forward propagation latency), bwd latency (backward propagation latency),\nstep (weights update latency), iter latency (sum of fwd, bwd and step latency)\n\nworld size:                                                   1\ndata parallel size:                                           1\nmodel parallel size:                                          1\nbatch size per GPU:                                           1024\nparams per gpu:                                               1.29 M\nparams of model = params per GPU * mp_size:                   1.29 M\nfwd MACs per GPU:                                             41271.95 G\nfwd flops per GPU:                                            82543.9 G\nfwd flops of model = fwd flops per GPU * mp_size:             82543.9 G\nfwd latency:                                                  1.89 s\nbwd latency:                                                  5.38 s\nfwd FLOPS per GPU = fwd flops per GPU / fwd latency:          43.68 TFLOPS\nbwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:      30.7 TFLOPS\nfwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):   34.07 TFLOPS\nstep latency:                                                 34.12 s\niter latency:                                                 41.39 s\nsamples/second:                                               24.74\n\n----------------------------- Aggregated Profile per GPU -----------------------------\nTop 1 modules in terms of params, MACs or fwd latency at different model depths:\ndepth 0:\n    params      - {'GPT2Model': '1.29 M'}\n    MACs        - {'GPT2Model': '41271.95 GMACs'}\n    fwd latency - {'GPT2Model': '1.84 s'}\ndepth 1:\n    params      - {'TransformerLanguageModel': '1.29 M'}\n    MACs        - {'TransformerLanguageModel': '39584.03 GMACs'}\n    fwd latency - {'TransformerLanguageModel': '1.83 s'}\ndepth 2:\n    params      - {'ParallelTransformer': '1.29 M'}\n    MACs        - {'ParallelTransformer': '39584.03 GMACs'}\n    fwd latency - {'ParallelTransformer': '1.81 s'}\ndepth 3:\n    params      - {'ModuleList': '1.28 M'}\n    MACs        - {'ModuleList': '39584.03 GMACs'}\n    fwd latency - {'ModuleList': '1.3 s'}\ndepth 4:\n    params      - {'ParallelTransformerLayerPart2': '688.15 k'}\n    MACs        - {'ParallelTransformerLayerPart2': '26388.28 GMACs'}\n    fwd latency - {'ParallelTransformerLayerPart2': '865.73 ms'}\ndepth 5:\n    params      - {'ParallelMLP': '491.54 k'}\n    MACs        - {'ParallelMLP': '26388.28 GMACs'}\n    fwd latency - {'ParallelMLP': '849.4 ms'}\n\n------------------------------ Detailed Profile per GPU ------------------------------\nEach module profile is listed after its name in the following order:\nparams, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n\nNote: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs(or latency) and the sum of its submodules'.\n2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n\nGPT2Model(\n  1.29 M, 100.00% Params, 41271.95 GMACs, 100.00% MACs, 1.84 s, 100.00% latency, 44.78 TFLOPS,\n  (language_model): TransformerLanguageModel(\n    1.29 M, 100.00% Params, 39584.03 GMACs, 95.91% MACs, 1.83 s, 99.11% latency, 43.34 TFLOPS,\n    (embedding): Embedding(\n      2, 0.00% Params, 0 MACs, 0.00% MACs, 18.1 ms, 0.98% latency, 0.0 FLOPS,\n      (word_embeddings): VocabParallelEmbedding(1, 0.00% Params, 0 MACs, 0.00% MACs, 164.75 us, 0.01% latency, 0.0 FLOPS, )\n      (position_embeddings): Embedding(1, 0.00% Params, 0 MACs, 0.00% MACs, 489.23 us, 0.03% latency, 0.0 FLOPS, 1024, 8192)\n      (embedding_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n    )\n    (transformer): ParallelTransformer(\n      1.29 M, 100.00% Params, 39584.03 GMACs, 95.91% MACs, 1.81 s, 98.11% latency, 43.78 TFLOPS,\n      (layers): ModuleList(\n        1.28 M, 98.73% Params, 39584.03 GMACs, 95.91% MACs, 1.3 s, 70.66% latency, 60.79 TFLOPS,\n        (0): ParallelTransformerLayerPart1(\n          49.15 k, 3.80% Params, 1099.65 GMACs, 2.66% MACs, 23.5 ms, 1.27% latency, 93.6 TFLOPS,\n          (input_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 128.75 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n          (attention): ParallelSelfAttention(\n            32.77 k, 2.53% Params, 1099.65 GMACs, 2.66% MACs, 22.8 ms, 1.24% latency, 96.46 TFLOPS,\n            (query_key_value): ColumnParallelLinear(24.58 k, 1.90% Params, 824.63 GMACs, 2.00% MACs, 8.93 ms, 0.48% latency, 184.7 TFLOPS, )\n            (scale_mask_softmax): FusedScaleMaskSoftmax(0, 0.00% Params, 134.22 MMACs, 0.00% MACs, 151.16 us, 0.01% latency, 1.78 TFLOPS, )\n            (attention_dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 79.63 us, 0.00% latency, 0.0 FLOPS, p=0.1, inplace=False)\n            (dense): RowParallelLinear(8.19 k, 0.63% Params, 274.88 GMACs, 0.67% MACs, 2.67 ms, 0.14% latency, 205.81 TFLOPS, )\n          )\n        )\n        (1): ParallelTransformerLayerPart2(\n          57.35 k, 4.43% Params, 2199.02 GMACs, 5.33% MACs, 77.53 ms, 4.21% latency, 56.73 TFLOPS,\n          (post_attention_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 116.11 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n          (mlp): ParallelMLP(\n            40.96 k, 3.16% Params, 2199.02 GMACs, 5.33% MACs, 76.19 ms, 4.13% latency, 57.72 TFLOPS,\n            (dense_h_to_4h): ColumnParallelLinear(32.77 k, 2.53% Params, 1099.51 GMACs, 2.66% MACs, 10.79 ms, 0.59% latency, 203.81 TFLOPS, )\n            (dense_4h_to_h): RowParallelLinear(8.19 k, 0.63% Params, 1099.51 GMACs, 2.66% MACs, 14.38 ms, 0.78% latency, 152.95 TFLOPS, )\n          )\n        )\n        ...\n        (23): ParallelTransformerLayerPart2(...)\n      )\n      (final_layernorm): FusedLayerNorm(16.38 k, 1.27% Params, 0 MACs, 0.00% MACs, 110.86 us, 0.01% latency, 0.0 FLOPS, torch.Size([8192]), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)\n------------------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed Inference on CPU with Core Binding\nDESCRIPTION: This bash command shows how to execute a Python script for inference using DeepSpeed on CPU. It uses the --bind_cores_to_rank option to distribute CPU cores among workers efficiently, optimizing performance for multi-socket systems.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/intel-inference/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --bind_cores_to_rank <python script>\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Launch Script for 1.5B GPT-2 Model\nDESCRIPTION: Bash script modifications to configure training of a 1.5 billion parameter GPT-2 model using DeepSpeed with ZeRO stage 1 optimization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n       --model-parallel-size 1 \\\n       --num-layers 48 \\\n       --hidden-size 1600 \\\n       --num-attention-heads 16 \\\n       --batch-size 1 \\\n       --deepspeed_config ds_zero_stage_1.config \\\n```\n\n----------------------------------------\n\nTITLE: Profiling CPU and GPU Activities with PyTorch Profiler in Python\nDESCRIPTION: This code snippet demonstrates how to profile both CPU and GPU activities during a model's forward pass using PyTorch Profiler. It shows how to specify activities to profile and how to print a summary table sorted by total CUDA time.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pytorch-profiler.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith profile(activities=[\n        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_forward\"):\n        model_engine(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Enabling Flash Attention and Rotary Position Embedding\nDESCRIPTION: Configuration options to enable Flash Attention v2 and Rotary Position Embedding (RoPE) for FPDT in Megatron-DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ulysses-offload.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--use-flash-attn-v2 \\\n--use-rotary-position-embeddings \\\n--rotary-percent 0.25 \\\n--rotary-position-embeddings-theta 100000000 \\\n```\n\n----------------------------------------\n\nTITLE: Launching Inference Test Without Tensor Parallelism\nDESCRIPTION: Bash command for running inference testing without DeepSpeed and tensor parallelism, with performance data collection enabled.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/automatic-tensor-parallelism.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_gpus <num_gpus> DeepSpeedExamples/inference/huggingface/text-generation/inference-test.py --name <model> --batch_size <batch_size> --test_performance\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimized AIO Handle\nDESCRIPTION: Implementation of AIO handle configuration using the optimal parameters discovered through performance tuning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from deepspeed.ops.op_builder import AsyncIOBuilder\n>>> h = AsyncIOBuilder().load().aio_handle(block_size=1048576,\n                                           queue_depth=32,\n                                           single_submit=False,\n                                           overlap_events=True,\n                                           intra_op_parallelism=8)\n```\n\n----------------------------------------\n\nTITLE: Running BERT Pretraining with DeepSpeed on Windows\nDESCRIPTION: Command to launch BERT language model pretraining with DeepSpeed on Windows. This example is found in the DeepSpeedExamples/training/HelloDeepSpeed directory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed train_bert_ds.py --checkpoint_dir experiment_deepspeed\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning OPT-125M with LoRA and CPU Offloading\nDESCRIPTION: Command to perform supervised fine-tuning on the facebook/opt-125m model using DeepSpeed with LoRA optimization and CPU offloading for memory efficiency.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed training\\step1_supervised_finetuning\\main.py --model_name_or_path facebook/opt-125m --gradient_accumulation_steps 8 --lora_dim 128 --only_optimize_lora --print_loss --zero_stage 2 --deepspeed --dtype bf16 --offload --output_dir output\n```\n\n----------------------------------------\n\nTITLE: Configuring Channel Pruning in DeepSpeed JSON\nDESCRIPTION: JSON configuration for DeepSpeed's channel pruning feature, designed for two back-to-back CONV2d layers. It specifies shared parameters for all pruning groups and defines specific pruning ratios for targeted modules along with their related modules.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n\"channel_pruning\":{\n      \"shared_parameters\":{\n        \"enabled\": true,\n        \"schedule_offset\": 0,\n        \"method\": \"topk\"\n      },\n      \"different_groups\":{\n        \"cp1\": {\n          \"params\": {\n              \"dense_ratio\": 0.5\n          },\n          \"modules\": [\n            \"layer....conv1\"\n          ],\n          \"related_modules\": [\n            [\"layer....conv2\", \"layer....bn1\"]\n          ]\n        }\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration for DeepSpeed\nDESCRIPTION: Modified optimizer setup to handle DeepSpeed integration, particularly for FP16 training\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_optimizer(model, args):\n    \"\"\"Setup the optimizer.\"\"\"\n\n    ......\n\n    # Use Adam.\n    optimizer = Adam(param_groups,\n                     lr=args.lr, weight_decay=args.weight_decay)\n\n    if args.deepspeed:\n        # fp16 wrapper is not required for DeepSpeed.\n        return optimizer\n```\n\n----------------------------------------\n\nTITLE: Backward Pass Implementation\nDESCRIPTION: Modified backward propagation step to handle DeepSpeed's automatic gradient handling and optimization\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef backward_step(optimizer, model, lm_loss, args, timers):\n    \"\"\"Backward step.\"\"\"\n\n    # Total loss.\n    loss = lm_loss\n\n    # Backward pass.\n    if args.deepspeed:\n        model.backward(loss)\n    else:\n        optimizer.zero_grad()\n        if args.fp16:\n            optimizer.backward(loss, update_master_grads=False)\n        else:\n            loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Communication Backend Initialization\nDESCRIPTION: Example of initializing distributed communication backend in a hardware-agnostic way.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-abstraction-interface.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.init_process_group('nccl')\n```\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.init_process_group(get_accelerator().communication_backend_name())\n```\n\n----------------------------------------\n\nTITLE: Pretraining BERT with Curriculum Learning in Bash\nDESCRIPTION: Example bash script for pretraining BERT model with curriculum learning. It includes configurations for curriculum learning in the DeepSpeed JSON config file.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nexamples_deepspeed/data_efficiency/bert/pretrain/ds_pretrain_bert_336M_run.sh\n```\n\n----------------------------------------\n\nTITLE: Specifying Micro-Batch Sizes for DeepSpeed Autotuning\nDESCRIPTION: This JSON configuration specifies a list of micro-batch sizes to explore during autotuning and sets a fixed gradient accumulation step. The autotuner will consider combinations of these parameters, constrained by max_train_batch_size if defined.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_micro_batch_size_per_gpu\": [1, 4, 16],\n  \"gradient_accumulation_steps\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OneCycle Learning Rate Schedule in PyTorch\nDESCRIPTION: This JSON snippet demonstrates how to configure a OneCycle learning rate schedule based on insights gained from the Learning Rate Range Test. It specifies the minimum and maximum learning rates, and the step sizes for the first and second parts of the cycle.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/lrrt.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"OneCycle\": {\n    \"cycle_min_lr\": 0.002,\n    \"cycle_max_lr\": 0.005,\n    \"cycle_first_step_size\": 2000,\n    \"cycle_second_step_size\": 2000,\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel File Write Example\nDESCRIPTION: Demonstrates 4-way parallel file write operation using async_pwrite with intra-operation parallelism.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n>>> import os\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nFalse\n>>> import torch\n>>> t=torch.empty(1024**3, dtype=torch.uint8).cuda()\n>>> from deepspeed.ops.op_builder import AsyncIOBuilder\n>>> h = AsyncIOBuilder().load().aio_handle(intra_op_parallelism=4)\n>>> h.async_pwrite(t,'/local_nvme/test_1GB.pt')\n>>> h.wait()\n1\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nTrue\n>>> os.path.getsize('/local_nvme/test_1GB.pt')\n1073741824\n```\n\n----------------------------------------\n\nTITLE: Running CIFAR10 Pretraining with DeepSpeed on Windows\nDESCRIPTION: Command to launch CIFAR10 image classification model pretraining using DeepSpeed. The example is located in the DeepSpeedExamples training/cifar directory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed cifar10_deepspeed.py --deepspeed\n```\n\n----------------------------------------\n\nTITLE: Configuring Asynchronous I/O in DeepSpeed (JSON)\nDESCRIPTION: JSON configuration for the asynchronous I/O module in DeepSpeed, used for offloading parameter and optimizer states to persistent (NVMe) storage. It allows customization of block size, queue depth, thread count, and I/O submission behavior.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aio\": {\n    \"block_size\": 1048576,\n    \"queue_depth\": 8,\n    \"thread_count\": 1,\n    \"single_submit\": false,\n    \"overlap_events\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling DeepSpeed Sparse Attention via Command Line\nDESCRIPTION: Command line argument to enable sparse attention feature in DeepSpeed for supporting long sequences.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n--deepspeed_sparse_attention\n```\n\n----------------------------------------\n\nTITLE: Running GPT-2 Finetuning with Random-LTD in DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run GPT-2 finetuning examples with random-LTD using DeepSpeed. It includes commands for both base and medium model configurations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nDeepSpeedExamples/data_efficiency/gpt_finetuning$ pip install -r requirement.txt\nDeepSpeedExamples/data_efficiency/gpt_finetuning$ bash ./bash_script/run_base_random_ltd.sh\nDeepSpeedExamples/data_efficiency/gpt_finetuning$ bash ./bash_script/run_medium_random_ltd.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Structured Sparse Pruning with SNIP Momentum in DeepSpeed\nDESCRIPTION: JSON configuration for structured sparse pruning using SNIP momentum method with a 4x1 block pattern. This example applies global pruning to all layers except classifier and pooler with 40% density.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n  \"sparse_pruning\":{\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"schedule_offset\": 30,\n      \"schedule_offset_end\": 90,\n      \"schedule_offset_stride\": 15,\n      \"method\": \"snip_momentum\",\n      \"block_pattern\": \"4x1\",\n      \"dense_ratio\": 0.4,\n      \"excluded_modules\": ['classifier', 'pooler']\n    },\n    \"different_groups\":{\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed on Multiple CPU Nodes\nDESCRIPTION: Command to launch DeepSpeed across multiple CPU nodes using MPI. This setup is suitable for distributed training or inference on CPU clusters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --hostfile=<hostfile> --bind_cores_to_rank --launcher impi --master_addr <master-ip> <deepspeed-model-script>\n```\n\n----------------------------------------\n\nTITLE: Validating XPU Availability and Accelerator Selection\nDESCRIPTION: Python code to check XPU availability and confirm if the XPU accelerator is correctly chosen in DeepSpeed. This is useful for verifying the setup on Intel XPU systems.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch; print('torch:', torch.__version__)\nimport intel_extension_for_pytorch; print('XPU available:', torch.xpu.is_available())\nfrom deepspeed.accelerator import get_accelerator; print('accelerator:', get_accelerator()._name)\n```\n\n----------------------------------------\n\nTITLE: Device Name Usage Example\nDESCRIPTION: Examples showing proper usage of device name allocation with the accelerator interface.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-abstraction-interface.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.empty(weight_shape, dtype=dtype, device=get_accelerator().current_device_name())\n```\n\n----------------------------------------\n\nTITLE: Applying 1-bit Quantization after Layer Reduction\nDESCRIPTION: This command executes the script for performing 1-bit quantization on a layer-reduced BERT model using DeepSpeed Compression.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ bash bash_script/XTC/layer_reduction_1bit.sh\n```\n\n----------------------------------------\n\nTITLE: Device Index Retrieval\nDESCRIPTION: Example of getting the current device index using the accelerator interface.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-abstraction-interface.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlocal_rank = get_accelerator().current_device()\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO Stages for Autotuning\nDESCRIPTION: JSON configuration to limit which ZeRO optimization stages the autotuner will explore. This example restricts autotuning to only test ZeRO stages 2 and 3, skipping stages 0 and 1.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"zero_optimization\": {\n    \"stage\": [2, 3]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpoint Options in DeepSpeed JSON\nDESCRIPTION: JSON configuration for DeepSpeed's checkpoint management system, controlling tag validation behavior, universal checkpoint loading, node-local storage usage, and parallel checkpoint writing settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n\"checkpoint\": {\n    \"tag_validation\"=\"Warn\",\n    \"load_universal\"=false,\n    \"use_node_local_storage\"=false,\n    \"parallel_write\":{\n        \"pipeline_stage\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Sparse Pruning Example in DeepSpeed\nDESCRIPTION: Commands to install requirements and run the sparse pruning example script in DeepSpeedExamples. This demonstrates how to use sparse pruning in practice.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/pruning_sparse.sh\n```\n\n----------------------------------------\n\nTITLE: Installing and Running BERT Inference with Triton in DeepSpeed\nDESCRIPTION: This code snippet shows how to install DeepSpeed with Triton support and run a BERT model inference example with Triton optimization. It clones the example repository and executes a test script with the Triton flag.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-triton/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install deepspeed[triton]\n\ngit clone https://github.com/deepspeedai/DeepSpeedExamples.git\ncd DeepSpeedExamples/inference/huggingface/fill-mask\n\ndeepspeed --num_gpus 1 test-bert.py --triton\n```\n\n----------------------------------------\n\nTITLE: Configuring Activation Quantization in DeepSpeed\nDESCRIPTION: JSON configuration for activation quantization in DeepSpeed. This example shows how to enable asymmetric quantization with 8-bit precision for attention output modules with dynamic range calibration.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n\"compression_training\": {\n  \"activation_quantization\": {\n    \"shared_parameters\":{\n      \"enabled\": true,\n      \"quantization_type\": \"asymmetric\",\n      \"range_calibration\": \"dynamic\",\n      \"schedule_offset\": 50\n    },\n    \"different_groups\":{\n      \"aq1\": {\n        \"params\": {\n            \"bits\": 8\n        },\n        \"modules\": [\n          \"attention.output\"\n        ]\n      }\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeroOneAdam Optimizer in DeepSpeed\nDESCRIPTION: Configuration for ZeroOneAdam optimizer, which is a variant of 1-bit Adam that further optimizes via adaptive variance freezing and 1-bit synchronization over optimizer states.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"optimizer\": {\n    \"type\": \"ZeroOneAdam\",\n    \"params\": {\n      \"lr\": 1e-3,\n      \"weight_decay\": 0.01,\n      \"bias_correction\": false,\n      \"var_freeze_step\": 1000,\n      \"var_update_scaler\": 16,\n      \"local_step_scaler\": 1000,\n      \"local_step_clipper\": 16,\n      \"cuda_aware\": false,\n      \"comm_backend_name\": \"nccl\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Llama-2-7B using ZeRO-Inference and CPU Offloading\nDESCRIPTION: Command to run token generation inference with Llama-2-7B using DeepSpeed's ZeRO-Inference with CPU offloading to handle models larger than GPU memory capacity.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed run_model.py --model meta-llama/Llama-2-7b-hf --batch-size 64 --prompt-len 8 --gen-len 32 --cpu-offload\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Arguments with argparse\nDESCRIPTION: Demonstrates how to set up command-line argument parsing for DeepSpeed training using argparse. Shows integration of DeepSpeed's built-in arguments with custom application arguments.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/initialize.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nparser = argparse.ArgumentParser(description='My training script.')\nparser.add_argument('--local_rank', type=int, default=-1,\n                    help='local rank passed from distributed launcher')\n# Include DeepSpeed configuration arguments\nparser = deepspeed.add_config_arguments(parser)\ncmd_args = parser.parse_args()\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed 1-bit Adam Training with deepspeed\nDESCRIPTION: Shell command to launch DeepSpeed 1-bit Adam training using the deepspeed launcher. This automatically detects available resources and launches the job.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbash run_squad_deepspeed_onebitadam.sh <PATH_TO_OUTPUT_DIR>\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradient Clipping in DeepSpeed\nDESCRIPTION: JSON configuration for enabling gradient clipping in DeepSpeed training. Specifies the maximum gradient norm threshold for clipping.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"gradient_clipping\": 1.0\n}\n```\n\n----------------------------------------\n\nTITLE: Running Channel Pruning Example for ResNet with DeepSpeed\nDESCRIPTION: Shell commands to install PyTorch and torchvision, then run the channel pruning example for ResNet using DeepSpeed. This demonstrates channel pruning for convolutional neural networks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install torch torchvision\nDeepSpeedExamples/compression/cifar$ bash run_compress.sh\n```\n\n----------------------------------------\n\nTITLE: Handling Input Padding for Sparse Attention\nDESCRIPTION: Utility code for padding and unpadding inputs/outputs to work with sparse attention blocks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif self.sparse_attention_config is not None:\n   pad_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds = SparseAttentionUtils.pad_to_block_size(\n      block_size=self.sparse_attention_config.block,\n      input_ids=input_ids,\n      attention_mask=extended_attention_mask,\n      token_type_ids=token_type_ids,\n      position_ids=None,\n      inputs_embeds=None,\n      pad_token_id=self.pad_token_id,\n      model_embeddings=self.embeddings)\n.\n.\n.\n# If BertEncoder uses sparse attention, and input_ids were padded, sequence output needs to be unpadded to original length\nif self.sparse_attention_config is not None and pad_len > 0:\n   encoded_layers[-1] = SparseAttentionUtils.unpad_sequence_output(pad_len, encoded_layers[-1])\n```\n\n----------------------------------------\n\nTITLE: Installing FlashAttention for DeepSpeed-Ulysses in Bash\nDESCRIPTION: This bash script demonstrates how to install FlashAttention, which can be combined with DeepSpeed-Ulysses for improved performance. It clones the FlashAttention repository and installs the package.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds-sequence.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# install\ncd ${WORK_DIR}\ngit clone -b v1.0.4 https://github.com/HazyResearch/flash-attention\ncd flash-attention\npython -m pip install .\n```\n\n----------------------------------------\n\nTITLE: Converting ZeRO Checkpoint to FP32 Weights\nDESCRIPTION: Bash command to use the zero_to_fp32.py script for offline consolidation of ZeRO checkpoints into FP32 weights.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /path/to/checkpoint_dir\n$ ./zero_to_fp32.py . pytorch_model.bin\n```\n\n----------------------------------------\n\nTITLE: Enabling input gradients for LoRA-only optimization with gradient checkpointing\nDESCRIPTION: This code snippet enables gradients for input embeddings, allowing the combination of gradient checkpointing and LoRA-only parameter optimization in Step 3 training of DeepSpeed-Chat.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel.enable_input_require_grads()\n```\n\n----------------------------------------\n\nTITLE: Configuring BFLOAT16 Training Options in DeepSpeed\nDESCRIPTION: JSON configuration for BFLOAT16 training format, which requires hardware support like NVIDIA A100. Simpler configuration than FP16 as it doesn't require loss scaling.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"enabled\": true\n }\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Flops Profiler in Training Workflow\nDESCRIPTION: Example demonstrating how to integrate DeepSpeed's FlopsProfiler into a training workflow. Shows profiling setup, start/stop timing, and collecting metrics during training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/flops-profiler.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.profiling.flops_profiler import FlopsProfiler\n\nmodel = Model()\nprof = FlopsProfiler(model)\n\nprofile_step = 5\nprint_profile= True\n\nfor step, batch in enumerate(data_loader):\n  # start profiling at training step \"profile_step\"\n  if step == profile_step:\n    prof.start_profile()\n\n  # forward() method\n  loss = model(batch)\n\n  # end profiling and print output\n  if step == profile_step: # if using multi nodes, check global_rank == 0 as well\n    prof.stop_profile()\n    flops = prof.get_total_flops()\n    macs = prof.get_total_macs()\n    params = prof.get_total_params()\n    if print_profile:\n        prof.print_model_profile(profile_step=profile_step)\n    prof.end_profile()\n\n  # runs backpropagation\n  loss.backward()\n\n  # weight update\n  optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Setting Fixed Micro-Batch Size for DeepSpeed Autotuning\nDESCRIPTION: This JSON configuration sets a fixed micro-batch size per GPU of 4 for the autotuning process. This differs from specifying a list, as it doesn't ask the autotuner to explore different sizes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"train_micro_batch_size_per_gpu\": [4],\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration for Autotuning in JSON\nDESCRIPTION: This JSON configuration file sets up DeepSpeed for autotuning. It specifies that train micro batch size and gradient accumulation steps should be automatically determined, and enables autotuning with argument mappings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"gradient_accumulation_steps\": \"auto\",\n  \"autotuning\": {\n    \"enabled\": true,\n    \"arg_mappings\": {\n      \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n      \"gradient_accumulation_steps\": \"--gradient_accumulation_steps\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Wall Clock Breakdown Analysis\nDESCRIPTION: JSON configuration to enable detailed wall clock time breakdown analysis for different parts of the training process.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"wall_clock_breakdown\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Running Activation Quantization Example in DeepSpeed\nDESCRIPTION: Commands to install requirements and run the activation quantization example script in DeepSpeedExamples. This demonstrates how to use activation quantization in practice.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/quant_activation.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Curriculum Learning in DeepSpeed\nDESCRIPTION: JSON configuration for enabling curriculum learning in DeepSpeed with sequence length-based difficulty scaling. Supports fixed linear scheduling with customizable difficulty progression from 8 to 1024 over 40000 steps.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"curriculum_learning\": {\n    \"enabled\": true,\n    \"curriculum_type\": \"seqlen\",\n    \"min_difficulty\": 8,\n    \"max_difficulty\": 1024,\n    \"schedule_type\": \"fixed_linear\",\n    \"schedule_config\": {\n      \"total_curriculum_step\": 40000,\n      \"difficulty_step\": 8\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed from source\nDESCRIPTION: Installs DeepSpeed in JIT mode from the cloned GitHub repository. This installation is quick as it doesn't compile C++/CUDA source files.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: Analyzing GPT-3 Data for Curriculum Learning in Bash\nDESCRIPTION: Example bash script for analyzing GPT-3 data as part of the curriculum learning process. This script is used in the Map stage of the data analysis, computing difficulty values for each data sample.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nexamples_deepspeed/data_efficiency/gpt/ds_analyze_*_data_map.sh\n```\n\n----------------------------------------\n\nTITLE: Creating LayerContainer for Transformer Parameters\nDESCRIPTION: This code defines an ExampleContainer class that inherits from LayerContainer to manage transformer layer parameters. It includes type annotations for parameters and a mapping dictionary to connect the source model parameter names to dependencies.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/inference/v2/model_implementations/AddingAModel.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.inference.module_implementations.layer_container_base import LayerContainer\n\nclass ExampleContainer(LayerContainer):\n    qkvw: UnfusedQKVParameter\n\n    attn_o: AttentionOutputParameter\n\n    PARAM_MAPPING: {\n        \"self_attn.q_proj.weight\": \"qkvw.query\",\n        \"self_attn.k_proj.weight\": \"qkvw.key\",\n        \"self_attn.v_proj.weight\": \"qkvw.value\",\n        \"self_attn.o_proj.weight\": \"attn_o.params\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed Model Tests\nDESCRIPTION: Executes model convergence tests which require 4 GPUs and DeepSpeedExamples data. Must be run from the tests/model directory after installing DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/contributing.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd tests/model/\npytest run_sanity_check.py\n```\n\n----------------------------------------\n\nTITLE: Adding DeepSpeed Transformer Kernel Argument in Python\nDESCRIPTION: Adds a command-line argument to enable the DeepSpeed Transformer Kernel in the argument parser.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nparser.add_argument('--deepspeed_transformer_kernel',\n                     default=False,\n                     action='store_true',\n                     help='Use DeepSpeed transformer kernel to accelerate.')\n```\n\n----------------------------------------\n\nTITLE: Importing SparseSelfAttention Module from DeepSpeed\nDESCRIPTION: Demonstrates how to import the SparseSelfAttention module from DeepSpeed for standalone use in any Transformer model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.ops.sparse_attention import SparseSelfAttention\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Autotuning Parameters in JSON\nDESCRIPTION: Default configuration object for DeepSpeed autotuning showing all available parameters and their default values. Controls aspects like enabled state, results directory, profiling steps, tuning algorithms, and batch sizes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"autotuning\": {\n    \"enabled\": false,\n    \"results_dir\": null,\n    \"exps_dir\": null,\n    \"overwrite\": false,\n    \"metric\": \"throughput\",\n    \"start_profile_step\": 3,\n    \"end_profile_step\": 5,\n    \"fast\": true,\n    \"max_train_batch_size\": null,\n    \"mp_size\": 1,\n    \"num_tuning_micro_batch_sizes\": 3,\n    \"tuner_type\": \"model_based\",\n    \"tuner_early_stopping\": 5,\n    \"tuner_num_trials\": 50,\n    \"arg_mappings\": null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building DeepSpeed binary wheel\nDESCRIPTION: Creates a binary wheel for DeepSpeed that can be installed on multiple machines with the same GPU type and software environment.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nDS_BUILD_OPS=1 python -m build --wheel --no-isolation --config-setting=\"--build-option=build_ext\" --config-setting=\"--build-option=-j8\"\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed Without SSH\nDESCRIPTION: Command structure for launching DeepSpeed training without passwordless SSH, specifying node configuration parameters including hostfile, node rank, and master node details.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/getting-started.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --hostfile=myhostfile --no_ssh --node_rank=<n> \\\n    --master_addr=<addr> --master_port=<port> \\\n    <client_entry.py> <client args> \\\n    --deepspeed --deepspeed_config ds_config.json\n```\n\n----------------------------------------\n\nTITLE: Launching 1-bit Adam with DeepSpeed\nDESCRIPTION: Example command to launch a script using 1-bit Adam optimizer with the DeepSpeed launcher, specifying the MPI implementation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndeepspeed --launcher=[mvapich|openmpi] script.py\n```\n\n----------------------------------------\n\nTITLE: Building DeepSpeed for specific GPU architectures\nDESCRIPTION: Builds DeepSpeed for a specific range of GPU architectures by setting the TORCH_CUDA_ARCH_LIST environment variable.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_CUDA_ARCH_LIST=\"6.1;7.5;8.6\" pip install ...\n```\n\n----------------------------------------\n\nTITLE: Performing Layer Reduction for BERT-base\nDESCRIPTION: This command runs the script for reducing the number of layers in a BERT-base model using DeepSpeed Compression.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ bash bash_script/XTC/layer_reduction.sh\n```\n\n----------------------------------------\n\nTITLE: Synchronous File Write Example\nDESCRIPTION: Demonstrates blocking file write operation using sync_pwrite to store a 1GB CUDA tensor to NVMe storage.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n>>> import os\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nFalse\n>>> import torch\n>>> t=torch.empty(1024**3, dtype=torch.uint8).cuda()\n>>> from deepspeed.ops.op_builder import AsyncIOBuilder\n>>> h = AsyncIOBuilder().load().aio_handle()\n>>> h.sync_pwrite(t,'/local_nvme/test_1GB.pt')\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nTrue\n>>> os.path.getsize('/local_nvme/test_1GB.pt')\n1073741824\n```\n\n----------------------------------------\n\nTITLE: Azure ML Training Script Reference\nDESCRIPTION: Instructions for running model training experiments on Azure using Azure ML recipes. Users need to set up their Azure ML workspace and use the provided submission script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-07-26-deepspeed-azure.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naml_submit.py\n```\n\n----------------------------------------\n\nTITLE: Integrating DeepSpeed-Ulysses Distributed Attention in Python\nDESCRIPTION: This snippet demonstrates how to replace the original self-attention with DeepSpeed-Ulysses's distributed attention. It requires importing the DistributedAttention class and using it to wrap the existing attention module.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.sequence.layer import DistributedAttention\n\n# Replace the original self-attention (attn) with DeepSpeed-Ulysses's self-attention\n\ndist_attn = DistributedAttention(attn, get_sequence_parallel_group())\n```\n\n----------------------------------------\n\nTITLE: Configuring ZeRO-3 for Weight Extraction in JSON\nDESCRIPTION: JSON configuration to enable gathering of 16-bit weights when saving a ZeRO-3 optimized model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up CSV Logging in DeepSpeed\nDESCRIPTION: JSON configuration for enabling CSV file logging in DeepSpeed. It specifies the output path and job name for the log files.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"csv_monitor\": {\n        \"enabled\": true,\n        \"output_path\": \"output/ds_logs/\",\n        \"job_name\": \"train_bert\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parallelizing DeepSpeed build process\nDESCRIPTION: Speeds up the build-all process by parallelizing the compilation using 8 CPU cores. Adjust -j to specify the number of cores to use.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nDS_BUILD_OPS=1 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\"\n```\n\n----------------------------------------\n\nTITLE: Installing CIFAR-10 Requirements with Git\nDESCRIPTION: Commands to initialize git submodules and install required dependencies for the CIFAR-10 model\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\ncd DeepSpeedExamples/cifar\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Manual Seed for Model Parallel CUDA in DeepSpeed\nDESCRIPTION: Function to manually set the seed for CUDA random number generation in model parallel scenarios in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.model_parallel_cuda_manual_seed\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Communication Logging in JSON\nDESCRIPTION: JSON configuration for enabling and customizing communication logging in DeepSpeed. Includes options for verbosity, profiling all operations, and debug mode.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comms_logger\": {\n    \"enabled\": true,\n    \"verbose\": false,\n    \"prof_all\": true,\n    \"debug\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Head Pruning Example for BERT with DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run the head pruning example for BERT using DeepSpeed. This shows how to apply head pruning to transformer models with multi-head attention.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/pruning_head.sh\n```\n\n----------------------------------------\n\nTITLE: Asynchronous File Write Example\nDESCRIPTION: Shows non-blocking file write operation using async_pwrite with waiting mechanism to store a 1GB CUDA tensor.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n>>> import os\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nFalse\n>>> import torch\n>>> t=torch.empty(1024**3, dtype=torch.uint8).cuda()\n>>> from deepspeed.ops.op_builder import AsyncIOBuilder\n>>> h = AsyncIOBuilder().load().aio_handle()\n>>> h.async_pwrite(t,'/local_nvme/test_1GB.pt')\n>>> h.wait()\n1\n>>> os.path.isfile('/local_nvme/test_1GB.pt')\nTrue\n>>> os.path.getsize('/local_nvme/test_1GB.pt')\n1073741824\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed ZeRO++ Settings\nDESCRIPTION: JSON configuration for enabling all three ZeRO++ optimizations (quantized weights, hierarchical partitioning, and quantized gradients) in DeepSpeed. Sets up stage 3 optimization with specific bucket sizes and communication parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zeropp.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"reduce_bucket_size\": 10000000,\n        \"reduce_scatter\": true,\n\n        \"zero_quantized_weights\": true,\n        \"zero_hpz_partition_size\": 16,\n        \"zero_quantized_gradients\": true,\n\n        \"contiguous_gradients\": true,\n        \"overlap_comm\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed FLOPS Profiler in Training Workflow\nDESCRIPTION: Example demonstrating how to integrate DeepSpeed's FlopsProfiler into a training workflow. Shows how to start/stop profiling at specific steps and collect metrics during training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.profiling.flops_profiler import FlopsProfiler\n\nmodel = Model()\nprof = FlopsProfiler(model)\n\nprofile_step = 5\nprint_profile= True\n\nfor step, batch in enumerate(data_loader):\n  # start profiling at training step \"profile_step\"\n  if step == profile_step:\n    prof.start_profile()\n\n  # forward() method\n  loss = model(batch)\n\n  # end profiling and print output\n  if step == profile_step: # if using multi nodes, check global_rank == 0 as well\n    prof.stop_profile()\n    flops = prof.get_total_flops()\n    macs = prof.get_total_macs()\n    params = prof.get_total_params()\n    if print_profile:\n        prof.print_model_profile(profile_step=profile_step)\n    prof.end_profile()\n\n  # runs backpropagation\n  loss.backward()\n\n  # weight update\n  optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed-FP6 Benchmarks\nDESCRIPTION: This snippet provides a link to a bash script for benchmarking DeepSpeed-FP6. The script is located in the DeepSpeedExamples repository on GitHub.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhttps://github.com/deepspeedai/DeepSpeedExamples/blob/master/benchmarks/inference/mii/run_fp6.sh\n```\n\n----------------------------------------\n\nTITLE: GPU Memory Management Example\nDESCRIPTION: Demonstration of memory management in DeepSpeed ZeRO stage 3, showing how to clear cached parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith zero.Init():\n    model = MyLargeModel()\n\nds_engine, _, _, _ = deepspeed.initialize(model, ...)\nfor batch in ...:\n    loss = ds_engine(batch)\n    ds_engine.backward(batch)\n    ds_engine.step()\n\n# Free GPU memory consumed by model parameters\nds_engine.empty_partition_cache()\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed 1-bit Adam Training with mpirun\nDESCRIPTION: Example mpirun command to launch DeepSpeed 1-bit Adam training on multiple nodes with InfiniBand support using MVAPICH2 library.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np 32 -ppn 4 -hostfile hosts -env MV2_USE_CUDA=1 -env MV2_SUPPORT_DL=1 -env MV2_ENABLE_AFFINITY=0 -env MV2_SMP_USE_CMA=0 bash run_squad_mpi_onebitadam.sh\n```\n\n----------------------------------------\n\nTITLE: Calculating Total Model Parameters in Python\nDESCRIPTION: This code snippet demonstrates how to calculate the total number of parameters in a PyTorch model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values()))\n```\n\n----------------------------------------\n\nTITLE: Creating conda environment for building DeepSpeed\nDESCRIPTION: Creates a conda environment with the necessary compilation toolchain and PyTorch for building DeepSpeed from source.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nconda env create -n deepspeed -f environment.yml --force\n```\n\n----------------------------------------\n\nTITLE: Output of Weight Quantization Example in DeepSpeed\nDESCRIPTION: Example output showing the accuracy of the clean model after applying weight quantization. This demonstrates the performance impact of weight quantization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nEpoch: 09 | Time: 27m 10s\nClean the best model, and the accuracy of the clean model is acc/mm-acc:0.8414671421293938/0.8422497965825875\n```\n\n----------------------------------------\n\nTITLE: Analyzing GPT Data for Curriculum Learning in Finetuning in Bash\nDESCRIPTION: Example bash script for analyzing GPT data for curriculum learning in the context of finetuning. This is necessary for metrics that require data analysis, such as vocabulary rarity.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\ndata_efficiency/gpt_finetuning/finetune/ds_analyze_gpt_data_*\n```\n\n----------------------------------------\n\nTITLE: Implementing Dense Attention in PyTorch\nDESCRIPTION: Original dense attention implementation showing matrix multiplication and softmax operations for self-attention calculation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nattention_scores = torch.matmul(query_layer, key_layer)\nattention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\n# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\nattention_scores = attention_scores + attention_mask\n\npdtype = attention_scores.dtype\n# Normalize the attention scores to probabilities.\nattention_probs = self.softmax(attention_scores)\n\n# This is actually dropping out entire tokens to attend to, which might\n# seem a bit unusual, but is taken from the original Transformer paper.\nattention_probs = self.dropout(attention_probs)\n\ncontext_layer = torch.matmul(attention_probs, value_layer)\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed Autotuner Class in Python\nDESCRIPTION: This snippet shows the import statement for the DeepSpeed autotuner class. It uses the autoclass directive from Sphinx to generate API documentation for the autotuner class, including its members.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/autotuning.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: deepspeed.autotuning.autotuner\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Communication Volume Ratio Formula for 1-bit Adam vs Original Adam\nDESCRIPTION: Mathematical formula showing how to calculate the communication volume ratio between original Adam and 1-bit Adam, where warmup represents the warmup stage percentage. For 15% warmup, original Adam requires 5x more communication than 1-bit Adam.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-09-onebit-adam-blog-post.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n1 / (warmup + (1 – warmup)/16)\n```\n\n----------------------------------------\n\nTITLE: Modified Checkpoint Load Function\nDESCRIPTION: Updated load_checkpoint function that implements DeepSpeed checkpoint loading functionality.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef load_checkpoint(model, optimizer, lr_scheduler, args):\n    \"\"\"Load a model checkpoint.\"\"\"\n\n    iteration, release = get_checkpoint_iteration(args)\n\n    if args.deepspeed:\n        checkpoint_name, sd = model.load_checkpoint(args.load, iteration)\n\n        if checkpoint_name is None:\n            if mpu.get_data_parallel_rank() == 0:\n                print(\"Unable to load checkpoint.\")\n            return iteration\n    else:\n        ......\n```\n\n----------------------------------------\n\nTITLE: Traditional Data Parallel Training Loop (Python)\nDESCRIPTION: Shows the equivalent traditional data parallel training loop for comparison with the pipeline parallel approach.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_iter = iter(train_loader)\nfor micro_batch in engine.gradient_accumulation_steps():\n    batch = next(data_iter)\n    loss = engine(batch)\n    engine.backward(loss)\n    engine.step()\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed with MPI-based 1-bit LAMB\nDESCRIPTION: Example command to launch DeepSpeed using the MPI-based implementation of 1-bit LAMB optimizer.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-lamb.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndeepspeed --launcher=[mvapich|openmpi] script.py\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Configuration for DeepSpeed Training\nDESCRIPTION: Python dictionary configuration for enabling TensorBoard monitoring in DeepSpeed training steps. Specifies the output path and job name for tensorboard logging.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"tensorboard\": {\n    \"enabled\": enable_tensorboard,\n    \"output_path\": f\"{tb_path}/ds_tensorboard_logs/\",\n    \"job_name\": f\"{tb_name}_tensorboard\"\n}\n```\n\n----------------------------------------\n\nTITLE: DeepNVMe Tuning Command Line Options\nDESCRIPTION: Complete command line options for ds_nvme_tune utility showing available configuration parameters for performance tuning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nusage: ds_nvme_tune [-h] --nvme_dir NVME_DIR [NVME_DIR ...] [--sweep_config SWEEP_CONFIG] [--no_read] [--no_write] [--io_size IO_SIZE] [--gpu] [--gds] [--flush_page_cache] [--log_dir LOG_DIR] [--loops LOOPS] [--verbose]\n\noptions:\n  -h, --help            show this help message and exit\n  --nvme_dir NVME_DIR [NVME_DIR ...]\n                        Directory in which to perform I/O tests. A writeable directory on a NVMe device.\n  --sweep_config SWEEP_CONFIG\n                        Performance sweep configuration json file.\n  --no_read             Disable read performance measurements.\n  --no_write            Disable write performance measurements.\n  --io_size IO_SIZE     Number of I/O bytes to read/write for performance measurements.\n  --gpu                 Test tensor transfers between GPU device and NVME device.\n  --gds                 Run the sweep over NVIDIA GPUDirectStorage operator\n  --flush_page_cache    Page cache will not be flushed and reported read speeds may be higher than actual ***Requires sudo access***.\n  --log_dir LOG_DIR     Output directory for performance log files. Default is ./_aio_bench_logs\n  --loops LOOPS         Count of operation repetitions\n  --verbose             Print debugging information.\n```\n\n----------------------------------------\n\nTITLE: Output of Sparse Pruning Example in DeepSpeed\nDESCRIPTION: Example output showing the accuracy of the clean model after applying sparse pruning. This demonstrates the performance impact of sparse pruning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nEpoch: 02 | Time: 26m 14s\nClean the best model, and the accuracy of the clean model is acc/mm-acc:0.8416709118695873/0.8447925142392189\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed GPU ZeroOneAdam\nDESCRIPTION: Python import statement for DeepSpeed's GPU-optimized ZeroOneAdam optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.runtime.fp16.onebit.zoadam.ZeroOneAdam\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Data Truncation in GPT-2 Model Forward Pass (Python)\nDESCRIPTION: Add a curriculum_seqlen argument to the forward() method in the GPT-2 model to enable sequence length-based curriculum learning. This modification is made in the megatron/model/gpt2_model.py file.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, curriculum_seqlen):\n    # Implementation details not provided in the original text\n    pass\n```\n\n----------------------------------------\n\nTITLE: Running Row Pruning Example for BERT with DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run the row pruning example for BERT using DeepSpeed. This demonstrates how to apply row pruning to transformer models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\nDeepSpeedExamples/compression/bert$ bash bash_script/pruning_row.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Learning Rate Range Test Scheduler in PyTorch\nDESCRIPTION: This JSON snippet shows how to add a Learning Rate Range Test scheduler to your PyTorch model configuration in DeepSpeed. It specifies the initial learning rate, step size, step rate, and whether to use staircase scaling.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/lrrt.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"scheduler\": {\n    \"type\": \"LRRangeTest\",\n    \"params\": {\n        \"lr_range_test_min_lr\": 0.0001,\n        \"lr_range_test_step_size\": 200,\n        \"lr_range_test_step_rate\": 5,\n        \"lr_range_test_staircase\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Function Class in DeepSpeed\nDESCRIPTION: Class that implements the checkpoint function in DeepSpeed, used for applying activation checkpointing to modules or functions.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.CheckpointFunction\n```\n\n----------------------------------------\n\nTITLE: Running SQuAD Evaluation\nDESCRIPTION: Command to evaluate the model's performance using EM and F1 scores on the development set.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython evaluate-v1.1.py <PATH_TO_DATA_DIR>/dev-v1.1.json <PATH_TO_DATA_DIR>/predictions.json\n```\n\n----------------------------------------\n\nTITLE: Setting Single GPU Training Configuration\nDESCRIPTION: DeepSpeed command parameters to configure training on a single GPU node, limiting the training to use only one GPU.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-offload.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n   deepspeed --num_nodes 1 --num_gpus 1 ...\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for DeepSpeed Compression Examples\nDESCRIPTION: This command installs the necessary requirements for running DeepSpeed Compression examples for BERT models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Model Initialization\nDESCRIPTION: Function signature showing DeepSpeed's initialize method parameters for setting up the model, optimizer, and scheduler\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef initialize(args,\n               model,\n               optimizer=None,\n               model_parameters=None,\n               training_data=None,\n               lr_scheduler=None,\n               mpu=None,\n               dist_init_required=True,\n               collate_fn=None):\n```\n\n----------------------------------------\n\nTITLE: Enabling PLD in DeepSpeed Client Script\nDESCRIPTION: Command line flag to enable Progressive Layer Dropping on Transformer blocks in the client script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--progressive_layer_drop\n```\n\n----------------------------------------\n\nTITLE: Listing DeepSpeed Dependencies in Python\nDESCRIPTION: This snippet enumerates the required Python packages for the DeepSpeed project. It includes libraries for array operations, data serialization, system monitoring, and deep learning frameworks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\neinops\nhjson\nmsgpack\nninja\nnumpy\npackaging>=20.0\npsutil\npy-cpuinfo\npydantic>=2.0.0\ntorch\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Configuring WarmupLR Scheduler in DeepSpeed\nDESCRIPTION: Configuration example for the WarmupLR scheduler in DeepSpeed, showing parameters for minimum and maximum learning rates during warmup and the number of warmup steps.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"scheduler\": {\n      \"type\": \"WarmupLR\",\n      \"params\": {\n          \"warmup_min_lr\": 0,\n          \"warmup_max_lr\": 0.001,\n          \"warmup_num_steps\": 1000\n      }\n  }\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Batch Size Configuration Example\nDESCRIPTION: Example configuration showing the relationship between batch size parameters. The train_batch_size (32) represents the total effective batch size across all GPUs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"train_batch_size\": 32,\n  \"train_micro_batch_size_per_gpu\": 4,\n  \"gradient_accumulation_steps\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Running Inference with LLAMA2-7B using CPU Offloading\nDESCRIPTION: Command to run token generation inference using LLAMA2-7B with DeepSpeed's ZeRO-Inference feature, which offloads model weights to CPU memory when GPU memory is insufficient.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed run_model.py --model meta-llama/Llama-2-7b-hf --batch-size 64 --prompt-len 8 --gen-len 32 --cpu-offload\n```\n\n----------------------------------------\n\nTITLE: Checking Activation Checkpointing Configuration in DeepSpeed\nDESCRIPTION: Function to check if activation checkpointing has been configured in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.is_configured\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Output Log\nDESCRIPTION: Detailed training output showing initialization, configuration settings, training progress, and final model accuracy metrics across different classes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed.pt cifar10_deepspeed.py --deepspeed_config ds_config.json\nWarning: Permanently added '[192.168.0.22]:42227' (ECDSA) to the list of known hosts.\ncmd=['pdsh', '-w', 'worker-0', 'export NCCL_VERSION=2.4.2; ', 'cd /data/users/deepscale/test/ds_v2/examples/cifar;', '/usr/bin/python', '-u', '-m', 'deepspeed.pt.deepspeed_launch', '--world_info=eyJ3b3JrZXItMCI6IFswXX0=', '--node_rank=%n', '--master_addr=192.168.0.22', '--master_port=29500', 'cifar10_deepspeed.py', '--deepspeed', '--deepspeed_config', 'ds_config.json']\nworker-0: Warning: Permanently added '[192.168.0.22]:42227' (ECDSA) to the list of known hosts.\nworker-0: 0 NCCL_VERSION 2.4.2\nworker-0: WORLD INFO DICT: {'worker-0': [0]}\nworker-0: nnodes=1, num_local_procs=1, node_rank=0\nworker-0: global_rank_mapping=defaultdict(<class 'list'>, {'worker-0': [0]})\nworker-0: dist_world_size=1\nworker-0: Setting CUDA_VISIBLE_DEVICES=0\nworker-0: Files already downloaded and verified\nworker-0: Files already downloaded and verified\nworker-0:  bird   car horse  ship\nworker-0: DeepSpeed info: version=2.1, git-hash=fa937e7, git-branch=master\nworker-0: [INFO 2020-02-06 19:53:49] Set device to local rank 0 within node.\nworker-0: 1 1\nworker-0: [INFO 2020-02-06 19:53:56] Using DeepSpeed Optimizer param name adam as basic optimizer\nworker-0: [INFO 2020-02-06 19:53:56] DeepSpeed Basic Optimizer = FusedAdam (\nworker-0: Parameter Group 0\nworker-0:     betas: [0.8, 0.999]\nworker-0:     bias_correction: True\nworker-0:     eps: 1e-08\nworker-0:     lr: 0.001\nworker-0:     max_grad_norm: 0.0\nworker-0:     weight_decay: 3e-07\nworker-0: )\nworker-0: [INFO 2020-02-06 19:53:56] DeepSpeed using configured LR scheduler = WarmupLR\nworker-0: [INFO 2020-02-06 19:53:56] DeepSpeed LR Scheduler = <deepspeed.pt.deepspeed_lr_schedules.WarmupLR object at 0x7f64c4c09c18>\nworker-0: [INFO 2020-02-06 19:53:56] rank:0 step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\nworker-0: DeepSpeedLight configuration:\nworker-0:   allgather_size ............... 500000000\nworker-0:   allreduce_always_fp32 ........ False\nworker-0:   disable_allgather ............ False\nworker-0:   dump_state ................... False\nworker-0:   dynamic_loss_scale_args ...... None\nworker-0:   fp16_enabled ................. False\nworker-0:   global_rank .................. 0\nworker-0:   gradient_accumulation_steps .. 1\nworker-0:   gradient_clipping ............ 0.0\nworker-0:   initial_dynamic_scale ........ 4294967296\nworker-0:   loss_scale ................... 0\nworker-0:   optimizer_name ............... adam\nworker-0:   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\nworker-0:   prescale_gradients ........... False\nworker-0:   scheduler_name ............... WarmupLR\nworker-0:   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\nworker-0:   sparse_gradients_enabled ..... False\nworker-0:   steps_per_print .............. 2000\nworker-0:   tensorboard_enabled .......... False\nworker-0:   tensorboard_job_name ......... DeepSpeedJobName\nworker-0:   tensorboard_output_path ......\nworker-0:   train_batch_size ............. 4\nworker-0:   train_micro_batch_size_per_gpu  4\nworker-0:   wall_clock_breakdown ......... False\nworker-0:   world_size ................... 1\nworker-0:   zero_enabled ................. False\nworker-0:   json = {\nworker-0:     \"optimizer\":{\nworker-0:         \"params\":{\nworker-0:             \"betas\":[\n                0.8,\n                0.999\n            ],\n            \"eps\":1e-08,\n            \"lr\":0.001,\n            \"weight_decay\":3e-07\n        },\n        \"type\":\"Adam\"\n    },\n    \"scheduler\":{\n        \"params\":{\n            \"warmup_max_lr\":0.001,\n            \"warmup_min_lr\":0,\n            \"warmup_num_steps\":1000\n        },\n        \"type\":\"WarmupLR\"\n    },\n    \"steps_per_print\":2000,\n    \"train_batch_size\":4,\n    \"wall_clock_breakdown\":false\n}\nworker-0: [INFO 2020-02-06 19:53:56] 0/50, SamplesPerSec=1292.6411179579866\nworker-0: [INFO 2020-02-06 19:53:56] 0/100, SamplesPerSec=1303.6726433398537\nworker-0: [INFO 2020-02-06 19:53:56] 0/150, SamplesPerSec=1304.4251022567403\n\n......\n\nworker-0: [2, 12000] loss: 1.247\nworker-0: [INFO 2020-02-06 20:35:23] 0/24550, SamplesPerSec=1284.4954513975558\nworker-0: [INFO 2020-02-06 20:35:23] 0/24600, SamplesPerSec=1284.384033658866\nworker-0: [INFO 2020-02-06 20:35:23] 0/24650, SamplesPerSec=1284.4433482972925\nworker-0: [INFO 2020-02-06 20:35:23] 0/24700, SamplesPerSec=1284.4664449792422\nworker-0: [INFO 2020-02-06 20:35:23] 0/24750, SamplesPerSec=1284.4950124403447\nworker-0: [INFO 2020-02-06 20:35:23] 0/24800, SamplesPerSec=1284.4756105952233\nworker-0: [INFO 2020-02-06 20:35:24] 0/24850, SamplesPerSec=1284.5251526215386\nworker-0: [INFO 2020-02-06 20:35:24] 0/24900, SamplesPerSec=1284.531217073863\nworker-0: [INFO 2020-02-06 20:35:24] 0/24950, SamplesPerSec=1284.5125323220368\nworker-0: [INFO 2020-02-06 20:35:24] 0/25000, SamplesPerSec=1284.5698818883018\nworker-0: Finished Training\nworker-0: GroundTruth:    cat  ship  ship plane\nworker-0: Predicted:    cat   car   car plane\nworker-0: Accuracy of the network on the 10000 test images: 57 %\nworker-0: Accuracy of plane : 61 %\nworker-0: Accuracy of   car : 74 %\nworker-0: Accuracy of  bird : 49 %\nworker-0: Accuracy of   cat : 36 %\nworker-0: Accuracy of  deer : 44 %\nworker-0: Accuracy of   dog : 52 %\nworker-0: Accuracy of  frog : 67 %\nworker-0: Accuracy of horse : 58 %\nworker-0: Accuracy of  ship : 70 %\nworker-0: Accuracy of truck : 59 %\n```\n\n----------------------------------------\n\nTITLE: Launching with MPI Run Command\nDESCRIPTION: Alternative command to launch training using the standard mpirun launcher.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np [num processes] -ppn [num GPUs on each node] -hostfile [hostfile] [MPI flags] python [training_script.py]\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed-FastGen via pip\nDESCRIPTION: Command to install the latest DeepSpeed-MII release which includes FastGen functionality\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed-mii\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Sparse Attention in JSON\nDESCRIPTION: JSON configuration for customizing sparse attention settings in DeepSpeed. Includes options for attention mode, block size, layout, and global attention patterns.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n\"sparse_attention\": {\n    \"mode\": \"fixed\",\n    \"block\": 16,\n    \"different_layout_per_head\": true,\n    \"num_local_blocks\": 4,\n    \"num_global_blocks\": 1,\n    \"attention\": \"bidirectional\",\n    \"horizontal_global_attention\": false,\n    \"num_different_global_patterns\": 4\n}\n```\n\n----------------------------------------\n\nTITLE: External Parameter Usage in PyTorch Language Model\nDESCRIPTION: Example showing how embeddings.weight is used as an external parameter across multiple functions in a language model implementation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LanguageModel(torch.nn.Module):\n    ...\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs)\n        ...\n        logits = compute_logits(output, self.embeddings.weight)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for DeepSpeed\nDESCRIPTION: This snippet lists the required Python packages and their versions for the DeepSpeed project. It includes dependencies for documentation (autodoc_pydantic, docutils, sphinx_rtd_theme), utilities (hjson, packaging, psutil, py-cpuinfo), and core dependencies like PyTorch and tqdm.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-readthedocs.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nautodoc_pydantic>=2.0.0\ndocutils<0.18\nhjson\npackaging\npsutil\npy-cpuinfo\npydantic>=2.0.0\nrecommonmark\nsphinx_rtd_theme\ntorch\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Running ZeroQuant Example for GPT with DeepSpeed\nDESCRIPTION: Shell commands to install requirements and run the ZeroQuant example for GPT using DeepSpeed. This demonstrates efficient post-training quantization for GPT models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/gpt2$ pip install -r requirements.txt\nDeepSpeedExamples/compression/gpt2$ bash bash_script/run_zero_quant.sh\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeedTransformerConfig in Python\nDESCRIPTION: This snippet shows how to import the DeepSpeedTransformerConfig class from the deepspeed module. This class is used to configure the transformer layer for efficient BERT pre-training and fine-tuning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/kernel.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom deepspeed import DeepSpeedTransformerConfig\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Root Schedule for Curriculum Learning\nDESCRIPTION: Example configuration for the fixed_root curriculum learning schedule in DeepSpeed. Shows settings for total steps, difficulty step, and root degree.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/curriculum-learning.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"schedule_type\": \"fixed_root\",\n\"schedule_config\": {\n  \"total_curriculum_step\": 15000,\n  \"difficulty_step\": 8,\n  \"root_degree\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed via pip\nDESCRIPTION: Installs the latest release of DeepSpeed using pip. This is the quickest way to get started and doesn't require specific PyTorch or CUDA versions.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A list of required Python packages including hjson for JSON handling, tabulate for table formatting, and xgboost for gradient boosting.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-autotuning-ml.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhjson\ntabulate\nxgboost\n```\n\n----------------------------------------\n\nTITLE: Running DCGAN with DeepSpeed in Bash\nDESCRIPTION: Executes the DCGAN training script with DeepSpeed enabled, using all detected GPUs by default. Specifies the dataset, CUDA usage, DeepSpeed configuration, and TensorBoard output path.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/gan.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed gan_deepspeed_train.py --dataset celeba --cuda --deepspeed_config gan_deepspeed_config.json --tensorboard_path './runs/deepspeed'\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks Manually in DeepSpeed\nDESCRIPTION: Manually executes all pre-commit formatting checks across all files in the repository. Used to verify code formatting compliance.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/contributing.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Monitor with Multiple Backends in JSON\nDESCRIPTION: JSON configuration for enabling TensorBoard, WandB, and CSV monitoring backends in DeepSpeed. Specifies output paths, job names, and project details for each backend.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tensorboard\": {\n    \"enabled\": true,\n    \"output_path\": \"output/ds_logs/\",\n    \"job_name\": \"train_bert\"\n  }\n  \"wandb\": {\n    \"enabled\": true,\n    \"team\": \"my_team\",\n    \"group\": \"my_group\",\n    \"project\": \"my_project\"\n  }\n  \"csv_monitor\": {\n    \"enabled\": true,\n    \"output_path\": \"output/ds_logs/\",\n    \"job_name\": \"train_bert\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating Huawei Ascend NPU Setup\nDESCRIPTION: Python code to verify torch_npu installation and accelerator availability\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> print('torch:',torch.__version__)\ntorch: 2.2.0\n>>> import torch_npu\n>>> print('torch_npu:',torch.npu.is_available(),\",version:\",torch_npu.__version__)\ntorch_npu: True ,version: 2.2.0\n>>> from deepspeed.accelerator import get_accelerator\n>>> print('accelerator:', get_accelerator()._name)\naccelerator: npu\n```\n\n----------------------------------------\n\nTITLE: Specifying Neural Compressor Dependency for DeepSpeed\nDESCRIPTION: This snippet specifies the required version of neural-compressor for the DeepSpeed project. It indicates that version 2.1.0 of neural-compressor is needed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-sparse_pruning.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nneural-compressor==2.1.0\n```\n\n----------------------------------------\n\nTITLE: Importing DS4Sci_EvoformerAttention in Python\nDESCRIPTION: Code snippet showing how to import DS4Sci_EvoformerAttention from the deepspeed.ops.deepspeed4science module.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.ops.deepspeed4science import DS4Sci_EvoformerAttention\n```\n\n----------------------------------------\n\nTITLE: Example GLUE Fine-tuning Command\nDESCRIPTION: Specific example of running GLUE fine-tuning on the MNLI task with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbash run_glue_bert_base_finetune.sh MNLI 32 3e-5 5 \"fine_tune_MNLI\" deepspeed_checkpoint.pt\n```\n\n----------------------------------------\n\nTITLE: Analyzing BERT Data for Curriculum Learning in Bash\nDESCRIPTION: Example bash script for analyzing BERT data as part of the curriculum learning process. This script is used in the Map stage of the data analysis, computing difficulty values for each data sample.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nexamples_deepspeed/data_efficiency/bert/ds_analyze_*_data_map.sh\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Installation Report Example\nDESCRIPTION: Example output of the ds_report command showing DeepSpeed installation status and system information\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n--------------------------------------------------\nDeepSpeed C++/CUDA extension op report\n--------------------------------------------------\nNOTE: Ops not installed will be just-in-time (JIT) compiled at\n    runtime if needed. Op compatibility means that your system\n    meet the required dependencies to JIT install the op.\n--------------------------------------------------\nJIT compiled ops requires ninja\nninja .................. [OKAY]\n--------------------------------------------------\nop name ................ installed .. compatible\n--------------------------------------------------\ndeepspeed_not_implemented  [NO] ....... [OKAY]\nasync_io ............... [NO] ....... [OKAY]\ncpu_adagrad ............ [NO] ....... [OKAY]\ncpu_adam ............... [NO] ....... [OKAY]\ncpu_lion ............... [NO] ....... [OKAY]\nfused_adam ............. [NO] ....... [OKAY]\ntransformer_inference .. [NO] ....... [OKAY]\n--------------------------------------------------\nDeepSpeed general environment info:\ntorch install path ............... ['/root/miniconda3/envs/ds/lib/python3.10/site-packages/torch']\ntorch version .................... 2.2.0\ndeepspeed install path ........... ['/root/miniconda3/envs/ds/lib/python3.10/site-packages/deepspeed']\ndeepspeed info ................... 0.14.4, unknown, unknown\ndeepspeed wheel compiled w. ...... torch 2.2\ntorch_npu install path ........... ['/root/miniconda3/envs/ds/lib/python3.10/site-packages/torch_npu']\ntorch_npu version ................ 2.2.0\nascend_cann version .............. 8.0.RC2.alpha002\nshared memory (/dev/shm) size .... 20.00 GB\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed GPU OnebitAdam\nDESCRIPTION: Python import statement for DeepSpeed's GPU-optimized OnebitAdam optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.runtime.fp16.onebit.adam.OnebitAdam\n```\n\n----------------------------------------\n\nTITLE: Launching BERT Pre-training with DeepSpeed and PLD\nDESCRIPTION: Shell command to start BERT pre-training using DeepSpeed with Progressive Layer Dropping enabled.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbash ds_train_bert_progressive_layer_drop_bsz4k_seq128.sh\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace Transformers from Source\nDESCRIPTION: Commands to install the required HuggingFace transformers package from source code, which is necessary for running the examples.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/autotuning.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Installing and Training ChatGPT Model with DeepSpeed\nDESCRIPTION: Commands for installing DeepSpeed and training a ChatGPT-style model using the OPT-13B architecture as actor model and OPT-350M as reward model.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install deepspeed>=0.9.0\n\ngit clone https://github.com/deepspeedai/DeepSpeedExamples.git\ncd DeepSpeedExamples/applications/DeepSpeed-Chat/\npip install -r requirements.txt\n\npython train.py --actor-model facebook/opt-13b --reward-model facebook/opt-350m --deployment-type single_node\n```\n\n----------------------------------------\n\nTITLE: Installing CANN Dependencies - OpenEuler\nDESCRIPTION: Command to install required system packages on OpenEuler-based systems for CANN toolkit\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyum install -y gcc gcc-c++ make cmake unzip zlib-devel libffi-devel openssl-devel pciutils net-tools sqlite-devel lapack-devel gcc-gfortran\n```\n\n----------------------------------------\n\nTITLE: Validating DeepSpeed Installation with ds_report\nDESCRIPTION: Command to verify that DeepSpeed is properly installed on Windows by running the ds_report utility, which displays configuration details.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nds_report\n```\n\n----------------------------------------\n\nTITLE: Estimating ZeRO-2 Memory Usage with Live Model in Python\nDESCRIPTION: This snippet demonstrates how to estimate memory requirements for a 3B parameter model using ZeRO-2 with a live model. It uses the estimate_zero2_model_states_mem_needs_all_live function from DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'from transformers import AutoModel; \nfrom deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live; \nmodel = AutoModel.from_pretrained(\"t5-3b\"); \nestimate_zero2_model_states_mem_needs_all_live(model, num_gpus_per_node=8, num_nodes=1)'\n```\n\n----------------------------------------\n\nTITLE: Launching 1-bit Adam with mpirun\nDESCRIPTION: Alternative command to launch a training script using 1-bit Adam optimizer with the standard mpirun launcher.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-adam.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np [#processes] -ppn [#GPUs on each node] -hostfile [hostfile] [MPI flags] python [training_script.py]\n```\n\n----------------------------------------\n\nTITLE: Pre-installing all DeepSpeed ops\nDESCRIPTION: Attempts to install all DeepSpeed C++/CUDA ops by setting the DS_BUILD_OPS environment variable. Only ops compatible with the machine will be installed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDS_BUILD_OPS=1 pip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Referencing Sequence Length Variable\nDESCRIPTION: Reference to the sequence length variable n that determines the computational complexity of attention mechanisms.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-09-sparse-attention.md#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nn\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed GPU FusedAdam\nDESCRIPTION: Python import statement for DeepSpeed's GPU-optimized FusedAdam optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.ops.adam.FusedAdam\n```\n\n----------------------------------------\n\nTITLE: MiCS Configuration with ZeRO-3\nDESCRIPTION: Configuration for MiCS (Model States Sharding) with ZeRO stage 3, specifying shard size and hierarchical parameter gathering.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"mics_shard_size\": 8,\n        \"mics_hierarchical_params_gather\": False,\n    },\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Output of Activation Quantization Example in DeepSpeed\nDESCRIPTION: Example output showing the accuracy of the clean model after applying activation quantization. This demonstrates the performance impact of activation quantization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nEpoch: 02 | Time: 28m 50s\nClean the best model, and the accuracy of the clean model is acc/mm-acc:0.8375955170657158/0.8422497965825875\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSpeed with 0/1 Adam using MPI\nDESCRIPTION: Example command to launch DeepSpeed with 0/1 Adam optimizer using the MPI backend.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndeepspeed --launcher=[mvapich|openmpi] script.py\n```\n\n----------------------------------------\n\nTITLE: Configuring BERT Dataset Paths in JSON\nDESCRIPTION: JSON configuration for specifying paths to Wikipedia and BookCorpus datasets used in BERT pre-training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"datasets\": {\n      \"wiki_pretrain_dataset\": \"/data/bert/bnorick_format/128/wiki_pretrain\",\n      \"bc_pretrain_dataset\": \"/data/bert/bnorick_format/128/bookcorpus_pretrain\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating DeepSpeed Installation\nDESCRIPTION: Command to run DeepSpeed environment report which validates the installation and shows compatible extensions/operations for the current machine.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nds_report\n```\n\n----------------------------------------\n\nTITLE: Estimating ZeRO-3 Memory Usage with Live Model in Python\nDESCRIPTION: This snippet demonstrates how to estimate memory requirements for a 3B parameter model using ZeRO-3 with a live model. It uses the estimate_zero3_model_states_mem_needs_all_live function from DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'from transformers import AutoModel; \nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \nmodel = AutoModel.from_pretrained(\"t5-3b\"); \nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=8, num_nodes=1)'\n```\n\n----------------------------------------\n\nTITLE: Calculating FLOPs per GPU for DeepSpeed Training\nDESCRIPTION: Formula used to calculate the number of floating point operations per GPU for large language model training, as defined in the Megatron paper. This calculation is used to measure performance in terms of TFLOPs/GPU.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-07-26-deepspeed-azure.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nFLOPs/GPU = 96 * B * s * l * h2 * (1 + s/6h + V/(16*l*h))\n```\n\n----------------------------------------\n\nTITLE: Running CIFAR-10 Training with DeepSpeed\nDESCRIPTION: Command to launch CIFAR-10 model training using DeepSpeed framework with a specified configuration file. Uses all available GPUs by default.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/cifar-10.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed cifar10_deepspeed.py --deepspeed_config ds_config.json\n```\n\n----------------------------------------\n\nTITLE: Initializing AIO Handle with Default Parameters\nDESCRIPTION: Example showing the help output for aio_handle constructor with default parameter values including block_size, queue_depth, and other performance-critical settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/deepnvme.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n>>> from deepspeed.ops.op_builder import AsyncIOBuilder\n>>> help(AsyncIOBuilder().load().aio_handle())\nHelp on aio_handle in module async_io object:\n\nclass aio_handle(pybind11_builtins.pybind11_object)\n |  Method resolution order:\n |      aio_handle\n |      pybind11_builtins.pybind11_object\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __init__(...)\n |      __init__(self: async_io.aio_handle, block_size: int = 1048576, queue_depth: int = 128, single_submit: bool = False, overlap_events: bool = False, intra_op_parallelism: int = 1) -> None\n |\n |      AIO handle constructor\n```\n\n----------------------------------------\n\nTITLE: Resetting Activation Checkpointing in DeepSpeed\nDESCRIPTION: Function to reset the activation checkpointing system in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.reset\n```\n\n----------------------------------------\n\nTITLE: Modifying Partitioned States Example\nDESCRIPTION: Demonstration of modifying parameters and optimizer states in ZeRO training after DeepSpeed initialization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/zero3.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[...]\nfrom deepspeed.runtime.zero.utils import is_zero_param\nfrom deepspeed.utils import safe_set_full_fp32_param, safe_set_full_optimizer_state\nfrom deepspeed.utils import safe_set_local_fp32_param, safe_set_local_optimizer_state\n# Here is an example to zero all the fp32 parameters and optimizer states.\nfor n, lp in model.named_parameters():\n    # 1. For zero stage 1, 2, or 3 set the full fp32 and their full optim states\n    zero_tensor = torch.zeros(lp.ds_shape) if is_zero_param(lp) else torch.zeros(lp.shape)\n\n    safe_set_full_fp32_param(lp, zero_tensor)\n    safe_get_full_optimizer_state(lp, zero_tensor, \"exp_avg\")\n    safe_get_full_optimizer_state(lp, zero_tensor, \"exp_avg_sq\")\n\n    # 2. For zero stage 3, each process sets its local fp32 parameters and their local optimizer states individually\n    zero_tensor_local = torch.zeros(lp.ds_tensor.shape)\n\n    safe_set_local_fp32_param(lp, zero_tensor_local)\n    safe_set_local_optimizer_state(lp, zero_tensor_local, \"exp_avg\")\n    safe_set_local_optimizer_state(lp, zero_tensor_local, \"exp_avg_sq\")\n\n[...]\n```\n\n----------------------------------------\n\nTITLE: Expressing Reduced Memory Footprint with Sparse Attention\nDESCRIPTION: Mathematical notation showing the reduced memory footprint O(wn) of sparse attention, where w is a parameter between 1 and n that depends on the attention structure.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-09-sparse-attention.md#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nO(wn)\n```\n\n----------------------------------------\n\nTITLE: Configuring Activation Checkpointing Profiling\nDESCRIPTION: JSON configuration to enable profiling of forward and backward time for activation checkpoint functions.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"activation_checkpointing\": {\n    \"profile\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running BERT Training Command\nDESCRIPTION: Bash command for running BERT training with specified parameters including batch size, sequence length, and gradient settings.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-pretraining.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py  \\\n    --cf bert_large_adam_seq128.json \\\n    --train_batch_size 64 \\\n    --max_seq_length 128 \\\n    --gradient_accumulation_steps 1  \\\n    --max_grad_norm 1.0 \\\n    --fp16 \\\n    --loss_scale 0 \\\n    --delay_allreduce \\\n    --max_steps 10 \\\n    --output_dir <path-to-model-output>\n```\n\n----------------------------------------\n\nTITLE: Installing Required System Packages for DeepSpeed Documentation\nDESCRIPTION: Command to install the necessary system packages (build-essential, zlib, and Ruby) that are prerequisites for building the DeepSpeed documentation with Jekyll.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install build-essential zlib1g-dev ruby-full\n```\n\n----------------------------------------\n\nTITLE: Running Pre-training Distillation for GPT Models using Megatron-DeepSpeed\nDESCRIPTION: This snippet shows how to run pre-training distillation for GPT models using Megatron-DeepSpeed. It includes steps to obtain the repository, navigate to the correct directory, and execute example scripts for different model sizes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Step 1: Obtain the latest version of Megatron-DeepSpeed\n# Step 2: Enter the compression directory\ncd Megatron-DeepSpeed/examples_deepspeed/compression\n\n# Step 3: Run example scripts\n# For 125M model\n./ds_pretrain_gpt_125M_dense_cl_kd.sh\n# For 350M model\n./ds_pretrain_gpt_350M_dense_kd.sh\n# For 1.3B model\n./ds_pretrain_gpt_1.3B_dense_cl_kd.sh\n\n# Step 4: Quantize the distilled model\n./125M-L10-Int8-test-64gpu-distilled-group48.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling Sparse Attention in DeepSpeed Launch Script\nDESCRIPTION: Command line argument to enable sparse attention in DeepSpeed launcher.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/sparse-attention.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--deepspeed_sparse_attention\n```\n\n----------------------------------------\n\nTITLE: Customizing DeepSpeed CPU Worker Configuration\nDESCRIPTION: Advanced command for launching DeepSpeed on Intel CPUs with custom worker count and core allocation. This allows fine-grained control over resource utilization.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --num_accelerators 4 --bind_cores_to_rank --bind_core_list <0-27,32-59> inference.py\n```\n\n----------------------------------------\n\nTITLE: Converting Accelerator Availability Check\nDESCRIPTION: Example of converting CUDA-specific availability check to hardware-agnostic version.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-abstraction-interface.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif torch.cuda.is_available():\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\nif get_accelerator().is_available():\n    ...\n```\n\n----------------------------------------\n\nTITLE: CUDA RNG States Tracker Class in DeepSpeed\nDESCRIPTION: Class for tracking CUDA random number generator states in DeepSpeed, used for managing random seeds during checkpointing.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.CudaRNGStatesTracker\n```\n\n----------------------------------------\n\nTITLE: Installing CANN Dependencies - Ubuntu\nDESCRIPTION: Command to install required system packages on Ubuntu-based systems for CANN toolkit\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napt-get install -y gcc g++ make cmake zlib1g zlib1g-dev openssl libsqlite3-dev libssl-dev libffi-dev unzip pciutils net-tools libblas-dev gfortran libblas3\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests and Benchmarks for DS4Sci_EvoformerAttention in Shell\nDESCRIPTION: Commands to execute unit tests and benchmarks for DS4Sci_EvoformerAttention using pytest and a Python benchmark script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds4sci_evoformerattention.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npytest -s tests/unit/ops/deepspeed4science/test_DS4Sci_EvoformerAttention.py\npython tests/benchmarks/DS4Sci_EvoformerAttention_bench.py\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Flops Profiler Output Example for BERT-Large\nDESCRIPTION: An example output from the DeepSpeed Flops Profiler showing comprehensive performance metrics for BERT-Large model running on an A100 GPU with batch size 80. The output includes parameter counts, MACs, FLOPS, latency measurements, and detailed breakdown of model components.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n-------------------------- DeepSpeed Flops Profiler --------------------------\nProfile Summary at step 10:\nNotations:\ndata parallel size (dp_size), model parallel size(mp_size),\nnumber of parameters (params), number of multiply-accumulate operations(MACs),\nnumber of floating-point operations (flops), floating-point operations per second (FLOPS),\nfwd latency (forward propagation latency), bwd latency (backward propagation latency),\nstep (weights update latency), iter latency (sum of fwd, bwd and step latency)\n\nworld size:                                                   1\ndata parallel size:                                           1\nmodel parallel size:                                          1\nbatch size per GPU:                                           80\nparams per gpu:                                               336.23 M\nparams of model = params per GPU * mp_size:                   336.23 M\nfwd MACs per GPU:                                             3139.93 G\nfwd flops per GPU:                                            6279.86 G\nfwd flops of model = fwd flops per GPU * mp_size:             6279.86 G\nfwd latency:                                                  76.67 ms\nbwd latency:                                                  108.02 ms\nfwd FLOPS per GPU = fwd flops per GPU / fwd latency:          81.9 TFLOPS\nbwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:      116.27 TFLOPS\nfwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):   102.0 TFLOPS\nstep latency:                                                 34.09 us\niter latency:                                                 184.73 ms\nsamples/second:                                               433.07\n\n----------------------------- Aggregated Profile per GPU -----------------------------\nTop modules in terms of params, MACs or fwd latency at different model depths:\ndepth 0:\n    params      - {'BertForPreTrainingPreLN': '336.23 M'}\n    MACs        - {'BertForPreTrainingPreLN': '3139.93 GMACs'}\n    fwd latency - {'BertForPreTrainingPreLN': '76.39 ms'}\ndepth 1:\n    params      - {'BertModel': '335.15 M', 'BertPreTrainingHeads': '32.34 M'}\n    MACs        - {'BertModel': '3092.96 GMACs', 'BertPreTrainingHeads': '46.97 GMACs'}\n    fwd latency - {'BertModel': '34.29 ms', 'BertPreTrainingHeads': '3.23 ms'}\ndepth 2:\n    params      - {'BertEncoder': '302.31 M', 'BertLMPredictionHead': '32.34 M'}\n    MACs        - {'BertEncoder': '3092.88 GMACs', 'BertLMPredictionHead': '46.97 GMACs'}\n    fwd latency - {'BertEncoder': '33.45 ms', 'BertLMPredictionHead': '2.61 ms'}\ndepth 3:\n    params      - {'ModuleList': '302.31 M', 'Embedding': '31.79 M', 'Linear': '31.26 M'}\n    MACs        - {'ModuleList': '3092.88 GMACs', 'Linear': '36.23 GMACs'}\n    fwd latency - {'ModuleList': '33.11 ms', 'BertPredictionHeadTransform': '1.83 ms''}\ndepth 4:\n    params      - {'BertLayer': '302.31 M', 'LinearActivation': '1.05 M''}\n    MACs        - {'BertLayer': '3092.88 GMACs', 'LinearActivation': '10.74 GMACs'}\n    fwd latency - {'BertLayer': '33.11 ms', 'LinearActivation': '1.43 ms'}\ndepth 5:\n    params      - {'BertAttention': '100.76 M', 'BertIntermediate': '100.76 M'}\n    MACs        - {'BertAttention': '1031.3 GMACs', 'BertIntermediate': '1030.79 GMACs'}\n    fwd latency - {'BertAttention': '19.83 ms', 'BertOutput': '4.38 ms'}\ndepth 6:\n    params      - {'LinearActivation': '100.76 M', 'Linear': '100.69 M'}\n    MACs        - {'LinearActivation': '1030.79 GMACs', 'Linear': '1030.79 GMACs'}\n    fwd latency - {'BertSelfAttention': '16.29 ms', 'LinearActivation': '3.48 ms'}\n\n------------------------------ Detailed Profile per GPU ------------------------------\nEach module profile is listed after its name in the following order:\nparams, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n\nBertForPreTrainingPreLN(\n  336.23 M, 100.00% Params, 3139.93 GMACs, 100.00% MACs, 76.39 ms, 100.00% latency, 82.21 TFLOPS,\n  (bert): BertModel(\n    335.15 M, 99.68% Params, 3092.96 GMACs, 98.50% MACs, 34.29 ms, 44.89% latency, 180.4 TFLOPS,\n    (embeddings): BertEmbeddings(...)\n    (encoder): BertEncoder(\n      302.31 M, 89.91% Params, 3092.88 GMACs, 98.50% MACs, 33.45 ms, 43.79% latency, 184.93 TFLOPS,\n      (FinalLayerNorm): FusedLayerNorm(...)\n      (layer): ModuleList(\n        302.31 M, 89.91% Params, 3092.88 GMACs, 98.50% MACs, 33.11 ms, 43.35% latency, 186.8 TFLOPS,\n        (0): BertLayer(\n          12.6 M, 3.75% Params, 128.87 GMACs, 4.10% MACs, 1.29 ms, 1.69% latency, 199.49 TFLOPS,\n          (attention): BertAttention(\n            4.2 M, 1.25% Params, 42.97 GMACs, 1.37% MACs, 833.75 us, 1.09% latency, 103.08 TFLOPS,\n            (self): BertSelfAttention(\n              3.15 M, 0.94% Params, 32.23 GMACs, 1.03% MACs, 699.04 us, 0.92% latency, 92.22 TFLOPS,\n              (query): Linear(1.05 M, 0.31% Params, 10.74 GMACs, 0.34% MACs, 182.39 us, 0.24% latency, 117.74 TFLOPS,...)\n              (key): Linear(1.05 M, 0.31% Params, 10.74 GMACs, 0.34% MACs, 57.22 us, 0.07% latency, 375.3 TFLOPS,...)\n              (value): Linear(1.05 M, 0.31% Params, 10.74 GMACs, 0.34% MACs, 53.17 us, 0.07% latency, 403.91 TFLOPS,...)\n              (dropout): Dropout(...)\n              (softmax): Softmax(...)\n            )\n            (output): BertSelfOutput(\n              1.05 M, 0.31% Params, 10.74 GMACs, 0.34% MACs, 114.68 us, 0.15% latency, 187.26 TFLOPS,\n              (dense): Linear(1.05 M, 0.31% Params, 10.74 GMACs, 0.34% MACs, 64.13 us, 0.08% latency, 334.84 TFLOPS, ...)\n              (dropout): Dropout(...)\n            )\n          )\n          (PreAttentionLayerNorm): FusedLayerNorm(...)\n          (PostAttentionLayerNorm): FusedLayerNorm(...)\n          (intermediate): BertIntermediate(\n            4.2 M, 1.25% Params, 42.95 GMACs, 1.37% MACs, 186.68 us, 0.24% latency, 460.14 TFLOPS,\n            (dense_act): LinearActivation(4.2 M, 1.25% Params, 42.95 GMACs, 1.37% MACs, 175.0 us, 0.23% latency, 490.86 TFLOPS,...)\n          )\n          (output): BertOutput(\n            4.2 M, 1.25% Params, 42.95 GMACs, 1.37% MACs, 116.83 us, 0.15% latency, 735.28 TFLOPS,\n            (dense): Linear(4.2 M, 1.25% Params, 42.95 GMACs, 1.37% MACs, 65.57 us, 0.09% latency, 1310.14 TFLOPS,...)\n            (dropout): Dropout(...)\n          )\n        )\n        ...\n        (23): BertLayer(...)\n      )\n    )\n    (pooler): BertPooler(...)\n  )\n  (cls): BertPreTrainingHeads(...)\n)\n------------------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Testing NCCL Backend for One-Bit Optimizers in DeepSpeed\nDESCRIPTION: These commands run tests for the accuracy and performance of the NCCL backend used in DeepSpeed's one-bit optimizers. They require a properly configured environment with the NCCL backend of PyTorch distributed installed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/tests/onebit/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython test_nccl_backend.py\npython test_nccl_perf.py\n```\n\n----------------------------------------\n\nTITLE: Merging Curriculum Learning Data Analysis Results in Bash\nDESCRIPTION: Example bash script for merging the results of the curriculum learning data analysis. This script is used in the Reduce stage to combine index files produced by all workers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/data-efficiency.md#2025-04-16_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nds_analyze_*_data_reduce.sh\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed GPU FusedLamb\nDESCRIPTION: Python import statement for DeepSpeed's GPU-optimized FusedLamb optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.ops.lamb.FusedLamb\n```\n\n----------------------------------------\n\nTITLE: Estimating ZeRO-2 Memory Usage without Live Model in Python\nDESCRIPTION: This snippet shows how to estimate memory requirements for a 3B parameter model using ZeRO-2 without loading the actual model. It uses the estimate_zero2_model_states_mem_needs_all_cold function from DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_cold; \nestimate_zero2_model_states_mem_needs_all_cold(total_params=2851e6, num_gpus_per_node=8, num_nodes=1)'\n```\n\n----------------------------------------\n\nTITLE: Launching with MPI for 1-bit LAMB\nDESCRIPTION: Alternative command using mpirun to launch training with the MPI-based implementation of 1-bit LAMB optimizer.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-lamb.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np [num processes] -ppn [num GPUs on each node] -hostfile [hostfile] [MPI flags] python [training_script.py]\n```\n\n----------------------------------------\n\nTITLE: Running GPT2-large Training with DeepSpeed Autotuning in Bash\nDESCRIPTION: This bash script demonstrates how to run GPT2-large training using DeepSpeed with autotuning enabled. It sets various parameters such as model name, batch size, and training configuration, then executes the training script with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=gpt2-large\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nHF_PATH=~/projects # REPLACE WITH YOUR HUGGING FACE PATH\nDS_CONFIG_PATH=ds_config.json # REPLACE WITH YOUR DEEPSPEED CONFIGURATION FILE PATH\n\nNEPOCHS=1\nNGPUS=16\nNNODES=1\nOUTPUT_DIR=./output_b${PER_DEVICE_TRAIN_BATCH_SIZE}_g${NGPUS}\n\ndeepspeed --autotuning run --num_nodes=$NNODES --num_gpus=$NGPUS $HF_PATH/transformers/examples/pytorch/language-modeling/run_clm.py --deepspeed $DS_CONFIG_PATH \\\n--model_name_or_path $MODEL_NAME \\\n--dataset_name wikitext \\\n--dataset_config_name wikitext-2-raw-v1 \\\n--do_train \\\n--do_eval \\\n--fp16 \\\n--per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n--learning_rate 2e-5 \\\n--num_train_epochs $NEPOCHS \\\n--output_dir ${OUTPUT_DIR} \\\n--overwrite_output_dir\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for CANN\nDESCRIPTION: Command to install required Python packages for CANN toolkit using pip\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip3 install attrs numpy decorator sympy cffi pyyaml pathlib2 psutil protobuf scipy requests absl-py wheel typing_extensions\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepSpeed Repository\nDESCRIPTION: Commands to clone the DeepSpeed repository and initialize the DeepSpeedExamples submodule containing the BingBertSquad example.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/deepspeedai/DeepSpeed\ncd DeepSpeed\ngit submodule update --init --recursive\ncd DeepSpeedExamples/training/BingBertSquad\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for TensorFlow Pretrained Model Files\nDESCRIPTION: Displays the file structure for loading a TensorFlow pretrained model, including the config JSON and checkpoint files.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n[test/tensorflow]\nbert_config.json\nbert_model.ckpt.data-00000-of-00001\nbert_model.ckpt.index\nbert_model.ckpt.meta\n```\n\n----------------------------------------\n\nTITLE: Cloning Megatron-LM Repository\nDESCRIPTION: Git command to initialize and update the Megatron-LM submodule in DeepSpeed repository\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/megatron.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed from local wheel\nDESCRIPTION: Installs DeepSpeed from a locally built wheel file. Useful for installing on multiple machines with the same environment.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Installing 1-bit ADAM Dependencies\nDESCRIPTION: Pip command to install the necessary dependencies for using 1-bit ADAM optimizer in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-lamb.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install deepspeed[1bit_adam]\n```\n\n----------------------------------------\n\nTITLE: Detailed Memory Calculation for ZeRO-3 in Python\nDESCRIPTION: This code snippet provides a detailed calculation of memory requirements for different ZeRO-3 configurations, including the largest layer memory and various offloading scenarios.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/memory.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"t5-large\")\n\n# shared params calculated only ones\ntotal_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n\nlargest_layer_params = 0\nfor m in model.modules():\n    # assuming no shared params within a single layer\n    layer_params = sum(p.numel() for p in m.parameters(recurse=False))\n    largest_layer_params = max(largest_layer_params, layer_params)\n\nlargest_layer_memory = (4*largest_layer_params)\n\ntotal_gpus = 4\n\ncase1 = largest_layer_memory + int(18*total_params/total_gpus)\ncase2 = largest_layer_memory\ncase3 = largest_layer_memory + int(2*total_params/total_gpus)\n\nprint(f\"total params:         {total_params/1e6:6.2f}M\")\nprint(f\"largest layer params: {largest_layer_params/1e6:6.2f}M\")\nprint(f\"largest layer memory: {largest_layer_memory>>20:6}MB\")\nprint(f\"case1 gpu memory: {(case1)>>20:6}MB\")\nprint(f\"case2 gpu memory: {(case2)>>20:6}MB\")\nprint(f\"case3 gpu memory: {(case3)>>20:6}MB\")\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepSpeed Repository and Updating Submodules\nDESCRIPTION: Commands to clone the DeepSpeed repository, navigate to the correct directory, and update submodules including DeepSpeedExamples.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/onebit-lamb.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/deepspeedai/DeepSpeed\ncd DeepSpeed\ngit submodule update --init --recursive\ncd DeepSpeedExamples/\n```\n\n----------------------------------------\n\nTITLE: Starting Local Documentation Server\nDESCRIPTION: Command to start a local Jekyll web server for previewing the DeepSpeed documentation website on port 4000.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbundle exec jekyll serve\n```\n\n----------------------------------------\n\nTITLE: Loading TensorFlow Pretrained Model in DeepSpeed\nDESCRIPTION: Shows the command-line arguments required to load a TensorFlow pretrained model for fine-tuning with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n--model_file /test/tensorflow/bert_model.ckpt \\\n--ckpt_type TF \\\n--origin_bert_config_file /test/tensorflow/bert_config.json \\\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepSpeed Repository and Initializing Submodules\nDESCRIPTION: Shell commands to clone the DeepSpeed repository, navigate to the directory, and initialize the DeepSpeedExamples submodule containing the BERT Pre-training example.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/deepspeedai/DeepSpeed\ncd DeepSpeed\ngit submodule update --init --recursive\ncd DeepSpeedExamples/\n```\n\n----------------------------------------\n\nTITLE: Citation for DeepSpeed Ulysses\nDESCRIPTION: BibTeX citation entry for the DeepSpeed Ulysses arxiv paper, including author details and publication information.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{jacobs2023deepspeed,\n      title={DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models},\n      author={Sam Ade Jacobs and Masahiro Tanaka and Chengming Zhang and Minjia Zhang and Shuaiwen Leon Song and Samyam Rajbhandari and Yuxiong He},\n      journal={arXiv preprint arXiv:2309.14509},\n      year={2023},\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed on Windows Using pip\nDESCRIPTION: Simple command to install DeepSpeed using pip package manager. This installs the latest version with prebuilt operators, requiring no CUDA SDK or C++ compiler.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Displaying BERT-large Training Times Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table comparing BERT-large training times using DeepSpeed on different numbers of V100 GPUs. It demonstrates DeepSpeed's ability to train BERT-large efficiently at various scales.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/training.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Devices        | Source    |        Training Time  |\n| -------------- | --------- | ---------------------:|\n| 1024 V100 GPUs | DeepSpeed |             **44** min|\n| 256 V100 GPUs  | DeepSpeed |             **2.4** hr|\n| 64 V100 GPUs   | DeepSpeed |            **8.68** hr|\n| 16 V100 GPUs   | DeepSpeed |           **33.22** hr|\n```\n\n----------------------------------------\n\nTITLE: Customizing ZeRO Stage 1 Configuration for DeepSpeed Autotuning\nDESCRIPTION: This JSON configuration customizes the ZeRO stage 1 parameters for autotuning. It specifies two values for reduce_bucket_size and a fixed value for allgather_bucket_size, reducing the number of combinations explored during tuning.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"reduce_bucket_size\": [5e7, 5e8],\n    \"allgather_bucket_size\": 5e8,\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks for DeepSpeed Development\nDESCRIPTION: Sets up pre-commit hooks to ensure consistent code formatting before commits. This needs to be run once before making any commits to the repository.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/contributing.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Installing specific DeepSpeed op (FusedLamb)\nDESCRIPTION: Installs DeepSpeed with only the FusedLamb op by using the DS_BUILD_FUSED_LAMB environment variable.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nDS_BUILD_FUSED_LAMB=1 pip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Defining Range Parameter for Sparse Attention\nDESCRIPTION: Definition of the parameter w that determines the memory footprint reduction in sparse attention, where w is between 1 and n.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-09-sparse-attention.md#2025-04-16_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n1 < w < n\n```\n\n----------------------------------------\n\nTITLE: Configuring Comet Logging in DeepSpeed\nDESCRIPTION: JSON configuration for enabling Comet logging in DeepSpeed. It includes various settings such as workspace, project, experiment name, and logging intervals.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/config-json.md#2025-04-16_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"comet\": {\n        \"enabled\": true,\n        \"workspace\": \"my_workspace\",\n        \"project\": \"my_project\",\n        \"samples_log_interval\": 50,\n        \"experiment_name\": \"llama-fine-tuning\",\n        \"experiment_key\": \"0c4a1c4a90664f2a8084e600b19a9d7\",\n        \"online\": false,\n        \"mode\": \"get\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed Unit Tests\nDESCRIPTION: Executes the unit tests using pytest with the forked flag, which is required for testing CUDA functionality in distributed tests.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/contributing.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest --forked tests/unit/\n```\n\n----------------------------------------\n\nTITLE: Citing DeepSpeed4Science in BibTeX Format\nDESCRIPTION: BibTeX citation entry for the DeepSpeed4Science white paper published on arXiv. Includes author information, title, journal, and publication year.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed4science/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{song2023deepspeed4science,\n  title={DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies},\n  author={Song, Shuaiwen Leon and Kruft, Bonnie and Zhang, Minjia and Li, Conglong and Chen, Shiyang and Zhang, Chengming and Tanaka, Masahiro and Wu, Xiaoxia and Rasley, Jeff and Awan, Ammar Ahmad and others},\n  journal={arXiv preprint arXiv:2310.04610},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Building DeepSpeed from Source on Windows\nDESCRIPTION: Instructions for installing DeepSpeed by building from source, which requires cloning the repository and running the Windows build script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbuild_win.bat\n```\n\n----------------------------------------\n\nTITLE: Markdown Model Mapping Table\nDESCRIPTION: A markdown table containing mappings between model aliases and their corresponding full Hugging Face model repository URLs. The table covers models for text generation, BERT-based tasks, and RoBERTa-based tasks.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-10-11-mii.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Alias | Model Name |\n| --- | --- |\n| text-gen-m1 | [sberbank-ai/rugpt3large_based_on_gpt2](https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2) |\n| text-gen-m2 | [skt/kogpt2-base-v2](https://huggingface.co/skt/kogpt2-base-v2) |\n[...additional rows omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tuner by Extending BaseTuner in Python\nDESCRIPTION: Example of how to implement a custom tuner class by inheriting from BaseTuner. The tuner works with a list of experiment descriptions and requires implementation of at least the next_batch method, with an optional override of the tune method.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/tuner/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass NewTuner(BaseTuner):\n    def __init__(self, exps: list, resource_manager):\n        super(NewTuner, self).__init__(exps, resource_manager)\n\n    def next_batch(self, sample_size=1):\n        pass\n\n    def tune(self): # if it differs from BaseTuner\n        pass\n```\n\n----------------------------------------\n\nTITLE: Deploying Stable Diffusion Model with MII-Azure\nDESCRIPTION: This code snippet shows how to deploy a Stable Diffusion model using MII-Azure deployment. It generates AML deployment assets for the model that can be deployed using Azure-CLI, leveraging DeepSpeed-Azure as its optimization backend.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2022-10-11-mii.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mii\nmii.deploy(task=\"text-to-image\",\n           model=\"CompVis/stable-diffusion-v1-4\",\n           deployment_name=\"sd-deployment\",\n           deployment_type=DeploymentType.AML)\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed-FP6 Benchmarks\nDESCRIPTION: This snippet provides a link to a bash script for running benchmarks using DeepSpeed-FP6.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhttps://github.com/deepspeedai/DeepSpeedExamples/blob/master/benchmarks/inference/mii/run_fp6.sh\n```\n\n----------------------------------------\n\nTITLE: Running Performance Benchmark for BERT with Triton in DeepSpeed\nDESCRIPTION: This code snippet demonstrates how to run a performance benchmark for BERT models with Triton optimization in DeepSpeed. It includes options for kernel injection, CUDA graphs, and Triton optimization with a specific model and precision.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-triton/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install deepspeed[triton]\n\ngit clone https://github.com/deepspeedai/DeepSpeedExamples.git\ncd DeepSpeedExamples/benchmarks/inference\n\ndeepspeed --num_gpus 1 triton-bert-benchmark.py --model bert-base-cased --dtype fp16 --kernel-inject --deepspeed --graphs --triton\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed GPU OnebitLamb\nDESCRIPTION: Python import statement for DeepSpeed's GPU-optimized OnebitLamb optimizer implementation\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/optimizers.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndeepspeed.runtime.fp16.onebit.lamb.OnebitLamb\n```\n\n----------------------------------------\n\nTITLE: Importing DeepSpeed Accelerator Interface\nDESCRIPTION: Basic import statement to access the DeepSpeed accelerator abstraction functionality.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-abstraction-interface.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.accelerator import get_accelerator\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for DeepSpeed Domino\nDESCRIPTION: BibTeX citation for the DeepSpeed Domino research paper that describes eliminating communication in LLM training via generic tensor slicing and overlapping.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{wang2024-deepspeed-domino,\n  title={{Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping}},\n  author={Guanhua Wang and Chengming Zhang and Zheyu Shen and Ang Li and Olatunji Ruwase},\n  journal={arXiv preprint arXiv:2409.15241},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Triton for FlashAttention in Bash\nDESCRIPTION: This bash script shows the installation steps for Triton, which is required for using FlashAttention with DeepSpeed-Ulysses. It clones the Triton repository, installs dependencies, and sets up the package.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ds-sequence.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# install triton\ngit clone -b legacy-backend https://github.com/openai/triton\ncd triton/python/\npip install cmake\npip install .\n```\n\n----------------------------------------\n\nTITLE: Jekyll Template for Rendering Blog Post List\nDESCRIPTION: This Liquid template code generates the HTML structure for displaying blog posts. Each post is wrapped in a div with a unique ID based on the post title, which enables filtering functionality.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts-landing.md#2025-04-16_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"posts-wrapper\">\n  {% for post in posts %}\n    <div class=\"post\" id=\"post-{{post.title | slugify}}\">\n      <p class=\"itemInteriorSection\">\n        {%- unless post.hidden -%}\n          {% include archive-single.html %}\n          {% if post.image %}\n            <a href=\"{{ post.link }}\"><img src=\"{{ post.image }}\"></a>\n          {% endif %}\n        {%- endunless -%}\n      </p>\n    </div>\n  {% endfor %}\n</div>\n```\n\n----------------------------------------\n\nTITLE: Testing MPI Backend for One-Bit Optimizers in DeepSpeed\nDESCRIPTION: These commands execute tests for the accuracy and performance of the MPI backend used in DeepSpeed's one-bit optimizers. They require a properly configured environment with an MPI implementation like MVAPICH2-GDR or OpenMPI installed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/tests/onebit/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython test_mpi_backend.py\npython test_mpi_perf.py\n```\n\n----------------------------------------\n\nTITLE: Installing Jekyll and Bundler\nDESCRIPTION: Command for installing Jekyll (the static site generator) and Bundler (dependency manager) via Ruby's gem command, which are required to build the documentation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngem install jekyll bundler\n```\n\n----------------------------------------\n\nTITLE: Frontmatter Configuration for Turing-NLG Blog Post\nDESCRIPTION: YAML frontmatter configuration for a blog post announcing the use of DeepSpeed to train the Turing-NLG language model. Includes metadata like title, date, external link, excerpt and tags.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-02-13-turing-nlg.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Turing-NLG: A 17-billion-parameter language model by Microsoft\"\ndate:   2020-02-13\nlink: https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\nexcerpt: \"DeepSpeed was used to train the world's largest language model.\"\ntags: training English\n---\n```\n\n----------------------------------------\n\nTITLE: Running BERT Model Pre-training with DeepSpeed\nDESCRIPTION: Command to launch BERT language model pre-training using DeepSpeed on Windows with a specified checkpoint directory.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed train_bert_ds.py --checkpoint_dir experiment_deepspeed\n```\n\n----------------------------------------\n\nTITLE: Abbreviated AlexNet Implementation (Python)\nDESCRIPTION: Shows a simplified version of the AlexNet architecture, which is used as an example for pipeline parallelism conversion.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/pipeline.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            ...\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            ...\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Configuring Ruby Gems Environment Variables\nDESCRIPTION: Bash commands to configure environment variables for Ruby Gems installation without sudo. These lines should be added to .bashrc to ensure proper permissions for gem installations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GEM_HOME=\"$HOME/gems\"\nexport PATH=\"$HOME/gems/bin:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed-FastGen Directory Structure\nDESCRIPTION: Project directory location reference showing the main repository path\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/2024-01-19/README.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nProject: /deepspeedai/DeepSpeed\n```\n\n----------------------------------------\n\nTITLE: Running 1-bit Quantization for BERT-base\nDESCRIPTION: This command executes the script for performing 1-bit quantization on a BERT-base model using DeepSpeed Compression.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/model-compression.md#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nDeepSpeedExamples/compression/bert$ bash bash_script/XTC/quant_1bit.sh\n```\n\n----------------------------------------\n\nTITLE: Jekyll Blog Post Collection Assignment\nDESCRIPTION: This Liquid template determines which posts to display based on pagination settings. If pagination is enabled, it uses paginator.posts; otherwise, it falls back to displaying all site posts.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts-landing.md#2025-04-16_snippet_3\n\nLANGUAGE: liquid\nCODE:\n```\n{% if paginator %}\n  {% assign posts = paginator.posts %}\n{% else %}\n  {% assign posts = site.posts %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for DeepSpeed-FP6\nDESCRIPTION: This bash snippet shows the commands to install the necessary packages for using DeepSpeed-FP6, including deepspeed-mii and qtorch.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed-mii\npip install qtorch\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for ZeRO-2 and DeepSpeed Blog Post in Markdown\nDESCRIPTION: This snippet defines the metadata for a blog post about ZeRO-2 and DeepSpeed, including the title, link, tags, and publication date. It uses YAML-like syntax within Markdown frontmatter.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-05-19-press-release.md#2025-04-16_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n---\ntitle: \"ZeRO-2 & DeepSpeed: Shattering Barriers of Deep Learning Speed & Scale\"\nexcerpt: \"\"\nlink: https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/\ntags: training ZeRO English\ndate: 2020-05-19 02:00:00\n---\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed with Autotuning Command Line\nDESCRIPTION: Command line syntax for launching DeepSpeed with autotuning enabled. The autotuning flag accepts either 'run' to find optimal configuration and train, or 'tune' to only find the configuration without training.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --autotuning=[run|tune] <user script> --deepspeed ds_config.json <other user args>\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for DeepSpeed\nDESCRIPTION: This snippet enumerates the Python package dependencies required for the DeepSpeed project. It includes libraries for language model evaluation, protocol buffers, tensor operations, model serialization, tokenization, and transformer models.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-inf.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle\nlm-eval==0.3.0\nprotobuf\nqtorch\nsafetensors\nsentencepiece\ntransformers>=4.32.1\n```\n\n----------------------------------------\n\nTITLE: Installing Required Ruby Packages\nDESCRIPTION: Command to install all the required Ruby gem dependencies for the DeepSpeed website. Must be run from the docs directory to locate the Gemfile.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to DeepSpeedExamples Repository\nDESCRIPTION: A markdown link to the DeepSpeedExamples GitHub repository containing BERT pre-training code using sparse attention.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-08-sparse-attention-news.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n[DeepSpeedExamples repo](https://github.com/deepspeedai/deepspeedexamples)\n```\n\n----------------------------------------\n\nTITLE: Running GLUE Fine-tuning with DeepSpeed\nDESCRIPTION: Shell command to launch GLUE fine-tuning using a pre-trained BERT model with DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/progressive_layer_dropping.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbash run_glue_bert_base_finetune.sh [task] [batch size] [learning rate] [number of epochs] [job name] [checkpoint path]\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed via pip on Windows\nDESCRIPTION: Simple pip command to install the latest version of DeepSpeed on Windows. This pre-compiled version doesn't require CUDA or C++ compiler installation.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Building ContainerMap in Policy Implementation\nDESCRIPTION: This code demonstrates how to implement the build_container_map method in a policy class. It creates transformer containers for each layer and maps them to the appropriate parameter prefixes using the ContainerMap class.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/inference/v2/model_implementations/AddingAModel.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.inference.module_implementations.inference_policy_base import ContainerMap\n\ndef build_container_map(self) -> ContainerMap:\n    map = ContainerMap()\n\n    transformer_containers = [MyTransformerContainer(self.model) for _ in range(self.model.num_layers)]\n    map.set_transformer_params(\"model.layers\", transformer_containers)\n\n    non_transformer_container = MyNonTransformerContainer(self.model)\n```\n\n----------------------------------------\n\nTITLE: Declaring MPI4Py Dependency\nDESCRIPTION: This snippet declares a dependency on the MPI4Py library. MPI4Py is a Python binding for the Message Passing Interface (MPI) standard, commonly used in parallel computing and distributed systems.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-1bit-mpi.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmpi4py\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration with Multiple Argument Mappings\nDESCRIPTION: Example DeepSpeed configuration JSON that maps both micro-batch size and gradient accumulation steps to their corresponding training script arguments. This is needed when the training script uses different parameter names than DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"autotuning\": {\n        \"enabled\": true,\n        \"arg_mappings\": {\n            \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n            \"gradient_accumulation_steps \": \"--gradient_accumulation_steps\"\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Train Batch Size for DeepSpeed Autotuning\nDESCRIPTION: This JSON configuration enables DeepSpeed Autotuning and sets a maximum train batch size of 1024. It also maps the train_micro_batch_size_per_gpu to the per_device_train_batch_size argument in the training script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"autotuning\": {\n        \"enabled\": true,\n        \"max_train_batch_size\": 1024,\n        \"arg_mappings\": {\n          \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Triton Dependency Version for DeepSpeed\nDESCRIPTION: This snippet defines the required version of Triton for the DeepSpeed project. It uses the '==' operator to pin the version to exactly 2.1.0, ensuring compatibility and consistency across installations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-triton.txt#2025-04-16_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntriton==2.1.0\n```\n\n----------------------------------------\n\nTITLE: Training Small-Scale OPT Model with DeepSpeed-Chat\nDESCRIPTION: Command to train a smaller 1.3B parameter OPT model on a single GPU. Suitable for testing and development on consumer-grade hardware with limited resources.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython train.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to DeepSpeed Repository\nDESCRIPTION: A markdown link to the DeepSpeed GitHub repository containing the source code for sparse attention kernels.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-08-sparse-attention-news.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n[DeepSpeed repo](https://github.com/deepspeedai/deepspeed)\n```\n\n----------------------------------------\n\nTITLE: Generating Post List by Tag in Jekyll\nDESCRIPTION: This snippet creates a section for each tag, listing all posts associated with that tag. It handles both internal and external post links, and formats the post date.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts_list_landing.md#2025-04-16_snippet_1\n\nLANGUAGE: liquid\nCODE:\n```\n{% for tag in sorted_tags %}\n  {% assign t = tag | first %}\n  {% assign posts = tag | last %}\n  <div style=\"text-transform:capitalize;\">\n    <h4 id=\"{{ t | downcase }}\">{{ t }}</h4>\n  </div>\n  <ul>\n  {% for post in posts %}\n    {% if post.tags contains t %}\n      {% if post.link %}\n        <li>\n          <span class=\"date\">{{ post.date | date: '%d %b %y' }}</span>:  <a href=\"{{ post.link }}\">{{ post.title }}</a>\n        </li>\n      {% else %}\n        <li>\n          <span class=\"date\">{{ post.date | date: '%d %b %y' }}</span>:  <a href=\"{{ post.url }}\">{{ post.title }}</a>\n        </li>\n      {% endif %}\n    {% endif %}\n  {% endfor %}\n  </ul>\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Requirements List\nDESCRIPTION: Complete list of Python package dependencies with their version constraints. Includes core ML packages, testing frameworks, documentation generators, and monitoring tools. Some dependencies like deepspeed-kernels are platform-specific.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-dev.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate\nclang-format==18.1.3\ncomet_ml>=3.41.0\ndeepspeed-kernels ; sys_platform == 'linux'\ndocutils<0.18\nfuture\nimportlib-metadata>=4\nmup\npre-commit>=3.2.0\npytest>=7.2.0\npytest-forked\npytest-randomly\npytest-xdist\nqtorch==0.3.0\nrecommonmark\nsphinx\nsphinx-rtd-theme\ntensorboard\ntorchvision\ntransformers>=4.39.0\nwandb\n```\n\n----------------------------------------\n\nTITLE: Running Model Tests for DeepSpeed\nDESCRIPTION: These commands navigate to the model test directory and execute the model test driver for DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd tests/model/\npytest run_sanity_check.py\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for a blog post about DeepSpeed-FastGen, including title, link, date, and tags.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2023-11-06-deepspeed-fastgen.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\"\nexcerpt: \"\"\nlink: https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-fastgen\ndate: 2023-11-06 00:00:00\ntags: inference English\n---\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for DeepSpeed\nDESCRIPTION: This snippet defines the minimum required versions for two Python packages: diffusers and triton. These dependencies are crucial for the DeepSpeed project to function correctly.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-sd.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndiffusers>=0.25.0\ntriton>=2.1.0\n```\n\n----------------------------------------\n\nTITLE: Testing Compressed Backend for One-Bit Optimizers in DeepSpeed\nDESCRIPTION: These commands run tests for the CompressedBackend, which abstracts the generic part of one-bit optimizers and implements accelerator-dependent parts using DeepSpeed's custom op builder. It requires support for PackbitsBuilder on the current accelerator.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/tests/onebit/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython test_compressed_backend.py\npython test_compressed_perf.py\n```\n\n----------------------------------------\n\nTITLE: Filtering Tutorials by Category in JavaScript\nDESCRIPTION: JavaScript function that filters tutorial elements based on selected category tags. It iterates through tutorials and toggles their visibility based on matching category selection.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/tutorials-landing.md#2025-04-16_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nfunction filterTutorialsUsingCategory(selectedCategory) {\n      {% for tutorial in tutorials %}\n        var cats = {{ tutorial.tags | jsonify }}\n        var tutorialDiv = document.getElementById(\"tutorial-{{tutorial.title | slugify}}\");\n        tutorialDiv.style.display = (selectedCategory == 'All' || cats.includes(selectedCategory))\n          ? 'unset'\n          : 'none';\n      {% endfor %}\n    }\n```\n\n----------------------------------------\n\nTITLE: Citing Ulysses-Offload in BibTeX Format\nDESCRIPTION: BibTeX citation for the Ulysses-Offload arxiv report. This snippet provides the necessary information for citing the technical details of the Ulysses-Offload system in academic papers.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{yao2024ulysses,\n\ntitle={ Training Ultra Long Context Language Model with Fully Pipelined\nDistributed Transformer},\n\nauthor={Jinghan Yao and Sam Ade Jacobs and Masahiro Tanaka and Olatunji\nRuwase and Aamir Shafi and Hari Subramoni and Dhabaleswar K. (DK) Panda\n},\n\njournal={https://arxiv.org/abs/2408.16978},\n\nyear={2024}\n\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed-FP6 Dependencies via pip\nDESCRIPTION: This snippet shows the pip commands required to install the necessary dependencies for using DeepSpeed-FP6, including deepspeed-mii and qtorch.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed-mii\npip install qtorch\n```\n\n----------------------------------------\n\nTITLE: Citation Format for DeepSpeed Universal Checkpoint in BibTeX\nDESCRIPTION: BibTeX citation format for the DeepSpeed Universal Checkpoint arXiv paper. This citation references the academic publication that describes the Universal Checkpointing technology in detail.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ucp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{lian2024-ucp,\ntitle={Universal Checkpointing: Efficient and Flexible Checkpointing for\nLarge Scale Distributed Training},\nauthor={Xinyu Lian and Sam Ade Jacobs and Lev Kurilenko and Masahiro Tanaka\nand Stas Bekman and Olatunji Ruwase and Minjia Zhang},\njournal={arxiv preprint arxiv:406.18820},\nyear={2024},\n\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Sorted Tag List in Jekyll\nDESCRIPTION: This snippet creates a sorted list of tags used in the blog posts. It displays each tag with the number of associated posts in parentheses.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts_list_landing.md#2025-04-16_snippet_0\n\nLANGUAGE: liquid\nCODE:\n```\n{% assign sorted_tags = (site.tags | sort:0) %}\n<ul class=\"tag-box\">\n\t{% for tag in sorted_tags %}\n\t\t{% assign t = tag | first %}\n\t\t{% assign ps = tag | last %}\n\t\t<li><a href=\"#{{ t | downcase }}\">{{ t }} <span class=\"size\">({{ ps.size }})</span></a></li>\n\t{% endfor %}\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Citation Format for DeepSpeed Chat\nDESCRIPTION: BibTeX citation format for referencing the DeepSpeed Chat framework in academic work.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{yao2023dschat,\n  title={{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}},\n  author={Zhewei Yao and Reza Yazdani Aminabadi and Olatunji Ruwase and Samyam Rajbhandari and Xiaoxia Wu and Ammar Ahmad Awan and Jeff Rasley and Minjia Zhang and Conglong Li and Connor Holmes and Zhongzhu Zhou and Michael Wyatt and Molly Smith and Lev Kurilenko and Heyang Qin and Masahiro Tanaka and Shuai Che and Shuaiwen Leon Song and Yuxiong He},\n  journal={arXiv preprint arXiv:2308.01320},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Running CIFAR10 Model Pre-training with DeepSpeed\nDESCRIPTION: Command to start CIFAR10 image classification model pre-training using DeepSpeed on Windows.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/windows/08-2024/chinese/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed cifar10_deepspeed.py --deepspeed\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to Blog Post\nDESCRIPTION: A markdown link to a detailed blog post about DeepSpeed sparse attention technology.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-08-sparse-attention-news.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[blog post](https://www.deepspeed.ai/2020/09/08/sparse-attention.html)\n```\n\n----------------------------------------\n\nTITLE: Citing DeepSpeed-FP6 with BibTeX entries\nDESCRIPTION: BibTeX entries for citing the ZeroQuant(4+2) and FP6-LLM arxiv reports related to DeepSpeed-FP6. These citations provide references to the key research papers underlying the FP6-centric strategy for LLM quantization and serving.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fp6/03-05-2024/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{wu2023zeroquant,\n  title={Zeroquant(4+2): Redefining llms quantization with a new fp6-centric strategy for diverse generative tasks},\n  author={Wu, Xiaoxia and Xia, Haojun and Youn, Stephen and Zheng, Zhen and Chen, Shiyang and Bakhtiari, Arash and Wyatt, Michael and Aminabadi, Reza Yazdani and He, Yuxiong and Ruwase, Olatunji and Song, Leon and others},\n  journal={arXiv preprint arXiv:2312.08583},\n  year={2023}\n}\n\n@article{xia2024fp6,\n  title={FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design},\n  author={Xia, Haojun and Zheng, Zhen and Wu, Xiaoxia and Chen, Shiyang and Yao, Zhewei and Youn, Stephen and Bakhtiari, Arash and Wyatt, Michael and Zhuang, Donglin and Zhou, Zhongzhu and others},\n  journal={arXiv preprint arXiv:2401.14112},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for DeepSpeed Azure Setup\nDESCRIPTION: Contains markdown documentation outlining the setup process for DeepSpeed on Azure, including AzureML integration and VM configuration options. Includes links to example implementations and training recipes.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/azure.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Getting Started with DeepSpeed on Azure\"\ntags: getting-started\n---\n\nThis tutorial will help you get started with DeepSpeed on Azure.\n\nIf you don't already have an Azure account please see more details here: [https://azure.microsoft.com/](https://azure.microsoft.com/).\n\n# DeepSpeed on Azure via AzureML\n\nThe recommended and simplest method to try DeepSpeed on Azure is through [AzureML](https://azure.microsoft.com/en-us/services/machine-learning/). A training example and a DeepSpeed autotuning example using AzureML v2 can be found [here](https://github.com/Azure/azureml-examples/tree/main/cli/jobs/deepspeed).\n\nFor AzureML v1 examples, please take a look at easy-to-use examples for Megatron-DeepSpeed, Transformers and CIFAR training [here](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed).\n\n> Our [Megatron-DeepSpeed](https://github.com/deepspeedai/megatron-deepspeed) contains the most up to date [recipe](https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml) for end-to-end training on AzureML.\n\n# DeepSpeed on Azure VMs\n\nIf you don't have access to AzureML or if want to build a custom environments using [Azure virtual machines](https://azure.microsoft.com/en-us/services/virtual-machines/) or Azure VM Scale-Sets ([VMSS](https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview)), we are working on easy-to-use cluster setup scripts that will be published in the next few weeks.\n\nIf you already have a cluster setup, you can use the [azure recipes](https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azure) that can easily be modified to train various model configurations.\n```\n\n----------------------------------------\n\nTITLE: Running Step 3 Characterization Sweep\nDESCRIPTION: Bash command to execute the Step 3 characterization script that sweeps across different training features like ZeRO stages, Hybrid Engine, ZeRO-Offload, and LoRA configurations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning$ bash training_scripts/opt/single_node/sweep/run_step3_sweep.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Minimal Micro-Batch Size for DeepSpeed Autotuning\nDESCRIPTION: This JSON configuration sets up the DeepSpeed Autotuner with a minimal micro-batch size per GPU of 4 and enables autotuning. It also maps the train_micro_batch_size_per_gpu to the per_device_train_batch_size argument in the training script.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"train_micro_batch_size_per_gpu\": 4,\n    \"autotuning\": {\n        \"enabled\": true,\n    },\n    \"arg_mappings\": {\n        \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing CheckpointEngine Class in Python\nDESCRIPTION: Abstract class definition for checkpoint management in DeepSpeed. The class provides methods for creating, saving, loading, and committing checkpoints with support for different storage backends. The implementation includes placeholder methods that should be overridden by concrete implementations.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/runtime/checkpoint_engine/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CheckpointEngine(object):\n    # init checkpoint engine for save/load\n    def __init__(self, config_params=None):\n        pass\n\n    def create(self, tag):\n        # create checkpoint on give tag for save/load.\n        pass\n\n    def save(self, state_dict, path: str):\n        pass\n\n    def load(self, path: str, map_location=None):\n        pass\n\n    def commit(self, tag):\n        # to tell checkpoint services if all files are ready.\n        pass\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks for DeepSpeed\nDESCRIPTION: This snippet shows how to install pre-commit hooks for DeepSpeed to ensure consistent formatting before commits.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed with 1-bit Adam Support\nDESCRIPTION: Shell command to install DeepSpeed with 1-bit Adam support using pip.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/zero-one-adam.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install deepspeed[1bit_adam]\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to Press Release\nDESCRIPTION: A markdown link to the press release for DeepSpeed sparse attention. The link uses a Liquid template variable for the URL.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-08-sparse-attention-news.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[press release]({{ site.press_release_v3 }})\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks Manually in DeepSpeed\nDESCRIPTION: This command allows developers to manually run pre-commit checks on modified files in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --files  $(git diff --name-only master)\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to Sparse Attention Tutorial\nDESCRIPTION: A markdown link to a tutorial on how to use DeepSpeed sparse attention.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-08-sparse-attention-news.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[Sparse attention tutorial](https://www.deepspeed.ai/tutorials/sparse-attention/)\n```\n\n----------------------------------------\n\nTITLE: Specifying Triton Dependency Version for DeepSpeed\nDESCRIPTION: This line defines the specific version of the Triton library required for the DeepSpeed project. It sets the Triton version to 1.0.0, which is likely crucial for compatibility with the project's codebase.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-sparse_attn.txt#2025-04-16_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntriton==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Expressing Quadratic Complexity of Attention Computation\nDESCRIPTION: Mathematical notation showing the quadratic complexity O(n²) of standard attention mechanisms, where n represents the sequence length.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2020-09-09-sparse-attention.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nO(n^2)\n```\n\n----------------------------------------\n\nTITLE: Profiling AlexNet using DeepSpeed Flops Profiler\nDESCRIPTION: This Python code demonstrates how to use the DeepSpeed Flops Profiler as a standalone package to profile the AlexNet model during inference. It shows the setup and usage of the get_model_profile function with various parameters.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/profiling/flops_profiler/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision.models as models\nimport torch\nfrom deepspeed.profiling.flops_profiler import get_model_profile\nfrom deepspeed.accelerator import get_accelerator\n\nwith get_accelerator().device(0):\n    model = models.alexnet()\n    batch_size = 256\n    flops, macs, params = get_model_profile(model=model, # model\n                                    input_shape=(batch_size, 3, 224, 224), # input shape to the model. If specified, the model takes a tensor with this shape as the only positional argument.\n                                    args=None, # list of positional arguments to the model.\n                                    kwargs=None, # dictionary of keyword arguments to the model.\n                                    print_profile=True, # prints the model graph with the measured profile attached to each module\n                                    detailed=True, # print the detailed profile\n                                    module_depth=-1, # depth into the nested modules, with -1 being the inner most modules\n                                    top_modules=1, # the number of top modules to print aggregated profile\n                                    warm_up=10, # the number of warm-ups before measuring the time of each module\n                                    as_string=True, # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)\n                                    output_file=None, # path to the output file. If None, the profiler prints to stdout.\n                                    ignore_modules=None) # the list of modules to ignore in the profiling\n```\n\n----------------------------------------\n\nTITLE: HTML Details Element for News Section\nDESCRIPTION: Collapsible HTML details element used to show additional news items with multilingual links\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/index.md#2025-04-16_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<details>\n<!-- NOTE: Maintain only three items in 'more news' section -->\n <summary>More news</summary>\n\n <ul>\n\n   <li> [2024/08] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md\"> DeepNVMe: Improving DL Applications through I/O Optimizations</a> [<a href=\"ttps://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> 日本語 </a>] [<a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> 中文 </a>]</li>\n\n   <li> [2024/07] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md\"> DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md\"> 日本語 </a>] </li>\n\n\n   <li> [2024/03] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md\"> DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md\"> 中文 </a>] </li>\n\n\n\n </ul>\n</details>\n```\n\n----------------------------------------\n\nTITLE: Jekyll Liquid Template for Category Filter Buttons\nDESCRIPTION: This template code generates filter buttons for each category/tag used in the blog. It includes an 'All' button and individual buttons for each category with post counts.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts-landing.md#2025-04-16_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"btn-group\">\n  <button id=\"All\" class=\"button-71\" role=\"button\" onclick=\"filterUsingCategory('All')\">All ({{ posts.size }})</button>\n  {% assign tags = site.tags | sort %}\n  {% for category in tags %}\n    {% assign cat = category | first %}\n    <button id=\"{{ cat }}\" class=\"button-71\" role=\"button\" onclick=\"filterUsingCategory(this.id)\">{{ cat }} ({{ site.tags[cat].size }})</button>\n  {% endfor %}\n  <hr />\n</div>\n```\n\n----------------------------------------\n\nTITLE: Adding Webrick Dependency\nDESCRIPTION: Command to add the webrick gem which may be required in newer Ruby versions as it's no longer included by default in the standard library.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbundle add webrick\n```\n\n----------------------------------------\n\nTITLE: Dynamic Post Category Filtering in Jekyll with JavaScript\nDESCRIPTION: This JavaScript function filters blog posts by selected category. It iterates through all posts, checking if the selected category matches any of the post's tags, and shows or hides posts accordingly.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/posts-landing.md#2025-04-16_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nfunction filterUsingCategory(selectedCategory) {\n  {% for post in posts %}\n    var cats = {{ post.tags | jsonify }}\n\n    var postDiv = document.getElementById(\"post-{{post.title | slugify}}\");\n    postDiv.style.display = (selectedCategory == 'All' || cats.includes(selectedCategory))\n      ? 'unset'\n      : 'none';\n  {% endfor %}\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Quantization Accuracy Comparison Table in Markdown\nDESCRIPTION: This markdown table compares the accuracy of different quantization approaches across various NLP tasks. It includes results for baseline (w/o QAT), basic quantization (Basic QAT), and MoQ across GLUE tasks and SQuAD, showcasing the effectiveness of the MoQ scheme in preserving or even improving accuracy.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_posts/2021-05-05-MoQ.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Task         |STSB\t|MRPC |COLA |WNLI |SST2 |RTE  |QNLI |QQP  |MNLI |SQuAD|ACC+ |\n|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n|w/o QAT(FP16)|88.71|88.12|56.78|56.34|91.74|65.3 |90.96|90.67|84.04|90.56|0    |\n|Basic QAT    |88.9 |88.35|52.78|55.3 |91.5 |64.2 |90.92|90.59|84.01|90.39|-0.87|\n|MoQ          |88.93|89|59.33|56.34|92.09 |67.15 |90.63|90.94|84.55|90.71|0.75 |\n```\n\n----------------------------------------\n\nTITLE: Training Independent Multiple Models with DeepSpeed in Python\nDESCRIPTION: This code snippet illustrates how to independently train multiple models on the same dataset using DeepSpeed. It creates individual DeepSpeedEngines for each model and processes them in a loop.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/training.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_engines = [engine for engine, _, _, _ in [deepspeed.initialize(m, ...,) for m in models]]\nfor batch in data_loader:\n   losses = [engine(batch) for engine in model_engines]\n   for engine, loss in zip(model_engines, losses):\n      engine.backward(loss)\n```\n\n----------------------------------------\n\nTITLE: Basic Text Generation Pipeline Using DeepSpeed-FastGen\nDESCRIPTION: Example of initializing and using a non-persistent pipeline for text generation with the Mistral-7B model. Shows how to create a pipeline and generate text from multiple prompts with specified token limits.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mii import pipeline\npipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\noutput = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Citing DeepSpeed4Science with BibTeX Reference\nDESCRIPTION: BibTeX citation format for referencing the DeepSpeed4Science white paper in academic publications. This citation includes author details, publication information, and other bibliographic data for the arXiv preprint.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_pages/deepspeed4science.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{song2023deepspeed4science,\n  title={DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies},\n  author={Song, Shuaiwen Leon and Kruft, Bonnie and Zhang, Minjia and Li, Conglong and Chen, Shiyang and Zhang, Chengming and Tanaka, Masahiro and Wu, Xiaoxia and Rasley, Jeff and Awan, Ammar Ahmad and others},\n  journal={arXiv preprint arXiv:2310.04610},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Training Large-Scale OPT Model with DeepSpeed-Chat\nDESCRIPTION: Command to train a 66B parameter OPT model using multi-node deployment with DeepSpeed-Chat. Configures both actor and reward models for distributed training across multiple GPUs.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython train.py --actor-model facebook/opt-66b --reward-model facebook/opt-350m --deployment-type multi_node\n```\n\n----------------------------------------\n\nTITLE: Citing DeepSpeed-VisualChat in BibTeX Format\nDESCRIPTION: BibTeX citation for the DeepSpeed-VisualChat arxiv report. This snippet provides the necessary information to cite the project in academic papers or research documents.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{yao2023deepspeed-visualchat,\n  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},\n  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},\n  journal={arXiv preprint arXiv:2309.14327},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Autotuning with Multi-node Configuration\nDESCRIPTION: Command line syntax for using DeepSpeed autotuning with explicit GPU and node configurations. This allows specifying the exact resources to be used during the autotuning process.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --autotuning=[run|tune] --num_gpus=$NUM_GPUS --num_nodes=$NUM_NODES <user script> --deepspeed ds_config.json <other user args>\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Training Loop with DeepSpeed in Python\nDESCRIPTION: This snippet demonstrates the basic structure of a training loop using DeepSpeed. It includes forward propagation, backward propagation, and weight updates using the DeepSpeedEngine.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/training.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor step, batch in enumerate(data_loader):\n    #forward() method\n    loss = model_engine(batch)\n\n    #runs backpropagation\n    model_engine.backward(loss)\n\n    #weight update\n    model_engine.step()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DeepSpeed Inference Engine in Python\nDESCRIPTION: This snippet demonstrates how to initialize and use the DeepSpeed inference engine within a data loading loop. It shows the basic structure for performing inference on batches of data.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/inference-engine.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor step, batch in enumerate(data_loader):\n    #forward() method\n    loss = engine(batch)\n```\n\n----------------------------------------\n\nTITLE: Adding Autotuning Configuration in DeepSpeed JSON Config\nDESCRIPTION: Example of how to enable autotuning in the DeepSpeed configuration file with argument mappings. This maps the DeepSpeed train_micro_batch_size_per_gpu parameter to the training script's per_device_train_batch_size argument.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/autotuning/README.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"autotuning\": {\n        \"enabled\": true,\n        \"arg_mappings\": {\n            \"train_micro_batch_size_per_gpu\": \"--per_device_train_batch_size\",\n        }\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for HuggingFace Pretrained Model Files\nDESCRIPTION: Shows the file structure for loading a HuggingFace pretrained model, including the config JSON and PyTorch model binary.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/bert-finetuning.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n[test/huggingface]\nbert-large-uncased-whole-word-masking-config.json\nbert-large-uncased-whole-word-masking-pytorch_model.bin\n```\n\n----------------------------------------\n\nTITLE: HTML Links and Layout Configuration\nDESCRIPTION: Jekyll layout configuration and HTML formatting for the documentation page header and news section\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/index.md#2025-04-16_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n---\nlayout: single\ntoc: true\ntoc_label: \"Contents\"\ntitle: \"Latest News\"\n\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Megatron-DeepSpeed for Ulysses-Offload\nDESCRIPTION: Example configuration for Megatron-DeepSpeed to enable Ulysses-Offload features, including FPDT, chunk size, and offloading.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/ulysses-offload.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmegatron_options=\"\\\n--ds-sequence-parallel-fpdt \\\n--ds-sequence-parallel-fpdt-chunk-size 65536 \\\n--ds-sequence-parallel-fpdt-offloading \\\n--ds-sequence-parallel-size 4\"\n```\n\n----------------------------------------\n\nTITLE: Executing Unit Tests for DeepSpeed\nDESCRIPTION: This command runs the unit tests for DeepSpeed using pytest with the --forked flag for CUDA functionality in distributed tests.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest --forked tests/unit/\n```\n\n----------------------------------------\n\nTITLE: Installing Huawei Ascend NPU Driver and Firmware\nDESCRIPTION: Commands to install the Huawei Ascend NPU driver and firmware packages\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/accelerator-setup-guide.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./Ascend-hdk-<soc_version>-npu-driver_x.x.x_linux-{arch}.run --full --install-for-all\n./Ascend-hdk-<soc_version>-npu-firmware_x.x.x.x.X.run --full\n```\n\n----------------------------------------\n\nTITLE: Specifying custom CUDA extensions directory\nDESCRIPTION: Sets a custom directory for saving and loading CUDA extensions, useful when using multiple virtual environments.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/_tutorials/advanced-install.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_EXTENSIONS_DIR=./torch-extensions deepspeed ...\n```\n\n----------------------------------------\n\nTITLE: Specifying SciPy Dependency for DeepSpeed\nDESCRIPTION: This line indicates that the SciPy library is a required dependency for the DeepSpeed project. SciPy is a Python library used for scientific and technical computing, which suggests that DeepSpeed may utilize some of its mathematical or scientific functions.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/requirements/requirements-deepcompile.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nscipy\n```\n\n----------------------------------------\n\nTITLE: Applying Activation Checkpointing in DeepSpeed\nDESCRIPTION: Function to apply activation checkpointing to a module or function in DeepSpeed.\nSOURCE: https://github.com/deepspeedai/DeepSpeed/blob/master/docs/code-docs/source/activation-checkpointing.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndeepspeed.checkpointing.checkpoint\n```"
  }
]
[
  {
    "owner": "farama-foundation",
    "repo": "pettingzoo",
    "content": "TITLE: Interacting with AEC Environment in Python\nDESCRIPTION: This snippet demonstrates how to interact with an AEC environment using the Rock-Paper-Scissors game as an example. It shows the basic loop of iterating through agents, getting observations, and taking actions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/aec.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.classic import rps_v2\n\nenv = rps_v2.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample() # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing Connect Four Training Loop with Evolution Strategy\nDESCRIPTION: A training loop implementation that handles agent-opponent interactions in Connect Four. Features include wandb logging, population-based training, opponent selection (self-play or random), state management for both players, and transition storage in replay buffer. The code handles alternating turns and maintains game state from both players' perspectives.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif max_episodes > 0:\n   if wb:\n      wandb.init(\n            project=\"AgileRL\",\n            name=\"{}-EvoHPO-{}-{}Opposition-CNN-{}\".format(\n               \"connect_four_v3\",\n               INIT_HP[\"ALGO\"],\n               LESSON[\"opponent\"],\n               datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n            ),\n            config={\n               \"algo\": \"Evo HPO Rainbow DQN\",\n               \"env\": \"connect_four_v3\",\n               \"INIT_HP\": INIT_HP,\n               \"lesson\": LESSON,\n            },\n      )\n\ntotal_steps = 0\ntotal_episodes = 0\npbar = trange(int(max_episodes / episodes_per_epoch))\n\n# Training loop\nfor idx_epi in pbar:\n   turns_per_episode = []\n   train_actions_hist = [0] * action_dim\n   for agent in pop:  # Loop through population\n         for episode in range(episodes_per_epoch):\n            env.reset()  # Reset environment at start of episode\n            observation, env_reward, done, truncation, _ = env.last()\n\n            (\n               p1_state,\n               p1_state_flipped,\n               p1_action,\n               p1_next_state,\n               p1_next_state_flipped,\n            ) = (None, None, None, None, None)\n\n            if LESSON[\"opponent\"] == \"self\":\n               # Randomly choose opponent from opponent pool if using self-play\n               opponent = random.choice(opponent_pool)\n            else:\n               # Create opponent of desired difficulty\n               opponent = Opponent(env, difficulty=LESSON[\"opponent\"])\n\n            # Randomly decide whether agent will go first or second\n            if random.random() > 0.5:\n               opponent_first = False\n            else:\n               opponent_first = True\n\n            score = 0\n            turns = 0  # Number of turns counter\n\n            for idx_step in range(max_steps):\n               # Player 0\"s turn\n               p0_action_mask = observation[\"action_mask\"]\n               p0_state = np.moveaxis(observation[\"observation\"], [-1], [-3])\n               p0_state_flipped = np.expand_dims(np.flip(p0_state, 2), 0)\n               p0_state = np.expand_dims(p0_state, 0)\n\n               if opponent_first:\n                     if LESSON[\"opponent\"] == \"self\":\n                        p0_action = opponent.get_action(\n                           p0_state, 0, p0_action_mask\n                        )[0]\n                     elif LESSON[\"opponent\"] == \"random\":\n                        p0_action = opponent.get_action(\n                           p0_action_mask, p1_action, LESSON[\"block_vert_coef\"]\n                        )\n                     else:\n                        p0_action = opponent.get_action(player=0)\n               else:\n                     p0_action = agent.get_action(\n                        p0_state, epsilon, p0_action_mask\n                     )[\n                        0\n                     ]  # Get next action from agent\n                     train_actions_hist[p0_action] += 1\n\n               env.step(p0_action)  # Act in environment\n               observation, env_reward, done, truncation, _ = env.last()\n               p0_next_state = np.moveaxis(\n                     observation[\"observation\"], [-1], [-3]\n               )\n               p0_next_state_flipped = np.expand_dims(\n                     np.flip(p0_next_state, 2), 0\n               )\n               p0_next_state = np.expand_dims(p0_next_state, 0)\n\n               if not opponent_first:\n                     score += env_reward\n               turns += 1\n\n               # Check if game is over (Player 0 win)\n               if done or truncation:\n                     reward = env.reward(done=True, player=0)\n                     memory.save2memoryVectEnvs(\n                        np.concatenate(\n                           (\n                                 p0_state,\n                                 p1_state,\n                                 p0_state_flipped,\n                                 p1_state_flipped,\n                           )\n                        ),\n                        [p0_action, p1_action, 6 - p0_action, 6 - p1_action],\n                        [\n                           reward,\n                           LESSON[\"rewards\"][\"lose\"],\n                           reward,\n                           LESSON[\"rewards\"][\"lose\"],\n                        ],\n                        np.concatenate(\n                           (\n                                 p0_next_state,\n                                 p1_next_state,\n                                 p0_next_state_flipped,\n                                 p1_next_state_flipped,\n                           )\n                        ),\n                        [done, done, done, done],\n                     )\n               else:  # Play continues\n                     if p1_state is not None:\n                        reward = env.reward(done=False, player=1)\n                        memory.save2memoryVectEnvs(\n                           np.concatenate((p1_state, p1_state_flipped)),\n                           [p1_action, 6 - p1_action],\n                           [reward, reward],\n                           np.concatenate(\n                                 (p1_next_state, p1_next_state_flipped)\n                           ),\n                           [done, done],\n                        )\n\n                     # Player 1\"s turn\n                     p1_action_mask = observation[\"action_mask\"]\n                     p1_state = np.moveaxis(\n                        observation[\"observation\"], [-1], [-3]\n                     )\n                     # Swap pieces so that the agent always sees the board from the same perspective\n                     p1_state[[0, 1], :, :] = p1_state[[0, 1], :, :]\n                     p1_state_flipped = np.expand_dims(np.flip(p1_state, 2), 0)\n                     p1_state = np.expand_dims(p1_state, 0)\n\n                     if not opponent_first:\n                        if LESSON[\"opponent\"] == \"self\":\n                           p1_action = opponent.get_action(\n                                 p1_state, 0, p1_action_mask\n                           )[0]\n                        elif LESSON[\"opponent\"] == \"random\":\n                           p1_action = opponent.get_action(\n                                 p1_action_mask,\n                                 p0_action,\n                                 LESSON[\"block_vert_coef\"],\n                           )\n                        else:\n                           p1_action = opponent.get_action(player=1)\n                     else:\n                        p1_action = agent.get_action(\n                           p1_state, epsilon, p1_action_mask\n                        )[\n                           0\n                        ]  # Get next action from agent\n                        train_actions_hist[p1_action] += 1\n\n                     env.step(p1_action)  # Act in environment\n                     observation, env_reward, done, truncation, _ = env.last()\n                     p1_next_state = np.moveaxis(\n                        observation[\"observation\"], [-1], [-3]\n                     )\n                     p1_next_state[[0, 1], :, :] = p1_next_state[[0, 1], :, :]\n                     p1_next_state_flipped = np.expand_dims(\n                        np.flip(p1_next_state, 2), 0\n                     )\n                     p1_next_state = np.expand_dims(p1_next_state, 0)\n\n                     if opponent_first:\n                        score += env_reward\n                     turns += 1\n\n                     # Check if game is over (Player 1 win)\n                     if done or truncation:\n                        reward = env.reward(done=True, player=1)\n                        memory.save2memoryVectEnvs(\n                           np.concatenate(\n                                 (\n                                    p0_state,\n                                    p1_state,\n                                    p0_state_flipped,\n                                    p1_state_flipped,\n                                 )\n                           ),\n                           [\n                                 p0_action,\n                                 p1_action,\n                                 6 - p0_action,\n                                 6 - p1_action,\n```\n\n----------------------------------------\n\nTITLE: Training Multiple Agents using MADDPG with AgileRL and PettingZoo\nDESCRIPTION: This code demonstrates how to train multiple agents using the MADDPG algorithm from AgileRL on the Space Invaders environment from PettingZoo. It covers environment setup, agent initialization, training loop implementation, and model saving.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MADDPG.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torch.nn.functional import mse_loss\nfrom agilerl.algorithms.maddpg import MADDPG\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom pettingzoo.atari import space_invaders_v2\nfrom agilerl.utils.utils import makeVectEnvPZ\nfrom agilerl.wrappers.pettingzoo_wrappers import PettingZooVectWrapper\n\n# Create a vectorized environment\nenv = makeVectEnvPZ(env_name=space_invaders_v2,\n                    num_envs=1,\n                    asynchronous=False,\n                    truncate_env=False,\n                    wrappers=PettingZooVectWrapper,\n                    auto_reset=True)\n\n# Create a list of agent configurations\nstate_dim = env.observation_space.shape\nn_actions = env.action_space.n\nagent_list = []\nfor i in range(env.num_agents):\n    agent_config = {\n        'agent_name': f\"agent_{i}\",\n        'state_dim': state_dim,\n        'action_dim': n_actions,\n        'one_hot': True,\n        'discrete_actions': True,\n        'n_agents': env.num_agents,\n        'net_config': {\n            'arch': 'mlp',\n            'hidden_size': [64, 64],\n            'mlp_output_activation': 'softmax',\n        }\n    }\n    agent_list.append(agent_config)\n\n# Initialize MADDPG agent\nmaddpg = MADDPG(\n    state_dim=state_dim,\n    action_dim=n_actions,\n    one_hot=True,\n    discrete_actions=True,\n    n_agents=env.num_agents,\n    agent_list=agent_list,\n    batch_size=128,\n    lr_actor=1e-4,\n    lr_critic=1e-3,\n    learn_step=100,\n    gamma=0.99,\n    tau=0.01,\n    use_grad_clip=True,\n    grad_clip_value=5.0,\n)\n\n# Initialize replay buffer\nmemory = MultiAgentReplayBuffer(state_dim=state_dim,\n                                action_dim=1,\n                                n_agents=env.num_agents,\n                                buffer_size=10_000,\n                                batch_size=128,\n                                discrete_actions=True,\n                                device=maddpg.device)\n\n# Training loop\ntotal_steps = 0\nmax_episodes = 1000\nmax_steps = 100\n\nfor episode in range(max_episodes):\n    state, _ = env.reset()\n    episode_reward = 0\n    agent_losses = [[] for _ in range(env.num_agents)]\n\n    for step in range(max_steps):\n        total_steps += 1\n\n        # Get actions for all agents\n        actions = maddpg.getAction(state)\n        # Execute actions in the environment\n        next_state, reward, terminated, truncated, _ = env.step(actions)\n        # Store experience in the replay buffer\n        memory.save2memoryVectEnvs(state, actions, reward, next_state, terminated)\n\n        state = next_state\n        episode_reward += np.mean(reward)\n\n        # Learn if enough samples are available in memory\n        if memory.counter > maddpg.batch_size:\n            agent_losses = maddpg.learn(memory)\n\n        if terminated.any() or truncated.any() or step == max_steps - 1:\n            break\n\n    if episode % 10 == 0:\n        print(f\"Episode: {episode}, total steps: {total_steps}, reward: {episode_reward}\")\n\n# Save the trained model\nmaddpg.saveCheckpoint(\"./maddpg_checkpoint.pth\")\n```\n\n----------------------------------------\n\nTITLE: PPO Implementation for Atari Games using CleanRL\nDESCRIPTION: Complete implementation of PPO algorithm for training agents on Atari environments, including CLI argument parsing, logging setup, neural network architecture, and training loop with evaluation.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/advanced_PPO.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/CleanRL/cleanrl_advanced.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: PPO Implementation with CleanRL for PettingZoo\nDESCRIPTION: Complete implementation of PPO algorithm using CleanRL framework to train agents in PettingZoo's Pistonball environment. The code includes environment setup, model architecture, and training loop.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/implementing_PPO.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/CleanRL/cleanrl.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Memory Storage and Learning Loop Implementation in Python\nDESCRIPTION: Code snippet showing memory storage for experience replay and learning loop implementation with reward handling and state management for both players in a two-player environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmemory.save2memoryVectEnvs(\n    np.concatenate((p0_state, p1_state, p0_state_flipped, p1_state_flipped)),\n    [p0_action, p1_action, 6 - p0_action, 6 - p1_action],\n    [LESSON[\"rewards\"][\"lose\"], reward, LESSON[\"rewards\"][\"lose\"], reward],\n    np.concatenate((p0_next_state, p1_next_state, p0_next_state_flipped, p1_next_state_flipped)),\n    [done, done, done, done]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Rock-Paper-Scissors Environment in Python\nDESCRIPTION: Example implementation of a Rock-Paper-Scissors environment using PettingZoo's Parallel API. Includes step function, state updates, and reward calculations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../code_examples/parallel_rps.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Training DQN Agent with Tianshou in PettingZoo's Tic-Tac-Toe Environment\nDESCRIPTION: A complete implementation of training a DQN agent against a random policy in the Tic-Tac-Toe environment. The code demonstrates environment setup, neural network definition, policy creation, collector configuration, and the training loop with Tianshou.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/intermediate.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport gym\nimport numpy as np\nimport torch\nfrom torch import nn\nimport pettingzoo\nfrom pettingzoo.classic import tictactoe_v3\n\nfrom tianshou.data import Collector\nfrom tianshou.env import DummyVectorEnv, PettingZooEnv\nfrom tianshou.policy import DQNPolicy, RandomPolicy, MultiAgentPolicyManager\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.utils.net.common import Net\n\n\n# Create the environment\ndef make_env():\n    # Return the wrapped environment\n    env = tictactoe_v3.env(render_mode=\"human\")\n    return PettingZooEnv(env)\n\n\n# Define the training pipeline\ntrain_envs = DummyVectorEnv([make_env for _ in range(10)])\ntest_envs = DummyVectorEnv([make_env for _ in range(10)])\n\n# Print the environment agent observation and action spaces\nenv = make_env()\nobs_space = env.observation_space\nact_space = env.action_space\nprint(f\"obs_space:{obs_space},act_space:{act_space}\")\n\n# Create the Q network for DQN agent\n# Tianshou networks have a different defintion than PyTorch networks\n# (see https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#q-network)\nnet = Net(\n    # Use the `Discrete` observation shape (remove the last dimension - 1)\n    # obs_shape = (3, 3, 3) with discrete and then flatten to (27,)\n    state_shape=np.array(obs_space.shape).prod(),\n    # The network outputs a value for each action (there are 9 actions in TicTacToe)\n    action_shape=act_space.n,\n    # DQN network parameters\n    hidden_sizes=[128, 128, 128, 128],\n    # Set to a (very) small number if encountering convergence issues\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\n# Create the policy for the first agent (The second agent will be random)\noptim = torch.optim.Adam(net.parameters(), lr=1e-4)\npolicy = DQNPolicy(\n    net,\n    optim,\n    discount_factor=0.9,\n    estimation_step=3,\n    target_update_freq=320,\n)\n\n# Random policy for the opponent\npolicy2 = RandomPolicy()\n\n# The agents.\n# The first agent is the DQN agent, called \"crosses\", 1 for \"crosses\" (The index is zero-indexed, so the first agent is 0)\n# The second agent is the Random agent, called \"noughts\", 0 for \"noughts\" (The index is zero-indexed, so the second agent is 1)\npolicies = {\"crosses\": policy, \"noughts\": policy2}\n\n# Initialize the policy manager\npolicy = MultiAgentPolicyManager(policies, env)\n\n\n# Create the collector for training and collecting experience for replay buffer\ntrain_collector = Collector(\n    policy,\n    train_envs,\n    # Since the environment output is a Dict, we need to preprocess it\n    # See the TicTacToe environments observation space\n    # The default is not preprocessing, but we need to preprocess the observation\n    # from Dict to ndarray (if not using RandomPolicy)\n    exploration_noise=True,\n)\n\n# Create a test collector for evaluating the policy\ntest_collector = Collector(policy, test_envs, exploration_noise=True)\n\n# Reset collectors\ntrain_collector.reset()\ntest_collector.reset()\n\n# Train the agent\nresult = offpolicy_trainer(\n    policy,\n    train_collector,\n    test_collector,\n    max_epoch=50,  # maximum number of epochs\n    step_per_epoch=1000,  # number of steps per epoch\n    step_per_collect=50,  # number of steps per data collection\n    episode_per_test=10,  # number of episodes per test\n    batch_size=64,  # batch size for updating model\n    train_fn=lambda epoch, env_step: policy.policies[\"crosses\"].set_eps(0.1),\n    test_fn=lambda epoch, env_step: policy.policies[\"crosses\"].set_eps(0.05),\n    stop_fn=lambda mean_reward: mean_reward >= 0.6,  # stop training if the mean reward is greater than or equal to 0.6\n    verbose=True,  # print verbose information\n)\nprint(f\"Finished training! Use {result['duration']}\")\n\n# Save the policy\ntorch.save(policy.policies[\"crosses\"].state_dict(), \"dqn_tictactoe.pth\")\nprint(\"Saved policy.\")\n```\n\n----------------------------------------\n\nTITLE: PPO Training and Evaluation Script for Waterworld\nDESCRIPTION: Complete implementation for training PPO agents in the Waterworld environment using Stable Baselines 3. Includes environment setup, model training, evaluation, and rendering functionality.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/waterworld.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/SB3/waterworld/sb3_waterworld_vector.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Interacting with PettingZoo Environment using AEC API\nDESCRIPTION: Demonstrates the basic interaction loop with a PettingZoo environment using the Agent Environment Cycle (AEC) API, including environment reset, getting observations, and taking actions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n    env.step(action)\n```\n\n----------------------------------------\n\nTITLE: Implementing Curriculum Environment Wrapper in Python\nDESCRIPTION: Defines a CurriculumEnv class that wraps a Connect Four environment to enable curriculum learning by modifying rewards based on lesson configurations. Includes methods for filling replay buffer, checking win conditions, and handling game state transitions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CurriculumEnv:\n   \"\"\"Wrapper around environment to modify reward for curriculum learning.\n\n   :param env: Environment to learn in\n   :type env: PettingZoo-style environment\n   :param lesson: Lesson settings for curriculum learning\n   :type lesson: dict\n   \"\"\"\n\n   def __init__(self, env, lesson):\n      self.env = env\n      self.lesson = lesson\n\n   def fill_replay_buffer(self, memory, opponent):\n      \"\"\"Fill the replay buffer with experiences collected by taking random actions in the environment.\n\n      :param memory: Experience replay buffer\n      :type memory: AgileRL experience replay buffer\n      \"\"\"\n      print(\"Filling replay buffer ...\")\n\n      pbar = tqdm(total=memory.memory_size)\n      while len(memory) < memory.memory_size:\n         # Randomly decide whether random player will go first or second\n         if random.random() > 0.5:\n               opponent_first = False\n         else:\n               opponent_first = True\n\n         mem_full = len(memory)\n         self.reset()  # Reset environment at start of episode\n         observation, reward, done, truncation, _ = self.last()\n\n         (\n               p1_state,\n               p1_state_flipped,\n               p1_action,\n               p1_next_state,\n               p1_next_state_flipped,\n         ) = (None, None, None, None, None)\n         done, truncation = False, False\n\n         while not (done or truncation):\n               # Player 0's turn\n               p0_action_mask = observation[\"action_mask\"]\n               p0_state = np.moveaxis(observation[\"observation\"], [-1], [-3])\n               p0_state_flipped = np.expand_dims(np.flip(p0_state, 2), 0)\n               p0_state = np.expand_dims(p0_state, 0)\n               if opponent_first:\n                  p0_action = self.env.action_space(\"player_0\").sample(p0_action_mask)\n               else:\n                  if self.lesson[\"warm_up_opponent\"] == \"random\":\n                     p0_action = opponent.get_action(\n                           p0_action_mask, p1_action, self.lesson[\"block_vert_coef\"]\n                     )\n                  else:\n                     p0_action = opponent.get_action(player=0)\n               self.step(p0_action)  # Act in environment\n               observation, env_reward, done, truncation, _ = self.last()\n               p0_next_state = np.moveaxis(observation[\"observation\"], [-1], [-3])\n               p0_next_state_flipped = np.expand_dims(np.flip(p0_next_state, 2), 0)\n               p0_next_state = np.expand_dims(p0_next_state, 0)\n\n               if done or truncation:\n                  reward = self.reward(done=True, player=0)\n                  memory.save2memoryVectEnvs(\n                     np.concatenate(\n                           (p0_state, p1_state, p0_state_flipped, p1_state_flipped)\n                     ),\n                     [p0_action, p1_action, 6 - p0_action, 6 - p1_action],\n                     [\n                           reward,\n                           LESSON[\"rewards\"][\"lose\"],\n                           reward,\n                           LESSON[\"rewards\"][\"lose\"],\n                     ],\n                     np.concatenate(\n                           (\n                              p0_next_state,\n                              p1_next_state,\n                              p0_next_state_flipped,\n                              p1_next_state_flipped,\n                           )\n                     ),\n                     [done, done, done, done],\n                  )\n               else:  # Play continues\n                  if p1_state is not None:\n                     reward = self.reward(done=False, player=1)\n                     memory.save2memoryVectEnvs(\n                           np.concatenate((p1_state, p1_state_flipped)),\n                           [p1_action, 6 - p1_action],\n                           [reward, reward],\n                           np.concatenate((p1_next_state, p1_next_state_flipped)),\n                           [done, done],\n                     )\n\n                  # Player 1's turn\n                  p1_action_mask = observation[\"action_mask\"]\n                  p1_state = np.moveaxis(observation[\"observation\"], [-1], [-3])\n                  p1_state[[0, 1], :, :] = p1_state[[0, 1], :, :]\n                  p1_state_flipped = np.expand_dims(np.flip(p1_state, 2), 0)\n                  p1_state = np.expand_dims(p1_state, 0)\n                  if not opponent_first:\n                     p1_action = self.env.action_space(\"player_1\").sample(\n                           p1_action_mask\n                     )\n                  else:\n                     if self.lesson[\"warm_up_opponent\"] == \"random\":\n                           p1_action = opponent.get_action(\n                              p1_action_mask, p0_action, LESSON[\"block_vert_coef\"]\n                           )\n                     else:\n                           p1_action = opponent.get_action(player=1)\n                  self.step(p1_action)  # Act in environment\n                  observation, env_reward, done, truncation, _ = self.last()\n                  p1_next_state = np.moveaxis(observation[\"observation\"], [-1], [-3])\n                  p1_next_state[[0, 1], :, :] = p1_next_state[[0, 1], :, :]\n                  p1_next_state_flipped = np.expand_dims(np.flip(p1_next_state, 2), 0)\n                  p1_next_state = np.expand_dims(p1_next_state, 0)\n\n                  if done or truncation:\n                     reward = self.reward(done=True, player=1)\n                     memory.save2memoryVectEnvs(\n                           np.concatenate(\n                              (p0_state, p1_state, p0_state_flipped, p1_state_flipped)\n                           ),\n                           [p0_action, p1_action, 6 - p0_action, 6 - p1_action],\n                           [\n                              LESSON[\"rewards\"][\"lose\"],\n                              reward,\n                              LESSON[\"rewards\"][\"lose\"],\n                              reward,\n                           ],\n                           np.concatenate(\n                              (\n                                 p0_next_state,\n                                 p1_next_state,\n                                 p0_next_state_flipped,\n                                 p1_next_state_flipped,\n                              )\n                           ),\n                           [done, done, done, done],\n                     )\n\n                  else:  # Play continues\n                     reward = self.reward(done=False, player=0)\n                     memory.save2memoryVectEnvs(\n                           np.concatenate((p0_state, p0_state_flipped)),\n                           [p0_action, 6 - p0_action],\n                           [reward, reward],\n                           np.concatenate((p0_next_state, p0_next_state_flipped)),\n                           [done, done],\n                     )\n\n         pbar.update(len(memory) - mem_full)\n      pbar.close()\n      print(\"Replay buffer warmed up.\")\n      return memory\n\n   def check_winnable(self, lst, piece):\n      \"\"\"Checks if four pieces in a row represent a winnable opportunity, e.g. [1, 1, 1, 0] or [2, 0, 2, 2].\n\n      :param lst: List of pieces in row\n      :type lst: List\n      :param piece: Player piece we are checking (1 or 2)\n      :type piece: int\n      \"\"\"\n      return lst.count(piece) == 3 and lst.count(0) == 1\n\n   def check_vertical_win(self, player):\n      \"\"\"Checks if a win is vertical.\n\n      :param player: Player who we are checking, 0 or 1\n      :type player: int\n      \"\"\"\n      board = np.array(self.env.env.board).reshape(6, 7)\n      piece = player + 1\n\n      column_count = 7\n      row_count = 6\n\n      # Check vertical locations for win\n      for c in range(column_count):\n         for r in range(row_count - 3):\n               if (\n                  board[r][c] == piece\n                  and board[r + 1][c] == piece\n                  and board[r + 2][c] == piece\n                  and board[r + 3][c] == piece\n               ):\n                  return True\n      return False\n\n   def check_three_in_row(self, player):\n      \"\"\"Checks if there are three pieces in a row and a blank space next, or two pieces - blank - piece.\n\n      :param player: Player who we are checking, 0 or 1\n      :type player: int\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Training DQN Agent on Leduc Hold'em with RLlib\nDESCRIPTION: Python script for training a Deep Q-Network agent on the Leduc Hold'em poker environment using Ray RLlib. It configures the training parameters, environment wrappers, and runs the training process.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/holdem.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/Ray/rllib_leduc_holdem.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Using Custom AEC Environment in Python\nDESCRIPTION: Demonstration of how to interact with a custom AEC environment in PettingZoo, including environment initialization, stepping through agents, and collecting observations and rewards.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../code_examples/aec_rps_usage.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Environment Logic in Python for PettingZoo\nDESCRIPTION: This code snippet defines a custom environment class 'CustomEnvironment' for a two-player prisoner-guard game. It includes methods for environment setup, step execution, and reward calculation. The environment uses a 7x7 grid where players can move in cardinal directions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/2-environment-logic.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium\nimport numpy as np\nfrom gymnasium.spaces import Discrete\n\nfrom pettingzoo import AECEnv\nfrom pettingzoo.utils import agent_selector\n\n\nclass CustomEnvironment(AECEnv):\n    metadata = {\"render_modes\": [\"human\"], \"name\": \"custom_environment_v0\"}\n\n    def __init__(self, render_mode=None):\n        super().__init__()\n        self.render_mode = render_mode\n\n        # Two agents: prisoner and guard\n        self.possible_agents = [\"prisoner\", \"guard\"]\n        self.agent_name_mapping = dict(\n            zip(self.possible_agents, list(range(len(self.possible_agents))))\n        )\n\n        # Observation space: 7x7 grid\n        self.observation_spaces = {agent: Discrete(49) for agent in self.possible_agents}\n\n        # Action space: 0: Up, 1: Right, 2: Down, 3: Left\n        self.action_spaces = {agent: Discrete(4) for agent in self.possible_agents}\n\n    def observation_space(self, agent):\n        return self.observation_spaces[agent]\n\n    def action_space(self, agent):\n        return self.action_spaces[agent]\n\n    def render(self):\n        if self.render_mode is None:\n            gymnasium.logger.warn(\n                \"You are calling render method without specifying any render mode.\")\n            return\n\n        # TODO: Implement render logic\n\n    def observe(self, agent):\n        # TODO: Implement observe logic\n        return np.zeros(49)\n\n    def close(self):\n        # TODO: Implement close logic\n        pass\n\n    def reset(self, seed=None, options=None):\n        self.agents = self.possible_agents[:]\n\n        # Reset game state\n        self.prisoner_pos = [0, 0]  # Top-left corner\n        self.guard_pos = [6, 6]  # Bottom-right corner\n        self.door_pos = [np.random.randint(2, 5), np.random.randint(2, 5)]  # Random middle position\n\n        self.current_agent = \"prisoner\"\n        self._agent_selector = agent_selector(self.agents)\n\n        self.terminations = {agent: False for agent in self.agents}\n        self.truncations = {agent: False for agent in self.agents}\n        self.rewards = {agent: 0 for agent in self.agents}\n        self.infos = {agent: {} for agent in self.agents}\n\n        self.state = {\n            \"prisoner\": self.prisoner_pos,\n            \"guard\": self.guard_pos,\n            \"door\": self.door_pos\n        }\n\n        return self.observe(self.current_agent), self.infos\n\n    def step(self, action):\n        if (\n            self.terminations[self.current_agent]\n            or self.truncations[self.current_agent]\n        ):\n            return self._was_dead_step(action)\n\n        # Current agent takes action\n        self._take_action(action)\n\n        # Check for game over conditions\n        self._check_game_over()\n\n        # Set up observations, rewards for all agents\n        self.rewards[self.current_agent] = self._get_reward()\n        self.rewards[self._get_opposite_agent()] = -self.rewards[self.current_agent]\n\n        # Switch to the next agent\n        self.current_agent = self._agent_selector.next()\n\n        return self.observe(self.current_agent), self.rewards, self.terminations, self.truncations, self.infos\n\n    def _take_action(self, action):\n        # Update position based on action\n        if self.current_agent == \"prisoner\":\n            self._update_position(self.prisoner_pos, action)\n        else:\n            self._update_position(self.guard_pos, action)\n\n    def _update_position(self, position, action):\n        if action == 0:  # Up\n            position[0] = max(0, position[0] - 1)\n        elif action == 1:  # Right\n            position[1] = min(6, position[1] + 1)\n        elif action == 2:  # Down\n            position[0] = min(6, position[0] + 1)\n        elif action == 3:  # Left\n            position[1] = max(0, position[1] - 1)\n\n    def _check_game_over(self):\n        # Prisoner reaches the door\n        if self.prisoner_pos == self.door_pos:\n            self.terminations = {agent: True for agent in self.agents}\n\n        # Guard catches the prisoner\n        if self.prisoner_pos == self.guard_pos:\n            self.terminations = {agent: True for agent in self.agents}\n\n    def _get_reward(self):\n        if self.prisoner_pos == self.door_pos:\n            return 1  # Prisoner wins\n        elif self.prisoner_pos == self.guard_pos:\n            return -1  # Guard wins\n        else:\n            return 0  # Game continues\n\n    def _get_opposite_agent(self):\n        return \"guard\" if self.current_agent == \"prisoner\" else \"prisoner\"\n\n    def _was_dead_step(self, action):\n        # TODO: Implement dead step logic\n        pass\n```\n\n----------------------------------------\n\nTITLE: Training MATD3 Agents in PettingZoo\nDESCRIPTION: Main implementation for training multiple agents using MATD3 algorithm in the simple speaker listener environment. Contains the core training loop and agent initialization.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MATD3.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/AgileRL/agilerl_matd3.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parallel Environment in Python\nDESCRIPTION: Demonstration of how to interact with a custom Parallel environment in PettingZoo, including environment initialization, stepping, and collecting observations and rewards.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../code_examples/parallel_rps_usage.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Implementing Texas Hold'em No Limit Game with ActionMaskAgent in Python\nDESCRIPTION: This code snippet demonstrates how to set up and run a Texas Hold'em No Limit game using the ActionMaskAgent from PettingZoo. It initializes the environment, creates an agent, and executes a series of actions while printing observations, rewards, and game states.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef texas_holdem_no_limit():\n    env = texas_holdem_no_limit_v6.env(num_players=2)\n    env.reset(seed=42)\n\n    for agent in env.agent_iter():\n        observation, reward, termination, truncation, info = env.last()\n\n        if termination or truncation:\n            action = None\n        else:\n            # this is where you would insert your policy\n            mask = observation['action_mask']\n            action = random.choice([i for i, legal in enumerate(mask) if legal])\n\n        print(f\"Observation: {observation}\")\n        print(f\"Reward: {reward}\")\n        print(f\"Termination: {termination}\")\n        print(f\"Truncation: {truncation}\")\n        print(f\"Return: {env.rewards[agent]}\")\n        print()\n        print(f\"Action: {action}\")\n        print()\n\n        env.step(action)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Action Masked PPO for Connect Four\nDESCRIPTION: Python script for training and evaluating agents using Maskable PPO on the Connect Four environment. It includes a custom wrapper for SB3 compatibility, training, evaluation, and a demo game with human rendering.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/connect_four.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Code content omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Implementing Action Masking in PettingZoo Environment\nDESCRIPTION: Code example showing how to implement action masking in a custom PettingZoo environment. The code demonstrates how to prevent invalid actions from being taken by agents, like preventing a chess pawn from moving forward when at the front of the board.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/3-action-masking.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/CustomEnvironment/tutorial3_action_masking.py\n   :language: python\n   :caption: /custom-environment/env/custom_environment.py\n```\n\n----------------------------------------\n\nTITLE: Connect Four Opponent AI Implementation\nDESCRIPTION: Implementation of various opponent AI difficulties including random, weak rule-based, and strong rule-based opponents. Includes board analysis and move selection logic.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Opponent:\n    \"\"\"Connect 4 opponent to train and/or evaluate against.\n\n    :param env: Environment to learn in\n    :type env: PettingZoo-style environment\n    :param difficulty: Difficulty level of opponent, 'random', 'weak' or 'strong'\n    :type difficulty: str\n    \"\"\"\n\n    def __init__(self, env, difficulty):\n        self.env = env.env\n        self.difficulty = difficulty\n        if self.difficulty == \"random\":\n            self.get_action = self.random_opponent\n        elif self.difficulty == \"weak\":\n            self.get_action = self.weak_rule_based_opponent\n        else:\n            self.get_action = self.strong_rule_based_opponent\n        self.num_cols = 7\n        self.num_rows = 6\n        self.length = 4\n        self.top = [0] * self.num_cols\n```\n\n----------------------------------------\n\nTITLE: Main Environment Loop - Python\nDESCRIPTION: Complete example of the main interaction loop with a PettingZoo environment, including initialization, agent iteration, and action execution.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/basic_usage.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import cooperative_pong_v5\n\nenv = cooperative_pong_v5.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        # this is where you would insert your policy\n        action = env.action_space(agent).sample()\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Running API Test for PettingZoo Environments in Python\nDESCRIPTION: Tests an environment's compliance with the PettingZoo API by running it for a specified number of cycles and verifying that it behaves according to requirements.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import api_test\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\napi_test(env, num_cycles=1000, verbose_progress=False)\n```\n\n----------------------------------------\n\nTITLE: Checking Win Conditions in Connect Four Game with NumPy\nDESCRIPTION: This code evaluates if a move has resulted in a win by checking for consecutive matching pieces in a line. It also detects draw conditions when the board is full. The function returns information about the game state including whether the move was valid, the reward, if the game ended, and optionally the length of consecutive pieces.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# |2x3|\nline = np.concatenate((both_dir[0][::-1], [piece], both_dir[1]))\nif \"\".join(map(str, [piece] * self.length)) in \"\".join(map(str, line)):\n      ended = True\n      break\n\n# ended = np.any(np.greater_equal(np.sum(lengths, 1), self.length - 1))\ndraw = True\nfor c, v in enumerate(self.top):\n    draw &= (v == self.num_rows) if c != col else (v == (self.num_rows - 1))\nended |= draw\nreward = (-1) ** (player) if ended and not draw else 0\n\nreturn (True, reward, ended) + ((lengths,) if return_length else ())\n```\n\n----------------------------------------\n\nTITLE: Applying Frame Stack and Color Reduction in Space Invaders\nDESCRIPTION: Example showing how to preprocess Space Invaders environment by converting it to grayscale and stacking 4 frames using SuperSuit wrappers.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.atari import space_invaders_v2\nfrom supersuit import color_reduction_v0, frame_stack_v1\n\nenv = space_invaders_v2.env()\n\nenv = frame_stack_v1(color_reduction_v0(env, 'full'), 4)\n```\n\n----------------------------------------\n\nTITLE: Training PPO Agents on Pistonball Using RLlib\nDESCRIPTION: Implements PPO training on the Pistonball environment using Ray RLlib. The code creates a parallel environment, wraps it for compatibility, configures the PPO algorithm, and trains agents for a specified number of iterations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/pistonball.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport supersuit as ss\nfrom ray import air, tune\nfrom ray.tune.registry import register_env\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.policy.policy import PolicySpec\nimport ray\nimport numpy as np\n\nfrom pettingzoo.butterfly import pistonball_v6\n\n\ndef env_creator(args):\n    env = pistonball_v6.parallel_env(\n        n_pistons=20,\n        time_penalty=-0.1,\n        continuous=True,\n        random_drop=True,\n        random_rotate=True,\n        ball_mass=0.75,\n        ball_friction=0.3,\n        ball_elasticity=1.5,\n        max_cycles=125,\n    )\n    env = ss.color_reduction_v0(env, mode=\"B\")\n    env = ss.dtype_v0(env, \"float32\")\n    env = ss.resize_v1(env, x_size=84, y_size=84)\n    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n    env = ss.frame_stack_v1(env, 3)\n    return env\n\n\nif __name__ == \"__main__\":\n    ray.init()\n\n    env_name = \"pistonball_v6\"\n    register_env(env_name, lambda config: env_creator(config))\n\n    config = (\n        PPOConfig()\n        .environment(env=env_name)\n        .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n        .training(\n            train_batch_size=512,\n            lr=2e-5,\n            gamma=0.99,\n            lambda_=0.9,\n            use_gae=True,\n            clip_param=0.4,\n            grad_clip=None,\n            entropy_coeff=0.1,\n            vf_loss_coeff=0.25,\n            sgd_minibatch_size=64,\n            num_sgd_iter=10,\n        )\n        .multi_agent(\n            policies={\n                \"piston_0\": PolicySpec(),\n                \"piston_1\": PolicySpec(),\n                \"piston_2\": PolicySpec(),\n                \"piston_3\": PolicySpec(),\n                \"piston_4\": PolicySpec(),\n                \"piston_5\": PolicySpec(),\n                \"piston_6\": PolicySpec(),\n                \"piston_7\": PolicySpec(),\n                \"piston_8\": PolicySpec(),\n                \"piston_9\": PolicySpec(),\n                \"piston_10\": PolicySpec(),\n                \"piston_11\": PolicySpec(),\n                \"piston_12\": PolicySpec(),\n                \"piston_13\": PolicySpec(),\n                \"piston_14\": PolicySpec(),\n                \"piston_15\": PolicySpec(),\n                \"piston_16\": PolicySpec(),\n                \"piston_17\": PolicySpec(),\n                \"piston_18\": PolicySpec(),\n                \"piston_19\": PolicySpec(),\n            },\n            policy_mapping_fn=lambda agent_id, *args, **kwargs: agent_id,\n        )\n        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n        .framework(\"torch\")\n    )\n\n    tune.Tuner(\n        \"PPO\",\n        run_config=air.RunConfig(\n            stop={\"training_iteration\": 500},\n        ),\n        param_space=config.to_dict(),\n    ).fit()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Atari Environments with SuperSuit\nDESCRIPTION: Example showing how to apply common preprocessing steps to Atari environments using SuperSuit wrappers. Includes handling frame flickering, adding stochasticity, frame skipping, and resizing observations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/atari.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport supersuit\nfrom pettingzoo.atari import space_invaders_v2\n\nenv = space_invaders_v2.env()\n\n# as per openai baseline's MaxAndSKip wrapper, maxes over the last 2 frames\n# to deal with frame flickering\nenv = supersuit.max_observation_v0(env, 2)\n\n# repeat_action_probability is set to 0.25 to introduce non-determinism to the system\nenv = supersuit.sticky_actions_v0(env, repeat_action_probability=0.25)\n\n# skip frames for faster processing and less control\n# to be compatible with gym, use frame_skip(env, (2,5))\nenv = supersuit.frame_skip_v0(env, 4)\n\n# downscale observation for faster processing\nenv = supersuit.resize_v1(env, 84, 84)\n\n# allow agent to see everything on the screen despite Atari's flickering screen problem\nenv = supersuit.frame_stack_v1(env, 4)\n```\n\n----------------------------------------\n\nTITLE: Loading DeepMind Control Multi-Agent Soccer with Shimmy\nDESCRIPTION: This snippet demonstrates how to load a DeepMind Control multi-agent soccer game using Shimmy's DmControlMultiAgentCompatibilityV0 wrapper, then run a simple interaction loop that samples random actions for each agent.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/shimmy_wrappers.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom shimmy import DmControlMultiAgentCompatibilityV0\nfrom dm_control.locomotion import soccer as dm_soccer\n\nenv = dm_soccer.load(team_size=2)\nenv = DmControlMultiAgentCompatibilityV0(env, render_mode=\"human\")\n\nobservations, infos = env.reset()\nwhile env.agents:\n    actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n    observations, rewards, terminations, truncations, infos = env.step(actions)\n```\n\n----------------------------------------\n\nTITLE: Agent Evaluation and Population Evolution in Python\nDESCRIPTION: Implementation of agent evaluation against random opponents and population evolution through tournament selection, including metrics tracking and performance logging.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor agent in pop:\n    with torch.no_grad():\n        rewards = []\n        for i in range(evo_loop):\n            env.reset()\n            observation, reward, done, truncation, _ = env.last()\n            player = -1\n            opponent = Opponent(env, difficulty=LESSON[\"eval_opponent\"])\n            opponent_first = random.random() > 0.5\n            score = 0\n            # ... evaluation loop implementation\n```\n\n----------------------------------------\n\nTITLE: Tianshou DQN Training Script with CLI and Logging\nDESCRIPTION: Complete Python script demonstrating DQN agent training with Tianshou, including command line argument parsing and logging implementation\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/advanced.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/Tianshou/3_cli_and_logging.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Implementing Random Agents in Rock-Paper-Scissors Using Tianshou and PettingZoo\nDESCRIPTION: Complete implementation of two random policy agents playing rock-paper-scissors using Tianshou with PettingZoo. The code demonstrates environment setup, policy creation, and collector configuration for multi-agent reinforcement learning.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/beginner.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\n\nfrom tianshou.data import Collector\nfrom tianshou.env import PettingZooEnv\nfrom tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n\nfrom pettingzoo.classic import rps_v2\n\n# Make the environment\nenv = rps_v2.env(render_mode=\"human\")\n# Convert the env to Tianshou's format\nenv = PettingZooEnv(env)\n\nobservation_space = env.observation_space\nif isinstance(observation_space, list):\n    observation_space = observation_space[0]\nassert isinstance(observation_space, gym.spaces.Box)\nstate_shape = observation_space.shape or observation_space.n\naction_shape = env.action_space.shape or env.action_space.n\n\n# The agents are set up with random policies\npolicies = MultiAgentPolicyManager([\n    RandomPolicy(action_shape=action_shape, action_space=env.action_space),\n    RandomPolicy(action_shape=action_shape, action_space=env.action_space),\n], env)\n\ncollector = Collector(policies, env)\n\nresult = collector.collect(n_episode=1)\nprint(f'Final reward: {result[\"rews\"].mean()}, length: {result[\"lens\"].mean()}')\n\n# close the environment\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Padding Action Spaces in Python\nDESCRIPTION: Equalizes action spaces across agents by padding smaller spaces to match the largest. Enables MARL methods requiring homogeneous action spaces. Handles both discrete and box action spaces.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npad_action_space_v0(env)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Trained MATD3 Agents\nDESCRIPTION: Code for loading saved MATD3 agents, testing their performance, and visualizing episodes as animated GIFs.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MATD3.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/AgileRL/render_agilerl_matd3.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Converting AEC Environment to Parallel\nDESCRIPTION: Demonstrates how to convert an AEC environment to a parallel environment using the aec_to_parallel wrapper. Uses pistonball environment as an example.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/pz_wrappers.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils.conversions import aec_to_parallel\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\nenv = aec_to_parallel(env)\n```\n\n----------------------------------------\n\nTITLE: Initializing Agent Indicator Function in Python\nDESCRIPTION: Adds an agent ID indicator to observations. Supports discrete and 1D/2D/3D box spaces. For 1D, appends one-hot vector of agent ID. For 2D/3D, adds channels with binary agent ID encoding. Useful for parameter sharing in heterogeneous agent environments.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nagent_indicator_v0(env, type_only=False)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Trained MADDPG Agents in Space Invaders Environment\nDESCRIPTION: This code snippet demonstrates how to load a saved MADDPG model, test its performance, and visualize episodes as a gif. It uses the trained model from the previous training block to render the agents' behavior in the Space Invaders environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MADDPG.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torch.nn.functional import mse_loss\nfrom agilerl.algorithms.maddpg import MADDPG\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom pettingzoo.atari import space_invaders_v2\nfrom agilerl.utils.utils import makeVectEnvPZ\nfrom agilerl.wrappers.pettingzoo_wrappers import PettingZooVectWrapper\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\n# Create a vectorized environment\nenv = makeVectEnvPZ(env_name=space_invaders_v2,\n                    num_envs=1,\n                    asynchronous=False,\n                    truncate_env=False,\n                    wrappers=PettingZooVectWrapper,\n                    auto_reset=True)\n\n# Load the saved MADDPG model\nmaddpg = MADDPG.load(\"./maddpg_checkpoint.pth\")\n\n# Test the loaded model\ntotal_reward = 0\nnum_episodes = 10\n\nfor episode in range(num_episodes):\n    state, _ = env.reset()\n    episode_reward = 0\n    terminated = truncated = False\n\n    while not (terminated or truncated):\n        actions = maddpg.getAction(state, evaluation=True)\n        next_state, reward, terminated, truncated, _ = env.step(actions)\n        state = next_state\n        episode_reward += np.mean(reward)\n\n    total_reward += episode_reward\n    print(f\"Episode {episode + 1} reward: {episode_reward}\")\n\naverage_reward = total_reward / num_episodes\nprint(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n\n# Render and save episodes as a gif\nnum_episodes_to_render = 5\nframes = []\n\nfor episode in range(num_episodes_to_render):\n    state, _ = env.reset()\n    terminated = truncated = False\n\n    while not (terminated or truncated):\n        frames.append(env.render())\n        actions = maddpg.getAction(state, evaluation=True)\n        next_state, reward, terminated, truncated, _ = env.step(actions)\n        state = next_state\n\n# Create animation\nfig = plt.figure()\nanim = animation.ArtistAnimation(fig, frames, interval=50, blit=True, repeat_delay=1000)\n\n# Save animation as gif\nanim.save('maddpg_space_invaders.gif', writer='pillow')\n\nprint(\"Animation saved as 'maddpg_space_invaders.gif'\")\n```\n\n----------------------------------------\n\nTITLE: Interacting with Parallel Environment in Python\nDESCRIPTION: Example showing how to create and interact with a parallel environment using the Pistonball game. Demonstrates environment initialization, action sampling, and the main interaction loop.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/parallel.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\nparallel_env = pistonball_v6.parallel_env(render_mode=\"human\")\nobservations, infos = parallel_env.reset(seed=42)\n\nwhile parallel_env.agents:\n    # this is where you would insert your policy\n    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}\n\n    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\nparallel_env.close()\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Total Reward in PettingZoo Environment\nDESCRIPTION: Example showing how to calculate the average total reward across multiple episodes in a PettingZoo environment using the pistonball game. This utility helps establish a random policy baseline.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/utils.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import average_total_reward\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\naverage_total_reward(env, max_episodes=100, max_steps=10000000000)\n```\n\n----------------------------------------\n\nTITLE: Running Performance Benchmark for PettingZoo Environments in Python\nDESCRIPTION: Measures and reports the number of steps and cycles an environment can complete in 5 seconds, providing a performance baseline to prevent regressions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import performance_benchmark\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\nperformance_benchmark(env)\n```\n\n----------------------------------------\n\nTITLE: PettingZoo Agent Implementation\nDESCRIPTION: Extended implementation of GymnasiumAgent for multi-agent scenarios with PettingZoo-specific adaptations\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/pettingzoo_agent.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Running a Waterworld Environment with Random Agents\nDESCRIPTION: Example code demonstrating how to initialize, reset, and run a Waterworld environment with random agent actions. The code shows the standard PettingZoo environment loop using agent iteration pattern for multi-agent environments.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/sisl.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.sisl import waterworld_v4\nenv = waterworld_v4.env(render_mode='human')\n\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample() # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Running Seed Test for PettingZoo Environments in Python\nDESCRIPTION: Checks that an environment's random behavior is deterministic when seeded. Tests both separate environments with the same seed and the same environment after reset with the same seed.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import seed_test, parallel_seed_test\nfrom pettingzoo.butterfly import pistonball_v6\nenv_fn = pistonball_v6.env\nseed_test(env_fn, num_cycles=10)\n\n# or for parallel environments\nparallel_env_fn = pistonball_v6.parallel_env\nparallel_seed_test(parallel_env_fn)\n```\n\n----------------------------------------\n\nTITLE: Saving Environment Observations as Images\nDESCRIPTION: Demonstrates how to save agent observations as image files in a PettingZoo environment. The function can save observations for specific agents or all agents in the environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/utils.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import save_observation\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\nenv.reset(seed=42)\nsave_observation(env, agent=None, all_agents=False)\n```\n\n----------------------------------------\n\nTITLE: Running Simple Tag Environment with Random Agents\nDESCRIPTION: Python code example demonstrating how to launch a Simple Tag environment with random agent actions. Shows the complete loop of resetting the environment, iterating through agents, and processing observations and actions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/mpe.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.mpe import simple_tag_v3\nenv = simple_tag_v3.env(render_mode='human')\n\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample() # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Running Custom Render Test for PettingZoo Environments in Python\nDESCRIPTION: Extends the standard render test with custom tests for non-standard rendering modes, allowing verification of additional render outputs.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import render_test\nfrom pettingzoo.butterfly import pistonball_v6\nenv_func = pistonball_v6.env\n\ncustom_tests = {\n    \"svg\": lambda render_result: isinstance(render_result, str)\n}\nrender_test(env_func, custom_tests=custom_tests)\n```\n\n----------------------------------------\n\nTITLE: Testing Observations for PettingZoo Environments in Python\nDESCRIPTION: Saves agent observations for visual inspection, which helps identify bugs in the observation space implementation that might not be caught by automated tests.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import test_save_obs\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\ntest_save_obs(env)\n```\n\n----------------------------------------\n\nTITLE: Implementing AEC Rock-Paper-Scissors Environment in Python\nDESCRIPTION: Example implementation of a Rock-Paper-Scissors environment using PettingZoo's AEC API. Includes agent initialization, step function, and observation/reward logic.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../code_examples/aec_rps.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Launching Pistonball Environment with Random Agents in Python\nDESCRIPTION: Example code to initialize and run a Pistonball environment from the Butterfly collection in PettingZoo, using random actions for all agents.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/butterfly.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\n\nenv = pistonball_v6.parallel_env(render_mode=\"human\")\nobservations, infos = env.reset()\n\nwhile env.agents:\n    # this is where you would insert your policy\n    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n\n    observations, rewards, terminations, truncations, infos = env.step(actions)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Running Max Cycles Test for PettingZoo Environments in Python\nDESCRIPTION: Tests that the max_cycles environment argument functions correctly by verifying the environment runs for the specified number of cycles.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import max_cycles_test\nfrom pettingzoo.butterfly import pistonball_v6\nmax_cycles_test(pistonball_v6)\n```\n\n----------------------------------------\n\nTITLE: Using TerminateIllegalWrapper with AEC Environment\nDESCRIPTION: Example of applying the TerminateIllegalWrapper to an AEC environment (tictactoe) and running a basic agent interaction loop.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/pz_wrappers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import TerminateIllegalWrapper\nfrom pettingzoo.classic import tictactoe_v3\nenv = tictactoe_v3.env()\nenv = TerminateIllegalWrapper(env, illegal_reward=-1)\n\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample()  # this is where you would insert your policy\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Rendering Trained DQN Agent on Leduc Hold'em\nDESCRIPTION: Python script for watching trained DQN agents play against each other in the Leduc Hold'em environment. It loads the saved model and renders the gameplay with the trained policies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/holdem.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/Ray/render_rllib_leduc_holdem.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Interacting with PettingZoo Environment in Python\nDESCRIPTION: This snippet demonstrates how to initialize and interact with a PettingZoo environment, specifically the knights_archers_zombies_v10 environment. It shows the basic loop for iterating through agents, getting observations, and taking actions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import knights_archers_zombies_v10\nenv = knights_archers_zombies_v10.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        # this is where you would insert your policy\n        action = env.action_space(agent).sample()\n\n    env.step(action)\n```\n\n----------------------------------------\n\nTITLE: Using BaseParallelWrapper with Parallel Environment\nDESCRIPTION: Demonstrates how to use BaseParallelWrapper with a parallel environment (pistonball) and run a basic interaction loop.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/pz_wrappers.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import BaseParallelWrapper\nfrom pettingzoo.butterfly import pistonball_v6\n\nparallel_env = pistonball_v6.parallel_env(render_mode=\"human\")\nparallel_env = BaseParallelWrapper(parallel_env)\n\nobservations, infos = parallel_env.reset()\n\nwhile parallel_env.agents:\n    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}  # this is where you would insert your policy\n    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n```\n\n----------------------------------------\n\nTITLE: Configuring Atari Environment Parameters in PettingZoo\nDESCRIPTION: Example showing the common parameters available when initializing an Atari environment in PettingZoo. Includes options for observation type, action space configuration, maximum cycles, and ROM installation path.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/atari.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# using space invaders as an example, but replace with any atari game\nfrom pettingzoo.atari import space_invaders_v2\n\nspace_invaders_v2.env(obs_type='rgb_image', full_action_space=True, max_cycles=100000, auto_rom_install_path=None)\n```\n\n----------------------------------------\n\nTITLE: Main Environment Loop Implementation\nDESCRIPTION: Main function implementing the environment interaction loop for LangChain agents\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/langchain_example.py\n   :pyobject: main\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Loading OpenSpiel Backgammon with TerminateIllegalWrapper\nDESCRIPTION: This example shows how to load an OpenSpiel game (chess) using Shimmy's OpenSpielCompatibilityV0 wrapper and wrap it with PettingZoo's TerminateIllegalWrapper to handle illegal actions. It then runs a simple interaction loop with random action sampling based on action masks.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/shimmy_wrappers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom shimmy import OpenSpielCompatibilityV0\nfrom pettingzoo.utils import TerminateIllegalWrapper\n\nenv = OpenSpielCompatibilityV0(game_name=\"chess\", render_mode=None)\nenv = TerminateIllegalWrapper(env, illegal_reward=-1)\n\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample(info[\"action_mask\"])  # this is where you would insert your policy\n    env.step(action)\n    env.render()\n```\n\n----------------------------------------\n\nTITLE: Running Space Invaders with Random Agents in PettingZoo\nDESCRIPTION: Example code showing how to initialize and run the Space Invaders environment with random agents. The code demonstrates the standard PettingZoo environment loop with agent iteration.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/atari.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.atari import space_invaders_v2\n\nenv = space_invaders_v2.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        action = env.action_space(agent).sample() # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Using AgentSelector for Cycling Through Agents in Python\nDESCRIPTION: Demonstration of the AgentSelector utility class, which helps in cycling through a list of agents in a PettingZoo environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import AgentSelector\nagents = [\"agent_1\", \"agent_2\", \"agent_3\"]\nselector = AgentSelector(agents)\nagent_selection = selector.reset()\n# agent_selection will be \"agent_1\"\nfor i in range(100):\n    agent_selection = selector.next()\n    # will select \"agent_2\", \"agent_3\", \"agent_1\", \"agent_2\", \"agent_3\", ...\n```\n\n----------------------------------------\n\nTITLE: Loading DeepMind Melting Pot Prisoner's Dilemma Environment\nDESCRIPTION: This snippet demonstrates how to load a DeepMind Melting Pot substrate (prisoner's dilemma) using Shimmy's MeltingPotCompatibilityV0 wrapper and run a basic interaction loop with random action sampling for each agent.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/shimmy_wrappers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom shimmy import MeltingPotCompatibilityV0\nenv = MeltingPotCompatibilityV0(substrate_name=\"prisoners_dilemma_in_the_matrix__arena\", render_mode=\"human\")\nobservations, infos = env.reset()\nwhile env.agents:\n    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n    observations, rewards, terminations, truncations, infos = env.step(actions)\n    env.step(actions)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing Black Death Handler in Python\nDESCRIPTION: Handles agent death by zeroing observations and rewards while ignoring actions, rather than removing them. Simplifies death mechanics by maintaining consistent agent presence with null values.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nblack_death_v2(env)\n```\n\n----------------------------------------\n\nTITLE: Connect Four Board Analysis\nDESCRIPTION: Analyzes the game board for potential wins by checking horizontal, vertical, and diagonal alignments. Counts the number of three-in-a-row sequences for a given player.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nboard = np.array(self.env.env.board).reshape(6, 7)\npiece = player + 1\n\n# Check horizontal locations\ncolumn_count = 7\nrow_count = 6\nthree_in_row_count = 0\n\n# Check vertical locations\nfor c in range(column_count):\n   for r in range(row_count - 3):\n         if self.check_winnable(board[r : r + 4, c].tolist(), piece):\n            three_in_row_count += 1\n\n# Check horizontal locations\nfor r in range(row_count):\n   for c in range(column_count - 3):\n         if self.check_winnable(board[r, c : c + 4].tolist(), piece):\n            three_in_row_count += 1\n\n# Check positively sloped diagonals\nfor c in range(column_count - 3):\n   for r in range(row_count - 3):\n         if self.check_winnable(\n            [\n               board[r, c],\n               board[r + 1, c + 1],\n               board[r + 2, c + 2],\n               board[r + 3, c + 3],\n            ],\n            piece,\n         ):\n            three_in_row_count += 1\n\n# Check negatively sloped diagonals\nfor c in range(column_count - 3):\n   for r in range(3, row_count):\n         if self.check_winnable(\n            [\n               board[r, c],\n               board[r - 1, c + 1],\n               board[r - 2, c + 2],\n               board[r - 3, c + 3],\n            ],\n            piece,\n         ):\n            three_in_row_count += 1\n\nreturn three_in_row_count\n```\n\n----------------------------------------\n\nTITLE: Action Masking in AEC Environment (Python)\nDESCRIPTION: This code snippet shows how to implement action masking in an AEC environment using the Chess game as an example. It demonstrates how to handle action masks from both the observation and info dictionaries.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/aec.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.classic import chess_v6\n\nenv = chess_v6.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        # invalid action masking is optional and environment-dependent\n        if \"action_mask\" in info:\n            mask = info[\"action_mask\"]\n        elif isinstance(observation, dict) and \"action_mask\" in observation:\n            mask = observation[\"action_mask\"]\n        else:\n            mask = None\n        action = env.action_space(agent).sample(mask) # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Using ClipOutOfBoundsWrapper with Parallel Environment\nDESCRIPTION: Shows how to use ClipOutOfBoundsWrapper with a parallel environment by converting between AEC and parallel formats.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/pz_wrappers.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import ClipOutOfBoundsWrapper\nfrom pettingzoo.sisl import multiwalker_v9\nfrom pettingzoo.utils import aec_to_parallel\n\nparallel_env = multiwalker_v9.env(render_mode=\"human\")\nparallel_env = ClipOutOfBoundsWrapper(parallel_env)\nparallel_env = aec_to_parallel(parallel_env)\n\nobservations, infos = parallel_env.reset()\n\nwhile parallel_env.agents:\n    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}  # this is where you would insert your policy\n    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n```\n\n----------------------------------------\n\nTITLE: Launching Knights Archers Zombies Environment with Manual Policy in Python\nDESCRIPTION: Example code to initialize and run a Knights Archers Zombies environment from the Butterfly collection in PettingZoo, using a manual policy for one agent and random actions for others.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/butterfly.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pygame\nfrom pettingzoo.butterfly import knights_archers_zombies_v10\n\nenv = knights_archers_zombies_v10.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nmanual_policy = knights_archers_zombies_v10.ManualPolicy(env)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    elif agent == manual_policy.agent:\n        # get user input (controls are WASD and space)\n        action = manual_policy(observation, agent)\n    else:\n        # this is where you would insert your policy (for non-player agents)\n        action = env.action_space(agent).sample()\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Action Masking Agent Implementation\nDESCRIPTION: Specialized agent implementation that uses action masks to determine valid actions in PettingZoo environments\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/action_masking_agent.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Converting Parallel Environment to AEC\nDESCRIPTION: Shows how to convert a parallel environment to an AEC environment using the parallel_to_aec wrapper. Uses pistonball environment as an example.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/pz_wrappers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils import parallel_to_aec\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.parallel_env()\nenv = parallel_to_aec(env)\n```\n\n----------------------------------------\n\nTITLE: Importing and using PettingZoo environment\nDESCRIPTION: Python code demonstrating how to import and use a PettingZoo environment. It shows creating an instance of the Tic-Tac-Toe environment, resetting it, and running a simple loop for agent interactions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/5-using-your-environment.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.classic import tictactoe_v3\n\nenv = tictactoe_v3.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        # This is where you would insert your policy\n        action = env.action_space(agent).sample()\n\n    env.step(action)\n```\n\n----------------------------------------\n\nTITLE: ParallelEnv Class Definition in Python\nDESCRIPTION: Class definition for ParallelEnv showing core attributes and methods. Includes agent management, space definitions, and environment interaction methods.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/parallel.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ParallelEnv:\n    agents: list[AgentID]        # List of current agent names\n    num_agents: int             # Length of agents list\n    possible_agents: list[AgentID]  # List of all possible agents\n    max_num_agents: int         # Length of possible_agents list\n    observation_spaces: Dict[AgentID, gym.spaces.Space]  # Dict of observation spaces\n    action_spaces: Dict[AgentID, gym.spaces.Space]      # Dict of action spaces\n```\n\n----------------------------------------\n\nTITLE: Gymnasium Agent Implementation\nDESCRIPTION: Implementation of the GymnasiumAgent class that handles basic environment interactions and fallbacks to random actions when needed\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/gymnasium_agent.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Creating Raw Environment - Python\nDESCRIPTION: Example of creating a raw environment without the default wrappers using the raw_env() constructor.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/basic_usage.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenvironment_parameters = {}  # any parameters to pass to the environment\nenv = knights_archers_zombies_v10.raw_env(**environment_parameters)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Requirements\nDESCRIPTION: Specifies required Python packages with their minimum version constraints. Includes PettingZoo with butterfly and atari environments, SuperSuit for environment wrappers, TensorBoard for visualization, and PyTorch for deep learning.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/CleanRL/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npettingzoo[butterfly,atari,testing]>=1.24.0\nSuperSuit>=3.9.0\ntensorboard>=2.11.2\ntorch>=1.13.1\n```\n\n----------------------------------------\n\nTITLE: Rendering Trained PPO Agents in Pistonball\nDESCRIPTION: Visualizes the performance of trained PPO agents in the Pistonball environment. The code loads the trained policies, creates a test environment, and renders the agents' gameplay with the learned policies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/pistonball.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport ray\nimport supersuit as ss\nfrom ray.rllib.algorithms.ppo import PPO\nfrom ray.tune.registry import register_env\n\nfrom pettingzoo.butterfly import pistonball_v6\n\n\ndef env_creator(args):\n    env = pistonball_v6.parallel_env(\n        n_pistons=20,\n        time_penalty=-0.1,\n        continuous=True,\n        random_drop=True,\n        random_rotate=True,\n        ball_mass=0.75,\n        ball_friction=0.3,\n        ball_elasticity=1.5,\n        max_cycles=125,\n    )\n    env = ss.color_reduction_v0(env, mode=\"B\")\n    env = ss.dtype_v0(env, \"float32\")\n    env = ss.resize_v1(env, x_size=84, y_size=84)\n    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n    env = ss.frame_stack_v1(env, 3)\n    return env\n\n\nif __name__ == \"__main__\":\n    ray.init()\n\n    env_name = \"pistonball_v6\"\n    register_env(env_name, lambda config: env_creator(config))\n\n    # Replace with your checkpoint's path\n    algorithm = PPO.from_checkpoint(\"/home/user/ray_results/PPO/PPO_pistonball_v6_e3b1c_00000_0_2023-01-24_16-41-55/checkpoint_000500\")\n\n    env = pistonball_v6.parallel_env(\n        n_pistons=20,\n        time_penalty=-0.1,\n        continuous=True,\n        random_drop=True,\n        random_rotate=True,\n        ball_mass=0.75,\n        ball_friction=0.3,\n        ball_elasticity=1.5,\n        max_cycles=125,\n        render_mode=\"human\",\n    )\n    env = ss.color_reduction_v0(env, mode=\"B\")\n    env = ss.dtype_v0(env, \"float32\")\n    env = ss.resize_v1(env, x_size=84, y_size=84)\n    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n    env = ss.frame_stack_v1(env, 3)\n\n    rewards = {agent: 0 for agent in env.possible_agents}\n    observations = env.reset()\n\n    for i in range(500):\n        actions = {agent: algorithm.compute_single_action(observations[agent], policy_id=agent) for agent in env.agents}\n        observations, reward, termination, truncation, info = env.step(actions)\n        for agent in env.agents:\n            rewards[agent] += reward[agent]\n\n    env.close()\n```\n\n----------------------------------------\n\nTITLE: Rock Paper Scissors Game Implementation\nDESCRIPTION: Implementation of Rock Paper Scissors game using PettingZooAgent\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/langchain_example.py\n   :pyobject: rock_paper_scissors\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Connect Four Environment Wrapper Methods\nDESCRIPTION: Wrapper methods for PettingZoo environment operations including last, step, and reset functions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef last(self):\n    \"\"\"Wrapper around PettingZoo env last method.\"\"\"\n    return self.env.last()\n\ndef step(self, action):\n    \"\"\"Wrapper around PettingZoo env step method.\"\"\"\n    self.env.step(action)\n\ndef reset(self):\n    \"\"\"Wrapper around PettingZoo env reset method.\"\"\"\n    self.env.reset()\n```\n\n----------------------------------------\n\nTITLE: SuperSuit Wrapper Function Definitions\nDESCRIPTION: Complete reference documentation for all available SuperSuit wrapper functions including parameters and functionality descriptions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Wrapper function definitions:\n\nclip_reward_v0(env, lower_bound=-1, upper_bound=1)\nclip_actions_v0(env)\ncolor_reduction_v0(env, mode='full')\ndtype_v0(env, dtype)\nflatten_v0(env)\nframe_skip_v0(env, num_frames)\ndelay_observations_v0(env, delay)\nsticky_actions_v0(env, repeat_action_probability)\nframe_stack_v1(env, num_frames=4)\nmax_observation_v0(env, memory)\nnormalize_obs_v0(env, env_min=0, env_max=1)\nreshape_v0(env, shape)\nresize_v1(env, x_size, y_size, linear_interp=False)\nnan_noop_v0(env)\nnan_zeros_v0(env)\nnan_random_v0(env)\nscale_actions_v0(env, scale)\n```\n\n----------------------------------------\n\nTITLE: Tic-Tac-Toe Game Implementation\nDESCRIPTION: Implementation of Tic-Tac-Toe game using ActionMaskAgent with move validation\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/langchain_example.py\n   :pyobject: tic_tac_toe\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for CleanRL PPO Training\nDESCRIPTION: List of required Python packages needed to run the PPO training implementation with CleanRL and PettingZoo.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/advanced_PPO.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/CleanRL/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Importing and Testing Custom PettingZoo Environment in Python\nDESCRIPTION: This code snippet demonstrates how to import a custom PettingZoo environment and run various tests on it. It includes checks for API conformity, rendering, and parallel API compatibility. The code assumes the custom environment is in the same directory and uses relative importing.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/4-testing-your-environment.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import api_test, parallel_api_test, seed_test, render_test\nfrom .custom_environment import env, parallel_env\n\nenv = env()\nparallel_env = parallel_env()\n\napi_test(env, num_cycles=1000, verbose_progress=False)\nparallel_api_test(parallel_env, num_cycles=1000, verbose_progress=False)\n\nseed_test(env, num_cycles=10, test_kept_state=True)\n\nrender_test(env)\n\nprint(\"All tests passed!\")\n```\n\n----------------------------------------\n\nTITLE: Note on PPO Implementation with Parameter Sharing\nDESCRIPTION: A note explaining the use of PPO with parameter sharing in the tutorials, allowing a single model to control all agents in an environment. It provides an example of how this works in a two-player game using a vectorized environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/index.md#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. note::\n\n    These tutorials utilize PPO with parameter sharing, allowing a single model to control all the agents in an environment.\n\n    For more information on PPO implementation details and multi-agent environments, see https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n\n        For example, if there is a two-player game, we can create a vectorized environment that spawns two sub-environments. Then, the vectorized environment produces a batch of two observations, where the first observation is from player 1 and the second observation is from player 2. Next, the vectorized environment takes a batch of two actions and tells the game engine to let player 1 execute the first action and player 2 execute the second action. Consequently, PPO learns to control both player 1 and player 2 in this vectorized environment.\n```\n\n----------------------------------------\n\nTITLE: Installing MPE Dependencies with pip\nDESCRIPTION: Command to install the unique dependencies required for MPE environments using pip.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/mpe.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pettingzoo[mpe]'\n```\n\n----------------------------------------\n\nTITLE: Running Parallel API Test for PettingZoo Environments in Python\nDESCRIPTION: Tests a parallel environment's compliance with the PettingZoo parallel API by running it for a specified number of cycles.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import parallel_api_test\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.parallel_env()\nparallel_api_test(env, num_cycles=1000)\n```\n\n----------------------------------------\n\nTITLE: Running Render Test for PettingZoo Environments in Python\nDESCRIPTION: Verifies that an environment's rendering function works correctly with standard modes ('human', 'ansi', 'rgb_array') and produces output of the expected type.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_tests.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.test import render_test\nfrom pettingzoo.butterfly import pistonball_v6\nenv_func = pistonball_v6.env\nrender_test(env_func)\n```\n\n----------------------------------------\n\nTITLE: Installing SISL Environment Dependencies\nDESCRIPTION: Command to install the required dependencies for SISL environments using pip. This will install all necessary packages to run the Multiwalker, Pursuit, and Waterworld environments.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/sisl.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pettingzoo[sisl]'\n```\n\n----------------------------------------\n\nTITLE: Padding Observations in Python\nDESCRIPTION: Pads observations with zeros to match the largest observation size across agents. Supports Discrete and Box observation spaces. Enables MARL methods requiring homogeneous observations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npad_observations_v0(env)\n```\n\n----------------------------------------\n\nTITLE: Applying Wrappers to PettingZoo Environments in Python\nDESCRIPTION: Example of applying a wrapper (ClipOutOfBoundsWrapper) to a PettingZoo environment. Demonstrates how to transform or validate environments using wrappers.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\nfrom pettingzoo.utils import ClipOutOfBoundsWrapper\n\nenv = pistonball_v6.env()\nwrapped_env = ClipOutOfBoundsWrapper(env)\n# Wrapped environments must be reset before use\nwrapped_env.reset()\n```\n\n----------------------------------------\n\nTITLE: Installing SuperSuit for Atari Environment Preprocessing\nDESCRIPTION: Command for installing the SuperSuit library, which provides preprocessing wrappers for PettingZoo environments including those needed for Atari games.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/atari.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install supersuit\n```\n\n----------------------------------------\n\nTITLE: Documenting AECEnv Methods with Sphinx RST\nDESCRIPTION: Auto-documentation directives for the core AECEnv methods using Sphinx's automethod directive. Documents the step, reset, observe, render and close methods that form the primary interface for agent-environment interaction.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/aec.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. automethod:: AECEnv.step\n.. automethod:: AECEnv.reset\n.. automethod:: AECEnv.observe\n.. automethod:: AECEnv.render\n.. automethod:: AECEnv.close\n```\n\n----------------------------------------\n\nTITLE: Reward Processing for Connect Four\nDESCRIPTION: Processes and returns rewards based on game state and lesson criteria. Includes special rewards for vertical wins and three-in-a-row situations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef reward(self, done, player):\n    \"\"\"Processes and returns reward from environment according to lesson criteria.\n\n    :param done: Environment has terminated\n    :type done: bool\n    :param player: Player who we are checking, 0 or 1\n    :type player: int\n    \"\"\"\n    if done:\n        reward = (\n              self.lesson[\"rewards\"][\"vertical_win\"]\n              if self.check_vertical_win(player)\n              else self.lesson[\"rewards\"][\"win\"]\n        )\n    else:\n        agent_three_count = self.check_three_in_row(1 - player)\n        opp_three_count = self.check_three_in_row(player)\n        if (agent_three_count + opp_three_count) == 0:\n              reward = self.lesson[\"rewards\"][\"play_continues\"]\n        else:\n              reward = (\n                 self.lesson[\"rewards\"][\"three_in_row\"] * agent_three_count\n                 + self.lesson[\"rewards\"][\"opp_three_in_row\"] * opp_three_count\n              )\n    return reward\n```\n\n----------------------------------------\n\nTITLE: Installing Atari Environment Dependencies in PettingZoo\nDESCRIPTION: Command for installing the necessary dependencies for using Atari environments in PettingZoo via pip.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/atari.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pettingzoo[atari]'\n```\n\n----------------------------------------\n\nTITLE: Initializing PettingZoo Environment - Python\nDESCRIPTION: Basic initialization of a PettingZoo environment using the pistonball game from the butterfly family.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/basic_usage.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\n```\n\n----------------------------------------\n\nTITLE: Running Connect Four Environment with Random Agents\nDESCRIPTION: Python code demonstrating how to initialize and run a Connect Four environment with random agents using PettingZoo.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/classic.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.classic import connect_four_v3\n\nenv = connect_four_v3.env(render_mode=\"human\")\nenv.reset(seed=42)\n\nfor agent in env.agent_iter():\n    observation, reward, termination, truncation, info = env.last()\n\n    if termination or truncation:\n        action = None\n    else:\n        mask = observation[\"action_mask\"]\n        action = env.action_space(agent).sample(mask)  # this is where you would insert your policy\n\n    env.step(action)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Installing Butterfly Dependencies for PettingZoo\nDESCRIPTION: Command to install the unique dependencies for Butterfly environments in PettingZoo using pip.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/butterfly.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pettingzoo[butterfly]'\n```\n\n----------------------------------------\n\nTITLE: Initializing PettingZoo Environment in Python\nDESCRIPTION: Example showing how to initialize a PettingZoo environment using the pistonball environment from the butterfly family.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\nenv = pistonball_v6.env()\n```\n\n----------------------------------------\n\nTITLE: Testing SB3 Action Masking on PettingZoo Classic Environments\nDESCRIPTION: A pytest script for testing SB3 action masking on various PettingZoo Classic environments. It yields good results on simpler environments like Connect Four, while more complex environments may require additional training and tuning.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/connect_four.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Code content omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for MATD3 Implementation\nDESCRIPTION: Requirements file listing the necessary dependencies for implementing MATD3 using AgileRL and PettingZoo.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MATD3.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/AgileRL/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Configuring PettingZoo Environment - Python\nDESCRIPTION: Example of initializing a PettingZoo environment with custom configuration parameters for the cooperative_pong game.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/basic_usage.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import cooperative_pong_v5\n\ncooperative_pong_v5.env(ball_speed=18, left_paddle_speed=25,\nright_paddle_speed=25, cake_paddle=True, max_cycles=900, bounce_randomness=False)\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Ray RLlib with PettingZoo Environments\nDESCRIPTION: Requirements file listing the necessary Python packages and their version constraints for running Ray RLlib with PettingZoo environments. This includes the PettingZoo library for multi-agent environments, Ray with RLlib for reinforcement learning, SuperSuit for environment wrappers, and deep learning frameworks like PyTorch and TensorFlow Probability.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/Ray/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPettingZoo[classic,butterfly]>=1.24.0\nPillow>=9.4.0\nray[rllib]==2.7.0\nSuperSuit>=3.9.0\ntorch>=1.13.1\ntensorflow-probability>=0.19.0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AgileRL MADDPG Tutorial\nDESCRIPTION: This code snippet lists the required dependencies for the AgileRL MADDPG tutorial. It includes packages like AgileRL, PettingZoo, and Gymnasium, which are necessary for implementing and running the MADDPG algorithm on the Space Invaders environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/MADDPG.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nagilerl==0.1.1\npettingzoo==1.23.1\ngymnasium==0.28.1\nautorom[accept-rom-license]==0.6.1\nautorom-accept-rom-license==0.6.1\nmatplotlib==3.7.1\nnumpy==1.23.5\ntorch==2.0.1\npillow==9.5.0\n```\n\n----------------------------------------\n\nTITLE: Metrics Logging and Model Saving in Python\nDESCRIPTION: Code for logging training metrics to WandB and saving the elite agent's model weights, including action histograms and performance statistics.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif wb:\n    wandb_dict = {\n        \"global_step\": total_steps,\n        \"train/mean_score\": np.mean(agent.scores[-episodes_per_epoch:]),\n        \"train/mean_turns_per_game\": mean_turns,\n        \"train/epsilon\": epsilon,\n        \"train/opponent_updates\": opp_update_counter,\n        \"eval/mean_fitness\": np.mean(fitnesses),\n        \"eval/best_fitness\": np.max(fitnesses),\n        \"eval/mean_turns_per_game\": eval_turns\n    }\n    wandb_dict.update(train_actions_dict)\n    wandb_dict.update(eval_actions_dict)\n    wandb.log(wandb_dict)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for PettingZoo SISL Environments\nDESCRIPTION: This code snippet lists the required Python packages and their minimum versions for running PettingZoo SISL environments. It includes PettingZoo with SISL environments, Stable Baselines3 for reinforcement learning algorithms, SuperSuit for environment wrappers, and pymunk for physics simulations.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/SB3/waterworld/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npettingzoo[sisl]>=1.24.0\nstable-baselines3>=2.0.0\nsupersuit>=3.9.0\npymunk\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PettingZoo with RLlib\nDESCRIPTION: Lists the required packages needed to run the RLlib PettingZoo Pistonball tutorial. Includes ray, pettingzoo, pettingzoo[butterfly], SuperSuit, and other dependencies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/pistonball.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nray[rllib]==2.0.0\ntorch\npettingzoo==1.24.2\npettingzoo[butterfly]\nsupersuit==3.9.0\n```\n\n----------------------------------------\n\nTITLE: Unwrapping PettingZoo Environment - Python\nDESCRIPTION: Example showing how to unwrap a PettingZoo environment to access the base environment underneath all wrapper layers.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/basic_usage.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import knights_archers_zombies_v10\n\nbase_env = knights_archers_zombies_v10.env().unwrapped\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements for PettingZoo Classic\nDESCRIPTION: Defines the minimum required versions for PettingZoo classic environments, Stable Baselines 3 reinforcement learning library, its contrib package, and pytest for testing.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/SB3/test/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\npettingzoo[classic]>=1.24.0\nstable-baselines3>=2.0.0\nsb3-contrib>=2.0.0\npytest\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RLlib DQN Poker Tutorial\nDESCRIPTION: Required dependencies for setting up the environment to train DQN agents on Leduc Hold'em using Ray RLlib and PettingZoo.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/holdem.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/Ray/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Configuration\nDESCRIPTION: Specifies required Python packages and their minimum versions. Includes PettingZoo butterfly environments (>=1.24.0), Stable Baselines 3 (>=2.0.0), and SuperSuit (>=3.9.0) for environment preprocessing.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/SB3/kaz/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npettingzoo[butterfly]>=1.24.0\nstable-baselines3>=2.0.0\nsupersuit>=3.9.0\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for PettingZoo Training Setup\nDESCRIPTION: Package dependency specifications for setting up PettingZoo environments with Stable-Baselines3 and SuperSuit. Requires PettingZoo butterfly environments version 1.24.0 or higher, Stable-Baselines3 version 2.0.0 or higher, and SuperSuit version 3.9.0 or higher.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/SB3/pistonball/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npettingzoo[butterfly]>=1.24.0\nstable-baselines3>=2.0.0\nsupersuit>=3.9.0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Required package dependencies that need to be installed to run the Tianshou training example\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/advanced.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/Tianshou/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Tianshou with PettingZoo\nDESCRIPTION: A requirements file listing all necessary dependencies for running the Tianshou training example with PettingZoo. This includes tianshou, pettingzoo, and related packages.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/intermediate.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntianshou>=0.4.11\npettingzoo>=1.22.0\ntorch>=1.13.1\nfastapi>=0.95.0\nuvicorn>=0.21.1\nnumpy>=1.24.2\nsupersuit>=3.8.0\nscikit-learn>=1.2.2\ntorchvision>=0.14.1\nmatplotlib>=3.7.1\ngympy>=1.0.0,<2.0.0\ngym==0.26.0\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for PettingZoo\nDESCRIPTION: This snippet defines the minimum required versions for numpy and pettingzoo packages. It ensures compatibility and proper functionality of the PettingZoo project.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/CustomEnvironment/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nnumpy>=1.21.0\npettingzoo>=1.24.0\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for DQN Implementation\nDESCRIPTION: Import statements for required packages including AgileRL components, PettingZoo environment, and utility libraries needed for DQN implementation with curriculum learning and self-play.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport copy\nimport os\nimport random\nfrom collections import deque\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport wandb\nimport yaml\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population, observation_space_channels_to_first\nfrom agilerl.algorithms.core.wrappers import OptimizerWrapper\nfrom tqdm import tqdm, trange\n\nfrom pettingzoo.classic import connect_four_v3\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for PettingZoo Classic and Tianshou\nDESCRIPTION: This snippet defines the required Python packages and their versions for a project using PettingZoo classic environments and the Tianshou reinforcement learning library. It ensures compatibility and proper functionality of the project components.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/Tianshou/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy<2.0.0\npettingzoo[classic]>=1.23.0\npackaging>=21.3\ntianshou==0.5.0\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for PettingZoo\nDESCRIPTION: This snippet lists the required Python packages and their versions for the PettingZoo project. It includes libraries for reinforcement learning, game environments, and various utility packages. Some dependencies have specific version requirements or Python version constraints.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/AgileRL/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nagilerl==2.2.1; python_version >= '3.10' and python_version < '3.12'\npettingzoo[classic,atari,mpe]>=1.23.1\nAutoROM>=0.6.1\nSuperSuit>=3.9.0\ntorch>=2.0.1\nnumpy>=1.24.2\ntqdm>=4.65.0\nfastrand==1.3.0\ngymnasium>=0.28.1\nimageio>=2.31.1\nPillow>=9.5.0\nPyYAML>=5.4.1\n```\n\n----------------------------------------\n\nTITLE: Environment Dependencies Installation Requirements\nDESCRIPTION: List of required dependencies to run the LangChain PettingZoo integration\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/langchain.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/LangChain/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Tianshou and PettingZoo Integration\nDESCRIPTION: List of required dependencies needed to follow the Tianshou and PettingZoo integration tutorial. These should be installed in a fresh virtual environment to avoid conflicts.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/beginner.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntianshou\npettingzoo[classic]\nnumpy\ntorch\npython-magic-bin; sys_platform == \"win32\"\npython-magic; sys_platform != \"win32\"\n```\n\n----------------------------------------\n\nTITLE: Environment Dependencies Configuration\nDESCRIPTION: Lists the required package dependencies for setting up the Waterworld environment and running the PPO training script.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/waterworld.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nsupersuit>=3.7.1\nstable-baselines3>=2.0.0\npettingzoo[sisl]>=1.22.0\n```\n\n----------------------------------------\n\nTITLE: Installing PettingZoo with all dependencies\nDESCRIPTION: Command to install PettingZoo with all optional dependencies, including those for all environments and renderers.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/5-using-your-environment.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pettingzoo[all]\"\n```\n\n----------------------------------------\n\nTITLE: Including CleanRL Tutorial Example in Python\nDESCRIPTION: This code snippet demonstrates how to include a CleanRL tutorial example using reStructuredText directives within a Python environment. It references an external Python file located at '../../tutorials/CleanRL/cleanrl.py'.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/tutorials.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../tutorials/CleanRL/cleanrl.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Knights-Archers-Zombies Tutorial\nDESCRIPTION: This code snippet lists the required dependencies for running the Knights-Archers-Zombies tutorial with Stable Baselines 3 and PettingZoo.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/kaz.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npettingzoo==1.23.1\nsupersuit==3.9.0\nstable-baselines3==2.1.0\nbutterfly==1.0.1\nwandb==0.15.11\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for PettingZoo Documentation\nDESCRIPTION: Commands to install PettingZoo package in development mode and required dependencies for building documentation. Uses pip to install the local package and requirements listed in docs/requirements.txt.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\npip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Environment Pages in PettingZoo Documentation\nDESCRIPTION: Command to generate environment documentation pages by executing the gen_envs_mds.py script. This script extracts documentation from Python environment files and converts it to Markdown format.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\npython _scripts/gen_envs_mds.py\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for SB3 Connect Four Tutorial\nDESCRIPTION: A list of required dependencies for following the SB3 Connect Four tutorial. It is recommended to install these in a new virtual environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/connect_four.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npettingzoo[classic]\nsb3-contrib\nstable-baselines3[extra]\nshimmy[all]\ngym==0.23.1\nmatplotlib\n```\n\n----------------------------------------\n\nTITLE: Building PettingZoo Documentation\nDESCRIPTION: Command to build the documentation once using Sphinx's make system. The dirhtml builder produces HTML files with directory structure.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake dirhtml\n```\n\n----------------------------------------\n\nTITLE: Auto-rebuilding PettingZoo Documentation\nDESCRIPTION: Command to continuously rebuild the documentation whenever changes are detected using sphinx-autobuild. This automatically updates the documentation in real-time while editing.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nsphinx-autobuild -b dirhtml . _build\n```\n\n----------------------------------------\n\nTITLE: Displaying RLlib Architecture Diagram in Markdown\nDESCRIPTION: This code snippet uses Markdown syntax to display an image of the RLlib architecture stack. It includes a figure directive with a URL, alt text, and width specification.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{figure} https://docs.ray.io/en/latest/_images/rllib-stack.svg\n    :alt: RLlib stack\n    :width: 80%\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for PettingZoo\nDESCRIPTION: A list of Python packages required for building the PettingZoo documentation. This includes Sphinx as the main documentation generator, sphinx-autobuild for live rebuilding, myst-parser for Markdown support, a custom Furo theme from GitHub, and sphinx_github_changelog for changelog integration.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx\nsphinx-autobuild\nmyst-parser\ngit+https://github.com/Farama-Foundation/Celshast#egg=furo\nsphinx_github_changelog\n```\n\n----------------------------------------\n\nTITLE: Defining PettingZoo Custom Environment Skeleton in Python\nDESCRIPTION: This code snippet provides a skeleton structure for creating a custom PettingZoo environment. It includes imports, class definition, and placeholder methods that need to be implemented for a functional environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/1-project-structure.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nfrom gymnasium.spaces import Discrete\nfrom pettingzoo import ParallelEnv\n\n\ndef env(render_mode=None):\n    \"\"\"\n    The env function often wraps the environment in wrappers by default.\n    You can find full documentation for these methods\n    elsewhere in the developer documentation.\n    \"\"\"\n    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n    env = raw_env(render_mode=internal_render_mode)\n    # This wrapper is only for environments which print results to the terminal\n    if render_mode == \"ansi\":\n        env = wrappers.CaptureStdoutWrapper(env)\n    # this wrapper helps error handling for discrete action spaces\n    env = wrappers.AssertOutOfBoundsWrapper(env)\n    # Provides a wide vareity of helpful user errors\n    # Strongly recommended\n    env = wrappers.OrderEnforcingWrapper(env)\n    return env\n\n\nclass raw_env(ParallelEnv):\n    metadata = {\n        \"render_modes\": [\"human\"],\n        \"name\": \"rps_v2\",\n    }\n\n    def __init__(self, render_mode=None):\n        \"\"\"\n        The init method takes in environment arguments and\n         should define the following attributes:\n        - possible_agents\n        - action_spaces\n        - observation_spaces\n\n        These attributes should not be changed after initialization.\n        \"\"\"\n        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n        self.agent_name_mapping = dict(\n            zip(self.possible_agents, list(range(len(self.possible_agents))))\n        )\n\n        # Gym spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n        self._observation_spaces = {agent: Discrete(4) for agent in self.possible_agents}\n        self.render_mode = render_mode\n\n    # this cache ensures that same space object is returned for the same agent\n    # allows action space seeding to work as expected\n    @functools.lru_cache(maxsize=None)\n    def observation_space(self, agent):\n        # Gym spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n        return Discrete(4)\n\n    @functools.lru_cache(maxsize=None)\n    def action_space(self, agent):\n        return Discrete(3)\n\n    def render(self):\n        \"\"\"\n        Renders the environment. In human mode, it can print to terminal, open\n        up a graphical window, or open up some other display that a human can see and understand.\n        \"\"\"\n        if self.render_mode is None:\n            gymnasium.logger.warn(\n                \"You are calling render method without specifying any render mode.\"\n            )\n            return\n\n        # ... [actual rendering logic here] ...\n\n    def close(self):\n        \"\"\"\n        Close should release any graphical displays, subprocesses, network connections\n        or any other environment data which should not be kept around after the\n        user is no longer using the environment.\n        \"\"\"\n        pass\n\n    def reset(self, seed=None, options=None):\n        \"\"\"\n        Reset needs to initialize the following attributes\n        - agents\n        - rewards\n        - _cumulative_rewards\n        - terminations\n        - truncations\n        - infos\n        - agent_selection\n        And must set up the environment so that render(), step(), and observe()\n        can be called without issues.\n\n        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by observe()\n\n        Returns the observations for each agent\n        \"\"\"\n        # ....\n\n    def step(self, actions):\n        \"\"\"\n        step(action) takes in an action for each agent and should return the\n        - observations\n        - rewards\n        - terminations\n        - truncations\n        - infos\n        dicts where each dict looks like {agent_1: item_1, agent_2: item_2}\n        \"\"\"\n        # ... [actual logic here] ...\n\n        return observations, rewards, terminations, truncations, infos\n```\n\n----------------------------------------\n\nTITLE: Installing PettingZoo via pip\nDESCRIPTION: Command to install PettingZoo using pip package manager. This installs the base version without any additional dependencies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/5-using-your-environment.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pettingzoo\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for CleanRL PPO Implementation\nDESCRIPTION: Required package dependencies that need to be installed in a virtual environment before running the PPO implementation.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/implementing_PPO.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.. literalinclude:: ../../../tutorials/CleanRL/requirements.txt\n   :language: text\n```\n\n----------------------------------------\n\nTITLE: Using DeprecatedModule for Version Management in Python\nDESCRIPTION: Example of using the DeprecatedModule class to manage versioning and guide users to newer versions of PettingZoo environments.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/content/environment_creation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.utils.deprecated_module import DeprecatedModule\nknights_archers_zombies_v0 = DeprecatedModule(\"knights_archers_zombies\", \"v0\", \"v10\")\n```\n\n----------------------------------------\n\nTITLE: Running PettingZoo Tests with Pytest in Python\nDESCRIPTION: Command to run the PettingZoo test suite using pytest. This ensures that existing tests pass before submitting new code.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v\n```\n\n----------------------------------------\n\nTITLE: Creating Hidden Table of Contents in RST\nDESCRIPTION: This RST code snippet creates a hidden table of contents for the AgileRL section, including links to DQN, MADDPG, and MATD3 pages.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/index.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n:hidden:\n:caption: AgileRL\n\nDQN\nMADDPG\nMATD3\n```\n\n----------------------------------------\n\nTITLE: Warning Message for Stable-Baselines3 Usage with PettingZoo\nDESCRIPTION: A warning message indicating that Stable-Baselines3 is designed for single-agent RL and not directly supporting multi-agent algorithms or environments. It clarifies that the tutorials are for demonstration purposes only.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/index.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. warning::\n\n    Note: SB3 is designed for single-agent RL and does not directly support multi-agent algorithms or environments. These tutorials are only intended for demonstration purposes, to show how SB3 can be adapted to work with PettingZoo.\n```\n\n----------------------------------------\n\nTITLE: Installing SWIG on macOS for PettingZoo Dependencies\nDESCRIPTION: Command to install SWIG on macOS systems using Homebrew, which may be required for some PettingZoo dependencies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbrew install swig\n```\n\n----------------------------------------\n\nTITLE: Connect Four Requirements Installation\nDESCRIPTION: Text file containing the required dependencies for running the Connect Four DQN implementation. These should be installed in a new virtual environment.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/DQN.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nrequirements.txt\n```\n\n----------------------------------------\n\nTITLE: Timeline Page Layout in Jekyll\nDESCRIPTION: Jekyll frontmatter and template for rendering a timeline page layout with an included timeline component.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/envs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: timeline_layout\ntitle: \"Environments\"\n---\n\n{% include timeline.html %}\n```\n\n----------------------------------------\n\nTITLE: Creating Hidden Table of Contents for CleanRL Tutorial\nDESCRIPTION: This code snippet defines a hidden table of contents for the CleanRL tutorial, including links to implementing PPO and advanced PPO sections.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n:caption: CleanRL\n\nimplementing_PPO\nadvanced_PPO\n```\n```\n\n----------------------------------------\n\nTITLE: Displaying Tianshou Architecture Diagram\nDESCRIPTION: This code snippet uses a figure directive to display an image of Tianshou's architecture. The image is located in the '_static/img/tutorials/' directory and is set to display at full width.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/tianshou/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{figure} /_static/img/tutorials/tianshou.png\n    :alt: Tianshou Architecture\n    :width: 100%\n```\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating PPO Agents for Knights-Archers-Zombies\nDESCRIPTION: This Python script sets up the Knights-Archers-Zombies environment, creates PPO agents using Stable Baselines 3, trains them, and evaluates their performance. It includes environment preprocessing, model creation, training loop, and visualization of results.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/sb3/kaz.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport supersuit as ss\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import CnnPolicy\nfrom pettingzoo.butterfly import knights_archers_zombies_v10\nimport wandb\nfrom wandb.integration.sb3 import WandbCallback\n\n# Set up template for tracking with Weights & Biases\nconfig = {\n    \"policy_type\": \"CnnPolicy\",\n    \"total_timesteps\": 2000000,\n    \"env_name\": \"knights_archers_zombies_v10\",\n}\nrun = wandb.init(\n    project=\"sb3\",\n    config=config,\n    sync_tensorboard=True,\n    save_code=True,\n)\n\nenv_kwargs = dict(vector_state=False, max_zombies=10)\nenv = knights_archers_zombies_v10.env(**env_kwargs)\nenv = ss.color_reduction_v0(env, mode=\"B\")\nenv = ss.resize_v1(env, x_size=84, y_size=84)\nenv = ss.frame_stack_v1(env, 3)\nenv = ss.pettingzoo_env_to_vec_env_v1(env)\nenv = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class=\"stable_baselines3\")\n\nmodel = PPO(\n    CnnPolicy,\n    env,\n    verbose=3,\n    tensorboard_log=f\"runs/{run.id}\",\n    learning_rate=1e-3,\n    batch_size=256,\n)\nmodel.learn(\n    total_timesteps=2000000,\n    callback=WandbCallback(\n        gradient_save_freq=100,\n        model_save_path=f\"models/{run.id}\",\n        verbose=2,\n    ),\n)\nmodel.save(f\"final_model_{run.id}\")\n\nrun.finish()\n```\n\n----------------------------------------\n\nTITLE: Creating Hidden Table of Contents in Markdown\nDESCRIPTION: This code snippet uses Markdown syntax to create a hidden table of contents with a caption. It includes two specific pages: 'pistonball' and 'holdem'.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/rllib/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n:caption: RLlib\n\npistonball\nholdem\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Sphinx Documentation\nDESCRIPTION: This code snippet defines a hidden table of contents for the Sphinx documentation system. It includes links to the four main sections of the custom environment tutorial.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/custom_environment/index.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n```{toctree}\n:hidden:\n:caption: Environment Creation\n\n1-project-structure\n2-environment-logic\n3-action-masking\n4-testing-your-environment\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Classic Environment Dependencies\nDESCRIPTION: Command to install dependencies for classic environments in PettingZoo using pip.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/environments/classic.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pettingzoo[classic]'\n```\n\n----------------------------------------\n\nTITLE: Displaying CleanRL Integration with Weights & Biases\nDESCRIPTION: This code snippet shows how to include a figure in the documentation, demonstrating CleanRL's integration with Weights & Biases for experiment tracking and visualization.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/cleanrl/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{figure} /_static/img/tutorials/cleanrl-wandb.png\n    :alt: CleanRl integration with Weights & Biases\n    :width: 80%\n```\n```\n\n----------------------------------------\n\nTITLE: Citing SuperSuit Framework in BibTeX Format\nDESCRIPTION: BibTeX citation entry for the SuperSuit paper, which describes simple microwrappers for reinforcement learning environments. Published as an arXiv preprint in 2020.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers/supersuit_wrappers.md#2025-04-22_snippet_6\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{SuperSuit,\n  Title = {SuperSuit: Simple Microwrappers for Reinforcement Learning Environments},\n  Author = {Terry, J K and Black, Benjamin and Hari, Ananth},\n  journal={arXiv preprint arXiv:2008.08932},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing PettingZoo Documentation Examples\nDESCRIPTION: Command to test documentation examples using pytest-markdown-docs plugin. This ensures that code examples in the documentation run successfully.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest docs --markdown-docs -m markdown-docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Changelog Display with Restructured Text for PettingZoo\nDESCRIPTION: This restructured text directive configures how to display the changelog for the PettingZoo project. It specifies GitHub releases and PyPI as data sources for generating the release notes documentation.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/release_notes/index.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. changelog::\n    :github: https://github.com/Farama-Foundation/PettingZoo/releases\n    :pypi: https://pypi.org/project/pettingzoo/\n    :changelog-url:\n```\n\n----------------------------------------\n\nTITLE: Including Related Wrapper Documentation in Toctree\nDESCRIPTION: A toctree directive that hides related wrapper documentation pages but includes them in the navigation structure. This makes the related wrapper documentation accessible through the navigation menu without displaying them directly on the current page.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/api/wrappers.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\nwrappers/pz_wrappers\nwrappers/supersuit_wrappers\nwrappers/shimmy_wrappers\n```\n```\n\n----------------------------------------\n\nTITLE: Installing AutoROM for PettingZoo Testing\nDESCRIPTION: Command to install AutoROM, which is required for running some PettingZoo tests.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nAutoROM -v\n```\n\n----------------------------------------\n\nTITLE: Including LangChain Tutorial in Table of Contents with Sphinx\nDESCRIPTION: A Sphinx toctree directive that includes the LangChain tutorial in the documentation's table of contents. It's hidden by default and appears under the 'LangChain' caption.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/langchain/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n:caption: LangChain\n\nlangchain\n```\n```\n\n----------------------------------------\n\nTITLE: Installing SWIG on Linux for PettingZoo Dependencies\nDESCRIPTION: Command to install SWIG on Linux systems, which may be required for some PettingZoo dependencies.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napt-get install swig\n```\n\n----------------------------------------\n\nTITLE: Installing MAgent2 Package via pip\nDESCRIPTION: Instructions for installing the MAgent2 package using pip. This command will download and install the standalone MAgent2 package that has been separated from the PettingZoo library.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/pettingzoo/magent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install magent2\n```\n\n----------------------------------------\n\nTITLE: Installing PettingZoo with Testing Dependencies in Python\nDESCRIPTION: Command to install PettingZoo with testing dependencies using pip. This is required for running tests and contributing to the codebase.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[testing,all]\"\n```\n\n----------------------------------------\n\nTITLE: Specifying PettingZoo and Stable Baselines 3 Dependencies\nDESCRIPTION: Lists required Python packages with minimum version constraints for a project using PettingZoo classic environments with Stable Baselines 3 reinforcement learning library and its contributed extensions.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/tutorials/SB3/connect_four/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\npettingzoo[classic]>=1.24.0\nstable-baselines3>=2.0.0\nsb3-contrib>=2.0.0\n```\n\n----------------------------------------\n\nTITLE: Inserting Figure in RST Documentation\nDESCRIPTION: This RST code snippet inserts a figure into the documentation, showing the performance of a trained MADDPG algorithm on 6 random episodes.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/index.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. figure:: test_looped.gif\n   :align: center\n   :height: 400px\n\n   Fig1: Performance of trained MADDPG algorithm on 6 random episodes\n```\n\n----------------------------------------\n\nTITLE: Warning Message for Python Version Compatibility in RST\nDESCRIPTION: This RST code snippet displays a warning message indicating that AgileRL only supports Python versions less than 3.12.\nSOURCE: https://github.com/farama-foundation/pettingzoo/blob/master/docs/tutorials/agilerl/index.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. warning::\n\n   AgileRL only supports versions of python <3.12.\n```"
  }
]
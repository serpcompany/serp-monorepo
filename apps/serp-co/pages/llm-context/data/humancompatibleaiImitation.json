[
  {
    "owner": "humancompatibleai",
    "repo": "imitation",
    "content": "TITLE: Implementing GAIL with imitation Library in Python\nDESCRIPTION: A complete example of implementing Generative Adversarial Imitation Learning (GAIL) using the imitation library and Stable Baselines3. The code demonstrates loading an expert policy, generating demonstrations, creating a GAIL trainer with a PPO algorithm as the generator, and evaluating the policy before and after training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/gail.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.ppo import MlpPolicy\n\nfrom imitation.algorithms.adversarial.gail import GAIL\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.policies.serialize import load_policy\nfrom imitation.rewards.reward_nets import BasicRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom imitation.util.util import make_vec_env\n\nSEED = 42\n\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(SEED),\n    n_envs=8,\n    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # to compute rollouts\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals-CartPole-v0\",\n    venv=env,\n)\n\nrollouts = rollout.rollout(\n    expert,\n    env,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n    rng=np.random.default_rng(SEED),\n)\n\nlearner = PPO(\n    env=env,\n    policy=MlpPolicy,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0004,\n    gamma=0.95,\n    n_epochs=5,\n    seed=SEED,\n)\nreward_net = BasicRewardNet(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    normalize_input_layer=RunningNorm,\n)\ngail_trainer = GAIL(\n    demonstrations=rollouts,\n    demo_batch_size=1024,\n    gen_replay_buffer_capacity=512,\n    n_disc_updates_per_round=8,\n    venv=env,\n    gen_algo=learner,\n    reward_net=reward_net,\n)\n\n# evaluate the learner before training\nenv.seed(SEED)\nlearner_rewards_before_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True,\n)\n\n# train the learner and evaluate again\ngail_trainer.train(20000)  # Train for 800_000 steps to match expert.\nenv.seed(SEED)\nlearner_rewards_after_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True,\n)\n\nprint(\"mean reward after training:\", np.mean(learner_rewards_after_training))\nprint(\"mean reward before training:\", np.mean(learner_rewards_before_training))\n```\n\n----------------------------------------\n\nTITLE: Training and Testing Imitation Algorithms via CLI\nDESCRIPTION: This bash script demonstrates how to use the Imitation library's CLI to train an expert RL algorithm, save demonstrations, and then train imitation learning algorithms (BC, GAIL, AIRL) using those demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/first_steps.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Train PPO agent on cartpole\npython -m imitation.scripts.train_rl with fast environment=cartpole \\\n  logger=tensorboard log_dir=quickstart/rl\n\n# Generate expert demonstrations\npython -m imitation.scripts.expert_demos with fast \\\n  environment=cartpole save_path=quickstart/demos.npz\n\n# Train BC, GAIL, and AIRL on expert demonstrations\npython -m imitation.scripts.train_imitation bc with fast \\\n  environment=cartpole rollout_path=quickstart/demos.npz \\\n  log_dir=quickstart/bc\n\npython -m imitation.scripts.train_adversarial gail with fast \\\n  environment=cartpole rollout_path=quickstart/demos.npz \\\n  log_dir=quickstart/gail\n\npython -m imitation.scripts.train_adversarial airl with fast \\\n  environment=cartpole rollout_path=quickstart/demos.npz \\\n  log_dir=quickstart/airl\n```\n\n----------------------------------------\n\nTITLE: Training a Behavioral Cloning Agent with Expert Demonstrations in Python\nDESCRIPTION: This code demonstrates how to train a Behavioral Cloning (BC) agent using expert demonstrations in the CartPole environment. It loads a pre-trained expert policy, collects demonstrations, creates a BC trainer, trains the model, and evaluates the resulting policy's performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/bc.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nfrom imitation.algorithms import bc\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\n\nrng = np.random.default_rng(0)\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=rng,\n    n_envs=1,\n    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals-CartPole-v0\",\n    venv=env,\n)\nrollouts = rollout.rollout(\n    expert,\n    env,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n    rng=rng,\n)\ntransitions = rollout.flatten_trajectories(rollouts)\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=transitions,\n    rng=rng,\n)\nbc_trainer.train(n_epochs=1)\nreward, _ = evaluate_policy(bc_trainer.policy, env, 10)\nprint(\"Reward:\", reward)\n```\n\n----------------------------------------\n\nTITLE: Implementing AIRL with Stable Baselines 3 on CartPole Environment\nDESCRIPTION: A complete example of training an AIRL agent on the CartPole environment. The code demonstrates the full workflow: setting up the environment, loading an expert policy, collecting demonstrations, initializing the learner policy and reward network, configuring the AIRL trainer, and evaluating the performance before and after training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/airl.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.ppo import MlpPolicy\n\nfrom imitation.algorithms.adversarial.airl import AIRL\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.policies.serialize import load_policy\nfrom imitation.rewards.reward_nets import BasicShapedRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom imitation.util.util import make_vec_env\n\nSEED = 42\n\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(SEED),\n    n_envs=8,\n    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # to compute rollouts\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals-CartPole-v0\",\n    venv=env,\n)\nrollouts = rollout.rollout(\n    expert,\n    env,\n    rollout.make_sample_until(min_episodes=60),\n    rng=np.random.default_rng(SEED),\n)\n\nlearner = PPO(\n    env=env,\n    policy=MlpPolicy,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0005,\n    gamma=0.95,\n    clip_range=0.1,\n    vf_coef=0.1,\n    n_epochs=5,\n    seed=SEED,\n)\nreward_net = BasicShapedRewardNet(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    normalize_input_layer=RunningNorm,\n)\nairl_trainer = AIRL(\n    demonstrations=rollouts,\n    demo_batch_size=2048,\n    gen_replay_buffer_capacity=512,\n    n_disc_updates_per_round=16,\n    venv=env,\n    gen_algo=learner,\n    reward_net=reward_net,\n)\n\nenv.seed(SEED)\nlearner_rewards_before_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True,\n)\nairl_trainer.train(20000)  # Train for 2_000_000 steps to match expert.\nenv.seed(SEED)\nlearner_rewards_after_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True,\n)\n\nprint(\"mean reward after training:\", np.mean(learner_rewards_after_training))\nprint(\"mean reward before training:\", np.mean(learner_rewards_before_training))\n```\n\n----------------------------------------\n\nTITLE: Implementing DAgger with the Imitation Library in Python\nDESCRIPTION: This code example demonstrates how to use DAgger (Dataset Aggregation) algorithm with the imitation library. It creates a CartPole environment, loads a pre-trained expert policy, initializes a BC (Behavioral Cloning) trainer, sets up the DAgger trainer, trains the policy, and evaluates its performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/dagger.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nimport numpy as np\nimport gymnasium as gym\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nfrom imitation.algorithms import bc\nfrom imitation.algorithms.dagger import SimpleDAggerTrainer\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\n\nrng = np.random.default_rng(0)\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=rng,\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals-CartPole-v0\",\n    venv=env,\n)\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    rng=rng,\n)\nwith tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n    print(tmpdir)\n    dagger_trainer = SimpleDAggerTrainer(\n        venv=env,\n        scratch_dir=tmpdir,\n        expert_policy=expert,\n        bc_trainer=bc_trainer,\n        rng=rng,\n    )\n    dagger_trainer.train(8_000)\n\nreward, _ = evaluate_policy(dagger_trainer.policy, env, 10)\nprint(\"Reward:\", reward)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Environment with Learned Reward in Python\nDESCRIPTION: This code snippet shows how to wrap the environment with the learned reward function after training the reward network using preference comparisons.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n\nlearned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)\n```\n\n----------------------------------------\n\nTITLE: Implementing Density-Based Reward Modeling with the imitation Library in Python\nDESCRIPTION: This code demonstrates how to use the DensityAlgorithm class from the imitation library to implement density-based reward modeling. It initializes the environment, loads expert demonstrations, creates a PPO policy, and trains the model using Gaussian kernel density estimation on state-action pairs.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/density.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.policies import ActorCriticPolicy\n\nfrom imitation.algorithms import density as db\nfrom imitation.data import serialize\nfrom imitation.util import util\n\nrng = np.random.default_rng(0)\n\nenv = util.make_vec_env(\"Pendulum-v1\", rng=rng, n_envs=2)\nrollouts = serialize.load(\"../tests/testdata/expert_models/pendulum_0/rollouts/final.npz\")\n\nimitation_trainer = PPO(\n    ActorCriticPolicy, env, learning_rate=3e-4, gamma=0.95, ent_coef=1e-4, n_steps=2048\n)\ndensity_trainer = db.DensityAlgorithm(\n    venv=env,\n    rng=rng,\n    demonstrations=rollouts,\n    rl_algo=imitation_trainer,\n    density_type=db.DensityType.STATE_ACTION_DENSITY,\n    is_stationary=True,\n    kernel=\"gaussian\",\n    kernel_bandwidth=0.4,\n    standardise_inputs=True,\n)\ndensity_trainer.train()\n\ndef print_stats(density_trainer, n_trajectories):\n    stats = density_trainer.test_policy(n_trajectories=n_trajectories)\n    print(\"True reward function stats:\")\n    pprint.pprint(stats)\n    stats_im = density_trainer.test_policy(true_reward=False, n_trajectories=n_trajectories)\n    print(\"Imitation reward function stats:\")\n    pprint.pprint(stats_im)\n\nprint(\"Stats before training:\")\nprint_stats(density_trainer, 1)\n\ndensity_trainer.train_policy(100)  # Train for 1_000_000 steps to approach expert performance.\n\nprint(\"Stats after training:\")\nprint_stats(density_trainer, 1)\n```\n\n----------------------------------------\n\nTITLE: Training SQIL on CartPole-v1 using Expert Demonstrations\nDESCRIPTION: This code snippet demonstrates how to use the SQIL algorithm from the imitation library to train an agent on the CartPole-v1 environment using expert demonstrations. It loads expert trajectories from the HuggingFace Datasets Hub, initializes the SQIL trainer, trains the model, and evaluates its performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/sqil.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport datasets\nimport gymnasium as gym\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nfrom imitation.algorithms import sqil\nfrom imitation.data import huggingface_utils\n\n# Download some expert trajectories from the HuggingFace Datasets Hub.\ndataset = datasets.load_dataset(\"HumanCompatibleAI/ppo-CartPole-v1\")\nrollouts = huggingface_utils.TrajectoryDatasetSequence(dataset[\"train\"])\n\nsqil_trainer = sqil.SQIL(\n    venv=DummyVecEnv([lambda: gym.make(\"CartPole-v1\")]),\n    demonstrations=rollouts,\n    policy=\"MlpPolicy\",\n)\n# Hint: set to 1_000_000 to match the expert performance.\nsqil_trainer.train(total_timesteps=1_000)\nreward, _ = evaluate_policy(sqil_trainer.policy, sqil_trainer.venv, 10)\nprint(\"Reward:\", reward)\n```\n\n----------------------------------------\n\nTITLE: Training BC and DAgger for Algorithm Comparison\nDESCRIPTION: Sets up and trains both Behavior Cloning (BC) and DAgger algorithms on the limited dataset. DAgger uses the expert policy to generate additional data during training, which gives it an advantage in this setup.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms.dagger import SimpleDAggerTrainer\nimport tempfile\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=transitions,\n    rng=rng,\n)\n\nbc_trainer.train(n_epochs=1)\n\n\nwith tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n    print(tmpdir)\n    dagger_bc_trainer = bc.BC(\n        observation_space=env.observation_space,\n        action_space=env.action_space,\n        rng=np.random.default_rng(),\n    )\n    dagger_trainer = SimpleDAggerTrainer(\n        venv=DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n        scratch_dir=tmpdir,\n        expert_policy=expert,\n        bc_trainer=dagger_bc_trainer,\n        rng=np.random.default_rng(),\n    )\n\n    dagger_trainer.train(5000)\n```\n\n----------------------------------------\n\nTITLE: Implementing MCE IRL with CliffWorld Environment in Python\nDESCRIPTION: Demonstrates the implementation of Maximum Causal Entropy IRL using a CliffWorld environment. The code sets up the environment, computes occupancy measures, creates a reward network, and trains the IRL model to generate imitation trajectories.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/mce_irl.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nfrom seals import base_envs\nfrom seals.diagnostics.cliff_world import CliffWorldEnv\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nfrom imitation.algorithms.mce_irl import (\n    MCEIRL,\n    mce_occupancy_measures,\n    mce_partition_fh,\n)\nfrom imitation.data import rollout\nfrom imitation.rewards import reward_nets\n\nrng = np.random.default_rng(0)\n\nenv_creator = partial(CliffWorldEnv, height=4, horizon=8, width=7, use_xy_obs=True)\nenv_single = env_creator()\n\nstate_env_creator = lambda: base_envs.ExposePOMDPStateWrapper(env_creator())\n\n# This is just a vectorized environment because `generate_trajectories` expects one\nstate_venv = DummyVecEnv([state_env_creator] * 4)\n\n_, _, pi = mce_partition_fh(env_single)\n\n_, om = mce_occupancy_measures(env_single, pi=pi)\n\nreward_net = reward_nets.BasicRewardNet(\n    env_single.observation_space,\n    env_single.action_space,\n    hid_sizes=[256],\n    use_action=False,\n    use_done=False,\n    use_next_state=False,\n)\n\n# training on analytically computed occupancy measures\nmce_irl = MCEIRL(\n    om,\n    env_single,\n    reward_net,\n    log_interval=250,\n    optimizer_kwargs={\"lr\": 0.01},\n    rng=rng,\n)\nocc_measure = mce_irl.train()\n\nimitation_trajs = rollout.generate_trajectories(\n    policy=mce_irl.policy,\n    venv=state_venv,\n    sample_until=rollout.make_min_timesteps(5000),\n    rng=rng,\n)\nprint(\"Imitation stats: \", rollout.rollout_stats(imitation_trajs))\n```\n\n----------------------------------------\n\nTITLE: Collecting Expert Demonstrations in Python\nDESCRIPTION: Collects expert demonstration data by rolling out the expert policy for 50 episodes, then flattens the trajectories into transitions suitable for behavior cloning training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\nrng = np.random.default_rng()\nrollouts = rollout.rollout(\n    expert,\n    env,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n    rng=rng,\n)\ntransitions = rollout.flatten_trajectories(rollouts)\n```\n\n----------------------------------------\n\nTITLE: Configuring GAIL Trainer\nDESCRIPTION: Sets up the GAIL training components including the PPO learner, reward network (discriminator), and GAIL trainer with specific hyperparameters.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms.adversarial.gail import GAIL\nfrom imitation.rewards.reward_nets import BasicRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nlearner = PPO(\n    env=env,\n    policy=MlpPolicy,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0004,\n    gamma=0.95,\n    n_epochs=5,\n    seed=SEED,\n)\nreward_net = BasicRewardNet(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    normalize_input_layer=RunningNorm,\n)\ngail_trainer = GAIL(\n    demonstrations=rollouts,\n    demo_batch_size=1024,\n    gen_replay_buffer_capacity=512,\n    n_disc_updates_per_round=8,\n    venv=env,\n    gen_algo=learner,\n    reward_net=reward_net,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment and Models for Atari Preference Comparisons\nDESCRIPTION: This snippet sets up the Atari environment, defines the reward network, and initializes various components for preference comparisons training. It includes environment preprocessing, reward network definition, and configuration of preference comparison algorithms.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\nimport gymnasium as gym\nfrom gymnasium.wrappers import TimeLimit\nimport numpy as np\n\nfrom seals.util import AutoResetWrapper\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.atari_wrappers import AtariWrapper\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\nfrom stable_baselines3.ppo import CnnPolicy\n\nfrom imitation.algorithms import preference_comparisons\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.policies.base import NormalizeFeaturesExtractor\nfrom imitation.rewards.reward_nets import CnnRewardNet\n\n\ndevice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n\nrng = np.random.default_rng()\n\n\n# Here we ensure that our environment has constant-length episodes by resetting\n# it when done, and running until 100 timesteps have elapsed.\n# For real training, you will want a much longer time limit.\ndef constant_length_asteroids(num_steps):\n    atari_env = gym.make(\"AsteroidsNoFrameskip-v4\")\n    preprocessed_env = AtariWrapper(atari_env)\n    endless_env = AutoResetWrapper(preprocessed_env)\n    limited_env = TimeLimit(endless_env, max_episode_steps=num_steps)\n    return RolloutInfoWrapper(limited_env)\n\n\n# For real training, you will want a vectorized environment with 8 environments in parallel.\n# This can be done by passing in n_envs=8 as an argument to make_vec_env.\n# The seed needs to be set to 1 for reproducibility and also to avoid win32\n# np.random.randint high bound error.\nvenv = make_vec_env(constant_length_asteroids, env_kwargs={\"num_steps\": 100}, seed=1)\nvenv = VecFrameStack(venv, n_stack=4)\n\nreward_net = CnnRewardNet(\n    venv.observation_space,\n    venv.action_space,\n).to(device)\n\nfragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=rng)\ngatherer = preference_comparisons.SyntheticGatherer(rng=rng)\npreference_model = preference_comparisons.PreferenceModel(reward_net)\nreward_trainer = preference_comparisons.BasicRewardTrainer(\n    preference_model=preference_model,\n    loss=preference_comparisons.CrossEntropyRewardLoss(),\n    epochs=3,\n    rng=rng,\n)\n\nagent = PPO(\n    policy=CnnPolicy,\n    env=venv,\n    seed=0,\n    n_steps=16,  # To train on atari well, set this to 128\n    batch_size=16,  # To train on atari well, set this to 256\n    ent_coef=0.01,\n    learning_rate=0.00025,\n    n_epochs=4,\n)\n\ntrajectory_generator = preference_comparisons.AgentTrainer(\n    algorithm=agent,\n    reward_fn=reward_net,\n    venv=venv,\n    exploration_frac=0.0,\n    rng=rng,\n)\n\npref_comparisons = preference_comparisons.PreferenceComparisons(\n    trajectory_generator,\n    reward_net,\n    num_iterations=2,\n    fragmenter=fragmenter,\n    preference_gatherer=gatherer,\n    reward_trainer=reward_trainer,\n    fragment_length=10,\n    transition_oversampling=1,\n    initial_comparison_frac=0.1,\n    allow_variable_horizon=False,\n    initial_epoch_multiplier=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Behavior Cloning Policy in Python\nDESCRIPTION: Trains the behavior cloning policy on the collected expert demonstrations and evaluates its performance, showing how it can match the expert's performance after training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbc_trainer.train(n_epochs=1)\nreward_after_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\nprint(f\"Reward after training: {reward_after_training}\")\n```\n\n----------------------------------------\n\nTITLE: Loading Trajectories from HuggingFace Dataset Hub in Python\nDESCRIPTION: Code example demonstrating how to load a public dataset from the HuggingFace Dataset Hub and convert it to a trajectory sequence that can be used with the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom imitation.data.huggingface_utils import TrajectoryDatasetSequence\n\nyour_dataset = datasets.load_dataset(\"your_hf_name/your_dataset_name\")\nyour_trajectories = TrajectoryDatasetSequence(your_dataset[\"train\"])\n```\n\n----------------------------------------\n\nTITLE: Creating and Training a DAgger Agent\nDESCRIPTION: Constructs a DAgger trainer using a Behavior Cloning (BC) trainer and trains the policy on the CartPole environment for 2000 iterations. The trainer uses a temporary directory to store intermediate results.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/2_train_dagger.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nfrom imitation.algorithms import bc\nfrom imitation.algorithms.dagger import SimpleDAggerTrainer\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    rng=np.random.default_rng(),\n)\n\nwith tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n    print(tmpdir)\n    dagger_trainer = SimpleDAggerTrainer(\n        venv=env,\n        scratch_dir=tmpdir,\n        expert_policy=expert,\n        bc_trainer=bc_trainer,\n        rng=np.random.default_rng(),\n    )\n\n    dagger_trainer.train(2000)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Training the AIRL Algorithm\nDESCRIPTION: Sets up the AIRL trainer with a PPO learner agent and a shaped reward network. The code evaluates the agent's performance before training, trains the agent using AIRL, and then evaluates it again to measure improvement.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/4_train_airl.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms.adversarial.airl import AIRL\nfrom imitation.rewards.reward_nets import BasicShapedRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n\nlearner = PPO(\n    env=venv,\n    policy=MlpPolicy,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0005,\n    gamma=0.95,\n    clip_range=0.1,\n    vf_coef=0.1,\n    n_epochs=5,\n    seed=SEED,\n)\nreward_net = BasicShapedRewardNet(\n    observation_space=venv.observation_space,\n    action_space=venv.action_space,\n    normalize_input_layer=RunningNorm,\n)\nairl_trainer = AIRL(\n    demonstrations=rollouts,\n    demo_batch_size=2048,\n    gen_replay_buffer_capacity=512,\n    n_disc_updates_per_round=16,\n    venv=venv,\n    gen_algo=learner,\n    reward_net=reward_net,\n)\n\nvenv.seed(SEED)\nlearner_rewards_before_training, _ = evaluate_policy(\n    learner, venv, 100, return_episode_rewards=True\n)\nairl_trainer.train(N_RL_TRAIN_STEPS)\nvenv.seed(SEED)\nlearner_rewards_after_training, _ = evaluate_policy(\n    learner, venv, 100, return_episode_rewards=True\n)\n```\n\n----------------------------------------\n\nTITLE: Post-training Evaluation\nDESCRIPTION: Evaluates the trained GAIL agent's performance and compares results with pre-training metrics.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nenv.seed(SEED)\nlearner_rewards_after_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True\n)\n\nprint(\n    \"Rewards before training:\",\n    np.mean(learner_rewards_before_training),\n    \"+/-\",\n    np.std(learner_rewards_before_training),\n)\nprint(\n    \"Rewards after training:\",\n    np.mean(learner_rewards_after_training),\n    \"+/-\",\n    np.std(learner_rewards_after_training),\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Expert Trajectories for AIRL Training\nDESCRIPTION: Collects 60 episodes of expert demonstrations using the loaded expert policy. These trajectories will be used by the discriminator to distinguish between expert and learner behavior.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/4_train_airl.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\nrollouts = rollout.rollout(\n    expert,\n    venv,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n    rng=np.random.default_rng(SEED),\n)\n```\n\n----------------------------------------\n\nTITLE: Training Agent with Learned Reward using PPO in Python\nDESCRIPTION: This snippet demonstrates how to train an agent using the Proximal Policy Optimization (PPO) algorithm with the learned reward function. It configures the PPO algorithm and trains the agent for a specified number of timesteps.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlearner = PPO(\n    seed=0,\n    policy=FeedForward32Policy,\n    policy_kwargs=dict(\n        features_extractor_class=NormalizeFeaturesExtractor,\n        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n    ),\n    env=learned_reward_venv,\n    batch_size=64,\n    ent_coef=0.01,\n    n_epochs=10,\n    n_steps=2048 // learned_reward_venv.num_envs,\n    clip_range=0.1,\n    gae_lambda=0.95,\n    gamma=0.97,\n    learning_rate=2e-3,\n)\nlearner.learn(1_000)  # Note: set to 100_000 to train a proficient expert\n```\n\n----------------------------------------\n\nTITLE: Training PPO with Preference Comparisons on Pendulum Environment in Python\nDESCRIPTION: This code demonstrates how to use preference comparisons to train a PPO agent on the Pendulum environment. It creates a reward model trained on 200 synthetic preference comparisons, configures a PreferenceComparisons instance with various components including a trajectory generator, reward network, fragmenter, and preference gatherer, and then evaluates the trained policy.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/algorithms/preference_comparisons.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.ppo import MlpPolicy\n\nfrom imitation.algorithms import preference_comparisons\nfrom imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\nfrom imitation.rewards.reward_nets import BasicRewardNet\nfrom imitation.rewards.reward_wrapper import RewardVecEnvWrapper\nfrom imitation.util.networks import RunningNorm\nfrom imitation.util.util import make_vec_env\n\nrng = np.random.default_rng(0)\n\nvenv = make_vec_env(\"Pendulum-v1\", rng=rng)\n\nreward_net = BasicRewardNet(\n    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm,\n)\n\nfragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=rng)\ngatherer = preference_comparisons.SyntheticGatherer(rng=rng)\npreference_model = preference_comparisons.PreferenceModel(reward_net)\nreward_trainer = preference_comparisons.BasicRewardTrainer(\n    preference_model=preference_model,\n    loss=preference_comparisons.CrossEntropyRewardLoss(),\n    epochs=10,\n    rng=rng,\n)\n\nagent = PPO(\n    policy=FeedForward32Policy,\n    policy_kwargs=dict(\n        features_extractor_class=NormalizeFeaturesExtractor,\n        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n    ),\n    env=venv,\n    n_steps=2048 // venv.num_envs,\n    clip_range=0.1,\n    ent_coef=0.01,\n    gae_lambda=0.95,\n    n_epochs=10,\n    gamma=0.97,\n    learning_rate=2e-3,\n)\n\ntrajectory_generator = preference_comparisons.AgentTrainer(\n    algorithm=agent,\n    reward_fn=reward_net,\n    venv=venv,\n    exploration_frac=0.05,\n    rng=rng,\n)\n\npref_comparisons = preference_comparisons.PreferenceComparisons(\n    trajectory_generator,\n    reward_net,\n    num_iterations=5, # Set to 60 for better performance\n    fragmenter=fragmenter,\n    preference_gatherer=gatherer,\n    reward_trainer=reward_trainer,\n    initial_epoch_multiplier=4,\n    initial_comparison_frac=0.1,\n    query_schedule=\"hyperbolic\",\n)\npref_comparisons.train(total_timesteps=50_000, total_comparisons=200)\n\nn_eval_episodes = 10\nreward_mean, reward_std = evaluate_policy(agent.policy, venv, n_eval_episodes)\nreward_stderr = reward_std/np.sqrt(n_eval_episodes)\nprint(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment and Loading Expert Policy for AIRL\nDESCRIPTION: Creates a vectorized CartPole environment from the seals package with fixed episode durations and loads a pre-trained expert policy from HuggingFace. The code also defines training parameters like the number of RL steps.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/4_train_airl.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\nfrom imitation.data.wrappers import RolloutInfoWrapper\n\nSEED = 42\n\nFAST = True\n\nif FAST:\n    N_RL_TRAIN_STEPS = 100_000\nelse:\n    N_RL_TRAIN_STEPS = 2_000_000\n\nvenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(SEED),\n    n_envs=8,\n    post_wrappers=[\n        lambda env, _: RolloutInfoWrapper(env)\n    ],  # needed for computing rollouts later\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals/CartPole-v0\",\n    venv=venv,\n)\n```\n\n----------------------------------------\n\nTITLE: Training Imitation Models using Python API\nDESCRIPTION: This Python script demonstrates how to use the Imitation library's Python API to load CartPole demonstrations and train BC, GAIL, and AIRL models on that data. It requires either 'seals' or 'imitation[test]' to be installed.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/first_steps.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport gym\nimport numpy as np\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.ppo import MlpPolicy\n\nfrom imitation.algorithms import bc\nfrom imitation.algorithms.adversarial import airl, gail\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.rewards.reward_nets import BasicRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom imitation.util.util import make_vec_env\n\nrng = np.random.default_rng(0)\n\nENV_NAME = \"seals/CartPole-v0\"\nvenv = make_vec_env(ENV_NAME, n_envs=8, rng=rng)\n\nreward_net = BasicRewardNet(\n    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n)\n\ndef train_expert(*, total_timesteps=100000):\n    expert = PPO(policy=MlpPolicy, env=venv, n_steps=64)\n    expert.learn(total_timesteps=total_timesteps)\n    return expert\n\nexpert = train_expert()\n\nexpert_reward, _ = evaluate_policy(expert, venv, 100)\nprint(f\"Expert reward: {expert_reward}\")\n\nrollouts = rollout.rollout(\n    expert,\n    make_vec_env(\n        ENV_NAME,\n        n_envs=5,\n        post_wrappers=[functools.partial(RolloutInfoWrapper, \"expert\")],\n        rng=rng,\n    ),\n    rollout.make_sample_until(min_timesteps=20000, min_episodes=60),\n    rng=rng,\n)\n\n\ndef train_bc(*, batch_size=32, epochs=50):\n    bc_trainer = bc.BC(\n        observation_space=venv.observation_space,\n        action_space=venv.action_space,\n        demonstrations=rollouts,\n        rng=rng,\n    )\n    bc_trainer.train(n_epochs=epochs, batch_size=batch_size)\n    return bc_trainer.policy\n\n\ndef train_gail(*, batch_size=32, total_timesteps=100000):\n    learner = PPO(policy=MlpPolicy, env=venv, n_steps=64, verbose=1)\n    gail_trainer = gail.GAIL(\n        demonstrations=rollouts,\n        demo_batch_size=batch_size,\n        gen_replay_buffer_capacity=2048,\n        n_disc_updates_per_round=4,\n        venv=venv,\n        gen_algo=learner,\n        reward_net=reward_net,\n        allow_variable_horizon=True,\n    )\n    gail_trainer.train(total_timesteps=total_timesteps)\n    return gail_trainer.gen_algo\n\n\ndef train_airl(*, batch_size=32, total_timesteps=100000):\n    learner = PPO(policy=MlpPolicy, env=venv, n_steps=64, verbose=1)\n    airl_trainer = airl.AIRL(\n        demonstrations=rollouts,\n        demo_batch_size=batch_size,\n        venv=venv,\n        gen_algo=learner,\n        reward_net=reward_net,\n        allow_variable_horizon=True,\n    )\n    airl_trainer.train(total_timesteps=total_timesteps)\n    return airl_trainer.gen_algo\n\n\nbc_policy = train_bc()\nbc_reward, _ = evaluate_policy(bc_policy, venv, 100)\nprint(f\"BC reward: {bc_reward}\")\n\ngail_policy = train_gail()\ngail_reward, _ = evaluate_policy(gail_policy, venv, 100)\nprint(f\"GAIL reward: {gail_reward}\")\n\nairl_policy = train_airl()\nairl_reward, _ = evaluate_policy(airl_policy, venv, 100)\nprint(f\"AIRL reward: {airl_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating SQIL Agent in Python\nDESCRIPTION: This snippet trains the SQIL agent for a specified number of timesteps and then evaluates its performance. It demonstrates how the agent's reward improves after training to match the expert's performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_sqil.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsqil_trainer.train(\n    total_timesteps=1_000,\n)  # Note: set to 1_000_000 to obtain good results\nreward_after_training, _ = evaluate_policy(sqil_trainer.policy, venv, 10)\nprint(f\"Reward after training: {reward_after_training}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Preference Comparisons Components in Python\nDESCRIPTION: This snippet sets up the necessary components for preference comparisons, including the environment, reward network, fragmenter, gatherer, preference model, and agent trainer. It uses the Pendulum-v1 environment and configures various hyperparameters for the PPO algorithm.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom imitation.algorithms import preference_comparisons\nfrom imitation.rewards.reward_nets import BasicRewardNet\nfrom imitation.util.networks import RunningNorm\nfrom imitation.util.util import make_vec_env\nfrom imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nimport numpy as np\n\nrng = np.random.default_rng(0)\n\nvenv = make_vec_env(\"Pendulum-v1\", rng=rng)\n\nreward_net = BasicRewardNet(\n    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n)\n\nfragmenter = preference_comparisons.RandomFragmenter(\n    warning_threshold=0,\n    rng=rng,\n)\ngatherer = preference_comparisons.SyntheticGatherer(rng=rng)\npreference_model = preference_comparisons.PreferenceModel(reward_net)\nreward_trainer = preference_comparisons.BasicRewardTrainer(\n    preference_model=preference_model,\n    loss=preference_comparisons.CrossEntropyRewardLoss(),\n    epochs=3,\n    rng=rng,\n)\n\n\n# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n# initial_epoch_multiplier, query_schedule) used in this example have been\n# approximately fine-tuned to reach a reasonable level of performance.\nagent = PPO(\n    policy=FeedForward32Policy,\n    policy_kwargs=dict(\n        features_extractor_class=NormalizeFeaturesExtractor,\n        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n    ),\n    env=venv,\n    seed=0,\n    n_steps=2048 // venv.num_envs,\n    batch_size=64,\n    ent_coef=0.01,\n    learning_rate=2e-3,\n    clip_range=0.1,\n    gae_lambda=0.95,\n    gamma=0.97,\n    n_epochs=10,\n)\n\ntrajectory_generator = preference_comparisons.AgentTrainer(\n    algorithm=agent,\n    reward_fn=reward_net,\n    venv=venv,\n    exploration_frac=0.05,\n    rng=rng,\n)\n\npref_comparisons = preference_comparisons.PreferenceComparisons(\n    trajectory_generator,\n    reward_net,\n    num_iterations=5,  # Set to 60 for better performance\n    fragmenter=fragmenter,\n    preference_gatherer=gatherer,\n    reward_trainer=reward_trainer,\n    fragment_length=100,\n    transition_oversampling=1,\n    initial_comparison_frac=0.1,\n    allow_variable_horizon=False,\n    initial_epoch_multiplier=4,\n    query_schedule=\"hyperbolic\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment and Expert Policy\nDESCRIPTION: Initializes the CartPole environment and loads a pre-trained expert policy from HuggingFace. Uses the seals package for fixed episode durations and sets up necessary wrappers for rollout collection.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\nfrom imitation.data.wrappers import RolloutInfoWrapper\n\nSEED = 42\n\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(SEED),\n    n_envs=8,\n    post_wrappers=[\n        lambda env, _: RolloutInfoWrapper(env)\n    ],  # needed for computing rollouts later\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals/CartPole-v0\",\n    venv=env,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating AIRL Training Results\nDESCRIPTION: Prints the mean and standard deviation of rewards before and after AIRL training. The code demonstrates the improvement in the agent's performance, noting that setting FAST=False would allow the agent to fully match the expert's performance of 500.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/4_train_airl.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    \"Rewards before training:\",\n    np.mean(learner_rewards_before_training),\n    \"+/-\",\n    np.std(learner_rewards_before_training),\n)\nprint(\n    \"Rewards after training:\",\n    np.mean(learner_rewards_after_training),\n    \"+/-\",\n    np.std(learner_rewards_after_training),\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating a Reward Network with an Environment using RewardVecEnvWrapper\nDESCRIPTION: Shows how to wrap a vectorized environment with a reward network to replace the original reward function. This is essential for using learned reward functions to train reinforcement learning policies.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/reward_networks.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom imitation.util import util\nfrom imitation.rewards.reward_wrapper import RewardVecEnvWrapper\nfrom imitation.rewards.reward_nets import BasicRewardNet\n\nreward_net = BasicRewardNet(obs_space, action_space)\nvenv = util.make_vec_env(\"Pendulum-v1\", n_envs=3, rng=rng)\nvenv = RewardVecEnvWrapper(venv, reward_net.predict_processed)\n```\n\n----------------------------------------\n\nTITLE: Training Nonlinear Reward Model with MCE IRL in Python\nDESCRIPTION: Trains a nonlinear reward model (MLP with one hidden layer) using MCE IRL on analytically computed occupancy measures. This should provide a more accurate approximation of the true reward function.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_mce_irl(om, hidden_sizes=[256])\n```\n\n----------------------------------------\n\nTITLE: Setting Up SQIL Trainer in Python\nDESCRIPTION: This snippet initializes the SQIL trainer using the imitation library. It creates a vectorized environment for CartPole-v1 and sets up the SQIL algorithm with the expert trajectories and a default MLP policy.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_sqil.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms import sqil\nimport gymnasium as gym\n\nvenv = DummyVecEnv([lambda: gym.make(\"CartPole-v1\")])\nsqil_trainer = sqil.SQIL(\n    venv=venv,\n    demonstrations=expert_trajectories,\n    policy=\"MlpPolicy\",\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Expert Policy to its BC Clone\nDESCRIPTION: Evaluates and compares the performance of the original expert policy against its behavior-cloned copy to determine if there's a significant difference in performance, which indicates potential limitations of imitation learning.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nbc_clone_rewards, _ = evaluate_policy(\n    expert_bc_trainer.policy, env, 10, return_episode_rewards=True\n)\n\nexpert_rewards, _ = evaluate_policy(expert, env, 10, return_episode_rewards=True)\n\nsignificant = is_significant_reward_improvement(bc_clone_rewards, expert_rewards, 0.05)\n\nprint(f\"Cloned expert rewards: {bc_clone_rewards}\")\nprint(f\"Expert rewards: {expert_rewards}\")\n\nprint(\n    f\"Expert is {'NOT ' if not significant else ''}significantly better than the cloned expert.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Behavior Cloning Trainer in Python\nDESCRIPTION: Sets up the behavior cloning algorithm with the environment observation and action spaces, and provides the collected expert demonstrations for training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms import bc\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=transitions,\n    rng=rng,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Reward Network in PyTorch for Imitation Learning\nDESCRIPTION: Demonstrates how to create a custom reward network by subclassing the RewardNet base class. The snippet includes the initialization and forward method implementation, showing the required tensor inputs and outputs for reward calculation.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/reward_networks.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom imitation.rewards.reward_nets import RewardNet\nimport torch as th\n\nclass MyRewardNet(RewardNet):\n    def __init__(self, observation_space, action_space):\n        super().__init__(observation_space, action_space)\n        # initialize your custom reward network here\n\n    def forward(self,\n        state: th.Tensor, # (batch_size, *obs_shape)\n        action: th.Tensor, # (batch_size, *action_shape)\n        next_state: th.Tensor, # (batch_size, *obs_shape)\n        done: th.Tensor, # (batch_size,)\n    ) -> th.Tensor:\n        # implement your custom reward network here\n        return th.zeros_like(done) # (batch_size,)\n```\n\n----------------------------------------\n\nTITLE: Training Agent with Shaped Learned Reward\nDESCRIPTION: This snippet trains a new agent using the shaped, learned reward. It initializes a PPO agent and trains it on the wrapped environment with the learned reward.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlearner = PPO(\n    policy=CnnPolicy,\n    env=learned_reward_venv,\n    seed=0,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0003,\n    n_epochs=10,\n    n_steps=64,\n)\nlearner.learn(1000)\n```\n\n----------------------------------------\n\nTITLE: Generating Trajectories from a Policy in Python\nDESCRIPTION: Code snippet demonstrating how to generate trajectories from a given policy and environment using the rollout function from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport imitation.data.rollout as rollout\n\nyour_trajectories = rollout.rollout(\n    your_policy,\n    your_env,\n    sample_until=rollout.make_sample_until(min_episodes=10),\n    rng=np.random.default_rng(),\n    unwrap=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Behavior Cloning Trainers with Different Demonstrations\nDESCRIPTION: Creates two Behavior Cloning (BC) trainers, one using expert demonstrations and the other using non-expert demonstrations. This setup allows direct comparison of imitation learning performance based on the quality of demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms import bc\n\nexpert_bc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=expert_transitions,\n    rng=rng,\n)\n\nnot_expert_bc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=not_expert_transitions,\n    rng=rng,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Observation Matching Environment in Python\nDESCRIPTION: Implementation of a custom Gym environment where the agent must match output actions to input observations. Includes observation/action space definitions and core environment methods.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, Optional\nfrom typing import Any\nimport numpy as np\nimport gymnasium as gym\n\nfrom gymnasium.spaces import Box\n\n\nclass ObservationMatchingEnv(gym.Env):\n    def __init__(self, num_options: int = 2):\n        self.state = None\n        self.num_options = num_options\n        self.observation_space = Box(0, 1, shape=(num_options,))\n        self.action_space = Box(0, 1, shape=(num_options,))\n\n    def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n        super().reset(seed=seed, options=options)\n        self.state = self.observation_space.sample()\n        return self.state, {}\n\n    def step(self, action):\n        reward = -np.abs(self.state - action).mean()\n        self.state = self.observation_space.sample()\n        return self.state, reward, False, False, {}\n```\n\n----------------------------------------\n\nTITLE: Training and Final Evaluation\nDESCRIPTION: Trains the SQIL agent and evaluates its performance after training. Note indicates that more timesteps are needed for optimal results.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsqil_trainer.train(\n    total_timesteps=1000,\n)  # Note: set to 300_000 to obtain good results\nreward_after_training, _ = evaluate_policy(sqil_trainer.policy, venv, 100)\nprint(f\"Reward after training: {reward_after_training}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Expert Trajectories\nDESCRIPTION: Collects expert demonstrations by rolling out the expert policy in the environment. Generates 60 episodes of expert trajectories for training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\nrollouts = rollout.rollout(\n    expert,\n    env,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n    rng=np.random.default_rng(SEED),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Trajectory Data Structure in Python\nDESCRIPTION: A dataclass definition for the Trajectory class that stores sequences of observations, actions, and optional info dictionaries from agent-environment interactions.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dataclasses.dataclass(frozen=True)\nclass Trajectory:\n    obs: np.ndarray\n    \"\"\"Observations, shape (trajectory_len + 1, ) + observation_shape.\"\"\"\n\n    acts: np.ndarray\n    \"\"\"Actions, shape (trajectory_len, ) + action_shape.\"\"\"\n\n    infos: Optional[np.ndarray]\n        \"\"\"An array of info dicts, shape (trajectory_len, ).\"\"\"\n\n    terminal: bool\n    \"\"\"Does this trajectory (fragment) end in a terminal state?\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Environment and Expert Policy Setup\nDESCRIPTION: Initializes the Pendulum environment, loads expert policy, and generates demonstration rollouts.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.policies.serialize import load_policy\nfrom stable_baselines3.common.policies import ActorCriticPolicy\nfrom stable_baselines3 import PPO\nfrom imitation.data import rollout\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom imitation.data.wrappers import RolloutInfoWrapper\nimport gymnasium as gym\nimport numpy as np\n\nSEED = 42\n\nrng = np.random.default_rng(seed=SEED)\nenv_name = \"Pendulum-v1\"\nrollout_env = DummyVecEnv(\n    [lambda: RolloutInfoWrapper(gym.make(env_name)) for _ in range(N_VEC)]\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=env_name,\n    venv=rollout_env,\n)\nrollouts = rollout.rollout(\n    expert,\n    rollout_env,\n    rollout.make_sample_until(min_timesteps=2000, min_episodes=57),\n    rng=rng,\n)\n\nenv = util.make_vec_env(env_name, n_envs=N_VEC, rng=rng)\n\n\nimitation_trainer = PPO(\n    ActorCriticPolicy, env, learning_rate=3e-4, gamma=0.95, ent_coef=1e-4, n_steps=2048\n)\ndensity_trainer = db.DensityAlgorithm(\n    venv=env,\n    rng=rng,\n    demonstrations=rollouts,\n    rl_algo=imitation_trainer,\n    density_type=db.DensityType.STATE_ACTION_DENSITY,\n    is_stationary=True,\n    kernel=\"gaussian\",\n    kernel_bandwidth=0.4,  # found using divination & some palm reading\n    standardise_inputs=True,\n)\ndensity_trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Generating Environment Rollouts\nDESCRIPTION: Function to generate rollout trajectories by executing a given policy in an environment. Records state transitions and rewards.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/developer.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndata.rollout.rollout\n```\n\n----------------------------------------\n\nTITLE: Training GAIL Agent\nDESCRIPTION: Trains the GAIL agent for 200,000 timesteps.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngail_trainer.train(200_000)\n```\n\n----------------------------------------\n\nTITLE: Collecting Demonstration Data from Expert and Non-Expert Policies\nDESCRIPTION: Collects trajectories (demonstrations) from both the expert and non-expert policies to be used for training imitation learning algorithms. The data is collected using the rollout module from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport numpy as np\n\nrng = np.random.default_rng()\nexpert_rollouts = rollout.rollout(\n    expert,\n    DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n    rng=rng,\n)\nexpert_transitions = rollout.flatten_trajectories(expert_rollouts)\n\n\nnot_expert_rollouts = rollout.rollout(\n    not_expert,\n    DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n    rng=rng,\n)\nnot_expert_transitions = rollout.flatten_trajectories(not_expert_rollouts)\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation\nDESCRIPTION: Implements the main training loop with periodic policy evaluation and statistics reporting.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef print_stats(density_trainer, n_trajectories, epoch=\"\"):\n    stats = density_trainer.test_policy(n_trajectories=n_trajectories)\n    print(\"True reward function stats:\")\n    pprint.pprint(stats)\n    stats_im = density_trainer.test_policy(\n        true_reward=False,\n        n_trajectories=n_trajectories,\n    )\n    print(f\"Imitation reward function stats, epoch {epoch}:\")\n    pprint.pprint(stats_im)\n\n\nnovice_stats = density_trainer.test_policy(n_trajectories=N_TRAJECTORIES)\nprint(\"Stats before training:\")\nprint_stats(density_trainer, 1)\n\nprint(\"Starting the training!\")\nfor i in range(N_ITERATIONS):\n    density_trainer.train_policy(N_RL_TRAIN_STEPS)\n    print_stats(density_trainer, 1, epoch=str(i))\n```\n\n----------------------------------------\n\nTITLE: Loading Expert Trajectories for SQIL in Python\nDESCRIPTION: This snippet downloads expert trajectories from the HuggingFace Datasets Hub and converts them to a format usable by the imitation library. It requires the datasets and imitation libraries.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_sqil.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nfrom imitation.data import huggingface_utils\n\n# Download some expert trajectories from the HuggingFace Datasets Hub.\ndataset = datasets.load_dataset(\"HumanCompatibleAI/ppo-CartPole-v1\")\n\n# Convert the dataset to a format usable by the imitation library.\nexpert_trajectories = huggingface_utils.TrajectoryDatasetSequence(dataset[\"train\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Trained DAgger Policy\nDESCRIPTION: Evaluates the performance of the trained DAgger policy using Stable Baselines 3's evaluation function. The policy is tested across 20 episodes, with a maximum possible reward of 500 indicating successful training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/2_train_dagger.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nreward, _ = evaluate_policy(dagger_trainer.policy, env, 20)\nprint(reward)\n```\n\n----------------------------------------\n\nTITLE: Training GAIL from Demonstrations\nDESCRIPTION: CLI command for training a GAIL (Generative Adversarial Imitation Learning) agent using collected demonstrations from a Pendulum environment with fast training options.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial gail with pendulum environment.fast demonstrations.fast policy_evaluation.fast rl.fast fast demonstrations.path=quickstart/rl/rollouts/final.npz demonstrations.source=local\n```\n\n----------------------------------------\n\nTITLE: Evaluating BC Model Performance\nDESCRIPTION: Evaluation of BC model performance before and after training using policy evaluation.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nreward_before_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\nprint(f\"Reward before training: {reward_before_training}\")\n\nbc_trainer.train(n_epochs=1)\nreward_after_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\nprint(f\"Reward after training: {reward_after_training}\")\n```\n\n----------------------------------------\n\nTITLE: Training an Expert Policy with PPO in CartPole Environment\nDESCRIPTION: Creates and trains a PPO expert policy on the CartPole-v1 environment. This expert will serve as a benchmark for comparison with other algorithms and as a source of demonstration data for imitation learning.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\n\nenv = gym.make(\"CartPole-v1\")\nexpert = PPO(\n    policy=MlpPolicy,\n    env=env,\n    seed=0,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0003,\n    n_epochs=10,\n    n_steps=64,\n)\nexpert.learn(10_000)  # set to 100_000 for better performance\n```\n\n----------------------------------------\n\nTITLE: Loading HuggingFace Policy in Python\nDESCRIPTION: Shows how to load a policy from HuggingFace using the Python API with custom organization and environment settings.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util import util\n\nvenv = util.make_vec_env(\"your-env\", n_envs=4, rng=np.random.default_rng())\nremote_policy = load_policy(\n    \"ppo-huggingface\",\n    organization=\"your-org\",\n    env_name=\"your-env\",\n    venv=venv,\n    )\n```\n\n----------------------------------------\n\nTITLE: Training AIRL from Demonstrations\nDESCRIPTION: CLI command for training an AIRL (Adversarial Inverse Reinforcement Learning) agent using collected demonstrations from a Pendulum environment with fast training options.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial airl with pendulum environment.fast demonstrations.fast policy_evaluation.fast rl.fast fast demonstrations.path=quickstart/rl/rollouts/final.npz demonstrations.source=local\n```\n\n----------------------------------------\n\nTITLE: Applying Normalization to Reward Networks in Imitation Learning\nDESCRIPTION: Demonstrates how to normalize a reward network's outputs using the NormalizedRewardNet wrapper. This stabilizes training by standardizing rewards using running statistics.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/reward_networks.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom imitation.rewards.reward_nets import NormalizedRewardNet\nfrom imitation.util.networks import RunningNorm\ntrain_reward_net = NormalizedRewardNet(\n    reward_net,\n    normalize_output_layer=RunningNorm,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Expert Policy from HuggingFace for DAgger Training\nDESCRIPTION: Initializes the CartPole environment and loads a pre-trained expert policy from HuggingFace model hub to serve as the expert for DAgger training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/2_train_dagger.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\n\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(),\n    n_envs=1,\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals/CartPole-v0\",\n    venv=env,\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Expert Trajectory Statistics in Python\nDESCRIPTION: This code calculates and prints statistics about the expert trajectories, including the number of trajectories, average length, and average return. It uses the rollout module from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_sqil.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\ntrajectory_stats = rollout.rollout_stats(expert_trajectories)\n\nprint(\n    f\"We have {trajectory_stats['n_traj']} trajectories.\"\n    f\"The average length of each trajectory is {trajectory_stats['len_mean']}.\"\n    f\"The average return of each trajectory is {trajectory_stats['return_mean']}.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Loading Expert Policy in Python\nDESCRIPTION: Initializes the CartPole environment with appropriate wrappers and loads a pre-trained expert policy from HuggingFace model hub for demonstration collection.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util.util import make_vec_env\nfrom imitation.data.wrappers import RolloutInfoWrapper\n\nenv = make_vec_env(\n    \"seals:seals/CartPole-v0\",\n    rng=np.random.default_rng(),\n    post_wrappers=[\n        lambda env, _: RolloutInfoWrapper(env)\n    ],  # needed for computing rollouts later\n)\nexpert = load_policy(\n    \"ppo-huggingface\",\n    organization=\"HumanCompatibleAI\",\n    env_name=\"seals/CartPole-v0\",\n    venv=env,\n)\n```\n\n----------------------------------------\n\nTITLE: Training Reward Model with Preference Comparisons\nDESCRIPTION: This snippet trains the reward model using preference comparisons. It specifies the total number of timesteps and comparisons for the training process.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npref_comparisons.train(\n    total_timesteps=16,\n    total_comparisons=15,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Expert Trajectories from HuggingFace\nDESCRIPTION: Downloads expert trajectories for the Pendulum-v1 environment from HuggingFace Datasets Hub and converts them to a format compatible with the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom imitation.data import huggingface_utils\n\n# Download some expert trajectories from the HuggingFace Datasets Hub.\ndataset = datasets.load_dataset(\"HumanCompatibleAI/ppo-Pendulum-v1\")\n\n# Convert the dataset to a format usable by the imitation library.\nexpert_trajectories = huggingface_utils.TrajectoryDatasetSequence(dataset[\"train\"])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Expert Trajectory Statistics\nDESCRIPTION: Calculates and displays statistics about the expert trajectories, including the number of trajectories, average length, and average return.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\ntrajectory_stats = rollout.rollout_stats(expert_trajectories)\n\nprint(\n    f\"We have {trajectory_stats['n_traj']} trajectories. \"\n    f\"The average length of each trajectory is {trajectory_stats['len_mean']}. \"\n    f\"The average return of each trajectory is {trajectory_stats['return_mean']}.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expert Policy Performance in Python\nDESCRIPTION: Evaluates the loaded expert policy to ensure it achieves good performance before using it to collect demonstration data. Uses SB3's evaluation function to test over 10 episodes.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nreward, _ = evaluate_policy(expert, env, 10)\nprint(reward)\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of BC Models Trained on Different Data\nDESCRIPTION: Evaluates and compares the performance of the two BC models (trained on expert vs. non-expert data) using statistical significance testing to determine if there's a meaningful difference in their performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nbc_expert_rewards, _ = evaluate_policy(\n    expert_bc_trainer.policy, env, 10, return_episode_rewards=True\n)\nbc_not_expert_rewards, _ = evaluate_policy(\n    not_expert_bc_trainer.policy, env, 10, return_episode_rewards=True\n)\nsignificant = is_significant_reward_improvement(\n    bc_not_expert_rewards, bc_expert_rewards, 0.05\n)\nprint(f\"Cloned expert rewards: {bc_expert_rewards}\")\nprint(f\"Cloned not-expert rewards: {bc_not_expert_rewards}\")\n\nprint(\n    f\"Cloned expert is {'NOT ' if not significant else ''}significantly better than the cloned not-expert.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Environment with Shaped Reward Model\nDESCRIPTION: This snippet creates a shaped reward model by combining the learned reward with the policy's value function. It then wraps the environment with this shaped reward model for further training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.rewards.reward_nets import ShapedRewardNet, cnn_transpose\nfrom imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n\n\ndef value_potential(state):\n    state_ = cnn_transpose(state)\n    return agent.policy.predict_values(state_)\n\n\nshaped_reward_net = ShapedRewardNet(\n    base=reward_net,\n    potential=value_potential,\n    discount_factor=0.99,\n)\n\n# GOTCHA: When using the NormalizedRewardNet wrapper, you should deactivate updating\n# during evaluation by passing update_stats=False to the predict_processed method.\nlearned_reward_venv = RewardVecEnvWrapper(venv, shaped_reward_net.predict_processed)\n```\n\n----------------------------------------\n\nTITLE: Pre-training Evaluation\nDESCRIPTION: Evaluates the untrained learner's performance before GAIL training begins.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/3_train_gail.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nenv.seed(SEED)\nlearner_rewards_before_training, _ = evaluate_policy(\n    learner, env, 100, return_episode_rewards=True\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing BC and DAgger Performance with Statistical Testing\nDESCRIPTION: Evaluates and statistically compares the performance of the BC and DAgger models to determine if there's a significant difference between these two imitation learning algorithms on the limited dataset.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nbc_rewards, _ = evaluate_policy(bc_trainer.policy, env, 10, return_episode_rewards=True)\ndagger_rewards, _ = evaluate_policy(\n    dagger_trainer.policy, env, 10, return_episode_rewards=True\n)\n\nsignificant = is_significant_reward_improvement(bc_rewards, dagger_rewards, 0.05)\n```\n\n----------------------------------------\n\nTITLE: Generating Expert Policy and Trajectories for MCE IRL in Python\nDESCRIPTION: Computes the optimal policy using Bellman backups, calculates occupancy measures, and generates expert trajectories. This provides the ground truth data for the IRL algorithm to learn from.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n_, _, pi = mce_partition_fh(env_single)\n\n_, om = mce_occupancy_measures(env_single, pi=pi)\n\nrng = np.random.default_rng()\nexpert = TabularPolicy(\n    state_space=env_single.state_space,\n    action_space=env_single.action_space,\n    pi=pi,\n    rng=rng,\n)\n\nexpert_trajs = rollout.generate_trajectories(\n    policy=expert,\n    venv=state_venv,\n    sample_until=rollout.make_min_timesteps(5000),\n    rng=rng,\n)\n\nprint(\"Expert stats: \", rollout.rollout_stats(expert_trajs))\n```\n\n----------------------------------------\n\nTITLE: Training Reward Model with Preference Comparisons in Python\nDESCRIPTION: This snippet demonstrates how to train the reward model using the preference comparisons algorithm. It specifies the total timesteps for agent training and the number of fragment comparisons to be made.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npref_comparisons.train(\n    total_timesteps=5_000,\n    total_comparisons=200,\n)\n```\n\n----------------------------------------\n\nTITLE: Printing Results of BC vs DAgger Comparison\nDESCRIPTION: Displays the results of the comparison between BC and DAgger, showing both the raw reward values and the outcome of the statistical significance test to demonstrate the effectiveness of the evaluation methodology.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"BC rewards: {bc_rewards}\")\nprint(f\"DAgger rewards: {dagger_rewards}\")\n\nprint(\n```\n\n----------------------------------------\n\nTITLE: Training a Suboptimal Policy with PPO for Comparison\nDESCRIPTION: Creates and trains a less capable PPO policy by training for fewer steps. This 'not-expert' agent serves as a comparison point to demonstrate performance differences in the statistical tests.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnot_expert = PPO(\n    policy=MlpPolicy,\n    env=env,\n    seed=0,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0003,\n    n_epochs=10,\n    n_steps=64,\n)\n\nnot_expert.learn(1_000)  # set to 10_000 for slightly better performance\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained Agent with Original Reward in Python\nDESCRIPTION: This code snippet shows how to evaluate the trained agent using the original reward function. It uses the evaluate_policy function from Stable Baselines 3 to compute the mean and standard error of the reward over multiple evaluation episodes.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nn_eval_episodes = 10\nreward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\nreward_stderr = reward_std / np.sqrt(n_eval_episodes)\nprint(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")\n```\n\n----------------------------------------\n\nTITLE: Training Linear Reward Model with MCE IRL in Python\nDESCRIPTION: Demonstrates training a linear reward model using MCE IRL. This example shows that a linear model is insufficient for capturing the true reward structure of the Cliffworld environment.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_mce_irl(om, hidden_sizes=[])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Untrained Policy Performance in Python\nDESCRIPTION: This code evaluates the performance of the untrained policy using the stable_baselines3 library. It runs 10 episodes and prints the average reward before training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_sqil.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nreward_before_training, _ = evaluate_policy(sqil_trainer.policy, venv, 10)\nprint(f\"Reward before training: {reward_before_training}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing MCE IRL Training Function in Python\nDESCRIPTION: Defines a function to train the MCE IRL algorithm, visualize results, and compare learned rewards and occupancy measures to the ground truth. It supports both linear and nonlinear reward models.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport torch as th\n\n\ndef train_mce_irl(demos, hidden_sizes, lr=0.01, **kwargs):\n    reward_net = reward_nets.BasicRewardNet(\n        env_single.observation_space,\n        env_single.action_space,\n        hid_sizes=hidden_sizes,\n        use_action=False,\n        use_done=False,\n        use_next_state=False,\n    )\n\n    mce_irl = MCEIRL(\n        demos,\n        env_single,\n        reward_net,\n        log_interval=250,\n        optimizer_kwargs=dict(lr=lr),\n        rng=rng,\n    )\n    occ_measure = mce_irl.train(**kwargs)\n\n    imitation_trajs = rollout.generate_trajectories(\n        policy=mce_irl.policy,\n        venv=state_venv,\n        sample_until=rollout.make_min_timesteps(5000),\n        rng=rng,\n    )\n    print(\"Imitation stats: \", rollout.rollout_stats(imitation_trajs))\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    env_single.draw_value_vec(occ_measure)\n    plt.title(\"Occupancy for learned reward\")\n    plt.xlabel(\"Gridworld x-coordinate\")\n    plt.ylabel(\"Gridworld y-coordinate\")\n    plt.subplot(1, 2, 2)\n    _, true_occ_measure = mce_occupancy_measures(env_single)\n    env_single.draw_value_vec(true_occ_measure)\n    plt.title(\"Occupancy for true reward\")\n    plt.xlabel(\"Gridworld x-coordinate\")\n    plt.ylabel(\"Gridworld y-coordinate\")\n    plt.show()\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    env_single.draw_value_vec(\n        reward_net(th.as_tensor(env_single.observation_matrix), None, None, None)\n        .detach()\n        .numpy()\n    )\n    plt.title(\"Learned reward\")\n    plt.xlabel(\"Gridworld x-coordinate\")\n    plt.ylabel(\"Gridworld y-coordinate\")\n    plt.subplot(1, 2, 2)\n    env_single.draw_value_vec(env_single.reward_matrix)\n    plt.title(\"True reward\")\n    plt.xlabel(\"Gridworld x-coordinate\")\n    plt.ylabel(\"Gridworld y-coordinate\")\n    plt.show()\n\n    return mce_irl\n```\n\n----------------------------------------\n\nTITLE: Setting up SQIL with SAC\nDESCRIPTION: Initializes the training environment and creates the SQIL trainer using SAC as the base algorithm. Configures environment settings and policy parameters.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms import sqil\nfrom imitation.util.util import make_vec_env\nimport numpy as np\nfrom stable_baselines3 import sac\n\nSEED = 42\n\nvenv = make_vec_env(\n    \"Pendulum-v1\",\n    rng=np.random.default_rng(seed=SEED),\n)\n\nsqil_trainer = sqil.SQIL(\n    venv=venv,\n    demonstrations=expert_trajectories,\n    policy=\"MlpPolicy\",\n    rl_algo_class=sac.SAC,\n    rl_kwargs=dict(seed=SEED),\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Reward Improvement with Pandas Dataframes in Python\nDESCRIPTION: This snippet demonstrates how to check if an expert policy shows statistically significant improvement over a non-expert policy using pandas to load and analyze monitor CSV files, with a significance test from the imitation.testing module.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.testing.reward_improvement import is_significant_reward_improvement\n\nexpert_monitor = pd.concat(\n    [\n        pd.read_csv(f, skiprows=1)\n        for f in Path(\"./output/train_rl/CartPole-v1/expert/monitor\").glob(\n            \"mon*.monitor.csv\"\n        )\n    ]\n)\nnon_expert_monitor = pd.concat(\n    [\n        pd.read_csv(f, skiprows=1)\n        for f in Path(\"./output/train_rl/CartPole-v1/non_expert/monitor\").glob(\n            \"mon*.monitor.csv\"\n        )\n    ]\n)\nif is_significant_reward_improvement(non_expert_monitor[\"r\"], expert_monitor[\"r\"], 0.05):\n    print(\"The expert improved over the non-expert with >95% probability\")\nelse:\n    print(\"No significant (p=0.05) reward improvement of expert over non-expert\")\n```\n\n----------------------------------------\n\nTITLE: Training Expert Policy with PPO\nDESCRIPTION: Implementation of expert policy training using PPO algorithm with evaluation before and after training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom gymnasium.wrappers import TimeLimit\n\nexpert = PPO(\n    policy=MlpPolicy,\n    env=env,\n    seed=0,\n    batch_size=64,\n    ent_coef=0.0,\n    learning_rate=0.0003,\n    n_epochs=10,\n    n_steps=64,\n)\n\nreward, _ = evaluate_policy(expert, env, 10)\nprint(f\"Reward before training: {reward}\")\n\nexpert.learn(10_000)\nreward, _ = evaluate_policy(expert, expert.get_env(), 10)\nprint(f\"Expert reward: {reward}\")\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Environment with Gym\nDESCRIPTION: Registration of the custom environment with Gym registry to enable standard environment creation methods like gym.make()\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngym.register(\n    id=\"custom/ObservationMatching-v0\",\n    entry_point=ObservationMatchingEnv,\n    max_episode_steps=500,\n)\n```\n\n----------------------------------------\n\nTITLE: Run DAgger with Random Expert\nDESCRIPTION: Runs DAgger algorithm on CartPole-v0 using a random policy as expert, with 2000 total timesteps and 10 expert demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_imitation dagger with \\\n    cartpole \\\n    dagger.total_timesteps=2000 \\\n    demonstrations.n_expert_demos=10 \\\n    expert.policy_type=random\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown Summary with Statistics\nDESCRIPTION: This command creates a comprehensive markdown summary of benchmark results, including aggregate statistics such as mean, standard deviation, and IQM with confidence intervals using the rliable library methodology.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython sacred_output_to_markdown_summary output/sacred --output summary.md\n```\n\n----------------------------------------\n\nTITLE: Collecting Expert Demonstrations\nDESCRIPTION: Collection of expert demonstrations through environment rollouts for behavior cloning.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrng = np.random.default_rng()\nrollouts = rollout.rollout(\n    expert,\n    venv,\n    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n    rng=rng,\n)\ntransitions = rollout.flatten_trajectories(rollouts)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Policy Before Training in Python\nDESCRIPTION: Measures the performance of the untrained behavior cloning policy to establish a baseline for comparison after training. Shows that an untrained policy performs poorly.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nreward_before_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\nprint(f\"Reward before training: {reward_before_training}\")\n```\n\n----------------------------------------\n\nTITLE: Visualize Saved Policy\nDESCRIPTION: Renders and visualizes a saved PPO policy for the Pendulum-v1 environment.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.eval_policy with \\\n        expert.policy_type=ppo \\\n        expert.loader_kwargs.path=output/train_rl/Pendulum-v1/my_run/policies/final/model.zip \\\n        environment.num_vec=1 \\\n        render=True \\\n        environment.gym_id='Pendulum-v1'\n```\n\n----------------------------------------\n\nTITLE: Uploading Trajectories to HuggingFace Dataset Hub in Python\nDESCRIPTION: Code snippet showing how to convert trajectories to a HuggingFace Dataset and push it to the HuggingFace Dataset Hub for sharing.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data.huggingface_utils import trajectories_to_dataset\n\ntrajectories_to_dataset(your_trajectories).push_to_hub(\"your_hf_name/your_dataset_name\")\n```\n\n----------------------------------------\n\nTITLE: Expert and Pre-Training Policy Evaluation\nDESCRIPTION: Evaluates the performance of both expert and learner policies before training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexpert_rewards, _ = evaluate_policy(expert, env, 100, return_episode_rewards=True)\n\nlearner_rewards_before_training, _ = evaluate_policy(\n    density_trainer.policy, env, 100, return_episode_rewards=True\n)\n```\n\n----------------------------------------\n\nTITLE: Tuning Hyperparameters for imitation Algorithms\nDESCRIPTION: This command tunes hyperparameters for a specified algorithm in a given environment using the default search space defined in the tuning configuration. The process includes both hyperparameter search and re-evaluation of the best configuration on separate seeds.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.tuning with <algo> 'parallel_run_config.base_named_configs=[\"<env>\"]'\n```\n\n----------------------------------------\n\nTITLE: Loading Trajectories from a HuggingFace Dataset in Python\nDESCRIPTION: Code example demonstrating how to load trajectories from a HuggingFace Dataset using the serialize module from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import serialize\nyour_trajectories = serialize.load(your_path)\n```\n\n----------------------------------------\n\nTITLE: Results Comparison\nDESCRIPTION: Prints and compares the mean rewards for expert, pre-training, and post-training performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Mean expert reward:\", np.mean(expert_rewards))\nprint(\"Mean reward before training:\", np.mean(learner_rewards_before_training))\nprint(\"Mean reward after training:\", np.mean(learner_rewards_after_training))\n```\n\n----------------------------------------\n\nTITLE: Comparing Algorithm Performance Against Benchmark\nDESCRIPTION: This command compares a new or modified algorithm against benchmark runs using the Probability of Improvement metric from the rliable library to determine if there's a significant performance difference.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython compute_probability_of_improvement.py <your_runs_dir> <baseline_runs_dir> --baseline-algo <algo>\n```\n\n----------------------------------------\n\nTITLE: Analyzing Trajectory Statistics in Python\nDESCRIPTION: Code snippet showing how to analyze a dataset of trajectories using the rollout_stats function to obtain statistics like mean return.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data.rollout import rollout_stats\n\nstats = rollout_stats(your_trajectories)\nprint(stats[\"return_mean\"])\n```\n\n----------------------------------------\n\nTITLE: Statistical Significance Testing with Small Sample Size\nDESCRIPTION: Uses the imitation library's permutation test to determine if there's a statistically significant difference between the expert and non-expert performance with a small sample size of 10 episodes.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.testing.reward_improvement import is_significant_reward_improvement\n\nexpert_rewards, _ = evaluate_policy(expert, env, 10, return_episode_rewards=True)\nnot_expert_rewards, _ = evaluate_policy(\n    not_expert, env, 10, return_episode_rewards=True\n)\n\nsignificant = is_significant_reward_improvement(\n    not_expert_rewards, expert_rewards, 0.001\n)\n\nprint(\n    f\"The expert is {'NOT ' if not significant else ''}significantly better than the not-expert.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Single Benchmark via Command Line in imitation\nDESCRIPTION: This command runs a single benchmark for a specific algorithm and environment combination. The train_script parameter should be either train_imitation (for BC, DAgger) or train_adversarial (for GAIL, AIRL), and the environment should be one of the five supported seals environments.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.<train_script> <algo> with <algo>_<env>\n```\n\n----------------------------------------\n\nTITLE: Running Transfer Learning Benchmark\nDESCRIPTION: Executes the transfer learning benchmark script. It allows choosing between AIRL and GAIL algorithms using flags, with GAIL as the default. Transfer rewards are loaded from a specific directory.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexperiments/transfer_learn_benchmark.sh\n```\n\n----------------------------------------\n\nTITLE: Statistical Significance Testing with Larger Sample Size\nDESCRIPTION: Repeats the permutation test with a larger sample size (100 episodes) to increase the statistical power of the test, enabling detection of smaller differences between the expert and non-expert.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.testing.reward_improvement import is_significant_reward_improvement\n\nexpert_rewards, _ = evaluate_policy(expert, env, 100, return_episode_rewards=True)\nnot_expert_rewards, _ = evaluate_policy(\n    not_expert, env, 100, return_episode_rewards=True\n)\n\nsignificant = is_significant_reward_improvement(\n    not_expert_rewards, expert_rewards, 0.001\n)\n\nprint(\n    f\"The expert is {'NOT ' if not significant else ''}significantly better than the not-expert.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running a Single Benchmark from Command Line\nDESCRIPTION: This command runs a single benchmark for a specific algorithm and environment combination using the appropriate training script.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.<train_script> <algo> with <algo>_<env>\n```\n\n----------------------------------------\n\nTITLE: Controlling Reward Network Normalization Updates during Inference\nDESCRIPTION: Shows how to create a reward function that doesn't update normalization statistics during inference. This is useful for evaluation scenarios where you want consistent reward scaling.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/reward_networks.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom functools import partial\neval_rew_fn = partial(reward_net.predict_processed, update_stats=False)\n```\n\n----------------------------------------\n\nTITLE: Multi-Episode Evaluation for More Robust Performance Estimates\nDESCRIPTION: Evaluates the expert and non-expert policies over 10 episodes to obtain more reliable performance estimates by averaging across multiple runs, reducing the impact of randomness.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexpert_reward, _ = evaluate_policy(expert, env, 10)\nnot_expert_reward, _ = evaluate_policy(not_expert, env, 10)\n\nprint(f\"Expert reward: {expert_reward:.2f}\")\nprint(f\"Not expert reward: {not_expert_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Running Hyperparameter Tuning Experiment\nDESCRIPTION: Executes a hyperparameter tuning experiment using a specified configuration and run name. The configuration should be defined in a separate Python file.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.parallel with YOUR_NAMED_CONFIG inner_run_name=RUN_NAME\n```\n\n----------------------------------------\n\nTITLE: Generating CSV Summary of Benchmark Results\nDESCRIPTION: This command generates a CSV summary of all benchmark runs found in the output directory. The CSV includes algorithm name, environment, achieved score, and expert score for each run found in the sacred output directory.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython sacred_output_to_csv.py output/sacred > summary.csv\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Reward Networks with Different Configurations\nDESCRIPTION: Demonstrates how to save and load reward networks with different configurations for training and evaluation. Shows options for loading with or without normalization wrappers.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/reward_networks.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport torch as th\nfrom imitation.rewards.serialize import load_reward\nfrom imitation.rewards.reward_nets import NormalizedRewardNet\n\nth.save(train_reward_net, path)\ntrain_reward_net = th.load(path)\n# We can also load the reward network as a reward function for use in evaluation\neval_rew_fn_normalized = load_reward(reward_type=\"RewardNet_normalized\", reward_path=path, venv=venv)\neval_rew_fn_unnormalized = load_reward(reward_type=\"RewardNet_unnormalized\", reward_path=path, venv=venv)\n# If we want to continue to update the reward networks normalization by default it is frozen for evaluation and retraining\nrew_fn_normalized = load_reward(reward_type=\"RewardNet_normalized\", reward_path=path, venv=venv, update_stats=True)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Untrained Policy\nDESCRIPTION: Evaluates the performance of the untrained policy using the evaluate_policy function from stable-baselines3.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nreward_before_training, _ = evaluate_policy(sqil_trainer.policy, venv, 100)\nprint(f\"Reward before training: {reward_before_training}\")\n```\n\n----------------------------------------\n\nTITLE: Training Imitation Learning Models\nDESCRIPTION: Executes the imitation learning benchmark script. It allows choosing between AIRL and GAIL algorithms using flags, with GAIL as the default.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexperiments/imit_benchmark.sh --run_name RUN_NAME\n```\n\n----------------------------------------\n\nTITLE: Loading Local Policy from File in Python\nDESCRIPTION: Demonstrates how to load a local policy file using the load_policy function with PPO or SAC policy types.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom imitation.policies.serialize import load_policy\nfrom imitation.util import util\n\nvenv = util.make_vec_env(\"your-env\", n_envs=4, rng=np.random.default_rng())\nlocal_policy = load_policy(\"ppo\", venv, path=\"path/to/model.zip\")\n```\n\n----------------------------------------\n\nTITLE: Generating Rollouts in Image Environments\nDESCRIPTION: This snippet demonstrates how to generate rollouts in image-based environments. It uses the agent's get_env() function to ensure correct observation space handling for image inputs.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import rollout\n\nrollouts = rollout.rollout(\n    learner,\n    # Note that passing venv instead of agent.get_env()\n    # here would fail.\n    learner.get_env(),\n    rollout.make_sample_until(min_timesteps=None, min_episodes=3),\n    rng=rng,\n)\n```\n\n----------------------------------------\n\nTITLE: Training RL Agent and Collecting Expert Demonstrations\nDESCRIPTION: CLI command that trains a PPO agent on the Pendulum environment and collects expert demonstrations. Uses Sacred for configuration with Tensorboard logging.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npython -m imitation.scripts.train_rl with pendulum environment.fast policy_evaluation.fast rl.fast fast logging.log_dir=quickstart/rl/\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Policy Type\nDESCRIPTION: Shows how to create and register a custom policy type with the policy registry for use in the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.policies.serialize import policy_registry\nfrom stable_baselines3.common import policies\n\ndef my_policy_loader(venv, some_param: int) -> policies.BasePolicy:\n    # load your policy here\n    return policy\n\npolicy_registry.register(\"my-policy\", my_policy_loader)\n```\n\n----------------------------------------\n\nTITLE: Training Behavior Cloning Models on Different Demonstration Sets\nDESCRIPTION: Trains both BC models (expert-based and non-expert-based) for 2 epochs each. This training process learns to mimic the behavior of the respective demonstration policies.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexpert_bc_trainer.train(n_epochs=2)\nnot_expert_bc_trainer.train(n_epochs=2)\n```\n\n----------------------------------------\n\nTITLE: Viewing Stable Baselines Training Stats with TensorBoard\nDESCRIPTION: Launches TensorBoard to view training statistics for regular RL, imitation learning, and transfer learning experiments.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --log_dir ~/ray_results\n```\n\n----------------------------------------\n\nTITLE: Initializing Cliffworld Environment for MCE IRL in Python\nDESCRIPTION: Sets up the Cliffworld environment and creates a vectorized version for generating trajectories. It uses custom wrappers to extract the full state information and make the environment fully observable.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nfrom seals import base_envs\nfrom seals.diagnostics.cliff_world import CliffWorldEnv\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nimport numpy as np\n\nfrom imitation.algorithms.mce_irl import (\n    MCEIRL,\n    mce_occupancy_measures,\n    mce_partition_fh,\n    TabularPolicy,\n)\nfrom imitation.data import rollout\nfrom imitation.rewards import reward_nets\n\nenv_creator = partial(CliffWorldEnv, height=4, horizon=40, width=7, use_xy_obs=True)\nenv_single = env_creator()\n\nstate_env_creator = lambda: base_envs.ExposePOMDPStateWrapper(env_creator())\n\n# This is just a vectorized environment because `generate_trajectories` expects one\nstate_venv = DummyVecEnv([state_env_creator] * 4)\n```\n\n----------------------------------------\n\nTITLE: Post-Training Evaluation\nDESCRIPTION: Evaluates the learner's policy after training completion.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlearner_rewards_after_training, _ = evaluate_policy(\n    density_trainer.policy, env, 100, return_episode_rewards=True\n)\n```\n\n----------------------------------------\n\nTITLE: Gathering TensorBoard Directories for Specific Run\nDESCRIPTION: Executes a Python script to gather TensorBoard directories for a specific run, allowing for viewing a subset of training progress.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nimitation.scripts.analyze gather_tb_directories with source_dir=~/ray_results run_name=RUN_NAME\n```\n\n----------------------------------------\n\nTITLE: Training MCE IRL on Sampled Expert Trajectories in Python\nDESCRIPTION: Applies MCE IRL to learn a reward function from sampled expert trajectories, demonstrating how the algorithm performs with stochastic approximations of occupancy measures instead of analytically computed ones.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/6_train_mce.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmce_irl_from_trajs = train_mce_irl(expert_trajs[0:10], hidden_sizes=[256])\n```\n\n----------------------------------------\n\nTITLE: Using BufferingWrapper for Vectorized Environments\nDESCRIPTION: Wrapper for VecEnv environments that saves trajectories from all environments into a buffer. Used for collecting and storing training data.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/developer.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndata.wrapper.BufferingWrapper\n```\n\n----------------------------------------\n\nTITLE: Analyzing Imitation Learning Results with Python\nDESCRIPTION: Runs a Python script to analyze the results of imitation learning experiments. It can be executed even while training is in progress.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.analyze with run_name=RUN_NAME\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Environment Setup\nDESCRIPTION: Setup of vectorized environments for training, including options for using make_vec_env helper or direct environment creation approaches.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gymnasium.wrappers import TimeLimit\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom imitation.util.util import make_vec_env\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n\nenv = gym.make(\"custom/ObservationMatching-v0\")\n\nvenv = make_vec_env(\n    \"custom/ObservationMatching-v0\",\n    rng=np.random.default_rng(),\n    n_envs=4,\n    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Imitation from Source (Bash)\nDESCRIPTION: These commands clone the Imitation repository from GitHub and install it in editable mode. This method is useful for contributors or users who need the latest unreleased features.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/installation.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone http://github.com/HumanCompatibleAI/imitation\ncd imitation\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Print Configuration Command\nDESCRIPTION: Command to display all configurable values for any script in the imitation package.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.<script> print_config\n```\n\n----------------------------------------\n\nTITLE: Examining Collected Demonstration Data in Python\nDESCRIPTION: Inspects the structure and content of the collected rollouts and transitions data, showing how the library functions transformed the expert demonstrations into training data.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/1_train_bc.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\nAfter flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\nThe transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Quick Point Estimate Evaluation of Policy Performance\nDESCRIPTION: Performs a quick single-episode evaluation of both the expert and non-expert policies to get a preliminary performance comparison. This demonstrates why point estimates can be unreliable.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nenv.reset(seed=0)\n\nexpert_reward, _ = evaluate_policy(expert, env, 1)\nnot_expert_reward, _ = evaluate_policy(not_expert, env, 1)\n\nprint(f\"Expert reward: {expert_reward:.2f}\")\nprint(f\"Not expert reward: {not_expert_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Loading Local Policy via Command Line\nDESCRIPTION: Shows how to load a local policy file using the command line interface for AIRL training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial airl \\\n    with expert.policy_type=ppo expert.loader_kwargs.path=\"path/to/model.zip\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Learned Agent on Original Reward\nDESCRIPTION: This snippet evaluates the performance of the trained agent using the original reward function. It uses the evaluate_policy function from Stable Baselines 3 to assess the agent's performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nreward, _ = evaluate_policy(learner.policy, venv, 10)\nprint(reward)\n```\n\n----------------------------------------\n\nTITLE: Printing Statistical Significance Comparison Between DAgger and BC in Python\nDESCRIPTION: Code snippet that prints whether the DAgger agent performs significantly better than Behavioral Cloning based on a significance test. It formats the output based on a boolean 'significant' variable.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nf\"Our DAgger agent is {'NOT ' if not significant else ''}significantly better than BC.\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Policy Type via Command Line\nDESCRIPTION: Demonstrates how to use a custom registered policy type through the command line interface.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial airl \\\n    with expert.policy_type=my-policy expert.loader_kwargs.some_param=42\n```\n\n----------------------------------------\n\nTITLE: Installing Imitation Learning Package from Source (Development Mode)\nDESCRIPTION: Demonstrates how to install the imitation package from source code in development mode, including development dependencies. This is useful for contributors or those who need the latest features.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Loading HuggingFace Policy via Command Line\nDESCRIPTION: Demonstrates loading a HuggingFace policy using the command line interface for AIRL training.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/experts.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial airl \\\n    with expert.policy_type=ppo-huggingface expert.loader_kwargs.organization=your-org\n```\n\n----------------------------------------\n\nTITLE: Logging in to HuggingFace using CLI\nDESCRIPTION: Bash command to log in to the HuggingFace platform using the command-line interface, which is required before uploading datasets.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ huggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown Summary with Aggregate Statistics\nDESCRIPTION: This command creates a comprehensive markdown summary of benchmark results including aggregate statistics like mean, standard deviation, and IQM (Inter Quartile Mean) with confidence intervals, as recommended by the rliable library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython sacred_output_to_markdown_summary output/sacred --output summary.md\n```\n\n----------------------------------------\n\nTITLE: Collecting Limited Data for Algorithm Comparison\nDESCRIPTION: Collects a very small dataset (just 1 episode) from the expert policy to create a challenging imitation learning scenario where the difference between algorithms might be more pronounced.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrollouts = rollout.rollout(\n    expert,\n    DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n    rollout.make_sample_until(min_timesteps=None, min_episodes=1),\n    rng=rng,\n)\ntransitions = rollout.flatten_trajectories(rollouts)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for ARM64 Macs (Bash)\nDESCRIPTION: These commands set environment variables to work around a bug in grpcio on ARM64 Macs. This is necessary before installing the Imitation library on these systems.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/installation.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=1\nexport GRPC_PYTHON_BUILD_SYSTEM_ZLIB=1\n```\n\n----------------------------------------\n\nTITLE: Comparing Algorithm Performance with Probability of Improvement\nDESCRIPTION: This command compares a new algorithm implementation against baseline benchmark runs using the Probability of Improvement metric from the rliable library. It helps determine if modifications to an algorithm result in statistically significant improvements.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython compute_probability_of_improvement.py <your_runs_dir> <baseline_runs_dir> --baseline-algo <algo>\n```\n\n----------------------------------------\n\nTITLE: Generating Expert Demonstrations from Models\nDESCRIPTION: Runs a script to generate expert demonstrations for use in imitation learning. The rollouts are saved in the 'output/train_experts/' directory.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexperiments/rollouts_from_policies.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Imitation from PyPI (Bash)\nDESCRIPTION: This command installs the latest PyPI release of the Imitation library using pip. It's the simplest way to install the library for most users.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/installation.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install imitation\n```\n\n----------------------------------------\n\nTITLE: Running a Single Benchmark from Python Script\nDESCRIPTION: This code snippet shows how to run a benchmark programmatically from within a Python script by using the Sacred experiment runner.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n...\nfrom imitation.scripts.<train_script> import <train_script>_ex\n<train_script>_ex.run(command_name=\"<algo>\", named_configs=[\"<algo>_<env>\"])\n```\n\n----------------------------------------\n\nTITLE: Saving Trajectories as a HuggingFace Dataset in Python\nDESCRIPTION: Code example showing how to save a sequence of trajectories to disk as a HuggingFace Dataset using the serialize module from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/trajectories.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.data import serialize\nserialize.save(your_path, your_trajectories)\n```\n\n----------------------------------------\n\nTITLE: Installing Imitation Learning Package from PyPI\nDESCRIPTION: Shows how to install the imitation package via pip from PyPI repository, which is the recommended installation method for most users.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install imitation\n```\n\n----------------------------------------\n\nTITLE: Generating CSV Summary of Benchmark Results\nDESCRIPTION: This command generates a CSV summary of all benchmark runs by processing the sacred output directories, containing algorithm, environment, score, and expert score for each run.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython sacred_output_to_csv.py output/sacred > summary.csv\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Setup\nDESCRIPTION: Defines training parameters with fast/slow options for testing and production use.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nFAST = True\n\nif FAST:\n    N_VEC = 1\n    N_TRAJECTORIES = 1\n    N_ITERATIONS = 1\n    N_RL_TRAIN_STEPS = 100\n\nelse:\n    N_VEC = 8\n    N_TRAJECTORIES = 10\n    N_ITERATIONS = 10\n    N_RL_TRAIN_STEPS = 100_000\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for Imitation (Bash)\nDESCRIPTION: This command installs additional dependencies used for running tests and building documentation for the Imitation library. It's intended for developers and contributors.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/installation.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Run GAIL on Swimmer Environment\nDESCRIPTION: Executes GAIL algorithm on seals/Swimmer-v0 environment with 5000 timesteps and 50 expert demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial gail with \\\n        environment.gym_id=\"seals:seals/Swimmer-v0\" \\\n        total_timesteps=5000 \\\n        demonstrations.n_expert_demos=50\n```\n\n----------------------------------------\n\nTITLE: Train DAgger Using Saved Demonstrations\nDESCRIPTION: Runs DAgger algorithm using previously saved demonstrations from a local path.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_imitation dagger with \\\n        seals_cartpole \\\n        dagger.total_timesteps=2000 \\\n        demonstrations.source=local \\\n        demonstrations.path=output/ppo/seals_cartpole/loaded/rollouts/final\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Dependencies for Experiments\nDESCRIPTION: Command for macOS users to install required system dependencies via Homebrew for running experiments in the imitation package.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nbrew install coreutils gnu-getopt parallel\n```\n\n----------------------------------------\n\nTITLE: Generate Rollouts from Pre-trained Expert\nDESCRIPTION: Loads a pre-trained HuggingFace PPO policy and generates 50 episodes of rollouts.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.eval_policy with seals_cartpole \\\n        expert.policy_type=ppo-huggingface \\\n        eval_n_episodes=50 \\\n        logging.log_dir=output/ppo/seals_cartpole/loaded \\\n        rollout_save_path=rollouts/final\n```\n\n----------------------------------------\n\nTITLE: Running a Single Benchmark via Python Script in imitation\nDESCRIPTION: This code snippet shows how to run a single benchmark from within a Python script using Sacred's experiment runner. It imports the appropriate training script experiment and runs it with the specified algorithm and environment configuration.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/benchmarking/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n...\nfrom imitation.scripts.<train_script> import <train_script>_ex\n<train_script>_ex.run(command_name=\"<algo>\", named_configs=[\"<algo>_<env>\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Density-Based Imitation Learning\nDESCRIPTION: Initial imports for density algorithms, data types, and utility functions from the imitation library.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/7_train_density.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\n\nfrom imitation.algorithms import density as db\nfrom imitation.data import types\nfrom imitation.util import util\n```\n\n----------------------------------------\n\nTITLE: Running Algorithm Scripts in Imitation Framework\nDESCRIPTION: This snippet shows the structure for calling different algorithm scripts in the imitation framework, including BC, DAgger, AIRL, GAIL, and Preference Comparison. It demonstrates the command format using Python modules.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.<script> [command] with <named_config> <config_values>\n```\n\n----------------------------------------\n\nTITLE: Direct Environment Initialization\nDESCRIPTION: Alternative approach to environment setup using direct class instantiation and manual wrapper application.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gymnasium.wrappers import TimeLimit\nfrom imitation.data import rollout\nfrom imitation.data.wrappers import RolloutInfoWrapper\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport numpy as np\n\nenv = ObservationMatchingEnv()\nenv = TimeLimit(env, max_episode_steps=500)\n\ndef _make_env():\n    _env = ObservationMatchingEnv()\n    _env = TimeLimit(_env, max_episode_steps=500)\n    _env = RolloutInfoWrapper(_env)\n    return _env\n\nvenv = DummyVecEnv([_make_env for _ in range(4)])\n```\n\n----------------------------------------\n\nTITLE: Importing RolloutInfoWrapper Environment Wrapper\nDESCRIPTION: A Gym environment wrapper that logs original observations and rewards in the info dictionary under the 'rollout' key during the final timestep of each episode. Useful for saving trajectory data while preserving original unwrapped environment information.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/developer.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndata.wrapper.RolloutInfoWrapper\n```\n\n----------------------------------------\n\nTITLE: Run Behavioral Cloning on CartPole\nDESCRIPTION: Executes BC algorithm on CartPole-v1 using a pre-trained PPO expert policy with 50 demonstrations and 2000 training batches.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_imitation bc with \\\n    cartpole \\\n    demonstrations.n_expert_demos=50 \\\n    bc.train_kwargs.n_batches=2000 \\\n    expert.policy_type=ppo \\\n    expert.loader_kwargs.path=tests/testdata/expert_models/cartpole_0/policies/final/model.zip\n```\n\n----------------------------------------\n\nTITLE: Tuning Algorithm Hyperparameters\nDESCRIPTION: This command runs the hyperparameter tuning script for a specific algorithm and environment, using a predefined search space to optimize performance.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/main-concepts/benchmarks.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.tuning with <algo> 'parallel_run_config.base_named_configs=[\"<env>\"]'\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Class Documentation Template with Jinja2\nDESCRIPTION: A Jinja2 template that creates structured documentation for Python classes. It includes sections for class name, module path, constructor documentation, methods list, and attributes list. The template uses Sphinx directives and autosummary features.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/_templates/autosummary/class.rst#2025-04-16_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\n   {% block methods %}\n   .. automethod:: __init__\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n   {% for item in methods %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Run AIRL on MountainCar\nDESCRIPTION: Executes AIRL algorithm on MountainCar-v0 using a HuggingFace PPO expert model, with 5000 timesteps and 500 demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_adversarial airl with \\\n    seals_mountain_car \\\n    total_timesteps=5000 \\\n    expert.policy_type=ppo-huggingface \\\n    demonstrations.n_expert_demos=500\n```\n\n----------------------------------------\n\nTITLE: Calling Utility Scripts in Imitation Framework\nDESCRIPTION: This bash command demonstrates how to call utility scripts in the imitation framework for tasks like reinforcement learning, policy evaluation, parallel execution, trajectory conversion, and analyzing experimental results.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.<script>\n```\n\n----------------------------------------\n\nTITLE: Creating Python Module Documentation Template in reStructuredText\nDESCRIPTION: A template for generating Python module documentation using Sphinx autodoc directives. The template uses variables like `fullname`, `module`, and `objname` that get replaced with actual values during documentation generation. It includes an underline for the fully qualified name and sets the current module context.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/_templates/autosummary/base.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. auto{{ objtype }}:: {{ objname }}\n```\n\n----------------------------------------\n\nTITLE: Train Expert and Save Rollouts\nDESCRIPTION: Trains an expert policy on seals_cartpole environment and saves 50 episodes of rollouts.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.train_rl with seals_cartpole \\\n        total_timesteps=40000 \\\n        logging.log_dir=output/ppo/seals_cartpole/trained \\\n        rollout_save_n_episodes=50\n```\n\n----------------------------------------\n\nTITLE: Installing Imitation Learning Package from Source (Regular Use)\nDESCRIPTION: Shows how to install the imitation package from source code for regular use without development dependencies.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/README.md#2025-04-16_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX Entry for Imitation Library\nDESCRIPTION: BibTeX citation format for referencing the imitation library in research papers, including author details, title, and arXiv information.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{gleave2022imitation,\n  author = {Gleave, Adam and Taufeeque, Mohammad and Rocamonde, Juan and Jenner, Erik and Wang, Steven H. and Toyer, Sam and Ernestus, Maximilian and Belrose, Nora and Emmons, Scott and Russell, Stuart},\n  title = {imitation: Clean Imitation Learning Implementations},\n  year = {2022},\n  howPublished = {arXiv:2211.11972v1 [cs.LG]},\n  archivePrefix = {arXiv},\n  eprint = {2211.11972},\n  primaryClass = {cs.LG},\n  url = {https://arxiv.org/abs/2211.11972},\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Imitation Learning Experiments with Custom Source Directory\nDESCRIPTION: Runs a Python script to analyze imitation learning experiments, specifying a custom source directory for the results.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m imitation.scripts.analyze with run_name=RUN_NAME source_dir=~/ray_results\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure with Jinja2 Template\nDESCRIPTION: This template organizes the documentation structure with several blocks for different Python module components including attributes, functions, classes, exceptions, and submodules. It includes testsetup configuration for function documentation and uses autosummary directives to generate summary tables.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/_templates/autosummary/module.rst#2025-04-16_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. automodule:: {{ fullname }}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Module Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block functions %}\n   {% if functions %}\n   .. rubric:: {{ _('Functions') }}\n   .. testsetup::\n      :skipif: skip_doctests\n\n      # import all functions from module since examples don't import them\n      from {{ fullname }} import *\n\n   .. autosummary::\n   {% for item in functions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block classes %}\n   {% if classes %}\n   .. rubric:: {{ _('Classes') }}\n\n   .. autosummary::\n   {% for item in classes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block exceptions %}\n   {% if exceptions %}\n   .. rubric:: {{ _('Exceptions') }}\n\n   .. autosummary::\n   {% for item in exceptions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n{% block modules %}\n{% if modules %}\n.. rubric:: Modules\n\n.. autosummary::\n   :toctree:\n   :recursive:\n{% for item in modules %}\n   {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Output Directory Structure for Imitation Learning Results\nDESCRIPTION: This code block shows the directory structure created when running imitation learning algorithms. It organizes results by algorithm, environment, and timestamp, with subdirectories for logs, monitoring data, and links to sacred experiment tracking.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/getting-started/cli.rst#2025-04-16_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\noutput\n├── <algo>\n│   └── <environment>\n│       └── <timestamp>\n│           ├── log\n│           ├── monitor\n│           └── sacred -> ../../../sacred/<script_name>/1\n└── sacred\n    └── <script_name>\n        ├── 1\n        └── _sources\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Dependencies with Homebrew\nDESCRIPTION: Installs necessary GNU-compatible binaries on macOS using Homebrew to ensure compatibility with experiment scripts.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/experiments/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install coreutils gnu-getopt parallel\n```\n\n----------------------------------------\n\nTITLE: Configuring Changelog Directive in ReStructuredText\nDESCRIPTION: Configuration block for the changelog directive that specifies where the release notes will be published. It includes URLs for documentation, GitHub releases, and PyPI package.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/release-notes.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. changelog::\n    :changelog-url: https://imitation.readthedocs.io/en/latest/development/release-notes.html\n    :github: https://github.com/HumanCompatibleAI/imitation/releases\n    :pypi: https://pypi.org/project/imitation/\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for Contributing Documentation\nDESCRIPTION: This snippet defines a table of contents using reStructuredText syntax. It sets up a hidden tree structure with a maximum depth of 2, captioned 'Contributing', and includes links to 'code-of-conduct' and 'ways-to-contribute' documents.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/contributing/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :caption: Contributing\n   :hidden:\n\n   code-of-conduct\n   ways-to-contribute\n```\n\n----------------------------------------\n\nTITLE: Markdown Link to Imitation Learning Baseline Comparison Notebook\nDESCRIPTION: A markdown link that directs users to download a Jupyter notebook tutorial that compares different baseline imitation learning algorithms. The notebook is hosted in the HumanCompatibleAI/imitation GitHub repository.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/9_compare_baselines.ipynb)\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hook for Imitation Project\nDESCRIPTION: This bash command installs a pre-commit hook to run linting and static type checks before committing changes to the Imitation project repository.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/contributing/ways-to-contribute.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training BC Model\nDESCRIPTION: Setup and training of Behavior Cloning model using collected expert demonstrations.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/10_train_custom_env.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom imitation.algorithms import bc\n\nbc_trainer = bc.BC(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    demonstrations=transitions,\n    rng=rng,\n)\n```\n\n----------------------------------------\n\nTITLE: Including LICENSE File in RST Documentation\nDESCRIPTION: RST directive to include the LICENSE file from the project's root directory, making the license text available directly in the documentation.\nSOURCE: https://github.com/HumanCompatibleAI/imitation/blob/master/docs/development/license.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../../LICENSE\n```"
  }
]
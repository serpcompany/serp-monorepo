[
  {
    "owner": "airbytehq",
    "repo": "airbyte",
    "content": "TITLE: Airbyte Standard Data Records JSON Format\nDESCRIPTION: Examples of Airbyte's JSON record format showing different record types including standard records, transformed records with meta information, state messages, and trace messages. Demonstrates handling of various data types, field modifications, and synchronization status updates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/users_with_generation_id_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"users\", \"emitted_at\": 1721428633000, \"data\": { \"id\" : 1, \"name\": \"Fred\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"users\", \"emitted_at\": 1721428634000, \"data\": { \"id\" : \"two\", \"name\": \"George\" }}}\n{\"type\": \"RECORD\", \"record\": {\"meta\": { \"changes\": [ { \"field\": \"name\", \"change\": \"TRUNCATED\", \"reason\": \"SOURCE_FIELD_SIZE_LIMITATION\" } ] }, \"stream\": \"users\", \"emitted_at\": 1721428635000, \"data\": { \"id\" : 3, \"name\": \"Harry\" }}}\n{\"type\": \"RECORD\", \"record\": {\"meta\": { \"changes\": [ { \"field\": \"name\", \"change\": \"NULLED\", \"reason\": \"SOURCE_SERIALIZATION_ERROR\" } ] }, \"stream\": \"users\", \"emitted_at\": 1721428634000, \"data\": { \"id\" : { \"id\" : 4 } }, \"name\": null }}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"users\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1721428636000}}\n```\n\n----------------------------------------\n\nTITLE: Defining Common Actor Interface Methods in Airbyte Protocol\nDESCRIPTION: This snippet defines the mandatory interface methods (`spec` and `check`) common to all Airbyte actors (Sources and Destinations). The `spec` method returns the actor's specification (`ConnectorSpecification`), while the `check` method validates the connection using the provided configuration (`Config`) and returns the status (`AirbyteConnectionStatus`). These methods form the base contract for any Airbyte connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nspec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\n```\n\n----------------------------------------\n\nTITLE: Custom API Cursor Field Filtering for Incremental Sync (Airbyte YAML)\nDESCRIPTION: This YAML snippet configures the DatetimeBasedCursor with explicit API request parameter settings to filter results based on the cursor field (e.g., timestamps). The 'start_time_option' and 'end_time_option' leverage RequestOption blocks to map Airbyte's internal window boundaries to the API's expected filter parameters. This approach requires the target API to support such fields and syntax. Inputs: correct request parameter names and valid injection points. Outputs: per-window filtered API calls, matching the connector's incremental sync logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nincremental_sync:\n  type: DatetimeBasedCursor\n  <...>\n  start_time_option:\n    type: RequestOption\n    field_name: \"created[gte]\"\n    inject_into: \"request_parameter\"\n  end_time_option:\n    type: RequestOption\n    field_name: \"created[lte]\"\n    inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Terraform Configuration for Airbyte Provider (HCL)\nDESCRIPTION: This HCL code snippet shows the initial setup in `main.tf` for using the Airbyte Terraform provider. It declares the required provider `airbytehq/airbyte` with a specific version within the `terraform` block and includes an empty `provider \"airbyte\"` block where configuration details will be added.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nterraform {\n    required_providers {\n        airbyte = {\n        source = \"airbytehq/airbyte\"\n        version = \"0.6.5\"\n        }\n    }\n}\n\nprovider \"airbyte\" {\n    # Configuration options\n}\n```\n\n----------------------------------------\n\nTITLE: Using min Filter in Jinja2\nDESCRIPTION: Demonstrates the `min` filter in Jinja2, which returns the smallest item from a sequence. The example finds the minimum value in the list `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_37\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|min }}\n```\n\n----------------------------------------\n\nTITLE: Example Multi-line Text Input (Text)\nDESCRIPTION: Provides an example of a multi-line text string, such as an RSA private key, where preserving the original line breaks is crucial. This type of input requires the `multiline: true` property in the connector specification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n---- BEGIN PRIVATE KEY ----\n123\n456\n789\n---- END PRIVATE KEY ----\n```\n\n----------------------------------------\n\nTITLE: Basic YAML Structure for Airbyte Connector Configuration\nDESCRIPTION: Demonstrates the fundamental structure of an Airbyte connector manifest YAML file, showing the main configuration sections including version, definitions, streams, check, and spec components.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/low-code-cdk-overview.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"0.1.0\"\ndefinitions:\n  <key-value pairs defining objects which will be reused in the YAML connector>\nstreams:\n  <list stream definitions>\ncheck:\n  <definition of connection checker>\nspec:\n  <connector spec>\n```\n\n----------------------------------------\n\nTITLE: URL-Encoding Scope Parameter in Airbyte Declarative OAuth (YAML Diff)\nDESCRIPTION: Demonstrates modifying the `consent_url` within the `oauth_config_specification` in a YAML file to apply URL encoding to the `scope_value` using the `| urlencode` filter. It also shows a change in how scopes are defined (space-separated instead of comma-separated). This change is presented as a diff against `scopes.yml`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_12\n\nLANGUAGE: diff\nCODE:\n```\n--- scopes.yml\n+++ scopes.yml\n@@ -80,10 +80,10 @@ spec:\n     oauth_config_specification:\n       oauth_connector_input_specification:\n         consent_url: >-\n-          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n-          redirect_uri_value }}&state={{ state_value }}&scope={{scope_value}}\n+          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{ redirect_uri_value }}&state={{ state_value }}&scope={{ scope_value | urlencode }}\n         access_token_url: >-\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n-        scope: my_scope_A:read,my_scope_B:read\n+        scope: my_scope_A:read my_scope_B:read\n       complete_oauth_output_specification:\n         required:\n           - access_token\n```\n\n----------------------------------------\n\nTITLE: Destination Interface Method Signatures in Airbyte Protocol\nDESCRIPTION: Defines the interface methods that a Destination connector must implement in the Airbyte Protocol, including spec(), check(), and write() methods that receive and load data into target systems.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nspec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\nwrite(Config, AirbyteCatalog, Stream<AirbyteMessage>(stdin)) -> Stream<AirbyteStateMessage>\n```\n\n----------------------------------------\n\nTITLE: Example Obfuscated Configuration Stored by Airbyte (JSON)\nDESCRIPTION: This JSON snippet illustrates the result of Airbyte's secret obfuscation process. The original value of the `api_token` field (marked with `airbyte_secret: true` in the spec) has been replaced with a JSON object containing a `_secret` key. The value associated with `_secret` is a deterministic coordinate string that Airbyte uses internally to retrieve the actual secret value from the configured secret persistence layer when needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/secrets.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"email\":\"itpartners@noodle.com\",\n    \"api_token\":{\n        \"_secret\":\"airbyte_workspace_4e7d7911-0307-40fe-9b79-f00c0dfbb082_secret_d66baab6-3c8d-4ae5-91a6-ca8d904c4780_v1\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive HTTPAPIBudget Configuration Example in YAML\nDESCRIPTION: This YAML snippet shows a comprehensive example of an HTTPAPIBudget configuration combining multiple rate limit policies. It defines custom rate limit headers, specifies multiple status codes (429, 420) for rate limit hits, and sets up four distinct policies: unlimited for GET /sandbox, fixed window (1000/hr) for GET /users, fixed window (500/hr) for POST /orders, and moving window (20/5min) for /internal paths.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napi_budget:\n  type: \"HTTPAPIBudget\"\n  # Use standard rate limit headers from your API\n  ratelimit_reset_header: \"X-RateLimit-Reset\"\n  ratelimit_remaining_header: \"X-RateLimit-Remaining\"\n  status_codes_for_ratelimit_hit: [429, 420]\n\n  policies:\n    # Policy 1: Unlimited\n    - type: \"UnlimitedCallRatePolicy\"\n      matchers:\n        - url_base: \"https://api.example.com\"\n          method: \"GET\"\n          url_path_pattern: \"^/sandbox\"\n\n    # Policy 2: 1000 calls per hour\n    - type: \"FixedWindowCallRatePolicy\"\n      period: \"PT1H\"\n      call_limit: 1000\n      matchers:\n        - method: \"GET\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/users\"\n\n    # Policy 3: 500 calls per hour\n    - type: \"FixedWindowCallRatePolicy\"\n      period: \"PT1H\"\n      call_limit: 500\n      matchers:\n        - method: \"POST\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/orders\"\n\n    # Policy 4: 20 calls every 5 minutes (moving window).\n    - type: \"MovingWindowCallRatePolicy\"\n      rates:\n        - limit: 20\n          interval: \"PT5M\"\n      matchers:\n        - url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/internal\"\n```\n```\n\n----------------------------------------\n\nTITLE: Demonstrating API Requests with Split Intervals for Guardian API\nDESCRIPTION: Example showing how a 10-day step and 1-second cursor granularity would break down API requests to the Guardian content API into manageable intervals during an incremental sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl 'https://content.guardianapis.com/search?from-date=2023-01-01T00:00:00Z&to-date=2023-01-09T23:59:59Z'\ncurl 'https://content.guardianapis.com/search?from-date=2023-01-10T00:00:00Z&to-date=2023-01-19T23:59:59Z'\ncurl 'https://content.guardianapis.com/search?from-date=2023-01-20T00:00:00Z&to-date=2023-01-29T23:59:59Z'\n...\n```\n\n----------------------------------------\n\nTITLE: Using forceescape Filter in Jinja2\nDESCRIPTION: Demonstrates the `forceescape` filter in Jinja2, which escapes HTML characters in a string, even if the string was previously marked as safe. The example escapes '<div>'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_25\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '<div>'|forceescape }}\n```\n\n----------------------------------------\n\nTITLE: Using unique Filter in Jinja2\nDESCRIPTION: Demonstrates the `unique` filter in Jinja2, which returns a list containing only the unique items from a sequence, preserving order (based on first appearance). The example finds unique elements in `[1, 2, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_51\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 2, 3]|unique }}\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-line String Input in Airbyte Connector Spec (JSON)\nDESCRIPTION: Shows how to configure a string field in the connector specification (`spec.json`) to accept multi-line input by adding the `multiline: true` annotation. This renders a text area input field in the Airbyte UI, preserving newline characters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n\"private_key\": {\n  \"type\": \"string\",\n  \"description\": \"RSA private key to use for SSH connection\",\n  \"airbyte_secret\": true,\n  \"multiline\": true\n},\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Pagination Strategy in Python\nDESCRIPTION: Example implementation of a custom pagination strategy class that extends PaginationStrategy. Shows required dataclass structure with field definitions and interface methods that must be implemented.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/custom-components.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass MyPaginationStrategy(PaginationStrategy):\n  my_field: Union[InterpolatedString, str]\n  parameters: InitVar[Mapping[str, Any]]\n\n  def __post_init__(self, parameters: Mapping[str, Any]):\n    pass\n\n  def next_page_token(self, response: requests.Response, last_records: List[Mapping[str, Any]]) -> Optional[Any]:\n    pass\n\n  def reset(self):\n    pass\n```\n\n----------------------------------------\n\nTITLE: RBAC Implementation Script in Python\nDESCRIPTION: Complete Python script that implements RBAC role mapping using the Airbyte API. It reads user-group and group-permission mappings from JSON files and creates corresponding permissions in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/role-mapping.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport airbyte_api\nfrom airbyte_api import api, models\n\nusersGroupsFile = open('usersGroups.json')\nusersGroups = json.load(usersGroupsFile)\ngroupPermissionsFile = open('groupPermissions.json')\ngroupPermissions = json.load(groupPermissionsFile)\n\n# 0. - Enter your own credentials to use Airbyte API. \ns = airbyte_api.AirbyteAPI(\n  security=models.Security(\n    bearer_auth='...'\n  ),\n)\n\n# 1. - List all users in your organization. Find your organization ID in the Airbyte settings page.\nres = s.users.list_users(request=api.ListUsersRequest(\n  api.ListUsersRequest(organization_id='00000000-00000000-00000000-00000000')\n))\n\nallAirbyteUsers = res.users_response.data\nprint(\"all users: \", allAirbyteUsers)\n\n# 2. grant permissions\n# for each user\nfor airbyteUserResponse in allAirbyteUsers:\n  if airbyteUserResponse.email in usersGroups:\n    userGroups = usersGroups[airbyteUserResponse.email]\n    # for each group where user belongs to\n    for group in userGroups:\n      if group in groupPermissions:\n        permissionsToGrant = groupPermissions[group]\n\t # for each permission to create\n        for permission in permissionsToGrant:\n          print(\"permission to grant: \", permission)\n          if permission[\"scope\"] == \"workspace\":\n            # create workspace level permission\n            permissionCreated = s.permissions.create_permission(\n              request=models.PermissionCreateRequest(\n                permission_type=permission[\"permissionType\"],\n                user_id=airbyteUserResponse.user_id,\n                workspace_id=permission[\"scopeId\"]\n              ))\n          elif permission[\"scope\"] == \"organization\":\n            # create organization permission\n            permissionCreated = s.permissions.create_permission(\n              request=models.PermissionCreateRequest(\n                permission_type=permission[\"permissionType\"],\n                user_id=airbyteUserResponse.user_id,\n                organization_id=permission[\"scopeId\"]\n              ))\n          else:\n            print(\"permission scope not supported!\")\n```\n\n----------------------------------------\n\nTITLE: Upgrading Airbyte with Helm on Kubernetes\nDESCRIPTION: Command for upgrading an existing Airbyte installation using Helm chart. Requires specifying the release name, values file, and desired version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/upgrading-airbyte.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install <RELEASE-NAME> airbyte/airbyte --values <VALUE.YAML> --version <HELM-APP-VERSION>\n```\n\n----------------------------------------\n\nTITLE: Implementing Check Connection in Exchange Rates Source\nDESCRIPTION: Example implementation of check_connection function from Exchange Rates API source showing config validation and access testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/basic-concepts.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example from source-exchange-rates/source.py\ncheck_connection(config)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Registry Authentication\nDESCRIPTION: Bash command to create a Kubernetes secret for authenticating with a private custom image registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic regcred \\\n--from-file=.dockerconfigjson=<path/to/.docker/config.json> \\\n--type=kubernetes.io/dockerconfigjson\n```\n\n----------------------------------------\n\nTITLE: Example AirbyteCatalog JSON for a Relational Database\nDESCRIPTION: JSON example of an AirbyteCatalog for a relational database with a single 'users' table containing 'name' and 'age' columns. This demonstrates how database tables map to AirbyteStreams in the protocol.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"users\",\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\"\n          },\n          \"age\": {\n            \"type\": \"number\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling CDC on Database Level\nDESCRIPTION: SQL commands to enable Change Data Capture at the database level. Requires db_owner or sysadmin privileges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nUSE {database name}\nGO\nEXEC sys.sp_cdc_enable_db\nGO\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Reverse Proxy for Airbyte Open Source\nDESCRIPTION: Example Nginx server block configuration to deploy Airbyte Open Source behind a reverse proxy. This setup enables SSL encryption (HTTPS) and allows access control management at the proxy level. Key directives include setting `client_max_body_size` for Airbyte API compatibility, specifying SSL certificate paths (`ssl_certificate`, `ssl_certificate_key`), proxying requests to the Airbyte service (running on `http://127.0.0.1:8000` via `proxy_pass`), forwarding cookies (`proxy_set_header Cookie`) if using Airbyte's basic authentication, and setting a suitable `proxy_read_timeout` for long-running operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operating-airbyte/security.md#2025-04-23_snippet_0\n\nLANGUAGE: nginx\nCODE:\n```\n```\n# Example nginx reverse proxy config\nserver {\n  listen 443 ssl;\n  server_name airbyte.<your-domain>.com;\n  client_max_body_size 200M;  # required for Airbyte API\n  ssl_certificate <path-to-your-cert>.crt.pem;\n  ssl_certificate_key <path-to-your-key>.key.pem;\n\n  location / {\n    proxy_pass http://127.0.0.1:8000;\n    proxy_set_header Cookie $http_cookie;  # if you use Airbytes basic auth\n    proxy_read_timeout 3600;  # set a number in seconds suitable for you\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Corrected JSON Schema for the 'surveys' Stream (JSON)\nDESCRIPTION: This JSON Schema defines the corrected structure for records within the \"surveys\" stream, intended to replace the initially generated schema. It details various properties like `analyze_url`, `collect_stats`, `date_created`, `id`, `title`, etc., along with their expected data types (string, number, object) and nested structures. This schema adheres to the JSON Schema specification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/5-discover.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$schema\": \"http://json-schema.org/schema#\",\n  \"properties\": {\n    \"analyze_url\": {\n      \"type\": \"string\"\n    },\n    \"collect_stats\": {\n      \"properties\": {\n        \"status\": {\n          \"properties\": {\n            \"open\": {\n              \"type\": \"number\"\n            }\n          },\n          \"type\": \"object\"\n        },\n        \"total_count\": {\n          \"type\": \"number\"\n        },\n        \"type\": {\n          \"properties\": {\n            \"weblink\": {\n              \"type\": \"number\"\n            }\n          },\n          \"type\": \"object\"\n        }\n      },\n      \"type\": \"object\"\n    },\n    \"date_created\": {\n      \"type\": \"string\"\n    },\n    \"date_modified\": {\n      \"type\": \"string\"\n    },\n    \"href\": {\n      \"type\": \"string\"\n    },\n    \"id\": {\n      \"type\": \"string\"\n    },\n    \"language\": {\n      \"type\": \"string\"\n    },\n    \"nickname\": {\n      \"type\": \"string\"\n    },\n    \"preview\": {\n      \"type\": \"string\"\n    },\n    \"question_count\": {\n      \"type\": \"number\"\n    },\n    \"response_count\": {\n      \"type\": \"number\"\n    },\n    \"title\": {\n      \"type\": \"string\"\n    }\n  },\n  \"type\": \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using default Filter in Jinja2\nDESCRIPTION: Demonstrates the `default` filter in Jinja2, which provides a default value if the variable is undefined or evaluates to false (depending on the `boolean` parameter, default is false). The example returns 'default' for an undefined variable.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_19\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ undefined_variable|default('default') }}\n```\n\n----------------------------------------\n\nTITLE: Hiding Content from Cloud using Custom Markdown Extension - Markdown\nDESCRIPTION: Wraps markdown content in environment-specific comment tags (`<!-- env:oss --> ... <!-- /env:oss -->`) so that it is rendered only in self-managed Airbyte builds and not in Airbyte Cloud UI. There are no dependencies required, but the markdown parser needs to interpret these directives. Input: Markdown content to show only in OSS; Output: Content displayed only in OSS UI and Airbyte docs. This pattern is limited to environments that recognize Airbyte's custom markdown extensions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- env:oss -->\\n\\nOnly Self-Managed builds of the Airbyte UI will render this content.\\n\\n<!-- /env:oss -->\n```\n\n----------------------------------------\n\nTITLE: Defining CursorPagination Strategy Schema in YAML\nDESCRIPTION: Defines the YAML schema for the `CursorPagination` strategy. It requires a `cursor_value` property, which typically uses string interpolation to extract the next page token from the response body, headers, or last records. Optional properties include `stop_condition` (to terminate pagination), `page_size`, and `$parameters`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nCursorPagination:\n  type: object\n  additionalProperties: true\n  required:\n    - cursor_value\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    cursor_value:\n      type: string\n    stop_condition:\n      type: string\n    page_size:\n      type: integer\n```\n\n----------------------------------------\n\nTITLE: Using JSON-Encoded Query Parameters for OAuth Exchange - Diff/YAML\nDESCRIPTION: This diff changes the Airbyte manifest so that access token exchange parameters are sent as a JSON-encoded body (via access_token_params) instead of as URL query parameters. The updated manifest moves all variables from the access_token_url to the access_token_params map. Requires understanding of Airbyte's OAuth spec, and receivers must accept requests with JSON-encoded bodies. Inputs are manifest YAML and dynamic config values; outputs are HTTP requests with JSON body parameters to the token endpoint.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n--- simple_oauth_manifest.yml\n+++ secret_header_manifest.yml\n      spec:\n          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n          redirect_uri_value }}&state={{ state }}\n        access_token_url: >-\n-          https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n+          https://yourconnectorservice.com/oauth/token\n+        access_token_params:\n+          client_id: \"{{ client_id_value }}\"\n+          client_secret: \"{{ client_secret_value }}\"\n+          redirect_uri: \"{{ redirect_uri_value }}\"\n      complete_oauth_output_specification:\n        required:\n```\n\n----------------------------------------\n\nTITLE: Example EmailOctopus API Response (After Hashing)\nDESCRIPTION: Shows the campaign record after applying an MD5 hash transformation to the `from.name` field using the expression `{{ record['from']['name'] | hash('md5') }}`. The original \"John Doe\" is replaced by its MD5 hash.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"00000000-0000-0000-0000-000000000000\",\n    \"status\": \"SENT\",\n    \"name\": \"Foo\",\n    \"subject\": \"Bar\",\n    \"from\": {\n        \"name\": \"4c2a904bafba06591225113ad17b5cec\",\n        \"email_address\": \"john.doe@gmail.com\"\n    },\n    \"created_at\": \"2023-04-13T15:28:37+00:00\",\n    \"sent_at\": \"2023-04-14T15:28:37+00:00\"\n}\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Access Token, Refresh Token, and Expiry - JSON\nDESCRIPTION: A JSON example showing all three expected fields: access_token, refresh_token, and expires_in, matching the advanced YAML configuration for one-time-use refresh tokens. Used to demonstrate how Airbyte maps token and expiry data from the OAuth server's response. All fields must be correctly typed and non-null.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_73\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"access_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\",\\n  \\\"refresh_token\\\": \\\"YOUR_REFRESH_TOKEN_123\\\",\\n  \\\"expires_in\\\": 7200\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Incremental Dependency State Format for Nested Streams (Airbyte JSON)\nDESCRIPTION: This JSON structure shows state management when using an incremental dependency between a parent and child stream in Airbyte. 'parent_state' tracks progress on the parent stream, and 'states' maintains individual partition cursors for child records. This format ensures child stream states are updated in sync with parent progress. Required: API ensures parent cursor updates on child changes. Input/output: atomic per-parent and per-partition sync checkpoints.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parent_state\": {\n    \"parent_stream\": { \"timestamp\": \"2024-08-01T00:00:00\" }\n  },\n  \"states\": [\n    { \"partition\": \"A\", \"timestamp\": \"2024-08-01T00:00:00\" },\n    { \"partition\": \"B\", \"timestamp\": \"2024-08-01T01:00:00\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Making an Incremental Sync API Request Using cURL - Bash\nDESCRIPTION: This cURL command demonstrates how to perform an incremental sync against The Guardian's content API by querying articles within a specified date range. The command constructs a GET request with query parameters from-date and to-date, set dynamically (e.g., using a past ISO8601 timestamp and 'now'). Prerequisites include cURL and appropriate network access. The endpoint returns JSON-formatted article data; modifying from-date and to-date allows retrieval of incremental data based on sync state.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl 'https://content.guardianapis.com/search?from-date=2023-04-09T00:00:00Z&to-date={`now`}'\n```\n\n----------------------------------------\n\nTITLE: Defining DpathExtractor Schema for Record Extraction - YAML\nDESCRIPTION: YAML schema defining a DpathExtractor, which specifies the field_path for extracting records from JSON responses using dpath syntax. The field_path parameter (an array of strings) guides how to traverse the JSON structure or arrays (supporting wildcards like '*'). This schema is crucial for Airbyte’s method of locating nested data within HTTP responses. No code dependencies are required, but the Airbyte platform internally uses the dpath Python package.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nDpathExtractor:\n  type: object\n  additionalProperties: true\n  required:\n    - field_path\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    field_path:\n      type: array\n      items:\n        type: string\n```\n\n----------------------------------------\n\nTITLE: Common Check Method in Airbyte Protocol\nDESCRIPTION: Defines the check() method which validates that given a configuration, the Actor can connect to and access required resources. Returns a success or failure status based on connection ability.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ncheck(Config) -> AirbyteConnectionStatus\n```\n\n----------------------------------------\n\nTITLE: Defining a Base HTTP Stream Class for SurveyMonkey in Python\nDESCRIPTION: Provides a template for `SurveyMonkeyBaseStream`, an abstract base class inheriting from Airbyte CDK's `HttpStream`. It initializes common stream properties like name, API path, primary key, and the data field within the JSON response. It defines default request parameters and includes basic response parsing logic, intended to be subclassed for specific SurveyMonkey API endpoints. A placeholder `<TODO>` indicates where the base URL should be defined.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#\n# Copyright (c) 2023 Airbyte, Inc., all rights reserved.\n#\n\n\nfrom abc import ABC\nfrom typing import Any, Iterable, List, Mapping, MutableMapping, Optional, Tuple, Union\n\nimport requests\nfrom airbyte_cdk.sources import AbstractSource\nfrom airbyte_cdk.sources.streams import Stream\nfrom airbyte_cdk.sources.streams.http import HttpStream\nfrom airbyte_cdk.sources.streams.http.requests_native_auth import Oauth2Authenticator, TokenAuthenticator\n\n\nclass SurveyMonkeyBaseStream(HttpStream, ABC):\n    def __init__(self, name: str, path: str, primary_key: Union[str, List[str]], data_field: str, **kwargs: Any) -> None:\n        self._name = name\n        self._path = path\n        self._primary_key = primary_key\n        self._data_field = data_field\n        super().__init__(**kwargs)\n\n    url_base = <TODO>\n\n    def next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n        return None\n\n    def request_params(\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\n    ) -> MutableMapping[str, Any]:\n        return {\"include\": \"response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats\"}\n\n    def parse_response(self, response: requests.Response, **kwargs) -> Iterable[Mapping]:\n        response_json = response.json()\n        if self._data_field:\n            yield from response_json.get(self._data_field, [])\n        else:\n            yield from response_json\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    def path(\n        self,\n        *,\n        stream_state: Optional[Mapping[str, Any]] = None,\n        stream_slice: Optional[Mapping[str, Any]] = None,\n        next_page_token: Optional[Mapping[str, Any]] = None,\n    ) -> str:\n        return self._path\n\n    @property\n    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:\n        return self._primary_key\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Cursor Pagination (Request Parameter) Example in YAML\nDESCRIPTION: Shows an example YAML configuration for `CursorPagination` where the cursor value is injected as a request parameter. The `cursor_value` uses interpolation `{{ last_records[-1]['id'] }}` to extract the 'id' of the last record from the previous response. The `page_token_option` specifies that this value should be injected as the 'from' request parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\npaginator:\n  type: \"DefaultPaginator\"\n  <...>\n  pagination_strategy:\n    type: \"CursorPagination\"\n    cursor_value: \"{{ last_records[-1]['id'] }}\"\n  page_token_option:\n    type: \"RequestPath\" # Note: Example text says RequestOption might be more appropriate here, but code shows RequestPath\n    field_name: \"from\"\n    inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Creating Db2 Read-Only User Role and Permissions in SQL\nDESCRIPTION: This SQL script creates a dedicated role `AIRBYTE_ROLE` for Airbyte, grants it the necessary `CONNECT` permission on the target database, and then assigns this role to the `AIRBYTE_USER`. This is a recommended step for secure access control when configuring the Db2 source connector. Replace `'DATABASE'` with the actual database name and ensure the `AIRBYTE_USER` exists.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/db2.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- create Airbyte role\nCREATE ROLE 'AIRBYTE_ROLE';\n\n-- grant Airbyte database access\nGRANT CONNECT ON 'DATABASE' TO ROLE 'AIRBYTE_ROLE'\nGRANT ROLE 'AIRBYTE_ROLE' TO USER 'AIRBYTE_USER'\n```\n\n----------------------------------------\n\nTITLE: Minimal Input Parameter Definition in spec.json (JSON)\nDESCRIPTION: Example of a minimal `spec.json` definition for a 'user_name' input parameter using JSON Schema. This approach lacks 'title' and 'description' fields, which is discouraged as it results in a less informative and user-friendly UI experience.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/ux-handbook.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_name\": {\n      \"type\": \"string\"\n    }\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Defining SurveyMonkey Dependent Substream Class in Python\nDESCRIPTION: Implements a substream class, SurveyMonkeySubstream, which extends HttpStream and requires a parent stream to generate slices. Handles pagination, request params, and response parsing but does not support incremental sync. Required dependencies: airbyte-cdk HttpStream base, requests, and parsed urlutil. Inputs are API responses and parent stream records; outputs are structured stream slices and parsed data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SurveyMonkeySubstream(HttpStream, ABC):\\n\\n    def __init__(self, name: str, path: str, primary_key: Union[str, List[str]], parent_stream: Stream, **kwargs: Any) -> None:\\n        self._name = name\\n        self._path = path\\n        self._primary_key = primary_key\\n        self._parent_stream = parent_stream\\n        super().__init__(**kwargs)\\n\\n    url_base = \\\"https://api.surveymonkey.com\\\"\\n\\n    def next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\\n        links = response.json().get(\\\"links\\\", {})\\n        if \\\"next\\\" in links:\\n            return {\\\"next_url\\\": links[\\\"next\\\"]}\\n        else:\\n            return {}\\n\\n    def request_params(\\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\\n    ) -> MutableMapping[str, Any]:\\n        if next_page_token:\\n            return urlparse(next_page_token[\\\"next_url\\\"]).query\\n        else:\\n            return {\\\"per_page\\\": _PAGE_SIZE}\\n\\n    def parse_response(self, response: requests.Response, **kwargs) -> Iterable[Mapping]:\\n        yield from response.json().get(\\\"data\\\", [])\\n\\n    @property\\n    def name(self) -> str:\\n        return self._name\\n\\n    def path(\\n        self,\\n        *,\\n        stream_state: Optional[Mapping[str, Any]] = None,\\n        stream_slice: Optional[Mapping[str, Any]] = None,\\n        next_page_token: Optional[Mapping[str, Any]] = None,\\n    ) -> str:\\n        try:\\n            return self._path.format(stream_slice=stream_slice)\\n        except Exception as e:\\n            raise e\\n\\n    @property\\n    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:\\n        return self._primary_key\\n\\n    def stream_slices(self, stream_state: Mapping[str, Any] = None, **kwargs) -> Iterable[Optional[Mapping[str, any]]]:\\n        for _slice in self._parent_stream.stream_slices():\\n            for parent_record in self._parent_stream.read_records(sync_mode=SyncMode.full_refresh, stream_slice=_slice):\\n                yield parent_record\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Schema with Array of Timestamps in JSON\nDESCRIPTION: JSON schema example defining an object with a single property \"appointments\", which is an array containing items of type string formatted as 'date-time' and specified as 'timestamp_with_timezone' via `airbyte_type`. This schema is used to illustrate how destinations might handle unsupported types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"appointments\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"format\": \"date-time\",\n        \"airbyte_type\": \"timestamp_with_timezone\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Single Access Token - JSON\nDESCRIPTION: This JSON snippet provides a sample OAuth 2.0 response containing only the access_token at the root. Airbyte expects to extract the access_token field as per the related YAML specification. Input is the token response from an OAuth server, and output is the same JSON structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_63\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"access_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\"\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Using sort Filter in Jinja2\nDESCRIPTION: Demonstrates the `sort` filter in Jinja2, which sorts the items of a sequence (e.g., a list). The example sorts the list `[3, 2, 1]` into ascending order.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_44\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [3, 2, 1]|sort }}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Metrics in Airbyte via YAML - YAML\nDESCRIPTION: This YAML code snippet updates Airbyte's Helm values.yaml file to enable enterprise metrics collection through OpenTelemetry. The configuration sets the enterprise edition, enables metrics, and specifies the OTLP collector endpoint to receive Airbyte's metrics. Prerequisites include deploying an OpenTelemetry collector endpoint independently, and this configuration is only available to users of Airbyte Self-Managed Enterprise edition. The main parameters are 'enabled', which turns on metrics reporting; and 'collectorEndpoint', set to the address of your OTel collector. This snippet must be included in your Helm values.yaml file before redeployment. Expects a valid OTLP endpoint and proper Helm context.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/collecting-metrics.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n    edition: enterprise # This is an enterprise-only feature\n    metrics:\n        enabled: true\n        otlp:\n            enabled: true\n            collectorEndpoint: \"YOUR_ENDPOINT\" # The OTel collector endpoint Airbyte sends metrics to. You configure this endpoint outside of Airbyte as part of your OTel deployment.\n```\n\n----------------------------------------\n\nTITLE: Defining Breaking Changes in metadata.yaml (YAML)\nDESCRIPTION: This snippet shows an example of the `releases` section, specifically defining `breakingChanges`. It illustrates how to document a breaking change for a specific version (e.g., `1.0.0`) by providing a user-friendly `message` explaining the change, its impact, and required user actions, along with an `upgradeDeadline`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreleases:\n  breakingChanges:\n    1.0.0:\n      message: \"This version changes the connector’s authentication by removing ApiKey authentication, which is now deprecated by the [upstream source](upsteam-docs-url.com). Users currently using ApiKey auth will need to reauthenticate with OAuth after upgrading to continue syncing.\"\n      upgradeDeadline: \"2023-12-31\"\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_definition` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_definition` table, representing the definition of a connector (source or destination). It stores metadata like name, icon, type, source category, timestamps, soft delete flag, resource requirements, visibility (public/custom), message timing constraints, default version ID, icon URL, metrics, and enterprise status. A foreign key links to `actor_definition_version` for the default version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name                     | Datatype     | Description                                              |\n| -------------------------------- | ------------ | -------------------------------------------------------- |\n| id                               | UUID         | Primary key. Unique identifier for the actor definition. |\n| name                             | VARCHAR(256) | Name of the connector.                                   |\n| icon                             | VARCHAR(256) | Icon for the connector.                                 |\n| actor_type                       | ENUM         | Indicates whether the actor is a source or destination. |\n| source_type                      | ENUM         | Source category (e.g., API, Database).                  |\n| created_at                       | TIMESTAMP    | Timestamp when the record was created.                   |\n| updated_at                       | TIMESTAMP    | Timestamp when the record was last modified.             |\n| tombstone                        | BOOLEAN      | Soft delete flag.                                        |\n| resource_requirements            | JSONB        | Defines default resource requirements.                   |\n| public                           | BOOLEAN      | Determines if the definition is publicly available.      |\n| custom                           | BOOLEAN      | Indicates if the connector is user-defined.              |\n| max_seconds_between_messages     | INT          | Maximum allowed seconds between messages.                |\n| default_version_id               | UUID         | Foreign key referencing `actor_definition_version(id)`.  |\n| icon_url                         | VARCHAR(256) | URL of the icon image.                                   |\n| metrics                          | JSONB        | Metadata about the connector.                            |\n| enterprise                       | BOOLEAN      | Whether the connector is part of the enterprise edition.|\n\n#### Indexes and Constraints\n\n- Primary Key: (`id`)\n- Foreign Key: `default_version_id` references `actor_definition_version(id)`\n```\n\n----------------------------------------\n\nTITLE: Creating Airbyte Entities in Snowflake (SQL)\nDESCRIPTION: SQL script to create the necessary Snowflake resources (role, user, warehouse, database, schema) and grant appropriate permissions for the Airbyte Snowflake destination connector. Variables for names and the user password need to be set before execution. Requires SECURITYADMIN and SYSADMIN roles.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- set variables (these need to be uppercase)\nset airbyte_role = 'AIRBYTE_ROLE';\nset airbyte_username = 'AIRBYTE_USER';\nset airbyte_warehouse = 'AIRBYTE_WAREHOUSE';\nset airbyte_database = 'AIRBYTE_DATABASE';\nset airbyte_schema = 'AIRBYTE_SCHEMA';\n\n-- set user password\nset airbyte_password = 'password';\n\nbegin;\n\n-- create Airbyte role\nuse role securityadmin;\ncreate role if not exists identifier($airbyte_role);\ngrant role identifier($airbyte_role) to role SYSADMIN;\n\n-- create Airbyte user\ncreate user if not exists identifier($airbyte_username)\npassword = $airbyte_password\ndefault_role = $airbyte_role\ndefault_warehouse = $airbyte_warehouse;\n\ngrant role identifier($airbyte_role) to user identifier($airbyte_username);\n\n-- change role to sysadmin for warehouse / database steps\nuse role sysadmin;\n\n-- create Airbyte warehouse\ncreate warehouse if not exists identifier($airbyte_warehouse)\nwarehouse_size = xsmall\nwarehouse_type = standard\nauto_suspend = 60\nauto_resume = true\ninitially_suspended = true;\n\n-- create Airbyte database\ncreate database if not exists identifier($airbyte_database);\n\n-- grant Airbyte warehouse access\ngrant USAGE\non warehouse identifier($airbyte_warehouse)\nto role identifier($airbyte_role);\n\n-- grant Airbyte database access\ngrant OWNERSHIP\non database identifier($airbyte_database)\nto role identifier($airbyte_role);\n\ncommit;\n\nbegin;\n\nUSE DATABASE identifier($airbyte_database);\n\n-- create schema for Airbyte data\nCREATE SCHEMA IF NOT EXISTS identifier($airbyte_schema);\n\ncommit;\n\nbegin;\n\n-- grant Airbyte schema access\ngrant OWNERSHIP\non schema identifier($airbyte_schema)\nto role identifier($airbyte_role);\n\ncommit;\n```\n\n----------------------------------------\n\nTITLE: Configuring PageIncrement Paginator Example in YAML\nDESCRIPTION: Provides an example YAML configuration for a `DefaultPaginator` using the `PageIncrement` strategy. It specifies injecting the `page_size` (set to 5) and the incrementing `page` number into request parameters using `RequestOption`. The `page_token_option` defines how the page number itself is injected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page_size\"\n  pagination_strategy:\n    type: \"PageIncrement\"\n    page_size: 5\n  page_token_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Parameters for SurveyMonkey API in Python\nDESCRIPTION: This snippet defines the request parameters for the SurveyMonkey API calls, including various fields to be included in the response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef request_params(\n    self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\n) -> MutableMapping[str, Any]:\n    return {\"include\": \"response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats\"}\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Caching for Survey Stream in Python\nDESCRIPTION: Shows how to enable caching for the SurveyMonkeyBaseStream class with a read-only 'use_cache' property. Ensures that fetched survey data can be reused across dependent streams. Relevant to Airbyte source connector optimization for repeated upstream API requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    @property\\n    def use_cache(self) -> bool:\\n        return True\\n\n```\n\n----------------------------------------\n\nTITLE: Documentation Placeholder Template with HTML Tags\nDESCRIPTION: Template code showing documentation structure with HideInUI tags and placeholders for connector name and documentation link.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/connector_acceptance_test/tests/doc_templates/source.txt#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<HideInUI>\n\nThis page contains the setup guide and reference information for the [{connector_name}]({docs_link}) source connector.\n\n</HideInUI>\n```\n\n----------------------------------------\n\nTITLE: Executing Airbyte Source Operations via Docker (Shell)\nDESCRIPTION: Provides the Docker commands to execute the standard Airbyte source operations (`spec`, `check`, `discover`, `read`). These commands run the specified source connector Docker image and pass necessary arguments like configuration, catalog, and state file paths via command-line flags. The `read` command outputs a stream of Airbyte messages (records or state) to STDOUT, which can be redirected to a file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol-docker.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ndocker run --rm -i <source-image-name> spec\ndocker run --rm -i <source-image-name> check --config <config-file-path>\ndocker run --rm -i <source-image-name> discover --config <config-file-path>\ndocker run --rm -i <source-image-name> read --config <config-file-path> --catalog <catalog-file-path> [--state <state-file-path>] > message_stream.json\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Error Handling in parse_response (Python)\nDESCRIPTION: Overrides the `parse_response` method to handle specific SurveyMonkey API error codes (1010-1018) related to authentication. If such an error is detected in the JSON response, it raises an `AirbyteTracedException` with `FailureType.config_error`, providing distinct internal and external messages. Otherwise, it yields records from the response data field or the entire response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    def parse_response(self, response: requests.Response, **kwargs) -> Iterable[Mapping]:\n        response_json = response.json()\n        # https://api.surveymonkey.com/v3/docs?shell#error-codes\n        if response_json.get(\"error\") in (1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018):\n            internal_message = \"Unauthorized credentials. Response: {response_json}\"\n            external_message = \"Can not get metadata with unauthorized credentials. Try to re-authenticate in source settings.\"\n            raise AirbyteTracedException(\n                message=external_message, internal_message=internal_message, failure_type=FailureType.config_error\n            )\n        elif self._data_field:\n            yield from response_json[self._data_field]\n        else:\n            yield from response_json\n```\n\n----------------------------------------\n\nTITLE: Defining date-related constants for stream slicing in Python\nDESCRIPTION: Establishes module-level constants controlling start date, slice size (in days), and datetime string formatting for both outgoing and incoming requests. These constants ensure consistent date handling across slicing, state updates, and API requests. Dependencies: datetime for timestamp computations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n_START_DATE = datetime.datetime(2020,1,1, 0,0,0).timestamp()\n_SLICE_RANGE = 365\n_OUTGOING_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n_INCOMING_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n```\n\n----------------------------------------\n\nTITLE: Querying Data Using Cursor Field in SQL for Incremental Sync\nDESCRIPTION: SQL query example showing how source connectors extract delta rows during incremental sync by comparing the cursor field against the last sync's maximum cursor value.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/core-concepts/sync-modes/incremental-append.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM table WHERE cursor_field >= 'last_sync_max_cursor_field_value'\n```\n\n----------------------------------------\n\nTITLE: Defining OAuth Schema for OAuthAuthenticator Configuration - YAML\nDESCRIPTION: This YAML snippet defines the schema used to configure OAuth authentication for Airbyte connectors. It specifies mandatory and optional fields such as token endpoints, client credentials, and refresh options, as well as advanced options like token updater paths. Dependencies include the use of Airbyte's config YAML, with required properties for token management and optional fine-tuning to match the OAuth provider's requirements. Inputs are typically supplied at deployment or connection configuration, and the output is a validated configuration object used during OAuth flows. Limitations may include schema extension or adaptation for alternative OAuth providers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_74\n\nLANGUAGE: yaml\nCODE:\n```\nOAuth:\n  type: object\n  additionalProperties: true\n  required:\n    - token_refresh_endpoint\n    - client_id\n    - client_secret\n    - refresh_token\n    - access_token_name\n    - expires_in_name\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    token_refresh_endpoint:\n      type: string\n    client_id:\n      type: string\n    client_secret:\n      type: string\n    refresh_token:\n      type: string\n    scopes:\n      type: array\n      items:\n        type: string\n      default: []\n    token_expiry_date:\n      type: string\n    access_token_name:\n      type: string\n      default: \"access_token\"\n    expires_in_name:\n      type: string\n      default: \"expires_in\"\n    refresh_request_body:\n      type: object\n    grant_type:\n      type: string\n      default: \"refresh_token\"\n    refresh_token_updater:\n        title: Token Updater\n        description: When the token updater is defined, new refresh tokens, access tokens and the access token expiry date are written back from the authentication response to the config object. This is important if the refresh token can only used once.\n        properties:\n          refresh_token_name:\n            title: Refresh Token Property Name\n            description: The name of the property which contains the updated refresh token in the response from the token refresh endpoint.\n            type: string\n            default: \"refresh_token\"\n          access_token_config_path:\n            title: Config Path To Access Token\n            description: Config path to the access token. Make sure the field actually exists in the config.\n            type: array\n            items:\n              type: string\n            default: [\"credentials\", \"access_token\"]\n          refresh_token_config_path:\n            title: Config Path To Refresh Token\n            description: Config path to the access token. Make sure the field actually exists in the config.\n            type: array\n            items:\n              type: string\n            default: [\"credentials\", \"refresh_token\"]\n\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with Array Data Types\nDESCRIPTION: A JSON example of an Airbyte RECORD message containing various array data types including strings, dates, timestamps, numbers, integers, booleans, and binary data. The message includes the stream name, emission timestamp, and the actual record data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"string_array\" : [\"foo bar\", \"some random special characters: ࠈൡሗ\"], \"array_date\" : [\"2021-01-23\", \"1504-02-29\"], \"array_timestamp_with_timezone\" : [\"2022-11-22T01:23:45+05:00\", \"9999-12-21T01:23:45-05:00\"], \"array_timestamp_without_timezone\" : [\"2022-11-22T01:23:45\", \"1504-02-29T01:23:45\"], \"array_number\" : [\"56.78\", \"0\", \"-12345.678\"], \"array_integer\" : [\"42\", \"0\", \"12345\"], \"array_boolean\" : [true, false], \"array_binary_data\" : [\"dGVzdA==\"] }}}\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Integration Test Structure for Airbyte Connector\nDESCRIPTION: Provides a template for an Airbyte integration test class (`FullRefreshTest`) using `unittest.TestCase`, `HttpMocker` for mocking HTTP requests, and `freezegun` for time freezing. It includes placeholders (`<TODO>`) for configuration, current time, API URL, response body, and catalog stream configuration, outlining the structure for a single-page read test.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Copyright (c) 2023 Airbyte, Inc., all rights reserved.\n\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any, Dict, Mapping, Optional\nfrom unittest import TestCase\n\nimport freezegun\nfrom airbyte_cdk.sources.source import TState\nfrom airbyte_cdk.test.catalog_builder import CatalogBuilder\nfrom airbyte_cdk.test.entrypoint_wrapper import EntrypointOutput, read\nfrom airbyte_cdk.test.mock_http import HttpMocker, HttpRequest, HttpResponse\nfrom airbyte_protocol.models import ConfiguredAirbyteCatalog, SyncMode\nfrom source_survey_monkey_demo import SourceSurveyMonkeyDemo\n\n_A_CONFIG = {\n    <TODO>\n}\n_NOW = <TODO>\n\n@freezegun.freeze_time(_NOW.isoformat())\nclass FullRefreshTest(TestCase):\n\n    @HttpMocker()\n    def test_read_a_single_page(self, http_mocker: HttpMocker) -> None:\n\n        http_mocker.get(\n            HttpRequest(url=),\n            HttpResponse(body=, status_code=)\n        )\n\n        output = self._read(_A_CONFIG, _configured_catalog(<TODO>, SyncMode.full_refresh))\n\n        assert len(output.records) == 2\n\n    def _read(self, config: Mapping[str, Any], configured_catalog: ConfiguredAirbyteCatalog, expecting_exception: bool = False) -> EntrypointOutput:\n        return _read(config, configured_catalog=configured_catalog, expecting_exception=expecting_exception)\n\ndef _read(\n    config: Mapping[str, Any],\n    configured_catalog: ConfiguredAirbyteCatalog,\n    state: Optional[Dict[str, Any]] = None,\n    expecting_exception: bool = False\n) -> EntrypointOutput:\n    return read(_source(configured_catalog, config, state), config, configured_catalog, state, expecting_exception)\n\n\ndef _configured_catalog(stream_name: str, sync_mode: SyncMode) -> ConfiguredAirbyteCatalog:\n    return CatalogBuilder().with_stream(stream_name, sync_mode).build()\n\n\ndef _source(catalog: ConfiguredAirbyteCatalog, config: Dict[str, Any], state: Optional[TState]) -> SourceSurveyMonkeyDemo:\n    return SourceSurveyMonkeyDemo()\n```\n\n----------------------------------------\n\nTITLE: JWT Authenticator Schema and Implementation\nDESCRIPTION: Comprehensive schema and example for JWT authentication including token configuration, headers, and payload customization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nJwtAuthenticator:\n  title: JWT Authenticator\n  description: Authenticator for requests using JWT authentication flow.\n  type: object\n  required:\n    - type\n    - secret_key\n    - algorithm\n  properties:\n    type:\n      type: string\n      enum: [JwtAuthenticator]\n    secret_key:\n      type: string\n      description: Secret used to sign the JSON web token.\n      examples:\n        - \"{{ config['secret_key'] }}\"\n    base64_encode_secret_key:\n      type: boolean\n      description: When set to true, the secret key will be base64 encoded prior to being encoded as part of the JWT. Only set to \"true\" when required by the API.\n      default: False\n    algorithm:\n      type: string\n      description: Algorithm used to sign the JSON web token.\n      enum:\n        [\n          \"HS256\",\n          \"HS384\",\n          \"HS512\",\n          \"ES256\",\n          \"ES256K\",\n          \"ES384\",\n          \"ES512\",\n          \"RS256\",\n          \"RS384\",\n          \"RS512\",\n          \"PS256\",\n          \"PS384\",\n          \"PS512\",\n          \"EdDSA\",\n        ]\n      examples:\n        - ES256\n        - HS256\n        - RS256\n        - \"{{ config['algorithm'] }}\"\n    token_duration:\n      type: integer\n      title: Token Duration\n      description: The amount of time in seconds a JWT token can be valid after being issued.\n      default: 1200\n      examples:\n        - 1200\n        - 3600\n    header_prefix:\n      type: string\n      title: Header Prefix\n      description: The prefix to be used within the Authentication header.\n      examples:\n        - \"Bearer\"\n        - \"Basic\"\n    jwt_headers:\n      type: object\n      title: JWT Headers\n      description: JWT headers used when signing JSON web token.\n      additionalProperties: false\n      properties:\n        kid:\n          type: string\n          title: Key Identifier\n          description: Private key ID for user account.\n          examples:\n            - \"{{ config['kid'] }}\"\n        typ:\n          type: string\n          title: Type\n          description: The media type of the complete JWT.\n          default: JWT\n          examples:\n            - JWT\n        cty:\n          type: string\n          title: Content Type\n          description: Content type of JWT header.\n          examples:\n            - JWT\n    additional_jwt_headers:\n      type: object\n      title: Additional JWT Headers\n      description: Additional headers to be included with the JWT headers object.\n      additionalProperties: true\n    jwt_payload:\n      type: object\n      title: JWT Payload\n      description: JWT Payload used when signing JSON web token.\n      additionalProperties: false\n      properties:\n        iss:\n          type: string\n          title: Issuer\n          description: The user/principal that issued the JWT. Commonly a value unique to the user.\n          examples:\n            - \"{{ config['iss'] }}\"\n        sub:\n          type: string\n          title: Subject\n          description: The subject of the JWT. Commonly defined by the API.\n        aud:\n          type: string\n          title: Audience\n          description: The recipient that the JWT is intended for. Commonly defined by the API.\n          examples:\n            - \"appstoreconnect-v1\"\n    additional_jwt_payload:\n      type: object\n      title: Additional JWT Payload Properties\n      description: Additional properties to be added to the JWT payload.\n      additionalProperties: true\n    $parameters:\n      type: object\n      additionalProperties: true\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: JwtAuthenticator\n  secret_key: \"{{ config['secret_key'] }}\"\n  base64_encode_secret_key: True\n  algorithm: RS256\n  token_duration: 3600\n  header_prefix: Bearer\n  jwt_headers:\n    kid: \"{{ config['kid'] }}\"\n    cty: \"JWT\"\n  additional_jwt_headers:\n    test: \"{{ config['test']}}\"\n  jwt_payload:\n    iss: \"{{ config['iss'] }}\"\n    sub: \"sub value\"\n    aud: \"aud value\"\n  additional_jwt_payload:\n    test: \"test custom payload\"\n```\n\n----------------------------------------\n\nTITLE: Defining User Schemas for Airbyte File Connectors (JSON)\nDESCRIPTION: Provides examples of user-defined schemas in JSON format for Airbyte file connectors supporting Avro, Jsonl, CSV, or Parquet. A schema is a map of column names to data types (string, number, integer, object, array, boolean, null). Defining a schema gives explicit control over the output stream's structure and types, overriding automatic inference and potentially packing unspecified columns into `_ab_additional_properties`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-sharepoint.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"id\": \"integer\", \"location\": \"string\", \"longitude\": \"number\", \"latitude\": \"number\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"username\": \"string\", \"friends\": \"array\", \"information\": \"object\"}\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Airbyte Embedded Modal\nDESCRIPTION: Code example showing how to install and initialize the Airbyte Embedded Modal JavaScript library. The example includes installing the package via npm, importing the library, obtaining an authentication token, and initializing the modal with event handling for completed configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/embedded-setup/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @airbyte-embedded/airbyte-embedded-modal\n```\n\nLANGUAGE: ts\nCODE:\n```\n// import the library\nimport {\n  AirbyteEmbeddedModal,\n  getToken,\n} from \"@airbyte-embedded/airbyte-embedded-modal\";\n\n// get a token (in your backend)\nconst token = await getToken(oauthKey, oauthSecret);\n\n// Initialize the modal (in your frontend)\nconst modal = new AirbyteEmbeddedModal({\n  token, // from your backend\n  workspaceId,\n  onFlowComplete: (source, configTemplate, partialConfig) => {\n    // create the connection on your backend\n    await createConnection(source, configTemplate, partialConfig);\n  },\n});\n\nawait modal.open();\n```\n\n----------------------------------------\n\nTITLE: Creating a logical replication slot for PostgreSQL CDC\nDESCRIPTION: SQL commands to grant REPLICATION to the postgres user and create a logical replication slot using the pgoutput plugin for CDC.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER user postgres with REPLICATION;\nSELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');\n```\n\n----------------------------------------\n\nTITLE: Configuring Request Options with HTTP Requester - YAML\nDESCRIPTION: This YAML example demonstrates configuring an HttpRequester with custom request parameters and headers using a request_options_provider. Requires a valid Airbyte connector configuration and knowledge of the target API's interface. The parameters and headers are specified as key-value pairs to be added to outgoing requests. Inputs are the specified configuration keys and values; outputs are requests sent with these options set.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  type: HttpRequester\n  url_base: \"https://api.exchangeratesapi.io/v1/\"\n  http_method: \"GET\"\n  request_options_provider:\n    request_parameters:\n      k1: v1\n      k2: v2\n    request_headers:\n      header_key1: header_value1\n      header_key2: header_value2\n```\n\n----------------------------------------\n\nTITLE: Visualizing Connector Publishing Workflow using Mermaid\nDESCRIPTION: A Mermaid flowchart diagram illustrating the sequence of steps performed by the Airbyte CI pipeline when publishing a connector. It shows the flow from validation and checking existence, through building, uploading specs, publishing to registries, pushing/pulling the image, and finally uploading metadata.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_21\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    validate[Validate the metadata file]\n    check[Check if the connector image already exists]\n    build[Build the connector image for all platform variants]\n    publish_to_python_registry[Push the connector image to the python registry if enabled]\n    upload_spec[Upload connector spec to the spec cache bucket]\n    push[Push the connector image from DockerHub, with platform variants]\n    pull[Pull the connector image from DockerHub to check SPEC can be run and the image layers are healthy]\n    upload_metadata[Upload its metadata file to the metadata service bucket]\n\n    validate-->check-->build-->upload_spec-->publish_to_python_registry-->push-->pull-->upload_metadata\n```\n\n----------------------------------------\n\nTITLE: Example JSON Object Response - JSON\nDESCRIPTION: This JSON snippet represents a simple API response containing a single record (\"id\": 1). When used with a selector whose extractor field_path is empty, this record will be extracted and wrapped as a one-element array. Intended for demonstration of record extraction from a simple JSON object response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Postgres for Logical Replication\nDESCRIPTION: Azure CLI commands to enable logical replication on Azure Database for Postgres. This sets the replication support parameter to logical and restarts the server to apply the change.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\naz postgres server configuration set --resource-group group --server-name server --name azure.replication_support --value logical\naz postgres server restart --resource-group group --name server\n```\n\n----------------------------------------\n\nTITLE: Defining Field Order in Airbyte Connector Spec UI (JSON)\nDESCRIPTION: Illustrates using the `order` property within JSON Schema property definitions (in `spec.json`) to explicitly control the display order of fields within the Airbyte connector configuration UI. Fields with lower `order` values appear first. Fields without the `order` property are rendered last.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"username\": {\"type\": \"string\", \"order\": 1},\n  \"password\": {\"type\": \"string\", \"order\": 2},\n  \"cloud_provider\": {\n    \"order\": 0,\n    \"type\": \"object\",\n    \"properties\" : {\n      \"name\": {\"type\": \"string\", \"order\": 0},\n      \"region\": {\"type\": \"string\", \"order\": 1}\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Big Number Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for big number data tests in Airbyte's JSON record format. Includes examples with extremely large numbers stored as strings, zero, and negative decimal numbers with timestamps and stream identifiers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"bignumber_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.1234\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"bignumber_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"bignumber_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"-12345.678\" }}}\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteRecordMessage Structure in YAML\nDESCRIPTION: Specifies the structure of the AirbyteRecordMessage, which contains the actual data being replicated. It includes fields for namespace, stream, data, and emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteRecordMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - stream\n    - data\n    - emitted_at\n  properties:\n    namespace:\n      description: \"namespace the data is associated with\"\n      type: string\n    stream:\n      description: \"stream the data is associated with\"\n      type: string\n    data:\n      description: \"record data\"\n      type: object\n      existingJavaType: com.fasterxml.jackson.databind.JsonNode\n    emitted_at:\n      description: \"when the data was emitted from the source. epoch in millisecond.\"\n      type: integer\n```\n\n----------------------------------------\n\nTITLE: Running Klarna Source Connector Docker Commands\nDESCRIPTION: Commands to run various operations of the Klarna source connector using Docker, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klarna/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-klarna:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klarna:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klarna:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-klarna:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Common Spec Method in Airbyte Protocol\nDESCRIPTION: Defines the spec() method which allows an actor to broadcast information about itself and its configuration requirements. This method is implemented by both Source and Destination connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nspec() -> ConnectorSpecification\n```\n\n----------------------------------------\n\nTITLE: Creating Dedicated Airbyte User and Role in Snowflake\nDESCRIPTION: SQL commands to create a dedicated Airbyte role and user in Snowflake with the necessary permissions for data replication. This script sets up variables for the role, username, and password, then creates the role and user, and grants schema access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/snowflake.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- set variables (these need to be uppercase)\nSET AIRBYTE_ROLE = 'AIRBYTE_ROLE';\nSET AIRBYTE_USERNAME = 'AIRBYTE_USER';\n\n-- set user password\nSET AIRBYTE_PASSWORD = '-password-';\n\nBEGIN;\n\n-- create Airbyte role\nCREATE ROLE IF NOT EXISTS $AIRBYTE_ROLE;\n\n-- create Airbyte user\nCREATE USER IF NOT EXISTS $AIRBYTE_USERNAME\nPASSWORD = $AIRBYTE_PASSWORD\nDEFAULT_ROLE = $AIRBYTE_ROLE\nDEFAULT_WAREHOUSE= $AIRBYTE_WAREHOUSE;\n\n-- grant Airbyte schema access\nGRANT OWNERSHIP ON SCHEMA $AIRBYTE_SCHEMA TO ROLE $AIRBYTE_ROLE;\n\nCOMMIT;\n```\n\n----------------------------------------\n\nTITLE: Marking Fields as Secret in Airbyte Connector Spec (JSON)\nDESCRIPTION: Demonstrates adding the `airbyte_secret: true` annotation to a JSON Schema property definition within `spec.json`. This instructs the Airbyte UI and API to obfuscate the field's value, which is essential for sensitive inputs like passwords.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"password\": {\n  \"type\": \"string\",\n  \"examples\": [\"hunter2\"],\n  \"airbyte_secret\": true\n},\n```\n\n----------------------------------------\n\nTITLE: Example Pytest Fixture for Dynamic Resource Management (Python)\nDESCRIPTION: Shows an example Python pytest yield-fixture defined in `integration_tests/acceptance.py`. This fixture demonstrates how to set up external resources (e.g., starting a Docker container using the `docker` library) before tests run and tear them down afterwards, enabling dynamic resource management for acceptance tests. The `autouse=True` and `scope=\"session\"` ensure it runs once for the test session.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture(scope=\"session\", autouse=True)\ndef connector_setup():\n    \"\"\" This fixture is a placeholder for external resources that acceptance test might require.\n    \"\"\"\n    client = docker.from_env()\n    container = client.containers.run(\"your/docker-image\", detach=True)\n    yield\n    container.stop()\n```\n\n----------------------------------------\n\nTITLE: Customizing OAuth Flow with Client Secret in Request Headers - Diff/YAML\nDESCRIPTION: This unified diff snippet demonstrates how to move the client_secret from an OAuth token URL query parameter to a custom request header in the Airbyte connector manifest. The new manifest removes the client_secret from the access_token_url query and adds a SECRETHEADER under access_token_headers, which is templated for the value. Requires Airbyte's declarative manifest syntax and access to environment variables or config values for client ID and client secret. The input is a YAML manifest file, and the output is a modified manifest supporting header-based secret exchange.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n--- simple_oauth_manifest.yml\n+++ secret_header_manifest.yml\n      spec:\n        https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n        redirect_uri_value }}&state={{ state }}\n        access_token_url: >-\n-         https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n+         https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&code={{auth_code_value}}\n+       access_token_headers:\n+         SECRETHEADER: \"{{ client_secret_value }}\"\n       complete_oauth_output_specification:\n         required:\n```\n\n----------------------------------------\n\nTITLE: Defining Ambiguous Timestamp Schema in JSON\nDESCRIPTION: JSON schema example defining a \"created_at\" property with type string and format \"date-time\" but without the `airbyte_type` field. This illustrates the ambiguity of standard JSON schema regarding timezone inclusion, which `airbyte_type` resolves.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"created_at\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Read Operation - Airbyte CLI (bash)\nDESCRIPTION: This command executes the 'read' operation for a source connector, specifying both a config and a catalog JSON file as inputs. Outputs are streamed records or relevant errors. Prerequisites: The referenced config and catalog files must exist and be valid; the operation will fail if required properties are missing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Making Authenticated API Request with Bearer Token (curl)\nDESCRIPTION: Illustrates how to use the obtained OAuth access token to authenticate subsequent API requests. This `curl` command makes a GET request to a protected API resource, including the access token in the `Authorization` header using the `Bearer` scheme.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_5\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X GET \\\n  -H \"Authorization: Bearer <access-token>\" \\\n  https://connect.squareup.com/v2/<stream path>\n```\n\n----------------------------------------\n\nTITLE: Grouping Configuration Fields in Airbyte Connector Spec UI (JSON)\nDESCRIPTION: Shows an example `connectionSpecification` that uses the `group` property on individual fields and a top-level `groups` array to organize configuration fields into distinct cards in the Airbyte UI. The `groups` array defines the order and optional titles for these cards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"connectionSpecification\": {\n    \"type\": \"object\",\n    \"required\": [\"username\", \"namespace\", \"account_id\"],\n    \"properties\": {\n      \"username\": {\n        \"type\": \"string\",\n        \"title\": \"Username\",\n        \"order\": 1,\n        \"group\": \"auth\"\n      },\n      \"password\": {\n        \"type\": \"string\",\n        \"title\": \"Password\",\n        \"always_show\": true,\n        \"order\": 2,\n        \"group\": \"auth\"\n      },\n      \"namespace\": {\n        \"type\": \"string\",\n        \"title\": \"Namespace\",\n        \"order\": 1,\n        \"group\": \"location\"\n      },\n      \"region\": {\n        \"type\": \"string\",\n        \"title\": \"Region\",\n        \"order\": 2,\n        \"group\": \"location\"\n      },\n      \"account_id\": {\n        \"type\": \"integer\",\n        \"title\": \"Account ID\"\n      }\n    },\n    \"groups\": [\n      {\n        \"id\": \"auth\",\n        \"title\": \"Authentication\"\n      },\n      {\n        \"id\": \"location\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte Record Messages in JSONL\nDESCRIPTION: This snippet shows three example Airbyte record messages in JSON Lines (JSONL) format. Each line represents a distinct record belonging to the \"id_and_name_cat\" stream, containing an \"id\" and \"name\" field within the \"data\" payload, along with a common \"emitted_at\" timestamp (999999). This format is standard for data exchange in Airbyte pipelines between sources and destinations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postgres/integration_tests/expected_records.txt#2025-04-23_snippet_0\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"1\", \"name\": \"one\"}, \"emitted_at\": 999999}\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"2\", \"name\": \"two\"}, \"emitted_at\": 999999}\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"3\", \"name\": \"three\"}, \"emitted_at\": 999999}\n```\n\n----------------------------------------\n\nTITLE: Source Interface Method Signatures in Airbyte Protocol\nDESCRIPTION: Defines the interface methods that a Source connector must implement in the Airbyte Protocol, including spec(), check(), discover(), and read() methods that extract data from underlying data stores.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nspec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\ndiscover(Config) -> AirbyteCatalog\nread(Config, ConfiguredAirbyteCatalog, State) -> Stream<AirbyteRecordMessage | AirbyteStateMessage>\n```\n\n----------------------------------------\n\nTITLE: Defining an API Content Record Structure - JSON\nDESCRIPTION: This JSON snippet represents the structure of a content record retrieved from The Guardian's /search API endpoint. Each record contains metadata fields such as id, type, sectionId, sectionName, webPublicationDate, and webTitle. The structure is essential for configuring the Airbyte connector's cursor field (e.g., webPublicationDate) enabling incremental syncs based on timestamp comparison. No external dependencies are required, but the actual API response may include additional nested fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"world/2022/oct/21/russia-ukraine-war-latest-what-we-know-on-day-240-of-the-invasion\",\n    \"type\": \"article\",\n    \"sectionId\": \"world\",\n    \"sectionName\": \"World news\",\n    \"webPublicationDate\": \"2022-10-21T14:06:14Z\",\n    \"webTitle\": \"Russia-Ukraine war latest: what we know on day 240 of the invasion\",\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Langchain Vector Store with Milvus\nDESCRIPTION: This Python code snippet shows how to initialize a Langchain vector store using the indexed data in Milvus. It sets up the OpenAI embeddings and configures the Milvus vector store for similarity search.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/milvus.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings = OpenAIEmbeddings(openai_api_key=\"my-key\")\nvector_store = Milvus(embeddings=embeddings, collection_name=\"my-collection\", connection_args={\"uri\": \"my-zilliz-endpoint\", \"token\": \"my-api-key\"})\nvector_store.fields.append(\"text\")\n# call  vs.fields.append() for all fields you need from the metadata\n\nvector_store.similarity_search(\"test\")\n```\n\n----------------------------------------\n\nTITLE: Using the Tags Section in metadata.yaml (YAML)\nDESCRIPTION: This snippet demonstrates how to use the optional `tags` section in `metadata.yaml`. It shows a list of tags, including `language:` prefixes to denote programming languages (e.g., `language:java`) and `keyword:` prefixes for searchable terms (e.g., `keyword:database`, `keyword:SQL`), enhancing connector discoverability.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntags:\n  - \"language:java\"\n  - \"keyword:database\"\n  - \"keyword:SQL\"\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Access Token, One-Time Refresh Token, and Expiry Date - YAML\nDESCRIPTION: Outlines a more complex OAuth2.0 configuration including the mapping of access_token, one-time-use refresh_token, and token_expiry_date (from expires_in). The YAML illustrates mapping to connector config and extracting fields from the OAuth response. Inputs must include all three keys; outputs follow the specification for token management.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_72\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - access_token\\n      - refresh_token\\n      - token_expiry_date\\n    properties:\\n      access_token:\\n        type: string\\n        path_in_connector_config:\\n          - access_token\\n        path_in_oauth_response:\\n          - access_token\\n      refresh_token:\\n        type: string\\n        path_in_connector_config:\\n          - refresh_token\\n        path_in_oauth_response:\\n          - refresh_token\\n      token_expiry_date:\\n        type: string\\n        path_in_connector_config:\\n          - token_expiry_date\\n        path_in_oauth_response:\\n          - expires_in\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Defining JSONSchema for Encoded JavaScript Set - JSON\nDESCRIPTION: This JSON schema defines the encoding method for storing a JavaScript Set within JSON data transferred by the Convex source connector. The schema marks the structure as an object with a $set property, specifying an array of string items. This snippet provides structural guidance for schema validation and downstream consumers requiring typed data extracts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/convex.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"description\": \"Set\",\n  \"properties\": {\n    \"$set\": {\n      \"type\": \"array\",\n      \"items\": { \"type\": \"string\" }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining User Schemas for Airbyte File Connector (JSON)\nDESCRIPTION: Demonstrates how to define a user schema using a JSON object mapping column names to Airbyte data types (string, number, integer, object, array, boolean, null). This schema overrides automatic inference and controls the output stream structure for formats like Avro, Jsonl, CSV, and Parquet.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-onedrive.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"id\": \"integer\", \"location\": \"string\", \"longitude\": \"number\", \"latitude\": \"number\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"username\": \"string\", \"friends\": \"array\", \"information\": \"object\"}\n```\n\n----------------------------------------\n\nTITLE: Launching the Airbyte Source via Run Script in Python\nDESCRIPTION: This is the standard Airbyte entrypoint script to create, configure, and launch a connector. It handles extracting config/state paths, error handling for invalid configurations, and launches the Airbyte source using the CDK. Dependencies are Airbyte's `entrypoint` and `models` modules. Inputs are command-line arguments; outputs are process start or error serialization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#\\n# Copyright (c) 2023 Airbyte, Inc., all rights reserved.\\n#\\n\\nimport sys\\nimport traceback\\nfrom datetime import datetime\\nfrom typing import List\\n\\nfrom airbyte_cdk.entrypoint import AirbyteEntrypoint, launch\\nfrom airbyte_cdk.models import AirbyteErrorTraceMessage, AirbyteMessage, AirbyteTraceMessage, TraceType, Type\\n\\nfrom .source import SourceSurveyMonkeyDemo\\n\\ndef _get_source(args: List[str]):\\n    config_path = AirbyteEntrypoint.extract_config(args)\\n    state_path = AirbyteEntrypoint.extract_state(args)\\n    try:\\n        return SourceSurveyMonkeyDemo(\\n            SourceSurveyMonkeyDemo.read_config(config_path) if config_path else None,\\n            SourceSurveyMonkeyDemo.read_state(state_path) if state_path else None,\\n        )\\n    except Exception as error:\\n        print(\\n            AirbyteMessage(\\n                type=Type.TRACE,\\n                trace=AirbyteTraceMessage(\\n                    type=TraceType.ERROR,\\n                    emitted_at=int(datetime.now().timestamp() * 1000),\\n                    error=AirbyteErrorTraceMessage(\\n                        message=f\"Error starting the sync. This could be due to an invalid configuration or catalog. Please contact Support for assistance. Error: {error}\",\\n                        stack_trace=traceback.format_exc(),\\n                    ),\\n                ),\\n            ).json()\\n        )\\n        return None\\n\\n\\ndef run():\\n    args = sys.argv[1:]\\n    source = _get_source(args)\\n    launch(source, args)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Airbyte Component Interactions using Mermaid\nDESCRIPTION: This Mermaid flowchart diagram illustrates the high-level architecture and interaction flow between the core components of the Airbyte platform. It shows how user interactions via the Web App trigger API calls, leading to configuration storage, workflow creation via Temporal, task execution by Workers, job creation via the Workload API, queuing, and finally, operation pod launching by the Launcher.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/high-level-view.md#2025-04-23_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n```mermaid\n---\ntitle: Architecture Overview\n---\n%%{init: {\"flowchart\": {\"defaultRenderer\": \"elk\"}} }%%\nflowchart LR\n    W[fa:fa-display WebApp/UI]\n    S[fa:fa-server Config API Server]\n    D[(fa:fa-table Config & Jobs)]\n    L[(fa:fa-server Launcher)]\n    OP[(fa:fa-superpowers Operation pod)]\n    Q[(fa:fa-superpowers Queue)]\n    T(fa:fa-calendar Temporal/Scheduler)\n    W2[1..n Airbyte Workers]\n    WL[fa:fa-server Workload API Server]\n    \n    W -->|sends API requests| S\n    S -->|store data| D\n    S -->|create workflow| T\n    T -->|launch task| W2\n    W2 -->|return status| T\n    W2 -->|creates job| WL\n    WL -->|queues workload| Q\n    Q  -->|reads from| L\n    L -->|launches| OP\n    O -->|reports status to| WL\n```\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands for Retently\nDESCRIPTION: Set of Docker commands to run the standard connector operations (spec, check, discover, read) for the Retently source connector. These commands use Docker volume mounts to access configuration files and test catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-retently/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-retently:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-retently:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-retently:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-retently:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring SubstreamPartitionRouter with Path Injection in YAML\nDESCRIPTION: Illustrates a YAML configuration for `SubstreamPartitionRouter` where the partition value (repository name) is injected directly into the request URL path instead of as a parameter. The `requester`'s `path` uses Jinja-like templating (`{{ stream_slice.repository }}`) to incorporate the `partition_field` value from the parent stream slice. No separate `request_option` is defined as injection happens via the path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nretriever:\n  <...>\n  requester:\n    <...>\n    path: \"/respositories/{{ stream_slice.repository }}/commits\"\n  partition_router:\n    type: SubstreamPartitionRouter\n    parent_streams_configs: # NOTE: schema uses 'parent_stream_configs', example uses 'parent_streams_configs'\n      - stream: \"#/repositories_stream\"\n        parent_key: \"id\"\n        partition_field: \"repository\"\n        incremental_dependency: true\n```\n\n----------------------------------------\n\nTITLE: Accessing Airbyte Pod Shell on Kubernetes\nDESCRIPTION: Demonstrates the `kubectl exec` command to get an interactive bash shell (`-it ... bash`) inside a specific Airbyte worker pod (`<pod name>`) running in a given namespace (`<namespace pod is in>`). This allows browsing the pod's filesystem, including logs and configuration files located typically under `/config` or `/tmp/workspace`. A specific example targeting a BigQuery destination worker pod is included.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it <pod name> -n <namespace pod is in> -c main bash\ne.g.\nkubectl exec -it destination-bigquery-worker-3607-0-chlle  -n jobs  -c main bash\n```\n\n----------------------------------------\n\nTITLE: Running Chartmogul Source Connector as Docker Container (Bash)\nDESCRIPTION: This group of commands shows how to invoke the manifest-only Chartmogul source connector inside a Docker container using various operational modes. The commands make use of Docker\\'s `run` command, with arguments such as `spec`, `check`, `discover`, and `read`, as required by the Airbyte connector protocol. Dependencies include a built Docker image (`airbyte/source-chartmogul:dev`), and appropriate credentials/configuration in the mounted `secrets` directory. Expected inputs include optional configuration files (`/secrets/config.json`, `/integration_tests/configured_catalog.json`). Outputs depend on the operation: spec yields the connector spec, check validates credentials, discover outputs the stream catalog, and read performs data synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chartmogul/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-chartmogul:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chartmogul:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chartmogul:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-chartmogul:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to execute the Fauna connector's main operations: spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config examples/config_localhost.json\npython main.py discover --config examples/config_localhost.json\npython main.py read --config examples/config_localhost.json --catalog examples/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-zenloop spec\npoetry run source-zenloop check --config secrets/config.json\npoetry run source-zenloop discover --config secrets/config.json\npoetry run source-zenloop read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Elasticsearch Connector Docker Commands\nDESCRIPTION: Various Docker commands for running the connector's spec, check, discover, and read operations with configuration files\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-elasticsearch/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-elasticsearch:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-elasticsearch:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-elasticsearch:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-elasticsearch:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Source with OAuth Authentication in JSON\nDESCRIPTION: This JSON configuration sets up the Snowflake source connector using OAuth authentication. It includes host, role, warehouse, database, schema, and OAuth credential details such as client ID, client secret, and refresh token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-snowflake/README.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"host\": \"ACCOUNT.REGION.PROVIDER.snowflakecomputing.com\",\n  \"role\": \"AIRBYTE_ROLE\",\n  \"warehouse\": \"AIRBYTE_WAREHOUSE\",\n  \"database\": \"AIRBYTE_DATABASE\",\n  \"schema\": \"AIRBYTE_SCHEMA\",\n  \"credentials\": {\n    \"auth_type\": \"OAuth\",\n    \"client_id\": \"client_id\",\n    \"client_secret\": \"client_secret\",\n    \"refresh_token\": \"refresh_token\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Type Transformer in Python Stream Class\nDESCRIPTION: Demonstrates how to configure the TypeTransformer for automatic schema normalization in a stream class.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/schemas.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airbyte_cdk.sources.utils.transform import TransformConfig, Transformer\nfrom airbyte_cdk.sources.streams.core import Stream\n\nclass MyStream(Stream):\n    ...\n    transformer = Transformer(TransformConfig.DefaultSchemaNormalization)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Using join Filter in Jinja2\nDESCRIPTION: Demonstrates the `join` filter in Jinja2, which concatenates the items of a sequence into a single string, using a specified separator. The example joins the list `['a', 'b', 'c']` with ', '.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_30\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ ['a', 'b', 'c']|join(', ') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ingress for Airbyte Self-Managed Enterprise\nDESCRIPTION: Kubernetes Ingress configuration for Airbyte Self-Managed Enterprise that includes the new /auth path required for the Enterprise version, along with routes for the webapp, Keycloak authentication, and server API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrading-from-community.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: # ingress name, example: enterprise-demo\n  annotations:\n    ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: # host, example: enterprise-demo.airbyte.com\n      http:\n        paths:\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte-webapp-svc\n                name: airbyte-enterprise-airbyte-webapp-svc\n                port:\n                  number: 80 # service port, example: 8080\n            path: /\n            pathType: Prefix\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte-keycloak-svc\n                name: airbyte-enterprise-airbyte-keycloak-svc\n                port:\n                  number: 8180\n            path: /auth\n            pathType: Prefix\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte--server-svc\n                name: airbyte-enterprise-airbyte-server-svc\n                port:\n                  number: 8001\n            path: /api/public\n            pathType: Prefix\n```\n\n----------------------------------------\n\nTITLE: Checking SQL Server Agent Status\nDESCRIPTION: SQL command to query the current state of the SQL Server Agent service\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nEXEC xp_servicecontrol 'QueryState', N'SQLServerAGENT';\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Availability Strategy (Python)\nDESCRIPTION: Overrides the `availability_strategy` property to return `None`. This disables the default Airbyte CDK mechanism for checking stream availability based on user permissions, allowing for custom error handling logic within the stream methods.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    @property\n    def availability_strategy(self) -> Optional[AvailabilityStrategy]:\n        return None\n\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteCatalog for Dynamic API Streams - Javascript (JSON Syntax)\nDESCRIPTION: This snippet specifies an AirbyteCatalog as a JSON object modeling multiple dynamically named streams (e.g., 'TSLA', 'FB'), each representing data for a specific stock symbol. Each stream includes symbol, price, and date, all typed via JSON Schema. Inputs: List of target stock symbols; Outputs: AirbyteCatalog with one stream per symbol. No dependencies; suitable for dynamic multi-stream API data sources.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_4\n\nLANGUAGE: Javascript\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"TSLA\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"FB\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Warning Log for Missing Hubspot Stream Permissions (JSON)\nDESCRIPTION: Illustrates a typical warning log message generated by Airbyte when attempting to sync a Hubspot stream (e.g., `workflows`) for which the provided API Key lacks the necessary permissions (e.g., `automation-access`). The sync will skip this stream but continue with others.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hubspot.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"LOG\",\n  \"log\": {\n    \"level\": \"WARN\",\n    \"message\": \"Stream `workflows` cannot be proceed. This API Key (EXAMPLE_API_KEY) does not have proper permissions! (requires any of [automation-access])\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Record Reading with State Updates\nDESCRIPTION: Example showing how to update stream state while reading records incrementally. The cursor value is updated after processing batches of records to maintain the latest sync position.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/incremental-stream.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef read_records(self, ...):\n   ...\n   yield record\n   yield record\n   yield record\n   self._cursor_value = max(record[self.cursor_field], self._cursor_value)\n   yield record\n   yield record\n   yield record\n   self._cursor_value = max(record[self.cursor_field], self._cursor_value)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated Read-Only User in CockroachDB - SQL\nDESCRIPTION: This snippet demonstrates how to create a new CockroachDB user with a specified password for use by the Airbyte connector. It requires CockroachDB v1.15.x or above and should be executed by a database administrator. The command creates a new user named 'airbyte'; replace 'your_password_here' with a secure password. Successful execution adds a new database principal for subsequent privilege assignment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/cockroachdb.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte PASSWORD 'your_password_here';\n```\n\n----------------------------------------\n\nTITLE: Generating Nullable Avro Schema for Time-Micros Logical Type\nDESCRIPTION: Illustrates the final Avro schema generated for a JSON field originally defined with `type: \"string\"` and `format: \"time\"`. It defines a union type allowing either `null` or an Avro `long` annotated with the `time-micros` logical type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"long\",\n      \"logicalType\": \"time-micros\"\n    }\n  ]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Handling Schemaless Objects in Airbyte (New Behavior)\nDESCRIPTION: Demonstrates the new handling of schemaless objects (defined with `\"type\": \"object\"` but no `properties`). Instead of attempting to infer a schema or storing extra fields, the entire object is serialized into a JSON string. The output schema type becomes 'string'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"object\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n{ \"id\": 1, \"name\": \"Alice\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Output Schema\n{ \"type\": \"string\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Output Data\n\"{\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\"}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SurveyMonkey API Access Token in JSON\nDESCRIPTION: This JSON snippet shows the structure for configuring the SurveyMonkey API access token in the secrets file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"<TODO>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor` table, representing a specific instance of a source or destination connector within a workspace. It contains details like workspace ID, actor definition ID, name, configuration, type (source/destination), soft delete flag, timestamps, and resource requirements. Foreign keys link to `workspace` and `actor_definition`. Indexes are defined on `actor_definition_id` and `workspace_id`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name            | Datatype     | Description                                             |\n| ---------------------- | ------------ | ------------------------------------------------------- |\n| id                     | UUID         | Primary key. Unique identifier for the actor.          |\n| workspace_id           | UUID         | Foreign key referencing the `workspace` table.         |\n| actor_definition_id    | UUID         | Foreign key referencing `actor_definition` table.      |\n| name                   | VARCHAR(256) | Name of the actor.                                     |\n| configuration          | JSONB        | Configuration JSON blob specific to the actor.         |\n| actor_type             | ENUM         | Indicates whether the actor is a source or destination.|\n| tombstone              | BOOLEAN      | Soft delete flag.                                      |\n| created_at             | TIMESTAMP    | Timestamp when the record was created.                 |\n| updated_at             | TIMESTAMP    | Timestamp when the record was last updated.            |\n| resource_requirements  | JSONB        | Defines resource requirements for the actor.           |\n\n#### Indexes and Constraints\n\n- Primary Key: (`id`)\n- Foreign Key: `workspace_id` references `workspace(id)`\n- Foreign Key: `actor_definition_id` references `actor_definition(id)`\n- Index: `actor_definition_id_idx` on (`actor_definition_id`)\n- Index: `actor_workspace_id_idx` on (`workspace_id`)\n```\n\n----------------------------------------\n\nTITLE: Running Gnews Source Connector Container - Bash\nDESCRIPTION: This set of commands illustrates common ways to run the Gnews source connector using Docker, including invoking spec, check, discover, and read operations. The commands mount local directories for secrets and integration tests, passing configuration files to the container as needed. Inputs include paths to config.json and configured_catalog.json files. Ensure the necessary config and catalog files are in place and that Docker is installed. These commands are intended for local manual runs and should not be used in production deployments without further customization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gnews/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gnews:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gnews:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gnews:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gnews:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Airbyte Connector Best Practices\nDESCRIPTION: Comprehensive guide detailing best practices for Airbyte connector development, including principles, quality certification checklist, testing requirements, connection validation, and rate limiting considerations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/best-practices.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Best Practices\n\nIn order to guarantee the highest quality for connectors, we've compiled the following best practices for connector development. Connectors which follow these best practices will be labelled as \"Airbyte Certified\" to indicate they've passed a high quality bar and will perform reliably in all production use cases. Following these guidelines is **not required** for your contribution to Airbyte to be accepted, as they add a barrier to entry for contribution \\(though adopting them certainly doesn't hurt!\\).\n\n## Principles of developing connectors\n\n1. **Reliability + usability &gt; more features.** It is better to support 1 feature that works reliably and has a great UX than 2 that are unreliable or hard to use. One solid connector is better than 2 finicky ones.\n2. **Fail fast.** A user should not be able to configure something that will not work.\n3. **Fail actionably.** If a failure is actionable by the user, clearly let them know what they can do. Otherwise, make it very easy for them to give us necessary debugging information \\(logs etc.\\)\n\nFrom these principles we extrapolate the following goals for connectors, in descending priority order:\n\n1. **Correct user input should result in a successful sync.** If there is an issue, it should be extremely easy for the user to see and report.\n2. **Issues arising from bad user input should print an actionable error message.** \"Invalid credentials\" is not an actionable message. \"Please verify your username/password is correct\" is better.\n3. **Wherever possible, a connector should support incremental sync.** This prevents excessive load on the underlying data source. _\\*\\*_\n4. **When running a sync, a connector should communicate its status frequently to provide clear feedback that it is working.** Output a log message at least every 5 minutes.\n5. **A connector should allow reading or writing as many entities as is feasible.** Supporting syncing all entities from an API is preferred to only supporting a small subset which would satisfy narrow use cases. Similarly, a database should support as many data types as is feasible.\n\nNote that in the above list, the _least_ important is the number of features it has \\(e.g: whether an API connector supports all entities in the API\\). The most important thing is that for its declared features, it is reliable and usable. The only exception are \"minimum viability\" features e.g: for some sources, it's not feasible to pull data without incremental due to rate limiting issues. In this case, those are considered usability issues.\n\n## Quality certification checklist\n\nWhen reviewing connectors, we'll use the following \"checklist\" to verify whether the connector is considered \"Airbyte certified\" or closer to beta or alpha:\n\n### Integration Testing\n\n**As much as possible, prove functionality via testing**. This means slightly different things depending on the type of connector:\n\n- **All connectors** must test all the sync modes they support during integration tests\n- **Database connectors** should test that they can replicate **all** supported data types in both `read` and `discover` operations\n- **API connectors** should validate records that every stream outputs data\n  - If this causes rate limiting problems, there should be a periodic CI build which tests this on a less frequent cadence to avoid rate limiting\n\n**Thoroughly test edge cases.** While Airbyte provides a [Standard Test Suite](testing-connectors/connector-acceptance-tests-reference.md) that all connectors must pass, it's not possible for the standard test suite to cover all edge cases. When in doubt about whether the standard tests provide sufficient evidence of functionality, write a custom test case for your connector.\n\n### Check Connection\n\n- **Verify permissions upfront**. The \"check connection\" operation should verify any necessary permissions upfront e.g: the provided API token has read access to the API entities.\n  - In some cases it's not possible to verify permissions without knowing which streams the user wants to replicate. For example, a provided API token only needs read access to the \"Employees\" entity if the user wants to replicate the \"Employees\" stream. In this case, the CheckConnection operation should verify the minimum needed requirements \\(e.g: the API token exists\\), and the \"read\" or \"write\" operation should verify all needed permissions based on the provided catalog, failing if a required permission is not granted.\n- **Provide actionable feedback for incorrect input.**\n  - Examples of non actionable error messages\n    - \"Can't connect\". The only recourse this gives the user is to guess whether they need to dig through logs or guess which field of their input configuration is incorrect.\n  - Examples of actionable error messages\n    - \"Your username/password combination is incorrect\"\n    - \"Unable to reach Database host: please verify that there are no firewall rules preventing Airbyte from connecting to the database\"\n    - etc...\n\n### Rate Limiting\n\nMost APIs enforce rate limits. Your connector should gracefully handle those \\(i.e: without failing the connector process\\). The most common way to handle rate limits is to implement backoff.\n\n## Maintaining connectors\n\nOnce a connector has been published for use within Airbyte, we must take special care to account for the customer impact of updates to the connector.\n```\n\n----------------------------------------\n\nTITLE: Enabling Table-Level Supplemental Logging for CDC in Oracle SQL\nDESCRIPTION: This SQL command enables supplemental logging for all columns on a specific table. This is a prerequisite for using Change Data Capture (CDC) incremental syncs with the Airbyte Oracle source connector, as it ensures that LogMiner captures the necessary data for all changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ... ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;\n```\n\n----------------------------------------\n\nTITLE: Wrapping and Configuring Streams for Concurrency Using Airbyte Python CDK\nDESCRIPTION: The `streams` method decorates one or more stream objects with concurrency facades and concurrently managed cursors. It handles authentication, state management, and incremental/final state logic to prepare streams for parallel reading. Requires Airbyte CDK classes for streaming, cursors, and authentication. Inputs include connector config/state, and outputs are ready-to-use, concurrency-enabled stream objects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    def streams(self, config: Mapping[str, Any]) -> List[Stream]:\\n        auth = TokenAuthenticator(config[\"access_token\"])\\n\\n        survey_stream = SurveyMonkeyBaseStream(name=\"surveys\", path=\"/v3/surveys\", primary_key=\"id\", data_field=\"data\", authenticator=auth, cursor_field=\"date_modified\")\\n        synchronous_streams = [\\n            survey_stream,\\n            SurveyMonkeySubstream(name=\"survey_responses\", path=\"/v3/surveys/{stream_slice[id]}/responses/\", primary_key=\"id\", authenticator=auth, parent_stream=survey_stream)\\n        ]\\n        state_manager = ConnectorStateManager(stream_instance_map={s.name: s for s in synchronous_streams}, state=self._state)\\n\\n        configured_streams = []\\n\\n        for stream in synchronous_streams:\\n\\n            if stream.cursor_field:\\n                cursor_field = CursorField(stream.cursor_field)\\n                legacy_state = state_manager.get_stream_state(stream.name, stream.namespace)\\n                cursor = ConcurrentCursor(\\n                    stream.name,\\n                    stream.namespace,\\n                    legacy_state,\\n                    self.message_repository,\\n                    state_manager,\\n                    stream.state_converter,\\n                    cursor_field,\\n                    self._get_slice_boundary_fields(stream, state_manager),\\n                    pendulum.from_timestamp(_START_DATE),\\n                    EpochValueConcurrentStreamStateConverter.get_end_provider()\\n                )\\n            else:\\n                cursor = FinalStateCursor(stream.name, stream.namespace, self.message_repository)\\n            configured_streams.append (\\n                StreamFacade.create_from_stream(stream,\\n                                                self,\\n                                                _logger,\\n                                                legacy_state,\\n                                                cursor)\\n                )\\n        return configured_streams\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Access Token and Refresh Token in Response - YAML\nDESCRIPTION: This YAML specifies both access_token and refresh_token as required outputs, mapping each to the connector config and each expected at the root of the OAuth server response. These fields are used by Airbyte for ongoing authentication and refresh operations. Inputs must provide both tokens in the root JSON response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_68\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - access_token\\n      - refresh_token\\n    properties:\\n      access_token:\\n        type: string\\n        path_in_connector_config:\\n          - access_token\\n        path_in_oauth_response:\\n          - access_token\\n      refresh_token:\\n        type: string\\n        path_in_connector_config:\\n          - refresh_token\\n        path_in_oauth_response:\\n          - refresh_token\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Example Record Structure in JSON\nDESCRIPTION: Illustrates an example JSON record that an Airbyte stream might produce. This structure is used as the basis for the corresponding JSON Schema definition shown later.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/schema-reference.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"hashidstring\",\n  \"date_created\": \"2022-11-22T01:23:45\",\n  \"date_updated\": \"2023-12-22T01:12:00\",\n  \"total\": 1000,\n  \"status\": \"published\",\n  \"example_obj\": {\n    \"steps\": \"walking\",\n    \"count_steps\": 30\n  },\n  \"example_string_array\": [\"first_string\", \"second_string\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Making a Request with API Key Authentication (Header) using cURL\nDESCRIPTION: This cURL command shows how to make a GET request to an API (CoinAPI.io example) using API Key authentication where the key is passed in a custom HTTP header ('X-CoinAPI-Key'). The specific header name and injection method (header, query parameter, or body) are configured in the connector builder. Replace `<api-key>` with the actual key and `<stream path>` with the desired API endpoint.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_2\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X GET \\\n  -H \"X-CoinAPI-Key: <api-key>\" \\\n  https://rest.coinapi.io/v1/<stream path>\n```\n\n----------------------------------------\n\nTITLE: Defining Tinyemail API Streams in Markdown\nDESCRIPTION: Specifies the available streams for the Tinyemail API connector, including their primary keys, pagination methods, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tinyemail.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| campaigns | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | No pagination | ✅ |  ❌  |\n| sender_details | id | No pagination | ✅ |  ❌  |\n| contact_members |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Initializing DocArrayHnswSearch Vector Store with LangChain in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a LangChain QA chain using DocArrayHnswSearch as the vector store. It sets up OpenAI embeddings, creates a DocArrayHnswSearch vector store with a local directory, and initializes a RetrievalQA chain.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/langchain.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import OpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import DocArrayHnswSearch\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nvector_store = DocArrayHnswSearch.from_params(embeddings, \"/tmp/airbyte_local/<your configured directory>\", 1536)\n\nqa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever())\n```\n\n----------------------------------------\n\nTITLE: Granting CDC Schema Access in MSSQL\nDESCRIPTION: SQL command to grant SELECT permissions on CDC schema to the user\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nUSE {database name};\nGRANT SELECT ON SCHEMA :: [cdc] TO {user name};\n```\n\n----------------------------------------\n\nTITLE: Global Substream Cursor State Format for Large Partitioned Streams (Airbyte JSON)\nDESCRIPTION: This JSON array represents a global cursor used for maintaining sync state across all partitions when dealing with very large parent streams. Only one state object is maintained, reducing memory and state complexity at the cost of granularity. Must be used cautiously: some records may be missed if API does not guarantee strong cursor semantics. Input: global timestamp. Output: single checkpoint for entire sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  { \"timestamp\": \"2024-08-01\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Stream Implementation Reference\nDESCRIPTION: Reference to AbstractSource's streams function implementation that defines available data streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/basic-concepts.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# From airbyte_cdk/sources/abstract_source.py\nstreams()\n```\n\n----------------------------------------\n\nTITLE: Per Partition with Fallback to Global Sync State Structure (Airbyte JSON)\nDESCRIPTION: This JSON object illustrates the default sync state structure used by Airbyte for partitioned streams using per-partition cursors, with a fallback to a global cursor if partition count or other constraints are exceeded. The 'states' array maintains cursor states per partition, while the top-level 'state' object holds a global cursor field. 'use_global_cursor' signals the current state mode. Prerequisite: connector must support returning state messages. Input: cursor values per partition. Output: resumable state snapshot for next sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"states\": [\n        {\"partition_key\": \"partition_1\", \"cursor_field\": \"2021-01-15\"},\n        {\"partition_key\": \"partition_2\", \"cursor_field\": \"2021-02-14\"}\n    ],\n    \"state\": {\n        \"cursor_field\": \"2021-02-15\"\n    },\n    \"use_global_cursor\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Defining GCP Service Account Secret for Kubernetes - YAML\nDESCRIPTION: This YAML manifest defines a Kubernetes Secret that contains a GCP service account credentials JSON blob under the gcp.json key, intended for Airbyte deployments using Google Secret Manager. The gcp.json entry must contain fields such as type, project_id, private_key_id, private_key, client_email, client_id, and various OAuth URLs. This enables Airbyte to authenticate to GCP services when retrieving or storing secrets through Google Secret Manager. All sensitive fields must be filled in appropriately. Inputs: GCP service account JSON. Outputs: A Kubernetes Secret object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: airbyte-config-secrets\\ntype: Opaque\\nstringData:\\n  gcp.json: ## {\\n  \\\"type\\\": \\\"service_account\\\",\\n  \\\"project_id\\\": \\\"cloud-proj\\\",\\n  \\\"private_key_id\\\": \\\"2f3b9c8e7d5a1b4f23e697c0d84af6e1\\\",\\n  \\\"private_key\\\": \\\"-----BEGIN PRIVATE KEY-----<REDACTED>\\\\n-----END PRIVATE KEY-----\\\\n\\\",\\n  \\\"client_email\\\": \\\"cloud-proj.iam.gserviceaccount.com\\\",\\n  \\\"client_id\\\": \\\"9876543210987654321\\\",\\n  \\\"auth_uri\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\",\\n  \\\"token_uri\\\": \\\"https://oauth2.googleapis.com/token\\\",\\n  \\\"auth_provider_x509_cert_url\\\": \\\"https://www.googleapis.com/oauth2/v1/certs\\\",\\n  \\\"client_x509_cert_url\\\": \\\"https://www.googleapis.com/robot/v1/metadata/x509/cloud-proj.iam.gserviceaccount.com\\\"\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteCatalog for Database Source - Javascript (JSON Syntax)\nDESCRIPTION: This snippet defines an AirbyteCatalog as a JSON object representing two relational database tables ('airlines' and 'pilots') as Airbyte streams. Each stream specifies supported sync modes, source-defined cursor status, and uses an embedded JSON Schema to describe fields/columns. This is typically the output of the 'discover' method in an Airbyte source connector. Dependencies: None. Inputs: table structure; Output: AirbyteCatalog JSON object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_1\n\nLANGUAGE: Javascript\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"airlines\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"number\"\n          },\n          \"name\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"pilots\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"number\"\n          },\n          \"airline_id\": {\n            \"type\": \"number\"\n          },\n          \"name\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using safe Filter in Jinja2\nDESCRIPTION: Demonstrates the `safe` filter in Jinja2, which marks a string as safe, meaning it should not be automatically HTML-escaped if autoescaping is enabled. The example marks '<div>' as safe.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_42\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '<div>'|safe }}\n```\n\n----------------------------------------\n\nTITLE: Configuring SubstreamPartitionRouter Example in YAML\nDESCRIPTION: Provides a basic YAML example configuration for `SubstreamPartitionRouter`. It demonstrates setting up a dependency on a parent stream (`#/repositories_stream`), linking via `parent_key` (\"id\"), defining the partition field (`partition_field`: \"repository\"), and injecting this field as a request parameter using `request_option`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npartition_router:\n  type: SubstreamPartitionRouter\n  parent_stream_configs:\n    - stream: \"#/repositories_stream\"\n      parent_key: \"id\"\n      partition_field: \"repository\"\n      request_option:\n        type: RequestOption\n        field_name: \"repository\"\n        inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Testing the Ruddr Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Ruddr connector using airbyte-ci, which validates the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ruddr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ruddr test\n```\n\n----------------------------------------\n\nTITLE: Building Cin7 Source Connector Image (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-cin7` connector. It requires `airbyte-ci` to be installed. Executing this command creates a Docker image tagged as `source-cin7:dev` which can then be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cin7/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cin7 build\n```\n\n----------------------------------------\n\nTITLE: Querying CDC Captured Columns\nDESCRIPTION: SQL command to retrieve information about which columns are being captured by CDC for a specific capture instance.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql/mssql-troubleshooting.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sys.sp_cdc_get_captured_columns \n    @capture_instance = N'<capture instance (typically schema_table)>';\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Docker image for the Wikipedia Pageviews connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wikipedia-pageviews/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wikipedia-pageviews build\n```\n\n----------------------------------------\n\nTITLE: GA4 Custom Report with Segments and Filters\nDESCRIPTION: Example demonstrating how to add segments and filters to a custom report for filtering specific data subsets.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\"ga:date\", \"ga:pagePath\", \"ga:segment\"],\n    \"metrics\": [\"ga:sessions\", \"ga:totalUsers\"],\n    \"segments\": [\"ga:sessionSource!=(direct)\"],\n    \"filter\": [\"ga:sessionSource!=(direct);ga:sessionSource!=(not set)\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Base Authenticator Schema in YAML\nDESCRIPTION: The base schema for authenticator configuration that supports multiple authentication types including OAuth, API Key, Bearer, and Basic HTTP authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nAuthenticator:\n  type: object\n  description: \"Authenticator type\"\n  anyOf:\n    - \"$ref\": \"#/definitions/OAuth\"\n    - \"$ref\": \"#/definitions/ApiKeyAuthenticator\"\n    - \"$ref\": \"#/definitions/BearerAuthenticator\"\n    - \"$ref\": \"#/definitions/BasicHttpAuthenticator\"\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Only Access Token in Response - YAML\nDESCRIPTION: This YAML configuration shows how to set up the OAuth connector such that the response is expected to have only the access_token at the root. The required and mapped properties ensure Airbyte knows where to fetch and store the token. It is dependent on the Airbyte connector framework and expects access_token as a string in both response and config.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_62\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - access_token\\n    properties:\\n      access_token:\\n        type: string\\n        path_in_connector_config:\\n          - access_token\\n        path_in_oauth_response:\\n          - access_token\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Build Customization with Python Hooks\nDESCRIPTION: Example of a build_customization.py module showing pre and post connector installation hooks for customizing the build process with environment variables or system dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/migration-to-base-image.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Running Destination Operations via Docker\nDESCRIPTION: Demonstrates how to execute the 'spec', 'check', and 'write' operations of the destination connector directly using Docker, simulating the Airbyte execution environment. Requires the connector Docker image to be built first using the Gradle build command. The 'check' and 'write' commands require volume mounting for secrets (`secrets/config.json`) and potentially sample files (`sample_files/configured_catalog.json`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# First build the container\n./gradlew :airbyte-integrations:connectors:destination-<name>:build\n\n# Then use the following commands to run it\n# Runs the \"spec\" command, used to find out what configurations are needed to run a connector\ndocker run --rm airbyte/destination-<name>:dev spec\n\n# Runs the \"check\" command, used to validate if the input configurations are valid\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-<name>:dev check --config /secrets/config.json\n\n# Runs the \"write\" command which reads records from stdin and writes them to the underlying destination\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/destination-<name>:dev write --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Destination with JSON in Terraform\nDESCRIPTION: Creates a BigQuery destination using JSON configuration in Terraform. This approach uses a heredoc to define the entire configuration as a JSON string, including carefully escaped service account credentials and BigQuery-specific settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_6\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_destination_custom\" \"my_destination_bigquery_custom\" {\n    configuration = <<-EOF\n        {\n            \"destination_type\": \"BigQuery\",\n            \"credentials_json\": \"{ \\\"type\\\": \\\"service_account\\\", \\\"project_id\\\": \\\"YOUR_PROJECT_ID\\\", \\\"private_key_id\\\": \\\"YOUR_PRIVATE_KEY_ID\\\", \\\"private_key\\\": \\\"-----BEGIN PRIVATE KEY-----\\\\n...YOUR_KEY...\\\\n-----END PRIVATE KEY-----\\\\n\\\", \\\"client_email\\\": \\\"you@example.iam.gserviceaccount.com\\\", \\\"client_id\\\": \\\"YOUR_CLIENT_ID\\\", \\\"auth_uri\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\", \\\"token_uri\\\": \\\"https://oauth2.googleapis.com/token\\\", \\\"auth_provider_x509_cert_url\\\": \\\"https://www.googleapis.com/oauth2/v1/certs\\\", \\\"client_x509_cert_url\\\": \\\"https://www.googleapis.com/robot/v1/metadata/x509/you@example.iam.gserviceaccount.com\\\", \\\"universe_domain\\\": \\\"googleapis.com\\\" }\",\n            \"loading_method\": {\n                \"method\": \"Standard\",\n                \"batched_standard_inserts\": {}\n            },\n            \"dataset_id\": \"YOUR_DATASET_ID\",\n            \"dataset_location\": \"us-central1\",\n            \"project_id\": \"YOUR_PROJECT_ID\",\n            \"transformation_priority\": \"batch\"\n        }\n    EOF\n    name          = \"BigQuery\"\n    workspace_id  = var.workspace_id\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Data with PyAirbyte Source\nDESCRIPTION: Basic example of using PyAirbyte to extract data from a Faker source. This snippet demonstrates how to initialize a source with configuration, check its connection, select all data streams, and read data. It also shows how to iterate through the results by stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/pyairbyte/getting-started.mdx#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport airbyte as ab\n\nsource = ab.get_source(\n    \"source-faker\",\n    config={\"count\": 5_000},\n    install_if_missing=True,\n)\nsource.check()\nsource.select_all_streams()\nresult = source.read()\n\nfor name, records in result.streams.items():\n    print(f\"Stream {name}: {len(list(records))} records\")\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating Airbyte Repository - Shell (bash)\nDESCRIPTION: This script demonstrates how to clone the Airbyte repository using git and change into the project root directory. It requires git and bash shell to be installed. The script outputs no data but prepares the user for further development steps by ensuring the correct working directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:airbytehq/airbyte.git\\ncd airbyte\n```\n\n----------------------------------------\n\nTITLE: Customizing Build Process for Airbyte Connector with build_customization.py - Python\nDESCRIPTION: This Python module provides optional hooks for customizing the Airbyte connector's build process using Dagger containers. It defines asynchronous pre_connector_install and post_connector_install functions to mutate the base and connector containers, typically by injecting environment variables. It depends on Dagger's Python SDK and is discovered automatically by the airbyte-ci build process. The code must be placed in build_customization.py in the connector's directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Creating User and Granting Permissions in Postgres\nDESCRIPTION: SQL commands to create a new user for Airbyte and grant necessary permissions on the target database.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/postgres.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte_user WITH PASSWORD '<password>';\nGRANT CREATE, TEMPORARY ON DATABASE <database> TO airbyte_user;\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteControlConnectorConfigMessage Schema in YAML\nDESCRIPTION: Schema definition for the AirbyteControlConnectorConfigMessage which allows a connector to update its configuration during a sync. This is especially useful for connectors with short-lived credentials or those requiring configuration updates mid-process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteControlConnectorConfigMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - config\n  properties:\n    config:\n      description: \"the config items from this connector's spec to update\"\n      type: object\n      additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Defining BackoffStrategy Schema with Various Strategies in YAML\nDESCRIPTION: Defines a polymorphic BackoffStrategy schema, allowing one of several referenced backoff strategy objects. This pattern enables composable retry delays using exponential, constant, or header-based wait logic in Airbyte error handlers. Ensures only valid backoff strategies can be used in downstream configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nBackoffStrategy:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/ExponentialBackoffStrategy\"\n    - \"$ref\": \"#/definitions/ConstantBackoffStrategy\"\n    - \"$ref\": \"#/definitions/WaitTimeFromHeader\"\n    - \"$ref\": \"#/definitions/WaitUntilTimeFromHeader\"\n```\n\n----------------------------------------\n\nTITLE: Integer Stream Records in Airbyte JSON Format\nDESCRIPTION: Examples of JSON records for an integer data stream named 'int_test' with different values including positive, zero, and negative integers. Each record includes a timestamp in the emitted_at field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"42\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"-12345\" }}}\n```\n\n----------------------------------------\n\nTITLE: Defining HttpSelector and RecordSelector Schema - YAML\nDESCRIPTION: Provides YAML schema definitions for the HttpSelector and RecordSelector objects used to extract and filter records from HTTP responses in Airbyte. The RecordSelector requires an extractor (a RecordExtractor) and may optionally define a record_filter and parameters. These schemas form the basis for configuring how records are located and prepared within Airbyte HTTP connectors. No external dependencies are required; schemas are typically used with Airbyte’s configuration tools.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nHttpSelector:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/RecordSelector\"\nRecordSelector:\n  type: object\n  required:\n    - extractor\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    extractor:\n      \"$ref\": \"#/definitions/RecordExtractor\"\n    record_filter:\n      \"$ref\": \"#/definitions/RecordFilter\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Jira Connector\nDESCRIPTION: Command to build a Docker image for the Jira connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jira build\n```\n\n----------------------------------------\n\nTITLE: Adding Refresh Token Support for OAuth in Airbyte Manifests - Diff/YAML\nDESCRIPTION: This diff details multiple manifest changes to enable refresh token workflows, including a new token_refresh_endpoint, access_token and refresh_token mappings, and additional required fields. The manifest is expanded to include a refresh token grant type, tracking for token expiry, and a refresh_token_updater block. Dependencies include knowledge of Airbyte manifest structure and OAuth 2.0 refresh token procedure. Inputs include manifest YAML and OAuth config fields; outputs are manifests supporting token renewal and robust authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_8\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ refresh_token.yml\n  definitions:\n     authenticator:\n       type: OAuthAuthenticator\n       refresh_request_body: {}\n+      grant_type: refresh_token\n       client_id: \"{{ config[\\\"client_id\\\"] }}\"\n       client_secret: \"{{ config[\\\"client_secret\\\"] }}\"\n+      refresh_token: \"{{ config[\\\"client_refresh_token\\\"] }}\"\n       access_token_value: \"{{ config[\\\"client_access_token\\\"] }}\"\n+      access_token_name: access_token\n+      refresh_token_updater:\n+        refresh_token_name: refresh_token\n+        refresh_token_config_path:\n+          - client_refresh_token\n+      token_refresh_endpoint: >-\n+        https://yourconnectorservice.com/oauth/refresh/endpoint\n\n streams:\n   - $ref: \"#/definitions/streams/moves\"\n spec:\n    required:\n      - client_id\n      - client_secret\n-     - client_access_token\n+     - client_refresh_token\n     properties:\n       client_id:\n         type: string\n@@ -68,9 +77,9 @@ spec:\n-      client_access_token:\n+      client_refresh_token:\n         type: string\n-        title: Access token\n+        title: Refresh token\n@@ -86,16 +95,22 @@ spec:\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n       complete_oauth_output_specification:\n         required:\n           - access_token\n+          - refresh_token\n+          - token_expiry_date\n         properties:\n           access_token:\n             type: string\n             path_in_connector_config:\n               - access_token\n+            path_in_oauth_response:\n+              - access_token\n+          refresh_token:\n+            type: string\n+            path_in_connector_config:\n+              - refresh_token\n+            path_in_oauth_response:\n+              - refresh_token\n+          token_expiry_date:\n+            type: string\n+            path_in_connector_config:\n+              - token_expiry_date\n+            path_in_oauth_response:\n+              - expires_in\n       complete_oauth_server_input_specification:\n         required:\n           - client_id\n```\n\n----------------------------------------\n\nTITLE: Adding User to CDC Role in MSSQL\nDESCRIPTION: SQL command to add user to the specified CDC role for table access\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sp_addrolemember '{role name}', '{user name}';\n```\n\n----------------------------------------\n\nTITLE: Obtaining Pocket Request Token with cURL Shell Command\nDESCRIPTION: Shell command to create a request token from the Pocket API using cURL. This is part of the OAuth authentication flow and requires replacing the consumer key with your actual key from the Pocket Developer Portal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/pocket.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl --insecure -X POST -H 'Content-Type: application/json' -H 'X-Accept: application/json' \\\n    https://getpocket.com/v3/oauth/request  -d '{\"consumer_key\":\"REPLACE-ME\",\"redirect_uri\":\"http://www.google.com\"}'\n```\n\n----------------------------------------\n\nTITLE: Using string Filter in Jinja2\nDESCRIPTION: Demonstrates the `string` filter in Jinja2, which converts an object (like a number) into its string representation. The example converts the integer 42 to the string '42'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_45\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 42|string }}\n```\n\n----------------------------------------\n\nTITLE: Complete JSON to Avro Schema Conversion Example\nDESCRIPTION: Comprehensive example demonstrating the full JSON schema to Avro schema conversion, including handling of integer fields, nested objects, special characters, and Airbyte-specific metadata fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"properties\": {\n    \"id\": {\n      \"type\": \"integer\"\n    },\n    \"user\": {\n      \"type\": [\"null\", \"object\"],\n      \"properties\": {\n        \"id\": {\n          \"type\": \"integer\"\n        },\n        \"field_with_spécial_character\": {\n          \"type\": \"integer\"\n        }\n      }\n    },\n    \"created_at\": {\n      \"type\": [\"null\", \"string\"],\n      \"format\": \"date-time\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"stream_name\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"_airbyte_raw_id\",\n      \"type\": {\n        \"type\": \"string\",\n        \"logicalType\": \"uuid\"\n      }\n    },\n    {\n      \"name\": \"_airbyte_extracted_at\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"_airbyte_generation_id\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\" : \"_airbyte_meta\",\n      \"type\" : {\n        \"type\" : \"record\",\n        \"name\" : \"_airbyte_meta\",\n        \"namespace\" : \"\",\n        \"fields\" : [\n          {\n            \"name\" : \"sync_id\",\n            \"type\" : \"long\"\n          },\n          {\n            \"name\" : \"changes\",\n            \"type\" : {\n              \"type\" : \"array\",\n              \"items\" : {\n                \"type\" : \"record\",\n                \"name\" : \"change\",\n                \"fields\" : [\n                  {\n                    \"name\" : \"field\",\n                    \"type\" : \"string\"\n                  },\n                  {\n                    \"name\" : \"change\",\n                    \"type\" : \"string\"\n                  },\n                  {\n                    \"name\" : \"reason\",\n                    \"type\" : \"string\"\n                  }\n                ]\n              }\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"id\",\n      \"type\": [\"null\", \"int\"],\n      \"default\": null\n    },\n    {\n      \"name\": \"user\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"record\",\n          \"name\": \"user\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": [\"null\", \"int\"],\n              \"default\": null\n            },\n            {\n              \"name\": \"field_with_special_character\",\n              \"type\": [\"null\", \"int\"],\n              \"doc\": \"_airbyte_original_name:field_with_spécial_character\",\n              \"default\": null\n            }\n          ]\n        }\n      ],\n      \"default\": null\n    },\n    {\n      \"name\": \"created_at\",\n      \"type\": [\n        \"null\",\n        { \"type\": \"long\", \"logicalType\": \"timestamp-micros\" }\n      ],\n      \"default\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Reader Options in JSON Format\nDESCRIPTION: Example JSON configuration for the reader_options field when using CSV files. This demonstrates how to customize delimiter, header handling, column naming, and date parsing for tab-separated files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/file.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"sep\" : \"\\t\", \"header\" : null, \"names\": [\"column1\", \"column2\"], \"parse_dates\": [\"column2\"]}\n```\n\n----------------------------------------\n\nTITLE: Paginated Table Queries with CTID - SQL\nDESCRIPTION: These SQL snippets demonstrate how to paginate table access using the CTID system column in PostgreSQL. By splitting data into CTID ranges, large tables can be read efficiently in chunks, allowing interrupted sync processes to be resumed from intermediate points. Dependencies: PostgreSQL source with CTID available; parameters X, Y, Z should be set to appropriate CTID values. Inputs: CTID range boundaries to paginate; output: row sets from the users table. Limitation: only works with databases exposing CTID and where chunking is possible.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/resumability.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from users where CTID >= X AND CTID < Y\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from users where CTID >= Y AND CTID < Z\n```\n\n----------------------------------------\n\nTITLE: Defining Paginator and NoPagination Schemas in YAML\nDESCRIPTION: Defines the top-level YAML schema for the `Paginator` component in Airbyte. It specifies that a paginator can either conform to the `DefaultPaginator` schema or the `NoPagination` schema. The `NoPagination` schema itself is defined as a simple object type allowing additional properties, signifying no specific pagination logic is applied.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nPaginator:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/DefaultPaginator\"\n    - \"$ref\": \"#/definitions/NoPagination\"\nNoPagination:\n  type: object\n  additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Example S3 Output Path with Detailed Breakdown\nDESCRIPTION: Provides a concrete example of an S3 output file path (`testing_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz`) generated by the Airbyte S3 destination. Includes a visual breakdown explaining each component of the path, corresponding to the default path format variables (bucket name, path, namespace, stream name, date, epoch, partition ID, format extension).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ntesting_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz\n↑              ↑                ↑      ↑     ↑          ↑          ↑ ↑\n|              |                |      |     |          |          | format extension\n|              |                |      |     |          |          unique incremental part id\n|              |                |      |     |          milliseconds since epoch\n|              |                |      |     upload date in YYYY_MM_DD\n|              |                |      stream name\n|              |                source namespace (if it exists)\n|              bucket path\nbucket name\n```\n\n----------------------------------------\n\nTITLE: Manually Documenting PyAirbyte Connector Usage using Markdown Components - Markdown\nDESCRIPTION: Shows how to document usage for a PyAirbyte-enabled connector by manually specifying a 'Usage with PyAirbyte' section and Airbyte documentation custom components. Tags like `<HideInUI>`, `<PyAirbyteExample>`, and `<SpecSchema>` are used. Required: Airbyte markdown renderer supporting these tags; Input: Name of the connector to document (`connector` prop); Output: Generated code/example/config preview in docs and hidden in UI as needed. Limitation: Effective only within Airbyte markdown/documentation context.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n<HideInUI>\\n\\n## Usage with PyAirbyte\\n\\n<PyAirbyteExample connector=\\\"source-google-sheets\\\" />\\n\\n<SpecSchema connector=\\\"source-google-sheets\\\" />\\n\\n</HideInUI>\n```\n\n----------------------------------------\n\nTITLE: Running MySQL Connector Performance Tests (CLI)\nDESCRIPTION: Command line instruction for running performance tests with optional CPU and memory limits.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mysql:performanceTest [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Defining HttpResponseFilter and ResponseAction Schemas in YAML\nDESCRIPTION: Illustrates schemas for HttpResponseFilter objects that map response conditions (HTTP codes, error messages, or predicates) to actions (SUCCESS, FAIL, IGNORE, RETRY). These filters determine how specific HTTP responses are handled during requests. ResponseAction is an enumerated string type listing permissible handler actions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nHttpResponseFilter:\n  type: object\n  required:\n    - action\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    action:\n      \"$ref\": \"#/definitions/ResponseAction\"\n    http_codes:\n      type: array\n      items:\n        type: integer\n      default: []\n    error_message_contains:\n      type: string\n    predicate:\n      type: string\nResponseAction:\n  type: string\n  enum:\n    - SUCCESS\n    - FAIL\n    - IGNORE\n    - RETRY\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Requester Configuration Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the Requester and HttpRequester objects used in Airbyte for preparing and sending HTTP requests to source APIs. It specifies required properties like url_base and path, as well as options for HTTP method, authentication, error handling, and request options via referenced sub-schemas. The schema enforces object structure and allows for extensibility through additional nested properties, using references to other definitions for advanced configuration. Required properties and supported HTTP methods (GET, POST) are explicitly enumerated to validate configuration inputs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/requester.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nRequester:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/HttpRequester\"\nHttpRequester:\n  type: object\n  additionalProperties: true\n  required:\n    - url_base\n    - path\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    url_base:\n      type: string\n      description: \"base url\"\n    path:\n      type: string\n      description: \"path\"\n    http_method:\n      \"$ref\": \"#/definitions/HttpMethod\"\n      default: \"GET\"\n    request_options_provider:\n      \"$ref\": \"#/definitions/RequestOptionsProvider\"\n    authenticator:\n      \"$ref\": \"#/definitions/Authenticator\"\n    error_handler:\n      \"$ref\": \"#/definitions/ErrorHandler\"\nHttpMethod:\n  type: string\n  enum:\n    - GET\n    - POST\n\n```\n\n----------------------------------------\n\nTITLE: Providing stream slicing and chunking for checkpointing in Python\nDESCRIPTION: Implements stream_slices to divide the data timeline into manageable chunks, supporting Airbyte's checkpointing for resilience and concurrency. The method yields time window dictionaries generated by chunk_dates, which splits the date range into slices of configurable size. Dependencies: datetime, _START_DATE, _SLICE_RANGE, chunk_dates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\n    def stream_slices(self, stream_state: Mapping[str, Any] = None, **kwargs) -> Iterable[Optional[Mapping[str, any]]]:\n        start_ts = stream_state.get(self._cursor_field, _START_DATE) if stream_state else _START_DATE\n        now_ts = datetime.datetime.now().timestamp()\n        if start_ts >= now_ts:\n            yield from []\n            return\n        for start, end in self.chunk_dates(start_ts, now_ts):\n            yield {\"start_date\": start, \"end_date\": end}\n\n    def chunk_dates(self, start_date_ts: int, end_date_ts: int) -> Iterable[Tuple[int, int]]:\n        step = int(_SLICE_RANGE * 24 * 60 * 60)\n        after_ts = start_date_ts\n        while after_ts < end_date_ts:\n            before_ts = min(end_date_ts, after_ts + step)\n            yield after_ts, before_ts\n            after_ts = before_ts + 1\n```\n\n----------------------------------------\n\nTITLE: Removing Fields with Transformations\nDESCRIPTION: Demonstrates how to remove specific fields from records using the transformations feature. This example removes the 'content' field from campaign records in the EmailOctopus API response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": [\n        {\n            \"id\": \"00000000-0000-0000-0000-000000000000\",\n            \"status\": \"SENT\",\n            \"name\": \"Foo\",\n            \"subject\": \"Bar\",\n            \"from\": {\n                \"name\": \"John Doe\",\n                \"email_address\": \"john.doe@gmail.com\"\n            },\n            \"content\": {\n                \"html\": \"<html>lots of text here...<html>\",\n                \"plain_text\": \"Lots of plain text here....\"\n            },\n            \"created_at\": \"2023-04-13T15:28:37+00:00\",\n            \"sent_at\": \"2023-04-14T15:28:37+00:00\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Updating stream state based on latest record in Python\nDESCRIPTION: Defines get_updated_state to maintain the stream's progress by tracking the latest value of the cursor field. Utilizes datetime.strptime to parse the cursor field from the latest record, converting it to a Unix timestamp for comparison and state persistence. Dependencies: datetime; expects current_stream_state and latest_record mappings and uses the _INCOMING_DATETIME_FORMAT constant for parsing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def get_updated_state(self, current_stream_state: MutableMapping[str, Any], latest_record: Mapping[str, Any]) -> Mapping[str, Any]:\n        state_value = max(current_stream_state.get(self.cursor_field, 0), datetime.datetime.strptime(latest_record.get(self._cursor_field, \"\"), _INCOMING_DATETIME_FORMAT).timestamp())\n        return {self._cursor_field: state_value}\n```\n\n----------------------------------------\n\nTITLE: Displaying Streams Table in Markdown\nDESCRIPTION: This snippet presents a markdown table that lists all available streams in the Help Scout connector, including their primary keys, pagination methods, and sync support details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/help-scout.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| conversations | id | DefaultPaginator | ✅ |  ✅  |\n| conversation_threads | id | DefaultPaginator | ✅ |  ✅  |\n| customers | id | DefaultPaginator | ✅ |  ✅  |\n| inboxes | id | DefaultPaginator | ✅ |  ❌  |\n| inbox_custom_fields | id | DefaultPaginator | ✅ |  ❌  |\n| inbox_folders | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| workflows | id | DefaultPaginator | ✅ |  ❌  |\n| tags | id | DefaultPaginator | ✅ |  ❌  |\n| teams | id | DefaultPaginator | ✅ |  ❌  |\n| team_members | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Initializing Chroma Vector Store with LangChain in Python\nDESCRIPTION: This code snippet shows how to set up a LangChain QA chain using Chroma as the vector store. It initializes the OpenAI embeddings, Chroma vector store with a local persist directory, and creates a RetrievalQA chain.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/langchain.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import OpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nvector_store = Chroma(embedding_function=embeddings, persist_directory=\"/tmp/airbyte_local/<your configured directory>\")\n\nqa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever())\n```\n\n----------------------------------------\n\nTITLE: Running Connector as a Docker Container - Bash\nDESCRIPTION: These commands run the Google-Ads source connector within a Docker container for different Airbyte command modes. Secrets and integration test directories are mounted from the current working directory to provide config and catalog files. The connector is launched with commands such as spec, check, discover, and read, each requiring different input files and mounting configurations as specified. Docker and the airbyte/source-google-ads:dev image must be present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-ads:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-ads:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-ads:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-ads:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Updating Connection-Specific Resources in SQL\nDESCRIPTION: SQL command to update resource requirements for a specific connection, setting CPU and memory limits/requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-connector-resources.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nupdate connection set resource_requirements = '{\"cpu_limit\": \"2\", \"cpu_request\": \"2\", \"memory_limit\": \"2048Mi\", \"memory_request\": \"2048Mi\"}' where id = '<id-from-step-1>';\n```\n\n----------------------------------------\n\nTITLE: Setting Default Concurrency, Rate Limit, and Logger Constants in Python\nDESCRIPTION: This snippet defines constants for default and maximum concurrency, rate limiting, and instantiates the module logger. These are referenced throughout the connector implementation to standardize concurrency and logging behavior. There are no external dependencies specific to this block aside from the logging import.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n_DEFAULT_CONCURRENCY = 10\\n_MAX_CONCURRENCY = 10\\n_RATE_LIMIT_PER_MINUTE = 120\\n_logger = logging.getLogger(\"airbyte\")\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Locally with Poetry - Bash\nDESCRIPTION: The following bash commands demonstrate how to invoke different operational modes (spec, check, discover, read) of the source-google-analytics-data-api connector using Poetry. Each command runs the connector in the project's isolated virtual environment, leveraging Poetry. Configuration paths are passed as required parameters, and the 'read' command additionally requires a properly configured catalog file. Required dependencies include Poetry, a built and installed project, and properly formatted secrets/config.json. These examples are directly executable in a Unix shell environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-analytics-data-api spec\npoetry run source-google-analytics-data-api check --config secrets/config.json\npoetry run source-google-analytics-data-api discover --config secrets/config.json\npoetry run source-google-analytics-data-api read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating SurveyMonkeyBaseStream instance for surveys in Python\nDESCRIPTION: Illustrates instantiating SurveyMonkeyBaseStream with proper arguments, including cursor_field set to 'date_modified'. This enables incremental sync on the 'date_modified' field for the surveys stream. The authenticator must be provided for API calls.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nreturn [SurveyMonkeyBaseStream(name=\"surveys\", path=\"/v3/surveys\", primary_key=\"id\", data_field=\"data\", cursor_field=\"date_modified\", authenticator=auth)]\n```\n\n----------------------------------------\n\nTITLE: API Key Authenticator Schema and Implementation\nDESCRIPTION: Schema and example for API Key authentication that sets custom HTTP headers for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nApiKeyAuthenticator:\n  type: object\n  additionalProperties: true\n  required:\n    - header\n    - api_token\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    header:\n      type: string\n    api_token:\n      type: string\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: \"ApiKeyAuthenticator\"\n  header: \"Authorization\"\n  api_token: \"Bearer hello\"\n```\n\n----------------------------------------\n\nTITLE: Defining RecordTransformation with AddFields and RemoveFields - YAML\nDESCRIPTION: YAML schema for RecordTransformation, which may include AddFields (to add new fields) or RemoveFields (to delete existing fields) when transforming records in Airbyte streams. These schemas allow users to flexibly modify record structures via configuration. Standard Airbyte YAML schema referencing is used.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nRecordTransformation:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/AddFields\"\n    - \"$ref\": \"#/definitions/RemoveFields\"\n```\n\n----------------------------------------\n\nTITLE: Triggering Asynchronous Airbyte Sync with Airflow Operator and Sensor in Python\nDESCRIPTION: This snippet shows how to orchestrate an asynchronous Airbyte sync job in an Airflow DAG using both AirbyteTriggerSyncOperator and AirbyteJobSensor. It requires the same Airflow and provider dependencies as the synchronous example but also includes the AirbyteJobSensor for non-blocking monitoring of the sync job. The AirbyteTriggerSyncOperator initiates the job asynchronously, and the AirbyteJobSensor monitors completion based on the job ID output. The two tasks are linked via Airflow's dependency notation. Inputs are configured connection/task IDs and outputs are monitored job completions. Particularly suitable for environments with constrained Airflow worker resources.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-the-airflow-airbyte-operator.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.utils.dates import days_ago\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor\n\nwith DAG(dag_id='airbyte_trigger_job_example_async',\n         default_args={'owner': 'airflow'},\n         schedule_interval='@daily',\n         start_date=days_ago(1)\n    ) as dag:\n\n    async_money_to_json = AirbyteTriggerSyncOperator(\n        task_id='airbyte_async_money_json_example',\n        airbyte_conn_id='airbyte_conn_example',\n        connection_id='1e3b5a72-7bfd-4808-a13c-204505490110',\n        asynchronous=True,\n    )\n\n    airbyte_sensor = AirbyteJobSensor(\n        task_id='airbyte_sensor_money_json_example',\n        airbyte_conn_id='airbyte_conn_example',\n        airbyte_job_id=async_money_to_json.output\n    )\n\n    async_money_to_json >> airbyte_sensor\n\n```\n\n----------------------------------------\n\nTITLE: Importing Moving Window Rate Limiting Policy for Airbyte Streams in Python\nDESCRIPTION: These imports provide classes for configuring HTTP call rate policies, including moving window rate limits, budgets, and rate definitions. They are necessary for enforcing throttling on API requests and preventing quota violations. Used in stream classes to build API call budgets and attach rate limiting policies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# import the following libraries\\nfrom airbyte_cdk.sources.streams.call_rate import MovingWindowCallRatePolicy, HttpAPIBudget, Rate\n```\n\n----------------------------------------\n\nTITLE: Formatting Custom JDBC URL Parameters for PostgreSQL\nDESCRIPTION: Provides an example format for specifying additional JDBC connection parameters as key-value pairs separated by '&' in the 'JDBC URL Parameters (Advanced)' field for the Airbyte PostgreSQL source connector. These parameters customize the connection beyond standard options but should not include keys managed by Airbyte like 'user' or 'password'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nkey1=value1&key2=value2&key3=value3\n```\n\n----------------------------------------\n\nTITLE: Verifying Local Connector Docker Image Existence (Shell)\nDESCRIPTION: Lists local Docker images and filters the output to find a specific Airbyte connector image tagged as `dev` (e.g., `airbyte/source-<source-name>:dev`). This command confirms the image was successfully built and is present in the local Docker daemon. Requires Docker.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker images ls | grep airbyte/source-<source-name>:dev\n```\n\n----------------------------------------\n\nTITLE: Defining OffsetIncrement Pagination Strategy Schema in YAML\nDESCRIPTION: Defines the YAML schema for the `OffsetIncrement` pagination strategy. It requires an integer `page_size` property and allows optional `$parameters`. This strategy works by incrementing an offset parameter (representing the number of records already fetched) in subsequent requests, based on the specified `page_size`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nOffsetIncrement:\n  type: object\n  additionalProperties: true\n  required:\n    - page_size\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    page_size:\n      type: integer\n```\n\n----------------------------------------\n\nTITLE: AirbyteCatalog JSON Schema Representation\nDESCRIPTION: The corresponding AirbyteCatalog definition that uses JsonSchema to model the nested flight data structure. This shows how Airbyte handles complex nested objects with their properties and types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"flights\",\n      \"supported_sync_modes\": [\n        \"full_refresh\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"airline\": {\n            \"type\": \"string\"\n          },\n          \"origin\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"airport_code\": {\n                \"type\": \"string\"\n              },\n              \"terminal\": {\n                \"type\": \"string\"\n              },\n              \"gate\": {\n                \"type\": \"string\"\n              }\n            }\n          },\n          \"destination\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"airport_code\": {\n                \"type\": \"string\"\n              },\n              \"terminal\": {\n                \"type\": \"string\"\n              },\n              \"gate\": {\n                \"type\": \"string\"\n              }\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Standard Connector Commands via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to run the standard Airbyte source connector operations (spec, check, discover, read) using the locally built Docker image (`airbyte/source-customer-io:dev`). The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing a `config.json` file with credentials. The `read` command also requires mounting an `integration_tests` directory containing a `configured_catalog.json` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-customer-io/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-customer-io:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-customer-io:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-customer-io:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-customer-io:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte STATE Message Example in JSON\nDESCRIPTION: An example of an Airbyte STATE message that tracks synchronization state. This GLOBAL state message contains shared state information, specifically a start_date that can be used for incremental syncs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_object_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": {\"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Processing Nested Stream with Complex Columns in JSON\nDESCRIPTION: These JSON records represent data from a stream with nested structures and complex columns. The records include fields like 'id', 'date', and a nested 'partition' object with arrays and special characters in column names.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"nested_stream_with_complex_columns_resulting_into_long_names\", \"emitted_at\": 1602638599000, \"data\": { \"id\": 4.2, \"date\": \"2020-08-29T00:00:00Z\", \"partition\": { \"double_array_data\": [[ { \"id\": \"EUR\" } ]], \"DATA\": [ {\"currency\": \"EUR\" } ], \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ] } }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"nested_stream_with_complex_columns_resulting_into_long_names\", \"emitted_at\": 1602638599100, \"data\": { \"id\": \"test record\", \"date\": \"2020-08-31T00:00:00Z\", \"partition\": { \"double_array_data\": [[ { \"id\": \"USD\" } ], [ { \"id\": \"GBP\" } ]], \"DATA\": [ {\"currency\": \"EUR\" } ], \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ] } }}}\n```\n\n----------------------------------------\n\nTITLE: Define Stream Schema in Airbyte Low-Code YAML\nDESCRIPTION: Describes the YAML structure for defining a `DeclarativeStream` in Airbyte's low-code CDK. It outlines required properties like `type` and `retriever`, and optional ones such as `schema_loader` (for inline or file-based schemas), `stream_cursor_field` (for incremental syncs), and `transformations`. The definition references other components like `Retriever`, `InlineSchemaLoader`, `JsonFileSchemaLoader`, `AddFields`, `CustomTransformation`, and `RemoveFields` defined elsewhere in the schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/yaml-overview.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nDeclarativeStream:\n  description: A stream whose behavior is described by a set of declarative low code components\n  type: object\n  additionalProperties: true\n  required:\n    - type\n    - retriever\n  properties:\n    type:\n      type: string\n      enum: [DeclarativeStream]\n    retriever:\n      \"$ref\": \"#/definitions/Retriever\"\n    schema_loader:\n      definition: The schema loader used to retrieve the schema for the current stream\n      anyOf:\n        - \"$ref\": \"#/definitions/InlineSchemaLoader\"\n        - \"$ref\": \"#/definitions/JsonFileSchemaLoader\"\n    stream_cursor_field:\n      definition: The field of the records being read that will be used during checkpointing\n      anyOf:\n        - type: string\n        - type: array\n          items:\n            - type: string\n    transformations:\n      definition: A list of transformations to be applied to each output record in the\n      type: array\n      items:\n        anyOf:\n          - \"$ref\": \"#/definitions/AddFields\"\n          - \"$ref\": \"#/definitions/CustomTransformation\"\n          - \"$ref\": \"#/definitions/RemoveFields\"\n    $parameters:\n      type: object\n      additional_properties: true\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-linnworks spec\npoetry run source-linnworks check --config secrets/config.json\npoetry run source-linnworks discover --config secrets/config.json\npoetry run source-linnworks read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining RequestOptionsProvider Schema - YAML\nDESCRIPTION: This YAML schema snippet defines the structure of the RequestOptionsProvider, which specifies how request options, such as parameters, headers, and body data, can be configured. Dependencies include Airbyte's schema referencing definitions like InterpolatedRequestOptionsProvider and RequestInput. The properties listed allow for extensible and interpolated request options in connectors. Inputs are objects with possible keys for parameters, headers, body data, and JSON body; outputs are validated YAML configs adhering to the schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nRequestOptionsProvider:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/InterpolatedRequestOptionsProvider\"\nInterpolatedRequestOptionsProvider:\n  type: object\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    request_parameters:\n      \"$ref\": \"#/definitions/RequestInput\"\n    request_headers:\n      \"$ref\": \"#/definitions/RequestInput\"\n    request_body_data:\n      \"$ref\": \"#/definitions/RequestInput\"\n    request_body_json:\n      \"$ref\": \"#/definitions/RequestInput\"\n```\n\n----------------------------------------\n\nTITLE: Querying Webflow Sites via API using cURL - Bash\nDESCRIPTION: This bash snippet demonstrates how to retrieve a list of Webflow sites using the Webflow API v1. It shows how to use cURL with required HTTP headers, including 'Authorization' for bearer token and 'accept-version' for API versioning. The key parameters are the API endpoint (https://api.webflow.com/sites) and the API key provided by the user. The expected output is a JSON array listing site details such as site ID, name, and timestamps. No special dependencies besides cURL are required.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/webflow.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://api.webflow.com/sites \\\n  -H \"Authorization: Bearer <your API Key>\" \\\n  -H \"accept-version: 1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Defining ListPartitionRouter Schema in YAML\nDESCRIPTION: Provides the YAML schema definition for `ListPartitionRouter`. This router iterates over a predefined list of values (`partition_values`) to partition data retrieval. Key properties include `type`, `cursor_field` (the field representing the partition), `partition_values`, and an optional `request_option` to inject the partition value into requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nListPartitionRouter:\n  description: Partition router that is used to retrieve records that have been partitioned according to a list of values\n  type: object\n  required:\n    - type\n    - cursor_field\n    - slice_values\n  properties:\n    type:\n      type: string\n      enum: [ListPartitionRouter]\n    cursor_field:\n      type: string\n    partition_values:\n      anyOf:\n        - type: string\n        - type: array\n          items:\n            type: string\n    request_option:\n      \"$ref\": \"#/definitions/RequestOption\"\n    $parameters:\n      type: object\n      additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-babelforce/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-babelforce:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-babelforce:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-babelforce:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-babelforce:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Granting Broad Read Access to Airbyte User in Oracle SQL\nDESCRIPTION: This SQL command grants the 'airbyte' user read access ('SELECT' privilege) to all tables across all schemas in the Oracle database. This is the simplest way to provide the necessary read permissions but offers less granular control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ANY TABLE TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Configuring WaitTimeFromHeader Backoff with Regex Extraction Example in YAML\nDESCRIPTION: Shows how to supplement a header-based backoff strategy with a regular expression, extracting only matching numeric values from the header for use as wait intervals. Useful for handling more complex or variable server response header formats in Airbyte integrations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"\n          regex: \"[-+]?\\d+\"\n```\n\n----------------------------------------\n\nTITLE: Resumable Full Refresh State Format\nDESCRIPTION: JSON format for storing the stream's state during pagination. Shows the structure for both active pagination state and completion state.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/resumable-full-refresh-stream.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"page\": 25\n}\n```\n\n----------------------------------------\n\nTITLE: Requesting Access Token Body for Airbyte Cloud (YAML)\nDESCRIPTION: Defines the required JSON payload structure for requesting an access token from the Airbyte Cloud API. The `client_id` and `client_secret` values must be populated with the credentials generated when creating the application in the Airbyte UI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{ \n  \"client_id\": \"\", \n  \"client_secret\": \"\" \n  }\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with `docker build` (Bash)\nDESCRIPTION: Shows the bash command to build a Docker image using the provided Dockerfile. It tags the resulting image as `airbyte/source-example-python:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -t airbyte/source-example-python:dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Replication with ConfiguredAirbyteCatalog - Javascript (JSON Syntax)\nDESCRIPTION: This snippet demonstrates creating a ConfiguredAirbyteCatalog JSON object that specifies which streams to replicate from the Airbyte catalog and how. Here, only the 'airlines' stream is configured for FULL_REFRESH sync mode; the 'pilots' stream is omitted (not replicated). Dependencies: The structure must match a corresponding AirbyteCatalog stream. Inputs: AirbyteCatalog JSON, selected sync modes; Outputs: ConfiguredAirbyteCatalog JSON.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_2\n\nLANGUAGE: Javascript\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"sync_mode\": \"FULL_REFRESH\",\n      \"stream\": {\n        \"name\": \"airlines\",\n        \"supported_sync_modes\": [\n          \"full_refresh\",\n          \"incremental\"\n        ],\n        \"source_defined_cursor\": false,\n        \"json_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"id\": {\n              \"type\": \"number\"\n            },\n            \"name\": {\n              \"type\": \"string\"\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte in Low-Resource Mode\nDESCRIPTION: Command to install Airbyte locally in low-resource mode, suitable for systems with fewer than 4 CPUs but at least 8GB of memory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nabctl local install --low-resource-mode\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Backoff Strategies for Fallback Logic in YAML\nDESCRIPTION: Shows configuration of multiple fallback backoff strategies: first extracting wait time from a header, then resorting to a constant wait if unavailable. This snippet enables robust, layered retry logic in Airbyte's error handling YAML.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"\n            - type: \"ConstantBackoff\"\n              backoff_time_in_seconds: 5\n```\n\n----------------------------------------\n\nTITLE: Running the Connector as a Docker Container - Bash\nDESCRIPTION: These docker run commands execute various modes of the Airbyte connector as a container, with input configuration files mounted via volumes for secure parameter passing. The commands demonstrate running the 'spec', 'check', 'discover', and 'read' operations using image 'airbyte/source-google-analytics-data-api:dev'. Volume mounting syntax ensures that local secrets and catalogs are accessible to the container at runtime. Docker must be installed, and proper paths to config/catalog files are necessary.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-analytics-data-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-data-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-data-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-analytics-data-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Introducing CDC Cursor Field Changes in MySQL 3.0.0\nDESCRIPTION: Documentation of breaking changes in MySQL CDC syncs that introduces a new default cursor field '_ab_cdc_cursor'. Explains required steps to migrate including dropping SCD tables, refreshing connection schema, or performing a reset.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# MySQL Migration Guide\n\n## Upgrading to 3.0.0\n\nCDC syncs now has default cursor field called `_ab_cdc_cursor`. You will need to force normalization to rebuild your destination tables by manually dropping the SCD tables, refreshing the connection schema (skipping the reset), and running a sync. Alternatively, you can just run a reset.\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Secret for Azure Blob Storage Connection String in YAML\nDESCRIPTION: Defines a Kubernetes Secret named `airbyte-config-secrets` of type `Opaque` to store the Azure Blob Storage connection string under the `azure-blob-store-connection-string` key. This connection string contains the storage account name and authentication key/token, required when configuring Airbyte to use Azure Blob Storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # Azure Secrets\n  azure-blob-store-connection-string: ## DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=wJalrXUtnFEMI/wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY/wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY==;EndpointSuffix=core.windows.net\n\n```\n\n----------------------------------------\n\nTITLE: Mapping JSON DateTime Format to Avro Logical Type\nDESCRIPTION: Shows the direct mapping of the JSON `date-time` format to an Avro `long` type with the `timestamp-micros` logical type. The long represents the number of microseconds from the Unix epoch (1970-01-01T00:00:00Z). This is an intermediate representation before the final nullable schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"long\",\n  \"logicalType\": \"timestamp-micros\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Providing Good Context in Pull Request Descriptions (Text Example)\nDESCRIPTION: This text snippet demonstrates an example of a well-contextualized pull request description. It explains the motivation (manual transcription of JSON Schema from OpenAPI docs is time-consuming) and the problem being solved (automating the conversion process) for a proposed feature to create an OpenAPI to JSON Schema generator.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/pull-requests-handbook.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n```text\nWhen creating or updating connectors, we spend a lot of time manually transcribing JSON Schema files based on OpenAPI docs. This is ncessary because OpenAPI and JSON schema are very similar but not perfectly compatible. This process is automatable. Therefore we should create a program which converts from OpenAPI to JSONSchema format.\n```\n```\n\n----------------------------------------\n\nTITLE: Creating and Granting Permissions to Airbyte User in Yellowbrick (SQL)\nDESCRIPTION: This set of SQL commands creates a dedicated user named `airbyte_user` with a specified password and grants it the necessary `CREATE` and `TEMPORARY` permissions on the target database (`<database>`). Replace `<password>` and `<database>` with actual values. This user will be used by Airbyte to connect and write data to Yellowbrick.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/yellowbrick.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte_user WITH ENCRYPTED PASSWORD '<password>';\nGRANT CREATE, TEMPORARY ON DATABASE <database> TO airbyte_user;\n```\n\n----------------------------------------\n\nTITLE: Async Job Retry Algorithm for Facebook Insights\nDESCRIPTION: Pseudocode algorithm that handles the retry logic for async jobs when fetching Facebook Insights data, including fallback mechanisms to request data at more granular levels when account-level requests fail.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/source_facebook_marketing/README.md#2025-04-23_snippet_2\n\nLANGUAGE: pseudocode\nCODE:\n```\ncreate async job for account level insight for the day A\n\tif async job failed:\n\t\trestart it\n\tif async job failed again:\n\t\tget list of campaigns for last 28 day\n\t\tcreate async job for each campaign and day A\n```\n\n----------------------------------------\n\nTITLE: Exchanging OAuth2 Code for Access Token using cURL\nDESCRIPTION: This cURL command demonstrates how to exchange the one-time use code for an access token and refresh token in the Snapchat Marketing API OAuth2 flow. It includes placeholders for code, client ID, client secret, and redirect URI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/snapchat-marketing.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \\\n-d \"code={one_time_use_code}\" \\\n-d \"client_id={client_id}\" \\\n-d \"client_secret={client_secret}\"  \\\n-d \"grant_type=authorization_code\"  \\\n-d \"redirect_uri=redirect_uri\"\nhttps://accounts.snapchat.com/login/oauth2/access_token\n```\n\n----------------------------------------\n\nTITLE: Implementing State Management in Python Stream Class\nDESCRIPTION: Example implementation of state getter and setter methods for managing stream cursors in a Python stream class. The state property helps track sync progress through pagination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/resumable-full-refresh-stream.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef state(self) -> Mapping[str, Any]:\n   return {self.cursor_field: str(self._cursor_value)}\n\n@state.setter\ndef state(self, value: Mapping[str, Any]):\n   self._cursor_value = value[self.cursor_field]\n```\n\n----------------------------------------\n\nTITLE: Defining Required Slack Bot Token Scopes\nDESCRIPTION: This snippet lists the required Bot Token Scopes that must be selected within the Slack App's 'OAuth & Permissions' settings during Step 1 of the setup process. These scopes grant the Airbyte connector the necessary permissions to read channel history, join channels, read user information, files, reactions, and other relevant data from the Slack workspace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/slack.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n```\n channels:history\n channels:join\n channels:read\n files:read\n groups:read\n links:read\n reactions:read\n remote_files:read\n team:read\n usergroups:read\n users:read\n users.profile:read\n```\n```\n\n----------------------------------------\n\nTITLE: Automated API Request Sequence for Offset Increment (HTTP/JSON)\nDESCRIPTION: Demonstrates the sequence of HTTP GET requests the Airbyte Connector Builder automatically makes when configured for Offset Increment pagination with a limit of 2. It shows how the 'offset' parameter is incremented until fewer records than the limit are returned.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.example.com/products?limit=2&offset=0\n  -> [\n       {\"id\": 1, \"name\": \"Product A\"},\n       {\"id\": 2, \"name\": \"Product B\"}\n     ]\n\nGET https://api.example.com/products?limit=2&offset=2\n  -> [\n       {\"id\": 3, \"name\": \"Product C\"},\n       {\"id\": 4, \"name\": \"Product D\"}\n     ]\n\nGET https://api.example.com/products?limit=3&offset=4\n  -> [\n       {\"id\": 5, \"name\": \"Product E\"}\n     ]\n     // less than 2 records returned -> stop\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table\nDESCRIPTION: Table showing mapping between Zoho CRM data types and their corresponding Airbyte data types, including format specifications and special handling cases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-crm.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type      | Airbyte Type | Notes                     |\n| :-------------------- | :----------- | :------------------------ |\n| `boolean`             | `boolean`    |                           |\n| `double`              | `number`     |                           |\n| `currency`            | `number`     |                           |\n| `integer`             | `integer`    |                           |\n| `profileimage`        | `string`     |                           |\n| `picklist`            | `string`     | enum                      |\n| `textarea`            | `string`     |                           |\n| `website`             | `string`     | format: uri               |\n| `date`                | `string`     | format: date              |\n| `datetime`            | `string`     | format: date-time         |\n| `text`                | `string`     |                           |\n| `phone`               | `string`     |                           |\n| `bigint`              | `string`     | airbyte_type: big_integer |\n| `event_reminder`      | `string`     |                           |\n| `email`               | `string`     | format: email             |\n| `autonumber`          | `string`     | airbyte_type: big_integer |\n| `jsonarray`           | `array`      |                           |\n| `jsonobject`          | `object`     |                           |\n| `multiselectpicklist` | `array`      |                           |\n| `lookup`              | `object`     |                           |\n| `ownerlookup`         | `object`     |                           |\n| `RRULE`               | `object`     |                           |\n| `ALARM`               | `object`     |                           |\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with Nested Array\nDESCRIPTION: Example of an Airbyte RECORD message containing a data stream with nested array structure. Includes various data types like strings, dates, timestamps, numbers, booleans and binary data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_object_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"object_array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"property_string\" : \"qqq\", \"property_array\" : [ { \"property_string\": \"foo bar\", \"property_date\": \"2021-01-23\", \"property_timestamp_with_timezone\": \"2022-11-22T01:23:45+00:00\", \"property_timestamp_without_timezone\": \"2022-11-22T01:23:45\", \"property_number\": \"56.78\",  \"property_integer\": \"42\", \"property_boolean\": true, \"property_binary_data\" :  \"dGVzdA==\" } ] }}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Secret Manager for Airbyte Connectors (YAML)\nDESCRIPTION: This YAML manifest configures Airbyte to use Google Secret Manager for handling connector secrets. It requires a Kubernetes secret that stores GCP service account credentials (as a gcp.json file), referenced in the configuration. Key parameters include project ID, the Kubernetes secret name, and the credentials key. The Airbyte cluster's ServiceAccount must have access to GSM and GCS as needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nsecretsManager:\n  type: googleSecretManager\n  storageSecretName: gcp-cred-secrets\n  googleSecretManager:\n    projectId: <project-id>\n    credentialsSecretKey: gcp.json\n\n```\n\n----------------------------------------\n\nTITLE: Converting Complex JSON Array Items to Merged Avro Records\nDESCRIPTION: Complex example showing how JSON arrays with multiple object schemas are converted to Avro by merging the object structures into a unified schema that accommodates all possible field types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"array_field\": {\n    \"type\": \"array\",\n    \"items\": [\n      {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id_part_1\": { \"type\": \"integer\" },\n              \"id_part_2\": { \"type\": \"string\" }\n            }\n          }\n        }\n      },\n      {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id_part_1\": { \"type\": \"string\" },\n              \"id_part_2\": { \"type\": \"integer\" }\n            }\n          },\n          \"message\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"array_field\": [\n    {\n      \"id\": {\n        \"id_part_1\": 1000,\n        \"id_part_2\": \"abcde\"\n      }\n    },\n    {\n      \"id\": {\n        \"id_part_1\": \"wxyz\",\n        \"id_part_2\": 2000\n      },\n      \"message\": \"test message\"\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"array_field\",\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"array\",\n      \"items\": [\n        \"boolean\",\n        {\n          \"type\": \"record\",\n          \"name\": \"array_field\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": [\n                \"null\",\n                {\n                  \"type\": \"record\",\n                  \"name\": \"id\",\n                  \"fields\": [\n                    {\n                      \"name\": \"id_part_1\",\n                      \"type\": [\"null\", \"int\", \"string\"],\n                      \"default\": null\n                    },\n                    {\n                      \"name\": \"id_part_2\",\n                      \"type\": [\"null\", \"string\", \"int\"],\n                      \"default\": null\n                    }\n                  ]\n                }\n              ],\n              \"default\": null\n            },\n            {\n              \"name\": \"message\",\n              \"type\": [\"null\", \"string\"],\n              \"default\": null\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"default\": null\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"array_field\": [\n    {\n      \"id\": {\n        \"id_part_1\": 1000,\n        \"id_part_2\": \"abcde\"\n      },\n      \"message\": null\n    },\n    {\n      \"id\": {\n        \"id_part_1\": \"wxyz\",\n        \"id_part_2\": 2000\n      },\n      \"message\": \"test message\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Secrets Manager for Airbyte Connectors (YAML)\nDESCRIPTION: This YAML manifest specifies configuration for Airbyte to use AWS Secrets Manager for secure storage of connector secrets. It supports both access key/secret-based ('credentials') and instance role-based ('instanceProfile') authentication, with optional KMS decryption and per-secret tagging. Required Kubernetes secrets must provide the AWS credentials, and the Airbyte cluster needs permission to use AWS Secrets Manager and (optionally) KMS. Parameters include AWS region, authentication type, and optional tags/KMS ARN.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsecretsManager:\n  type: awsSecretManager\n  awsSecretManager:\n    region: <aws-region>\n    authenticationType: credentials ## Use \"credentials\" or \"instanceProfile\"\n    tags: ## Optional - You may add tags to new secrets created by Airbyte.\n      - key: ## e.g. team\n        value: ## e.g. deployments\n      - key: business-unit\n        value: engineering\n    kms: ## Optional - ARN for KMS Decryption.\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Helm Values for Azure Blob Storage in YAML\nDESCRIPTION: Specifies Helm chart values under `global.storage` to configure Airbyte to use Azure Blob Storage for log, state, and workload output storage. It sets the storage `type` to `Azure`, references the Kubernetes secret (`secretName`) containing the connection string, defines the target container names under `bucket`, and specifies the key within the secret that holds the connection string (`azure.connectionStringSecretKey`). Requires a pre-existing Kubernetes secret (e.g., `airbyte-config-secrets`) with the Azure connection string.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"Azure\"\n    secretName: airbyte-config-secrets # Name of your Kubernetes secret.\n    bucket: ## Name Containers that you've created. We recommend storing the following all in one Container.\n      log: airbyte-container\n      state: airbyte-container\n      workloadOutput: airbyte-container\n    azure:\n      connectionStringSecretKey: azure-blob-store-connection-string # key of your Kubernetes secret\n\n```\n\n----------------------------------------\n\nTITLE: Example metadata.yaml Structure for Postgres Source (YAML)\nDESCRIPTION: This snippet shows a complete example of a `metadata.yaml` file for the Postgres source connector. It demonstrates key fields like `connectorType`, `definitionId`, `dockerImageTag`, `dockerRepository`, `allowedHosts`, `registries`, `tags`, `supportLevel`, and `documentationUrl`, defining the core metadata for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  allowedHosts:\n    hosts:\n      - ${host}\n      - ${tunnel_method.tunnel_host}\n  connectorSubtype: database\n  connectorType: source\n  definitionId: decd338e-5647-4c0b-adf4-da0e75f5a750\n  dockerImageTag: 2.0.28\n  maxSecondsBetweenMessages: 7200\n  dockerRepository: airbyte/source-postgres\n  githubIssueLabel: source-postgres\n  icon: postgresql.svg\n  license: MIT\n  name: Postgres\n  tags:\n    - language:java\n    - language:python\n  registries:\n    cloud:\n      dockerRepository: airbyte/source-postgres-strict-encrypt\n      enabled: true\n    oss:\n      enabled: true\n  supportLevel: certified\n  documentationUrl: https://docs.airbyte.com/integrations/sources/postgres\nmetadataSpecVersion: \"1.0\"\n```\n\n----------------------------------------\n\nTITLE: Defining MovingWindowCallRatePolicy Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the MovingWindowCallRatePolicy object. This policy limits calls based on a sliding time window, defined by one or more 'rates' (each specifying a 'limit' and 'interval' in ISO 8601 duration format). It requires 'type', 'rates', and 'matchers' properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nMovingWindowCallRatePolicy:\n  type: object\n  title: Moving Window Call Rate Policy\n  description: A policy that allows a fixed number of calls within a moving time window.\n  required:\n    - type\n    - rates\n    - matchers\n  properties:\n    type:\n      type: string\n      enum: [MovingWindowCallRatePolicy]\n    rates:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/Rate\"\n    matchers:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/HttpRequestRegexMatcher\"\n    additionalProperties: true\n```\n```\n\n----------------------------------------\n\nTITLE: Granting Schema Access Permissions in Postgres\nDESCRIPTION: SQL commands to grant a user read-only access to schemas and tables in Postgres. This includes usage permissions on schemas, select permissions on existing tables, and default privileges for future tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT USAGE ON SCHEMA <schema_name> TO <user_name>;\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>;\nALTER DEFAULT PRIVILEGES IN SCHEMA <schema_name> GRANT SELECT ON TABLES TO <user_name>;\n```\n\n----------------------------------------\n\nTITLE: Specifying Common Advanced OAuth Input/Output Specification - YAML\nDESCRIPTION: This YAML snippet defines required OAuth parameters (client_id and client_secret) and their types for both input and output specifications in Airbyte connectors. It includes mapping properties for how these secrets are referenced in the connector config. Dependencies include Airbyte's declarative connector system, and parameters are plain strings with property mappings. Input and output schemas must conform to these requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_61\n\nLANGUAGE: yaml\nCODE:\n```\ncomplete_oauth_server_input_specification:\\n  required:\\n    - client_id\\n    - client_secret\\n  properties:\\n    client_id:\\n      type: string\\n    client_secret:\\n      type: string\\ncomplete_oauth_server_output_specification:\\n  required:\\n    - client_id\\n    - client_secret\\n  properties:\\n    client_id:\\n      type: string\\n      path_in_connector_config:\\n        - client_id\\n    client_secret:\\n      type: string\\n      path_in_connector_config:\\n        - client_secret\\n\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Manually Resetting Raw Table Loading State\nDESCRIPTION: SQL command to reset the _airbyte_loaded_at column in a raw table, which is part of the process for manually triggering a final table recreation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/core-concepts/typing-deduping.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE airbyte_internal.<your_raw_table> SET _airbyte_loaded_at = NULL\n```\n\n----------------------------------------\n\nTITLE: Defining Squarespace Data Streams in Markdown\nDESCRIPTION: This snippet lists the available data streams from the Squarespace connector, including their primary keys, pagination methods, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/squarespace.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| inventory | variantId | DefaultPaginator | ✅ |  ❌  |\n| store_pages | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ✅  |\n| profiles | id | DefaultPaginator | ✅ |  ❌  |\n| orders | id | DefaultPaginator | ✅ |  ✅  |\n| transactions | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Granting Minimum Required Permissions on SingleStore (SQL)\nDESCRIPTION: This snippet demonstrates the SQL commands necessary to create an Airbyte user and grant it the minimal set of required privileges for synchronization operations (CRUD and function executions). The user is created with a specified password and given database-level permissions (including create, drop, execute, and routine privileges) on all tables within the target database. Make sure to replace 'password' and 'db' with appropriate values for your environment. The user host is set to '%' (all hosts); narrow this for extra security.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/singlestore.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE USER airbyte IDENTIFIED BY 'password';\nGRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, INDEX, ALTER, EXECUTE, CREATE ROUTINE, ALTER ROUTINE, CREATE DATABASE ON db.* TO 'airbyte'@'%';\n```\n\n----------------------------------------\n\nTITLE: Configuring Error Handler to Retry on 404 HTTP Status in YAML\nDESCRIPTION: Provides a configuration snippet for requester error handling, demonstrating how to retry requests that return HTTP 404 errors by setting a corresponding response filter and action. No specific dependencies beyond YAML syntax; used within Airbyte's request configuration for custom retry logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: RETRY\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Source Connector via JSON - JSON\nDESCRIPTION: Defines a JSON object representing the configuration for an Airbyte source connector. This snippet illustrates how to specify the connector's name, workspace ID, definition ID, and secure configuration values like client credentials and start date. Designed for direct use in the Airbyte API or Terraform provider, dependencies include access to an Airbyte workspace, a valid connector definition, and replacement of placeholder secrets. Inputs are the JSON key-value fields; outputs are the correct configuration of a source connector when submitted via API. Sensitive data (client_id, client_secret) must be replaced before use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.6.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"Airbyte\",\n  \"workspaceId\": \"c0e5b294-2c71-475b-ae9c-6d70b36ff4f4\",\n  \"definitionId\": \"284f6466-3004-4d83-a9b2-e4b36cbbbd41\",\n  \"configuration\": {\n    \"client_id\": \"******\", // Replace this with your real client ID.\n    \"start_date\": \"2024-01-01T00:00:00Z\",\n    \"client_secret\": \"******\" // Replace this with your real client secret.\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteLogMessage Structure in YAML\nDESCRIPTION: Specifies the structure of the AirbyteLogMessage, which is used for debugging an Actor. It includes fields for log level, message content, and an optional stack trace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteLogMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - level\n    - message\n  properties:\n    level:\n      description: \"log level\"\n      type: string\n      enum:\n        - FATAL\n        - ERROR\n        - WARN\n        - INFO\n        - DEBUG\n        - TRACE\n    message:\n      description: \"log message\"\n      type: string\n    stack_trace:\n      description: \"an optional stack trace if the log message corresponds to an exception\"\n      type: string\n```\n\n----------------------------------------\n\nTITLE: Configuring Connector Resource Limits in Airbyte\nDESCRIPTION: YAML configuration for setting CPU and memory limits for connector pods in Airbyte. This helps control resource usage and prevent pod scheduling issues by defining both request and limit values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  edition: \"enterprise\"\n  ...\n  jobs:\n    resources:\n      limits:\n        cpu: ## e.g. 250m\n        memory: ## e.g. 500m\n      requests:\n        cpu: ## e.g. 75m\n        memory: ## e.g. 150m\n```\n\n----------------------------------------\n\nTITLE: Running Kvdb Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run various connector operations locally, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Streams Configuration Table in Markdown\nDESCRIPTION: Table defining the available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/open-data-dc.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| locations |  | No pagination | ✅ |  ❌  |\n| units | UnitNum | No pagination | ✅ |  ❌  |\n| ssls | SSL | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Node Selector Format\nDESCRIPTION: Format for specifying Kubernetes node selectors using key-value pairs\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-airbyte.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nkey1=value1,key2=value2\n```\n\n----------------------------------------\n\nTITLE: Importing Concurrent Stream Facade and Cursor Support Libraries in Python\nDESCRIPTION: These import statements bring in Airbyte CDK modules necessary for wrapping streams in FACADES to enable concurrency and for tracking state with concurrent cursors. They are prerequisites for building and configuring stream objects with advanced concurrent capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# import the following libraries\\nfrom airbyte_cdk.sources.streams.concurrent.adapters import StreamFacade\\nfrom airbyte_cdk.sources.streams.concurrent.cursor import CursorField, ConcurrentCursor, FinalStateCursor\n```\n\n----------------------------------------\n\nTITLE: Basic Parameter Schema Definition in YAML\nDESCRIPTION: Defines the basic schema structure for parameters, allowing any additional properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/parameters.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"$parameters\":\n  type: object\n  additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Decoding XML API Response to Normalized JSON (Airbyte, XML)\nDESCRIPTION: Shows an example XML response from an API, which Airbyte converts into a normalized JSON format using standard attribute-to-property translation rules. Requires XML parsing capability and adherence to a published XML-to-JSON mapping convention. Nested elements, attributes (with '@'), and lists are handled per convention, making data consumable by JSON-based record selection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\n<weatherdata>\n  <location>\n    <name>Lyon</name>\n    <type></type>\n    <country>FR</country>\n    <timezone>7200</timezone>\n  </location>\n  <sun rise=\\\"2024-10-11T05:52:02\\\" set=\\\"2024-10-11T17:02:14\\\"></sun>\n  <forecast>\n    <time from=\\\"2024-10-10T21:00:00\\\" to=\\\"2024-10-11T00:00:00\\\">\n      <symbol number=\\\"800\\\" name=\\\"clear sky\\\" var=\\\"01n\\\"></symbol>\n      <precipitation probability=\\\"0\\\"></precipitation>\n      <windDirection deg=\\\"156\\\" code=\\\"SSE\\\" name=\\\"South-southeast\\\"></windDirection>\n      <windSpeed mps=\\\"0.59\\\" unit=\\\"m/s\\\" name=\\\"Calm\\\"></windSpeed>\n      <windGust gust=\\\"0.73\\\" unit=\\\"m/s\\\"></windGust>\n    </time>\n    <time from=\\\"2024-10-11T00:00:00\\\" to=\\\"2024-10-11T03:00:00\\\">\n      <symbol number=\\\"800\\\" name=\\\"clear sky\\\" var=\\\"01n\\\"></symbol>\n      <precipitation probability=\\\"0\\\"></precipitation>\n      <windDirection deg=\\\"307\\\" code=\\\"NW\\\" name=\\\"Northwest\\\"></windDirection>\n      <windSpeed mps=\\\"0.77\\\" unit=\\\"m/s\\\" name=\\\"Calm\\\"></windSpeed>\n      <windGust gust=\\\"0.89\\\" unit=\\\"m/s\\\"></windGust>\n    </time>\n    ...\n  </forecast>\n</weatherdata>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"weatherdata\": {\n    \"location\": {\n      \"name\": \"Lyon\",\n      \"country\": \"FR\",\n      \"timezone\": \"7200\",\n    },\n    \"sun\": {\n      \"@rise\": \"2024-10-11T05:52:02\",\n      \"@set\": \"2024-10-11T17:02:14\"\n    },\n    \"forecast\": {\n      \"time\": [\n        {\n          \"@from\": \"2024-10-10T21:00:00\",\n          \"@to\": \"2024-10-11T00:00:00\",\n          \"symbol\": {\n            \"@number\": \"800\",\n            \"@name\": \"clear sky\",\n            \"@var\": \"01n\"\n          },\n          \"precipitation\": {\n            \"@probability\": \"0\"\n          },\n          \"windDirection\": {\n            \"@deg\": \"156\",\n            \"@code\": \"SSE\",\n            \"@name\": \"South-southeast\"\n          },\n          \"windSpeed\": {\n            \"@mps\": \"0.59\",\n            \"@unit\": \"m/s\",\n            \"@name\": \"Calm\"\n          },\n          \"windGust\": {\n            \"@gust\": \"0.73\",\n            \"@unit\": \"m/s\"\n          }\n        },\n        {\n          \"@from\": \"2024-10-11T00:00:00\",\n          \"@to\": \"2024-10-11T03:00:00\",\n          \"symbol\": {\n            \"@number\": \"800\",\n            \"@name\": \"clear sky\",\n            \"@var\": \"01n\"\n          },\n          \"precipitation\": {\n            \"@probability\": \"0\"\n          },\n          \"windDirection\": {\n            \"@deg\": \"307\",\n            \"@code\": \"NW\",\n            \"@name\": \"Northwest\"\n          },\n          \"windSpeed\": {\n            \"@mps\": \"0.77\",\n            \"@unit\": \"m/s\",\n            \"@name\": \"Calm\"\n          },\n          \"windGust\": {\n            \"@gust\": \"0.89\",\n            \"@unit\": \"m/s\"\n          }\n        },\n        ...\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Oracle User for Airbyte (SQL)\nDESCRIPTION: Creates a new Oracle user named 'airbyte' identified by a specified password and grants it the necessary permission (CREATE SESSION) to establish a connection to the database. This is a recommended step for isolating Airbyte's database access for security and auditing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oracle.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte IDENTIFIED BY <your_password_here>;\nGRANT CREATE SESSION TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Invalid Enum Definition in Airbyte Connector Spec\nDESCRIPTION: Example of an invalid enum implementation in a connector spec where duplicate values are used. This demonstrates Airbyte's restriction that enum values must be unique, unlike some JSONSchema drafts that don't enforce this restriction.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"connection_specification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"File Source Spec\",\n    \"type\": \"object\",\n    \"required\": [\"format\"],\n    \"properties\": {\n      \"dataset_name\": {\n        ...\n      },\n      \"format\": {\n        type: \"string\",\n        enum: [\"a_format\", \"another_format\", \"a_format\"]\n      },\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: SurveyMonkey Survey Responses Schema Definition (JSON Schema)\nDESCRIPTION: Defines the output JSON schema for the survey_responses stream in SurveyMonkey connector. Specifies permissible fields, data types, and nullability. Used to validate records emitted by the connector and for catalog configuration. No dependencies; interpreted by Airbyte and standard JSON schema tools.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"$schema\\\": \\\"http://json-schema.org/schema#\\\",\\n  \\\"properties\\\": {\\n    \\\"analyze_url\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"collect_stats\\\": {\\n      \\\"properties\\\": {\\n        \\\"status\\\": {\\n          \\\"properties\\\": {\\n            \\\"open\\\": {\\n              \\\"type\\\": [\\\"number\\\", \\\"null\\\"]\\n            }\\n          },\\n          \\\"type\\\": [\\\"object\\\", \\\"null\\\"]\\n        },\\n        \\\"total_count\\\": {\\n          \\\"type\\\": [\\\"number\\\", \\\"null\\\"]\\n        },\\n        \\\"type\\\": {\\n          \\\"properties\\\": {\\n            \\\"weblink\\\": {\\n              \\\"type\\\": [\\\"number\\\", \\\"null\\\"]\\n            }\\n          },\\n          \\\"type\\\": [\\\"object\\\", \\\"null\\\"]\\n        }\\n      },\\n      \\\"type\\\": [\\\"object\\\", \\\"null\\\"]\\n    },\\n    \\\"date_created\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"date_modified\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"href\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"id\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"language\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"nickname\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"preview\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    },\\n    \\\"question_count\\\": {\\n      \\\"type\\\": [\\\"number\\\", \\\"null\\\"]\\n    },\\n    \\\"response_count\\\": {\\n      \\\"type\\\": [\\\"number\\\", \\\"null\\\"]\\n    },\\n    \\\"title\\\": {\\n      \\\"type\\\": [\\\"string\\\", \\\"null\\\"]\\n    }\\n  },\\n  \\\"type\\\": \\\"object\\\"\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Requesting OAuth Access Token using Refresh Token (curl)\nDESCRIPTION: Demonstrates a `curl` command making a POST request to an OAuth token refresh endpoint. It sends the client ID, client secret, refresh token, and grant type (`refresh_token`) in the JSON body to obtain a new access token, as shown in the Square API example context. This is typically the first step in an OAuth flow using a refresh token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_3\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"client_id\": \"<client id>\", \"client_secret\": \"<client secret>\", \"refresh_token\": \"<refresh token>\", \"grant_type\": \"refresh_token\" }' \\\n  <token refresh endpoint>\n```\n\n----------------------------------------\n\nTITLE: Build Workflow Diagrams\nDESCRIPTION: Mermaid flowcharts showing the build pipeline workflows for both Python/Low Code and Java connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_19\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    arch(For each platform amd64/arm64)\n    connector[Build connector image]\n    load[Load to docker host with :dev tag, current platform]\n    spec[Get spec]\n    arch-->connector-->spec--\"if success\"-->load\n```\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    arch(For each platform amd64/arm64)\n    distTar[Gradle distTar task run]\n    base[Build integration base]\n    java_base[Build integration base Java]\n    normalization[Build Normalization]\n    connector[Build connector image]\n\n    arch-->base-->java_base-->connector\n    distTar-->connector\n    normalization--\"if supports normalization\"-->connector\n\n    load[Load to docker host with :dev tag]\n    spec[Get spec]\n    connector-->spec--\"if success\"-->load\n```\n\n----------------------------------------\n\nTITLE: Example MovingWindowCallRatePolicy Usage in YAML\nDESCRIPTION: This YAML snippet provides an example of using the MovingWindowCallRatePolicy within an HTTPAPIBudget. It sets a rate limit allowing a maximum of 100 calls within any rolling 1-minute ('PT1M') period for GET requests to 'https://api.example.com' with paths starting with '/orders'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napi_budget:\n  type: \"HTTPAPIBudget\"\n  policies:\n    - type: \"MovingWindowCallRatePolicy\"\n      rates:\n        - limit: 100\n          interval: \"PT1M\"\n      matchers:\n        - method: \"GET\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/orders\"\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring External Database for Airbyte in YAML\nDESCRIPTION: This YAML configuration sets up an external database for Airbyte, disabling the default internal Postgres and configuring the external database details. It's added to the values.yaml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\n  enabled: false\n\nglobal:\n  database:\n    # -- Secret name where database credentials are stored\n    secretName: \"\" # e.g. \"airbyte-config-secrets\"\n\n    # -- The database host\n    host: \"\"\n    # -- The key within `secretName` where host is stored \n    #hostSecretKey: \"\" # e.g. \"database-host\"\n\n    # -- The database port\n    port: \"\"\n    # -- The key within `secretName` where port is stored \n    #portSecretKey: \"\" # e.g. \"database-port\" \n\n    # -- The database name\n    database: \"\"\n    # -- The key within `secretName` where the database name is stored \n    #databaseSecretKey: \"\" # e.g. \"database-name\" \n\n    # -- The database user\n    user: \"\" # -- The key within `secretName` where the user is stored \n    #userSecretKey: \"\" # e.g. \"database-user\"\n\n    # -- The key within `secretName` where password is stored\n    passwordSecretKey: \"\" # e.g.\"database-password\"\n```\n\n----------------------------------------\n\nTITLE: Transforming Array Schema to String Schema in JSON\nDESCRIPTION: Example showing how a destination connector might conceptually transform a source schema if it doesn't natively support array types. In this case, the \"appointments\" array field is treated as a single string field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"appointments\": {\n      \"type\": \"string\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Local Docker Image into abctl Kind Cluster (Shell)\nDESCRIPTION: Uses the `kind` CLI to load a specified local Docker image (e.g., `airbyte/source-<source-name>:dev`) into the nodes of the Kubernetes cluster named `airbyte-abctl`. This makes the custom image accessible to the Airbyte deployment within the cluster. Requires `kind` and the specified local image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkind load docker-image airbyte/source-<source-name>:dev -n airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Disabling Airbyte Authentication via values.yaml (YAML)\nDESCRIPTION: Provides the YAML configuration snippet to add to the `values.yaml` file for Helm or `abctl` deployments. Setting `global.auth.enabled` to `false` disables Airbyte's built-in authentication, which is useful when using an external authentication proxy or SSO.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  auth:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Local Connector Execution Commands\nDESCRIPTION: Commands for running the connector locally to test specification, configuration check, and write operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Various Data Types in JSON\nDESCRIPTION: This snippet includes records for testing different data types, particularly focusing on integer and big integer values. It verifies max values for int64 and handles 28-digit values for big integers, which are beyond int64 capacity but within BigQuery NUMERIC column bounds.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"types_testing\",\n    \"data\": {\n      \"id\": 1,\n      \"airbyte_integer_column\": 9223372036854775807,\n      \"nullable_airbyte_integer_column\": 9223372036854775807,\n      \"big_integer_column\": \"1234567890123456789012345678\",\n      \"nullable_big_integer_column\": \"1234567890123456789012345678\"\n    },\n    \"emitted_at\": 1623959926\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Injecting Page Size into GraphQL Request Body - YAML\nDESCRIPTION: This YAML snippet configures a component to inject a page size variable into a GraphQL request's JSON body using the field_path property. It requires the RequestOption schema with body_json injection. field_path specifies the nested keys to insert the value, allowing flexible parameterization of GraphQL variables. Input values are injected at the specified path; outputs enable GraphQL requests to be correctly paginated.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npage_size_option:\n  request_option:\n    type: RequestOption\n    inject_into: body_json\n    field_path:\n      - variables\n      - limit\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte catalog for incremental sync in JSON\nDESCRIPTION: Defines an Airbyte catalog JSON object to support both full_refresh and incremental sync for the surveys stream, with incremental mode and destination overwrite enabled. This configuration is required for enabling stateful incremental extraction in Airbyte pipelines. Only minimal JSON schema details are specified.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"stream\": {\n        \"name\": \"surveys\",\n        \"json_schema\": {},\n        \"supported_sync_modes\": [\"full_refresh\", \"incremental\"]\n      },\n      \"sync_mode\": \"incremental\",\n      \"destination_sync_mode\": \"overwrite\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Records with Record Filter\nDESCRIPTION: Shows how to use the Record Filter feature to exclude certain records from the final output based on a condition. This example filters out records with 'expired' status.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": 2,\n    \"status\": \"active\"\n  },\n  {\n    \"id\": 3,\n    \"status\": \"expired\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Integration Environment\nDESCRIPTION: SQL commands to set up a complete integration environment in Snowflake including warehouse, database, role, user, and necessary permissions. Creates an XSMALL warehouse with auto-suspend and auto-resume features, sets up a dedicated database, creates a role with full privileges, and configures a user with appropriate access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/setup/snowflake.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE WAREHOUSE INTEGRATION_TEST_WAREHOUSE_NORMALIZATION WITH WAREHOUSE_SIZE = 'XSMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 600 AUTO_RESUME = TRUE;\n\nCREATE DATABASE INTEGRATION_TEST_NORMALIZATION;\n\nCREATE ROLE INTEGRATION_TESTER_NORMALIZATION;\n\nGRANT ALL PRIVILEGES ON WAREHOUSE INTEGRATION_TEST_WAREHOUSE_NORMALIZATION TO ROLE INTEGRATION_TESTER_NORMALIZATION;\nGRANT ALL PRIVILEGES ON DATABASE INTEGRATION_TEST_NORMALIZATION TO ROLE INTEGRATION_TESTER_NORMALIZATION;\nGRANT ALL PRIVILEGES ON FUTURE SCHEMAS IN DATABASE INTEGRATION_TEST_NORMALIZATION TO ROLE INTEGRATION_TESTER_NORMALIZATION;\nGRANT ALL PRIVILEGES ON FUTURE TABLES IN DATABASE INTEGRATION_TEST_NORMALIZATION TO ROLE INTEGRATION_TESTER_NORMALIZATION;\n\n# Add real password here and remove this comment\nCREATE USER INTEGRATION_TEST_USER_NORMALIZATION PASSWORD='test' DEFAULT_ROLE=INTEGRATION_TESTER_NORMALIZATION DEFAULT_WAREHOUSE=INTEGRATION_TEST_WAREHOUSE_NORMALIZATION MUST_CHANGE_PASSWORD=false;\n\nGRANT ROLE INTEGRATION_TESTER_NORMALIZATION TO USER INTEGRATION_TEST_USER_NORMALIZATION;\n\nCREATE SCHEMA INTEGRATION_TEST_NORMALIZATION.TEST_SCHEMA;\n```\n\n----------------------------------------\n\nTITLE: Building and Running Custom Connector Docker Image\nDESCRIPTION: Commands to build a custom Docker image for the connector and run various connector commands using the Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-aws-datalake:dev .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-aws-datalake:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-aws-datalake:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-aws-datalake:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: MySQL Documentation Quote on Identifier Case Sensitivity\nDESCRIPTION: A quote from MySQL documentation recommending consistent use of lowercase for database and table names to ensure maximum portability and ease of use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mysql.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"It is best to adopt a consistent convention, such as always creating and referring to databases and tables using lowercase names.\\n This convention is recommended for maximum portability and ease of use.\"\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector's various functions within a Docker container\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-timeplus:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-timeplus:dev check --config /secrets/config.json\ncat integration_tests/messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-timeplus:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Setting MySQL System Variable in SQL\nDESCRIPTION: SQL command to set the local_infile system variable to true, which is required for Airbyte to use LOAD DATA LOCAL INFILE for data loading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mysql.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSET GLOBAL local_infile = true\n```\n\n----------------------------------------\n\nTITLE: Defining UnlimitedCallRatePolicy Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the UnlimitedCallRatePolicy object. This policy type allows unlimited API calls for requests that match the specified criteria. It requires the 'type' property to be 'UnlimitedCallRatePolicy' and a 'matchers' array referencing HttpRequestRegexMatcher definitions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nUnlimitedCallRatePolicy:\n  type: object\n  title: Unlimited Call Rate Policy\n  description: A policy that allows unlimited calls for specific requests.\n  required:\n    - type\n    - matchers\n  properties:\n    type:\n      type: string\n      enum: [UnlimitedCallRatePolicy]\n    matchers:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/HttpRequestRegexMatcher\"\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Terraform Provider with Variables (HCL)\nDESCRIPTION: This HCL snippet updates the `main.tf` file to configure the Airbyte provider. It uses variables (`var.client_id`, `var.client_secret`) for sensitive credentials and explicitly sets the `server_url` for the Airbyte instance. This approach avoids hardcoding secrets directly in the main configuration file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nterraform {\n    required_providers {\n        airbyte = {\n        source = \"airbytehq/airbyte\"\n        version = \"0.6.5\"\n        }\n    }\n}\n\nprovider \"airbyte\" {\n    // highlight-start\n    client_id = var.client_id\n    client_secret = var.client_secret\n\n    # Include server_url if running locally\n    server_url = \"http://localhost:8000/api/public/v1/\"\n    // highlight-end\n}\n```\n\n----------------------------------------\n\nTITLE: Assigning Public Key to Snowflake User (SQL)\nDESCRIPTION: SQL command to associate a public key with a specific Snowflake user for key pair authentication. Replace `<user_name>` with the target username and `<public_key_value>` with the actual public key content (excluding -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- markers).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nalter user <user_name> set rsa_public_key=<public_key_value>;\n```\n\n----------------------------------------\n\nTITLE: Creating Database Tables for Airbyte Source Example - SQL\nDESCRIPTION: This snippet defines two relational database tables, 'airlines' and 'pilots', using SQL. The 'airlines' table contains 'id' and 'name' columns (integer, varchar), while the 'pilots' table includes 'id', 'airline_id', and 'name' columns. These schemas represent the raw source data structure that will be modeled in the Airbyte catalog examples. There are no dependencies. Inputs are DDL (Data Definition Language) statements, and outputs are SQL database tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE \"airlines\" (\n    \"id\"   INTEGER,\n    \"name\" VARCHAR\n);\n\nCREATE TABLE \"pilots\" (\n    \"id\"   INTEGER,\n    \"airline_id\" INTEGER,\n    \"name\" VARCHAR\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a dedicated PostgreSQL user for Airbyte in Cloud SQL\nDESCRIPTION: SQL command to create a new PostgreSQL user with a password for Airbyte to use for data replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER <user_name> PASSWORD 'your_password_here';\n```\n\n----------------------------------------\n\nTITLE: Mapping JSON Date Format to Avro Logical Type\nDESCRIPTION: Shows the direct mapping of the JSON `date` format to an Avro `int` type with the `date` logical type. The integer represents the number of days from the Unix epoch (1970-01-01). This is an intermediate representation before the final nullable schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"int\",\n  \"logicalType\": \"date\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Heartbeat Table to Airbyte Publication in PostgreSQL\nDESCRIPTION: SQL command to add the previously created `airbyte_heartbeat` table to the specified Airbyte publication in PostgreSQL. This ensures that changes made to the heartbeat table are captured by the CDC process, helping to advance the LSN. Requires the publication name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER PUBLICATION <publicationName> ADD TABLE airbyte_heartbeat;\n```\n\n----------------------------------------\n\nTITLE: Importing State Converter for Concurrent Cursor Management in Python\nDESCRIPTION: This import statement provides the required `EpochValueConcurrentStreamStateConverter` which is used to define timestamp-based state conversion for streams supporting incremental reads. It is a prerequisite for cursor management in concurrent environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# import the following library\\nfrom airbyte_cdk.sources.streams.concurrent.state_converters.datetime_stream_state_converter import EpochValueConcurrentStreamStateConverter\n```\n\n----------------------------------------\n\nTITLE: Configuring Transaction Replication Retention in MSSQL\nDESCRIPTION: SQL commands to modify retention settings for Transaction Replication\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sp_changedistributiondb\n  @database = 'distribution',\n  @property = 'max_distretention',\n  @value = 14400 -- 14400 minutes (10 days)\n\nEXEC sp_changedistributiondb\n  @database = 'distribution',\n  @property = 'history_retention',\n  @value = 14400 -- 14400 minutes (10 days)\n\nUSE [msdb]\nGO\nEXEC msdb.dbo.sp_update_jobstep @job_name=N'Distribution clean up: distribution', @step_id=1 ,\n\t\t@command=N'EXEC dbo.sp_MSdistribution_cleanup @min_distretention = 0, @max_distretention = 14400'\nGO\n```\n\n----------------------------------------\n\nTITLE: Entity-Relationship Diagram Component in Markdown\nDESCRIPTION: This code snippet represents a custom Markdown component for displaying an Entity-Relationship Diagram (ERD) for the Instagram connector. It's likely rendered by a specialized documentation system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instagram.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<EntityRelationshipDiagram></EntityRelationshipDiagram>\n```\n\n----------------------------------------\n\nTITLE: Using JSON Schema for Connectors ERD Confirmed Relationships\nDESCRIPTION: JSON schema for defining confirmed relationships between streams in a connector's entity-relationship diagram. The schema allows specifying valid relations and false positives identified during manual validation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"streams\": [\n        {\n            \"name\": <stream_name>,\n            \"relations\": {\n                <stream_name property>: \"<target stream>.<target stream column>\"\n            }\n            \"false_positives\": {\n                <stream_name property>: \"<target stream>.<target stream column>\"\n            }\n        },\n        <...>\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Exchanging Authorization Code for Refresh Token in Strava API\nDESCRIPTION: This cURL command exchanges the authorization code and scope for a refresh token in the Strava API. It requires the client ID, client secret, and authorization code obtained from the OAuth process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/strava.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://www.strava.com/oauth/token \\\n-F client_id=YOUR_CLIENT_ID \\\n-F client_secret=YOUR_CLIENT_SECRET \\\n-F code=AUTHORIZATION_CODE \\\n-F grant_type=authorization_code\n```\n\n----------------------------------------\n\nTITLE: New Handling of Typed Arrays with Mismatched Data in Airbyte\nDESCRIPTION: Shows the new behavior for typed arrays when input data contains elements of a different type. Non-conforming elements (like \"Alice\" in an integer array) are now converted to null, and these conversion failures are reported in `_airbyte_meta.changes[]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"array\", \"items\": { \"type\": \"integer\" } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Schema\n{ \"type\": \"array\", \"items\": [\"null\", \"integer\"] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data\n[1, null*]\n```\n\n----------------------------------------\n\nTITLE: Creating a publication for PostgreSQL CDC replication\nDESCRIPTION: SQL command to create a publication that includes the tables to be replicated using CDC in PostgreSQL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE PUBLICATION airbyte_publication FOR TABLE tbl1, tbl2, tbl3;\n```\n\n----------------------------------------\n\nTITLE: Defining ExponentialBackoffStrategy Schema in YAML\nDESCRIPTION: Schema block for configuring exponential retry backoff. Allows optional 'factor' parameter (default 5) and arbitrary additional parameters. Used in Airbyte YAML to specify exponentially increasing delay strategies for error retries.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nExponentialBackoffStrategy:\n  type: object\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    factor:\n      type: integer\n      default: 5\n```\n\n----------------------------------------\n\nTITLE: Installing Amazon SQS Connector\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Example Woocommerce API Response (Composite Key)\nDESCRIPTION: Provides an example response from the Woocommerce API for shipping zone locations. This data lacks a single unique ID, demonstrating the need for a composite primary key defined by the `code` and `type` fields for downstream deduplication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"code\": \"BR\",\n    \"type\": \"country\"\n  },\n  {\n    \"code\": \"DE\",\n    \"type\": \"country\"\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte to Use Google Secret Manager - YAML\nDESCRIPTION: This YAML snippet configures Airbyte's global secrets management for Google Secret Manager by referencing a Kubernetes secret containing the GCP credentials blob. The type must be set to googleSecretManager, secretName matches the Airbyte credential secret (usually airbyte-config-secrets), and projectId and credentialsSecretKey specify the GCP project and the key used to retrieve the credentials (gcp.json). Prerequisite: The aforementioned Kubernetes secret with appropriate service account credentials should exist. Inputs: projectId, credentialsSecretKey. Outputs: Airbyte set up to use Google Secret Manager for secret retrieval/storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\\n  secretsManager:\\n    type: googleSecretManager\\n    secretName: \\\"airbyte-config-secrets\\\" # Name of your Kubernetes secret.\\n    googleSecretManager:\\n      projectId: <project-id>\\n      credentialsSecretKey: gcp.json\\n\n```\n\n----------------------------------------\n\nTITLE: Defining the Fixed Schemaless Schema in Airbyte (JSON)\nDESCRIPTION: This JSON structure represents the fixed schema applied when the 'schemaless schema' option is chosen for an Airbyte source. It defines a single top-level field named 'data' of type 'object'. During synchronization, the entire original record from the source is nested within this 'data' field in the destination. This approach avoids schema inference issues but results in less structured data in the destination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/schemaless-sources-and-destinations.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"data\": object}\n```\n\n----------------------------------------\n\nTITLE: Executing Airbyte Source Read Operation via CLI\nDESCRIPTION: This bash command uses `poetry` to run the `read` command of the custom Airbyte source connector (`source-survey-monkey-demo`). It requires a configuration file (`secrets/config.json`) and a configured catalog (`integration_tests/configured_catalog.json`) to specify credentials and the data streams to sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Output Schema for GlassFlow Messages in Markdown\nDESCRIPTION: This snippet outlines the structure of messages sent to GlassFlow, including the stream name, namespace, emission timestamp, and data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/glassflow.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `stream`: the name of stream where the data is coming from.\n- `namespace`: namespace if available from the stream.\n- `emitted_at`: timestamp the `AirbyteRecord` was emitted at.\n- `data`: `AirbyteRecord` data.\n```\n\n----------------------------------------\n\nTITLE: Defining Nested Table Schema with Array and Object in JSON\nDESCRIPTION: This snippet illustrates a JSON schema where a column can be an array or a nested object, mapping known columns to their respective datatypes for complex data ingestion. The snippet is intended for Airbyte's user-provided schema feature, allowing the connector to preserve explicit typing rather than inferring it. This is valuable for datasets anticipating nested fields or evolving data structures. Inputs must be valid JSON with appropriate type-mappings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-blob-storage.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\\\"username\\\": \\\"string\\\", \\\"friends\\\": \\\"array\\\", \\\"information\\\": \\\"object\\\"}\n```\n\n----------------------------------------\n\nTITLE: Requesting OAuth 2.0 Refresh Token for TrustPilot API\nDESCRIPTION: HTTP GET request to obtain an OAuth 2.0 refresh token for TrustPilot API authentication. Requires API key and secret, along with TrustPilot username and password.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/trustpilot.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.trustpilot.com/v1/oauth/oauth-business-users-for-applications/accesstoken\nAuthorization: Basic base64(apikey:secret)\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=password&username=YOUR_TRUSTPILOT_USERNAME_OR_LOGIN_EMAIL_HERE&password=YOUR_TRUSTPILOT_PASSWORD_HERE\n```\n\n----------------------------------------\n\nTITLE: Configuring oneOf Dropdown Display Type in Airbyte Connector\nDESCRIPTION: Example of configuring the display_type property for a oneOf field to control how it appears in the UI. This configures the update_method field to show as a dropdown menu with options for CDC or standard incremental updates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n\"update_method\": {\n  \"type\": \"object\",\n  \"title\": \"Update Method\",\n  \"display_type\": \"dropdown\",\n  \"oneOf\": [\n    {\n      \"title\": \"Read Changes using Binary Log (CDC)\",\n      \"description\": \"<i>Recommended</i> - Incrementally reads new inserts, updates, and deletes using the MySQL <a href=\\\"https://docs.airbyte.com/integrations/sources/mysql/#change-data-capture-cdc\\\">binary log</a>. This must be enabled on your database.\",\n      \"required\": [\"method\"],\n      \"properties\": {\n        \"method\": {\n          \"type\": \"string\",\n          \"const\": \"CDC\",\n          \"order\": 0\n        },\n        \"initial_waiting_seconds\": {\n          ...\n        },\n        \"server_time_zone\": {\n          ...\n        }\n      }\n    },\n    {\n      \"title\": \"Scan Changes with User Defined Cursor\",\n      \"description\": \"Incrementally detects new inserts and updates using the <a href=\\\"https://docs.airbyte.com/understanding-airbyte/connections/incremental-append/#user-defined-cursor\\\">cursor column</a> chosen when configuring a connection (e.g. created_at, updated_at).\",\n      \"required\": [\"method\"],\n      \"properties\": {\n        \"method\": {\n          \"type\": \"string\",\n          \"const\": \"STANDARD\",\n          \"order\": 0\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example API Request with Page Increment (HTTP)\nDESCRIPTION: Shows an example HTTP GET request to an API endpoint demonstrating the use of 'page_size' and 'page' query parameters for Page Increment pagination. This request asks for page 1 with a size of 2 records.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.example.com/products?page_size=2&page=1\n```\n\n----------------------------------------\n\nTITLE: Creating an Airbyte DAG with Airflow Operator in Python\nDESCRIPTION: This Python snippet demonstrates setting up an Airflow DAG that triggers an Airbyte synchronization job using the AirbyteTriggerSyncOperator. Dependencies include Apache Airflow, the Airbyte provider package (apache-airflow-providers-airbyte), and a configured Airbyte HTTP connection in Airflow. Parameters such as 'airbyte_conn_id', 'connection_id', 'asynchronous', 'timeout', and 'wait_seconds' control the connection and job execution behavior. The DAG can be customized for schedule, task ownership, and sync job configuration. The output is an Airflow DAG named 'trigger_airbyte_job_example' that executes an Airbyte sync job for the specified connection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-the-airflow-airbyte-operator.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.utils.dates import days_ago\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\n\nwith DAG(dag_id='trigger_airbyte_job_example',\n         default_args={'owner': 'airflow'},\n         schedule_interval='@daily',\n         start_date=days_ago(1)\n    ) as dag:\n\n    money_to_json = AirbyteTriggerSyncOperator(\n        task_id='airbyte_money_json_example',\n        airbyte_conn_id='airbyte_conn_example',\n        connection_id='1e3b5a72-7bfd-4808-a13c-204505490110',\n        asynchronous=False,\n        timeout=3600,\n        wait_seconds=3\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Selected Records Output from Inner Field - JSON\nDESCRIPTION: Shows the extracted records from a nested array located at 'data.records'. Demonstrates result of applying a nested field_path extraction using Airbyte configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1\n  },\n  {\n    \"id\": 2\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Granting CDC Permissions to Airbyte User in Oracle SQL\nDESCRIPTION: This set of SQL commands grants the necessary privileges to the 'airbyte' user for performing Change Data Capture (CDC) incremental syncs. These permissions allow the user to access LogMiner, transaction information, and manage necessary internal objects used by Debezium.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nGRANT FLASHBACK ANY TABLE TO airbyte;\nGRANT SELECT ANY TABLE TO airbyte;\nGRANT SELECT_CATALOG_ROLE TO airbyte;\nGRANT EXECUTE_CATALOG_ROLE TO airbyte;\nGRANT SELECT ANY TRANSACTION TO airbyte;\nGRANT LOGMINING TO airbyte;\nGRANT CREATE TABLE TO airbyte;\nGRANT LOCK ANY TABLE TO airbyte;\nGRANT CREATE SEQUENCE TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Installing abctl via Curl Script for Mac/Linux\nDESCRIPTION: Command to install abctl, Airbyte's command-line tool, using a curl script. This is the recommended quick installation method for Mac and Linux users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsfS https://get.airbyte.com | bash -\n```\n\n----------------------------------------\n\nTITLE: Running the Airbyte Source Connector Locally - Python CLI\nDESCRIPTION: This sequence demonstrates running the Outbrain Amplify source connector locally using a Python script and various operational commands: spec, check, discover, and read. It requires a valid Python environment and appropriate configuration files (such as secrets/config.json). Outputs include connector specifications, status check results, discovered schemas, and streaming data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Schema Diff Example: Non-Problematic Type Difference (name)\nDESCRIPTION: Illustrates a difference detected between the declared schema (allowing string or null) and the schema inferred from test data (only null encountered) for the `name` field. This difference might be acceptable if null values are possible but weren't present in the test set, meaning the declared schema is correctly more permissive.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_14\n\nLANGUAGE: diff\nCODE:\n```\n     \"name\": {\n-      \"type\": [\n-        \"string\",\n-        \"null\"\n-      ]\n+      \"type\": \"null\"\n     },\n```\n\n----------------------------------------\n\nTITLE: Defining Source Connector Interfaces in Pseudocode\nDESCRIPTION: Defines the standard interfaces for an Airbyte source connector using language-agnostic pseudocode. It outlines the `spec` function to return the connector's configuration specification, `check` to validate a configuration, `discover` to find the source schema, and `read` to retrieve data records and state messages based on configuration and catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol-docker.md#2025-04-23_snippet_0\n\nLANGUAGE: pseudocode\nCODE:\n```\n```\nspec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\ndiscover(Config) -> AirbyteCatalog\nread(Config, ConfiguredAirbyteCatalog, State) -> Stream<AirbyteRecordMessage | AirbyteStateMessage>\n```\n```\n\n----------------------------------------\n\nTITLE: Example FixedWindowCallRatePolicy Usage in YAML\nDESCRIPTION: This YAML snippet demonstrates how to use the FixedWindowCallRatePolicy within an HTTPAPIBudget. It configures a rate limit of 1000 calls per hour ('PT1H') for GET requests to 'https://api.example.com' whose path starts with '/users'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napi_budget:\n type: \"HTTPAPIBudget\"\n policies:\n   - type: \"FixedWindowCallRatePolicy\"\n     period: \"PT1H\"\n     call_limit: 1000\n     matchers:\n       - method: \"GET\"\n         url_base: \"https://api.example.com\"\n         url_path_pattern: \"^/users\"\n```\n```\n\n----------------------------------------\n\nTITLE: Dynamically Modifying Static Schemas in Airbyte Connector Streams\nDESCRIPTION: This snippet shows how to override the get_json_schema method to extend a static schema with dynamically determined properties. It first calls the parent class implementation to get the base schema from a JSON file, then adds a new property before returning the modified schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/source_rki_covid/schemas/TODO.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema\n```\n\n----------------------------------------\n\nTITLE: Defining HttpRequestRegexMatcher Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the HttpRequestRegexMatcher object, used within rate limit policies. It specifies criteria for matching HTTP requests, including the HTTP 'method', 'url_base', a regex pattern for the 'url_path_pattern', query 'params', and 'headers'. A request must match all specified criteria in a matcher.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nHttpRequestRegexMatcher:\n  type: object\n  properties:\n    method:\n      type: string\n      description: The HTTP method (e.g. GET, POST).\n    url_base:\n      type: string\n      description: The base URL to match (e.g. \"https://api.example.com\" without trailing slash).\n    url_path_pattern:\n      type: string\n      description: A regular expression to match the path portion.\n    params:\n      type: object\n      additionalProperties: true\n    headers:\n      type: object\n      additionalProperties: true\n  additionalProperties: true\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Key Vault for Airbyte Connectors (YAML)\nDESCRIPTION: This YAML snippet configures Airbyte to leverage Azure Key Vault for external secrets storage. It specifies the vault URL, tenant ID, and optional tags for new secrets. Prerequisites include an accessible Azure Key Vault and appropriate cluster identity permissions. The parameters allow you to add context or organizational labels to secrets as needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  secretsManager:\n    type: azureKeyVault\n    azureKeyVault:\n      vaultUrl: ## https://my-vault.vault.azure.net/\n      tenantId: ## 3fc863e9-4740-4871-bdd4-456903a04d4e\n      tags: ## Optional - You may add tags to new secrets created by Airbyte.\n        - key: ## e.g. team\n          value: ## e.g. deployments\n        - key: business-unit\n          value: engineering\n\n```\n\n----------------------------------------\n\nTITLE: Installing Milvus Connector with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Initializing the SurveyMonkeyBaseStream with cursor field in Python\nDESCRIPTION: Modifies the SurveyMonkeyBaseStream class constructor to accept and store a cursor_field for incremental sync functionality. All required stream attributes are set, and the parent HttpStream is initialized with additional arguments. Required for enabling incremental logic throughout the stream implementation; expects arguments for Airbyte's HttpStream base class.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SurveyMonkeyBaseStream(HttpStream, ABC):\n    def __init__(self, name: str, path: str, primary_key: Union[str, List[str]], data_field: Optional[str], cursor_field: Optional[str],\n**kwargs: Any) -> None:\n        self._name = name\n        self._path = path\n        self._primary_key = primary_key\n        self._data_field = data_field\n        self._cursor_field = cursor_field\n        super().__init__(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: CDC Excluded Records with Deletion Tracking\nDESCRIPTION: Change Data Capture records showing insertion and deletion operations with CDC-specific fields like updated_at, lsn, and deleted_at timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"dedup_cdc_excluded\",\"data\":{\"id\":1,\"name\":\"mazda\",\"_ab_cdc_updated_at\":1623849130530,\"_ab_cdc_lsn\":26971624,\"_ab_cdc_deleted_at\":null},\"emitted_at\":1623859926}}\n```\n\n----------------------------------------\n\nTITLE: Selecting Nested Array from JSON Response\nDESCRIPTION: Demonstrates how to select a nested array of records from a JSON response using the Field Path setting. This example uses the NY Times 'Archive' API response structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"copyright\": \"Copyright (c) 2020 The New York Times Company. All Rights Reserved.\",\n  \"response\": {\n    \"docs\": [\n      {\n        \"abstract\": \"From the Treaty of Versailles to Prohibition, the events of that year shaped America, and the world, for a century to come. \",\n        \"web_url\": \"https://www.nytimes.com/2018/12/31/opinion/1919-america.html\",\n        \"snippet\": \"From the Treaty of Versailles to Prohibition, the events of that year shaped America, and the world, for a century to come. \"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Gmail Connector with Airbyte CI - Bash\nDESCRIPTION: This Bash snippet runs the build process for the Gmail connector using the 'airbyte-ci' tool. It constructs a development Docker image named 'source-gmail:dev', which can be used for local testing. Dependencies include an installed and configured 'airbyte-ci' environment, and the '--name=source-gmail' parameter specifies the connector to build.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gmail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gmail build\n```\n\n----------------------------------------\n\nTITLE: Testing NewsData.io Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the NewsData.io source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-newsdata-io/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-newsdata-io test\n```\n\n----------------------------------------\n\nTITLE: Building and Running Custom Facebook Pages Connector\nDESCRIPTION: Commands to build a custom Docker image for the Facebook Pages connector and run the spec command against it, useful for testing modifications to the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-facebook-pages:dev .\n# Running the spec command against your patched connector\ndocker run airbyte/source-facebook-pages:dev spec\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Source-Nutshell Connector via airbyte-ci in Bash\nDESCRIPTION: This Bash command runs the connector's acceptance tests locally using airbyte-ci with the test command. It targets the source-nutshell connector, verifying its compatibility and functionality in a local development environment. Requires the airbyte-ci tool and any test dependencies to be properly set up prior to execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nutshell/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nutshell test\n```\n\n----------------------------------------\n\nTITLE: Example Test Records for Schema Detection\nDESCRIPTION: Presents a JSON array of sample records used during the 'Test Read' phase in the Airbyte Connector Builder. These records are analyzed to automatically infer the data structure and generate a 'Detected schema'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": \"40205efe-5f94-11ed-aa11-7d1ac831a909\",\n    \"status\": \"SENT\",\n    \"subject\": \"Hello from Integration Test\",\n    \"created_at\": \"2022-11-08T18:36:25+00:00\",\n    \"sent_at\": \"2022-11-08T18:36:55+00:00\"\n  },\n  {\n    \"id\": \"91546616-5ef0-11ed-90c7-fbeacb2ee1eb\",\n    \"status\": \"SENT\",\n    \"subject\": \"Hello my first campaign\",\n    \"created_at\": \"2022-11-07T23:04:44+00:00\",\n    \"sent_at\": \"2022-11-08T12:48:27+00:00\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: JSONL Output Format Example with Airbyte Metadata - Text\nDESCRIPTION: This snippet demonstrates the output format of Airbyte's JSON Lines (JSONL) exports, with one JSON object per line. Each object includes system-generated metadata fields (_airbyte_ab_id, _airbyte_emitted_at) alongside _airbyte_data containing the source row. Outputs are newline-delimited JSON objects; input is derived from source JSON arrays. Compression and filename conventions may apply. Actual content structure depends on schema and normalization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n{ \\\"_airbyte_ab_id\\\": \\\"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\\\", \\\"_airbyte_emitted_at\\\": \\\"1622135805000\\\", \\\"_airbyte_data\\\": { \\\"user_id\\\": 123, \\\"name\\\": { \\\"first\\\": \\\"John\\\", \\\"last\\\": \\\"Doe\\\" } } }\\n{ \\\"_airbyte_ab_id\\\": \\\"0a61de1b-9cdd-4455-a739-93572c9a5f20\\\", \\\"_airbyte_emitted_at\\\": \\\"1631948170000\\\", \\\"_airbyte_data\\\": { \\\"user_id\\\": 456, \\\"name\\\": { \\\"first\\\": \\\"Jane\\\", \\\"last\\\": \\\"Roe\\\" } } }\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Secret for GCS Credentials in YAML\nDESCRIPTION: Defines a Kubernetes Secret named `airbyte-config-secrets` of type `Opaque` to store Google Cloud Storage service account credentials. The credentials, including the private key, are provided as a JSON string under the `gcp.json` key within `stringData`. This secret is used by Airbyte when configured with GCS storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  gcp.json: |\n  {\n    \"type\": \"service_account\",\n    \"project_id\": \"cloud-proj\",\n    \"private_key_id\": \"2f3b9c8e7d5a1b4f23e697c0d84af6e1\",\n    \"private_key\": \"-----BEGIN PRIVATE KEY-----<REDACTED>\\n-----END PRIVATE KEY-----\\n\",\n    \"client_email\": \"cloud-proj.iam.gserviceaccount.com\",\n    \"client_id\": \"9876543210987654321\",\n    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/cloud-proj.iam.gserviceaccount.com\"\n  }\n\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the GitBook Connector using airbyte-ci (Bash)\nDESCRIPTION: This command executes the standard acceptance tests defined for the `source-gitbook` connector using the `airbyte-ci` tool. It validates the connector's functionality against defined specifications. Requires `airbyte-ci` and a previously built connector image (e.g., `source-gitbook:dev`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitbook/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gitbook test\n```\n\n----------------------------------------\n\nTITLE: Configuring ListPartitionRouter Example in YAML\nDESCRIPTION: Demonstrates a practical YAML configuration for a `ListPartitionRouter`. This example iterates over two repository names (\"airbyte\", \"airbyte-secret\"), identifies the partition using the `cursor_field` \"repository\", and uses a `request_option` to inject the current repository name as a request parameter named \"repository\" into outgoing HTTP requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npartition_router:\n  type: ListPartitionRouter\n  values: # NOTE: the schema above incorrectly uses 'partition_values' or 'slice_values', example uses 'values'\n    - \"airbyte\"\n    - \"airbyte-secret\"\n  cursor_field: \"repository\"\n  request_option:\n    type: RequestOption\n    field_name: \"repository\"\n    inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-sharepoint build\n```\n\n----------------------------------------\n\nTITLE: Running MongoDB Integration Tests\nDESCRIPTION: Command to execute acceptance and custom integration tests for the MongoDB connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mongodb-v2/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mongodb-v2:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Creating Acceptance Test for New Format in Java\nDESCRIPTION: Write an acceptance test for the new output format, extending AzureBlobStorageDestinationAcceptanceTest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-azure-blob-storage/README.md#2025-04-23_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class NewFormatAzureBlobStorageDestinationAcceptanceTest extends AzureBlobStorageDestinationAcceptanceTest {\n    // Implement test methods for new format\n}\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteMessage Structure in YAML\nDESCRIPTION: Specifies the structure of the AirbyteMessage, which serves as an envelope for all messages in the Airbyte protocol. It includes fields for different message types such as RECORD, STATE, LOG, SPEC, CONNECTION_STATUS, CATALOG, and TRACE.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - type\n  properties:\n    type:\n      description: \"Message type\"\n      type: string\n      enum:\n        - RECORD\n        - STATE\n        - LOG\n        - SPEC\n        - CONNECTION_STATUS\n        - CATALOG\n        - TRACE\n    log:\n      description: \"log message: any kind of logging you want the platform to know about.\"\n      \"$ref\": \"#/definitions/AirbyteLogMessage\"\n    spec:\n      \"$ref\": \"#/definitions/ConnectorSpecification\"\n    connectionStatus:\n      \"$ref\": \"#/definitions/AirbyteConnectionStatus\"\n    catalog:\n      description: \"catalog message: the catalog\"\n      \"$ref\": \"#/definitions/AirbyteCatalog\"\n    record:\n      description: \"record message: the record\"\n      \"$ref\": \"#/definitions/AirbyteRecordMessage\"\n    state:\n      description: \"schema message: the state. Must be the last message produced. The platform uses this information\"\n      \"$ref\": \"#/definitions/AirbyteStateMessage\"\n    trace:\n      description: \"trace message: a message to communicate information about the status and performance of a connector\"\n      \"$ref\": \"#/definitions/AirbyteTraceMessage\"\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Access and Refresh Token - JSON\nDESCRIPTION: A JSON response containing both access_token and refresh_token at the top level, matching the YAML configuration. Both tokens are required for proper Airbyte operation and must be present as shown. The mapping ensures correct assignment in Airbyte configs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_69\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"access_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\",\\n  \\\"refresh_token\\\": \\\"YOUR_REFRESH_TOKEN_123\\\"\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes Secret for Airbyte Authentication (YAML)\nDESCRIPTION: Displays an example YAML structure of the `airbyte-auth-secrets` Kubernetes Secret retrieved using `kubectl`. It includes metadata and base64 encoded `data` fields for `instance-admin-client-id`, `instance-admin-client-secret`, and `instance-admin-password`. The actual values are base64 encoded.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  instance-admin-client-id: Y2Q1ZTc4ZWEtMzkwNy00ZThmLWE1ZWMtMjIyNGVhZTFiYzcw\n  instance-admin-client-secret: cmhvQkhCODlMRmh1REdXMWt3REpHZTJMaUd3N3c2MjU=\n  instance-admin-password: d0V2bklvZEo1QUNHQnpPRWxrOWNSeHdFUGpJMWVzMWg=\nkind: Secret\nmetadata:\n  creationTimestamp: \"2024-07-31T04:22:54Z\"\n  name: airbyte-auth-secrets\n  namespace: airbyte-abctl\n  resourceVersion: \"600\"\n  uid: f47170eb-f739-4e58-9013-b7afb3ac336a\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Snowflake Integration Environment\nDESCRIPTION: SQL script for dropping the integration test environment in Snowflake, including database, user, role, and warehouse.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nDROP DATABASE IF EXISTS INTEGRATION_TEST_DESTINATION;\nDROP USER IF EXISTS INTEGRATION_TEST_USER_DESTINATION;\nDROP ROLE IF EXISTS INTEGRATION_TESTER_DESTINATION;\nDROP WAREHOUSE IF EXISTS INTEGRATION_TEST_WAREHOUSE_DESTINATION;\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Field Display in Airbyte Connector Spec (JSON)\nDESCRIPTION: Provides an example `connectionSpecification` demonstrating how optional fields are handled in the UI. By default, optional fields are collapsed. Setting `always_show: true` on an optional field prevents it from being collapsed and allows it to be ordered alongside required fields using the `order` property.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"connectionSpecification\": {\n    \"type\": \"object\",\n    \"required\": [\"username\", \"account_id\"],\n    \"properties\": {\n      \"username\": {\n        \"type\": \"string\",\n        \"title\": \"Username\",\n        \"order\": 1\n      },\n      \"password\": {\n        \"type\": \"string\",\n        \"title\": \"Password\",\n        \"order\": 2,\n        \"always_show\": true\n      },\n      \"namespace\": {\n        \"type\": \"string\",\n        \"title\": \"Namespace\",\n        \"order\": 3\n      },\n      \"region\": {\n        \"type\": \"string\",\n        \"title\": \"Region\",\n        \"order\": 4\n      },\n      \"account_id\": {\n        \"type\": \"integer\",\n        \"title\": \"Account ID\",\n        \"order\": 5\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating User and Granting CDC Permissions in MSSQL\nDESCRIPTION: SQL commands to create a new user with appropriate CDC access permissions, including db_datareader role assignment and schema access\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nUSE {database name};\nCREATE LOGIN {user name}\n  WITH PASSWORD = '{password}';\nCREATE USER {user name} FOR LOGIN {user name};\nEXEC sp_addrolemember 'db_datareader', '{user name}';\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Docker Registry Secret for Airbyte - Bash\nDESCRIPTION: This Bash command uses kubectl to create a Kubernetes secret named regcred from a Docker authentication configuration file. The secret is of type kubernetes.io/dockerconfigjson, required for pulling images from private registries. The .dockerconfigjson file path must be provided, containing valid Docker registry credentials, and kubectl must be properly set up for the target cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic regcred \\\n--from-file=.dockerconfigjson=<path/to/.docker/config.json> \\\n--type=kubernetes.io/dockerconfigjson\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Date Format\nDESCRIPTION: Represents a JSON schema definition for a field intended to store a date. This schema uses the standard `string` type with the `date` format specifier, indicating the expected string content follows a date pattern.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"string\",\n  \"format\": \"date\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Creating External Data Source in MSSQL\nDESCRIPTION: SQL command to create an external data source in MSSQL. This data source points to the Azure Blob container using the previously created credential.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mssql.md#2025-04-23_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL DATA SOURCE <data_source_name>\nWITH (\n    TYPE = BLOB_STORAGE,\n    LOCATION = 'https://<storage_account>.blob.core.windows.net/<container_name>',\n    CREDENTIAL = <credential_name>\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Website Overview Stream Schema in JSON\nDESCRIPTION: JSON schema for the website_overview stream in the Google Analytics connector. It includes various metrics like users, sessions, pageviews, and bounce rate.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Example Directory Structure for SFTP Folder Path Configuration\nDESCRIPTION: Illustrates a sample directory structure on a remote SFTP server. This example is used to explain how the 'Folder Path' setting in the Airbyte SFTP connector configuration works, allowing users to specify a particular directory (e.g., `/logs/2022`) for data replication, thereby ignoring other folders.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nRoot\n| - logs\n|   | - 2021\n|   | - 2022\n|\n| - files\n|   | - 2021\n|   | - 2022\n```\n\n----------------------------------------\n\nTITLE: Making POST Request to Obtain Access Token\nDESCRIPTION: API endpoint for obtaining an access token with client credentials. The request is made to the applications/token endpoint with client_id and client_secret in the request body.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/api-access-config.md#2025-04-23_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nPOST <YOUR_WEBAPP_URL>/api/v1/applications/token\n```\n\n----------------------------------------\n\nTITLE: Testing Connector Commands in Airbyte CI\nDESCRIPTION: Examples of using the airbyte-ci test command with various options to test single or multiple connectors, certified connectors, and modified connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pokeapi test\nairbyte-ci connectors --name=source-pokeapi --name=source-bigquery test\nairbyte-ci connectors --support-level=certified test\nairbyte-ci connectors --modified test\nairbyte-ci connectors --modified test --only-step=\"acceptance\" --acceptance.-k=test_full_refresh\n```\n\n----------------------------------------\n\nTITLE: Documenting Connector Configuration and Streams in Markdown - Markdown\nDESCRIPTION: This Markdown snippet documents the configuration requirements and available data streams for the Canny manifest-only source connector in Airbyte. It uses markdown tables to describe required input fields, each stream’s capabilities, and a changelog for updates. Users need their Canny API key, which they can retrieve by going to their Canny account's settings page. The snippet expects the user to be familiar with Markdown formatting and does not itself perform any runtime logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/canny.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Canny\nA manifest only source for Canny. https://canny.io/\n\n## Configuration\n\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. You can find your secret API key in Your Canny Subdomain &gt; Settings &gt; API |  |\n\n## Streams\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| boards | id | No pagination | ✅ |  ❌  |\n| categories | id | DefaultPaginator | ✅ |  ❌  |\n| changelog_entries | id | DefaultPaginator | ✅ |  ❌  |\n| comments | id | DefaultPaginator | ✅ |  ❌  |\n| companies | id | DefaultPaginator | ✅ |  ❌  |\n| posts | id | DefaultPaginator | ✅ |  ❌  |\n| status_changes | id | DefaultPaginator | ✅ |  ❌  |\n| tags | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| votes | id | DefaultPaginator | ✅ |  ❌  |\n\n## Changelog\n\n<details>\n  <summary>Expand to review</summary>\n\n| Version | Date       | Pull Request                                             | Subject                                                                                   |\n|---------|------------|----------------------------------------------------------|-------------------------------------------------------------------------------------------|\n| 0.0.21 | 2025-04-19 | [58253](https://github.com/airbytehq/airbyte/pull/58253) | Update dependencies |\n| 0.0.20 | 2025-04-12 | [57653](https://github.com/airbytehq/airbyte/pull/57653) | Update dependencies |\n| 0.0.19 | 2025-04-05 | [57188](https://github.com/airbytehq/airbyte/pull/57188) | Update dependencies |\n| 0.0.18 | 2025-03-29 | [56580](https://github.com/airbytehq/airbyte/pull/56580) | Update dependencies |\n| 0.0.17 | 2025-03-22 | [56093](https://github.com/airbytehq/airbyte/pull/56093) | Update dependencies |\n| 0.0.16 | 2025-03-08 | [55398](https://github.com/airbytehq/airbyte/pull/55398) | Update dependencies |\n| 0.0.15 | 2025-03-01 | [54867](https://github.com/airbytehq/airbyte/pull/54867) | Update dependencies |\n| 0.0.14 | 2025-02-22 | [54212](https://github.com/airbytehq/airbyte/pull/54212) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [53884](https://github.com/airbytehq/airbyte/pull/53884) | Update dependencies |\n| 0.0.12 | 2025-02-08 | [53406](https://github.com/airbytehq/airbyte/pull/53406) | Update dependencies |\n| 0.0.11 | 2025-02-01 | [52902](https://github.com/airbytehq/airbyte/pull/52902) | Update dependencies |\n| 0.0.10 | 2025-01-25 | [52153](https://github.com/airbytehq/airbyte/pull/52153) | Update dependencies |\n| 0.0.9 | 2025-01-18 | [51765](https://github.com/airbytehq/airbyte/pull/51765) | Update dependencies |\n| 0.0.8 | 2025-01-11 | [51237](https://github.com/airbytehq/airbyte/pull/51237) | Update dependencies |\n| 0.0.7 | 2024-12-28 | [50489](https://github.com/airbytehq/airbyte/pull/50489) | Update dependencies |\n| 0.0.6 | 2024-12-21 | [50171](https://github.com/airbytehq/airbyte/pull/50171) | Update dependencies |\n| 0.0.5 | 2024-12-14 | [49574](https://github.com/airbytehq/airbyte/pull/49574) | Update dependencies |\n| 0.0.4 | 2024-12-12 | [49013](https://github.com/airbytehq/airbyte/pull/49013) | Update dependencies |\n| 0.0.3 | 2024-11-04 | [48235](https://github.com/airbytehq/airbyte/pull/48235) | Update dependencies |\n| 0.0.2 | 2024-10-29 | [47727](https://github.com/airbytehq/airbyte/pull/47727) | Update dependencies |\n| 0.0.1 | 2024-09-15 | [45588](https://github.com/airbytehq/airbyte/pull/45588) | Initial release by [@pabloescoder](https://github.com/pabloescoder) via Connector Builder |\n\n</details>\n```\n\n----------------------------------------\n\nTITLE: Defining an Airbyte Stream Schema using JSON Schema\nDESCRIPTION: Defines the JSON Schema for an Airbyte stream based on the preceding example record. It includes required fields like `$schema`, `type`, `additionalProperties: true`, and specifies data types (often allowing `null` for reliability) for each property, including nested objects and arrays. This schema definition is necessary when building connectors with Python or Java CDKs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/schema-reference.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"additionalProperties\": true,\n  \"properties\": {\n    \"id\": {\n      \"type\": [\"null\", \"string\"]\n    },\n    \"date_created\": {\n      \"format\": \"date-time\",\n      \"type\": [\"null\", \"string\"]\n    },\n    \"date_updated\": {\n      \"format\": \"date-time\",\n      \"type\": [\"null\", \"string\"]\n    },\n    \"total\": {\n      \"type\": [\"null\", \"integer\"]\n    },\n    \"status\": {\n      \"type\": [\"string\", \"null\"],\n      \"enum\": [\"published\", \"draft\"]\n    },\n    \"example_obj\": {\n      \"type\": [\"null\", \"object\"],\n      \"additionalProperties\": true,\n      \"properties\": {\n        \"steps\": {\n          \"type\": [\"null\", \"string\"]\n        },\n        \"count_steps\": {\n          \"type\": [\"null\", \"integer\"]\n        }\n      }\n    },\n    \"example_string_array\": {\n      \"items\": {\n        \"type\": [\"null\", \"string\"]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Integration Environment\nDESCRIPTION: SQL script for setting up an integration environment in Snowflake for Airbyte destination testing. Creates warehouse, database, schema, role, and user with necessary privileges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE WAREHOUSE INTEGRATION_TEST_WAREHOUSE_DESTINATION WITH WAREHOUSE_SIZE = 'XSMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 600 AUTO_RESUME = TRUE;\n\nCREATE DATABASE INTEGRATION_TEST_DESTINATION;\nCREATE SCHEMA INTEGRATION_TEST_DESTINATION.RESTRICTED_SCHEMA;\n\nCREATE ROLE INTEGRATION_TESTER_DESTINATION;\n\n# put real bucket name here and remove this comment\nCREATE STORAGE INTEGRATION IF NOT EXISTS GCS_AIRBYTE_INTEGRATION\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = GCS\n  ENABLED = TRUE\n  STORAGE_ALLOWED_LOCATIONS = ('gcs://bucketname');\n\n# put real bucket name here and remove this comment\nCREATE STAGE IF NOT EXISTS GCS_AIRBYTE_STAGE\n  url = 'gcs://bucketname'\n  storage_integration = GCS_AIRBYTE_INTEGRATION;\n\nGRANT ALL PRIVILEGES ON WAREHOUSE INTEGRATION_TEST_WAREHOUSE_DESTINATION TO ROLE INTEGRATION_TESTER_DESTINATION;\nGRANT ALL PRIVILEGES ON DATABASE INTEGRATION_TEST_DESTINATION TO ROLE INTEGRATION_TESTER_DESTINATION;\n\nGRANT ALL PRIVILEGES ON FUTURE SCHEMAS IN DATABASE INTEGRATION_TEST_DESTINATION TO ROLE INTEGRATION_TESTER_DESTINATION;\nGRANT ALL PRIVILEGES ON FUTURE TABLES IN DATABASE INTEGRATION_TEST_DESTINATION TO ROLE INTEGRATION_TESTER_DESTINATION;\n\nGRANT USAGE ON INTEGRATION GCS_AIRBYTE_INTEGRATION TO ROLE INTEGRATION_TESTER_DESTINATION;\nGRANT USAGE ON STAGE GCS_AIRBYTE_STAGE TO ROLE INTEGRATION_TESTER_DESTINATION;\n\n# Add real password here and remove this comment\nCREATE USER INTEGRATION_TEST_USER_DESTINATION PASSWORD='test' DEFAULT_ROLE=INTEGRATION_TESTER_DESTINATION DEFAULT_WAREHOUSE=INTEGRATION_TEST_WAREHOUSE_DESTINATION MUST_CHANGE_PASSWORD=false;\n\nGRANT ROLE INTEGRATION_TESTER_DESTINATION TO USER INTEGRATION_TEST_USER_DESTINATION;\n\nCREATE SCHEMA INTEGRATION_TEST_DESTINATION.TEST_SCHEMA;\n\nDESC STORAGE INTEGRATION GCS_AIRBYTE_INTEGRATION;\n```\n\n----------------------------------------\n\nTITLE: MySQL Server Binlog Configuration\nDESCRIPTION: Configuration settings required in MySQL server config file to enable binary logging for CDC functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nserver-id                  = 223344\nlog_bin                    = mysql-bin\nbinlog_format              = ROW\nbinlog_row_image           = FULL\nbinlog_expire_logs_seconds  = 864000\n```\n\n----------------------------------------\n\nTITLE: Importing SchemaGenerator Component for Schema Creation - JavaScript\nDESCRIPTION: This code snippet imports the SchemaGenerator component from a local file to facilitate UI-based schema generation for mock source stream configuration. Its dependency is the './e2e-test.js' module, which must provide the SchemaGenerator export. Parameters or usage context are not directly specified, but this enables the next usage of the <SchemaGenerator /> component within the documentation, allowing developers to interactively generate JSON schemas.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/e2e-test.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport {SchemaGenerator} from './e2e-test.js'\n```\n\n----------------------------------------\n\nTITLE: Defining AWS IAM Policy for S3 Access (JSON)\nDESCRIPTION: This JSON policy defines the necessary permissions for an AWS IAM Role used by Airbyte to interact with Amazon S3. It grants permissions to list all buckets, list and get location for a specific bucket, and perform object operations (Put, Get, Delete) within that specific bucket. The placeholder `YOUR-S3-BUCKET-NAME` must be replaced with the actual S3 bucket name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/infrastructure/aws.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\":\n    [\n      { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"*\" },\n      {\n        \"Effect\": \"Allow\",\n        \"Action\": [\"s3:ListBucket\", \"s3:GetBucketLocation\"],\n        \"Resource\": \"arn:aws:s3:::YOUR-S3-BUCKET-NAME\"\n      },\n      {\n        \"Effect\": \"Allow\",\n        \"Action\":\n          [\n            \"s3:PutObject\",\n            \"s3:PutObjectAcl\",\n            \"s3:GetObject\",\n            \"s3:GetObjectAcl\",\n            \"s3:DeleteObject\"\n          ],\n        \"Resource\": \"arn:aws:s3:::YOUR-S3-BUCKET-NAME/*\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Converting JSON Arrays with Type-Specific Items to Avro\nDESCRIPTION: Example showing how a JSON schema with an array field having different item types (string, number) is converted to Avro schema with a union type to accommodate different item types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"array_field\": {\n    \"type\": \"array\",\n    \"items\": [{ \"type\": \"string\" }, { \"type\": \"number\" }]\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"array_field\",\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"array\",\n      \"items\": [\"null\", \"string\", \"number\"]\n    }\n  ],\n  \"default\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Secret Field in Airbyte Spec (JSON)\nDESCRIPTION: This JSON snippet demonstrates how a field within an Airbyte configuration specification is marked as sensitive. The `\"airbyte_secret\": true` property flags the `api_token` field, indicating to Airbyte that its value should be treated as a secret, not exposed via the API or logs, and obfuscated in storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/secrets.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"email\": { \"type\": \"string\" }\n   \"api_token\": { \"type\": \"string\" , \"airbyte_secret\": true}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Deployment Values for Airbyte Data Plane\nDESCRIPTION: This YAML configuration sets up deployment values for the Airbyte data plane. It includes settings for the control plane URL, edition, data plane credentials, storage configuration, and secrets manager configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nairbyteUrl: https://airbyte.example.com # Base URL for the control plane so Airbyte knows where to authenticate\n\nedition: enterprise # Required for Self-Managed Enterprise\n\n# Logging:\n#  level: DEBUG\n\ndataPlane:\n  # Used to render the data plane creds secret into the Helm chart.\n  secretName: airbyte-config-secrets\n  id: \"preview-data-plane\"\n\n  # Describe secret name and key where each of the client ID and secret are stored\n  clientIdSecretName: airbyte-config-secrets\n  clientIdSecretKey: \"DATA_PLANE_CLIENT_ID\"\n  clientSecretSecretName: airbyte-config-secrets\n  clientSecretSecretKey: \"DATA_PLANE_CLIENT_SECRET\"\n\n# Describe the secret name and key where the Airbyte license key is found\nenterprise:\n  secretName: airbyte-config-secrets\n  licenseKeySecretKey: AIRBYTE_LICENSE_KEY\n\n# S3 bucket secrets/config\n# Only set this section if the control plane has also set these values.\nstorage:\n  secretName: airbyte-config-secrets\n  type: \"s3\"\n  bucket:\n    log: my-bucket-name\n    state: my-bucket-name\n    workloadOutput: my-bucket-name \n  s3:\n    region: \"us-west-2\"\n    authenticationType: credentials\n    accessKeyIdSecretKey: S3_ACCESS_KEY_ID\n    secretAccessKeySecretKey: S3_SECRET_ACCESS_KEY\n\n# Secret manager secrets/config\n# Only set this section if the control plane has also set these values.\nsecretsManager:\n  secretName: airbyte-config-secrets\n  type: AWS_SECRET_MANAGER\n  awsSecretManager:\n    region: us-west-2 \n    authenticationType: credentials\n    accessKeyIdSecretKey: AWS_SECRET_MANAGER_ACCESS_KEY_ID \n    secretAccessKeySecretKey: AWS_SECRET_MANAGER_SECRET_ACCESS_KEY\n```\n\n----------------------------------------\n\nTITLE: Executing Airbyte Destination Operations via Docker (Shell)\nDESCRIPTION: Shows the Docker commands for executing standard Airbyte destination operations (`spec`, `check`, `write`). These commands interact with the destination connector's Docker image. The `write` command specifically takes configuration and catalog file paths as arguments and consumes a stream of `AirbyteMessage` objects piped into its STDIN, typically from a source's `read` command output.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol-docker.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ndocker run --rm -i <destination-image-name> spec\ndocker run --rm -i <destination-image-name> check --config <config-file-path>\ncat <&0 | docker run --rm -i <destination-image-name> write --config <config-file-path> --catalog <catalog-file-path>\n```\n```\n\n----------------------------------------\n\nTITLE: Basic Connector Specification in YAML\nDESCRIPTION: Base connector specification for a declarative source that connects to the Pokemon API and pulls data from the moves stream. Contains basic stream definitions, schema configuration, and HTTP requester setup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 6.13.0\n\ntype: DeclarativeSource\n\ncheck:\n  type: CheckStream\n  stream_names:\n    - moves\n\ndefinitions:\n  streams:\n    moves:\n      type: DeclarativeStream\n      name: moves\n      retriever:\n        type: SimpleRetriever\n        requester:\n          $ref: \"#/definitions/base_requester\"\n          path: /api/v2/move/\n          http_method: GET\n        record_selector:\n          type: RecordSelector\n          extractor:\n            type: DpathExtractor\n            field_path: []\n        paginator:\n          type: DefaultPaginator\n          page_token_option:\n            type: RequestPath\n          pagination_strategy:\n            type: CursorPagination\n            cursor_value: \"{{ response.get('next') }}\"\n            stop_condition: \"{{ response.get('next') is none }}\"\n      schema_loader:\n        type: InlineSchemaLoader\n        schema:\n          $ref: \"#/schemas/moves\"\n  base_requester:\n    type: HttpRequester\n    url_base: https://pokeapi.co\n\nstreams:\n  - $ref: \"#/definitions/streams/moves\"\n\nspec:\n  type: Spec\n  connection_specification:\n    type: object\n    $schema: http://json-schema.org/draft-07/schema#\n    required: []\n    properties: {}\n    additionalProperties: true\n\nschemas:\n  moves:\n    type: object\n    $schema: http://json-schema.org/schema#\n    additionalProperties: true\n    properties:\n      count:\n        type:\n          - number\n          - \"null\"\n      next:\n        type:\n          - string\n          - \"null\"\n      previous:\n        type:\n          - string\n          - \"null\"\n      results:\n        type:\n          - array\n          - \"null\"\n        items:\n          type:\n            - object\n            - \"null\"\n          properties:\n            name:\n              type:\n                - string\n                - \"null\"\n            url:\n              type:\n                - string\n                - \"null\"\n```\n\n----------------------------------------\n\nTITLE: Example OAuth Token Response with Access and Refresh Tokens - JSON\nDESCRIPTION: This JSON snippet is an example response from an OAuth /oauth/token endpoint containing both access_token and refresh_token. Used to illustrate how the manifest should extract and persist these values. Required for flows using access token renewal via refresh tokens.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"YOUR_ACCESS_TOKEN_123\",\n  \"refresh_token\": \"YOUR_REFRESH_TOKEN_123\",\n  \"expires_in\": 7200,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Incident.io API Key in Markdown\nDESCRIPTION: Markdown table showing the configuration parameters for the Incident.io source connector. It specifies the API key input, its type, and description.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/incident-io.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it at https://app.incident.io/settings/api-keys |  |\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Cursor Field in Incremental Sync\nDESCRIPTION: SQL query pattern used by Airbyte source connectors to extract delta rows during incremental sync. This shows how connectors use the cursor field to fetch only data that has changed since the last successful sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/core-concepts/sync-modes/incremental-append-deduped.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from table where cursor_field > 'last_sync_max_cursor_field_value'\n```\n\n----------------------------------------\n\nTITLE: Adding Static Fields to a Stream with AddFields - YAML\nDESCRIPTION: Snippet showing how to configure a stream with the AddFields transformation to insert a static field ('field1') with value 'static_value' in each record. The transformation is defined under 'transformations' in the stream configuration. No external dependencies; applies at ingestion time in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"field1\" ]\n            value: \"static_value\"\n```\n\n----------------------------------------\n\nTITLE: Example Source Schema for Faker Integration in JSON\nDESCRIPTION: Sample JSON schema for a 'users' stream showing the expected data structure with typed fields including nested objects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/core-concepts/typing-deduping.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"number\",\n  \"first_name\": \"string\",\n  \"age\": \"number\",\n  \"address\": {\n    \"city\": \"string\",\n    \"zip\": \"string\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Mapping Documentation to Multiple or Nested UI Fields using FieldAnchor - Markdown\nDESCRIPTION: Anchors documentation to multiple or nested configuration fields in the Airbyte UI using one or several jsonpath expressions in the `field` attribute of `<FieldAnchor>`. The renderer must support Airbyte's custom tags and support multiple, comma-separated fields for simultaneous highlighting. Input: Comma-separated field paths; Output: Content mapped and highlighted for those fields when selected. Limitation: Only operational in special Airbyte documentation/renderer contexts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n## Configuring Unstructured Streams\\n\\n<FieldAnchor field=\"streams.0.format[unstructured],streams.1.format[unstructured],streams.2.format[unstructured]\">\\n\\n...config-related instructions here...\\n\\n</FieldAnchor>\n```\n\n----------------------------------------\n\nTITLE: Configuring the State Converter for Incremental Sync with Timestamps in Python\nDESCRIPTION: This line assigns an instance of the `EpochValueConcurrentStreamStateConverter` to a variable for use in streams. It ensures state tracking and cursor movement are based on epoch timestamps, which is necessary for correct incremental sync handling. This is only required for streams with incremental cursor fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstate_converter = EpochValueConcurrentStreamStateConverter()\n```\n\n----------------------------------------\n\nTITLE: Running Test Suite with Airbyte-CI - Bash\nDESCRIPTION: Runs the full test suite for the connector using Airbyte's CI tool. This validates connector functionality with unit and integration tests as configured. Requires 'airbyte-ci' to be installed and connector code and configurations to be present. Output includes test results to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-v4 test\n```\n\n----------------------------------------\n\nTITLE: Running the Discover Operation for Zendesk Talk Connector (Bash)\nDESCRIPTION: Executes the `discover` command within a temporary Docker container using the `airbyte/source-zendesk-talk:dev` image. It discovers the available data schema (streams and fields) based on the provided configuration. Requires Docker, the built connector image, and a valid `config.json` file mounted from the `secrets` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-talk:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Defining Products Schema in JSON\nDESCRIPTION: JSON schema for the products dataset, including fields for id, make, model, year, price, and timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hardcoded-records.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"make\": \"Mazda\",\n  \"model\": \"MX-5\",\n  \"year\": 2008,\n  \"price\": 2869,\n  \"created_at\": \"2022-02-01T17:02:19+00:00\",\n  \"updated_at\": \"2022-11-01T17:02:19+00:00\"\n}\n```\n\n----------------------------------------\n\nTITLE: Granting Replication Permissions for CDC in Postgres\nDESCRIPTION: SQL command to grant replication permissions to a user for enabling Change Data Capture (CDC) in Postgres. This is required for the CDC replication method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER <user_name> REPLICATION;\n```\n\n----------------------------------------\n\nTITLE: Fetching Notes for Parent Stream Partitions - Woocommerce API - curl - Bash\nDESCRIPTION: Demonstrates how the dynamic partitioning mechanism translates parent stream records (orders) into API requests for child partitioned resources (notes) via curl commands. Each request uses the extracted 'id' value in the endpoint path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/123/notes\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/456/notes\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/789/notes\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Test Suite with airbyte-ci (Bash)\nDESCRIPTION: This Bash command executes the Airbyte connector's full automated test suite locally through the airbyte-ci tool, specifically for the 'source-paypal-transaction' connector. Dependencies include a properly installed airbyte-ci and a clone of the Airbyte repository. The command validates that the connector meets project standards. Output will include test results; all tests must pass before contributions are accepted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paypal-transaction/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paypal-transaction test\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Parameter Logic for Pagination in Python\nDESCRIPTION: This Python method `request_params` determines the query parameters for API requests. If a `next_page_token` (containing the next page URL) is provided, it parses the query string from that URL. Otherwise, it returns the default parameters for the initial request, including the desired fields and the `per_page` value set by the `_PAGE_SIZE` constant.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def request_params(\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\n    ) -> MutableMapping[str, Any]:\n        if next_page_token:\n            return urlparse(next_page_token[\"next_url\"]).query\n        else:\n            return {\"include\": \"response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats\",\n                    \"per_page\": _PAGE_SIZE}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests via Gradle\nDESCRIPTION: Executes the unit tests defined within the destination connector module as part of the Gradle build process. Running the build command also triggers the unit tests. This command should be run from the Airbyte project root and is useful for test-driven development in Java.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-<name>:build\n```\n\n----------------------------------------\n\nTITLE: Defining Target Stream Schema for Exchange Rates Connector (JSON)\nDESCRIPTION: This JSON snippet illustrates the desired output schema for the Airbyte stream created in the tutorial. It specifies the structure for records, including the base currency ('base'), the date of the rates ('date'), and a nested object ('rates') containing key-value pairs of target currencies and their exchange rates relative to the base.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/tutorial.mdx#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"base\": \"USD\",\n  \"date\": \"2022-07-15\",\n  \"rates\": {\n    \"CAD\": 1.28,\n    \"EUR\": 0.98\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Lemlist Connector Commands via Docker using Bash\nDESCRIPTION: These commands demonstrate how to execute standard Airbyte source connector operations (spec, check, discover, read) using the locally built 'airbyte/source-lemlist:dev' Docker image. The 'check', 'discover', and 'read' commands require a 'secrets/config.json' file containing credentials, mounted into the container. The 'read' command also requires a configured catalog file mounted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lemlist/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-lemlist:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lemlist:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lemlist:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-lemlist:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Advanced Slack Notification with Multiple Attachments and Fields\nDESCRIPTION: This comprehensive JSON example shows how to create a Slack notification with multiple attachments and structured fields. It includes a main message and two separate attachments with different colors, titles, and formatted field data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/Google/vocab.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"Airbyte Notification System\",\n  \"attachments\": [\n    {\n      \"color\": \"#0000FF\",\n      \"title\": \"Sync Status\",\n      \"fields\": [\n        {\n          \"title\": \"Status\",\n          \"value\": \"Completed\",\n          \"short\": true\n        },\n        {\n          \"title\": \"Duration\",\n          \"value\": \"5 minutes\",\n          \"short\": true\n        }\n      ]\n    },\n    {\n      \"color\": \"#00FF00\",\n      \"title\": \"Data Metrics\",\n      \"fields\": [\n        {\n          \"title\": \"Records Synced\",\n          \"value\": \"1,234\",\n          \"short\": true\n        },\n        {\n          \"title\": \"Bytes Transferred\",\n          \"value\": \"42.5 MB\",\n          \"short\": true\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Starburst Galaxy Connector Docker Commands\nDESCRIPTION: Example commands for running the Starburst Galaxy connector Docker image, including spec retrieval, configuration check, and write operation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-starburst-galaxy/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-starburst-galaxy:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-starburst-galaxy:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-starburst-galaxy:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Set of commands to run the connector locally for specification, configuration checking, discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-marketo spec\npoetry run source-marketo check --config secrets/config.json\npoetry run source-marketo discover --config secrets/config.json\npoetry run source-marketo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Source-Gitlab Connector Commands Locally with Poetry in Bash\nDESCRIPTION: This group of Bash commands runs various commands (spec, check, discover, read) for the Airbyte Gitlab source connector using Poetry. Each command expects prerequisite configuration files (e.g., secrets/config.json, integration_tests/configured_catalog.json) and must be executed inside the connector's directory. Dependencies include a properly installed Poetry environment and generated credential/config files. Inputs and outputs depend on the specific subcommand, e.g., 'read' consumes both config and catalog files. These commands are useful for validating, discovering schema, and performing data reads during local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-gitlab spec\npoetry run source-gitlab check --config secrets/config.json\npoetry run source-gitlab discover --config secrets/config.json\npoetry run source-gitlab read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n\n```\n\n----------------------------------------\n\nTITLE: Example API Response with Offset Increment (JSON)\nDESCRIPTION: Displays a sample JSON response corresponding to the Offset Increment API request. It returns the records starting from the specified offset (3) up to the limit (2).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\"id\": 4, \"name\": \"Product D\"},\n    {\"id\": 5, \"name\": \"Product E\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Xero connector in Docker. Includes commands for spec, check, discover, and read operations with necessary volume mounts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xero/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-xero:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-xero:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-xero:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-xero:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Onfleet Connector with Airbyte CI - Bash\nDESCRIPTION: This Bash command executes the acceptance tests for the Onfleet connector using the Airbyte CI tool. It is used during local development to validate connector functionality and ensure conformance. The only dependency is the pre-installed airbyte-ci tool, and the connector name is supplied via the '--name' flag. The command runs all standard tests defined for the connector and outputs the results to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onfleet/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onfleet test\n```\n\n----------------------------------------\n\nTITLE: Running Jira Connector as Docker Container\nDESCRIPTION: Series of commands to run various Jira connector operations as a Docker container, including spec generation, configuration check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-jira:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-jira:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-jira:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-jira:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Cursor Field in Salesforce Connector Python Class\nDESCRIPTION: This snippet shows how the Salesforce connector defines the cursor field property which is used for incremental syncing. The replication_key is dynamically assigned based on available timestamp fields like SystemModstamp, LastModifiedDate, CreatedDate, or LoginTime.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/BOOTSTRAP.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef cursor_field(self) -> str:\n    return self.replication_key\n```\n\n----------------------------------------\n\nTITLE: Configuring Squarespace Connector in Markdown\nDESCRIPTION: This snippet outlines the configuration parameters for the Squarespace connector, including the API key and start date for data replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/squarespace.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it at https://developers.squarespace.com/commerce-apis/authentication-and-permissions |  |\n| `start_date` | `string` | Start date. Any data before this date will not be replicated. |  |\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Verifying JSON Data Replication with Local Destination in Bash\nDESCRIPTION: This command allows users to verify that data was successfully replicated by reading the contents of a JSON file created by Airbyte in a local destination. Users need to replace placeholders with their actual path and stream name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/set-up-a-connection.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncat /tmp/airbyte_local/YOUR_PATH/_airbyte_raw_YOUR_STREAM_NAME.jsonl\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Java Connector Base Image\nDESCRIPTION: Generated Dockerfile showing the configuration of airbyte/java-connector-base. It starts with Amazon Corretto 21, installs system dependencies, creates an airbyte user, sets up directories, and configures environment variables for connector commands.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/base_images/README.md#2025-04-23_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM docker.io/amazoncorretto:21-al2023@sha256:c90f38f8a5c4494cb773a984dc9fa9a727b3e6c2f2ee2cba27c834a6e101af0d\nRUN sh -c set -o xtrace && yum install -y shadow-utils tar openssl findutils && yum update -y --security && yum clean all && rm -rf /var/cache/yum && groupadd --gid 1000 airbyte && useradd --uid 1000 --gid airbyte --shell /bin/bash --create-home airbyte && mkdir /secrets && mkdir /config && mkdir --mode 755 /airbyte && mkdir --mode 755 /custom_cache && chown -R airbyte:airbyte /airbyte && chown -R airbyte:airbyte /custom_cache && chown -R airbyte:airbyte /secrets && chown -R airbyte:airbyte /config && chown -R airbyte:airbyte /usr/share/pki/ca-trust-source && chown -R airbyte:airbyte /etc/pki/ca-trust && chown -R airbyte:airbyte /tmp\nENV AIRBYTE_SPEC_CMD=/airbyte/javabase.sh --spec\nENV AIRBYTE_CHECK_CMD=/airbyte/javabase.sh --check\nENV AIRBYTE_DISCOVER_CMD=/airbyte/javabase.sh --discover\nENV AIRBYTE_READ_CMD=/airbyte/javabase.sh --read\nENV AIRBYTE_WRITE_CMD=/airbyte/javabase.sh --write\nENV AIRBYTE_ENTRYPOINT=/airbyte/base.sh\n```\n\n----------------------------------------\n\nTITLE: Testing Missive Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Missive source connector using airbyte-ci test framework.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-missive/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-missive test\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Persona Connector - Bash\nDESCRIPTION: This Bash snippet builds the development Docker image (\\\"source-persona:dev\\\") for the Persona connector using the airbyte-ci CLI. It requires the airbyte-ci tool to be installed and available in your PATH. Running this command prepares the Persona connector for local manual or automated testing. Ensure that all connector dependencies are set up before building.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-persona/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-persona build\n```\n\n----------------------------------------\n\nTITLE: Adding JSON-Encoded Body in Request Options - YAML\nDESCRIPTION: This YAML snippet illustrates how to specify a JSON-encoded body for outgoing HTTP requests using request_body_json within a request_options_provider. The configuration is used by the HttpRequester and requires the endpoint to accept JSON input. The main parameter is the body data under request_body_json, which becomes the HTTP request body in JSON format. Output is a correctly formed JSON-body HTTP request.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  type: HttpRequester\n  url_base: \"https://api.exchangeratesapi.io/v1/\"\n  http_method: \"GET\"\n  request_options_provider:\n    request_body_json:\n      key: value\n```\n\n----------------------------------------\n\nTITLE: Example Record Data with Timestamp Array in JSON\nDESCRIPTION: Example record data conforming to a schema containing an array of timestamp strings. This data corresponds to the schema with the 'appointments' array and is used to illustrate how a destination might process or transform it if native array types are not supported.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"appointments\": [\"2021-11-22T01:23:45+00:00\", \"2022-01-22T14:00:00+00:00\"] }\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Docker Image with Various Commands - Shell\nDESCRIPTION: These commands demonstrate how to run the built connector Docker image with different subcommands for typical workflows: showing the spec, checking configuration, discovering schema, and performing data reads. Directories are mounted as Docker volumes to provide configuration and secrets as needed. Each command executes a specific Airbyte protocol action inside the containerized Java connector. Outputs, such as discovered catalogs or data reads, conform to Airbyte's integration contract.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n```\ndocker run --rm airbyte/source-e2e-test:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-e2e-test:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-e2e-test:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-e2e-test:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n```\n\n----------------------------------------\n\nTITLE: Encoding JavaScript Set for JSON Transfer - JavaScript\nDESCRIPTION: This snippet shows how a JavaScript Set object is represented for serialization to JSON, following the documented encoding pattern for unsupported types in the Convex source connector. It demonstrates the transformation of a Set containing string elements into a JSON-compatible structure that can be interpreted by downstream systems. This approach is crucial for maintaining data integrity when synchronizing source data to Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/convex.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nnew Set([\"a\", \"b\"])\n```\n\n----------------------------------------\n\nTITLE: Granting Granular Read Access to Oracle User (SQL)\nDESCRIPTION: Demonstrates how to grant the 'airbyte' user selective, read-only access (`SELECT` privilege) to specific tables (`<table_1>`, `<table_2>`) within designated schemas (`<schema_a>`, `<schema_b>`). This provides more fine-grained permission control compared to granting access to all tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oracle.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON \"<schema_a>\".\"<table_1>\" TO airbyte;\nGRANT SELECT ON \"<schema_b>\".\"<table_2>\" TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running the connector locally to test specifications, configuration, discovery and data reading\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-box-data-extract spec\npoetry run source-box-data-extract check --config secrets/config.json\npoetry run source-box-data-extract discover --config secrets/config.json\npoetry run source-box-data-extract read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Nested Fields to Objects in Records - YAML\nDESCRIPTION: Shows how to use AddFields transformation to insert a new key ('field1') inside a nested object (e.g., 'data') in each record. The field path is a list of key names, supporting modifications deep within record structures. Used for complex transformations in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"data\", \"field1\" ]\n            value: \"static_value\"\n```\n\n----------------------------------------\n\nTITLE: Defining an ErrorHandler Schema in YAML\nDESCRIPTION: Defines the structure of an ErrorHandler object in YAML, referencing either a DefaultErrorHandler or CompositeErrorHandler via the 'anyOf' field. This schema is required for validating error handler configurations within Airbyte's requester system. It enforces object typing and assists tools in recognizing different handler implementations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nErrorHandler:\n  type: object\n  description: \"Error handler\"\n  anyOf:\n    - \"$ref\": \"#/definitions/DefaultErrorHandler\"\n    - \"$ref\": \"#/definitions/CompositeErrorHandler\"\n```\n\n----------------------------------------\n\nTITLE: Using indent Filter in Jinja2\nDESCRIPTION: Demonstrates the `indent` filter in Jinja2, which adds a specified amount of whitespace indentation to the beginning of each line in a string. The example indents 'hello' by 4 spaces.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_28\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello'|indent(4) }}\n```\n\n----------------------------------------\n\nTITLE: Exporting Spreadsheet Sheet as Streamed JSON Line - Airbyte - JSON\nDESCRIPTION: This snippet demonstrates how a spreadsheet's row is serialized to JSON for streaming purposes in Airbyte. Each object contains a \"stream\" indicating the sheet name, a \"data\" field with key-value pairs for the row, and an \"emitted_at\" timestamp reflecting when the record was exported. Dependencies include a source spreadsheet and the Airbyte framework. Inputs are parsed per row from the sheet; outputs are structured JSON objects ready for downstream ingestion. The format supports varied sheet structures and is robust to column name variations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"Sheet1 one col&special name%?\",\n  \"data\": { \"ID\": \"aa\" },\n  \"emitted_at\": 1673989565000\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"Sheet2-two-cols\",\n  \"data\": { \"ID\": \"a\", \"Name\": \"a\" },\n  \"emitted_at\": 1673989565000\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"Sheet3-two-cols-with-diagram\",\n  \"data\": { \"ID\": \"a\", \"Name\": \"a\" },\n  \"emitted_at\": 1673989566000\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"Sheet4-two-cols-no-header\",\n  \"data\": { \"1\": \"2\", \"a\": \"b\" },\n  \"emitted_at\": 1673989567000\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"Sheet6-2000-rows\",\n  \"data\": { \"ID\": \"1\", \"Name\": \"HKOYohtoy\" },\n  \"emitted_at\": 1673989567000\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Replication Identity for Postgres Table (SQL)\nDESCRIPTION: Modifies a Postgres table (`tbl1`) to use the `DEFAULT` replication identity. This is required for logical replication (CDC) using publications, allowing Postgres to identify changes to rows based on the primary key or unique index. This command is part of setting up CDC for specific tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE tbl1 REPLICA IDENTITY DEFAULT;\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for GCS Configuration\nDESCRIPTION: This YAML snippet defines a Kubernetes secret for Airbyte configuration with GCS storage. It includes fields for license key, data plane credentials, AWS/S3 access keys, and GCP credentials JSON.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # Enterprise License Key\n  license-key: your-airbyte-license-key\n\n  # Insert the data plane credentials received in step 2\n  DATA_PLANE_CLIENT_ID: your-data-plane-client-id\n  DATA_PLANE_CLIENT_SECRET: your-data-plane-client-id\n  \n  # Only set these values if they are also set on your control plane\n  AWS_SECRET_MANAGER_ACCESS_KEY_ID: your-aws-secret-manager-access-key\n  AWS_SECRET_MANAGER_SECRET_ACCESS_KEY: your-aws-secret-manager-secret-key\n  S3_ACCESS_KEY_ID: your-s3-access-key\n  S3_SECRET_ACCESS_KEY: your-s3-secret-key\n\n  # GCP Secrets\n  gcp.json: <CREDENTIALS_JSON_BLOB>\n```\n\n----------------------------------------\n\nTITLE: Configuration-Based Custom Transform with Closure\nDESCRIPTION: Implementation of a custom transform function using a closure to incorporate configuration-based formatting.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/schemas.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyStream(Stream):\n    ...\n    transformer = TypeTransformer(TransformConfig.CustomSchemaNormalization)\n    ...\n    def __init__(self, config_based_date_format):\n        self.config_based_date_format = config_based_date_format\n        transform_function = self.get_custom_transform()\n        self.transformer.registerCustomTransform(transform_function)\n\n    def get_custom_transform(self):\n        def custom_transform_function(original_value, field_schema):\n            if original_value and \"format\" in field_schema and field_schema[\"format\"] == \"date\":\n                transformed_value = pendulum.from_format(original_value, self.config_based_date_format).to_date_string()\n                return transformed_value\n            return original_value\n        return custom_transform_function\n```\n\n----------------------------------------\n\nTITLE: MSSQL Data Type Mapping Changes\nDESCRIPTION: Table showing the remapping of MSSQL data types to their corresponding Airbyte types in version 3.0.0. Includes original MSSQL types, current Airbyte types, and new Airbyte type mappings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Mssql type     | Current Airbyte Type | New Airbyte Type  |\n| -------------- | -------------------- | ----------------- |\n| date           | string               | date              |\n| datetime       | string               | timestamp         |\n| datetime2      | string               | timestamp         |\n| datetimeoffset | string               | timestamp with tz |\n| smalldatetime  | string               | timestamp         |\n| time           | string               | time              |\n```\n\n----------------------------------------\n\nTITLE: Generating Encrypted Private Key using OpenSSL (Bash)\nDESCRIPTION: Uses the `openssl` command-line tool to generate a 2048-bit RSA private key, encrypted with AES-256-CBC in PKCS#8 PEM format, saved to `rsa_key.p8`. This provides an encrypted alternative for Snowflake key pair authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -v2 aes-256-cbc -out rsa_key.p8\n```\n\n----------------------------------------\n\nTITLE: Defining Monthly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema for the monthly_active_users stream in the Google Analytics connector. It includes the count of 30-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_30dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Using int Filter in Jinja2\nDESCRIPTION: Demonstrates the `int` filter in Jinja2, which converts a value (typically a string or float) into an integer. The example converts the string '42' to the integer 42.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_29\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '42'|int }}\n```\n\n----------------------------------------\n\nTITLE: Running Shopify connector commands locally\nDESCRIPTION: Series of commands to execute the connector's core functions locally through Poetry, including specification, configuration checking, source discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-shopify spec\npoetry run source-shopify check --config secrets/config.json\npoetry run source-shopify discover --config secrets/config.json\npoetry run source-shopify read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Rendering Destination Connectors Registry Component in JSX\nDESCRIPTION: Uses the ConnectorRegistry component to render a list of all destination connectors available in Airbyte. The 'type' prop is set to 'destination' to specifically display connectors that push data to target systems.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/README.md#2025-04-23_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<ConnectorRegistry type=\"destination\"/>\n```\n\n----------------------------------------\n\nTITLE: New Handling of Union-Typed Arrays in Airbyte\nDESCRIPTION: Demonstrates the new, improved behavior for arrays of union types (`oneOf`). The output schema correctly reflects the possible types within the union. Elements matching one of the types are preserved, while non-matching elements (like `false` when only integer/string are allowed) are converted to null.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema (Union: Integer or String)\n{ \"type\": \"array\", \"items\": { \"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Matching)\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Schema\n{ \"type\": \"array\", \"items\": [ \"null\", \"integer\", \"string\" ] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema (Union: Integer or String)\n{ \"type\": \"array\", \"items\": { \"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Mismatched - boolean)\n[1, false]\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Schema\n{ \"type\": \"array\", \"items\": [ \"null\", \"integer\", \"string\" ] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data\n[1, null*]\n```\n\n----------------------------------------\n\nTITLE: Setting Concurrent Sync Limits in Airbyte\nDESCRIPTION: YAML configuration for limiting the number of concurrent syncs and checks in Airbyte. This helps prevent system overload by controlling how many operations can run simultaneously.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nworker:\n  extraEnvs: ## We recommend setting both environment variables with a single, shared value.\n    - name: MAX_SYNC_WORKERS\n      value: ## e.g. 5\n    - name: MAX_CHECK_WORKERS\n      value: ## e.g. 5\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Configuration Secrets Using kubectl Command\nDESCRIPTION: Bash command to create Kubernetes secrets containing sensitive credentials for Airbyte deployment using kubectl. This method creates the same secret as the YAML manifest but via CLI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic airbyte-config-secrets \\\n  --from-literal=license-key='' \\\n  --from-literal=database-host='' \\\n  --from-literal=database-port='' \\\n  --from-literal=database-name='' \\\n  --from-literal=database-user='' \\\n  --from-literal=database-password='' \\\n  --from-literal=instance-admin-email='' \\\n  --from-literal=instance-admin-password='' \\\n  --from-literal=s3-access-key-id='' \\\n  --from-literal=s3-secret-access-key='' \\\n  --from-literal=aws-secret-manager-access-key-id='' \\\n  --from-literal=aws-secret-manager-secret-access-key='' \\\n  --namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Running the Public-Apis Connector Locally with Poetry\nDESCRIPTION: Commands to run the connector locally for various operations including specification retrieval, connection checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-public-apis spec\npoetry run source-public-apis check --config secrets/config.json\npoetry run source-public-apis discover --config secrets/config.json\npoetry run source-public-apis read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Record Message with Nested Array Structure in Airbyte JSON Format\nDESCRIPTION: Example of an Airbyte RECORD message containing a stream named 'object_array_test_1' with a nested array structure. The record includes various data types such as strings, dates, timestamps, numbers, big numbers, integers, and booleans within an array property.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_object_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"object_array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"property_string\" : \"qqq\", \"property_array\" : [ { \"property_string\": \"foo bar\", \"property_date\": \"2021-01-23\", \"property_timestamp_with_timezone\": \"2022-11-22T01:23:45+00:00\", \"property_timestamp_without_timezone\": \"2022-11-22T01:23:45\", \"property_number\": 56.78, \"property_big_number\": \"100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.1234\", \"property_integer\": 42, \"property_boolean\": true } ] }}}\n```\n\n----------------------------------------\n\nTITLE: Running Google Analytics V4 Connector in Docker (Bash)\nDESCRIPTION: Illustrates multiple invocation patterns for running the connector as a Docker container. Each command runs a specific subcommand (spec, check, discover, or read) using the Docker image built previously. It mounts necessary directories for configs and tests, passes in configuration and catalog files as required, and ensures containers are cleaned up after execution using the --rm flag. Requires Docker installed and the previously built image available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-analytics-v4:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-v4:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-v4:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-analytics-v4:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Formatting Airbyte Record Message in JSON\nDESCRIPTION: This snippet shows the structure of an Airbyte record message. It includes metadata like stream name and emission timestamp, along with a data object containing various property types such as strings, dates, timestamps, numbers, and booleans.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_object_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"object_test_1\", \"emitted_at\": 1602637589100, \"data\": {\"property_string\": \"foo bar\", \"property_date\": \"2021-01-23\", \"property_timestamp_with_timezone\": \"2022-11-22T01:23:45+00:00\", \"property_timestamp_without_timezone\": \"2022-11-22T01:23:45\", \"property_number\": 56.78, \"property_big_number\": \"100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.1234\", \"property_integer\": 42, \"property_boolean\": true }}}\n```\n\n----------------------------------------\n\nTITLE: Example EmailOctopus API Response (Before Hashing)\nDESCRIPTION: Illustrates the JSON structure of campaign records returned by the EmailOctopus API, specifically showing the `from.name` field before any transformation is applied. This serves as the input data for the hashing example.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": [\n        {\n            \"id\": \"00000000-0000-0000-0000-000000000000\",\n            \"status\": \"SENT\",\n            \"name\": \"Foo\",\n            \"subject\": \"Bar\",\n            \"from\": {\n                \"name\": \"John Doe\",\n                \"email_address\": \"john.doe@gmail.com\"\n            },\n            \"created_at\": \"2023-04-13T15:28:37+00:00\",\n            \"sent_at\": \"2023-04-14T15:28:37+00:00\"\n        },\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Full Replication Identity for Postgres Table (SQL)\nDESCRIPTION: Modifies a Postgres table (`tbl1`) to use the `FULL` replication identity. This logs the entire row content on change and is recommended as an alternative to `DEFAULT` only in specific cases like tables with TOAST-able data types or very large field values, noting potential performance impacts and increased WAL storage size.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE tbl1 REPLICA IDENTITY FULL;\n```\n\n----------------------------------------\n\nTITLE: Enabling API Rate Limiting for Airbyte Streams in Python\nDESCRIPTION: This constructor for `SurveyMonkeyBaseStream` attaches a moving window rate limit policy using `MovingWindowCallRatePolicy` and `HttpAPIBudget`, which ensures API calls are throttled to not exceed the specified rate. It expects previously defined rate constants and uses Airbyte CDK's HTTP streaming interfaces. Parameters include API rate, endpoint configuration, and cursor settings. Limitation: rate limit values must match API documentation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass SurveyMonkeyBaseStream(HttpStream, ABC):\\n    def __init__(self, name: str, path: str, primary_key: Union[str, List[str]], data_field: Optional[str], cursor_field: Optional[str],\\n**kwargs: Any) -> None:\\n        self._name = name\\n        self._path = path\\n        self._primary_key = primary_key\\n        self._data_field = data_field\\n        self._cursor_field = cursor_field\\n        super().__init__(**kwargs)\\n\\n        policies = [\\n            MovingWindowCallRatePolicy(\\n                rates=[Rate(limit=_RATE_LIMIT_PER_MINUTE, interval=datetime.timedelta(minutes=1))],\\n                matchers=[],\\n            ),\\n        ]\\n        api_budget = HttpAPIBudget(policies=policies)\\n        super().__init__(api_budget=api_budget, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Removing Fields in Records Using RemoveFields - YAML\nDESCRIPTION: Configuration snippet specifying use of RemoveFields with 'field_pointers' targeting paths to delete fields ('path.to.field1' and 'path2') in each record. Utilized within Airbyte stream transformations for pruning sensitive or unneeded data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nthe_stream:\n  <...>\n  transformations:\n      - type: RemoveFields\n        field_pointers:\n          - [ \"path\", \"to\", \"field1\" ]\n          - [ \"path2\" ]\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic Fields Based on Stream Slice - YAML\nDESCRIPTION: Demonstrates how to dynamically add a field ('start_date') with a value derived from the stream slice context using Jinja expressions. The 'value' utilizes templated variables, allowing each record to be enriched using runtime metadata as Airbyte processes the stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"start_date\" ]\n            value: { { stream_slice[ 'start_date' ] } }\n```\n\n----------------------------------------\n\nTITLE: Generating Public Key from Private Key using OpenSSL (Bash)\nDESCRIPTION: Uses the `openssl` command-line tool to extract the public key from a private key file (`rsa_key.p8`) and save it to `rsa_key.pub`. This public key needs to be associated with the Snowflake user.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n```\n\n----------------------------------------\n\nTITLE: Adding SHA-256 Code Challenge to Airbyte Declarative OAuth (YAML Diff)\nDESCRIPTION: Illustrates how to update the `consent_url` in the `oauth_config_specification` (YAML) to include a `code_challenge` parameter generated by applying a SHA-256 hash to the `state_value` using the `| codechallengeS256` filter. This is often used in PKCE flows. This diff is applied to `base_oauth.yml`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ base_oauth.yml\n  spec:\n     oauth_config_specification:\n       oauth_connector_input_specification:\n         consent_url: >-\n-          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n-          redirect_uri_value }}&state={{ state_value }}\n+          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n+          redirect_uri_value }}&state={{ state_value }}&code_challenge={{ state_value | codechallengeS256 }}\n         access_token_url: >-\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n       complete_oauth_output_specification:\n         required:\n           - access_token\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite with airbyte-ci - bash\nDESCRIPTION: Executes the full CI test suite for the Github source connector locally with 'airbyte-ci'. Ensures connector logic and integration meet Airbyte quality standards before release. The test results are output to the terminal. Requires 'airbyte-ci' to be installed and the connector dependencies to be in place.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-github test\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run the connector's various operations locally using Poetry, including specification retrieval, connection checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-rki-covid spec\npoetry run source-rki-covid check --config secrets/config.json\npoetry run source-rki-covid discover --config secrets/config.json\npoetry run source-rki-covid read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example Content Record Returned from API Sync - JSON\nDESCRIPTION: This JSON object provides a sample of an article record as returned by The Guardian's API, showcasing all relevant metadata fields such as id, type, sectionId, sectionName, and webPublicationDate. This snapshot demonstrates how the latest encountered record's timestamp is stored to maintain incremental sync state between runs. The content mirrors a real-world API response used for checkpointing between data syncs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"business/live/2023/apr/15/uk-bosses-more-optimistic-energy-prices-fall-ai-spending-boom-economics-business-live\",\n  \"type\": \"liveblog\",\n  \"sectionId\": \"business\",\n  \"sectionName\": \"Business\",\n  \"webPublicationDate\": \"2023-04-15T07:30:58Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Flight Object Example with Nested Data in JavaScript\nDESCRIPTION: A sample JSON object representing flight data with nested structures for origin and destination information, including airport code, terminal, and gate details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"airline\": \"alaska\",\n  \"origin\": {\n    \"airport_code\": \"SFO\",\n    \"terminal\": \"2\",\n    \"gate\": \"G23\"\n  },\n  \"destination\": {\n    \"airport_code\": \"JFK\",\n    \"terminal\": \"7\",\n    \"gate\": \"1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Thinkific Connector in Markdown\nDESCRIPTION: This snippet shows the configuration parameters required for the Thinkific connector, including the API key and subdomain.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. Your Thinkific API key for authentication. |  |\n| `subdomain` | `string` | subdomain. The subdomain of your Thinkific URL (e.g., if your URL is example.thinkific.com, your subdomain is \"example\". |  |\n```\n\n----------------------------------------\n\nTITLE: Illustrating Sample API Response for Exchange Rates Stream (JSON)\nDESCRIPTION: This JSON object represents a sample record returned when testing the initial stream configuration against the Exchange Rates API's `/exchangerates_data/latest` endpoint within the Airbyte Connector Builder. It shows the default base currency (EUR in this example), the date the rates were fetched, and the corresponding exchange rates for various other currencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/tutorial.mdx#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"base\": \"EUR\",\n  \"date\": \"2023-04-13\",\n  \"rates\": {\n    \"AED\": 4.053261,\n    \"AFN\": 95.237669,\n    \"ALL\": 112.964844,\n    \"AMD\": 432.048005,\n    // ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SurveyMonkey API Stream in Python\nDESCRIPTION: This snippet sets up the authentication and configures the stream for the SurveyMonkey API, specifying the name, path, and primary key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef streams(self, config: Mapping[str, Any]) -> List[Stream]:\n    auth = TokenAuthenticator(token=config[\"access_token\"])\n    return [SurveyMonkeyBaseStream(name=\"surveys\", path=\"v3/surveys\", primary_key=\"id\", data_field=\"data\", authenticator=auth)]\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: A markdown table documenting version history with dates, pull request references, and change descriptions spanning from version 0.2.3 to 1.2.3. Includes links to GitHub pull requests and brief descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-teams.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                        |\n| :------ | :--------- | :------------------------------------------------------- | :----------------------------- |\n| 1.2.3 | 2024-10-29 | [47758](https://github.com/airbytehq/airbyte/pull/47758) | Update dependencies |\n| 1.2.2 | 2024-10-28 | [47453](https://github.com/airbytehq/airbyte/pull/47453) | Update dependencies |\n| 1.2.1 | 2024-08-16 | [44196](https://github.com/airbytehq/airbyte/pull/44196) | Bump source-declarative-manifest version |\n| 1.2.0 | 2024-08-15 | [44116](https://github.com/airbytehq/airbyte/pull/44116) | Refactor connector to manifest-only format |\n| 1.1.11 | 2024-08-12 | [43772](https://github.com/airbytehq/airbyte/pull/43772) | Update dependencies |\n| 1.1.10 | 2024-08-10 | [43547](https://github.com/airbytehq/airbyte/pull/43547) | Update dependencies |\n| 1.1.9 | 2024-08-03 | [43251](https://github.com/airbytehq/airbyte/pull/43251) | Update dependencies |\n| 1.1.8 | 2024-07-27 | [42691](https://github.com/airbytehq/airbyte/pull/42691) | Update dependencies |\n| 1.1.7 | 2024-07-20 | [41853](https://github.com/airbytehq/airbyte/pull/41853) | Update dependencies |\n| 1.1.6 | 2024-07-10 | [41382](https://github.com/airbytehq/airbyte/pull/41382) | Update dependencies |\n| 1.1.5 | 2024-07-09 | [41318](https://github.com/airbytehq/airbyte/pull/41318) | Update dependencies |\n| 1.1.4 | 2024-07-06 | [40913](https://github.com/airbytehq/airbyte/pull/40913) | Update dependencies |\n| 1.1.3 | 2024-06-25 | [40366](https://github.com/airbytehq/airbyte/pull/40366) | Update dependencies |\n| 1.1.2 | 2024-06-22 | [40026](https://github.com/airbytehq/airbyte/pull/40026) | Update dependencies |\n| 1.1.1 | 2024-06-04 | [39046](https://github.com/airbytehq/airbyte/pull/39046) | [autopull] Upgrade base image to v1.2.1 |\n| 1.1.0 | 2024-03-24 | [36223](https://github.com/airbytehq/airbyte/pull/36223) | Migration to low code |\n| 1.0.0 | 2024-01-04 | [33959](https://github.com/airbytehq/airbyte/pull/33959) | Schema updates |\n| 0.2.5 | 2021-12-14 | [8429](https://github.com/airbytehq/airbyte/pull/8429) | Update titles and descriptions |\n| 0.2.4 | 2021-12-07 | [7807](https://github.com/airbytehq/airbyte/pull/7807) | Implement OAuth support |\n| 0.2.3 | 2021-12-06 | [8469](https://github.com/airbytehq/airbyte/pull/8469) | Migrate to the CDK |\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Custom Reports for Google Analytics v4 in JSON\nDESCRIPTION: This JSON snippet provides an example for configuring multiple custom reports at once for use with the Google Analytics v4 connector, where each object in the array represents a unique report with separate dimension and metric arrays. This format requires no more than seven unique dimensions and ten metrics per report, and all names must match Google Analytics fields. The expected input is an array of report definitions, and each object will be processed as an independent report stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\"ga:date\", \"ga:pagePath\"],\n    \"metrics\": [\"ga:screenPageViews\", \"ga:totalUsers\"]\n  },\n  {\n    \"name\": \"sessions_by_region\",\n    \"dimensions\": [\"ga:date\", \"ga:region\"],\n    \"metrics\": [\"ga:totalUsers\", \"ga:sessions\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for BigQuery Destination Connector\nDESCRIPTION: Gradle command to run unit tests for the BigQuery destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-bigquery:unitTest\n```\n\n----------------------------------------\n\nTITLE: Testing Dropbox Sign Source Connector with Acceptance Tests - Bash\nDESCRIPTION: This snippet executes the Airbyte acceptance test suite against the Dropbox Sign source connector using the Airbyte CLI. The command requires the Airbyte CLI (`airbyte-ci`) and expects the connector build image to be present. It runs all acceptance tests to validate the connector's compatibility and correctness. This is a crucial step before deploying or publishing the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dropbox-sign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dropbox-sign test\n```\n\n----------------------------------------\n\nTITLE: Generating RSA SSH Key Pair for Tunneling (Shell)\nDESCRIPTION: Executes the `ssh-keygen` command to generate an RSA SSH key pair. The `-t rsa` specifies the RSA algorithm, `-m PEM` ensures the private key is in PEM format (compatible with Airbyte configuration), and `-f myuser_rsa` sets the output filenames for the private (`myuser_rsa`) and public (`myuser_rsa.pub`) keys. This key pair is used for setting up SSH Key Authentication for tunneling connections in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Plaid Source Connector Container Commands with Docker (Bash)\nDESCRIPTION: This set of Bash commands demonstrates different ways to run the Airbyte Plaid source connector Docker container for various tasks: displaying the spec, checking configuration, discovering schema, and reading data. The commands use standard Docker CLI options and mount local configuration and test directories as needed. Each subcommand accepts its own set of parameters, typically referencing local files for config and catalog, and assumes that secrets are provided in a mapped directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plaid/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-plaid:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-plaid:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-plaid:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-plaid:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Handling Exceptions with AirbyteTraceMessageUtility in Java\nDESCRIPTION: Illustrates the recommended approach for handling exceptions within an Airbyte Java connector using the `AirbyteTraceMessageUtility`. This utility class allows emitting structured error messages (`AirbyteTraceMessage`) that provide user-friendly feedback and logging information, differentiating between configuration errors (`emitConfigErrorTrace`) for issues like invalid credentials and system errors (`emitSystemErrorTrace`) for other failures.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_3\n\nLANGUAGE: java\nCODE:\n```\ntry {\n  // some connector code responsible for doing X\n}\ncatch (ExceptionIndicatingIncorrectCredentials credErr) {\n  AirbyteTraceMessageUtility.emitConfigErrorTrace(\n    credErr, \"Connector failed due to incorrect credentials while doing X. Please check your connection is using valid credentials.\")\n  throw credErr\n}\ncatch (ExceptionIndicatingKnownErrorY knownErr) {\n  AirbyteTraceMessageUtility.emitSystemErrorTrace(\n    knownErr, \"Connector failed because of reason Y while doing X. Please check/do/make ... to resolve this.\")\n  throw knownErr\n}\ncatch (Exception e) {\n  AirbyteTraceMessageUtility.emitSystemErrorTrace(\n    e, \"Connector failed while doing X. Possible reasons for this could be ...\")\n  throw e\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring No Network Access in YAML\nDESCRIPTION: YAML configuration to prevent network access for a connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  allowedHosts:\n    hosts: []\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte with Standard Resources\nDESCRIPTION: Command to install Airbyte locally using abctl. This is the default installation for systems with 4 or more CPUs and at least 8GB of memory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nabctl local install\n```\n\n----------------------------------------\n\nTITLE: Granting Permissions to ClickHouse User\nDESCRIPTION: SQL commands to grant the necessary permissions to an Airbyte user in ClickHouse. These permissions allow the user to create tables, write data, and perform other required operations for the connector to function properly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/clickhouse.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT CREATE ON * TO airbyte_user;\nGRANT CREATE ON default * TO airbyte_user;\nGRANT DROP ON * TO airbyte_user;\nGRANT TRUNCATE ON * TO airbyte_user;\nGRANT INSERT ON * TO airbyte_user;\nGRANT SELECT ON * TO airbyte_user;\nGRANT CREATE DATABASE ON airbyte_internal.* TO airbyte_user;\nGRANT CREATE TABLE ON airbyte_internal.* TO airbyte_user;\nGRANT DROP ON airbyte_internal.* TO airbyte_user;\nGRANT TRUNCATE ON airbyte_internal.* TO airbyte_user;\nGRANT INSERT ON airbyte_internal.* TO airbyte_user;\nGRANT SELECT ON airbyte_internal.* TO airbyte_user;\n```\n\n----------------------------------------\n\nTITLE: Example AirbyteCatalog JSON for an API with Multiple Resources\nDESCRIPTION: JSON example of an AirbyteCatalog for an API with multiple endpoints ('customers' and 'products'). This demonstrates how complex nested structures from API responses can be represented in AirbyteStreams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"customers\",\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"products\",\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"required\": [\"name\", \"features\"],\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\"\n          },\n          \"features\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"required\": [\"name\", \"productId\"],\n              \"properties\": {\n                \"name\": { \"type\": \"string\" },\n                \"productId\": { \"type\": \"number\" }\n              }\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSO Authentication for Airbyte in YAML\nDESCRIPTION: This YAML configuration sets up SSO authentication for Airbyte, including instance admin details and identity provider information. It's added to the values.yaml file under the global section.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nauth:\n  instanceAdmin:\n    firstName: ## First name of admin user.\n    lastName: ## Last name of admin user.\n  identityProvider:\n    type: oidc\n    secretName: airbyte-config-secrets ## Name of your Kubernetes secret.\n    oidc:\n      domain: ## e.g. company.example\n      appName: ## e.g. airbyte\n      display-name: ## e.g. Company SSO - optional, falls back to appName if not provided\n      clientIdSecretKey: client-id\n      clientSecretSecretKey: client-secret\n```\n\n----------------------------------------\n\nTITLE: Running Genesys Connector as a Docker Container\nDESCRIPTION: Commands to run the Genesys connector in a Docker container for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-genesys:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-genesys:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-genesys:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-genesys:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring DatetimeBasedCursor for Incremental Sync - YAML\nDESCRIPTION: This YAML configuration demonstrates setting up DatetimeBasedCursor for incremental syncs, injecting start and end times into request parameters. Dependencies include the DatetimeBasedCursor component and APIs supporting time window query parameters. Key options are field_name and inject_into for start_time_option and end_time_option. Inputs are the datetime intervals and request option mapping; outputs are requests filtered by date ranges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2021-02-01T00:00:00.000000+0000\",\n  end_datetime: \"2021-03-01T00:00:00.000000+0000\",\n  step: \"P1D\"\n  start_time_option:\n    type: \"RequestOption\"\n    field_name: \"created[gte]\"\n    inject_into: \"request_parameter\"\n  end_time_option:\n    type: \"RequestOption\"\n    field_name: \"created[lte]\"\n    inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Defining Airbyte Stream Schema in JSON\nDESCRIPTION: Example of an `AirbyteStream` object demonstrating how to define the structure and data types for a stream named \"users\" using JSON schema. It includes properties like \"username\" (string), \"age\" (integer), and \"appointments\" (array of timestamp with timezone). The top-level type must be \"object\".\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"users\",\n  \"json_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"username\": {\n        \"type\": \"string\"\n      },\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"appointments\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\",\n          \"airbyte_type\": \"timestamp_with_timezone\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account Key Authentication in JSON for Google Sheets\nDESCRIPTION: This JSON structure represents the format of the Google Cloud service account key required for authentication when using the Service Account Key Authentication method in Airbyte Open Source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-sheets.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"YOUR_PROJECT_ID\",\n  \"private_key_id\": \"YOUR_PRIVATE_KEY\",\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Milvus Collection with Python Client\nDESCRIPTION: This Python code creates a Milvus collection using the Milvus Python client. It sets up a collection with a primary key field, a vector field, and an index for efficient similarity search.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/milvus.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pymilvus import CollectionSchema, FieldSchema, DataType, connections, Collection\n\nconnections.connect() # connect to locally running Milvus instance without authentication\n\npk = FieldSchema(name=\"pk\",dtype=DataType.INT64, is_primary=True, auto_id=True)\nvector = FieldSchema(name=\"vector\",dtype=DataType.FLOAT_VECTOR,dim=1536)\nschema = CollectionSchema(fields=[pk, vector], enable_dynamic_field=True)\ncollection = Collection(name=\"test_collection\", schema=schema)\ncollection.create_index(field_name=\"vector\", index_params={ \"metric_type\":\"L2\", \"index_type\":\"IVF_FLAT\", \"params\":{\"nlist\":1024} })\n```\n\n----------------------------------------\n\nTITLE: Implementing the check_connection Method (Python)\nDESCRIPTION: Defines the `check_connection` method for the source connector. It attempts to retrieve the first stream, get a stream slice, and read the first record using `SyncMode.full_refresh`. If successful in reading a record, it returns `(True, None)`; otherwise, it catches exceptions and returns `(False, error_message)`, indicating a connection failure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def check_connection(self, logger, config) -> Tuple[bool, any]:\n        first_stream = next(iter(self.streams(config)))\n\n        stream_slice = next(iter(first_stream.stream_slices(sync_mode=SyncMode.full_refresh)))\n\n        try:\n            read_stream = first_stream.read_records(sync_mode=SyncMode.full_refresh, stream_slice=stream_slice)\n            first_record = None\n            while not first_record:\n                first_record = next(read_stream)\n                if isinstance(first_record, AirbyteMessage):\n                    if first_record.type == \"RECORD\":\n                        first_record = first_record.record\n                        return True, None\n                    else:\n                        first_record = None\n            return True, None\n        except Exception as e:\n            return False, f\"Unable to connect to the API with the provided credentials - {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Source with Username/Password Authentication in JSON\nDESCRIPTION: This JSON configuration sets up the Snowflake source connector using username/password authentication. It includes host, role, warehouse, database, schema, and credential details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-snowflake/README.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"host\": \"ACCOUNT.REGION.PROVIDER.snowflakecomputing.com\",\n  \"role\": \"AIRBYTE_ROLE\",\n  \"warehouse\": \"AIRBYTE_WAREHOUSE\",\n  \"database\": \"AIRBYTE_DATABASE\",\n  \"schema\": \"AIRBYTE_SCHEMA\",\n  \"credentials\": {\n    \"auth_type\": \"username/password\",\n    \"username\": \"AIRBYTE_USER\",\n    \"password\": \"SOMEPASSWORD\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Operations via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to run standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built Docker image (`airbyte/source-configcat:dev`). They utilize Docker volumes (`-v`) to mount local directories (`secrets`, `integration_tests`) into the container for accessing configuration and catalog files. The `--rm` flag ensures the container is removed after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-configcat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-configcat:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-configcat:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-configcat:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-configcat:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Connector via Docker (Bash)\nDESCRIPTION: Executes the standard Airbyte connector commands (`spec`, `check`, `discover`, `read`) using the pre-built Docker image (`airbyte/source-declarative-manifest:dev`). Volumes are mounted (`-v`) to provide access to configuration files (`secrets/config.json`) and catalog files (`integration_tests/configured_catalog.json`) from the host machine into the container. Requires Docker and the connector image (`airbyte/source-declarative-manifest:dev`) to be available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-declarative-manifest:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-declarative-manifest:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-declarative-manifest:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-declarative-manifest:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Metrics in Airbyte - YAML\nDESCRIPTION: This snippet illustrates the required configuration entries for enabling OpenTelemetry (OTel) metrics monitoring in Self-Managed Enterprise instances of Airbyte. By editing the 'values.yaml' file, users enable metric collection and define the OTel collector endpoint that receives telemetry data. Dependencies include an existing OTel collector endpoint as part of the overall monitoring deployment. Key parameters: 'global.edition' (must be 'enterprise'), 'metrics.enabled', 'metrics.otlp.enabled', and 'metrics.otlp.collectorEndpoint' (the OTel endpoint URL). The input is the YAML file; the effect is to start sending sync and performance metrics from Airbyte to a user-provided collector, with configuration applicable only in enterprise distributions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.5.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n    edition: enterprise # This is an enterprise-only feature\n    metrics:\n        enabled: true\n        otlp:\n            enabled: true\n            collectorEndpoint: \"YOUR_ENDPOINT\" # The OTel collector endpoint Airbyte sends metrics to. You configure this endpoint outside of Airbyte as part of your OTel deployment.\n```\n\n----------------------------------------\n\nTITLE: Importing Airbyte Traced Exception Utilities (Python)\nDESCRIPTION: Imports `AirbyteTracedException` and `FailureType` from the `airbyte_cdk.utils.traced_exception` module. These are used to raise structured errors with user-friendly messages and specific failure types (like `config_error`) for better error reporting in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# import the following library\nfrom airbyte_cdk.utils.traced_exception import AirbyteTracedException, FailureType\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally using Poetry (Bash)\nDESCRIPTION: These commands demonstrate how to execute the different modes of the Google Search Console source connector locally using `poetry run`. This includes checking the specification (`spec`), validating credentials (`check`), discovering the schema (`discover`), and reading data (`read`) using a configuration file and a catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-search-console spec\npoetry run source-google-search-console check --config secrets/config.json\npoetry run source-google-search-console discover --config secrets/config.json\npoetry run source-google-search-console read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Deploying Self-Managed Enterprise Upgrade with Helm\nDESCRIPTION: Helm command to upgrade an existing Airbyte instance to Self-Managed Enterprise while maintaining configuration through values.yaml file. This preserves existing state and configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrading-from-community.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade \\\n--namespace airbyte \\\n--values ./values.yaml \\\n--install [RELEASE_NAME] \\\n--version [RELEASE_VERSION] \\\nairbyte/airbyte\n```\n\n----------------------------------------\n\nTITLE: Defining SubstreamPartitionRouter Schema in YAML\nDESCRIPTION: Presents the YAML schema definition for `SubstreamPartitionRouter`. This router is designed for streams whose data retrieval depends on records from one or more parent streams. It requires `type` and `parent_stream_configs` properties, where `parent_stream_configs` defines the relationship with parent streams (parent key, partition field, etc.).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nSubstreamPartitionRouter:\n  description: Partition router that is used to retrieve records that have been partitioned according to records from the specified parent streams\n  type: object\n  required:\n    - type\n    - parent_stream_configs\n  properties:\n    type:\n      type: string\n      enum: [SubstreamPartitionRouter]\n    parent_stream_configs:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/ParentStreamConfig\"\n    $parameters:\n      type: object\n      additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Testing Finnhub Source Connector with airbyte-ci - Bash\nDESCRIPTION: Runs the acceptance test suite for the Finnhub source connector using the airbyte-ci CLI tool. Assumes the connector has already been built and that airbyte-ci is available in the environment. Uses the '--name=source-finnhub' flag to target the connector and 'test' command to execute the tests. Outputs acceptance test results to the console. There are no required input parameters except the connector name and test command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finnhub/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finnhub test\n```\n\n----------------------------------------\n\nTITLE: Generating Sparse Nested Stream Record with Deep Nesting in JSON\nDESCRIPTION: This JSON object represents a record for a sparse nested stream. It includes deeply-nested fields with sparse data to test handling of such structures in Airbyte. The record contains an ID, updated timestamp, nested objects, and nested arrays.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_sparse_nested_streams/data_input/messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"sparse_nested_stream\", \"data\": {\"id\": 1, \"updated_at\": 100, \"obj_nest1\": {\"obj_nest2\": {\"foo\": \"bar\"}}, \"arr_nest1\": [{\"arr_nest2\": [{\"foo\": \"bar1\"}, {\"foo\": \"bar2\"}]}, {\"arr_nest2\": [{\"foo\": \"baz1\"}, {\"foo\": \"baz2\"}]}]}, \"emitted_at\": 1672567200}}\n```\n\n----------------------------------------\n\nTITLE: Example API Response for Page Increment (JSON)\nDESCRIPTION: Displays a sample JSON response corresponding to the first Page Increment API request. It returns the records for page 1.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\"id\": 1, \"name\": \"Product A\"},\n    {\"id\": 2, \"name\": \"Product B\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Supported Data Streams Table in Markdown\nDESCRIPTION: Markdown table showing the status of different AppsFlyer data stream categories and their support status in the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appsflyer.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Category                     | Status                |\n|------------------------------|-----------------------|\n| Raw Data (Non-Organic)       | ✔️ (Except Reinstall) |\n| Raw Data (Organic)           | ✔️ (Except Reinstall) |\n| Raw Data (Retargeting)       | ✔️                    |\n| Ad Revenue                   | ❌                     |\n| Postback                     | ❌                     |\n| Protect360 Fraud             | ❌                     |\n| Aggregate Report             | ✔️                    |\n| Aggregate Retargeting Report | ✔️                    |\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Granting Broad Read Access to Oracle User (SQL)\nDESCRIPTION: Grants the 'airbyte' user read-only access to all tables it can reach within the Oracle database using the `SELECT ANY TABLE` privilege. This is a simpler, less granular method for granting the necessary permissions for data replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oracle.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ANY TABLE TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Testing NinjaOne RMM Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the NinjaOne RMM connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ninjaone-rmm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ninjaone-rmm test\n```\n\n----------------------------------------\n\nTITLE: Running the Lever Hiring Connector Test Suite using airbyte-ci\nDESCRIPTION: Executes the full Continuous Integration (CI) test suite for the Lever Hiring source connector locally using the `airbyte-ci` tool. This command requires `airbyte-ci` to be installed and accessible in the environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lever-hiring/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lever-hiring test\n```\n\n----------------------------------------\n\nTITLE: Testing the Fulcrum Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Fulcrum source connector using airbyte-ci. This validates the connector's functionality according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fulcrum/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fulcrum test\n```\n\n----------------------------------------\n\nTITLE: Testing the Source-Float Connector using airbyte-ci\nDESCRIPTION: Command to run the acceptance tests for the source-float connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-float/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-float test\n```\n\n----------------------------------------\n\nTITLE: Configuring OffsetIncrement Paginator Example in YAML\nDESCRIPTION: Provides an example YAML configuration for a `DefaultPaginator` using the `OffsetIncrement` strategy. It demonstrates injecting the `page_size` (set to 5) and the calculated `offset` into request parameters. The `page_token_option` specifies the `offset` field name and injection method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page_size\"\n  pagination_strategy:\n    type: \"OffsetIncrement\"\n    page_size: 5\n  page_token_option:\n    type: \"RequestOption\"\n    field_name: \"offset\"\n    inject_into: \"request_parameter\"\n```\n\n----------------------------------------\n\nTITLE: Building Tinyemail Connector with Airbyte CI (Bash)\nDESCRIPTION: This command builds a development image of the Tinyemail connector using airbyte-ci. The resulting image is tagged as 'source-tinyemail:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tinyemail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tinyemail build\n```\n\n----------------------------------------\n\nTITLE: Running Confluence Connector CI Tests with airbyte-ci (Bash)\nDESCRIPTION: Executes the complete CI test suite for the `source-confluence` connector locally using the `airbyte-ci` tool. This command helps ensure code quality and correctness before contributing changes. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-confluence/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-confluence test\n```\n\n----------------------------------------\n\nTITLE: Testing NoCRM Connector with Airbyte CI\nDESCRIPTION: Command to execute acceptance tests for the NoCRM connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nocrm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nocrm test\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Connection Settings Configuration\nDESCRIPTION: Table showing the available connection settings and their descriptions for Airbyte data transfers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/configuring-connections.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Connection Setting                                                                                       | Description                                                            |\n| --------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n| Connection Name                                                                               | A custom name for your connection                                      |\n| [Schedule Type](/using-airbyte/core-concepts/sync-schedules.md)                               | Configure how often data syncs (can be scheduled, cron, or manually triggered) |\n| [Destination Namespace](/using-airbyte/core-concepts/namespaces.md)                           | Determines where the replicated data is written to in the destination             |\n| [Destination Stream Prefix](/using-airbyte/configuring-schema.md)                                                                     | (Optional) Adds a prefix to each table name in the destination                   |\n| [Detect and propagate schema changes](using-airbyte/schema-change-management.md) | Set how Airbyte handles schema changes in the source                       |\n| [Connection Data Residency](/cloud/managing-airbyte-cloud/manage-data-residency.md)           | Determines where data will be processed (Cloud only)                              |\n```\n\n----------------------------------------\n\nTITLE: Building Huntr Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Huntr source connector using airbyte-ci. The resulting image is tagged as 'source-huntr:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-huntr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-huntr build\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob External Log Storage for Airbyte in YAML\nDESCRIPTION: This YAML configuration sets up Azure Blob storage as external log storage for Airbyte. It's added to the values.yaml file under the global section.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"Azure\"\n    storageSecretName: airbyte-config-secrets # Name of your Kubernetes secret.\n    bucket: ## S3 bucket names that you've created. We recommend storing the following all in one bucket.\n      log: airbyte-bucket\n      state: airbyte-bucket\n      workloadOutput: airbyte-bucket\n    azure:\n      connectionStringSecretKey: azure-blob-store-connection-string\n```\n\n----------------------------------------\n\nTITLE: Testing Cisco Meraki Connector with Airbyte CI - Bash\nDESCRIPTION: This snippet shows how to run acceptance tests for the Cisco Meraki connector using the airbyte-ci CLI utility. Acceptance tests verify the connector's behavior against expected requirements and ensure it functions properly before deployment. Make sure airbyte-ci is installed and you have any required test dependencies or configuration files in place. The --name parameter specifies which connector to test. On execution, results from the test suite are printed to the console; use this step after making local changes to validate your connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cisco-meraki/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cisco-meraki test\n```\n\n----------------------------------------\n\nTITLE: Testing the RentCast Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the RentCast source connector using airbyte-ci. This validates that the connector meets Airbyte's requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rentcast/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rentcast test\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Refresh Test with Mocked SurveyMonkey API Call in Python\nDESCRIPTION: Provides the complete implementation for the `test_read_a_single_page` integration test. It defines the configuration (`_A_CONFIG`) with an access token, mocks a specific GET request to the SurveyMonkey surveys endpoint (`/v3/surveys`) including query parameters, provides a sample JSON response body containing two survey records, and sets the expected status code to 200. Finally, it asserts that exactly two records are processed by the connector for the 'surveys' stream in full refresh mode.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_A_CONFIG = {\n\t\"access_token\": \"access_token\"\n}\n_NOW = datetime.now(timezone.utc)\n\n@freezegun.freeze_time(_NOW.isoformat())\nclass FullRefreshTest(TestCase):\n\n    @HttpMocker()\n    def test_read_a_single_page(self, http_mocker: HttpMocker) -> None:\n\n        http_mocker.get(\n            HttpRequest(url=\"https://api.surveymonkey.com/v3/surveys?include=response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats\"),\n            HttpResponse(body=\"\"\"\n            {\n  \"data\": [\n    {\n      \"id\": \"1234\",\n      \"title\": \"My Survey\",\n      \"nickname\": \"\",\n      \"href\": \"https://api.surveymonkey.com/v3/surveys/1234\"\n    },\n    {\n      \"id\": \"1234\",\n      \"title\": \"My Survey\",\n      \"nickname\": \"\",\n      \"href\": \"https://api.surveymonkey.com/v3/surveys/1234\"\n    }\n  ],\n  \"per_page\": 50,\n  \"page\": 1,\n  \"total\": 2,\n  \"links\": {\n    \"self\": \"https://api.surveymonkey.com/v3/surveys?page=1&per_page=50\"\n  }\n}\n\"\"\", status_code=200)\n        )\n\n        output = self._read(_A_CONFIG, _configured_catalog(\"surveys\", SyncMode.full_refresh))\n\n        assert len(output.records) == 2\n```\n\n----------------------------------------\n\nTITLE: Defining a User Schema for GCS Source (JSON)\nDESCRIPTION: Example of a user-defined schema in JSON format for the Airbyte GCS source connector. This schema defines columns 'id', 'location', 'longitude', and 'latitude' with their respective data types (integer, string, number). Providing a schema allows for explicit type definition and column selection. The schema must be a map of column names to valid data types (string, number, integer, object, array, boolean, null).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gcs.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"id\": \"integer\", \"location\": \"string\", \"longitude\": \"number\", \"latitude\": \"number\"}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema Using oneOf Constraint\nDESCRIPTION: Shows an example JSON schema using the `oneOf` keyword, a combined restriction indicating that the field's value must conform to exactly one of the specified sub-schemas (in this case, either a `string` or an `integer`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"oneOf\": [{ \"type\": \"string\" }, { \"type\": \"integer\" }]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Example Raw Configuration with Secret (JSON)\nDESCRIPTION: This JSON snippet shows an example of raw configuration data corresponding to the specification above. It includes both a non-sensitive field (`email`) and a sensitive field (`api_token`) with its actual value (`fake-token`) before Airbyte processes it for secret obfuscation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/secrets.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"email\":\"itpartners@noodle.com\",\n   \"api_token\": \"fake-token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Airbyte CDK Models (Python)\nDESCRIPTION: Imports necessary classes `AirbyteMessage` and `SyncMode` from the `airbyte_cdk.models` module. These are used within the `check_connection` method to interact with the Airbyte protocol and define sync behavior.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# import the following libraries\nfrom airbyte_cdk.models import AirbyteMessage, SyncMode\n```\n\n----------------------------------------\n\nTITLE: Airbyte Configured Catalog Example for Surveys and Survey Responses (JSON)\nDESCRIPTION: Provides example Airbyte configured catalog with two stream entries for surveys and survey_responses, specifying sync modes and supported schemas. Intended for use in connector integration and catalog validation. Key for aligning Python connector code with Airbyte CDK integration tests or production runs. No dynamic dependency. Inputs: none. Outputs: configured JSON ready to be read by Airbyte platform.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"streams\\\": [\\n    {\\n      \\\"stream\\\": {\\n        \\\"name\\\": \\\"surveys\\\",\\n        \\\"json_schema\\\": {},\\n        \\\"supported_sync_modes\\\": [\\\"full_refresh\\\", \\\"incremental\\\"]\\n      },\\n      \\\"sync_mode\\\": \\\"incremental\\\",\\n      \\\"destination_sync_mode\\\": \\\"overwrite\\\"\\n    },\\n    {\\n      \\\"stream\\\": {\\n        \\\"name\\\": \\\"survey_responses\\\",\\n        \\\"json_schema\\\": {},\\n        \\\"supported_sync_modes\\\": [\\\"full_refresh\\\"]\\n      },\\n      \\\"sync_mode\\\": \\\"full_refresh\\\",\\n      \\\"destination_sync_mode\\\": \\\"overwrite\\\"\\n    }\\n  ]\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Secret with abctl (Shell)\nDESCRIPTION: Shows the `abctl` command to install or update a local Airbyte deployment using a custom Kubernetes secret file (`secret.yaml`) defined previously. This allows setting a predefined password.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --secret secret.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking User Network Policy in Snowflake (SQL)\nDESCRIPTION: SQL command to check if a network policy is set for a specific user in Snowflake. Replace `<username>` with the actual Snowflake username. This helps diagnose connection issues related to user-specific network restrictions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARAMETERS LIKE 'network_policy' IN USER <username>;\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Platform Acceptance Tests with Gradle (Bash)\nDESCRIPTION: Executes two Gradle commands sequentially. First, it cleans previous build artifacts and rebuilds the platform. Second, it runs the acceptance tests located in the `:oss:airbyte-tests:acceptanceTests` subproject. By default, these tests use test containers to manage the Airbyte instance under test. Run from the `airbyte-platform` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew clean build\n./gradlew :oss:airbyte-tests:acceptanceTests\n```\n\n----------------------------------------\n\nTITLE: Rendered Example of JSON-Encoded OAuth Parameters - JSON\nDESCRIPTION: This JSON object shows an example payload for a token exchange using JSON-encoded parameters, as expected when using access_token_params in the manifest. No external dependencies are needed. The input is a set of credentials, and the output is a JSON object to be sent in the HTTP request body.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"client_id\": \"YOUR_CLIENT_ID_123\",\n  \"client_secret\": \"YOUR_CLIENT_SECRET_123\",\n  \"redirect_uri\": \"https://cloud.airbyte.com\",\n}\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteStateMessage Structure in YAML\nDESCRIPTION: Specifies the structure of the AirbyteStateMessage, which enables Sources to emit checkpoints during data replication. It includes fields for different state types (LEGACY, STREAM, GLOBAL) and their corresponding data structures.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteStateMessage:\n  type: object\n  additionalProperties: true\n  properties:\n    state_type:\n      \"$ref\": \"#/definitions/AirbyteStateType\"\n    stream:\n      \"$ref\": \"#/definitions/AirbyteStreamState\"\n    global:\n      \"$ref\": \"#/definitions/AirbyteGlobalState\"\n    data:\n      description: \"(Deprecated) the state data\"\n      type: object\n      existingJavaType: com.fasterxml.jackson.databind.JsonNode\nAirbyteStateType:\n  type: string\n  description: >\n    The type of state the other fields represent.\n    Is set to LEGACY, the state data should be read from the `data` field for backwards compatibility.\n    If not set, assume the state object is type LEGACY.\n    GLOBAL means that the state should be read from `global` and means that it represents the state for all the streams. It contains one shared\n    state and individual stream states.\n    PER_STREAM means that the state should be read from `stream`. The state present in this field correspond to the isolated state of the\n    associated stream description.\n  enum:\n    - GLOBAL\n    - STREAM\n    - LEGACY\nAirbyteStreamState:\n  type: object\n  additionalProperties: true\n  required:\n    - stream_descriptor\n  properties:\n    stream_descriptor:\n      \"$ref\": \"#/definitions/StreamDescriptor\"\n    stream_state:\n      \"$ref\": \"#/definitions/AirbyteStateBlob\"\nAirbyteGlobalState:\n  type: object\n  additionalProperties: true\n  required:\n    - stream_states\n  properties:\n    shared_state:\n      \"$ref\": \"#/definitions/AirbyteStateBlob\"\n    stream_states:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/AirbyteStreamState\"\n```\n\n----------------------------------------\n\nTITLE: Structuring Redis Hash Fields\nDESCRIPTION: Shows the structure of Redis hash fields, including Airbyte-specific metadata fields and the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redis.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- `_airbyte_ab_id`: Sequential id for a given key generated by using the INCR Redis command.\n- `_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n- `_airbyte_data`: a json text/object representing the data that was received from the data source.\n```\n\n----------------------------------------\n\nTITLE: Detailed `acceptance-test-config.yml` Example with Options (YAML)\nDESCRIPTION: Provides a more comprehensive example of the `acceptance-test-config.yml` file. It shows how to specify the connector image, base path, custom environment variables, and configure different test suites like `spec`, `connection`, and `incremental`. It also demonstrates providing multiple input sets for a test (e.g., `connection`) and bypassing tests with a reason (`bypass_reason`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nconnector_image: string # Docker image to test, for example 'airbyte/source-pokeapi:0.1.0'\nbase_path: string # Base path for all relative paths, optional, default - ./\ncustom_environment_variables:\n  foo: bar\nacceptance_tests: # Tests configuration\n  spec: # list of the test inputs\n    bypass_reason: \"Explain why you skipped this test\"\n  connection: # list of the test inputs\n    tests:\n      - config_path: string # set #1 of inputs\n        status: string\n      - config_path: string # set #2 of inputs\n        status: string\n    # discovery:  # skip this test\n  incremental:\n    bypass_reason: \"Incremental sync are not supported on this connector\"\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands with Docker (bash)\nDESCRIPTION: This group of bash snippets demonstrates how to run standard source connector commands (spec, check, discover, read) for the ClickUp API connector using Docker. These commands utilize local directories for secrets and test configurations, mount them as volumes, and execute connector operations by specifying required configurations. The commands depend on docker and presuppose the presence of generated config files and, for read operations, a configured catalog JSON.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clickup-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-clickup-api:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clickup-api:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clickup-api:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-clickup-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Gong Source Connector Docker Commands (Bash)\nDESCRIPTION: This snippet demonstrates various ways to execute the Airbyte Gong source connector container for different operations: retrieving the spec, checking configuration, discovering schema, and running a read operation. These commands mount local directories for secrets and integration tests as needed, passing configuration files via --config and, in the read case, a catalog file via --catalog. The --rm flag ensures the container is removed after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gong/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gong:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gong:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gong:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gong:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: GitHub Repository Format Examples\nDESCRIPTION: Examples of valid repository format specifications for the GitHub connector configuration, showing single repository, multiple repositories, and organization-wide repository patterns.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/invalid_links.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte              # single repository\nairbytehq/airbyte airbytehq/another-repo  # multiple repositories\nairbytehq/*                      # all repositories in organization\n```\n\n----------------------------------------\n\nTITLE: Migrating Legacy OAuth Credentials in Airbyte Declarative Config (YAML Diff)\nDESCRIPTION: Shows comprehensive changes required in the YAML configuration to migrate from a legacy OAuth setup to Declarative OAuth, specifically handling credentials stored in nested config paths (e.g., `config[\"super_secret_credentials\"][\"pokemon_client_id\"]`). It involves updating the `authenticator` definition, the `spec` properties, and various paths within `oauth_config_specification` to point to the new, nested locations, ensuring backward compatibility. This diff compares `base_oauth.yml` to `legacy_to_declarative_oauth.yml`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_14\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ legacy_to_declarative_oauth.yml\n  definitions:\n     authenticator:\n       type: OAuthAuthenticator\n       refresh_request_body: {}\n-      client_id: '{{ config[\"client_id\"] }}'\n-      client_secret: '{{ config[\"client_secret\"] }}'\n-      access_token_value: '{{ config[\"client_access_token\"] }}'\n+      client_id: '{{ config[\"super_secret_credentials\"][\"pokemon_client_id\"] }}'\n+      client_secret: '{{ config[\"super_secret_credentials\"][\"pokemon_client_secret\"] }}'\n+      access_token_value: '{{ config[\"super_secret_credentials\"][\"pokemon_access_token\"] }}'\n \n streams:\n   - $ref: \"#/definitions/streams/moves\"\n spec:\n     type: object\n     $schema: http://json-schema.org/draft-07/schema#\n     required:\n-      - client_id\n-      - client_secret\n-      - client_access_token\n+      - super_secret_credentials\n     properties:\n-      client_id:\n-\t\ttype: string\n-\t\ttitle: Client ID\n-\t\tairbyte_secret: true\n-\t\torder: 0\n-      client_secret:\n-\t\ttype: string\n-\t\ttitle: Client secret\n-\t\tairbyte_secret: true\n-\t\torder: 1\n-      client_access_token:\n-\t\ttype: string\n-\t\ttitle: Access token\n-\t\tairbyte_secret: true\n-\t\tairbyte_hidden: false\n-\t\torder: 2\n+      super_secret_credentials:\n+\t\trequired:\n+\t\t  - pokemon_client_id\n+\t\t  - pokemon_client_secret\n+\t\t  - pokemon_access_token\n+\t\tpokemon_client_id:\n+\t\t  type: string\n+\t\t  title: Client ID\n+\t\t  airbyte_secret: true\n+\t\t  order: 0\n+\t\tpokemon_client_secret:\n+\t\t  type: string\n+\t\t  title: Client secret\n+\t\t  airbyte_secret: true\n+\t\t  order: 1\n+\t\tpokemon_access_token:\n+\t\t  type: string\n+\t\t  title: Access token\n+\t\t  airbyte_secret: true\n+\t\t  airbyte_hidden: false\n+\t\t  order: 2\n     additionalProperties: true\n   advanced_auth:\n     auth_flow_type: oauth2.0\n     oauth_config_specification:\n       oauth_connector_input_specification:\n         consent_url: >-\n--          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n--          redirect_uri_value }}&state={{ state_value }}\n+          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{ redirect_uri_value }}&state={{ s\ntate_value }}\n         access_token_url: >-\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_\ncode_value}}\n+        client_id_key: pokemon_client_id\n+        client_secret_key: pokemon_client_secret\n       complete_oauth_output_specification:\n         required:\n--          - access_token\n+          - pokemon_access_token\n         properties:\n          access_token:\n             type: string\n             path_in_connector_config:\n--              - access_token\n+              - super_secret_credentials\n+              - pokemon_access_token\n             path_in_oauth_response:\n               - access_token\n       complete_oauth_server_input_specification:\n         required:\n--          - client_id\n--          - client_secret\n+          - pokemon_client_id\n+          - pokemon_client_secret\n         properties:\n--          client_id:\n+          pokemon_client_id:\n             type: string\n--          client_secret:\n+          pokemon_client_secret:\n             type: string\n       complete_oauth_server_output_specification:\n         required:\n--          - client_id\n--          - client_secret\n+          - pokemon_client_id\n+          - pokemon_client_secret\n         properties:\n--          client_id:\n+          pokemon_client_id:\n             type: string\n             path_in_connector_config:\n--              - client_id\n--          client_secret:\n+              - super_secret_credentials\n+              - pokemon_client_id\n+          pokemon_client_secret:\n             type: string\n             path_in_connector_config:\n--              - client_secret\n+              - super_secret_credentials\n+              - pokemon_client_secret\n\n```\n\n----------------------------------------\n\nTITLE: Defining Example Legacy Infinite Feed Catalog Schema - JSON\nDESCRIPTION: This snippet demonstrates a simple JSON schema for a legacy Airbyte integration test mode, defining a stream with one column named 'column1' of type string. Dependencies include compliance with JSON Schema draft-07. Expected usage is to copy this schema into the catalog configuration for the legacy Infinite Feed mode, where 'column1' increments starting from 1. Input is not required; output is via generated data conforming to this schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/e2e-test.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"type\\\": \\\"object\\\",\\n  \\\"properties\\\": {\\n    \\\"column1\\\": { \\\"type\\\": \\\"string\\\" }\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Successful Sync Webhook Payload Example in JSON\nDESCRIPTION: Example of the JSON payload structure sent by Airbyte when a sync operation succeeds. Contains workspace, connection details, and sync statistics.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/manage-airbyte-cloud-notifications.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"workspace\": {\n            \"id\":\"b510e39b-e9e2-4833-9a3a-963e51d35fb4\",\n            \"name\":\"Workspace1\",\n            \"url\":\"https://link/to/ws\"\n        },\n        \"connection\":{\n            \"id\":\"64d901a1-2520-4d91-93c8-9df438668ff0\",\n            \"name\":\"Connection\",\n            \"url\":\"https://link/to/connection\"\n        },\n        \"source\":{\n            \"id\":\"c0655b08-1511-4e72-b7da-24c5d54de532\",\n            \"name\":\"Source\",\n            \"url\":\"https://link/to/source\"\n        },\n        \"destination\":{\n            \"id\":\"5621c38f-8048-4abb-85ca-b34ff8d9a298\",\n            \"name\":\"Destination\",\n            \"url\":\"https://link/to/destination\"\n        },\n        \"jobId\":9988,\n        \"startedAt\":\"2024-01-01T00:00:00Z\",\n        \"finishedAt\":\"2024-01-01T01:00:00Z\",\n        \"bytesEmitted\":1000,\n        \"bytesCommitted\":1000,\n        \"recordsEmitted\":89,\n        \"recordsCommitted\":89,\n        \"bytesEmittedFormatted\": \"1000 B\",\n        \"bytesCommittedFormatted\":\"90 B\",\n        \"success\":true,\n        \"durationInSeconds\":3600,\n        \"durationFormatted\":\"1 hours 0 min\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image Manually\nDESCRIPTION: Alternative command to build the connector Docker image directly using docker build, creating an image tagged as airbyte/source-python-http-tutorial:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-python-http-tutorial:dev .\n```\n\n----------------------------------------\n\nTITLE: Describing Example Airbyte Metadata Field Value - JSON\nDESCRIPTION: This JSON snippet provides an example structure for the \\_airbyte_meta field in Airbyte's output. It represents the metadata associated with a record, including a monotonically increasing sync_id and a list of changes reflecting any modifications performed by Airbyte. The snippet demonstrates how errors are documented via change objects specifying the field affected, the change type, and the reason; it is intended for documentation reference and does not require external dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-metadata-fields.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"sync_id\\\": 1234,\\n  \\\"changes\\\": [\\n    {\\n      \\\"field\\\": \\\"foo\\\",\\n      \\\"change\\\": \\\"NULLED\\\",\\n      \\\"reason\\\": \\\"DESTINATION_SERIALIZATION_ERROR\\\"\\n    }\\n  ]\\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector CLI Commands Locally Using Poetry - Bash\nDESCRIPTION: Executes various Airbyte File Source Connector commands (spec, check, discover, and read) via the Poetry virtual environment. Each command expects configuration files (such as secrets/config.json and sample_files/configured_catalog.json) to be present and properly formatted. Outputs command results for connector specification, configuration validation, schema discovery, or data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-file spec\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-file check --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-file discover --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-file read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Statuspage.io API Supported Features\nDESCRIPTION: A markdown table showing the supported features of the Statuspage.io API connector, including full refresh sync and incremental sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/statuspage.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Second Example API Request with Page Increment (HTTP)\nDESCRIPTION: Shows the subsequent HTTP GET request for Page Increment pagination, incrementing the 'page' parameter to 2 while keeping the 'page_size' the same.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_7\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.example.com/products?page_size=2&page=2\n```\n\n----------------------------------------\n\nTITLE: Running CI test suite with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gcs test\n```\n\n----------------------------------------\n\nTITLE: Filtering Records with Record Filter Condition - YAML\nDESCRIPTION: Selector snippet demonstrating the use of record_filter within a selector to filter records based on a Jinja-like condition. The filter expression is evaluated per record, e.g., keeping only those with 'created_at' less than the stream slice's 'start_time'. Requires Airbyte’s internal Jinja expression parser.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  extractor:\n    field_path: []\n  record_filter:\n    condition: \"{{ record['created_at'] < stream_slice['start_time'] }}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secrets with YAML for Airbyte Configuration (YAML)\nDESCRIPTION: This YAML manifest defines a Kubernetes Secret resource named `airbyte-config-secrets`. It uses `stringData` to store sensitive key-value pairs, which will be automatically base64 encoded by Kubernetes upon creation. This secret is intended to hold configuration values required by Airbyte components deployed via Helm and should be applied to the `airbyte` namespace using `kubectl apply -f <filename>.yaml -n airbyte`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/creating-secrets.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # Examples\n  key-1: \"value-1\"\n  key-2: \"value-2\"\n```\n\n----------------------------------------\n\nTITLE: Running the Full Airbyte CI Test Suite Locally with airbyte-ci (Bash)\nDESCRIPTION: Executes the Airbyte CI test suite for the 'source-polygon-stock-api' connector locally. Requires airbyte-ci to be installed and configured. This command ensures the connector passes all automated integration and unit tests before further steps such as publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-polygon-stock-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-polygon-stock-api test\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Bucket Policy in JSON\nDESCRIPTION: JSON policy for granting necessary S3 bucket permissions. It allows actions like PutObject, GetObject, DeleteObject, and others on the specified bucket and its contents.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:DeleteObject\",\n        \"s3:PutObjectAcl\",\n        \"s3:ListBucket\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: New Parquet Representation of Airbyte Union Types\nDESCRIPTION: Shows the improved method for representing Airbyte union types (`oneOf`) in Parquet. The new representation uses a typed disjoint record with a dedicated 'type' field indicating which member is populated, and named fields corresponding to the possible types (e.g., 'integer', 'string', 'object'). Only the 'type' field and the field corresponding to the actual data type are set.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n// Airbyte Union Type (Integer or String)\n{\"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] }\n```\n\nLANGUAGE: text\nCODE:\n```\n// New Parquet Type\nOptional Record { type: String, integer: Optional Int32, string: Optional String }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Airbyte Union Type (Boolean or Object)\n{\"oneOf\": [ {\"type\": \"boolean\"}, {\"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" } } } ] }\n```\n\nLANGUAGE: text\nCODE:\n```\n// New Parquet Type\nOptional Record { type: String, boolean: Optional Boolean, object: Optional Record { id: Optional Uint32, name: Optional String } }\n```\n\n----------------------------------------\n\nTITLE: Listing Workspaces via Airbyte API (YAML)\nDESCRIPTION: Shows an example HTTP GET request to the public API endpoint for listing workspaces. `<YOUR_WEBAPP_URL>` should be replaced with the appropriate URL (either `https://api.airbyte.com` for Cloud or the self-managed URL). The request must include the obtained `access_token` as a Bearer Token in the Authorization header.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nGET <YOUR_WEBAPP_URL>/api/public/v1/workspaces\n```\n\n----------------------------------------\n\nTITLE: Configuring Invoiceninja API Key in Markdown\nDESCRIPTION: Markdown table showing the configuration parameter for the Invoiceninja connector. It specifies the API key input required for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiceninja.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Configuring CDC Data Retention Period in MSSQL\nDESCRIPTION: SQL commands to modify the CDC cleanup job retention period and restart the job\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sp_cdc_change_job @job_type='cleanup', @retention = {minutes}\n\nEXEC sys.sp_cdc_stop_job @job_type = 'cleanup';\n\nEXEC sys.sp_cdc_start_job @job_type = 'cleanup';\n```\n\n----------------------------------------\n\nTITLE: Example Record with Timezone for Ambiguous Schema in JSON\nDESCRIPTION: An example record showing a timestamp string *with* timezone information (`+00:00`). This format is valid for a standard JSON schema `{\"type\": \"string\", \"format\": \"date-time\"}`, highlighting the need for `airbyte_type` to specify timezone requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"created_at\": \"2021-11-22T01:23:45+00:00\"}\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated Read-Only User in Postgres\nDESCRIPTION: SQL commands to create a dedicated read-only user for replicating data in Postgres. This creates a new user with a password and grants the necessary permissions for data replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER <user_name> PASSWORD 'your_password_here';\n```\n\n----------------------------------------\n\nTITLE: Date Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for date data tests in Airbyte's JSON record format. Includes examples with different dates including current, historical, and far-future dates with timestamps and stream identifiers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"date_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"2021-01-23\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"date_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"1504-02-29\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"date_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"9999-12-23\" }}}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command for building the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-adjust build\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Airbyte\nDESCRIPTION: Command to create a dedicated Kubernetes namespace for the Airbyte deployment. This isolates Airbyte resources within the cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_catalog` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_catalog` table, which stores the discovered schema (catalog) for an actor. It includes the catalog JSON, a hash of the catalog for efficient comparison, and timestamps. An index exists on the `catalog_hash`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name   | Datatype    | Description                                     |\n| ------------- | ----------- | ----------------------------------------------- |\n| id            | UUID        | Primary key. Unique identifier for the catalog. |\n| catalog       | JSONB       | JSON representation of the catalog.             |\n| catalog_hash  | VARCHAR(32) | Hash of the catalog for quick comparison.       |\n| created_at    | TIMESTAMP   | Timestamp when the record was created.          |\n| modified_at   | TIMESTAMP   | Timestamp when the record was last modified.    |\n\n#### Indexes and Constraints\n\n- Primary Key: (`id`)\n- Index: `actor_catalog_catalog_hash_id_idx` on (`catalog_hash`)\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster DAG for Airbyte Sync\nDESCRIPTION: Demonstrates how to create a basic Dagster DAG that triggers an Airbyte synchronization job. The code configures Airbyte resource with host and port settings, defines a sync operation with a specific connection ID, and creates a simple job to execute the sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-dagster-integration.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job\nfrom dagster_airbyte import airbyte_resource, airbyte_sync_op\n\nmy_airbyte_resource = airbyte_resource.configured(\n    {\n        \"host\": {\"env\": \"AIRBYTE_HOST\"},\n        \"port\": {\"env\": \"AIRBYTE_PORT\"},\n    }\n)\nsync_foobar = airbyte_sync_op.configured({\"connection_id\": \"your-connection-uuid\"}, name=\"sync_foobar\")\n\n@job(resource_defs={\"airbyte\": my_airbyte_resource})\ndef my_simple_airbyte_job():\n    sync_foobar()\n```\n\n----------------------------------------\n\nTITLE: Creating a Read-Only Role for Multiple Collections with Incremental Sync in Fauna\nDESCRIPTION: This FQL query creates a role with permissions for incrementally syncing multiple collections ('users' and 'products'), including read access to their respective indexes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true },\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"users\"),\n      actions: { read: true },\n    },\n    {\n      resource: Index(\"users-ts\"),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"products\"),\n      actions: { read: true },\n    },\n    {\n      resource: Index(\"products-ts\"),\n      actions: { read: true },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Invoiced Connector\nDESCRIPTION: This command executes the acceptance tests for the Invoiced connector using airbyte-ci. It verifies that the connector meets the required standards and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-invoiced/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-invoiced test\n```\n\n----------------------------------------\n\nTITLE: Running MarkDownLint on Specific Files and Directories\nDESCRIPTION: Commands for running MarkDownLint on single files, directories, or recursively through subdirectories, with an option to automatically fix issues.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmarkdownlint-cli2 \"./docs/myfolder/myfile.md\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nmarkdownlint-cli2 \"./docs/myfolder/*.md\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nmarkdownlint-cli2 \"./docs/folder/**/*.md\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nmarkdownlint-cli2 --fix \"../docs/myfolder/*.md\"\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role Trust Relationship Policy with External ID\nDESCRIPTION: This JSON defines the trust relationship for an IAM role, allowing Airbyte's AWS account to assume the role. It includes an external ID condition which must be set to your Airbyte workspace ID to prevent the confused deputy problem.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/s3.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::094410056844:user/delegated_access_user\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"{your-airbyte-workspace-id}\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Locally Running the Google-Directory Source Connector (Python Bash Commands)\nDESCRIPTION: Executes the source connector in various operational modes (spec, check, discover, read) via Poetry. Each command interacts with the connector for configuration and data extraction tasks, requiring specific config files such as 'secrets/config.json' and 'sample_files/configured_catalog.json'. The commands are designed for local workflows and expect the relevant files to exist; key outputs are command-line responses indicating connector status or data discovery results.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-directory spec\npoetry run source-google-directory check --config secrets/config.json\npoetry run source-google-directory discover --config secrets/config.json\npoetry run source-google-directory read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the PostHog Connector as a Docker Container - Docker\nDESCRIPTION: Contains Docker run commands to execute different connector operations as containers. The '-v' flags mount local directories into the container for accessing secrets and sample files. These commands allow testing and using the connector in an isolated, reproducible Docker environment with access to required configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-posthog/README.md#2025-04-23_snippet_3\n\nLANGUAGE: docker\nCODE:\n```\ndocker run --rm airbyte/source-posthog:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-posthog:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-posthog:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/source-posthog:dev read --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuration Example - Branch Format\nDESCRIPTION: Examples of how to specify GitHub repository branches in the connector configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/unit_tests/data/docs/correct.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte/master airbytehq/airbyte/my-branch\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run various connector operations within a Docker container, including specification, checking, discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-marketo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-marketo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-marketo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-marketo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Parsing Airbyte JSON Records for Multiple Streams\nDESCRIPTION: This JSON snippet represents a collection of Airbyte records for multiple streams. It includes RECORD, STATE, and TRACE message types, demonstrating various stream naming conventions and data structures. The records contain information such as stream names, timestamps, and data payloads.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/edge_case_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"streamWithCamelCase\",\n    \"emitted_at\": 1602637589000,\n    \"data\": { \"data\" : \"one\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_with_underscores\",\n    \"emitted_at\": 1602637589100,\n    \"data\": { \"data\" : \"one\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_with_edge_case_field_names\",\n    \"emitted_at\": 1602637589200,\n    \"data\": { \"fieldWithCamelCase\" : \"one\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_with_edge_case_field_names\",\n    \"emitted_at\": 1602637589300,\n    \"data\": { \"field_with_underscore\" : \"one\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream-with:spécial:character_names\",\n    \"emitted_at\": 1602637589400,\n    \"data\": { \"field_with_spécial_character\" : \"one\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"CapitalCase\",\n    \"emitted_at\": 1602637589000,\n    \"data\": { \"deleted\" : true }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"reserved_keywords\",\n    \"emitted_at\": 1602637589000,\n    \"data\": { \"order\" : \"ascending\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"groups\",\n    \"emitted_at\": 1602637589000,\n    \"data\": { \"authorization\" : \"into\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"ProperCase\",\n    \"emitted_at\": 1602637589000,\n    \"data\": { \"ProperCase\" : true }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name\",\n    \"emitted_at\": 1602637589200,\n    \"data\": { \"some_id\" : \"101\", \"some_field\" : \"some_field_1\", \"some_next_field\" : \"some_next_field_1\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name\",\n    \"emitted_at\": 1602637589250,\n    \"data\": { \"some_id\" : \"102\", \"some_field\" : \"some_field_2\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name\",\n    \"emitted_at\": 1602637589300,\n    \"data\": { \"some_id\" : \"103\", \"some_next_field\" : \"some_next_field_3\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name\",\n    \"emitted_at\": 1602637589350,\n    \"data\": { \"some_id\" : \"104\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name_next\",\n    \"emitted_at\": 1602637589400,\n    \"data\": { \"some_id\" : \"201\", \"next_field_name\" : \"next_field_name_1\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name_next\",\n    \"emitted_at\": 1602637589450,\n    \"data\": { \"some_id\" : \"202\", \"next_field_name\" : \"next_field_name_2\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_name_next\",\n    \"emitted_at\": 1602637589500,\n    \"data\": { \"some_id\" : \"203\" }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_with_binary_data\",\n    \"emitted_at\": 1602637589500,\n    \"data\": { \"some_id\" : \"303\", \"binary_field_name\":\"dGVzdA==\" }\n  }\n}\n{\n  \"type\": \"STATE\",\n  \"state\": {\n    \"type\": \"GLOBAL\",\n    \"global\": {\n      \"shared_state\": {\n        \"start_date\": \"2020-09-02\"\n      }\n    }\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"streamWithCamelCase\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream_with_underscores\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream_with_edge_case_field_names\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream-with:spécial:character_names\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"CapitalCase\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"reserved_keywords\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"groups\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"ProperCase\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream_name\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream_name_next\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"stream_with_binary_data\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n{\n  \"type\": \"TRACE\",\n  \"trace\": {\n    \"type\": \"STREAM_STATUS\",\n    \"stream_status\": {\n      \"stream_descriptor\": {\n        \"name\": \"STREAM_WITH_ALL_CAPS\"\n      },\n      \"status\": \"COMPLETE\"\n    },\n    \"emitted_at\": 1602637589101\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Hubspot Object Structure After Airbyte Unnesting (JSON)\nDESCRIPTION: Demonstrates the transformed JSON object structure *after* Airbyte unnests the top-level `properties` field (since version 1.5.0). The original nested `properties` field is retained, but its contents (`hs_note_body`, `hs_created_by`) are also promoted to the top level, prefixed with `properties_`, to provide easier access in the destination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hubspot.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"updatedAt\": \"2020-01-01\",\n  \"properties\": {\n    \"hs_note_body\": \"World's best boss\",\n    \"hs_created_by\": \"Michael Scott\"\n  },\n  \"properties_hs_note_body\": \"World's best boss\",\n  \"properties_hs_created_by\": \"Michael Scott\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Reply.io Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Reply.io source connector using the airbyte-ci tool. This validates that the connector works correctly before contributing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-reply-io/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-reply-io test\n```\n\n----------------------------------------\n\nTITLE: Defining ConstantBackoffStrategy Schema in YAML\nDESCRIPTION: Provides a YAML schema for a constant backoff strategy, requiring a 'backoff_time_in_seconds' value. This configuration prescribes a fixed-duration retry delay between error occurrences, directly modifiable via schema properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nConstantBackoffStrategy:\n  type: object\n  additionalProperties: true\n  required:\n    - backoff_time_in_seconds\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    backoff_time_in_seconds:\n      type: number\n```\n\n----------------------------------------\n\nTITLE: Building the Ding Connect Connector with airbyte-ci - Bash\nDESCRIPTION: Runs the build process for the Ding Connect Airbyte connector using the airbyte-ci CLI tool. Requires the airbyte-ci tool to be installed and available in the system path. The command builds a development Docker image (source-ding-connect:dev), which can then be used for local testing and validation of the connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ding-connect/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ding-connect build\n```\n\n----------------------------------------\n\nTITLE: Cloud Redirect URI Configuration\nDESCRIPTION: The redirect URI format required for Airbyte Cloud SSO integration with Microsoft Entra ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/azure-entra-id.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://cloud.airbyte.com/auth/realms/<your-company-identifier>/broker/default/endpoint\n```\n\n----------------------------------------\n\nTITLE: Creating Workspace and Destination with Templates in JavaScript\nDESCRIPTION: Functions for creating a workspace and destination in Airbyte Embedded using templates. The createWorkspace function sets up a new workspace and adds a templated S3 destination, while the createDestination function handles destination creation with error checking for existing destinations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/embedded-setup/README.md#2025-04-23_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nasync function createWorkspace(body: Workspace) {\n  const workspaceResponse = await createAirbyteWorkspace(body);\n  if (\"error\" in workspaceResponse) return workspaceResponse;\n\n  const templatedBody = renderTemplate(\"destination-s3.mustache.json\", {\n    name: `songsync-destination-${workspaceResponse.workspaceId}`,\n    workspaceId: workspaceResponse.workspaceId,\n    access_key_id: process.env.AWS_ACCESS_KEY_ID,\n    secret_access_key: process.env.AWS_SECRET_ACCESS_KEY,\n    s3_bucket_name: process.env.AWS_S3_BUCKET,\n    s3_bucket_region: process.env.AWS_REGION,\n  }) as Destination;\n\n  await createAirbyteDestination(templatedBody);\n\n  return workspaceResponse;\n}\n\nasync function createDestination(body: Destination) {\n  const workspaceId = body.workspaceId;\n\n  const existingDestinations = await getDestinations(workspaceId);\n  if (existingDestinations.data.length > 0) {\n    throw new Error(`Destination already exists for workspace ${workspaceId}`);\n  }\n\n  const vars = fillTemplateVarsFromBody(body, {\n    name: body.name,\n    workspaceId: workspaceId,\n  });\n\n  const templatedBody = renderTemplate(\n    `destination-${body.configuration.destinationType}.mustache.json`,\n    vars\n  ) as Destination;\n\n  const response = await createAirbyteDestination(templatedBody);\n  return response;\n}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Qonto Connector\nDESCRIPTION: Command to run the full test suite for the Qonto source connector using airbyte-ci tool to verify functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qonto/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-qonto test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands in Docker Container - bash\nDESCRIPTION: This series of Docker commands demonstrates how to run the main connector operations ('spec', 'check', 'discover', 'read') inside a Docker container, optionally mounting host directories for configuration and test files. The usage of '--rm' ensures cleanup after execution, and volume mounts allow the container access to sensitive credentials or catalog files residing on the host, aligning with Airbyte's operational best practices for containerized connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-commercetools:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-commercetools:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-commercetools:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-commercetools:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the source-countercyclical Connector using airbyte-ci (Bash)\nDESCRIPTION: This command runs the acceptance tests defined for the `source-countercyclical` connector using the `airbyte-ci` tool. This helps ensure the connector functions correctly after local changes. Requires `airbyte-ci` to be installed and the connector image to be built (e.g., using the build command).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-countercyclical/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-countercyclical test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-amazon-seller-partner spec\npoetry run source-amazon-seller-partner check --config secrets/config.json\npoetry run source-amazon-seller-partner discover --config secrets/config.json\npoetry run source-amazon-seller-partner read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Pipedrive Connector Spec Command via Docker (Bash)\nDESCRIPTION: Executes the `spec` command within a temporary Docker container using the locally built `airbyte/source-pipedrive:dev` image. This command outputs the connector's specification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pipedrive:dev spec\n```\n\n----------------------------------------\n\nTITLE: Enabling Unsafe Code via Helm values.yaml in YAML\nDESCRIPTION: This YAML configuration snippet updates the `values.yaml` file for Airbyte deployments managed via Helm charts (without abctl). It sets the `AIRBYTE_ENABLE_UNSAFE_CODE` environment variable to `true` for the `workload-launcher` and `connector-builder-server` services to permit the execution of custom components. Enabling this carries security risks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/custom-components.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml title=\"values.yaml\"\nworkload-launcher:\n  extraEnv:\n    AIRBYTE_ENABLE_UNSAFE_CODE: true\nconnector-builder-server:\n  extraEnv:\n    AIRBYTE_ENABLE_UNSAFE_CODE: true\n```\n```\n\n----------------------------------------\n\nTITLE: Registering SurveyMonkey Streams in Airbyte Python Source\nDESCRIPTION: Demonstrates how to instantiate and return a list of SurveyMonkey streams, wiring up authentication and stream dependencies in the Airbyte source 'streams' method. Inputs: config with access_token. Outputs: list of initialized stream objects with correct relationships. Requires TokenAuthenticator, SurveyMonkeyBaseStream, and SurveyMonkeySubstream implementations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def streams(self, config: Mapping[str, Any]) -> List[Stream]:\\n        auth = TokenAuthenticator(token=config[\\\"access_token\\\"])\\n        surveys = SurveyMonkeyBaseStream(name=\\\"surveys\\\", path=\\\"/v3/surveys\\\", primary_key=\\\"id\\\", data_field=\\\"data\\\", cursor_field=\\\"date_modified\\\", authenticator=auth)\\n        survey_responses = SurveyMonkeySubstream(name=\\\"survey_responses\\\", path=\\\"/v3/surveys/{stream_slice[id]}/responses/\\\", primary_key=\\\"id\\\", authenticator=auth, parent_stream=surveys)\\n        return [\\n            surveys,\\n            survey_responses\\n            ]\\n\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the full test suite locally for the `source-customer-io` connector. It's a crucial step for verifying changes during local development or before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-customer-io/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-customer-io test\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output after RemoveFields - JSON\nDESCRIPTION: A JSON record after removal of specified fields using RemoveFields transformation ('path.to.field1', 'path2'). Demonstrates the effect of field removal, with retained and pruned data visible. Used for reference and testing Airbyte transformations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"path\":\n  {\n    \"to\":\n    {\n      \"field2\": \"data_to_keep\"\n    }\n  },\n  \"path3\": \"data_to_keep\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Pinecone Connector Commands Locally\nDESCRIPTION: Commands to run the Pinecone connector locally for specification, configuration check, and data writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python main.py spec\npoetry run python main.py check --config secrets/config.json\npoetry run python main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating Dedicated ClickHouse User for Airbyte (SQL)\nDESCRIPTION: Creates a dedicated user named 'airbyte' in ClickHouse, allowing connections from any host ('%'). This user is intended for Airbyte connections and requires a secure password to be set in place of 'your_password_here'. Creating a dedicated user is recommended for better permission control and auditing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/clickhouse.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Discover Command via Docker (Bash)\nDESCRIPTION: Executes the `discover` command within a temporary Docker container. It mounts the local `secrets` directory and uses the `config.json` file to discover the schema of the available data streams from the source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coingecko-coins:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Running the read command for Outreach Connector via Docker\nDESCRIPTION: Executes the `read` command in a temporary Docker container to extract data based on the provided configuration and catalog. It mounts both the `secrets` directory (for `config.json`) and the `integration_tests` directory (for `configured_catalog.json`) into the container. Requires valid `secrets/config.json` and `integration_tests/configured_catalog.json` files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-outreach:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Set of commands to run the connector locally for specification, configuration checking, discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-notion spec\npoetry run source-notion check --config secrets/config.json\npoetry run source-notion discover --config secrets/config.json\npoetry run source-notion read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing SSH Connection with Private Key in Bash\nDESCRIPTION: Shows how to test the SSH connection to a remote server using the private key after the corresponding public key has been added to the server's authorized keys. This verifies the key pair authentication setup before configuring the SFTP Bulk connector. Requires the username and IP address/hostname.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp-bulk.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nssh <username>@<server_ip_address>\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Twelve Data Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Twelve Data source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twelve-data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twelve-data test\n```\n\n----------------------------------------\n\nTITLE: Hiding Configuration Fields from Airbyte UI (JSON)\nDESCRIPTION: Demonstrates using the `airbyte_hidden: true` annotation within a JSON Schema property definition (`spec.json`) to hide a field from the Airbyte UI. This is useful for technical or advanced settings that should only be configurable via the API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"first_name\": {\n    \"type\": \"string\",\n    \"title\": \"First Name\"\n  },\n  \"secret_name\": {\n    \"type\": \"string\",\n    \"title\": \"You can't see me!!!\",\n    \"airbyte_hidden\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining SurveyMonkey Source Class in Python\nDESCRIPTION: This snippet defines the main source class for the SurveyMonkey connector. It includes methods for checking connection and configuring streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass SourceSurveyMonkeyDemo(AbstractSource):\n    def check_connection(self, logger, config) -> Tuple[bool, any]:\n        return True, None\n\n    def streams(self, config: Mapping[str, Any]) -> List[Stream]:\n        auth = <TODO>\n        return [SurveyMonkeyBaseStream(name=<TODO>, path=<TODO>, primary_key=<TODO>, data_field=<TODO>, authenticator=auth)]\n```\n\n----------------------------------------\n\nTITLE: Running Connector as Docker Container\nDESCRIPTION: Commands to run the connector as a Docker container for various operations including specification, configuration checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-facebook-marketing:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-facebook-marketing:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-facebook-marketing:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-facebook-marketing:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Reports with Segments and Filters for Google Analytics v4 in JSON\nDESCRIPTION: This snippet illustrates a JSON configuration for defining a custom Google Analytics v4 report that includes segments and filters, allowing users to query subsets of their data. The required fields include 'segments' (array of segment IDs or expressions) and 'filter' (filter expression), with prerequisite knowledge of valid GA segment and filter syntax. The input expects well-formed JSON with segment and filter data, producing a detailed report configuration suitable for advanced queries; be sure to add 'ga:segment' as a dimension when using segments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\"ga:date\", \"ga:pagePath\", \"ga:segment\"],\n    \"metrics\": [\"ga:sessions\", \"ga:totalUsers\"],\n    \"segments\": [\"ga:sessionSource!=(direct)\"],\n    \"filter\": [\"ga:sessionSource!=(direct);ga:sessionSource!=(not set)\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Testing the Source Fillout Connector Locally (Bash)\nDESCRIPTION: Executes the acceptance test suite for the `source-fillout` connector using the `airbyte-ci` tool. This command validates the connector's functionality against Airbyte's specifications, ensuring it behaves as expected. Requires `airbyte-ci` to be installed and is typically run after building the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fillout/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fillout test\n```\n\n----------------------------------------\n\nTITLE: Feature Support for Instatus Connector\nDESCRIPTION: This table outlines the supported features of the Instatus source connector in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instatus.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| SSL connection    | Yes                  |\n| Namespaces        | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Defining Teamtailor Data Streams in Markdown\nDESCRIPTION: This snippet lists all available data streams from the Teamtailor API, including their primary keys, pagination methods, and sync capabilities. It provides a comprehensive overview of the data that can be extracted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/teamtailor.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| candidates | id | DefaultPaginator | ✅ |  ❌  |\n| custom-fields | id | DefaultPaginator | ✅ |  ❌  |\n| departments | id | DefaultPaginator | ✅ |  ❌  |\n| jobs | id | DefaultPaginator | ✅ |  ❌  |\n| job-applications | id | DefaultPaginator | ✅ |  ❌  |\n| job-offers | id | DefaultPaginator | ✅ |  ❌  |\n| locations | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| todos | id | DefaultPaginator | ✅ |  ❌  |\n| teams | id | DefaultPaginator | ✅ |  ❌  |\n| team_memberships | id | DefaultPaginator | ✅ |  ❌  |\n| stages | id | DefaultPaginator | ✅ |  ❌  |\n| roles | id | DefaultPaginator | ✅ |  ❌  |\n| regions | id | DefaultPaginator | ✅ |  ❌  |\n| referrals | id | DefaultPaginator | ✅ |  ❌  |\n| questions | id | DefaultPaginator | ✅ |  ❌  |\n| notes | id | DefaultPaginator | ✅ |  ❌  |\n| nps_responses | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Using Docker (Bash)\nDESCRIPTION: These Bash commands run various Airbyte 'source-paypal-transaction' connector commands inside Docker containers. They include running 'spec', 'check', 'discover', and 'read' commands. Volumes are mounted as needed for secrets and test configuration. Users must have created the required '/secrets/config.json' file and, for 'read', '/integration_tests/configured_catalog.json'. Outputs are produced as per each command's typical behavior (config validation, discovery catalog, or source data). Adjust volume mount paths as necessary for your environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paypal-transaction/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-paypal-transaction:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-paypal-transaction:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-paypal-transaction:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-paypal-transaction:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte Customer Record Structure in JSON Lines Format\nDESCRIPTION: This sample shows the structure of Airbyte output records in JSON Lines format. Each line contains a complete JSON object with a 'type' field indicating it's a RECORD, and a nested 'record' object containing the stream name, emission timestamp, customer data payload, and extraction timestamp. The customer data includes identifiers, contact information, account details, and metadata.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/stripe_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637450, \"data\": {\"id\": \"cus_I1l0XHrjLYwLR2\", \"object\": \"customer\", \"account_balance\": 0, \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 3\", \"discount\": null, \"email\": \"customer3@test.com\", \"invoice_prefix\": \"EC156D8F\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637460, \"data\": {\"id\": \"cus_I1l0INzfeSf2MM\", \"object\": \"customer\", \"account_balance\": 0, \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 2\", \"discount\": null, \"email\": \"customer2@test.com\", \"invoice_prefix\": \"D4564D22\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637470, \"data\": {\"id\": \"cus_I1l0cRVFy4ZhwQ\", \"object\": \"customer\", \"account_balance\": 0, \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 1\", \"discount\": null, \"email\": \"customer1@test.com\", \"invoice_prefix\": \"92A8C396\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n```\n\n----------------------------------------\n\nTITLE: Defining Output Schema for Doris Destination in Markdown\nDESCRIPTION: Describes the structure of the output tables in Doris, including the three columns that will be created for each stream: _airbyte_ab_id, _airbyte_emitted_at, and _airbyte_data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/doris.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `_airbyte_ab_id`: an uuid assigned by Airbyte to each event that is processed. The column type in Doris is `VARCHAR(40)`.\n- `_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Doris is `BIGINT`.\n- `_airbyte_data`: a json blob representing with the event data. The column type in Doris is `String`.\n```\n\n----------------------------------------\n\nTITLE: Defining Customers Schema in JSON\nDESCRIPTION: Comprehensive JSON schema for the customers dataset, including personal information, order details, addresses, and marketing consent.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hardcoded-records.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 6569096478909,\n  \"email\": \"test@test.com\",\n  \"created_at\": \"2023-04-13T02:30:04-07:00\",\n  \"updated_at\": \"2023-04-24T06:53:48-07:00\",\n  \"first_name\": \"New Test\",\n  \"last_name\": \"Customer\",\n  \"orders_count\": 0,\n  \"state\": \"disabled\",\n  \"total_spent\": 0.0,\n  \"last_order_id\": null,\n  \"note\": \"updated_mon_24.04.2023\",\n  \"verified_email\": true,\n  \"multipass_identifier\": null,\n  \"tax_exempt\": false,\n  \"tags\": \"\",\n  \"last_order_name\": null,\n  \"currency\": \"USD\",\n  \"phone\": \"+380639379992\",\n  \"addresses\": [\n    {\n      \"id\": 8092523135165,\n      \"customer_id\": 6569096478909,\n      \"first_name\": \"New Test\",\n      \"last_name\": \"Customer\",\n      \"company\": \"Test Company\",\n      \"address1\": \"My Best Accent\",\n      \"address2\": \"\",\n      \"city\": \"Fair Lawn\",\n      \"province\": \"New Jersey\",\n      \"country\": \"United States\",\n      \"zip\": \"07410\",\n      \"phone\": \"\",\n      \"name\": \"New Test Customer\",\n      \"province_code\": \"NJ\",\n      \"country_code\": \"US\",\n      \"country_name\": \"United States\",\n      \"default\": true\n    }\n  ],\n  \"accepts_marketing\": true,\n  \"accepts_marketing_updated_at\": \"2023-04-13T02:30:04-07:00\",\n  \"marketing_opt_in_level\": \"single_opt_in\",\n  \"tax_exemptions\": \"[]\",\n  \"email_marketing_consent\": {\n    \"state\": \"subscribed\",\n    \"opt_in_level\": \"single_opt_in\",\n    \"consent_updated_at\": \"2023-04-13T02:30:04-07:00\"\n  },\n  \"sms_marketing_consent\": {\n    \"state\": \"not_subscribed\",\n    \"opt_in_level\": \"single_opt_in\",\n    \"consent_updated_at\": null,\n    \"consent_collected_from\": \"SHOPIFY\"\n  },\n  \"admin_graphql_api_id\": \"gid://shopify/Customer/6569096478909\",\n  \"default_address\": {\n    \"id\": 8092523135165,\n    \"customer_id\": 6569096478909,\n    \"first_name\": \"New Test\",\n    \"last_name\": \"Customer\",\n    \"company\": \"Test Company\",\n    \"address1\": \"My Best Accent\",\n    \"address2\": \"\",\n    \"city\": \"Fair Lawn\",\n    \"province\": \"New Jersey\",\n    \"country\": \"United States\",\n    \"zip\": \"07410\",\n    \"phone\": \"\",\n    \"name\": \"New Test Customer\",\n    \"province_code\": \"NJ\",\n    \"country_code\": \"US\",\n    \"country_name\": \"United States\",\n    \"default\": true\n  },\n  \"shop_url\": \"airbyte-integration-test\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Redirects in Docusaurus\nDESCRIPTION: Example of how to set up URL redirects in the Docusaurus configuration file. This allows maintaining backward compatibility when moving or renaming documentation pages.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docusaurus/README.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n//                        {\n//                         from: '/some-lame-path',\n//                         to: '/a-much-cooler-uri',\n//                        },\n```\n\n----------------------------------------\n\nTITLE: Configuration Table in Markdown\nDESCRIPTION: Table showing the configuration parameters for the Open Data DC connector, including API key, location, and MARID fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/open-data-dc.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `location` | `string` | location. address or place or block |  |\n| `marid` | `string` | marid. A unique identifier (Master Address Repository). |  |\n```\n\n----------------------------------------\n\nTITLE: Example of JSON Structure for Updated Asset Fields in Facebook Marketing API\nDESCRIPTION: Demonstrates the new object structure for asset fields that were previously string types in the Facebook Marketing API. This table shows the transformation from simple string values to structured JSON objects with multiple properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/facebook-marketing-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Field                  | Old Type | New Type | Example                                                                                                                           |\n| :--------------------- | :------- | :------- | :-------------------------------------------------------------------------------------------------------------------------------- |\n| `body_asset`           | string   | object   | `{\"text\": \"Body Text\", \"id\": \"12653412653\"}`                                                                                      |\n| `call_to_action_asset` | string   | object   | `{\"name\": \"Action Name\", \"id\": \"12653412653\"}`                                                                                    |\n| `description_asset`    | string   | object   | `{\"text\": \"Description\", \"id\": \"12653412653\"}`                                                                                    |\n| `image_asset`          | string   | object   | `{\"hash\": \"hash_value\", \"url\": \"http://url\",\"id\": \"12653412653\" }`                                                                |\n| `link_url_asset`       | string   | object   | `{\"website_url\": \"http://url\",\"id\": \"12653412653\" }`                                                                              |\n| `title_asset`          | string   | object   | `{\"text\": \"Text\", \"id\": \"12653412653\" }`                                                                                          |\n| `video_asset`          | string   | object   | `{\"video_id\": \"2412334234\", \"url\": \"http://url\", \"thumbnail_url: \"http://url\", \"video_name\": \"Video Name\", \"id\": \"12653412653\" }`\n```\n\n----------------------------------------\n\nTITLE: Running Java Connector Unit and Integration Tests with Gradle - Bash\nDESCRIPTION: These bash commands execute unit and integration tests for a Java-based Airbyte connector using gradle. The first command runs all unit tests under the specified connector module, while the second executes integration tests. Both need to be run from the project root or appropriate relative path. Some tests may require configuration via a `config.json` file within a `.secrets` directory. Outputs include build and test result logs, and gradle must be installed along with all Java runtime dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Unit tests\\n./gradlew :airbyte-integrations:connectors:source-postgres:test\\n# Integration tests\\n./gradlew :airbyte-integrations:connectors:source-postgres:integrationTestJava\n```\n\n----------------------------------------\n\nTITLE: Referencing json-avro-converter for Object Conversion in Java\nDESCRIPTION: This snippet references the airbytehq/json-avro-converter library, which is used for converting JSON objects to Avro objects. It's a fork of the allegro/json-avro-converter library, modified to meet Airbyte's specific requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nairbytehq/json-avro-converter\n```\n\n----------------------------------------\n\nTITLE: Basic Parameter Inheritance Example in YAML\nDESCRIPTION: Shows how parameters are passed from outer component to inner component, where both components can access the defined MyKey value.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/parameters.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    k2: v2\n```\n\n----------------------------------------\n\nTITLE: Running CAT Tests\nDESCRIPTION: Command to execute connector acceptance tests using pytest with the connector acceptance test plugin.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest -p connector_acceptance_test.plugin --acceptance-test-config=../../connectors/source-faker --pdb\n```\n\n----------------------------------------\n\nTITLE: Selecting an Inner Field from Nested Objects - YAML\nDESCRIPTION: Selector configuration in YAML for extracting records from a deep-nested JSON array, by specifying the field_path as [\"data\", \"records\"]. Requires the root JSON object to have this nested structure. Suitable for Airbyte connectors working with multi-level API responses.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  extractor:\n    field_path: [\"data\", \"records\"]\n```\n\n----------------------------------------\n\nTITLE: Defining ConfiguredAirbyteStream Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for ConfiguredAirbyteStream, SyncMode, and DestinationSyncMode. It outlines the required fields, properties, and enum values for each object type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nConfiguredAirbyteStream:\n  type: object\n  additionalProperties: true\n  required:\n    - stream\n    - sync_mode\n    - destination_sync_mode\n  properties:\n    stream:\n      \"$ref\": \"#/definitions/AirbyteStream\"\n    sync_mode:\n      \"$ref\": \"#/definitions/SyncMode\"\n      default: full_refresh\n    cursor_field:\n      description: Path to the field that will be used to determine if a record is new or modified since the last sync. This field is REQUIRED if `sync_mode` is `incremental`. Otherwise it is ignored.\n      type: array\n      items:\n        type: string\n    destination_sync_mode:\n      \"$ref\": \"#/definitions/DestinationSyncMode\"\n      default: append\n    primary_key:\n      description: Paths to the fields that will be used as primary key. This field is REQUIRED if `destination_sync_mode` is `*_dedup`. Otherwise it is ignored.\n      type: array\n      items:\n        type: array\n        items:\n          type: string\nSyncMode:\n  type: string\n  enum:\n    - full_refresh\n    - incremental\nDestinationSyncMode:\n  type: string\n  enum:\n    - append\n    - overwrite\n    - append_dedup # SCD Type 1 & 2\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for the connector development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Visualizing Airbyte Job Status State Machine using Mermaid\nDESCRIPTION: This Mermaid diagram defines the state machine for an Airbyte Sync Job. It illustrates the possible transitions between states: starting from 'pending', moving to 'running', potentially becoming 'incomplete' on attempt failure before retrying, and finally reaching 'succeeded', 'cancelled', or 'failed'. A note clarifies the conditions for moving from 'incomplete' back to 'running' or to 'failed'. This visualization requires a Mermaid renderer to be displayed correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/jobs.md#2025-04-23_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ntitle: Job Status State Machine\n---\nstateDiagram-v2\ndirection TB\nstate NonTerminal {\n    [*] --> pending\n    pending\n    running\n    incomplete\n    note left of incomplete\n        When an attempt fails, the job status is transitioned to incomplete.\n        If this is the final attempt, then the job is transitioned to failed.\n        Otherwise it is transitioned back to running upon new attempt creation.\n\n    end note\n}\nnote left of NonSuccess\n    All Non Terminal Statuses can be transitioned to cancelled or failed\nend note\n\npending --> running\nrunning --> incomplete\nincomplete --> running\nrunning --> succeeded\nstate NonSuccess {\n    cancelled\n    failed\n}\nNonTerminal --> NonSuccess\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Key Pair for Airbyte Tunnel (Text)\nDESCRIPTION: Shows the `ssh-keygen` command used to generate an RSA key pair suitable for SSH tunnel connections in Airbyte. The private key is generated in PEM format (`-m PEM`), which is required by the connector, and the public key is saved in the standard format for use in the bastion host's `authorized_keys` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oracle.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Mailosaur Stream Specifications\nDESCRIPTION: Table defining the available data streams in the Mailosaur connector, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mailosaur.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Messages | id | No pagination | ✅ |  ❌  |\n| Servers | id | No pagination | ✅ |  ❌  |\n| Transactions | timestamp | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Defining AWS Credentials Secret for Kubernetes - YAML\nDESCRIPTION: This Kubernetes Secret manifest securely stores AWS Secrets Manager credentials (an access key ID and secret access key) in the airbyte-config-secrets secret for use by Airbyte on AWS. Required fields include apiVersion, kind, metadata, type, and stringData; the stringData section must include both aws-secret-manager-access-key-id and aws-secret-manager-secret-access-key. These secrets are referenced by Airbyte to authenticate with AWS Secret Manager and must be created in advance. The secret data is stored as plain text (Opaque type), and its presence is a prerequisite for AWS-based secret management in Airbyte. Inputs: AWS credentials. Outputs: A Kubernetes secret resource.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: airbyte-config-secrets\\ntype: Opaque\\nstringData:\\n  # AWS Secret Manager\\n  aws-secret-manager-access-key-id: ## e.g. AKIAIOSFODNN7EXAMPLE\\n  aws-secret-manager-secret-access-key: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\\n\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to activate the virtual environment and install the required dependencies for the connector from requirements.txt.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Conflict Stream Name Record Structure\nDESCRIPTION: Shows the structure of records in the 'conflict_stream_name' stream. It contains nested objects with the same name as the stream and parent objects, demonstrating potential naming conflicts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"RECORD\",\n  \"record\":{\n    \"stream\":\"conflict_stream_name\",\n    \"data\":{\n      \"id\":1,\n      \"conflict_stream_name\":{\n        \"conflict_stream_name\": {\n          \"groups\": \"1\",\n          \"custom_fields\": [{\"id\":1, \"value\":3}, {\"id\":2, \"value\":4}],\n          \"conflict_stream_name\": 3\n        }\n      }\n    },\n    \"emitted_at\":1623861660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing All Connectors with Airbyte CI\nDESCRIPTION: This command retrieves a list of all connectors using the Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors list\n```\n\n----------------------------------------\n\nTITLE: Formatting Postgres to Airbyte Type Mappings - Markdown Table - Markdown\nDESCRIPTION: This snippet provides a Markdown-formatted table mapping PostgreSQL types to Airbyte/JSON-compatible types, along with notes clarifying conversion logic, error handling, and special values. It requires no external dependencies, only a Markdown renderer or viewer. The table lists each source type ('Postgres Type'), its mapped destination representation ('Resulting Type') and any conversion caveats in a 'Notes' column. Inputs and outputs are purely presentational, meant for documentation readers. Any changes or additions should follow this tabular pattern, and the table is limited to supported Postgres types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n| Postgres Type                         | Resulting Type | Notes                                                                                                                                                |\n| ------------------------------------- | -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `bigint`                              | number         |                                                                                                                                                      |\n| `bigserial`, `serial8`                | number         |                                                                                                                                                      |\n| `bit`                                 | string         | Fixed-length bit string (e.g. \"0100\").                                                                                                               |\n| `bit varying`, `varbit`               | string         | Variable-length bit string (e.g. \"0100\").                                                                                                            |\n| `boolean`, `bool`                     | boolean        |                                                                                                                                                      |\n| `box`                                 | string         |                                                                                                                                                      |\n| `bytea`                               | string         | Variable length binary string with hex output format prefixed with \"\\x\" (e.g. \"\\x6b707a\").                                                           |\n| `character`, `char`                   | string         |                                                                                                                                                      |\n| `character varying`, `varchar`        | string         |                                                                                                                                                      |\n| `cidr`                                | string         |                                                                                                                                                      |\n| `circle`                              | string         |                                                                                                                                                      |\n| `date`                                | string         | Parsed as ISO8601 date time at midnight. CDC mode doesn't support era indicators. Issue: [#14590](https://github.com/airbytehq/airbyte/issues/14590) |\n| `double precision`, `float`, `float8` | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: [#8902](https://github.com/airbytehq/airbyte/issues/8902).      |\n| `hstore`                              | string         |                                                                                                                                                      |\n| `inet`                                | string         |                                                                                                                                                      |\n| `integer`, `int`, `int4`              | number         |                                                                                                                                                      |\n| `interval`                            | string         |                                                                                                                                                      |\n| `json`                                | string         |                                                                                                                                                      |\n| `jsonb`                               | string         |                                                                                                                                                      |\n| `line`                                | string         |                                                                                                                                                      |\n| `lseg`                                | string         |                                                                                                                                                      |\n| `macaddr`                             | string         |                                                                                                                                                      |\n| `macaddr8`                            | string         |                                                                                                                                                      |\n| `money`                               | number         |                                                                                                                                                      |\n| `numeric`, `decimal`                  | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: [#8902](https://github.com/airbytehq/airbyte/issues/8902).      |\n| `path`                                | string         |                                                                                                                                                      |\n| `pg_lsn`                              | string         |                                                                                                                                                      |\n| `point`                               | string         |                                                                                                                                                      |\n| `polygon`                             | string         |                                                                                                                                                      |\n| `real`, `float4`                      | number         |                                                                                                                                                      |\n| `smallint`, `int2`                    | number         |                                                                                                                                                      |\n| `smallserial`, `serial2`              | number         |                                                                                                                                                      |\n| `serial`, `serial4`                   | number         |                                                                                                                                                      |\n| `text`                                | string         |                                                                                                                                                      |\n| `time`                                | string         | Parsed as a time string without a time-zone in the ISO-8601 calendar system.                                                                         |\n| `timetz`                              | string         | Parsed as a time string with time-zone in the ISO-8601 calendar system.                                                                              |\n| `timestamp`                           | string         | Parsed as a date-time string without a time-zone in the ISO-8601 calendar system.                                                                    |\n| `timestamptz`                         | string         | Parsed as a date-time string with time-zone in the ISO-8601 calendar system.                                                                         |\n| `tsquery`                             | string         |                                                                                                                                                      |\n| `tsvector`                            | string         |                                                                                                                                                      |\n| `uuid`                                | string         |                                                                                                                                                      |\n| `xml`                                 | string         |                                                                                                                                                      |\n```\n\n----------------------------------------\n\nTITLE: Cloning Forked Airbyte Repository for Connector Development (Bash)\nDESCRIPTION: Clones the user's fork of the `airbytehq/airbyte` repository using SSH and then navigates into the newly created `airbyte` directory. `{YOUR_USERNAME}` should be replaced with the user's GitHub username. Requires `git` and SSH setup for GitHub.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:{YOUR_USERNAME}/airbyte.git\ncd airbyte\n```\n\n----------------------------------------\n\nTITLE: YAML Ambiguous Path Reference\nDESCRIPTION: Demonstrates handling of ambiguous path references in YAML when dealing with nested structures.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/references.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nnested:\n  path: \"first one\"\nnested.path: \"uh oh\"\nvalue: \"ref(nested.path)\"\n```\n\n----------------------------------------\n\nTITLE: Running Shopify connector as Docker container\nDESCRIPTION: Commands to run the Shopify connector Docker container with various operations including spec, check, discover, and read, with appropriate volume mounts for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-shopify:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-shopify:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-shopify:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-shopify:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally using Python\nDESCRIPTION: These commands execute the main connector script ('main.py') with different arguments to perform core operations: 'spec' (get specification), 'check' (validate configuration), 'discover' (find schemas), and 'read' (read data). Requires a configuration file ('secrets/config.json') and potentially a catalog file ('integration_tests/configured_catalog.json').\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands in a Docker Container - Bash\nDESCRIPTION: Runs the connector inside a Docker container using the dev image built earlier. Commands include spec, check, discover, and read. Volumes are mounted to provide authentication secrets and, in some cases, integration test catalogs. Ensures containerized execution mimics production deployment for verifiable results. The 'docker run --rm' flag automatically removes the container after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-drive:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-drive:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-drive:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-drive:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining a Concurrent Source Adapter for Airbyte in Python\nDESCRIPTION: This class `SourceSurveyMonkeyDemo` inherits from `ConcurrentSourceAdapter` to support concurrent stream reading. It initializes concurrency parameters, sets up logging and state management, and provides custom logic for slice boundaries. Dependencies include Airbyte CDK concurrency adapters, logging configuration, and message repositories. It expects configuration for worker count and manages concurrent state using provided Airbyte constructs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SourceSurveyMonkeyDemo(ConcurrentSourceAdapter):\\n    message_repository = InMemoryMessageRepository(Level(AirbyteLogFormatter.level_mapping[_logger.level]))\\n\\n    def __init__(self, config: Optional[Mapping[str, Any]], state: Optional[Mapping[str, Any]]):\\n        if config:\\n            concurrency_level = min(config.get(\"num_workers\", _DEFAULT_CONCURRENCY), _MAX_CONCURRENCY)\\n        else:\\n            concurrency_level = _DEFAULT_CONCURRENCY\\n        _logger.info(f\"Using concurrent cdk with concurrency level {concurrency_level}\")\\n        concurrent_source = ConcurrentSource.create(\\n            concurrency_level, concurrency_level // 2, _logger, self._slice_logger, self.message_repository\\n        )\\n        super().__init__(concurrent_source)\\n        self._config = config\\n        self._state = state\\n\\n    def _get_slice_boundary_fields(self, stream: Stream, state_manager: ConnectorStateManager) -> Optional[Tuple[str, str]]:\\n        return (\"start_date\", \"end_date\")\n```\n\n----------------------------------------\n\nTITLE: Running Hibob Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Hibob source connector, including spec, check, discover, and read. These commands use the airbyte/source-hibob:dev image and mount necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hibob/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-hibob:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hibob:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hibob:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-hibob:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Required Shopify Admin API Scopes for Custom App (Airbyte OSS)\nDESCRIPTION: This list specifies the required `read_` access scopes that must be enabled when configuring a custom Shopify application for use with the Airbyte Shopify source connector in an Open Source deployment. Granting these scopes ensures the connector has permission to read all necessary data from the Shopify store via the Admin API. Users should only select scopes prefixed with `read_` and avoid `write_` scopes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/shopify.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n- `read_analytics`\n- `read_assigned_fulfillment_orders`\n- `read_content`\n- `read_customers`\n- `read_discounts`\n- `read_draft_orders`\n- `read_fulfillments`\n- `read_gdpr_data_request`\n- `read_gift_cards`\n- `read_inventory`\n- `read_legal_policies`\n- `read_locations`\n- `read_locales`\n- `read_marketing_events`\n- `read_merchant_managed_fulfillment_orders`\n- `read_online_store_pages`\n- `read_order_edits`\n- `read_orders`\n- `read_price_rules`\n- `read_product_listings`\n- `read_products`\n- `read_publications`\n- `read_reports`\n- `read_resource_feedbacks`\n- `read_script_tags`\n- `read_shipping`\n- `read_shopify_payments_accounts`\n- `read_shopify_payments_bank_accounts`\n- `read_shopify_payments_disputes`\n- `read_shopify_payments_payouts`\n- `read_themes`\n- `read_third_party_fulfillment_orders`\n- `read_translations`\n```\n\n----------------------------------------\n\nTITLE: Generating Unencrypted Private Key using OpenSSL (Bash)\nDESCRIPTION: Uses the `openssl` command-line tool to generate a 2048-bit RSA private key in unencrypted PKCS#8 PEM format, saved to `rsa_key.p8`. This key is used for Snowflake key pair authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\n```\n\n----------------------------------------\n\nTITLE: Creating Heartbeat Table for PostgreSQL CDC in Airbyte\nDESCRIPTION: SQL statement to create an `airbyte_heartbeat` table with an auto-incrementing ID, a timestamp defaulting to the current time, and a text field. This table is used to generate artificial events for advancing the LSN when using CDC with low transaction volumes on a primary or standalone PostgreSQL instance, requiring write access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE airbyte_heartbeat (\n\tid SERIAL PRIMARY KEY,\n\ttimestamp TIMESTAMP NOT NULL DEFAULT current_timestamp,\n\ttext TEXT\n);\n```\n\n----------------------------------------\n\nTITLE: Running Iterable Connector Commands Locally with Poetry\nDESCRIPTION: Demonstrates how to run various connector commands locally using Poetry, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-iterable spec\npoetry run source-iterable check --config secrets/config.json\npoetry run source-iterable discover --config secrets/config.json\npoetry run source-iterable read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Destination in JSON\nDESCRIPTION: JSON configuration for connecting to a Snowflake destination in Airbyte. Includes host, role, warehouse, database, schema, username, and password details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake/README.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"host\": \"testhost.snowflakecomputing.com\",\n  \"role\": \"AIRBYTE_ROLE\",\n  \"warehouse\": \"AIRBYTE_WAREHOUSE\",\n  \"database\": \"AIRBYTE_DATABASE\",\n  \"schema\": \"AIRBYTE_SCHEMA\",\n  \"username\": \"AIRBYTE_USER\",\n  \"credentials\": {\n    \"password\": \"test\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 CSV Source Connector in Airbyte\nDESCRIPTION: YAML configuration for setting up an AWS S3 source connector for CSV files in Airbyte. This example shows required parameters including dataset name, path pattern, authentication, CSV parsing options, and schema inference settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/config/vocabularies/Airbyte/reject.txt#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Accept the defaults where they make sense for your data\ndataset: s3_data\npath_pattern: \"*.csv\"\ndata_interval: P1D\n\n# S3 bucket\nbucket: airbyte-bucket\naws_access_key_id: \"{{ secrets.AWS_ACCESS_KEY_ID }}\"\naws_secret_access_key: \"{{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n\n# CSV configuration\nformat: csv\ndecimal_separator: \".\"\nquote_char: '\"'\ndouble_quote: true\ndelete_literally: false\nescape_char: \"\\\\\" \ndelimiter: \",\"\nheader_definition: \n  header_row: true\nnull_values: [\"\", \"NA\", \"null\"]\n\n# Schema inference\nschemas:\n  - name: users \n    format: csv\n    globs: [\"**\"]\n    filetype: csv\n    legacy_mode: False\n    validation_policy: Emit Record\n    parsing_mode: default\n    columns:\n      - name: user_id\n        data_type: integer\n        constraints:\n          nullable: false\n      - name: name\n        data_type: string\n      - name: date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Using `airbyte-ci connectors up-to-date` Command Examples (Shell)\nDESCRIPTION: Shell examples demonstrating various ways to use the `airbyte-ci connectors up-to-date` command. Examples include updating a specific connector (`source-openweather`), creating a pull request automatically (`--create-prs`), and updating dependencies without bumping the version (`--no-bump`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-openweather up-to-date\n```\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-openweather up-to-date --create-prs\n```\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-openweather up-to-date --no-bump\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Configuration Secrets in Kubernetes\nDESCRIPTION: YAML manifest for creating a Kubernetes secret containing sensitive credentials for Airbyte, including license key, database credentials, admin credentials, and AWS S3 access keys.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # Enterprise License Key\n  license-key: ## e.g. xxxxx.yyyyy.zzzzz\n\n  # Database Secrets\n  database-host: ## e.g. database.internal\n  database-port: ## e.g. 5432\n  database-name: ## e.g. airbyte\n  database-user: ## e.g. airbyte\n  database-password: ## e.g. password\n\n  # Instance Admin\n  instance-admin-email: ## e.g. admin@company.example\n  instance-admin-password: ## e.g. password\n\n  # SSO OIDC Credentials\n  client-id: ## e.g. e83bbc57-1991-417f-8203-3affb47636cf\n  client-secret: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n  # AWS S3 Secrets\n  s3-access-key-id: ## e.g. AKIAIOSFODNN7EXAMPLE\n  s3-secret-access-key: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n  # Azure Blob Storage Secrets\n  azure-blob-store-connection-string: ## DefaultEndpointsProtocol=https;AccountName=azureintegration;AccountKey=wJalrXUtnFEMI/wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY/wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY==;EndpointSuffix=core.windows.net\n\n  # AWS Secret Manager\n  aws-secret-manager-access-key-id: ## e.g. AKIAIOSFODNN7EXAMPLE\n  aws-secret-manager-secret-access-key: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n  # Azure Secret Manager\n  azure-key-vault-client-id: ## 3fc863e9-4740-4871-bdd4-456903a04d4e\n  azure-key-vault-client-secret: ## KWP6egqixiQeQoKqFZuZq2weRbYoVxMH\n```\n\n----------------------------------------\n\nTITLE: Running Gainsight PX Connector Docker Commands in Bash\nDESCRIPTION: Standard source connector commands for running the Gainsight PX connector in a Docker container. Commands include spec, check, discover, and read operations, with appropriate volume mounts for configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gainsight-px/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gainsight-px:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gainsight-px:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gainsight-px:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gainsight-px:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Square Source Connector Commands\nDESCRIPTION: Commands to run various operations of the Square source connector as a docker container, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-square/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-square:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-square:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-square:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-square:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running SFTP-JSON Connector Commands Locally\nDESCRIPTION: Series of commands to run various connector operations locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-sftp-json spec\npoetry run destination-sftp-json check --config secrets/config.json\npoetry run destination-sftp-json discover --config secrets/config.json\npoetry run destination-sftp-json read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector operations within a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-yandex-metrica:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yandex-metrica:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yandex-metrica:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-yandex-metrica:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Stripe Connector as Docker Container\nDESCRIPTION: These commands demonstrate how to run various Stripe connector operations using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-stripe:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-stripe:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-stripe:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-stripe:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Quickbooks Connector\nDESCRIPTION: Command to run the full test suite for the Quickbooks source connector using airbyte-ci. This validates that the connector functions correctly before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-quickbooks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-quickbooks test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally using Poetry - Bash\nDESCRIPTION: These Bash commands showcase how to locally run various standard operations of the Google-Ads source connector using Poetry. Each command utilizes a specific entrypoint (spec, check, discover, read) to interact with the connector. A valid config file at secrets/config.json is required for all but the spec command, and each operation outputs results relevant to Airbyte workflows. Limitations include ensuring the config path and that Poetry's virtual environment is correctly configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-ads spec\npoetry run source-google-ads check --config secrets/config.json\npoetry run source-google-ads discover --config secrets/config.json\npoetry run source-google-ads read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining HTTPAPIBudget Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the HTTPAPIBudget object. It outlines required properties like 'type' and 'policies', and optional properties for customizing rate limit header names ('ratelimit_reset_header', 'ratelimit_remaining_header') and status codes indicating rate limiting ('status_codes_for_ratelimit_hit'). Policies can be of different types (FixedWindow, MovingWindow, Unlimited).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nHTTPAPIBudget:\n  type: object\n  title: HTTP API Budget\n  description: >\n    An HTTP-specific API budget that extends APIBudget by updating rate limiting information based\n    on HTTP response headers. It extracts available calls and the next reset timestamp from the HTTP responses.\n  required:\n    - type\n    - policies\n  properties:\n    type:\n      type: string\n      enum: [HTTPAPIBudget]\n    policies:\n      type: array\n      description: List of call rate policies that define how many calls are allowed.\n      items:\n        anyOf:\n          - \"$ref\": \"#/definitions/FixedWindowCallRatePolicy\"\n          - \"$ref\": \"#/definitions/MovingWindowCallRatePolicy\"\n          - \"$ref\": \"#/definitions/UnlimitedCallRatePolicy\"\n    ratelimit_reset_header:\n      type: string\n      default: \"ratelimit-reset\"\n      description: The HTTP response header name that indicates when the rate limit resets.\n    ratelimit_remaining_header:\n      type: string\n      default: \"ratelimit-remaining\"\n      description: The HTTP response header name that indicates the number of remaining allowed calls.\n    status_codes_for_ratelimit_hit:\n      type: array\n      default: [429]\n      items:\n        type: integer\n      description: List of HTTP status codes that indicate a rate limit has been hit.\n  additionalProperties: true\n```\n```\n\n----------------------------------------\n\nTITLE: Locally Running Google Analytics V4 Connector Commands (Bash)\nDESCRIPTION: Shows how to execute the Airbyte Google Analytics V4 connector subcommands locally via Poetry. These four commands ('spec', 'check', 'discover', and 'read') respectively output the connector specification, validate the configuration, discover schemas, and perform data extraction. Assumes that credentials and configuration files exist at the specified locations and that development dependencies have been installed. The outputs will depend on the presence of configuration and catalog files as required by each command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-analytics-v4 spec\npoetry run source-google-analytics-v4 check --config secrets/config.json\npoetry run source-google-analytics-v4 discover --config secrets/config.json\npoetry run source-google-analytics-v4 read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for Python\nDESCRIPTION: Commands to create a virtual environment, activate it, and install dependencies for the Langchain connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-langchain/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Granting Schema Usage to a User in CockroachDB - SQL\nDESCRIPTION: This snippet grants a user (in this case 'airbyte') usage privileges on a specific schema within the CockroachDB database. This is necessary to allow the Airbyte connector to access metadata and objects within the named schema. Replace <schema_name> with the intended schema identifier. The command should be run after creating the user and may be repeated for multiple schemas as needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/cockroachdb.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT USAGE ON SCHEMA <schema_name> TO airbyte\n```\n\n----------------------------------------\n\nTITLE: Building the Freshdesk Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Freshdesk source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshdesk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshdesk build\n```\n\n----------------------------------------\n\nTITLE: Building the Smartreach Connector in Airbyte\nDESCRIPTION: Command to build the Smartreach connector locally using airbyte-ci. This creates a development image named 'source-smartreach:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartreach/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartreach build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Newsdata source connector docker container for various operations including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-newsdata/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-newsdata:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-newsdata:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-newsdata:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-newsdata:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Source Connector Commands in Docker (bash)\nDESCRIPTION: These commands run various standard operations on the source-plausible connector container using Docker. They support specification discovery (spec), configuration checking (check), schema discovery (discover), and data reading (read) operations. Dependencies: Requires previously built airbyte/source-plausible:dev Docker image and appropriate config files under secrets/ and integration_tests/. Inputs: configuration and catalog files as needed. Outputs: operation-specific responses from the connector CLI. Constraints: Ensure all referenced paths and files exist.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plausible/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-plausible:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-plausible:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-plausible:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-plausible:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Iceberg V2 Connector Unit Tests with Gradle\nDESCRIPTION: Gradle command to run unit tests for the Iceberg V2 destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-iceberg-v2:test\n```\n\n----------------------------------------\n\nTITLE: Defining a Single Custom Report for Google Analytics v4 in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure a single custom report for Google Analytics v4 by specifying the report name, an array of dimension names, and an array of metric names. It requires the Google Analytics v4 connector and expects the user to provide valid GA dimension and metric keys. The input specifies the main attributes for the report, and the output is a structured query configuration; ensure all fields and names conform to Google Analytics field definitions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\n      \"ga:date\",\n      \"ga:pagePath\",\n      \"ga:sessionDefaultChannelGrouping\"\n    ],\n    \"metrics\": [\"ga:screenPageViews\", \"ga:totalUsers\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Granting Select Privileges to Airbyte User in ClickHouse (SQL)\nDESCRIPTION: Grants read-only (SELECT) privileges to the 'airbyte' user on all tables within a specified database (`<database name>`). This command should be run after creating the 'airbyte' user to allow Airbyte to access the necessary data for replication. The grant can be limited to specific tables if needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/clickhouse.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON <database name>.* TO 'airbyte'@'%';\n```\n\n----------------------------------------\n\nTITLE: Setting Up pg_cron, Creating Log Table, and Scheduling Job in PostgreSQL\nDESCRIPTION: Multi-statement SQL script for setting up the `pg_cron` based heartbeat mechanism. It enables the `pg_cron` extension if not already enabled, creates a `periodic_log` table to record job executions, and schedules a cron job named 'periodic_logger' to insert a default row into the `periodic_log` table every minute (`*/1 * * * *`). This generates regular WAL activity.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION IF NOT EXISTS pg_cron;\n\nCREATE TABLE periodic_log (\n\tlog_id SERIAL PRIMARY KEY,\n\tlog_time TIMESTAMP DEFAULT current_timestamp\n);\n\nSELECT cron.schedule(\n\t'periodic_logger',               -- job name\n\t'*/1 * * * *',                   -- cron expression (every minute)\n\t$$INSERT INTO periodic_log DEFAULT VALUES$$ -- the SQL statement to run\n);\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite with airbyte-ci - Bash\nDESCRIPTION: Runs the full CI test suite for the connector locally using Airbyte’s airbyte-ci tool. This validates the connector against Airbyte's standard integration and acceptance tests, ensuring compliance and stability before publishing. This command requires the airbyte-ci tool and appropriate test configuration files. Outputs detailed test results in the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-support test\n```\n\n----------------------------------------\n\nTITLE: Building the Kisi Source Connector with Airbyte CI\nDESCRIPTION: This command creates a dev image (source-kisi:dev) that can be used to test the connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kisi/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kisi build\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies using Poetry (Bash)\nDESCRIPTION: Installs the connector's Python dependencies, including development dependencies specified in `pyproject.toml`, using the Poetry package manager. This command should be run from the connector's root directory. Requires Poetry (~=1.7) and Python (~=3.9) to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Required GCS Bucket Permissions for Airbyte Service Account\nDESCRIPTION: List of required permissions that need to be assigned to the service account or user for the GCS bucket. These permissions allow Airbyte to create, list, get, delete objects and manage multipart uploads in the bucket.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/gcs.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nstorage.multipartUploads.abort\nstorage.multipartUploads.create\nstorage.objects.create\nstorage.objects.delete\nstorage.objects.get\nstorage.objects.list\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Airbyte Destination Connector\nDESCRIPTION: Command to run acceptance tests for an Airbyte destination connector. This should be executed from the Airbyte repository root and replaces '<name>' with the specific destination name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-<name>:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Defining Huntr Connector Streams in Markdown\nDESCRIPTION: Markdown table listing the available streams for the Huntr connector. It includes stream names, primary keys, pagination details, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/huntr.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| members | id | DefaultPaginator | ✅ |  ❌  |\n| organization_invitations | id | DefaultPaginator | ✅ |  ❌  |\n| member_fields | id | DefaultPaginator | ✅ |  ❌  |\n| activities | id | DefaultPaginator | ✅ |  ❌  |\n| notes | id | DefaultPaginator | ✅ |  ❌  |\n| actions | id | DefaultPaginator | ✅ |  ❌  |\n| candidates | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running the check command for Outreach Connector via Docker\nDESCRIPTION: Executes the `check` command within a temporary Docker container to validate connection settings. It mounts the local `secrets` directory containing the `config.json` file into the container and passes the path to the configuration file as an argument. Requires a valid `secrets/config.json` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-outreach:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Running Smartengage Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Smartengage source connector using Docker. They include getting the spec, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartengage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-smartengage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smartengage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smartengage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-smartengage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Thinkific Courses Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Thinkific Courses source connector using airbyte-ci. The resulting image (source-thinkific-courses:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-thinkific-courses/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-thinkific-courses build\n```\n\n----------------------------------------\n\nTITLE: Testing the Financial Modelling Connector\nDESCRIPTION: Command to run acceptance tests for the Financial Modelling connector using airbyte-ci. This validates that the connector functions as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-financial-modelling/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-financial-modelling test\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Operations - Shell (bash)\nDESCRIPTION: This snippet shows how to run the 'check' operation of an Airbyte source connector using Poetry. The command takes a config file path as input and outputs connection status to standard output. It assumes the target connector ('source-survey-monkey-demo') is installed and a valid secrets/config.json file is available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo check --config secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-looker build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Huntr Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Huntr source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-huntr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-huntr test\n```\n\n----------------------------------------\n\nTITLE: Verifying pg_cron Job Execution via Log Table in PostgreSQL\nDESCRIPTION: SQL query to retrieve records from the `periodic_log` table, ordered by the log time descending. This allows verification that the scheduled `pg_cron` job ('periodic_logger') is executing correctly and inserting rows periodically as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM periodic_log ORDER BY log_time DESC;\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment using Shell\nDESCRIPTION: This shell command creates a Python virtual environment named '.venv' in the current directory. This isolates project dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Testing BoldSign Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the BoldSign source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-boldsign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-boldsign test\n```\n\n----------------------------------------\n\nTITLE: Installing airbyte-ci\nDESCRIPTION: Command to install the airbyte-ci tool after cleaning the previous installation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nmake tools.airbyte-ci.install\n```\n\n----------------------------------------\n\nTITLE: Defining Resource Trees for Connector Orchestrator\nDESCRIPTION: Python code defining resource trees for organizing resources in the orchestrator, including GCS, metadata, and other functionalities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nGCS_RESOURCE_TREE = {\n    \"gcp_gcs_client\": gcp_gcs_client.configured(\n        {\n            \"gcp_gcs_cred_string\": {\"env\": \"GCS_CREDENTIALS\"},\n        }\n    ),\n    \"registry_directory_manager\": gcs_file_manager.configured({\"gcs_bucket\": {\"env\": \"METADATA_BUCKET\"}, \"prefix\": REGISTRIES_FOLDER}),\n    \"registry_report_directory_manager\": gcs_file_manager.configured({\"gcs_bucket\": {\"env\": \"METADATA_BUCKET\"}, \"prefix\": REPORT_FOLDER}),\n    \"root_metadata_directory_manager\": gcs_file_manager.configured({\"gcs_bucket\": {\"env\": \"METADATA_BUCKET\"}, \"prefix\": \"\"}),\n}\n\nMETADATA_RESOURCE_TREE = {\n    **SLACK_RESOURCE_TREE,\n    **GCS_RESOURCE_TREE,\n    \"all_metadata_file_blobs\": gcs_directory_blobs.configured(\n        {\"gcs_bucket\": {\"env\": \"METADATA_BUCKET\"}, \"prefix\": METADATA_FOLDER, \"match_regex\": f\".*/{METADATA_FILE_NAME}$\"}\n    ),\n    \"latest_metadata_file_blobs\": gcs_directory_blobs.configured(\n        {\"gcs_bucket\": {\"env\": \"METADATA_BUCKET\"}, \"prefix\": METADATA_FOLDER, \"match_regex\": f\".*latest/{METADATA_FILE_NAME}$\"}\n    ),\n}\n\n# ...\n\nRESOURCES = {\n    **METADATA_RESOURCE_TREE,\n    **DATA_WAREHOUSE_RESOURCE_TREE,\n    **REGISTRY_RESOURCE_TREE,\n    **REGISTRY_ENTRY_RESOURCE_TREE,\n    **CONNECTOR_TEST_REPORT_RESOURCE_TREE,\n}\n```\n\n----------------------------------------\n\nTITLE: Running Salesforce Connector in Docker\nDESCRIPTION: Commands to run various Salesforce connector operations as a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-salesforce:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-salesforce:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-salesforce:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-salesforce:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running NY Times Connector Commands via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to run standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the previously built `airbyte/source-nytimes:dev` Docker image. The `check`, `discover`, and `read` commands require a configuration file mounted from the local `secrets` directory, and the `read` command also requires a configured catalog file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nytimes/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-nytimes:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-nytimes:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-nytimes:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-nytimes:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Switching to Admin Database in MongoDB Shell\nDESCRIPTION: MongoDB shell command to switch to the admin database\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntest> use admin\nswitched to db admin\n```\n\n----------------------------------------\n\nTITLE: Generating Schema Files from Connector Read Output\nDESCRIPTION: Command to execute a connector's read operation and pipe the output to schema_generator to automatically infer and generate schema files for all streams. The generated schemas are placed in the schemas directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/schema_generator/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/<your-connector-image-name>:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json | schema_generator --infer-schemas\n```\n\n----------------------------------------\n\nTITLE: CSV Example Usage: Escaping Quotes in a Field\nDESCRIPTION: This CSV snippet demonstrates how a field can contain a literal double quote within its value by using a backslash as an escape character. The example is used to illustrate Airbyte's 'Escape Character' CSV setting, guiding users to set up correct parsing of complex values. Fields are comma separated and the escape character precedes the embedded double quote, ensuring correct data import. Expected input is plain text; output is properly parsed values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-blob-storage.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Running Oura Source Connector in Docker for Standard Operations - Bash\nDESCRIPTION: These Bash commands use Docker to run standard Airbyte source connector commands (`spec`, `check`, `discover`, and `read`) with the Oura source connector image. They demonstrate mounting directories for secrets and integration test catalogs as needed. Prerequisites include building the Docker image (`airbyte/source-oura:dev`), preparing credentials in `secrets/config.json`, and, for the read command, a configured catalog in `integration_tests/configured_catalog.json`. Each command outputs results for different stages (specification, connection check, schema discovery, and data read).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oura/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-oura:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-oura:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-oura:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-oura:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Dockerized Openweather Source Connector Commands - Bash\nDESCRIPTION: This snippet shows a sequence of Docker commands to invoke Airbyte's source-openweather connector in different operational modes (spec, check, discover, read). It presumes that the Docker image (airbyte/source-openweather:dev) is available and that the user has prepared credential and catalog files as per the expected format. The commands mount local directories as volumes to supply configuration and integration test files, enabling flexible connector usage and validation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openweather/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-openweather:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-openweather:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-openweather:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-openweather:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Redpanda Topic Format in Markdown\nDESCRIPTION: Shows the format used for creating Redpanda topics based on the stream and namespace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redpanda.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nThe Redpanda topic will be created with the following format `{namespace}_{stream}`\n```\n\n----------------------------------------\n\nTITLE: Updating S3 Policy with AWS Glue Permissions\nDESCRIPTION: This JSON policy extends the previous S3 bucket policy by adding necessary AWS Glue permissions. These permissions allow the connector to interact with Glue databases, tables, and partitions, which are required for the AWS Glue catalog integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-data-lake.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\",\n        \"s3:GetObject*\",\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket*\",\n        \"glue:TagResource\",\n        \"glue:UnTagResource\",\n        \"glue:BatchCreatePartition\",\n        \"glue:BatchDeletePartition\",\n        \"glue:BatchDeleteTable\",\n        \"glue:BatchGetPartition\",\n        \"glue:CreateDatabase\",\n        \"glue:CreateTable\",\n        \"glue:CreatePartition\",\n        \"glue:DeletePartition\",\n        \"glue:DeleteTable\",\n        \"glue:GetDatabase\",\n        \"glue:GetPartition\",\n        \"glue:GetPartitions\",\n        \"glue:GetTable\",\n        \"glue:GetTables\",\n        \"glue:UpdateDatabase\",\n        \"glue:UpdatePartition\",\n        \"glue:UpdateTable\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response for Listing Workspaces (JSON)\nDESCRIPTION: Illustrates the expected JSON response format when successfully calling the List Workspaces API endpoint. The response contains a 'data' array, where each object represents a workspace with its ID, name, and data residency setting.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"workspaceId\": \"b5367aab-9d68-4fea-800f-0000000000\",\n      \"name\": \"Finance Team\",\n      \"dataResidency\": \"auto\"\n    },\n    {\n      \"workspaceId\": \"b5367aab-9d68-4fea-800f-0000000001\",\n      \"name\": \"Analytics Team\",\n      \"dataResidency\": \"auto\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Service Account JSON Key Structure for Google Search Console\nDESCRIPTION: This snippet illustrates the structure of the JSON key file obtained when creating a service account for Google Search Console authentication in Airbyte Open Source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-search-console.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"service account\", \"project_id\": YOUR_PROJECT_ID, \"private_key_id\": YOUR_PRIVATE_KEY, ...}\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte RECORD Message for User 'aa' (JSON)\nDESCRIPTION: This JSON line represents a single record message from the 'users' stream in the Airbyte protocol. It contains data for key 'aa' with a corresponding escaped JSON string '{\"aa\": 2}' as the value, and includes an emission timestamp. This format is typical for data exchange in Airbyte connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/integration_tests/expected_records.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"users\", \"data\": {\"key\": \"aa\", \"value\": \"{\\\"aa\\\": 2}\"}, \"emitted_at\": 1640962800000}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Commands Locally with Poetry (Bash)\nDESCRIPTION: Executes standard Airbyte connector commands (`spec`, `check`, `discover`, `read`) locally using `poetry run`. This leverages the environment managed by Poetry. Requires Poetry, installed dependencies via `poetry install`, and a `secrets/config.json` file for `check`, `discover`, and `read`. The `read` command additionally requires a `sample_files/configured_catalog.json` which defines the schema and streams to sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-sheets spec\npoetry run source-google-sheets check --config secrets/config.json\npoetry run source-google-sheets discover --config secrets/config.json\npoetry run source-google-sheets read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Gorgias Source Connector Streams\nDESCRIPTION: This table lists all available streams in the Gorgias source connector, including their primary keys, pagination methods, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gorgias.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| account | domain | No pagination | ✅ |  ✅  |\n| customers | id | DefaultPaginator | ✅ |  ✅  |\n| custom-fields | id | DefaultPaginator | ✅ |  ✅  |\n| events | id | DefaultPaginator | ✅ |  ✅  |\n| integrations | id | DefaultPaginator | ✅ |  ✅  |\n| jobs | id | DefaultPaginator | ✅ |  ✅  |\n| macros | id | DefaultPaginator | ✅ |  ✅  |\n| views | id | DefaultPaginator | ✅ |  ✅  |\n| rules | id | DefaultPaginator | ✅ |  ✅  |\n| satisfaction-surveys | id | DefaultPaginator | ✅ |  ✅  |\n| tags | id | DefaultPaginator | ✅ |  ✅  |\n| teams | id | DefaultPaginator | ✅ |  ✅  |\n| tickets | id | DefaultPaginator | ✅ |  ✅  |\n| messages | id | DefaultPaginator | ✅ |  ✅  |\n| users | id | DefaultPaginator | ✅ |  ✅  |\n| views_items | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Building Linear Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Linear source connector using airbyte-ci. Creates a dev image tagged as 'source-linear:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linear/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linear build\n```\n\n----------------------------------------\n\nTITLE: Building NewsData.io Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the NewsData.io source connector using airbyte-ci tool. Creates a dev image tagged as 'source-newsdata-io:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-newsdata-io/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-newsdata-io build\n```\n\n----------------------------------------\n\nTITLE: Extracting Next Page Token from API Response in Python\nDESCRIPTION: This Python method `next_page_token` processes an HTTP response object from the SurveyMonkey API. It parses the JSON response body, looks for a 'links' dictionary, and extracts the URL associated with the 'next' key if present. This URL is returned in a dictionary format to be used for the next paginated request; an empty dictionary is returned if no 'next' link exists.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n   def next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n        links = response.json().get(\"links\", {})\n        if \"next\" in links:\n            return {\"next_url\": links[\"next\"]}\n        else:\n            return {}\n```\n\n----------------------------------------\n\nTITLE: Creating Master Encryption Key in MSSQL\nDESCRIPTION: SQL command to create a master encryption key in MSSQL for secure credential storage. This step is optional and may be required in certain environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mssql.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = '<your_password>';\n```\n\n----------------------------------------\n\nTITLE: Expected Records JSON Example for Airbyte Connector Testing\nDESCRIPTION: An example of the expected_records.jsonl file format used in Airbyte connector testing. This file contains sample records that are used to validate the output of a connector's read operation, including stream name, data payload, and emission timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value0\", \"field_2\": \"value0\", \"field_3\": null, \"field_4\": {\"is_true\": true}, \"field_5\": 123}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value1\", \"field_2\": \"value1\", \"field_3\": null, \"field_4\": {\"is_true\": false}, \"field_5\": 456}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value2\", \"field_2\": \"value2\", \"field_3\": null, \"field_4\": {\"is_true\": true}, \"field_5\": 678}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value3\", \"field_2\": \"value3\", \"field_3\": null, \"field_4\": {\"is_true\": false}, \"field_5\": 91011}, \"emitted_at\": 1626172757000}\n```\n\n----------------------------------------\n\nTITLE: OAuth Token Response Structure\nDESCRIPTION: Example JSON response structure from the OAuth token endpoint showing the access token format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"YOUR_ACCESS_TOKEN_123\"\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Secret with kubectl (Shell)\nDESCRIPTION: Demonstrates the `kubectl apply` command to create or update the Kubernetes secret defined in `secret.yaml` within a specific Kubernetes namespace. This applies the custom password or API credentials to an Airbyte deployment running directly on Kubernetes. Replace `<YOUR_NAMESPACE>` with the correct namespace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f secret.yaml -n <YOUR_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte to Use Azure Key Vault - YAML\nDESCRIPTION: This YAML configuration allows Airbyte to connect to Azure Key Vault for secret storage. You specify the secrets manager type (azureKeyVault), the Kubernetes secret name (secretsManagerSecretName), and configuration within azureKeyVault, including vaultUrl, tenantId, and optional tags. These settings link Airbyte to a previously configured Kubernetes secret (airbyte-config-secrets) containing the client credentials. Inputs: vaultUrl, tenantId, tags. Outputs: Configured connection to Azure Key Vault for Airbyte secret management.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\\n  secretsManager:\\n    type: azureKeyVault\\n    secretsManagerSecretName: \\\"airbyte-config-secrets\\\" # Name of your Kubernetes secret.\\n    azureKeyVault:\\n      vaultUrl: ## https://my-vault.vault.azure.net/\\n      tenantId: ## 3fc863e9-4740-4871-bdd4-456903a04d4e\\n      tags: ## Optional - You may add tags to new secrets created by Airbyte.\\n        - key: ## e.g. team\\n          value: ## e.g. deployments\\n        - key: business-unit\\n          value: engineering\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Teamtailor API Connection in Markdown\nDESCRIPTION: This snippet outlines the configuration parameters required for connecting to the Teamtailor API. It specifies the input types and descriptions for the API version and API key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/teamtailor.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `x_api_version` | `string` | X-Api-Version. The version of the API |  |\n| `api` | `string` | api.  |  |\n```\n\n----------------------------------------\n\nTITLE: Automated API Request Sequence for Page Increment (HTTP/JSON)\nDESCRIPTION: Demonstrates the sequence of HTTP GET requests the Airbyte Connector Builder automatically makes when configured for Page Increment pagination starting from page 1 with a page size of 2. It shows how the 'page' parameter is incremented until fewer records than the page size are returned.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_9\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.example.com/products?page_size=2&page=1\n  -> [\n       {\"id\": 1, \"name\": \"Product A\"},\n       {\"id\": 2, \"name\": \"Product B\"}\n     ]\n\nGET https://api.example.com/products?page_size=2&page=2\n  -> [\n       {\"id\": 3, \"name\": \"Product C\"},\n       {\"id\": 4, \"name\": \"Product D\"}\n     ]\n\nGET https://api.example.com/products?page_size=3&page=3\n  -> [\n       {\"id\": 5, \"name\": \"Product E\"}\n     ]\n     // less than 2 records returned -> stop\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Only Refresh Token in Response - YAML\nDESCRIPTION: This YAML configures the OAuth output to require and extract only the refresh_token at the root level of the response. It maps the token from the response to the connector configuration, ensuring the framework can perform refresh operations. The input is the OAuth connector flow and output is the expected refresh_token response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_66\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - refresh_token\\n    properties:\\n      refresh_token:\\n        type: string\\n        path_in_connector_config:\\n          - refresh_token\\n        path_in_oauth_response:\\n          - refresh_token\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Granting Broad Read Access to SingleStore User (SQL)\nDESCRIPTION: Shows the SQL command to grant SELECT (read-only) permissions on all tables within a specified database to the 'airbyte' user. This is a simpler alternative to granting permissions table by table.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/singlestore.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT\nSELECT\nON <your_database_name_here>.* TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Generating Python Models from JSON Schema in Connector Metadata Service\nDESCRIPTION: Command to generate Python models from JSON Schema specifications using Poetry and poe. The generated models are stored in 'models/generated' directory and are based on schemas in 'models/src'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run poe generate-models\n```\n\n----------------------------------------\n\nTITLE: Running the Check Operation via Docker\nDESCRIPTION: Provides the commands to build the destination connector's Docker image via Gradle and subsequently run its 'check' operation using Docker. This command validates the connection configuration provided in the specified JSON file (`/secrets/config.json`), which is mounted into the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n# First build the connector\n./gradlew :airbyte-integrations:connectors:destination-<name>:build\n\n# Run the check method\ndocker run -v $(pwd)/secrets:/secrets --rm airbyte/destination-<name>:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: S3 Destination Configuration Template in JSON\nDESCRIPTION: A Mustache template for S3 destination configuration in Airbyte Embedded. The template defines parameters for S3 connection including access keys, bucket information, path configuration, and Parquet formatting options with compression settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/embedded-setup/README.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"{{{name}}}\",\n  \"definitionId\": \"4816b78f-1489-44c1-9060-4b19d5fa9362\",\n  \"workspaceId\": \"{{{workspaceId}}}\",\n  \"configuration\": {\n    \"destinationType\": \"s3\",\n    \"access_key_id\": \"{{{access_key_id}}}\",\n    \"secret_access_key\": \"{{{secret_access_key}}}\",\n    \"s3_bucket_name\": \"{{{s3_bucket_name}}}\",\n    \"s3_bucket_path\": \"${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_\",\n    \"s3_bucket_region\": \"{{{s3_bucket_region}}}\",\n    \"format\": {\n      \"format_type\": \"Parquet\",\n      \"page_size_kb\": 1024,\n      \"block_size_mb\": 128,\n      \"compression_codec\": \"UNCOMPRESSED\",\n      \"dictionary_page_size_kb\": 1024,\n      \"max_padding_size_mb\": 8\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SendGrid Contact Export Creation Step (UI)\nDESCRIPTION: Describes the specific settings within the Airbyte Connector Builder UI's 'Creation' tab to initiate a SendGrid contacts export job. It involves setting the API endpoint URL, selecting the POST HTTP method, and configuring Bearer Token authentication with a SendGrid API key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/async-streams.md#2025-04-23_snippet_0\n\nLANGUAGE: Configuration/Text\nCODE:\n```\n- **URL** field: `https://api.sendgrid.com/v3/marketing/contacts/exports`\n- **HTTP Method** dropdown: `POST`\n- In the **Authentication** section: \n  - Select **Bearer Token** authentication type\n  - Fill out the **API Key** user input with your SendGrid API key\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Manifest-Only Airbyte Source Connector in Bash\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the manifest-only Airbyte source-coin-api connector using the airbyte-ci tool. The only dependency is airbyte-ci, which must be installed prior to running this command. The command builds the Docker image and tags it as airbyte/source-coin-api:dev within your local Docker registry. No parameters other than the connector name are necessary; output is a local Docker image ready for use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coin-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coin-api build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Hibob Source Connector\nDESCRIPTION: Command to build the Docker image for the Hibob source connector using airbyte-ci. This creates an image tagged as airbyte/source-hibob:dev on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hibob/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hibob build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Manifest-only Connector Using Airbyte CI (bash)\nDESCRIPTION: This bash snippet demonstrates how to build the Docker image for the ClickUp API manifest-only source connector using the airbyte-ci CLI tool. Requires airbyte-ci to be installed and available in the environment. The command specifies the connector by name and triggers the build process, resulting in a local Docker image tagged as airbyte/source-clickup-api:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clickup-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clickup-api build\n```\n\n----------------------------------------\n\nTITLE: Running Salesforce Connector Commands Locally\nDESCRIPTION: Commands to run various Salesforce connector operations locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-salesforce spec\npoetry run source-salesforce check --config secrets/config.json\npoetry run source-salesforce discover --config secrets/config.json\npoetry run source-salesforce read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Sendinblue connector tests with airbyte-ci\nDESCRIPTION: Command to run the complete test suite for the Sendinblue source connector using the airbyte-ci tool. This validates that the connector functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendinblue/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendinblue test\n```\n\n----------------------------------------\n\nTITLE: Running Greenhouse Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run various Greenhouse connector operations locally using Poetry, including spec retrieval, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-greenhouse spec\npoetry run source-greenhouse check --config secrets/config.json\npoetry run source-greenhouse discover --config secrets/config.json\npoetry run source-greenhouse read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Referencing Image Pull Secret in Airbyte values.yaml - YAML\nDESCRIPTION: This YAML snippet extends the previous custom registry configuration by specifying imagePullSecrets under the global.image section in values.yaml. It instructs Kubernetes to use the regcred secret for authenticating Docker image pulls from private registries. The regcred secret must exist before deploying Airbyte, and the YAML format must be kept valid for Helm parsing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  image:\n    registry: ghcr.io/NAMESPACE\n  // highlight-start\n  imagePullSecrets:\n    - name: regcred\n  // highlight-end\n```\n\n----------------------------------------\n\nTITLE: Testing Microsoft Lists Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Microsoft Lists source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-lists/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-lists test\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Connector Build with CDK\nDESCRIPTION: Gradle configuration showing how to reference the CDK in a connector's build.gradle file, including version pinning and local development options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/README.md#2025-04-23_snippet_2\n\nLANGUAGE: groovy\nCODE:\n```\nplugins {\n    id 'airbyte-java-connector'\n}\n\nairbyteJavaConnector {\n    cdkVersionRequired = '0.1.0'   // The CDK version to pin to.\n    features = ['db-destinations'] // An array of CDK features to depend on.\n    useLocalCdk = false            // Use 'true' to use a live reference to the\n                                   // local cdk project.\n}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations within a Docker container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-bing-ads:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bing-ads:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bing-ads:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-bing-ads:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Twilio Taskrouter connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio-taskrouter/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twilio-taskrouter test\n```\n\n----------------------------------------\n\nTITLE: Building the Outreach Connector Docker Image using airbyte-ci\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-outreach` connector. It requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-outreach:dev` on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-outreach build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for HoorayHR Source Connector\nDESCRIPTION: This command executes the acceptance tests for the HoorayHR source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hoorayhr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hoorayhr test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry and Pytest (Bash)\nDESCRIPTION: Executes the connector's unit tests located in the `unit_tests` directory using Pytest, invoked via `poetry run`. This ensures tests run within the correct dependency environment managed by Poetry. Requires Poetry and installed development dependencies (including Pytest).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for the Connector (Python Bash Command)\nDESCRIPTION: Runs unit tests for the connector using pytest via Poetry. This ensures the correctness of Python code in 'unit_tests' and is a core part of the development and CI process. Requires Poetry and pytest as dependencies. No parameters are needed; it outputs test results to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Eventzilla connector locally\nDESCRIPTION: Command to build the source-eventzilla connector locally, creating a dev image that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventzilla/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventzilla build\n```\n\n----------------------------------------\n\nTITLE: Building the Google Calendar Connector Locally using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-google-calendar` connector. The resulting image will be tagged as `source-google-calendar:dev`, enabling local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-calendar/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-calendar build\n```\n\n----------------------------------------\n\nTITLE: Running dbt with custom profiles directory\nDESCRIPTION: Command to execute dbt run with custom profiles and project directories pointing to the current working directory after modifying SQL files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --profiles-dir=$(pwd) --project-dir=$(pwd)\n```\n\n----------------------------------------\n\nTITLE: Building YNAB Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the YNAB source connector using airbyte-ci. Creates a dev image tagged as source-you-need-a-budget-ynab:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-you-need-a-budget-ynab/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-you-need-a-budget-ynab build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Fleetio Connector\nDESCRIPTION: Command to run the full test suite for the Fleetio source connector using airbyte-ci. This validates that any changes made to the connector meet all requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fleetio/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fleetio test\n```\n\n----------------------------------------\n\nTITLE: Defining AWS IAM Policy for Secrets Manager Access (JSON)\nDESCRIPTION: This JSON policy grants permissions for an AWS IAM Role used by Airbyte to interact with AWS Secrets Manager. It allows actions like getting, creating, listing, describing, tagging, and updating secrets. Access is restricted by a condition requiring that the secrets accessed must have the resource tag `AirbyteManaged` set to `true`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/infrastructure/aws.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:TagResource\",\n                \"secretsmanager:UpdateSecret\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Condition\": {\n                \"ForAllValues:StringEquals\": {\n                    \"secretsmanager:ResourceTag/AirbyteManaged\": \"true\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing SSO Login Tabs Component\nDESCRIPTION: React/JSX implementation using Tabs and TabItem components to display different SSO login instructions for Cloud and Self-Managed versions of Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso.md#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<Tabs groupId=\"cloud-hosted\">\n  <TabItem value=\"cloud\" label=\"Cloud\">\n    Once we inform you that you're all set up, you can log into Airbyte using SSO by visiting [cloud.airbyte.com/sso](https://cloud.airbyte.com/sso) or select the **Continue with SSO** option on the login screen.\n    \n    Specify your _company identifier_ and hit \"Continue with SSO\". You'll be forwarded to your IdP's login page (e.g. Okta login page). Log into your work account and you'll be forwarded back to Airbyte Cloud and be logged in.\n    \n    *Note:* you were already logged into your company's Okta account you might not see any login screen and directly get forwarded back to Airbyte Cloud.\n  </TabItem>\n  <TabItem value=\"self-managed\" label=\"Self-Managed\">\n    Accessing your self hosted Airbyte will automatically forward you to your IdP's login page (e.g. Okta login page). Log into your work account and you'll be forwarded back to your Airbyte and be logged in.\n  </TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Identifying Resumed Streams in Logs - angular2html\nDESCRIPTION: This log snippet shows the output generated by Airbyte's platform when a stream resumes, providing identifiers for resumed streams and namespaces. Useful for debugging and monitoring; appears in Airbyte log files. Dependencies: Airbyte v1.0.0 or newer, and a resumable sync in progress. The log lines include number of resumed streams and their names/namespaces.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/resumability.md#2025-04-23_snippet_2\n\nLANGUAGE: angular2html\nCODE:\n```\n2024-07-08 22:58:40 replication-orchestrator > Number of Resumed Full Refresh Streams: {1}\n2024-07-08 22:58:40 replication-orchestrator >  Resumed stream name: activities namespace: null\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations within a Docker container, including specification, checking, discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-twilio:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twilio:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twilio:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-twilio:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running unit tests for Source Scaffold Java JDBC connector\nDESCRIPTION: Gradle command to run unit tests for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scaffold-java-jdbc/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-scaffold-java-jdbc:unitTest\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteConnectionStatus Structure in YAML\nDESCRIPTION: Specifies the structure of the AirbyteConnectionStatus message, which reports whether an Actor was able to connect to its underlying data store with all necessary permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteConnectionStatus:\n  description: Airbyte connection status\n  type: object\n  additionalProperties: true\n  required:\n    - status\n  properties:\n    status:\n      type: string\n      enum:\n        - SUCCEEDED\n        - FAILED\n    message:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: Importing Airbyte CDK and Logging Dependencies in Python\nDESCRIPTION: These import statements provide essential modules for concurrency, state handling, message logging, and time management for an Airbyte Python connector. Dependencies include Airbyte CDK's concurrent source adapter, connector state management, streaming support, and external libraries like logging and pendulum. These are fundamental for enabling the advanced concurrency and logging features described in the connector implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# import the following libraries\\nimport logging\\nimport pendulum\\nfrom airbyte_cdk.logger import AirbyteLogFormatter\\nfrom airbyte_cdk.models import Level\\nfrom airbyte_cdk.sources.concurrent_source.concurrent_source_adapter import ConcurrentSourceAdapter, ConcurrentSource\\nfrom airbyte_cdk.sources.connector_state_manager import ConnectorStateManager\\nfrom airbyte_cdk.sources.message.repository import InMemoryMessageRepository\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Component in YAML\nDESCRIPTION: YAML configuration example showing how to reference and configure a custom component using its fully qualified class name and custom fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/custom-components.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npagination_strategy:\n  type: \"CustomPaginationStrategy\"\n  class_name: \"my_connector_module.MyPaginationStrategy\"\n  my_field: \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Lever Hiring Connector Commands via Docker\nDESCRIPTION: Executes standard Airbyte source connector commands (`spec`, `check`, `discover`, `read`) using the locally built Docker image (`airbyte/source-lever-hiring:dev`). The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing `config.json` via `-v $(pwd)/secrets:/secrets`. The `read` command additionally requires mounting an `integration_tests` directory containing `configured_catalog.json`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lever-hiring/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-lever-hiring:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lever-hiring:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lever-hiring:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-lever-hiring:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Dropbox Sign Source Connector Using Airbyte CLI - Bash\nDESCRIPTION: This snippet shows the command to build the Dropbox Sign source connector for local testing within the Airbyte platform. It requires the Airbyte CLI (`airbyte-ci`) to be installed and available in your environment. Running this command will produce a dev Docker image (`source-dropbox-sign:dev`), which can be used to test changes or local development before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dropbox-sign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dropbox-sign build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to run unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest -s unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run the connector operations within a Docker container, including specification, checking, and writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-couchbase:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-couchbase:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-couchbase:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building JustSift Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the JustSift source connector using airbyte-ci. The resulting image (source-just-sift:dev) can be used for local testing of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-just-sift/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-just-sift build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Surveycto Connector\nDESCRIPTION: Command to execute unit tests for the Surveycto connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Configuring SendGrid Contact Export Download Step (UI)\nDESCRIPTION: Outlines the UI configuration in the 'Download' tab for retrieving the results of a completed SendGrid contacts export job. It uses the `{{ download_target }}` variable (extracted during polling) as the download URL, specifies the GET HTTP method, sets the expected response format to CSV, leaves the Download Extractor field path empty to use the entire response, and sets the primary key to `CONTACT_ID`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/async-streams.md#2025-04-23_snippet_2\n\nLANGUAGE: Configuration/Templating\nCODE:\n```\n- **URL** field: `{{ download_target }}`\n- **HTTP Method** dropdown: `GET`\n- **HTTP Response Format** dropdown: `CSV`\n- In the **Download Extractor** section:\n  - Leave the **Field Path** empty since we want to use the entire CSV content\n- Set **Primary Key** to: `CONTACT_ID`\n```\n\n----------------------------------------\n\nTITLE: Custom Schema Transformation Implementation\nDESCRIPTION: Shows how to implement custom schema transformation logic with a transform function decorator.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/schemas.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyStream(Stream):\n    ...\n    transformer = Transformer(TransformConfig.CustomSchemaNormalization)\n    ...\n\n    @transformer.registerCustomTransform\n    def transform_function(original_value: Any, field_schema: Dict[str, Any]) -> Any:\n        # transformed_value = ...\n        return transformed_value\n```\n\n----------------------------------------\n\nTITLE: User-Group Mapping Configuration in YAML\nDESCRIPTION: YAML configuration that maps user email addresses to company-specific roles/groups. Each user can belong to multiple groups.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/role-mapping.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{ \n\"user1@company.com\": [\"companyGroup1\", \"companyGroup2\"], \n\"user1@company.com\": [\"companyGroup2\", \"companyGroup3\"] \n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airbyte Auth Secrets from Kubernetes (Shell)\nDESCRIPTION: Demonstrates the `kubectl` command to fetch the `airbyte-auth-secrets` Kubernetes secret in YAML format. This secret contains authentication details for Airbyte deployments managed via Helm on Kubernetes. Remember to replace `<YOUR_NAMESPACE>` with the actual namespace where Airbyte is deployed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get secret airbyte-auth-secrets -n <YOUR_NAMESPACE> -o yaml\n```\n\n----------------------------------------\n\nTITLE: Granting Granular Read Access to Airbyte User in Oracle SQL\nDESCRIPTION: These SQL commands demonstrate how to grant the 'airbyte' user read access ('SELECT' privilege) to specific tables within specific schemas. This approach provides more granular control over data access compared to granting `SELECT ANY TABLE`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON \"<schema_a>\".\"<table_1>\" TO airbyte;\nGRANT SELECT ON \"<schema_b>\".\"<table_2>\" TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container, including volume mounts for secrets and tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-databend:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-databend:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-databend:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-databend:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Starting Development Server - pnpm/Docusaurus - Bash\nDESCRIPTION: This snippet runs the Docusaurus development server using pnpm, enabling live previews of documentation as you edit. Requires dependencies to be installed first. Application is served by default at http://localhost:3005/; output can be stopped with Ctrl+C in the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Running Google Sheets Connector as Docker Container\nDESCRIPTION: Commands to run various connector operations using the built Docker image. Includes specifying connector details, checking configuration, discovering schema, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-google-sheets:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-google-sheets:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-google-sheets:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-google-sheets:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-amazon-ads spec\npoetry run source-amazon-ads check --config secrets/config.json\npoetry run source-amazon-ads discover --config secrets/config.json\npoetry run source-amazon-ads read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Mapping Documentation to Specific UI Fields using FieldAnchor - Markdown\nDESCRIPTION: This snippet anchors documentation content to a specific UI field via the `<FieldAnchor field=\"field_name\"> ... </FieldAnchor>` tag. Designed for mapping guidance to Airbyte's interface, it requires the markdown renderer to support the `FieldAnchor` tag. Key parameters: `field` attribute with jsonpath syntax. Input: Field name and instructions; Output: Highlighted content in the in-app documentation for a selected field. Limitation: Only affects UI for sources/destinations; not rendered specially on docs.airbyte.com.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## Configuring Widgets\\n\\n<FieldAnchor field=\"widget_option\">\\n\\n...config-related instructions here...\\n\\n</FieldAnchor>\n```\n\n----------------------------------------\n\nTITLE: Structuring Redis Key for Hash Implementation\nDESCRIPTION: Demonstrates the structure of Redis keys for the hash implementation. The key is composed of namespace, stream, and id, separated by colons.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redis.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n    namespace:stream:id\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Markdown table showing the mapping between SharePoint integration types and Airbyte data types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-sharepoint-enterprise.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Building AWS Cloudtrail Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet illustrates how to build the Docker image for the manifest-only AWS Cloudtrail source connector using Airbyte's ci tools. It requires the airbyte-ci utility to be installed, which orchestrates the build process and outputs a Docker image tagged as airbyte/source-aws-cloudtrail:dev. No arguments besides the connector name and 'build' directive are required. The output image will be used for local runs and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aws-cloudtrail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aws-cloudtrail build\n```\n\n----------------------------------------\n\nTITLE: Example Directory Structure for Path Pattern Demonstration (Text)\nDESCRIPTION: Shows a sample folder hierarchy (`MyFolder`) containing various subfolders and CSV files. This structure is used as a reference to explain how different glob path patterns select specific files for the Airbyte File connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-onedrive.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMyFolder\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Running Hubspot Connector Commands Locally\nDESCRIPTION: Series of commands to run the Hubspot connector locally for various operations like spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-hubspot spec\npoetry run source-hubspot check --config secrets/config.json\npoetry run source-hubspot discover --config secrets/config.json\npoetry run source-hubspot read --config secrets/config.json --catalog integration_tests/basic_read_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Records from Sheet6\nDESCRIPTION: JSON records containing ID and Name fields from a spreadsheet source. Each record includes a stream identifier, data payload with ID and Name fields, and an emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"699\",\"Name\":\"ompyMvlyX\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Nested Access Token under Data - YAML\nDESCRIPTION: Configures OAuth output specification to expect access_token nested under the data property, using path_in_oauth_response to navigate structure. Required fields and type mappings are specified similarly to the single access token scenario. It is critical that the input response matches the nested structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_64\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - access_token\\n    properties:\\n      access_token:\\n        type: string\\n        path_in_connector_config:\\n          - access_token\\n        path_in_oauth_response:\\n          - data\\n          - access_token\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Building Salesforce Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the Salesforce connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesforce build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Papersign Connector using Bash\nDESCRIPTION: This command executes the acceptance test suite for the `source-papersign` connector using the `airbyte-ci` tool. It validates the connector's functionality against predefined test cases, typically run after building the connector image. This step requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-papersign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-papersign test\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuthAuthenticator for Airbyte Connector - YAML\nDESCRIPTION: This YAML code shows how to set up the OAuthAuthenticator for an Airbyte connector, supplying endpoints, client credentials, and (optionally) the refresh token. Dependencies include the OAuthAuthenticator available in Airbyte, and the OAuth schema defined elsewhere. The configuration references user-supplied values via templating (e.g., {{ config['api_key'] }}), with inputs being specific to the user's OAuth credentials. The expected output is an authenticator instance configured to obtain and refresh tokens as specified. Empty string values or templated values may require pre-population before runtime.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_75\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: \"OAuthAuthenticator\"\n  token_refresh_endpoint: \"https://api.searchmetrics.com/v4/token\"\n  client_id: \"{{ config['api_key'] }}\"\n  client_secret: \"{{ config['client_secret'] }}\"\n  refresh_token: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining Variables for Airbyte Terraform Provider (HCL)\nDESCRIPTION: This HCL code snippet defines variables in a separate `variables.tf` file. It declares `client_id`, `client_secret`, and `workspace_id` as string types with default placeholder values. These variables are referenced in `main.tf` to configure the Airbyte provider securely.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_2\n\nLANGUAGE: hcl\nCODE:\n```\nvariable \"client_id\" {\n    type = string\n    default = \"YOUR_CLIENT_ID\"\n}\n\nvariable \"client_secret\" {\n    type = string\n    default = \"YOUR_CLIENT_SECRET\"\n}\n\nvariable \"workspace_id\" {\n    type = string\n    default = \"YOUR_AIRBYTE_WORKSPACE_ID\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Airbyte Password via Kubernetes Secret (YAML)\nDESCRIPTION: Provides the YAML structure for a Kubernetes Secret to be saved in a file (e.g., `secret.yaml`). This secret uses `stringData` to define a custom `instance-admin-password`. Optionally, `instance-admin-client-id` and `instance-admin-client-secret` can also be overridden for known API credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-auth-secrets\ntype: Opaque\nstringData:\n  instance-admin-password: # password\n\n  # Override these if you want to access the API with known credentials\n  #instance-admin-client-id: # my-client-id\n  #instance-admin-client-secret: # my-client-secret\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Secret for Database Credentials - YAML\nDESCRIPTION: This snippet defines a Kubernetes Secret manifest to store sensitive database credentials for an external Postgres used by Airbyte. The secret utilizes the 'Opaque' type and includes keys for database user and password (such as 'database-user' and 'database-password'). Ensure this Secret is created in your cluster before deploying Airbyte and reference the appropriate key names in your Helm chart configuration. Required fields like 'database-user' and 'database-password' should be set to match your external database setup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/database.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: airbyte-config-secrets\\ntype: Opaque\\nstringData:\\n  # Database Secrets\\n  database-user: ## e.g. airbyte\\n  database-password: ## e.g. password\n```\n\n----------------------------------------\n\nTITLE: Displaying JSONL Output Example with No Flattening\nDESCRIPTION: Example showing how data is stored in Azure Blob Storage using JSONL format without root level flattening, keeping original data in the _airbyte_data field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/azure-blob-storage.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{ \"_airbyte_raw_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_extracted_at\": \"1622135805000\", \"_airbyte_generation_id\": \"11\", \"_airbyte_meta\": { \"changes\": [], \"sync_id\": 10111 }, \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_extracted_at\": \"1631948170000\", \"_airbyte_generation_id\": \"12\", \"_airbyte_meta\": { \"changes\": [], \"sync_id\": 10112 }, \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }\n```\n\n----------------------------------------\n\nTITLE: Testing CDC Deduplication with Excluded Fields in JSON\nDESCRIPTION: This snippet demonstrates CDC (Change Data Capture) behavior with records having the same _ab_cdc_updated_at but different _ab_cdc_lsn values. It's used to test deduplication logic in SCD (Slowly Changing Dimension) models.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"dedup_cdc_excluded\",\n    \"data\": {\n      \"id\": 8,\n      \"name\": \"foo1\",\n      \"_ab_cdc_updated_at\": 1623850900000,\n      \"_ab_cdc_lsn\": 27010232,\n      \"_ab_cdc_deleted_at\": null\n    },\n    \"emitted_at\": 1623861660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Required Google Secret Manager Roles - Text\nDESCRIPTION: This snippet lists the IAM roles required for Airbyte service accounts to interact with Google Secret Manager. These roles should be granted to enable read and manage permissions on secrets and their versions: 'roles/secretmanager.secretAccessor', 'roles/secretmanager.secretVersionAdder', 'roles/secretmanager.secretVersionManager', and 'roles/secretmanager.viewer'. Assigning these permissions allows Airbyte to securely retrieve and manage secrets essential for integration tasks. Ensure these roles are only granted to trusted service accounts for security.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/infrastructure/gcp.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nroles/secretmanager.secretAccessor\\nroles/secretmanager.secretVersionAdder\\nroles/secretmanager.secretVersionManager\\nroles/secretmanager.viewer\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry (Bash)\nDESCRIPTION: Installs the project's dependencies, including development dependencies, as specified in the `pyproject.toml` file using the Poetry package manager. This command should be run from the connector's root directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - bash\nDESCRIPTION: Installs the required dependencies for the Github source connector project using Poetry. Poetry must already be installed as a prerequisite. The '--with dev' flag ensures both main and development dependencies are installed. The command should be run from the connector's directory. Outputs to the terminal and updates the virtual environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Locating Local File Destination Output (CSV/JSON)\nDESCRIPTION: A shell script that helps locate data files generated by Airbyte's local file destinations (CSV, JSON). It first uses a temporary BusyBox container to list files within the `/local` directory (mapped from the host's `/tmp/airbyte_local`), simulating the container's view. It then uses the host's `find` command to list files directly within `/tmp/airbyte_local`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n#!/usr/bin/env bash\n\necho \"In the container:\"\n\ndocker run -it --rm -v /tmp/airbyte_local:/local busybox find /local\n\necho \"\"\necho \"On the host:\"\n\nfind /tmp/airbyte_local\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using Poetry - Bash\nDESCRIPTION: This Bash snippet demonstrates how to install project dependencies for the Google-Ads source connector using Poetry. This command should be executed from within the connector directory after ensuring Poetry (~=1.7) and Python (~=3.9) are installed. The --with dev flag ensures that all development dependencies are included. No inputs are required, and the output is a fully-installed Python environment for the project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Workspace Resource Roles Permission Matrix\nDESCRIPTION: Markdown table detailing the permissions available to different workspace-level roles (Reader, Runner, Editor, Admin). Permissions include reading workspace details, managing connections, and updating workspace settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/rbac.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Permissions           | Reader    | Runner | Editor    | Admin    |\n| ---------------------- | :--------: | :--------:| :--------:| :--------: |\n| **Read Workspace**<br /><ul><li>List the connections in a workspace</li><li>Read individual connections</li><li>Read workspace settings (data residency, users, connector versions, notification settings) </li></ul> | X | X | X | X |\n| **Sync Connection**<br /><ul><li>Start/cancel syncs and refreshes</li></ul> | | X | X | X |\n| **Modify Connector Settings**<br /><ul><li>Create, modify, delete  sources and destinations in a workspace</li></ul> | | | X | X |\n| **Update Connection**<br /><ul><li>Modify a connection, including name, replication settings, normalization, DBT</li><li>Clear connection data</li><li>Create/Delete a connection</li><li> Create/Update/Delete connector builder connectors</li></ul> |  | | X | X |\n| **Update Workspace**<br /><ul><li> Update workspace settings (data residency, users, connector versions, notification settings)</li><li> Modify workspace connector versions</li></ul> | | |  | X |\n```\n\n----------------------------------------\n\nTITLE: MongoDB Atlas Credentials Configuration\nDESCRIPTION: JSON configuration template for MongoDB Atlas connection credentials. Used for testing the connector with an Atlas cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mongodb-v2/README.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n     \"cluster_type\": \"ATLAS_REPLICA_SET\"\n     \"database\": \"database_name\",\n     \"username\": \"username\",\n     \"password\": \"password\",\n     \"connection_string\": \"mongodb+srv://cluster0.abcd1.mongodb.net/\",\n     \"auth_source\": \"auth_database\",\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Docker Images into Kind Cluster\nDESCRIPTION: Commands for loading custom connector Docker images into a Kind (Kubernetes in Docker) cluster. This is required when running Airbyte in Kind to make the connector images available to the cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-custom-connectors.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind load docker-image <image-name>:<image-tag> -n airbyte-abctl\n```\n\nLANGUAGE: bash\nCODE:\n```\nkind load docker-image airbyte/source-custom:1 -n airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Casting Record Fields to Schema Types\nDESCRIPTION: Illustrates how to cast string fields to number types by modifying the declared schema and enabling 'Cast Record Fields to Schema Types'. This example converts latitude and longitude from strings to numbers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"street\": \"Kulas Light\",\n    \"city\": \"Gwenborough\",\n    \"geo\": {\n      \"lat\": \"-37.3159\",\n      \"lng\": \"81.1496\"\n    }\n  },\n  {\n    \"street\": \"Victor Plains\",\n    \"city\": \"Wisokyburgh\",\n    \"geo\": {\n      \"lat\": \"-43.9509\",\n      \"lng\": \"-34.4618\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Traffic Sources Stream Schema in JSON\nDESCRIPTION: JSON schema for the traffic_sources stream in the Google Analytics connector. It includes metrics related to traffic sources, medium, and social networks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_source\":\"(direct)\",\"ga_medium\":\"(none)\",\"ga_socialNetwork\":\"(not set)\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for BigQuery Destination Connector\nDESCRIPTION: Gradle command to run acceptance and custom integration tests for the BigQuery destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-bigquery:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Datetime-Based Incremental Sync with Lookback Window (Airbyte YAML)\nDESCRIPTION: This YAML configuration demonstrates advanced incremental sync with a DatetimeBasedCursor, including a 'lookback_window' to retrieve records before the defined 'start_datetime'. The 'datetime_format' supports microseconds and timezone (RFC3339+). 'lookback_window' is an ISO 8601 duration. All parameters must match the data source API requirements. The resulting window of extraction spans from the calculated lookback date to 'end_datetime'. Inputs: Full datetime strings and ISO durations. Output: Extended data inclusion window, as needed for late-arriving records or data corrections.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2022-02-01T00:00:00.000000+0000\"\n  end_datetime: \"2022-03-01T00:00:00.000000+0000\"\n  datetime_format: \"%Y-%m-%dT%H:%M:%S.%f%z\"\n  cursor_granularity: \"PT0.000001S\"\n  lookback_window: \"P31D\"\n  step: \"P1D\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kubernetes Ingress for Airbyte UI with NGINX (YAML)\nDESCRIPTION: This YAML defines a Kubernetes Ingress resource using the NGINX ingress controller, routing traffic to Airbyte's webapp and authentication services. Key configurable parameters include service hostnames, paths, and backend service names/ports. The annotated setting 'ssl-redirect: false' disables automatic HTTP->HTTPS redirection. Usage requires a deployed and configured NGINX ingress controller.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: # ingress name, example: enterprise-demo\n  annotations:\n    ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: # host, example: enterprise-demo.airbyte.com\n      http:\n        paths:\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte-webapp-svc\n                name: airbyte-enterprise-airbyte-webapp-svc\n                port:\n                  number: 80 # service port, example: 8080\n            path: /\n            pathType: Prefix\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte-keycloak-svc\n                name: airbyte-enterprise-airbyte-keycloak-svc\n                port:\n                  number: 8180\n            path: /auth\n            pathType: Prefix\n\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Mailjet Mail connector in Docker. Includes commands for spec, check, discover, and read operations with mounted config and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-mail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailjet-mail:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailjet-mail:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailjet-mail:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailjet-mail:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Bamboo HR Connector Commands via Docker in Bash\nDESCRIPTION: These commands demonstrate executing standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-bamboo-hr:dev` Docker image. The `check`, `discover`, and `read` commands require a configuration file (`secrets/config.json`) mounted into the container, while the `read` command also needs a catalog file (`integration_tests/configured_catalog.json`). Docker volumes (`-v`) are used for mounting files, and `--rm` ensures container cleanup after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bamboo-hr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-bamboo-hr:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bamboo-hr:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bamboo-hr:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-bamboo-hr:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running source connector commands on the Railz docker container\nDESCRIPTION: These commands demonstrate how to run standard source connector operations on the Railz connector docker container, including specification retrieval, configuration validation, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-railz/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-railz:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-railz:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-railz:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-railz:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands\nDESCRIPTION: Set of Docker commands for running various connector operations including spec, check, discover, and read with configuration files\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braze/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-braze:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-braze:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-braze:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-braze:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard docker commands for running the Workable source connector with different operations like spec, check, discover and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workable/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-workable:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-workable:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-workable:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-workable:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Handling Record-Level Errors in Airbyte Destinations\nDESCRIPTION: Describes how Airbyte handles record-level errors (e.g., large records, mistyped data) by storing error information in the `_airbyte_meta.errrors` field within the destination. This enhances data sync resilience for database sources, particularly for resumable syncs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.0.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n`_airbyte_meta.errrors`\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Docker Registry in Airbyte values.yaml - YAML\nDESCRIPTION: This YAML snippet configures Airbyte to pull images from a custom Docker image registry by setting the global.image.registry field within the Helm values.yaml file. Replace ghcr.io/NAMESPACE with the domain and path of the actual registry being used. This config file is consumed during Airbyte deployment (typically via Helm) and changes the source from which all platform and connector images are fetched.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  image:\n    registry: ghcr.io/NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Example Regex Pattern for Filtering SFTP Files by Name\nDESCRIPTION: Provides an example Java-style regular expression used in the 'File Pattern' setting of the Airbyte SFTP connector. This specific pattern (`log-([0-9]{4})([0-9]{2})([0-9]{2})`) filters for files matching the format `log-YYYYMMDD`, demonstrating how to replicate only files that conform to a specific naming convention.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp.md#2025-04-23_snippet_3\n\nLANGUAGE: regex\nCODE:\n```\nlog-([0-9]{4})([0-9]{2})([0-9]{2})\n```\n\n----------------------------------------\n\nTITLE: Configuring Datetime-Based Incremental Sync (Airbyte YAML)\nDESCRIPTION: This YAML configuration snippet specifies a DatetimeBasedCursor for incremental sync in an Airbyte connector. It partitions data from the data source (like an API) into daily time windows using the 'step' parameter, formatted according to the given 'datetime_format'. The 'start_datetime' and 'end_datetime' parameters define the extraction period, and 'cursor_granularity' sets the minimum progress step. The 'incremental_sync' configuration must be placed within the stream definition for a source, and a compatible Airbyte version is required. Inputs: dates in RFC3339 or specified format, ISO 8601 durations. Output: partitioned sync requests per specified interval.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2022-01-01T00:00:00\"\n  end_datetime: \"2022-01-05T12:00:00\"\n  datetime_format: \"%Y-%m-%dT%H:%M:%S\"\n  cursor_granularity: \"PT1S\"\n  step: \"P1D\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Configurable Base Currency in API Response (JSON)\nDESCRIPTION: This JSON snippet shows the result of testing the stream after making the base currency a user-configurable input (`{{ config['base'] }}`) and setting the test value to 'USD'. The example demonstrates the API response reflects the user-provided base currency ('USD') and the corresponding rates relative to it.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/tutorial.mdx#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"base\": \"USD\",\n    \"date\": \"2023-04-13\",\n    \"rates\": {\n      \"AED\": 3.6723,\n      \"AFN\": 86.286329,\n      \"ALL\": 102.489617,\n      \"AMD\": 391.984204,\n      // ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Connector Test Steps in Airbyte CI\nDESCRIPTION: This function defines the test steps for a Python connector in Airbyte CI. It returns a step tree (DAG) that includes building connector images, running unit tests, integration tests, CLI validation, and acceptance tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_test_steps(context: ConnectorContext) -> STEP_TREE:\n    \"\"\"\n    Get all the tests steps for a Python connector.\n    \"\"\"\n    return [\n        [StepToRun(id=CONNECTOR_TEST_STEP_ID.BUILD, step=BuildConnectorImages(context))],\n        [\n            StepToRun(\n                id=CONNECTOR_TEST_STEP_ID.UNIT,\n                step=UnitTests(context),\n                args=lambda results: {\"connector_under_test\": results[CONNECTOR_TEST_STEP_ID.BUILD].output[LOCAL_BUILD_PLATFORM]},\n                depends_on=[CONNECTOR_TEST_STEP_ID.BUILD],\n            )\n        ],\n        [\n            StepToRun(\n                id=CONNECTOR_TEST_STEP_ID.INTEGRATION,\n                step=IntegrationTests(context),\n                args=lambda results: {\"connector_under_test\": results[CONNECTOR_TEST_STEP_ID.BUILD].output[LOCAL_BUILD_PLATFORM]},\n                depends_on=[CONNECTOR_TEST_STEP_ID.BUILD],\n            ),\n            StepToRun(\n                id=CONNECTOR_TEST_STEP_ID.PYTHON_CLI_VALIDATION,\n                step=PyAirbyteValidation(context),\n                args=lambda results: {\"connector_under_test\": results[CONNECTOR_TEST_STEP_ID.BUILD].output[LOCAL_BUILD_PLATFORM]},\n                depends_on=[CONNECTOR_TEST_STEP_ID.BUILD],\n            ),\n            StepToRun(\n                id=CONNECTOR_TEST_STEP_ID.ACCEPTANCE,\n                step=AcceptanceTests(context, context.concurrent_cat),\n                args=lambda results: {\"connector_under_test_container\": results[CONNECTOR_TEST_STEP_ID.BUILD].output[LOCAL_BUILD_PLATFORM]},\n                depends_on=[CONNECTOR_TEST_STEP_ID.BUILD],\n            ),\n        ],\n    ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Helm Values for S3 Storage in YAML\nDESCRIPTION: Specifies Helm chart values under `global.storage` to configure Airbyte to use AWS S3 for log, state, and workload output storage. It sets the storage `type` to `S3`, references the Kubernetes secret (`secretName`) containing credentials, defines the target bucket name(s) under `bucket`, and specifies the AWS `region` and `authenticationType` (`credentials` or `instanceProfile`). Requires a pre-existing Kubernetes secret (e.g., `airbyte-config-secrets`) with S3 keys.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"S3\"\n    secretName: airbyte-config-secrets # Name of your Kubernetes secret.\n    bucket: ## S3 bucket names that you've created. We recommend storing the following all in one bucket.\n      log: airbyte-bucket\n      state: airbyte-bucket\n      workloadOutput: airbyte-bucket\n    s3:\n      region: \"\" ## e.g. us-east-1\n      authenticationType: credentials ## Use \"credentials\" or \"instanceProfile\"\n\n```\n\n----------------------------------------\n\nTITLE: Adding High-Volume Table to Airbyte Publication in PostgreSQL (Read-Only Fix)\nDESCRIPTION: SQL command to add an existing high-volume table to the specified Airbyte publication in PostgreSQL. This is a read-only workaround to advance the Log Sequence Number (LSN) and resolve CDC sync failures due to low transaction volumes, but may increase disk consumption. Requires the publication name and the high-volume table name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER PUBLICATION <publicationName> ADD TABLE <high_volume_table>;\n```\n\n----------------------------------------\n\nTITLE: Granting Read-Only Access to All Tables in a Schema - SQL\nDESCRIPTION: This snippet provides read access to all tables in a CockroachDB schema for the 'airbyte' user. The 'GRANT SELECT' statement authorizes the user to select from existing tables, and the 'ALTER DEFAULT PRIVILEGES' statement ensures read access on any new tables created in the future within the specified schema. Replace <schema_name> accordingly. These grants are recommended for enabling full replication capabilities in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/cockroachdb.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO airbyte;\n\n# Allow airbyte user to see tables created in the future\nALTER DEFAULT PRIVILEGES IN SCHEMA <schema_name> GRANT SELECT ON TABLES TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Selecting the Whole Response with Field Path - YAML\nDESCRIPTION: YAML snippet for configuring a selector that extracts all records from the root of an HTTP response, including when the response is a JSON array or a single object (wrapped as a one-element array). The extractor’s field_path is set to an empty array to achieve this. Input should be a JSON array or object; output matching records are extracted accordingly. Used in Airbyte stream configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  extractor:\n    field_path: []\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the Freshcaller connector locally for specification retrieval, connection checking, data discovery, and reading data using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run main.py spec\npoetry run main.py check --config secrets/config.json\npoetry run main.py discover --config secrets/config.json\npoetry run main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing AssemblyAI Connector Using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the AssemblyAI connector using airbyte-ci. Validates the connector's functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-assemblyai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-assemblyai test\n```\n\n----------------------------------------\n\nTITLE: Defining PageIncrement Pagination Strategy Schema in YAML\nDESCRIPTION: Defines the YAML schema for the `PageIncrement` pagination strategy. It requires an integer `page_size` property and allows optional `$parameters`. This strategy assumes pagination is handled by incrementing a page number parameter in subsequent requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nPageIncrement:\n  type: object\n  additionalProperties: true\n  required:\n    - page_size\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    page_size:\n      type: integer\n```\n\n----------------------------------------\n\nTITLE: Running Connector Operations Locally\nDESCRIPTION: Commands to run various connector operations like spec, check, discover, and write locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config integration_tests/config.json\npython main.py discover --config integration_tests/config.json\ncat integration_tests/messages.jsonl| python main.py write --config integration_tests/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally, including spec, check, discover, and read functionalities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py check --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py discover --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Specifying OneOf-Option Documentation Highlighting in Connector UI Schema - JSON\nDESCRIPTION: A JSON schema object demonstrating the structure of a field using a \"oneOf\" key for multiple configuration options in Airbyte's UI, including a comment hint ('<!-- highlight-next-line -->') for doc generation. Used to show how to connect schema options to documentation anchoring through the value of a `const` property. Input: JSON schema for a connector property; Output: An example mapping for documentation highlighting. Prerequisite: Understanding Airbyte connector schema properties and jsonpath.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"replication_method\\\": {\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"title\\\": \\\"Update Method\\\",\\n    \\\"oneOf\\\": [\\n      {\\n        \\\"title\\\": \\\"Read Changes using Binary Log (CDC)\\\",\\n        \\\"required\\\": [\\\"method\\\"],\\n        \\\"properties\\\": {\\n          \\\"method\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            <!-- highlight-next-line -->\\n            \\\"const\\\": \\\"CDC\\\",\\n            \\\"order\\\": 0\\n          },\\n          \\\"initial_waiting_seconds\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"title\\\": \\\"Initial Waiting Time in Seconds (Advanced)\\\",\\n          }\\n        }\\n      },\\n      {\\n        \\\"title\\\": \\\"Scan Changes with User Defined Cursor\\\",\\n        \\\"required\\\": [\\\"method\\\"],\\n        \\\"properties\\\": {\\n          \\\"method\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            <!-- highlight-next-line -->\\n            \\\"const\\\": \\\"STANDARD\\\",\\n            \\\"order\\\": 0\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci for Google-Ads Connector - Bash\nDESCRIPTION: This Bash snippet demonstrates building the Docker image for the Google-Ads connector using Airbyte's airbyte-ci tool. Prior installation of airbyte-ci is required. The command will create the image with the tag airbyte/source-google-ads:dev on the local host. There are no required inputs beyond the airbyte-ci setup, and the expected output is a ready-to-use Docker image for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-ads build\n```\n\n----------------------------------------\n\nTITLE: Installing the Connector with Poetry\nDESCRIPTION: This command installs the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Second Example API Response for Page Increment (JSON)\nDESCRIPTION: Displays the sample JSON response corresponding to the second Page Increment API request. It returns the records for page 2.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\"id\": 3, \"name\": \"Product C\"},\n    {\"id\": 4, \"name\": \"Product D\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for SQLite Connector\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-sqlite test\n```\n\n----------------------------------------\n\nTITLE: Running RSS Source Connector Commands Locally\nDESCRIPTION: Commands for running various operations of the RSS source connector locally, including specification viewing, configuration checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-rss spec\npoetry run source-rss check --config secrets/config.json\npoetry run source-rss discover --config secrets/config.json\npoetry run source-rss read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building PandaDoc Source Connector Image with airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image tagged `source-pandadoc:dev` for the PandaDoc source connector. This requires `airbyte-ci` to be installed and configured. The command facilitates local testing and development of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pandadoc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pandadoc build\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: Detailed version history table showing updates and changes to the Omnisend connector over time, including version numbers, dates, pull request references, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/omnisend.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject        |\n|:--------|:-----------| :------------------------------------------------------- | :------------- |\n| 0.3.8 | 2025-04-19 | [58483](https://github.com/airbytehq/airbyte/pull/58483) | Update dependencies |\n| 0.3.7 | 2025-04-12 | [57871](https://github.com/airbytehq/airbyte/pull/57871) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounting for configs and tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-amazon-seller-partner:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-seller-partner:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-seller-partner:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-amazon-seller-partner:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: AWS CloudTrail Data Type Mapping\nDESCRIPTION: Markdown table showing the mapping between AWS CloudTrail data types and corresponding Airbyte types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aws-cloudtrail.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `integer`    |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Running Firestore Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-firestore spec\npoetry run destination-firestore check --config secrets/config.json\npoetry run destination-firestore discover --config secrets/config.json\npoetry run destination-firestore read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Performance Tests via Gradle\nDESCRIPTION: Command to execute performance tests using Gradle with optional CPU and memory limits. Supports parameters for limiting CPU cores (minimum 2) and memory allocation (minimum 6MB).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mssql/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mssql:performanceTest [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Kyriba Connector\nDESCRIPTION: Command to build a Docker image for the Kyriba connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kyriba build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Smartsheets Connector\nDESCRIPTION: Command to add a new dependency to the Smartsheets connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Bigcommerce source connector docker container for different operations like spec, check, discover and read\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bigcommerce/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-bigcommerce:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bigcommerce:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-bigcommerce:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-bigcommerce:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Microsoft Lists Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Microsoft Lists source connector using airbyte-ci tool. Creates a dev image tagged as source-microsoft-lists:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-lists/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-lists build\n```\n\n----------------------------------------\n\nTITLE: Webflow Sites API Sample Response - JSON\nDESCRIPTION: This JSON snippet represents a sample response returned by the Webflow 'sites' API endpoint. It highlights key fields such as '_id', 'createdOn', 'name', 'shortName', 'lastPublished', 'previewUrl', 'timezone', and 'database'. It is intended for illustrative purposes to help users recognize the expected output structure after querying the endpoint. The actual field values will be replaced with responses specific to the user's Webflow sites. No runtime dependencies are required for viewing this sample.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/webflow.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\"_id\":\"<redacted>\",\"createdOn\":\"2021-03-26T15:46:04.032Z\",\"name\":\"Airbyte\",\"shortName\":\"airbyte-dev\",\"lastPublished\":\"2022-06-09T12:55:52.533Z\",\"previewUrl\":\"https://screenshots.webflow.com/sites/<redacted>\",\"timezone\":\"America/Los_Angeles\",\"database\":\"<redacted>\"}]\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies via Poetry (Bash)\nDESCRIPTION: Demonstrates how to install all required dependencies for the connector using Poetry with the development group. This ensures the development environment is correctly set up with versions as specified in the project's pyproject.toml. Poetry (version ~1.7) and Python (~3.9) are prerequisites, and the project directory should be the current working directory. Expects that poetry is already installed and available in the system path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Parameter String Interpolation in YAML\nDESCRIPTION: Shows how to use parameter values in string interpolation using the {{ parameters['key'] }} syntax.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/parameters.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    k2: \"MyKey is {{ parameters['MyKey'] }}\"\n```\n\n----------------------------------------\n\nTITLE: Granting TiDB Permissions to Airbyte User (SQL)\nDESCRIPTION: SQL command to grant the necessary privileges (CREATE, INSERT, SELECT, DROP, CREATE VIEW, ALTER) on a specific database to the 'airbyte'@'%' user. Replace `<database name>` with the actual target database name where Airbyte will create tables. These permissions allow Airbyte to manage schemas, tables, and data within the specified database.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/tidb.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT CREATE, INSERT, SELECT, DROP, CREATE VIEW, ALTER ON <database name>.* TO 'airbyte'@'%';\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Connector\nDESCRIPTION: Command to execute the integration acceptance tests for the Fauna connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest -p integration_tests.acceptance\n```\n\n----------------------------------------\n\nTITLE: Copying Replicated Data to Host Machine with Docker\nDESCRIPTION: Command for copying output files from the Airbyte server container to the host machine's current working directory. This allows users to access the replicated data files locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/csv.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path}/{filename}.csv .\n```\n\n----------------------------------------\n\nTITLE: Testing Imagga Source Connector for Airbyte using airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the Imagga source connector. It uses airbyte-ci to execute the test suite for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-imagga/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-imagga test\n```\n\n----------------------------------------\n\nTITLE: Running Commercetools Connector Commands Locally - bash\nDESCRIPTION: This sequence illustrates how to execute the main operational commands (spec, check, discover, read) for the Commercetools connector via Poetry. The commands require a configuration file containing connection details and, in the case of 'read', a catalog JSON file. The outputs provide connector specifications, configuration validation, schema discovery, and data streaming, facilitating local development and testing workflows.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-commercetools spec\npoetry run source-commercetools check --config secrets/config.json\npoetry run source-commercetools discover --config secrets/config.json\npoetry run source-commercetools read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Processing Sheet6 Stream Data in JSON Format\nDESCRIPTION: Stream of JSON records containing sequential IDs from 1320-1422 with corresponding random 9-character names. Each record includes a stream identifier, data payload with ID and Name fields, and an emitted_at timestamp of 1673989569000 or 1673989570000.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1320\",\"Name\":\"UvRLthGfO\"},\"emitted_at\":1673989569000}\n```\n\n----------------------------------------\n\nTITLE: Test Workflow Diagram\nDESCRIPTION: Mermaid flowchart showing the test pipeline workflow including static code analysis and test execution steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_17\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    entrypoint[[For each selected connector]]\n    subgraph static [\"Static code analysis\"]\n      qa[Run QA checks]\n      sem[\"Check version follows semantic versioning\"]\n      incr[\"Check version is incremented\"]\n      metadata_validation[\"Run metadata validation on metadata.yaml\"]\n      sem --> incr\n    end\n    subgraph tests [\"Tests\"]\n        build[Build connector docker image]\n        unit[Run unit tests]\n        integration[Run integration tests]\n        pyairbyte_validation[Python CLI smoke tests via PyAirbyte]\n        cat[Run connector acceptance tests]\n        secret[Load connector configuration]\n\n        unit-->secret\n        unit-->build\n        secret-->integration\n        secret-->cat\n        secret-->pyairbyte_validation\n        build-->integration\n        build-->cat\n    end\n    entrypoint-->static\n    entrypoint-->tests\n    report[\"Build test report\"]\n    tests-->report\n    static-->report\n```\n\n----------------------------------------\n\nTITLE: Defining Hugging Face Datasets Streams in Markdown\nDESCRIPTION: This snippet outlines the available streams in the Hugging Face Datasets connector, including their properties such as primary key, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hugging-face-datasets.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| rows |  | DefaultPaginator | ✅ |  ❌  |\n| splits |  | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Formatting JSON Lines (JSONL) in Airbyte Output\nDESCRIPTION: Example of JSON Lines format used in Airbyte output, showing the structure with Airbyte-specific fields and the original data from the source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/gcs.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Registry Publishing in metadata.yaml (YAML)\nDESCRIPTION: This snippet shows the `remoteRegistries` section, used to configure publishing to external registries like PyPI. It demonstrates enabling PyPI publishing (`enabled: true`) and specifying the target `packageName`. Currently, this configuration does not trigger automated publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nremoteRegistries:\n  pypi:\n    enabled: true\n    packageName: airbyte-source-connector-name\n```\n\n----------------------------------------\n\nTITLE: Defining the cursor_field property in Python\nDESCRIPTION: Establishes the cursor field for incremental reading by implementing a property in the base stream Python class. The property returns the instance's private cursor field, which identifies the timeline reference for records. No external dependencies are required, but the class must have _cursor_field set during initialization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    @property\n    def cursor_field(self) -> Optional[str]:\n        return self._cursor_field\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for DateTime Format\nDESCRIPTION: Represents a JSON schema definition for a field intended to store a specific instant in time (timestamp). This schema uses the standard `string` type with the `date-time` format specifier.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"string\",\n  \"format\": \"date-time\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Running Recharge connector operations locally\nDESCRIPTION: Commands to run the Recharge connector locally for various operations including retrieving specs, checking connection, discovering schemas, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-recharge spec\npoetry run source-recharge check --config secrets/config.json\npoetry run source-recharge discover --config secrets/config.json\npoetry run source-recharge read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Google Forms Connector Acceptance Tests using airbyte-ci (Bash)\nDESCRIPTION: Executes the `airbyte-ci` command to run the standard acceptance tests against the locally built `source-google-forms` connector. This step validates the connector's functionality against the Airbyte specification and is typically performed after building the connector image. Requires `airbyte-ci`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-forms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-forms test\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Commands Locally\nDESCRIPTION: Commands to run the Firebolt connector locally for specification, configuration check, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\ncat integration_tests/messages.jsonl | python main.py write --config secrets/config_sql.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Cursor Pagination (Request Path) Example in YAML\nDESCRIPTION: Illustrates a YAML configuration for `CursorPagination` where the API provides the full URL for the next page in the response headers. The `cursor_value` `{{ headers['link']['next']['url'] }}` extracts this URL. The `page_token_option` is set to `RequestPath`, indicating the extracted cursor value represents the entire path/URL for the subsequent request, overriding the base URL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\npaginator:\n  type: \"DefaultPaginator\"\n  <...>\n  pagination_strategy:\n    type: \"CursorPagination\"\n    cursor_value: \"{{ headers['link']['next']['url'] }}\"\n  page_token_option:\n    type: \"RequestPath\"\n```\n\n----------------------------------------\n\nTITLE: Example Dockerfile for Building Connector Image (Dockerfile)\nDESCRIPTION: Demonstrates a basic Dockerfile structure for building a Python connector image. It starts from the `airbyte/python-connector-base` image, copies the connector code, and installs it using pip. This method is presented as an alternative (not preferred) to using `airbyte-ci`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM airbyte/python-connector-base:1.1.0\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte with abctl and Custom Host - Shell\nDESCRIPTION: Uses abctl to locally install Airbyte, specifying the host with the --host argument. The [HOSTNAME] placeholder should be replaced by the FQDN or IP address you want Airbyte accessible from. Outputs installation status and configures Airbyte to listen externally if needed. Ensure appropriate security group rules for the chosen port.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --host [HOSTNAME]\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running different connector operations locally using Poetry, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-monday spec\npoetry run source-monday check --config secrets/config.json\npoetry run source-monday discover --config secrets/config.json\npoetry run source-monday read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Data Plane Creation Response in JSON\nDESCRIPTION: The successful response from the data plane creation API call, containing the data plane ID, client ID, and client secret needed for authentication when deploying the data plane.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataplaneId\": \"uuid-string\",\n  \"clientId\": \"client-id-string\",\n  \"clientSecret\": \"client-secret-string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running NASA Connector Docker Commands\nDESCRIPTION: Standard commands for running the NASA source connector in Docker, including spec, check, discover, and read operations with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nasa/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-nasa:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-nasa:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-nasa:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-nasa:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Upgrading Airbyte Self-Managed Enterprise with Helm (Shell)\nDESCRIPTION: This shell command upgrades (or installs if not already present) the Airbyte Self-Managed Enterprise deployment in a specified namespace, reusing the supplied values.yaml file. The upgrade workflow is typically preceded by 'helm repo update' to ensure latest chart versions. It is applicable when changing Airbyte platform version or updating deployment configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nhelm upgrade \\\n--namespace airbyte \\\n--values ./values.yaml \\\n--install airbyte-enterprise \\\nairbyte/airbyte\n\n```\n\n----------------------------------------\n\nTITLE: Displaying MQTT Connector Features in Markdown Table\nDESCRIPTION: A markdown table showing the supported features of the MQTT destination connector, including sync types and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mqtt.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Supported?\\(Yes/No\\) | Notes |\n| :----------------------------- | :------------------- | :---- |\n| Full Refresh Sync              | No                   |       |\n| Incremental - Append Sync      | Yes                  |       |\n| Incremental - Append + Deduped | No                   |       |\n| Namespaces                     | Yes                  |       |\n```\n\n----------------------------------------\n\nTITLE: Creating IAM Policy JSON for S3 Bucket Access\nDESCRIPTION: This JSON defines an AWS IAM policy that grants the necessary permissions (s3:GetObject and s3:ListBucket) to read from a specific S3 bucket. The policy must be applied to either an IAM user or role to authenticate the S3 connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/s3.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n                \"arn:aws:s3:::{your-bucket-name}/*\",\n                \"arn:aws:s3:::{your-bucket-name}\"\n        ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating R2 Output Path Format\nDESCRIPTION: This snippet shows the full path structure of the output data using the default R2 Path Format. It illustrates how different components like bucket name, namespace, stream name, and timestamps are combined.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<bucket-name>/<source-namespace-if-exists>/<stream-name>/<upload-date>_<epoch>_<partition-id>.<format-extension>\n```\n\n----------------------------------------\n\nTITLE: Displaying Pinecone Connector Features in Markdown\nDESCRIPTION: This markdown table shows the supported features of the Pinecone connector, including sync types and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/pinecone.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Supported? | Notes                                                                                                             |\n| :----------------------------- | :--------- | :---------------------------------------------------------------------------------------------------------------- |\n| Full Refresh Sync              | Yes        |                                                                                                                   |\n| Incremental - Append Sync      | Yes        |                                                                                                                   |\n| Incremental - Append + Deduped | Yes        | Deleting records via CDC is not supported (see issue [#29827](https://github.com/airbytehq/airbyte/issues/29827)) |\n| Namespaces                     | Yes        |                                                                                                                   |\n```\n\n----------------------------------------\n\nTITLE: Running TiDB Source Connector Docker Commands\nDESCRIPTION: Examples of running various connector commands using the built Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tidb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/source-tidb:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tidb:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tidb:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tidb:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Solarwinds Service Desk Connector in Markdown\nDESCRIPTION: This snippet shows the configuration parameters for the Solarwinds Service Desk connector, including the API key and start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/solarwinds-service-desk.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key_2` | `string` | API Key. Refer to `https://documentation.solarwinds.com/en/success_center/swsd/content/completeguidetoswsd/token-authentication-for-api-integration.htm#link4` |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Configuring File Source with Reader Options in Markdown\nDESCRIPTION: Example table showing how to configure the File source connector with different reader options for various data sources and formats.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/file.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Dataset Name  | Storage | URL                                             | Reader Impl | Reader Options                  | Description                                                                                                                                      |\n| ------------- | ------- | ----------------------------------------------- | ----------- | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n| landsat_index | GCS     | gs://gcp-public-data-landsat/index.csv.gz       | GCFS        | `{\"compression\": \"gzip\"}`       | Additional reader options to specify a compression option to `read_csv`                                                                          |\n| GDELT         | S3      | s3://gdelt-open-data/events/20190914.export.csv |             | `{\"sep\": \"\\t\", \"header\": null}` | Here is TSV data separated by tabs without header row from [AWS Open Data](https://registry.opendata.aws/gdelt/)                                 |\n| server_logs   | local   | /local/logs.log                                 |             | `{\"sep\": \";\"}`                  | After making sure a local text file exists at `/tmp/airbyte_local/logs.log` with logs file from some server that are delimited by ';' delimiters |\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteCatalog for Static API Stream - Javascript (JSON Syntax)\nDESCRIPTION: This snippet defines an AirbyteCatalog as a JSON object for an API source that outputs a single static stream ('ticker'), which holds stock ticker data (symbol, price, date). Each field is described with JSON Schema type constraints. Inputs: API response fields intended for cataloging as a stream; Outputs: AirbyteCatalog JSON. Dependencies: None; designed for API sources following similar static stream mapping.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/beginners-guide-to-catalog.md#2025-04-23_snippet_3\n\nLANGUAGE: Javascript\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"name\": \"ticker\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Slack Notification with Attachments\nDESCRIPTION: This JSON demonstrates how to create a Slack notification with attachments. The structure includes a color-coded sidebar, a title with a link, and formatted message text for a more sophisticated notification appearance.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/Google/vocab.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"attachments\": [\n    {\n      \"color\": \"#FF0000\",\n      \"title\": \"Airbyte Notification\",\n      \"title_link\": \"http://airbyte.io\",\n      \"text\": \"This is a notification from Airbyte with an attachment.\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Markdown table defining the required configuration parameters for the Akeneo connector, including host, API credentials, and authentication details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/akeneo.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `host` | `string` | Host.  |  |\n| `api_username` | `string` | API Username.  |  |\n| `password` | `string` | password.  |  |\n| `client_id` | `string` | Client ID.  |  |\n| `secret` | `string` | Secret.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building Strava Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Strava source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-strava:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-strava/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-strava build\n```\n\n----------------------------------------\n\nTITLE: Updating HTTP request parameters for incremental reading in Python\nDESCRIPTION: Implements request_params to dynamically build request parameters for the API based on the paging and slicing logic. Incorporates standard paging, fields to include, and start/end modification timestamps converted to formatted strings. Utilizes datetime for timestamp formatting, expects stream_slice to provide date boundaries, and depends on _OUTGOING_DATETIME_FORMAT and _PAGE_SIZE.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def request_params(\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\n    ) -> MutableMapping[str, Any]:\n        if next_page_token:\n            return urlparse(next_page_token[\"next_url\"]).query\n        else:\n            return {\n                \"per_page\": _PAGE_SIZE, \"include\": \"response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats\",\n                \"start_modified_at\": datetime.datetime.strftime(datetime.datetime.fromtimestamp(stream_slice[\"start_date\"]), _OUTGOING_DATETIME_FORMAT),\n                \"end_modified_at\": datetime.datetime.strftime(datetime.datetime.fromtimestamp(stream_slice[\"end_date\"]), _OUTGOING_DATETIME_FORMAT)\n                    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Child Component Parameters in YAML\nDESCRIPTION: Example showing how to configure parameters for child components when using custom components, specifically addressing the URL base parameter requirement.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/custom-components.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  paginator:\n    type: \"DefaultPaginator\"\n    <...>\n    $parameters:\n      url_base: \"https://example.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role Trust Relationship for Airbyte Cloud\nDESCRIPTION: JSON policy for setting up a trust relationship for an IAM role in Airbyte Cloud. It allows the Airbyte instance's AWS account to assume the role, using the Airbyte workspace ID as an external ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::094410056844:user/delegated_access_user\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"{your-airbyte-workspace-id}\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Airbyte Worker to Debug Specific Connectors (Bash)\nDESCRIPTION: Command to launch Airbyte services using Docker Compose, enabling remote JVM debugging for a specific container (e.g., `destination-postgres`) launched dynamically by the `worker`. It sets the `DEBUG_CONTAINER_IMAGE` environment variable to target the desired connector image and uses both `docker-compose.yaml` and `docker-compose.debug.yaml`. The `VERSION=dev` variable is also passed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nDEBUG_CONTAINER_IMAGE=\"destination-postgres:5005\" VERSION=\"dev\" docker compose -f docker-compose.yaml -f docker-compose.debug.yaml up\n```\n\n----------------------------------------\n\nTITLE: Setting Helm Values for External Postgres and Disabling Internal Database - YAML\nDESCRIPTION: This snippet illustrates the configuration required in a Helm 'values.yaml' file to disable Airbyte's internal Postgres and configure an external Postgres database, referencing credentials from the previously defined Kubernetes Secret. It includes toggling 'postgresql.enabled' to false, marking the database type as external, and specifying the secret references for host, port, database name, user, and password. The database password must be passed exclusively via the specified secret and not directly in the values file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/database.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\\n  enabled: false\\n\\nglobal:\\n  database:\\n    type: external\\n\\n    # -- Secret name where database credentials are stored\\n    secretName: \"\" # e.g. \"airbyte-config-secrets\"\\n\\n    # -- The database host\\n    host: \"\"\\n\\n    # -- The database port\\n    port: \"\"\\n\\n    # -- The database name\\n    database: \"\"\\n\\n    # -- The key within `secretName` where database user is stored\\n    userSecretKey: \"\" # e.g. \"database-user\"\\n\\n    # -- The key within `secretName` where password is stored\\n    passwordSecretKey: \"\" # e.g.\"database-password\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation of Airbyte Cloud Limits\nDESCRIPTION: Structured documentation detailing the technical limitations of Airbyte Cloud, including workspace counts, connector instances, stream limits, and record size constraints. Includes special notes for Airbyte Teams customers and handling of large records in specific destinations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/understand-airbyte-cloud-limits.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nproducts: cloud\n---\n\n# Airbyte Cloud limits\n\nUnderstanding the following limitations will help you more effectively manage Airbyte Cloud.\n\n- Max number of workspaces per user: 3*\n- Max number of instances of the same source connector: 10*\n- Max number of destinations in a workspace: 20*\n- Max number of streams that can be returned by a source in a discover call: 1K\n- Max number of streams that can be configured to sync in a single connection: 1K\n- Max number of fields that can be selected to sync in a single connection: 20k\n- Size of a single record: 20MB**\n\n---\n\n* Limits on workspaces, sources, and destinations do not apply to customers of\nAirbyte Teams. To learn more\n[contact us](https://airbyte.com/talk-to-sales)!\n\n** The effective maximum size of the record may vary based per destination. Some destinations may\nfail to sync if a record cannot be stored, but Destinations which support\n[typing and deduping](/using-airbyte/core-concepts/typing-deduping) will adjust your record so that\nthe sync does not fail, given the database/file constraints. For example, the maximum size of a\nrecord in MongoDB is 16MB - records larger than that will need to be modified. At the very least,\nprimary keys and cursors will be maintained. Any modifications to the record will be stored within\n`airbyte_meta.changes` for your review within the destination.\n```\n\n----------------------------------------\n\nTITLE: Defining Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema example for the weekly_active_users stream in Google Analytics, showing the count of 7-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_7dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Mux API Configuration Parameters\nDESCRIPTION: Configuration table showing the required input parameters for connecting to the Mux API, including username, password, start date, and playback ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mux.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `playback_id` | `string` | Playback ID. The playback id for your video asset shown in website details |  |\n```\n\n----------------------------------------\n\nTITLE: Running Timely Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Timely source connector, including spec, check, discover, and read. These commands use the locally built Docker image and mount necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-timely/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-timely:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-timely:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-timely:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-timely:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Airbyte and Removing All Data\nDESCRIPTION: Command to completely uninstall Airbyte, including all persisted data. This performs a clean removal of the installation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nabctl local uninstall --persisted\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-instagram spec\npoetry run source-instagram check --config secrets/config.json\npoetry run source-instagram discover --config secrets/config.json\npoetry run source-instagram read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry - Bash\nDESCRIPTION: This snippet shows how to add a new dependency to the project using Poetry. Replace <package-name> with the desired Python package. After execution, Poetry will update pyproject.toml and poetry.lock, which should be committed to maintain dependency consistency.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Copying SSH Public Key via ssh-copy-id in Bash\nDESCRIPTION: Demonstrates using the `ssh-copy-id` command to transfer a generated public SSH key to a specified remote server for passwordless authentication as part of the SFTP Bulk connector setup. Requires the username and IP address/hostname of the server.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp-bulk.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nssh-copy-id <username>@<server_ip_address>\n```\n\n----------------------------------------\n\nTITLE: Missive Streams Table\nDESCRIPTION: Detailed table showing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/missive.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| contact_books | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ✅  |\n| contact_groups | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| teams | id | DefaultPaginator | ✅ |  ❌  |\n| shared_labels | id | DefaultPaginator | ✅ |  ❌  |\n| organizations | id | DefaultPaginator | ✅ |  ❌  |\n| conversations | id | No pagination | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Building Campayn Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Campayn connector using airbyte-ci tool. Creates a dev image tagged as source-campayn:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-campayn/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-campayn build\n```\n\n----------------------------------------\n\nTITLE: Enabling Local Infile Loading on SingleStore (SQL)\nDESCRIPTION: This snippet sets the system variable 'local_infile' to true, which is required for Airbyte to leverage the LOAD DATA LOCAL INFILE command during data ingestion. This change must be made with sufficient privileges, typically as a user with SUPER or SYSTEM_VARIABLES_ADMIN permissions. The command affects the global database setting and may need to be repeated following a server restart.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/singlestore.md#2025-04-23_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSET GLOBAL local_infile = true\n```\n\n----------------------------------------\n\nTITLE: Parsing Partitioned API Parent Stream Records - JSON Example - JSON\nDESCRIPTION: Shows the JSON structure of parent stream records fetched from the Woocommerce API orders endpoint. Each record contains an 'id' property (used as the partition key) and additional order details. This is used to exemplify how the Parent Stream partitioning mechanism extracts partition values dynamically from fetched records.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"id\": 123, \"currency\": \"EUR\", \"shipping_total\": \"12.23\", ... }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"id\": 456, \"currency\": \"EUR\", \"shipping_total\": \"45.56\", ... }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"id\": 789, \"currency\": \"EUR\", \"shipping_total\": \"78.89\", ... }\n```\n\n----------------------------------------\n\nTITLE: Defining Flat Table Schema Mapping in JSON\nDESCRIPTION: This snippet defines a flat schema for a tabular dataset in JSON format, mapping each column name to its corresponding datatype. The Airbyte connector uses this JSON schema to override inference-based typing, providing explicit control over which columns and datatypes will appear in the output stream. The schema must be valid JSON and keys correspond to column names with primitive or complex types as allowed (string, number, integer, object, array, boolean, null). Expected input is a JSON object; output is schema-driven record parsing. No external dependencies are required beyond JSON parsing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-blob-storage.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\\\"id\\\": \\\"integer\\\", \\\"location\\\": \\\"string\\\", \\\"longitude\\\": \\\"number\\\", \\\"latitude\\\": \\\"number\\\"}\n```\n\n----------------------------------------\n\nTITLE: Running Strava Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Strava source connector, including spec retrieval, configuration checking, schema discovery, and data reading. These commands use the locally built Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-strava/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-strava:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-strava:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-strava:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-strava:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining DefaultPaginator Schema in YAML\nDESCRIPTION: Defines the YAML schema for the `DefaultPaginator`. It requires `page_token_option` and `pagination_strategy` properties. Optional properties include `$parameters`, `page_size`, and `page_size_option`. This schema relies on other definitions like `RequestOption`, `RequestPath`, and `PaginationStrategy` to specify how page size and tokens are handled and how the next page is determined.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nDefaultPaginator:\n  type: object\n  additionalProperties: true\n  required:\n    - page_token_option\n    - pagination_strategy\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    page_size:\n      type: integer\n    page_size_option:\n      \"$ref\": \"#/definitions/RequestOption\"\n    page_token_option:\n      anyOf:\n        - \"$ref\": \"#/definitions/RequestOption\"\n        - \"$ref\": \"#/definitions/RequestPath\"\n    pagination_strategy:\n      \"$ref\": \"#/definitions/PaginationStrategy\"\n```\n\n----------------------------------------\n\nTITLE: Testing FreeAgent Source Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the FreeAgent source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-free-agent-connector/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-free-agent-connector test\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Page Read with HttpMocker in Python\nDESCRIPTION: This Python unit test uses HttpMocker to simulate two pages of responses from the SurveyMonkey surveys API. It verifies that the connector's read operation correctly fetches records from both mocked pages by asserting the total number of records returned.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    @HttpMocker()\n    def test_read_multiple_pages(self, http_mocker: HttpMocker) -> None:\n\n        http_mocker.get(\n            HttpRequest(url=\"https://api.surveymonkey.com/v3/surveys?include=response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats&per_page=1000\"),\n            HttpResponse(body=\"\"\"\n            {\n  \"data\": [\n    {\n      \"id\": \"1234\",\n      \"title\": \"My Survey\",\n      \"nickname\": \"\",\n      \"href\": \"https://api.surveymonkey.com/v3/surveys/1234\"\n    }\n  ],\n  \"per_page\": 50,\n  \"page\": 1,\n  \"total\": 2,\n  \"links\": {\n    \"self\": \"https://api.surveymonkey.com/v3/surveys?page=1&per_page=50\",\n    \"next\": \"https://api.surveymonkey.com/v3/surveys?include=response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats&per_page=1000&page=2\"\n  }\n}\n\"\"\", status_code=200)\n        )\n        http_mocker.get(\n            HttpRequest(url=\"https://api.surveymonkey.com/v3/surveys?include=response_count,date_created,date_modified,language,question_count,analyze_url,preview,collect_stats&per_page=1000&page=2\"),\n            HttpResponse(body=\"\"\"\n            {\n  \"data\": [\n    {\n      \"id\": \"5678\",\n      \"title\": \"My Survey\",\n      \"nickname\": \"\",\n      \"href\": \"https://api.surveymonkey.com/v3/surveys/1234\"\n    }\n  ],\n  \"per_page\": 50,\n  \"page\": 1,\n  \"total\": 2,\n  \"links\": {\n    \"self\": \"https://api.surveymonkey.com/v3/surveys?page=1&per_page=50\"\n  }\n}\n\"\"\", status_code=200)\n        )\n\n        output = self._read(_A_CONFIG, _configured_catalog(\"surveys\", SyncMode.full_refresh))\n\n        assert len(output.records) == 2\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Inventory Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Zoho Inventory connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-inventory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-inventory test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Amazon SQS Connector\nDESCRIPTION: Command to build a Docker image for the Amazon SQS connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nairbyte-ci connectors --name=destination-amazon-sqs build\n```\n\n----------------------------------------\n\nTITLE: Define Retriever Schema in Airbyte Low-Code YAML\nDESCRIPTION: Describes the YAML structure for defining a `retriever` component within a Stream in Airbyte's low-code CDK. It specifies required properties like `requester` and `record_selector`, and optional ones like `paginator` (for handling API pagination) and `stream_slicer` (equivalent to `partition_router` for data partitioning). This component orchestrates how data is fetched by coordinating the requester, record selector, paginator, and partition router/stream slicer.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/yaml-overview.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nretriever:\n  description: Retrieves records by synchronously sending requests to fetch records. The retriever acts as an orchestrator between the requester, the record selector, the paginator, and the partition router.\n  type: object\n  required:\n    - requester\n    - record_selector\n    - requester\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    requester:\n      \"$ref\": \"#/definitions/Requester\"\n    record_selector:\n      \"$ref\": \"#/definitions/HttpSelector\"\n    paginator:\n      \"$ref\": \"#/definitions/Paginator\"\n    stream_slicer:\n      \"$ref\": \"#/definitions/StreamSlicer\"\nPrimaryKey:\n  type: string\n```\n\n----------------------------------------\n\nTITLE: Building the Hugging Face Datasets Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the source-hugging-face-datasets connector for local testing using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hugging-face-datasets/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hugging-face-datasets build\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte CI for Development Using Poetry\nDESCRIPTION: Commands to install Airbyte CI in development mode using Poetry, which is useful for contributors working on the pipelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-ci/connectors/pipelines/\npoetry install\npoetry env activate\ncd ../../\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Commands in Docker\nDESCRIPTION: Commands to run the Firebolt source connector operations in the Docker container, including spec generation, connection checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-firebolt:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-firebolt:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-firebolt:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-firebolt:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Billing Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zoho Billing connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-billing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-billing test\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Airbyte but Keeping Data\nDESCRIPTION: Command to stop all Airbyte containers while preserving your configuration and data for future use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nabctl local uninstall\n```\n\n----------------------------------------\n\nTITLE: Describing Output Schema for Unleash API Features - YAML - Documentation\nDESCRIPTION: This YAML code snippet defines the output schema for feature records returned by the Unleash API connector in Airbyte. It lists the properties expected for each record, including string, boolean, array, and object fields relevant to Unleash features. This schema serves as a reference for developers integrating or analyzing Unleash data, highlighting fields such as 'name', 'description', 'strategies', 'parameters', and more. No code execution is involved; the snippet is a static example for documentation purposes and does not specify data types or sample values beyond type annotations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/unleash.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\\n{\\n    \\\"name\\\": \\\"string\\\",\\n    \\\"description\\\": \\\"string\\\"\\n    \\\"project\\\": \\\"string\\\"\\n    \\\"type\\\": \\\"string\\\"\\n    \\\"enabled\\\": \\\"boolean\\\"\\n    \\\"stale\\\": \\\"boolean\\\"\\n    \\\"strategies\\\": \\\"array\\\"\\n    \\\"strategy\\\": \\\"string\\\"\\n    \\\"parameters\\\": \\\"object\\\"\\n    \\\"impressionData\\\": \\\"boolean\\\"\\n    \\\"variants\\\": \\\"array\\\"\\n}\\n```\n```\n\n----------------------------------------\n\nTITLE: Defining PaginationStrategy Schema in YAML\nDESCRIPTION: Defines the YAML schema for `PaginationStrategy`. It uses `anyOf` to indicate that a valid strategy must conform to one of the referenced schemas: `CursorPagination`, `OffsetIncrement`, or `PageIncrement`. This allows flexibility in choosing the pagination method suitable for the target API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nPaginationStrategy:\n  type: object\n  anyOf:\n    - \"$ref\": \"#/definitions/CursorPagination\"\n    - \"$ref\": \"#/definitions/OffsetIncrement\"\n    - \"$ref\": \"#/definitions/PageIncrement\"\n```\n\n----------------------------------------\n\nTITLE: Building Codefresh Connector Image using Airbyte CI (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a development Docker image for the `source-codefresh` Airbyte connector. It requires `airbyte-ci` to be installed and configured. The command specifically targets the `source-codefresh` connector and creates a local Docker image tagged `source-codefresh:dev`, suitable for local testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-codefresh/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-codefresh build\n```\n\n----------------------------------------\n\nTITLE: Accessing MongoDB Credentials for Airbyte Employees\nDESCRIPTION: Steps for Airbyte employees to access and create the MongoDB credentials file. Employees should access the 'MongoDB Integration Test User' secret on Rippling and create a file with the contents in the specified location.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mongodb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nCreate a file with the contents at `secrets/credentials.json`\n```\n\n----------------------------------------\n\nTITLE: Example Docker Run Command Structure for Airbyte Jobs\nDESCRIPTION: Illustrates the basic structure of a `docker run` command used internally by Airbyte OSS to execute sync tasks. It highlights the use of Docker volumes (`-v` flag) to mount the `airbyte_workspace` volume to `/data` inside the container and a temporary local directory (`/tmp/airbyte_local`) to `/local`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndocker run --rm -i -v airbyte_workspace:/data -v /tmp/airbyte_local:/local -w /data/9/2 --network host ...\n```\n\n----------------------------------------\n\nTITLE: Copying Data Files from Airbyte Server Using Docker - Bash\nDESCRIPTION: Shows how to copy a data file from the Airbyte server Docker container to the host using the 'docker cp' command. No additional dependencies are needed other than Docker itself. Replace '{destination_path}' with the actual relative path in the container; the file will be copied to the current working directory. The command is designed to run in environments where the server and host share a file system via Docker mounts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/sqlite.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path} .\n```\n\n----------------------------------------\n\nTITLE: Querying Adjust Filters Data Endpoint in Shell\nDESCRIPTION: Example curl command to retrieve available event metrics from the Adjust filters data endpoint using an API key for authentication. The response is piped through jq for JSON formatting.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/adjust.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncurl --header 'Authorization: Bearer API_KEY' 'https://dash.adjust.com/control-center/reports-service/filters_data?required_filters=event_metrics' | jq\n```\n\n----------------------------------------\n\nTITLE: Building Qdrant Connector Docker Image Manually\nDESCRIPTION: Alternative command to build the Docker image for the Qdrant connector using the docker build command directly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-qdrant:dev .\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Harvest Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Harvest source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harvest/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-harvest test\n```\n\n----------------------------------------\n\nTITLE: Testing Breezy HR Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Breezy HR source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-breezy-hr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-breezy-hr test\n```\n\n----------------------------------------\n\nTITLE: Running Jina AI Reader Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-jina-ai-reader spec\npoetry run source-jina-ai-reader check --config secrets/config.json\npoetry run source-jina-ai-reader discover --config secrets/config.json\npoetry run source-jina-ai-reader read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Waiteraid Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Waiteraid source connector. It's useful for verifying changes and ensuring compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-waiteraid/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-waiteraid test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the LinkedIn Pages connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-pages/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-linkedin-pages:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linkedin-pages:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linkedin-pages:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-linkedin-pages:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Executing CI Test Suite for Bamboo HR Connector using airbyte-ci in Bash\nDESCRIPTION: This command uses the `airbyte-ci` tool to run the full Continuous Integration (CI) test suite for the `source-bamboo-hr` connector locally. This is essential for verifying connector functionality and quality before contributing changes or publishing a new version. It requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bamboo-hr/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bamboo-hr test\n```\n\n----------------------------------------\n\nTITLE: Running SFTP-Bulk Connector as Docker Container\nDESCRIPTION: Commands to run the connector in a Docker container. These include operations for specification, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sftp-bulk:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sftp-bulk:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sftp-bulk:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sftp-bulk:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Jira Connector\nDESCRIPTION: Command to execute unit tests for the Jira connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Installing Kyve Connector Dependencies with Poetry\nDESCRIPTION: Command to install the Kyve connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Stripe Source with JSON in Terraform\nDESCRIPTION: Creates a Stripe source resource using JSON configuration in Terraform. This approach allows defining the source type directly in the JSON object with parameters like account ID, client secret, and data collection settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_source_custom\" \"my_source_stripe\" {\n    configuration = jsonencode({\n        \"source_type\" : \"stripe\",\n        \"account_id\" : \"YOUR_STRIPE_ACCOUNT_ID\",\n        \"client_secret\" : \"YOUR_STRIPE_CLIENT_SECRET\",\n        \"start_date\" : \"2023-07-01T00:00:00Z\",\n        \"lookback_window_days\" : 0,\n        \"slice_range\" : 365\n        })\n    name = \"Stripe\"\n    workspace_id = var.workspace_id\n}\n```\n\n----------------------------------------\n\nTITLE: Batch JSON Input Example for Multi-row Airbyte Export - JSON\nDESCRIPTION: An example input array containing multiple user objects for batch processing in Airbyte. Each object follows the same schema as the per-record source format. This demonstrates what a source might supply before mapping to JSONL/CSV/Parquet exports. No dependencies; intended as sample input.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\\n  {\\n    \\\"user_id\\\": 123,\\n    \\\"name\\\": {\\n      \\\"first\\\": \\\"John\\\",\\n      \\\"last\\\": \\\"Doe\\\"\\n    }\\n  },\\n  {\\n    \\\"user_id\\\": 456,\\n    \\\"name\\\": {\\n      \\\"first\\\": \\\"Jane\\\",\\n      \\\"last\\\": \\\"Roe\\\"\\n    }\\n  }\\n]\n```\n\n----------------------------------------\n\nTITLE: Oncehub Streams Table in Markdown\nDESCRIPTION: Markdown table listing all available data streams with their properties including primary keys, pagination type, and sync support details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oncehub.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| bookings | id | CursorPagination | ✅ |  ✅  |\n| booking_pages | id | CursorPagination | ✅ |  ❌  |\n| event_types | id | CursorPagination | ✅ |  ❌  |\n| master_pages | id | CursorPagination | ✅ |  ❌  |\n| users | id | CursorPagination | ✅ |  ❌  |\n| teams | id | CursorPagination | ✅ |  ❌  |\n| contacts | id | CursorPagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Defining Page Size Constant in Python\nDESCRIPTION: This Python code defines a constant `_PAGE_SIZE` with a value of 1000. This constant represents the number of records to request per page from the SurveyMonkey API, used within the `request_params` method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a pagination constant\n_PAGE_SIZE: int = 1000\n```\n\n----------------------------------------\n\nTITLE: Creating a Logical Replication Slot in Postgres\nDESCRIPTION: SQL command to create a logical replication slot in Postgres using the pgoutput plugin. This slot is required for CDC replication and should be used exclusively by Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');\n```\n\n----------------------------------------\n\nTITLE: Running Standard Connector Commands via Docker for Persistiq\nDESCRIPTION: These commands demonstrate executing standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-persistiq:dev` Docker image. The `spec` command retrieves the connector's specification. The `check`, `discover`, and `read` commands require mounting a local `secrets` directory (containing `config.json`) to `/secrets` inside the container. The `read` command additionally requires mounting an `integration_tests` directory (containing `configured_catalog.json`) for reading data according to a specified catalog. The `--rm` flag ensures container cleanup after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-persistiq/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-persistiq:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-persistiq:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-persistiq:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-persistiq:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Primetric Source Connector in Bash\nDESCRIPTION: Set of Docker commands to run standard source connector operations including viewing specifications, checking configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-primetric/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-primetric:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-primetric:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-primetric:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-primetric:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Algolia Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Algolia source connector using airbyte-ci. Creates a dev image tagged as source-algolia:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-algolia/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-algolia build\n```\n\n----------------------------------------\n\nTITLE: Building Pipeliner Connector Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image tagged as `source-pipeliner:dev`. This image allows for local testing and development of the connector. Requires the `airbyte-ci` tool to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipeliner/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pipeliner build\n```\n\n----------------------------------------\n\nTITLE: Apptivo Connector Configuration Schema\nDESCRIPTION: Configuration parameters required for setting up the Apptivo connector, including API key and access key specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/apptivo.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it in your Apptivo account under Business Settings -&gt; API Access. |  |\n| `access_key` | `string` | Access Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests with Gradle\nDESCRIPTION: Command to execute acceptance tests for the MSSQL connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-mssql-v2:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Testing Help Scout Source Connector in Bash\nDESCRIPTION: This command runs the acceptance tests for the Help Scout source connector using airbyte-ci. It verifies the connector's functionality and compliance with Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-help-scout/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-help-scout test\n```\n\n----------------------------------------\n\nTITLE: Defining Global State in Airbyte JSON Format\nDESCRIPTION: This JSON object represents a STATE message in Airbyte's protocol. It defines a global state with a shared start date, which is typically used for incremental syncs or checkpointing in data pipelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/namespace_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\":  {\"start_date\": \"2022-08-17\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Using dictsort Filter in Jinja2\nDESCRIPTION: Demonstrates the `dictsort` filter in Jinja2, which sorts a dictionary by key (or value, or item) and returns a list of (key, value) tuples. The example sorts the dictionary `{'b': 1, 'a': 2}` by key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_20\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ {'b': 1, 'a': 2}|dictsort }}\n```\n\n----------------------------------------\n\nTITLE: Pulling, Tagging, and Pushing a Connector Image - Bash\nDESCRIPTION: This sequence of Bash commands demonstrates pulling an official Airbyte connector image (destination-google-sheets) from Docker Hub, tagging it for a custom registry (ghcr.io/NAMESPACE), and pushing the re-tagged image to the custom registry. The user must have permission to pull from Docker Hub and push to the registry, and replace NAMESPACE appropriately. Each command should be run sequentially for successful transfer.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull airbyte/destination-google-sheets:latest\ndocker tag airbyte/desination-google-sheets:latest ghcr.io/NAMESPACE/desination-google-sheets:latest\ndocker push ghcr.io/NAMESPACE/destination-google-sheets:latest    \n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands for Serpstat\nDESCRIPTION: Commands to run the standard operations for the Serpstat source connector as a Docker container, including spec, check, discover, and read operations. These commands use the locally built Docker image and mount local directories as needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-serpstat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-serpstat:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-serpstat:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-serpstat:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-serpstat:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining JSONL File Metadata Schema for Airbyte - JSON\nDESCRIPTION: This JSON snippet specifies the structure of a single record in an Airbyte-exported JSON Lines (JSONL) file. It includes metadata fields _airbyte_ab_id and _airbyte_emitted_at, plus _airbyte_data which contains the source data. This defines the schema used for each line in JSONL exports. All parameters are required; actual values are populated at export time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"_airbyte_ab_id\\\": \\\"<uuid>\\\",\\n  \\\"_airbyte_emitted_at\\\": \\\"<timestamp-in-millis>\\\",\\n  \\\"_airbyte_data\\\": \\\"<json-data-from-source>\\\"\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Production Build and Serving - pnpm/Docusaurus - Bash\nDESCRIPTION: This snippet creates a static production build of the documentation site and serves it locally using pnpm. Changes will not auto-update during preview. Results are served at http://localhost:3000/. The running server can be stopped with Ctrl+C.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npnpm build\\npnpm serve\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Incident.io Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Incident.io source connector using the airbyte-ci tool. It verifies that the connector meets Airbyte's compatibility requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-incident-io/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-incident test\n```\n\n----------------------------------------\n\nTITLE: Generating RSA Private Key for SSH Tunneling (Text)\nDESCRIPTION: Provides the command-line instruction using `ssh-keygen` to generate an RSA private key (`myuser_rsa`) in PEM format. This key is required when configuring Airbyte to connect to SingleStore via an SSH tunnel using key authentication. The corresponding public key needs to be added to the bastion host's `authorized_keys`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/singlestore.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Running SQLite Connector in Docker Container\nDESCRIPTION: Commands for running various connector operations in a Docker container including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-sqlite:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-sqlite:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-sqlite:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-sqlite:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Ringcentral connector Docker commands\nDESCRIPTION: Standard Docker commands for running the Ringcentral source connector container with various operations including spec, check, discover, and read. These commands mount local directories to access configuration files and write output.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ringcentral/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-ringcentral:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ringcentral:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ringcentral:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-ringcentral:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte Stream Records with Different Naming Conventions\nDESCRIPTION: Example JSON records demonstrating various stream naming patterns including camelCase, snake_case, special characters, and different field name formats. Records contain stream data with timestamps and various data fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/edge_case_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"streamWithCamelCase\", \"emitted_at\": 1602637589000, \"data\": { \"data\" : \"one\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_with_underscores\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"one\" }}}\n```\n\n----------------------------------------\n\nTITLE: Comparing Old and New Primary Keys for LinkedIn Ads v5.0.0 Analytics Streams (Markdown)\nDESCRIPTION: Displays the old and new primary key structures for the `ad_campaign_analytics` and `Custom Ad Analytics Reports` streams following the v5.0.0 update. The new primary key adds `sponsoredCampaign` to improve uniqueness and data integrity, requiring a data refresh.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Old PK                               | New PK                                                  | \n|:-------------------------------------|:--------------------------------------------------------|\n| `[string_of_pivot_values, end_date]` | `[string_of_pivot_values, end_date, sponsoredCampaign]` | \n```\n\n----------------------------------------\n\nTITLE: Using last Filter in Jinja2\nDESCRIPTION: Demonstrates the `last` filter in Jinja2, which returns the last item from a sequence (like a list). The example retrieves the last item from `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_31\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|last }}\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Source High Level Connector\nDESCRIPTION: This command executes the acceptance tests for the source-high-level connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-high-level/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-high-level test\n```\n\n----------------------------------------\n\nTITLE: Sample Credentials Output\nDESCRIPTION: Example output from the credentials command showing the login information needed to access Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nCredentials:\nEmail: user@example.com\nPassword: random_password\nClient-Id: 03ef466c-5558-4ca5-856b-4960ba7c161b\nClient-Secret: m2UjnDO4iyBQ3IsRiy5GG3LaZWP6xs9I\n```\n\n----------------------------------------\n\nTITLE: Defining Pages Stream Schema in JSON\nDESCRIPTION: JSON schema for the pages stream in the Google Analytics connector. It includes metrics related to specific pages, such as pageviews, entrances, and exit rates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_hostname\":\"mydemo.com\",\"ga_pagePath\":\"/home5\",\"ga_pageviews\":63,\"ga_uniquePageviews\":9,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_entrances\":9,\"ga_entranceRate\":14.285714285714285,\"ga_bounceRate\":0.0,\"ga_exits\":9,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Bluetally Data Streams in Markdown\nDESCRIPTION: Stream configuration table showing available data streams (assets and employees) with their properties including primary keys, pagination type, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bluetally.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| assets | id | DefaultPaginator | ✅ |  ✅  |\n| employees | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Implementing Pattern Descriptors in Airbyte Connector Spec\nDESCRIPTION: Example of using pattern descriptors to provide a user-friendly format hint for date fields. This shows how to set the pattern_descriptor property alongside a regex pattern to improve validation error messages.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n\"start_date\": {\n  \"type\": \"string\",\n  \"title\": \"Start date\",\n  \"pattern\": \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\",\n  \"pattern_descriptor\": \"YYYY-MM-DD\"\n},\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte source with state, catalog, and config in Bash\nDESCRIPTION: Provides a Bash command to run the custom SurveyMonkey Airbyte source in incremental mode using poetry, specifying the config file, catalog, and an input state for resuming extraction. Requires poetry, source-survey-monkey-demo, and necessary test configuration files in the specified paths. Outputs only records modified since the last known state.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json --state integration_tests/sample_state.json\n```\n\n----------------------------------------\n\nTITLE: Using batch Filter in Jinja2\nDESCRIPTION: Demonstrates the `batch` filter in Jinja2, which groups items from a sequence into batches of a specified size. The example batches the list `[1, 2, 3, 4, 5, 6]` into sub-lists of size 2.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_16\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3, 4, 5, 6]|batch(2) }}\n```\n\n----------------------------------------\n\nTITLE: Importing datetime for timestamp conversions in Python\nDESCRIPTION: Imports Python's datetime library, necessary for handling and converting date and time values throughout the incremental reading process. Used by methods for timestamp calculations and formatting in the stream implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# import the following library\nimport datetime\n```\n\n----------------------------------------\n\nTITLE: Granting Permissions to Airbyte User in Redshift (SQL)\nDESCRIPTION: SQL commands to grant necessary permissions to the airbyte_user in Redshift. This includes creating schemas, granting usage on existing schemas, and providing select access to the SVV_TABLE_INFO table.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redshift.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGRANT CREATE ON DATABASE database_name TO airbyte_user; -- add create schema permission\nGRANT usage, create on schema my_schema TO airbyte_user; -- add create table permission\nGRANT SELECT ON TABLE SVV_TABLE_INFO TO airbyte_user; -- add select permission for svv_table_info\n```\n\n----------------------------------------\n\nTITLE: Failed Sync Webhook Payload Example in JSON\nDESCRIPTION: Example of the JSON payload structure sent by Airbyte when a sync operation fails. Includes workspace, connection, source, destination details along with error information and sync statistics.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/manage-airbyte-cloud-notifications.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"workspace\": {\n            \"id\":\"b510e39b-e9e2-4833-9a3a-963e51d35fb4\",\n            \"name\":\"Workspace1\",\n            \"url\":\"https://link/to/ws\"\n        },\n        \"connection\":{\n            \"id\":\"64d901a1-2520-4d91-93c8-9df438668ff0\",\n            \"name\":\"Connection\",\n            \"url\":\"https://link/to/connection\"\n        },\n        \"source\":{\n            \"id\":\"c0655b08-1511-4e72-b7da-24c5d54de532\",\n            \"name\":\"Source\",\n            \"url\":\"https://link/to/source\"\n        },\n        \"destination\":{\n            \"id\":\"5621c38f-8048-4abb-85ca-b34ff8d9a298\",\n            \"name\":\"Destination\",\n            \"url\":\"https://link/to/destination\"\n        },\n        \"jobId\":9988,\n        \"startedAt\":\"2024-01-01T00:00:00Z\",\n        \"finishedAt\":\"2024-01-01T01:00:00Z\",\n        \"bytesEmitted\":1000,\n        \"bytesCommitted\":90,\n        \"recordsEmitted\":89,\n        \"recordsCommitted\":45,\n        \"errorMessage\":\"Something failed\",\n        \"errorType\": \"config_error\",\n        \"errorOrigin\": \"source\",\n        \"bytesEmittedFormatted\": \"1000 B\",\n        \"bytesCommittedFormatted\":\"90 B\",\n        \"success\":false,\n        \"durationInSeconds\":3600,\n        \"durationFormatted\":\"1 hours 0 min\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using capitalize Filter in Jinja2\nDESCRIPTION: Demonstrates the `capitalize` filter in Jinja2, which capitalizes the first character of a string and converts the rest to lowercase. The example applies it to the string 'hello'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_17\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello'|capitalize }}\n```\n\n----------------------------------------\n\nTITLE: Defining Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema for the weekly_active_users stream in the Google Analytics connector. It includes the count of 7-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_7dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing Airbyte Images to Custom Registry\nDESCRIPTION: Bash command to tag all Airbyte platform images and push them to a custom GitHub registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nabctl images manifest | xargs -L1 -I{} docker tag {} ghcr.io/NAMESPACE/{} && docker push ghcr.io/NAMESPACE/{}\n```\n\n----------------------------------------\n\nTITLE: Running Shortio Connector Docker Commands\nDESCRIPTION: Standard commands for running the Shortio source connector Docker container, including spec, check, discover, and read operations. These commands demonstrate how to use the connector with configuration files and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shortio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-shortio:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-shortio:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-shortio:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-shortio:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite with airbyte-ci\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci, which runs acceptance tests according to the configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-python-http-tutorial test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, and data writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with Airbyte-CI - Bash\nDESCRIPTION: Invokes the Airbyte-CI testing pipeline to execute the complete set of tests for the Azure-Table connector. Requires 'airbyte-ci' to be installed and configured. Outputs diagnostics from the test suite to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-azure-table test\n```\n\n----------------------------------------\n\nTITLE: Configuring Registries Section in metadata.yaml (YAML)\nDESCRIPTION: This snippet illustrates the structure of the `registries` section within `metadata.yaml`. It shows how to configure the `cloud` and `oss` registries, including enabling/disabling them and overriding the default Docker repository for a specific registry (e.g., using `airbyte/source-postgres-strict-encrypt` for the cloud registry).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nregistries:\n  cloud:\n    dockerRepository: airbyte/source-postgres-strict-encrypt\n    enabled: true\n  oss:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Describing Supported Streams Using Markdown Table\nDESCRIPTION: This snippet presents a Markdown table enumerating all supported data streams in the Linear Airbyte connector. Fields such as Stream Name, Primary Key, Pagination mechanism, and synchronization capabilities provide users with a summary of extractable entities and their sync methods. The streams support only full sync and use DefaultPaginator, while incremental sync is not available. This table is valuable for planning data extraction from Linear into Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linear.md#2025-04-23_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\\n|-------------|-------------|------------|---------------------|----------------------|\\n| teams | id | DefaultPaginator | ✅ |  ❌  |\\n| users | id | DefaultPaginator | ✅ |  ❌  |\\n| cycles | id | DefaultPaginator | ✅ |  ❌  |\\n| issues | id | DefaultPaginator | ✅ |  ❌  |\\n| comments | id | DefaultPaginator | ✅ |  ❌  |\\n| projects | id | DefaultPaginator | ✅ |  ❌  |\\n| customers | id | DefaultPaginator | ✅ |  ❌  |\\n| attachments | id | DefaultPaginator | ✅ |  ❌  |\\n| issue_labels | id | DefaultPaginator | ✅ |  ❌  |\\n| customer_needs | id | DefaultPaginator | ✅ |  ❌  |\\n| customer_tiers | id | DefaultPaginator | ✅ |  ❌  |\\n| issue_relations | id | DefaultPaginator | ✅ |  ❌  |\\n| workflow_states | id | DefaultPaginator | ✅ |  ❌  |\\n| project_statuses | id | DefaultPaginator | ✅ |  ❌  |\\n| customer_statuses | id | DefaultPaginator | ✅ |  ❌  |\\n| project_milestones | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: E-commerce Database Schema for Sample Data (Faker) Source in SQL\nDESCRIPTION: SQL schema definitions for the tables generated by the Faker source. The schema includes users, users_address, products, and purchases tables that simulate an e-commerce dataset with appropriate data types and relationships.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/faker.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE \"public\".\"users\" (\n    \"address\" jsonb,\n    \"occupation\" text,\n    \"gender\" text,\n    \"academic_degree\" text,\n    \"weight\" int8,\n    \"created_at\" timestamptz,\n    \"language\" text,\n    \"telephone\" text,\n    \"title\" text,\n    \"updated_at\" timestamptz,\n    \"nationality\" text,\n    \"blood_type\" text,\n    \"name\" text,\n    \"id\" float8,\n    \"age\" int8,\n    \"email\" text,\n    \"height\" text,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_users_hashid\" text\n);\n\nCREATE TABLE \"public\".\"users_address\" (\n    \"_airbyte_users_hashid\" text,\n    \"country_code\" text,\n    \"province\" text,\n    \"city\" text,\n    \"street_number\" text,\n    \"state\" text,\n    \"postal_code\" text,\n    \"street_name\" text,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_address_hashid\" text\n);\n\nCREATE TABLE \"public\".\"products\" (\n    \"id\" float8,\n    \"make\" text,\n    \"year\" float8,\n    \"model\" text,\n    \"price\" float8,\n    \"created_at\" timestamptz,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_dev_products_hashid\" text,\n);\n\nCREATE TABLE \"public\".\"purchases\" (\n    \"id\" float8,\n    \"user_id\" float8,\n    \"product_id\" float8,\n    \"purchased_at\" timestamptz,\n    \"added_to_cart_at\" timestamptz,\n    \"returned_at\" timestamptz,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_dev_purchases_hashid\" text,\n);\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands for running the source connector in different modes including spec, check, discover, and read operations. Each command demonstrates how to mount volumes and pass configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tyntec-sms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tyntec-sms:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tyntec-sms:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tyntec-sms:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tyntec-sms:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Upgrading MSSQL Source with CDC Replication Method\nDESCRIPTION: SQL query to update Microsoft SQL Source configurations in the Airbyte database for connections using the Logical Replication (CDC) method when upgrading from connector version 0.4.17 to 0.4.18+.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"CDC\"}', true)\nWHERE actor_definition_id ='b5ea17b1-f170-46dc-bc31-cc744ca984c1' AND (configuration->>'replication_method' = 'CDC');\n```\n\n----------------------------------------\n\nTITLE: Schema for AddFields Transformation - YAML\nDESCRIPTION: YAML schema for specifying AddFields, defining how new fields can be appended to records. The 'fields' key is an array of field definitions, where each must provide a 'path' (defining where to insert the new field) and a 'value'. This schema is referenced in Airbyte transformations for record enrichment. Each field value is typically static or can be computed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nAddFields:\n  type: object\n  required:\n    - fields\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    fields:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/AddedFieldDefinition\"\nAddedFieldDefinition:\n  type: object\n  required:\n    - path\n    - value\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    path:\n      \"$ref\": \"#/definitions/FieldPointer\"\n    value:\n      type: string\nFieldPointer:\n  type: array\n  items:\n    type: string\n```\n\n----------------------------------------\n\nTITLE: Building the Source-Float Connector using airbyte-ci\nDESCRIPTION: Command to build a dev image (source-float:dev) that can be used for local testing of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-float/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-float build\n```\n\n----------------------------------------\n\nTITLE: Generating Dynamic Table of Contents from YAML Schema - JavaScript\nDESCRIPTION: This snippet builds a dynamic table of contents (TOC) structure using both static entries and data extracted from the imported YAML schema. It uses object mapping to iterate through definitions, variables, macros, and filters, constructing TOC entries for each. Dependencies include the React environment and an imported YAML schema. Inputs are schema properties (definitions, interpolation.variables, interpolation.macros, interpolation.filters) and outputs is the constructed 'toc' array for documentation navigation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/reference.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nexport const toc = [\n{\n\"value\": \"Components:\",\n\"id\": \"components\",\n\"level\": 2\n},\n{\nvalue: \"DeclarativeSource\",\nid: \"/definitions/DeclarativeSource\",\nlevel: 3\n},\n...Object.keys(schema.definitions).map((id) => ({\nvalue: id,\nid: `/definitions/${id}`,\nlevel: 3\n})),\n{\n\"value\": \"Interpolation variables:\",\n\"id\": \"variables\",\n\"level\": 2\n},\n...schema.interpolation.variables.map((def) => ({\nvalue: def.title,\nid: `/variables/${def.title}`,\nlevel: 3\n})),\n{\n\"value\": \"Interpolation macros:\",\n\"id\": \"macros\",\n\"level\": 2\n},\n...schema.interpolation.macros.map((def) => ({\nvalue: def.title,\nid: `/macros/${def.title}`,\nlevel: 3\n})),\n{\n\"value\": \"Interpolation filters:\",\n\"id\": \"filters\",\n\"level\": 2\n},\n...schema.interpolation.filters.map((def) => ({\nvalue: def.title,\nid: `/filters/${def.title}`,\nlevel: 3\n}))\n];\n```\n\n----------------------------------------\n\nTITLE: Running the discover command for Outreach Connector via Docker\nDESCRIPTION: Executes the `discover` command in a temporary Docker container to find the schema of available data streams. It requires mounting the `secrets` directory with the `config.json` file and passing the configuration file path. Requires a valid `secrets/config.json`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-outreach:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Implementing Airbyte Connection Task in Prefect\nDESCRIPTION: Python script that creates a Prefect Flow to trigger an Airbyte sync job. Configures the AirbyteConnectionTask with server connection details and registers the flow with the Prefect project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-prefect-task.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import Flow\nfrom prefect.tasks.airbyte.airbyte import AirbyteConnectionTask\n\nairbyte_conn = AirbyteConnectionTask(\n        airbyte_server_host=\"localhost\",\n        airbyte_server_port=8000,\n        airbyte_api_version=\"v1\",\n        connection_id=\"04e128af-1092-4a83-bf33-1b8c85395d74\"\n)\n\nwith Flow(\"first-airbyte-task\") as flow:\n    flow.add_task(airbyte_conn)\n\n# Register the flow under the \"airbyte\" project\nflow.register(project_name=\"airbyte\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Docker Registry in Helm for Airbyte\nDESCRIPTION: This YAML configuration snippet demonstrates how to specify a custom Docker registry for Airbyte deployments using Helm. By setting the `global.image.registry` value, all platform Docker images will be pulled from the specified registry (e.g., `my-registry.foo.com/`). This is useful for environments requiring images from internal or approved repositories. The registry value must include a trailing slash.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.2.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  image:\n    registry: my-registry.foo.com/\n```\n\n----------------------------------------\n\nTITLE: Requesting Access Token Body for Self-Managed Airbyte (YAML)\nDESCRIPTION: Defines the required JSON payload structure for requesting an access token from a Self-Managed Airbyte API. The `client_id` and `client_secret` values must be populated with the credentials generated when creating the application in the Airbyte UI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n{ \n  \"client_id\": \"\", \n  \"client_secret\": \"\" \n  }\n```\n\n----------------------------------------\n\nTITLE: Composite Error Handler Configuration with Multiple Backoff Strategies in YAML\nDESCRIPTION: Example of a CompositeErrorHandler that retries differently based on error properties: applies constant backoff when response contains a 'code' field, and exponential backoff for HTTP 403 errors. Demonstrates advanced, contextual retry configuration in Airbyte YAML integration setups.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    type: \"CompositeErrorHandler\"\n    error_handlers:\n      - response_filters:\n          - predicate: \"{{ 'code' in response }}\"\n            action: RETRY\n        backoff_strategies:\n          - type: \"ConstantBackoffStrategy\"\n            backoff_time_in_seconds: 5\n      - response_filters:\n          - http_codes: [ 403 ]\n            action: RETRY\n        backoff_strategies:\n          - type: \"ExponentialBackoffStrategy\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Airbyte Connection in Terraform\nDESCRIPTION: Establishes a connection between a source and destination in Airbyte using Terraform. This code creates a connection named 'Stripe to BigQuery' that references the previously defined source and destination resources by their IDs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_8\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_connection\" \"stripe_to_bigquery\" {\n    name           = \"Stripe to BigQuery\"\n    source_id      = airbyte_source_stripe.my_source_stripe.source_id\n    destination_id = airbyte_destination_bigquery.my_destination_bigquery.destination_id\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Default S3 Output Path Structure\nDESCRIPTION: Shows the general format of the output path generated by the Airbyte S3 destination connector when using the default S3 Path Format `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_`. It uses placeholders to represent components like bucket name, namespace, stream name, upload date, epoch timestamp, partition ID, and format extension.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n<bucket-name>/<source-namespace-if-exists>/<stream-name>/<upload-date>_<epoch>_<partition-id>.<format-extension>\n```\n\n----------------------------------------\n\nTITLE: Starting and Enabling Docker Service - Shell\nDESCRIPTION: Starts the Docker service immediately and enables it to start automatically on system boot. Two commands are chained for sequential execution; both must succeed for Docker to be available after reboots. Requires administrative privileges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl start docker\nsudo systemctl enable docker\n```\n\n----------------------------------------\n\nTITLE: Creating a Postgres Publication for CDC Tables (SQL)\nDESCRIPTION: Creates a Postgres logical replication publication named `airbyte_publication`. This publication groups tables (`tbl1`, `tbl2`, `tbl3` are placeholders) whose data changes (inserts, updates, deletes) should be published for Change Data Capture (CDC). The publication name is customizable and is required by Airbyte's CDC replication method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE PUBLICATION airbyte_publication FOR TABLE <tbl1, tbl2, tbl3>;`\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema Refresh Interval in Airbyte (YAML, Airbyte)\nDESCRIPTION: This snippet shows how to configure Airbyte's schema discovery refresh interval by setting the `DISCOVER_REFRESH_WINDOW_MINUTES` environment variable in `values.yaml`. The default is 1440 minutes (24 hours), ensuring schemas are refreshed once daily, which can help balance performance with change detection. Requires editing Airbyte's Helm chart or deployment values and applies to self-managed Airbyte instances.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.4.md#2025-04-23_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nworker:\n  env_vars:\n    DISCOVER_REFRESH_WINDOW_MINUTES: 1440 # Airbyte automatically refreshes schemas no more than once per day (1440 minutes).\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands with Docker - Bash\nDESCRIPTION: These snippets provide various Docker commands to execute the Airbyte DynamoDB connector's 'spec', 'check', 'discover', and 'read' operations using the locally built Docker image. They mount necessary config and secrets files from the host system and execute each connector phase as a Docker container. To use these commands, Docker must be installed and the image 'airbyte/source-dynamodb:dev' already built. The secrets and config files must exist at the specified locations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-dynamodb:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dynamodb:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dynamodb:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-dynamodb:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Decoding JSON Lines API Response to Normalized JSON Array (Airbyte, JSON Lines)\nDESCRIPTION: Presents a response with each line containing a separate JSON object, as is customary with JSON Lines. Airbyte combines all objects into a normalized JSON array. Requires handling newline-separated objects, with expected output being a JSON list of record objects for consistent downstream processing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n{\"name\": \"Alice\", \"age\": 25, \"city\": \"Los Angeles\"}\n{\"name\": \"Bob\", \"age\": 50, \"city\": \"Las Vegas\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n  },\n  {\n    \"name\": \"Alice\",\n    \"age\": 25,\n    \"city\": \"Los Angeles\"\n  },\n  {\n    \"name\": \"Bob\",\n    \"age\": 50,\n    \"city\": \"Las Vegas\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Testing the SavvyCal Connector in Bash\nDESCRIPTION: Command for running the acceptance tests for the SavvyCal connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-savvycal/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-savvycal test\n```\n\n----------------------------------------\n\nTITLE: Granting read permissions to PostgreSQL user for Airbyte replication\nDESCRIPTION: SQL commands to grant the necessary read permissions to the Airbyte user for accessing schema and tables in PostgreSQL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT USAGE ON SCHEMA <schema_name> TO <user_name>;\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>;\nALTER DEFAULT PRIVILEGES IN SCHEMA <schema_name> GRANT SELECT ON TABLES TO <user_name>;\n```\n\n----------------------------------------\n\nTITLE: Running Salesforce Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Salesforce connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesforce test\n```\n\n----------------------------------------\n\nTITLE: Defining Dummy Fields Schema in JSON\nDESCRIPTION: JSON schema for dummy fields, consisting of five fields with repetitive values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hardcoded-records.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"field1\": \"valuevaluevaluevaluevalue1\",\n  \"field2\": \"valuevaluevaluevaluevalue1\",\n  \"field3\": \"valuevaluevaluevaluevalue1\",\n  \"field4\": \"valuevaluevaluevaluevalue1\",\n  \"field5\": \"valuevaluevaluevaluevalue1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Airbyte Record Structure in JSON\nDESCRIPTION: This snippet illustrates the structure of an Airbyte record in JSON format. It includes the record type, stream name, emission timestamp, and the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/integration_tests/test_data/test_destination_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_1\", \"emitted_at\": 1602637589000, \"data\": {\"id\" : \"1\"}}}\n```\n\n----------------------------------------\n\nTITLE: Generating RSA Keys for Airbyte Encryption in Bash\nDESCRIPTION: This script generates the RSA encryption keys in the required hex-encoded DER format that Airbyte expects for the encryption mapping feature. It creates a 2048-bit RSA key pair and converts the public key to the proper format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/mappings.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nopenssl genpkey -algorithm RSA -out private_key.pem -pkeyopt rsa_keygen_bits:2048\nopenssl rsa -pubout -outform DER -in private_key.pem -out public_key.der\nxxd -p public_key.der | tr -d '\\n'\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Vercel Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Vercel source connector using airbyte-ci. It helps ensure the connector's functionality meets the required standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vercel/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vercel test\n```\n\n----------------------------------------\n\nTITLE: Testing the Shipstation Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Shipstation connector, validating its functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shipstation/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shipstation test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec generation, configuration checking, discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-cart spec\npoetry run source-cart check --config secrets/config.json\npoetry run source-cart discover --config secrets/config.json\npoetry run source-cart read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Deepset Connector\nDESCRIPTION: Command to run the full CI test suite for the Deepset connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-deepset test\n```\n\n----------------------------------------\n\nTITLE: Installing Tiktok-Marketing Connector Dependencies with Poetry\nDESCRIPTION: This command installs the necessary dependencies for the Tiktok-Marketing connector using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle JDBC Property for LONG/LONG RAW Columns\nDESCRIPTION: Specifies the `useFetchSizeWithLongColumn=true` JDBC connection property required by the Oracle Thin driver (like `ojdbc8` used by Airbyte) to fetch data from `LONG` or `LONG RAW` columns. This property is essential for compatibility with these legacy data types, although Oracle generally recommends using LOB types instead. Note that this property is specific to the Oracle Thin driver and should not be used with other driver types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oracle/BOOTSTRAP.md#2025-04-23_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nuseFetchSizeWithLongColumn=true\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Appfollow connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appfollow/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appfollow test\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite with Airbyte CI (bash)\nDESCRIPTION: This bash snippet shows how to execute the full suite of CI tests for the source-clickup-api connector using airbyte-ci. The command requires airbyte-ci to be installed and will validate the connector by running its associated test suite locally, helping contributors verify correctness before publishing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clickup-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clickup-api test\n```\n\n----------------------------------------\n\nTITLE: Executing MSSQL Connector Docker Commands\nDESCRIPTION: Set of Docker commands for running the MSSQL connector image locally with various operations including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm airbyte/destination-mssql-v2:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-mssql-v2:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-mssql-v2:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-mssql-v2:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Default SQS Message Body Output\nDESCRIPTION: Demonstrates the default message body structure when no message_body_key is specified.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/bootstrap.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"parent_key\": {\n        \"nested_key\": \"nested_value\"\n    },\n    \"top_key\": \"top_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Segment Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Segment source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-segment/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-segment test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Leadfeeder Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Leadfeeder source connector using airbyte-ci. It verifies the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-leadfeeder/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-leadfeeder test\n```\n\n----------------------------------------\n\nTITLE: Displaying Avni Connector Version History\nDESCRIPTION: Markdown table showing version history, release dates, and associated pull requests for the Avni connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/avni.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n| 0.1.1 | 2025-02-26 | [54696](https://github.com/airbytehq/airbyte/pull/54696) | Update requests-mock version |\n| 0.1.0 | 2023-09-07 | [30222](https://github.com/airbytehq/airbyte/pull/30222) | Avni Source Connector |\n```\n\n----------------------------------------\n\nTITLE: Building GreytHR Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the GreytHR source connector using airbyte-ci. The resulting image (source-greythr:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greythr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-greythr build\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite using airbyte-ci (Bash)\nDESCRIPTION: Executes the complete CI test suite (including unit tests, integration tests, and acceptance tests) for the `source-google-sheets` connector locally using the `airbyte-ci` tool. This command replicates the checks performed in the CI environment. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-sheets test\n```\n\n----------------------------------------\n\nTITLE: Running Python Connector Tests with Pytest - Bash\nDESCRIPTION: This bash command is used within a Python connector directory to run all pytest-based unit and integration tests using the poetry environment manager. It assumes both poetry and pytest are pre-installed and configured in the connector's environment. The command must be executed from within the relevant connector's folder, and it will auto-discover and execute all test cases conforming to pytest conventions. Outputs include detailed pass/fail logs for all tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# From connector directory\\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Defining Google Tasks Streams in Markdown\nDESCRIPTION: Markdown table outlining the available streams for the Google Tasks connector, including their primary keys, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-tasks.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| tasks | id | DefaultPaginator | ✅ |  ✅  |\n| lists_tasks | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running airbyte-ci Test Suite for Connector (Bash)\nDESCRIPTION: Executes the airbyte-ci test suite for the Clockify source connector to ensure it passes all integration and unit tests. Assumes airbyte-ci is installed and configured as per Airbyte's guidelines. The --name flag specifies the connector; test runs all relevant test cases. Input: None. Output: Test result summaries and possible logs. Limitations: airbyte-ci configuration must be correct and dependencies satisfied.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clockify/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clockify test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for The Guardian API Source Connector\nDESCRIPTION: Command to run the full test suite for the source-the-guardian-api connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-the-guardian-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-the-guardian-api test\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kubernetes Ingress for Airbyte with Amazon ALB (YAML)\nDESCRIPTION: This YAML manifest creates an Ingress resource configured for Amazon Application Load Balancer (ALB), forwarding requests to Airbyte's services. It uses ALB-specific annotations for load balancer class, SSL redirection, internal/external visibility, ACM certificate ARN, and optional subnet/security group assignment. The manifest references webapp and auth services with customizable ports and paths. Requires installation of AWS Load Balancer Controller and proper IAM permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: # ingress name, e.g. enterprise-demo\n  annotations:\n    # Specifies that the Ingress should use an AWS ALB.\n    kubernetes.io/ingress.class: \"alb\"\n    # Redirects HTTP traffic to HTTPS.\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    # Creates an internal ALB, which is only accessible within your VPC or through a VPN.\n    alb.ingress.kubernetes.io/scheme: internal\n    # Specifies the ARN of the SSL certificate managed by AWS ACM, essential for HTTPS.\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-x:xxxxxxxxx:certificate/xxxxxxxxx-xxxxx-xxxx-xxxx-xxxxxxxxxxx\n    # Sets the idle timeout value for the ALB.\n    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=30\n    # [If Applicable] Specifies the VPC subnets and security groups for the ALB\n    # alb.ingress.kubernetes.io/subnets: '' e.g. 'subnet-12345, subnet-67890'\n    # alb.ingress.kubernetes.io/security-groups: <SECURITY_GROUP>\nspec:\n  rules:\n    - host: # e.g. enterprise-demo.airbyte.com\n      http:\n        paths:\n          - backend:\n              service:\n                name: airbyte-enterprise-airbyte-webapp-svc\n                port:\n                  number: 80\n            path: /\n            pathType: Prefix\n          - backend:\n              service:\n                name: airbyte-enterprise-airbyte-keycloak-svc\n                port:\n                  number: 8180\n            path: /auth\n            pathType: Prefix\n\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A markdown table documenting version history with dates, pull request references and descriptions of changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mixpanel.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version    | Date       | Pull Request                                             | Subject                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|:-----------|:-----------|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 3.6.0-rc.2 | 2025-04-17 | [58116](https://github.com/airbytehq/airbyte/pull/58116) | Update backoff strategy                                                                                                                                                                                                                                                                                                                                                                                                            |\n| 3.6.0-rc.1 | 2025-04-14 | [55189](https://github.com/airbytehq/airbyte/pull/55189) | Update airbyte-cdk, set up concurrency                                                                                                                                                                                                                                                                                                                                                                                             |\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: Version history table showing updates and changes to the connector, including version numbers, dates, and associated pull requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zapsign.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.4 | 2025-04-19 | [58552](https://github.com/airbytehq/airbyte/pull/58552) | Update dependencies |\n| 0.0.3 | 2025-04-13 | [58037](https://github.com/airbytehq/airbyte/pull/58037) | Update dependencies |\n| 0.0.2 | 2025-04-05 | [57383](https://github.com/airbytehq/airbyte/pull/57383) | Update dependencies |\n| 0.0.1 | 2025-04-04 | [57008](https://github.com/airbytehq/airbyte/pull/57008) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image via airbyte-ci - Bash\nDESCRIPTION: Runs the custom Airbyte 'airbyte-ci' CLI tool to build the Google Analytics V4 source connector image for local development. This approach automates the image build process using Airbyte's standardized build infrastructure, with the resulting Docker image placed in the local registry. The command requires airbyte-ci to be installed and the working directory to be properly set up.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-v4-service-account-only build\n```\n\n----------------------------------------\n\nTITLE: Configuring SurveyMonkey Stream Catalog in JSON\nDESCRIPTION: This JSON snippet defines the configured catalog for the SurveyMonkey stream, specifying the sync mode and destination sync mode.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"streams\": [\n    {\n      \"stream\": {\n        \"name\": \"surveys\",\n        \"json_schema\": {},\n        \"supported_sync_modes\": [\"full_refresh\"]\n      },\n      \"sync_mode\": \"full_refresh\",\n      \"destination_sync_mode\": \"overwrite\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Local Connector Operations\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read functions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-looker spec\npoetry run source-looker check --config secrets/config.json\npoetry run source-looker discover --config secrets/config.json\npoetry run source-looker read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Publishing Connector Command in Airbyte CI\nDESCRIPTION: Example of using the airbyte-ci publish command to publish modified connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --modified publish\n```\n\n----------------------------------------\n\nTITLE: Enabling local_infile in MariaDB ColumnStore\nDESCRIPTION: SQL command to set the local_infile system variable to true, which is required for Airbyte to use LOAD DATA LOCAL INFILE to load data into tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mariadb-columnstore.md#2025-04-23_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET GLOBAL local_infile = true\n```\n\n----------------------------------------\n\nTITLE: Installing Firestore Connector Dependencies\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Required Azure Storage Blob Data Reader Permissions\nDESCRIPTION: JSON configuration showing the minimum required permissions for the Storage Blob Data Reader role in Azure, including actions and data actions needed for blob storage access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-blob-storage.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n   {\n      \"actions\": [\n         \"Microsoft.Storage/storageAccounts/blobServices/containers/read\",\n         \"Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action\"\n      ],\n      \"notActions\": [],\n      \"dataActions\": [\n         \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\"\n      ],\n      \"notDataActions\": []\n   }\n]\n```\n\n----------------------------------------\n\nTITLE: Building Brex Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the Brex source connector using airbyte-ci. Creates a source-brex:dev image for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-brex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-brex build\n```\n\n----------------------------------------\n\nTITLE: Updating an Existing Workspace with Region Association in Bash\nDESCRIPTION: API request to update an existing workspace's data residency settings by sending a PATCH request to /v1/workspaces/{workspaceId}. This changes which region's data plane the workspace uses.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PATCH \"https://example.com/api/public/v1/workspaces/{workspaceId}\" \\\n  -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Updated Workspace Name\",\n    \"dataResidency\": \"us-west\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: RECORD Message with Array Data Types in Airbyte JSON Format\nDESCRIPTION: An example of a RECORD type message in Airbyte containing different array data types including strings, dates, timestamps, numbers, big numbers, integers, and booleans. The message includes the stream name, emission timestamp, and the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"string_array\" : [\"foo bar\", \"some random special characters: ࠈൡሗ\"], \"array_date\" : [\"2021-01-23\", \"1504-02-29\"], \"array_timestamp_with_timezone\" : [\"2022-11-22T01:23:45+05:00\", \"9999-12-21T01:23:45-05:00\"], \"array_timestamp_without_timezone\" : [\"2022-11-22T01:23:45\", \"1504-02-29T01:23:45\"], \"array_number\" : [56.78, 0, -12345.678], \"array_big_number\" : [\"-12345.678\", \"100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.1234\"], \"array_integer\" : [42, 0, 12345], \"array_boolean\" : [true, false] }}}\n```\n\n----------------------------------------\n\nTITLE: Testing CDC Streams in Airbyte\nDESCRIPTION: This snippet introduces the inclusion of CDC (Change Data Capture) streams in the test suite. It focuses on testing deduplication sync modes and handling deleted rows in deduplicated tables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/README.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# CDC\n\nWe've also included some streams as if they were produced by a CDC source, especially to test how they would behave regarding dedup sync modes where deleted rows should be removed from deduplicated tables\n```\n\n----------------------------------------\n\nTITLE: Ignoring Errors Using a Predicate on the Response in YAML\nDESCRIPTION: Uses a response filter with a predicate expression (e.g., string interpolation) to ignore errors when the response includes a specific field (such as 'code'). This code leverages dynamic response inspection for more flexible error ignoring strategies in Airbyte configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - predicate: \"{{ 'code' in response }}\"\n          action: IGNORE\n```\n\n----------------------------------------\n\nTITLE: Adding User to Docker Group - Shell\nDESCRIPTION: Adds the specified user (typically 'ec2-user') to the system's 'docker' group to grant permission for controlling Docker without sudo. This command modifies user group membership and may require a shell logout/login to take effect. Should be run with root privileges and takes the username as the final parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo usermod -a -G docker ec2-user\n```\n\n----------------------------------------\n\nTITLE: Sample Airbyte state message in JSON\nDESCRIPTION: Demonstrates the structure of a typical state message output by Airbyte after an incremental read. Captures current stream and its last processed date_modified as a Unix timestamp. Used to resume incremental reads from the correct point.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"STATE\",\n  \"state\": {\n    \"type\": \"STREAM\",\n    \"stream\": {\n      \"stream_descriptor\": { \"name\": \"surveys\", \"namespace\": null },\n      \"stream_state\": { \"date_modified\": 1623348420.0 }\n    },\n    \"sourceStats\": { \"recordCount\": 0.0 }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Sendgrid Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Sendgrid source connector using Docker. They include running the spec, check, discover, and read commands with appropriate configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendgrid/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sendgrid:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sendgrid:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sendgrid:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sendgrid:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Enterprise Edition in YAML\nDESCRIPTION: This snippet sets the global edition to enterprise in the Helm values file. It's required for deploying Airbyte Self-Managed Enterprise.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  edition: enterprise\n```\n\n----------------------------------------\n\nTITLE: Granting Server State View Permission in MSSQL\nDESCRIPTION: SQL command to grant VIEW SERVER STATE permissions for monitoring SQL Server Agent status\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nUSE master;\nGRANT VIEW SERVER STATE TO {user name};\n```\n\n----------------------------------------\n\nTITLE: Running Ringcentral connector tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Ringcentral source connector using the airbyte-ci tool, which validates the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ringcentral/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ringcentral test\n```\n\n----------------------------------------\n\nTITLE: Creating Milvus Collection via REST API\nDESCRIPTION: This code snippet demonstrates how to create a Milvus collection using the REST API when using Zilliz cloud. It sets up a collection with a primary key field and a vector field for storing embeddings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/milvus.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nPOST /v1/vector/collections/create\n{\n  \"collectionName\": \"my-collection\",\n  \"dimension\": 1536,\n  \"metricType\": \"L2\",\n  \"vectorField\": \"vector\",\n  \"primaryField\": \"pk\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-cumulio:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-cumulio:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-cumulio:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-cumulio:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Output Record Format After Firebase Realtime Database Sync\nDESCRIPTION: Example showing how Firebase data is transformed after being synced through Airbyte. Each key-value pair in the original JSON becomes a record with 'key' and 'value' fields, where the value is a stringified JSON object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/firebase-realtime-database.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"key\": \"liam\", \"value\": \"{\\\"address\\\": \\\"somewhere\\\", \\\"age\\\": 24}}\"}\n{\"key\": \"olivia\", \"value\": \"{\\\"address\\\": \\\"somewhere\\\", \\\"age\\\": 30}}\"}\n```\n\n----------------------------------------\n\nTITLE: Copying Airbyte Local Files using Docker CP Command\nDESCRIPTION: Command to copy files from the Airbyte server container's temporary directory to a local Windows path. This creates accessible copies of the files that can be read directly on the Windows host system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/locating-files-local-destination.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/airbyte_local <local path>\n```\n\n----------------------------------------\n\nTITLE: Amazon Ads Prerequisites Configuration\nDESCRIPTION: Required configuration parameters for setting up the Amazon Ads connector including client credentials, region settings, and optional parameters for data filtering.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- Client ID\n- Client Secret\n- Refresh Token\n- Region\n- Start Date (Optional)\n- Profile IDs (Optional)\n- Marketplace IDs (Optional)\n```\n\n----------------------------------------\n\nTITLE: Configuring oneOf Dropdown Fields in Airbyte Connector Spec\nDESCRIPTION: Implementation of a oneOf dropdown selection for file providers in a connector specification. This example demonstrates how to structure the provider options with required option_title fields and const values for proper UI rendering.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-specification-reference.md#2025-04-23_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"connection_specification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"File Source Spec\",\n    \"type\": \"object\",\n    \"required\": [\"dataset_name\", \"format\", \"url\", \"provider\"],\n    \"properties\": {\n      \"dataset_name\": {\n        ...\n      },\n      \"format\": {\n        ...\n      },\n      \"reader_options\": {\n        ...\n      },\n      \"url\": {\n        ...\n      },\n      \"provider\": {\n        \"type\": \"object\",\n        \"oneOf\": [\n          {\n            \"required\": [\n              \"option_title\"\n            ],\n            \"properties\": {\n              \"option_title\": {\n                \"type\": \"string\",\n                \"const\": \"HTTPS: Public Web\",\n                \"order\": 0\n              }\n            }\n          },\n          {\n            \"required\": [\n              \"option_title\"\n            ],\n            \"properties\": {\n              \"option_title\": {\n                \"type\": \"string\",\n                \"const\": \"GCS: Google Cloud Storage\",\n                \"order\": 0\n              },\n              \"service_account_json\": {\n                \"type\": \"string\",\n                \"description\": \"In order to access private Buckets stored on Google Cloud, this connector would need a service account json credentials with the proper permissions as described <a href=\\\"https://cloud.google.com/iam/docs/service-accounts\\\" target=\\\"_blank\\\">here</a>. Please generate the credentials.json file and copy/paste its content to this field (expecting JSON formats). If accessing publicly available data, this field is not necessary.\"\n              }\n            }\n          }\n        ]\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using urlize Filter in Jinja2\nDESCRIPTION: Demonstrates the `urlize` filter in Jinja2, which finds URLs within plain text and converts them into clickable HTML `<a>` links. The example converts 'http://example.com' within a string.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_53\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'Check this out: http://example.com'|urlize }}\n```\n\n----------------------------------------\n\nTITLE: MSSQL Source Connector Changelog Table\nDESCRIPTION: Markdown table containing version history with release dates and descriptions of changes made to the MSSQL source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n| 3.7.1   | 2024-02-20 | [35405](https://github.com/airbytehq/airbyte/pull/35405) | Change query syntax to make it compatible with Azure Synapse. |\n| 3.7.0   | 2024-01-30 | [33311](https://github.com/airbytehq/airbyte/pull/33311) | Source mssql with checkpointing initial sync. |\n...\n```\n\n----------------------------------------\n\nTITLE: BambooHR Sync Mode Support Table\nDESCRIPTION: Markdown table showing the supported sync modes and features of the BambooHR connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bamboo-hr.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | Yes        |\n| SSL connection            | Yes        |\n| Namespaces                | No         |\n```\n\n----------------------------------------\n\nTITLE: Requesting Access Token Endpoint for Self-Managed Airbyte (YAML)\nDESCRIPTION: Specifies the HTTP POST endpoint URL used to request an access token for a Self-Managed Airbyte instance. Users need to replace `<YOUR_WEBAPP_URL>` with the actual URL used to access their Airbyte UI. This endpoint requires a JSON body containing the `client_id` and `client_secret`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nPOST <YOUR_WEBAPP_URL>/api/v1/applications/token\n```\n\n----------------------------------------\n\nTITLE: Updating Connector Definition Resources in SQL\nDESCRIPTION: SQL command to update resource requirements for a connector definition, including CPU and memory limits/requests for sync jobs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-connector-resources.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nupdate actor_definition set resource_requirements = '{\"jobSpecific\": [{\"jobType\": \"sync\", \"resourceRequirements\": {\"cpu_limit\": \"2\", \"cpu_request\": \"2\", \"memory_limit\": \"2048Mi\", \"memory_request\": \"2048Mi\"}}]}' where id = '<id-from-step-1>';\n```\n\n----------------------------------------\n\nTITLE: Example Request with Lookback Window for Guardian API\nDESCRIPTION: Demonstration of how a 2-day lookback window affects the API request by starting the date range 2 days before the last cursor value to catch updates to existing records.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl 'https://content.guardianapis.com/search?from-date=2023-04-13T07:30:58Z&to-date=<now>'\n```\n\n----------------------------------------\n\nTITLE: Installing abctl using Go\nDESCRIPTION: Command to install abctl using Go package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ngo install github.com/airbytehq/abctl@latest\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Discover Command via Docker (Bash)\nDESCRIPTION: Executes the `discover` command within a Docker container to fetch the schema (catalog) of available data streams from the source API. Requires Docker, the connector image (`airbyte/source-google-pagespeed-insights:dev`), and a valid `config.json` mounted from the local `secrets` directory. Outputs the discovered catalog in JSON format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-pagespeed-insights:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests using Poetry and Pytest - Bash\nDESCRIPTION: Runs the unit test suite for the Google Drive connector using Poetry as the package runner and pytest as the testing framework. Expects the tests to be located in the unit_tests directory. This command validates code correctness and should be run before making code changes or before merging PRs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Querying Connector Definition ID in SQL\nDESCRIPTION: SQL query to find the connector definition ID by searching for a specific image name in the actor_definition table.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-connector-resources.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from actor_definition where actor_definition.docker_repository like '%<image-name>';\n```\n\n----------------------------------------\n\nTITLE: Global State Message in Airbyte JSON Format\nDESCRIPTION: Example of an Airbyte STATE message with type GLOBAL, which stores shared state information across all streams. This particular state includes a start_date parameter used for incremental synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_object_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from Strava Token Exchange\nDESCRIPTION: This JSON object represents the response from the Strava API after exchanging the authorization code for a refresh token. It contains the refresh token, access token, and athlete information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/strava.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"token_type\": \"Bearer\",\n    \"expires_at\": 1562908002,\n    \"expires_in\": 21600,\n    \"refresh_token\": \"REFRESHTOKEN\",\n    \"access_token\": \"ACCESSTOKEN\",\n    \"athlete\": {\n        \"id\": 123456,\n        \"username\": \"MeowTheCat\",\n        \"resource_state\": 2,\n        \"firstname\": \"Meow\",\n        \"lastname\": \"TheCat\",\n        \"city\": \"\",\n        \"state\": \"\",\n        \"country\": null,\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an API Key with the Read-Only Role in Fauna\nDESCRIPTION: This FQL query creates an API key associated with the previously created 'airbyte-readonly' role. The secret output from this command will be used as authentication for the Airbyte connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nCreateKey({\n  name: \"airbyte-readonly\",\n  role: Role(\"airbyte-readonly\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Stream Method Granularity Condition in Python\nDESCRIPTION: This code snippet illustrates how the 'stream' method handles different granularities for report streams based on the connector version. It shows the basic streams list and the differences in report stream initialization between versions < 0.1.13 and >= 0.1.13.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"stream\" method has granularity condition depend on that report streams supports by different connector version:\n\n- For all version:\n  basic streams list:\n  - ad_groups\n  - ads\n  - campaigns\n  - advertisers\n- for < 0.1.13 - expose report streams initialized with 'report_granularity' argument, like:\n  Example:\n\n  - AdsReports(report_granularity='DAILY')\n  - AdsReports(report_granularity='LIFETIME')\n    streams list:\n    - advertisers_reports\n    - advertisers_audience_reports\n    - campaigns_audience_reports_by_country\n    - ad_group_audience_reports\n    - ads_audience_reports\n    - ad_groups_reports\n    - ads_reports\n    - campaigns_reports\n\n- for >= 0.1.13 - expose report streams in format: <report*type>*<granularity>, like:\n  Example:\n  - AdsReportsDaily(Daily, AdsReports)\n  - AdsReportsLifetime(Lifetime, AdsReports)\n    streams:\n    - campaigns_audience_reports_daily\n    - campaigns_audience_reports_by_country_daily\n    - campaigns_audience_reports_by_platform_daily\n    - campaigns_reports_daily\n    - advertisers_audience_reports_daily\n    - advertisers_audience_reports_by_country_daily\n    - advertisers_audience_reports_by_platform_daily\n    - advertisers_reports_daily\n    - ad_group_audience_reports_daily\n    - ad_group_audience_reports_by_country_daily\n    - ad_group_audience_reports_by_platform_daily\n    - ads_reports_lifetime\n    - advertiser_ids\n    - campaigns_reports_lifetime\n    - advertisers_audience_reports_lifetime\n    - ad_groups_reports_lifetime\n    - ad_groups_reports_daily\n    - advertisers_reports_lifetime\n    - ads_reports_daily\n    - ads_audience_reports_daily\n    - ads_audience_reports_by_country_daily\n    - ads_audience_reports_by_platform_daily\n    - ads_reports_hourly\n    - ad_groups_reports_hourly\n    - advertisers_reports_hourly\n    - campaigns_reports_hourly\n```\n\n----------------------------------------\n\nTITLE: Configuring High Strictness Level Tests in YAML\nDESCRIPTION: Example YAML configuration for high strictness level acceptance tests, including test declarations and bypass reasons for unsupported tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nconnector_image: \"airbyte/source-pokeapi\"\ntest_strictness_level: high\nacceptance_tests:\n  spec:\n    tests:\n      - spec_path: \"source_pokeapi/spec.json\"\n  connection:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n        status: \"succeed\"\n  discovery:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n  basic_read:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n  full_refresh:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n        configured_catalog_path: \"integration_tests/configured_catalog.json\"\n  incremental:\n    bypass_reason: \"Incremental syncs are not supported on this connector.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Helm Values to Use dev Server Image Tag (YAML)\nDESCRIPTION: A YAML snippet intended for a `values.yaml` file used with Helm (via `abctl local install`). It configures the deployment to use the Docker image tagged `dev` for the Airbyte server component, overriding the default tag.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  image:\n    tag: dev\n```\n\n----------------------------------------\n\nTITLE: Number Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for number data tests in Airbyte's JSON record format. Includes examples with positive decimal numbers, zero, and negative decimal numbers with timestamps and stream identifiers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : 56.78 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : 0 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : -12345.678 }}}\n```\n\n----------------------------------------\n\nTITLE: Running Clickhouse Connector Docker Commands\nDESCRIPTION: Various Docker commands for running the connector in different modes including spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-clickhouse/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-clickhouse:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-clickhouse:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-clickhouse:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-clickhouse:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Connector Docker Commands\nDESCRIPTION: Commands to run the Qdrant connector's operations (spec, check, write) using the built Docker image. These are used for testing the containerized connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-qdrant:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-qdrant:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-qdrant:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Docker Run Commands for Connector Testing\nDESCRIPTION: Commands for running the connector in Docker container to test specification, configuration check, and write operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-langchain:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-langchain:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-langchain:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Initializing a Connector Project - Airbyte with Poetry - Shell (bash)\nDESCRIPTION: This multi-step script guides the user through duplicating the Airbyte repository, creating a directory for a new source connector, initializing a Python project using Poetry, and adding the 'airbyte-cdk' as a dependency. Prerequisites include installed git, Poetry, and Python. Inputs include the connector name; outputs are a ready-to-code connector skeleton. Limitations: User must follow prompts interactively during 'poetry init'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:airbytehq/airbyte.git\\ncd airbyte\\n\\n# Make a directory for a new connector and navigate to it\\nmkdir airbyte-integrations/connectors/source-exchange-rates-tutorial\\ncd airbyte-integrations/connectors/source-exchange-rates-tutorial\\n\\n# Initialize a project, follow Poetry prompts, and then add airbyte-cdk as a dependency.\\npoetry init\\npoetry add airbyte-cdk\n```\n\n----------------------------------------\n\nTITLE: Running Pipeliner Connector Acceptance Tests using airbyte-ci (Bash)\nDESCRIPTION: This command executes the acceptance test suite for the `source-pipeliner` connector using the `airbyte-ci` tool. Running these tests helps verify the connector's functionality and adherence to Airbyte standards during local development. Requires the `airbyte-ci` tool and potentially a previously built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipeliner/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pipeliner test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Pytest via Poetry - Bash\nDESCRIPTION: Runs all unit tests present in the 'unit_tests' directory for the connector using Poetry to ensure the correct environment and dependencies. Requires project dependencies installed and the 'pytest' package available within the Poetry environment. Outputs test results to the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Querying DataScope Locations API via HTTP GET Request - HTTP\nDESCRIPTION: This HTTP request demonstrates how to retrieve location data from the DataScope API. The endpoint is accessed via a GET method and requires authentication using an API key. Optional query parameters such as sort, alt, or prettyPrint may be included as specified in the connector documentation. The expected output is a JSON response containing location data, and this endpoint is intended to be used by the Airbyte DataScope connector or manually for testing API connectivity.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/datascope.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://www.mydatascope.com/api/external/locations\n```\n\n----------------------------------------\n\nTITLE: Running the Read Operation for Zendesk Talk Connector (Bash)\nDESCRIPTION: Executes the `read` command within a temporary Docker container using the `airbyte/source-zendesk-talk:dev` image. It reads data from the source based on the provided configuration and catalog files. Requires Docker, the built image, a valid `config.json` (in `secrets`), and a `configured_catalog.json` (in `integration_tests`) mounted into the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zendesk-talk:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Illumina Basespace Connector in Markdown\nDESCRIPTION: Markdown table defining the configuration inputs for the Illumina Basespace connector. It includes the access token, domain, and user parameters with their descriptions and default values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/illumina-basespace.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `access_token` | `string` | Access Token. BaseSpace access token. Instructions for obtaining your access token can be found in the BaseSpace Developer Documentation. |  |\n| `domain` | `string` | Domain. Domain name of the BaseSpace instance (e.g., euw2.sh.basespace.illumina.com) |  |\n| `user` | `string` | User. Providing a user ID restricts the returned data to what that user can access. If you use the default (&#39;current&#39;), all data accessible to the user associated with the API key will be shown. | current |\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record and State Message Format\nDESCRIPTION: Sample Airbyte messages showing the structured format for different data types (integer, float, and default number) and a state message. Each record includes metadata like stream name and emission timestamp along with the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/number_data_type_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : 42 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : 0 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"int_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : -12345 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : 56.78 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : 0 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : -12345.678 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : 10000000000000000000000.1234 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : 0 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : -12345.678 }}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Defining WaitTimeFromHeader Backoff Schema in YAML\nDESCRIPTION: Outlines the schema for WaitTimeFromHeader, which extracts wait intervals from a specified response header (optionally using a regex). This enables dynamic, server-guided backoff timing in HTTP retries within Airbyte YAML configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nWaitTimeFromHeader:\n  type: object\n  additionalProperties: true\n  required:\n    - header\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    header:\n      type: string\n    regex:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations within a Docker container, including spec, check, discover, and read functions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-looker:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-looker:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-looker:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-looker:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Hubspot Connector as Docker Container\nDESCRIPTION: Commands to run various Hubspot connector operations using the built Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-hubspot:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hubspot:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hubspot:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-hubspot:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Productboard Source Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the Productboard connector using airbyte-ci. This creates a dev image (source-productboard:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-productboard/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-productboard build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Deepset Connector\nDESCRIPTION: Command to build the Docker image for the Deepset connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-deepset build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite locally using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yahoo-finance-price/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yahoo-finance-price test\n```\n\n----------------------------------------\n\nTITLE: Running K6 Cloud Source Connector CI Test Suite\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the K6 Cloud source connector locally. It's useful for verifying changes and ensuring the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-k6-cloud/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-k6-cloud test\n```\n\n----------------------------------------\n\nTITLE: Testing Deputy Airbyte Connector with Airbyte CI - Bash\nDESCRIPTION: Executes the Airbyte CI acceptance tests for the 'source-deputy' connector, verifying its behavior and integration using the standard Airbyte testing framework. The 'airbyte-ci' tool must be installed and configured beforehand. The '--name=source-deputy' argument points to the specific connector, 'test' triggers the test suite. Input: the test command; Output: test results in the local environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-deputy/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-deputy test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands inside Docker - bash\nDESCRIPTION: This set of bash commands demonstrates how to run Airbyte source connector operations (spec, check, discover, read) using a Docker container. It uses volume mounts for config and test files, ensuring that sensitive data is not embedded inside the image. The snippet assumes that the image airbyte/source-outbrain-amplify:dev is available in the local docker registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-outbrain-amplify:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-outbrain-amplify:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-outbrain-amplify:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-outbrain-amplify:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Intruder Source Connector\nDESCRIPTION: Set of Docker commands to run various operations for the Intruder source connector, including spec, check, discover, and read. These commands use the locally built Docker image and mount necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intruder/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-intruder:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-intruder:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-intruder:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-intruder:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Ticketmaster Source Connector in Airbyte\nDESCRIPTION: This command builds a development image of the Ticketmaster source connector using airbyte-ci. The resulting image (source-ticketmaster:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ticketmaster/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ticketmaster build\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Pypi source connector using airbyte-ci. This validates that all connector functionality is working correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pypi/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pypi test\n```\n\n----------------------------------------\n\nTITLE: Testing PayFit Connector with Airbyte CI - Bash\nDESCRIPTION: This snippet shows how to execute acceptance tests for the PayFit source connector using the Airbyte CI tool. It relies on a proper environment setup with 'airbyte-ci' installed. The command executes the 'test' target on the specified connector; acceptance tests validate connector compliance before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-payfit/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-payfit test\n```\n\n----------------------------------------\n\nTITLE: Testing the Youtube Data Connector (Bash)\nDESCRIPTION: This command executes the acceptance tests for the `source-youtube-data` connector using the `airbyte-ci` tool. It verifies the connector's functionality against predefined test cases. Requires `airbyte-ci` to be installed and the connector image (e.g., `source-youtube-data:dev`) to be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-youtube-data test\n```\n\n----------------------------------------\n\nTITLE: Running Toggl Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Toggl source connector using Docker. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-toggl/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-toggl:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-toggl:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-toggl:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-toggl:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yahoo-finance-price/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-yahoo-finance-price:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yahoo-finance-price:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yahoo-finance-price:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-yahoo-finance-price:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example Command for Generating Artifacts\nDESCRIPTION: Example command demonstrating how to generate insights.json and sbom.json artifacts and save them both locally and to Google Cloud Storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_insights/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconnectors-insights generate --output-directory <path-to-local-output-dir> --gcs-uri=gs://<bucket>/<key-prefix> --connector-directory airbyte-integrations/connectors/ --concurrency 2 --rewrite\n```\n\n----------------------------------------\n\nTITLE: Creating a Data Plane with Airbyte API in Bash\nDESCRIPTION: API request to create a new data plane within a specific region by sending a POST request to the /v1/regions/<REGION_ID>/dataplanes endpoint. The data plane will handle data syncing jobs in that region.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url https://example.com/api/public/v1/regions/116a49ab-b04a-49d6-8f9e-4d9d6a4189cc/dataplanes \\\n  --header 'authorization: Bearer $TOKEN' \\\n  --header 'content-type: application/json' \\\n  --data '{\n  \"name\": \"aws-us-west-3-dp-8\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Defining Thinkific Data Streams in Markdown\nDESCRIPTION: This snippet lists the available data streams for the Thinkific connector, including their primary keys, pagination type, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| courses | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| promotions | id | DefaultPaginator | ✅ |  ❌  |\n| categories | id | DefaultPaginator | ✅ |  ❌  |\n| reviews | id | DefaultPaginator | ✅ |  ❌  |\n| enrollments | id | DefaultPaginator | ✅ |  ❌  |\n| groups | id | DefaultPaginator | ✅ |  ❌  |\n| instructors | id | DefaultPaginator | ✅ |  ❌  |\n| orders | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ❌  |\n| coupons | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Expense Connector with Airbyte-CI\nDESCRIPTION: Command to build a development image of the Zoho Expense source connector using airbyte-ci. Creates a dev image tagged as 'source-zoho-expense:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-expense/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-expense build\n```\n\n----------------------------------------\n\nTITLE: Copying Catalog File from Kubernetes Pod to Local Machine\nDESCRIPTION: Shows the `kubectl cp` command used to copy a file (e.g., `destination_catalog.json`) from a specific path (`/config/destination_catalog.json`) inside an Airbyte worker pod (`<normalisation-pod-name>`) in a given namespace (`<namespace pods are in>`) to the current directory (`./catalog.json`) on the local machine. A specific example and a subsequent `cat` command to view the copied file are included.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl cp <namespace pods are in>/<normalisation-pod-name>:/config/destination_catalog.json ./catalog.json\ne.g.\nkubectl cp jobs/normalization-worker-3605-0-sxtox:/config/destination_catalog.json ./catalog.json\ncat ./catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Printify Source Connector with airbyte-ci\nDESCRIPTION: Command to run the acceptance tests for the Printify source connector, which verifies that the connector functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-printify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-printify test\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultPaginator for Page and Page Size - YAML\nDESCRIPTION: Configures a DefaultPaginator in YAML to set page_size_option and page_token_option using request parameters. Requires the DefaultPaginator component and an API supporting pagination via request parameters. Key parameters are field_name (for page and page_size) and inject_into (should be request_parameter). The configuration results in outgoing HTTP requests correctly parameterized for pagination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: request_parameter\n    field_name: page_size\n  pagination_strategy:\n    type: \"PageIncrement\"\n    page_size: 5\n  page_token:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page\"\n```\n\n----------------------------------------\n\nTITLE: Launching BusyBox Container to Browse Airbyte Docker Volume\nDESCRIPTION: Shows the command to start an interactive BusyBox Docker container (`-it`). It mounts the `airbyte_workspace` named volume to the `/data` directory within the container, allowing users to browse the volume's contents using standard shell commands. The container is automatically removed (`--rm`) upon exit.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndocker run -it --rm --volume airbyte_workspace:/data busybox\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Connector with Gradle\nDESCRIPTION: Command to build the connector using Gradle from the Airbyte repository root. This approach is typically used in CI environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-python-http-tutorial:build\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Secret for S3 Credentials in YAML\nDESCRIPTION: Defines a Kubernetes Secret named `airbyte-config-secrets` of type `Opaque` to store AWS S3 access key ID (`s3-access-key-id`) and secret access key (`s3-secret-access-key`). These credentials are required when configuring Airbyte to use S3 for state and logging storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # AWS S3 Secrets\n  s3-access-key-id: ## e.g. AKIAIOSFODNN7EXAMPLE\n  s3-secret-access-key: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n```\n\n----------------------------------------\n\nTITLE: Example Public Key Format for Airbyte Encryption\nDESCRIPTION: This is an example of the public key format that Airbyte expects for the encryption mapping feature. Users should generate their own keys rather than using this example.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/mappings.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n30820122300d06092a864886f70d01010105000382010f003082010a0282010100ce01c1000b6712bd5f694402c82ffb7b60867130b6e3284eac39577ff0f9b12a69920af4e53f4d83843ce86ba4975bb0298e6cf0ffbb8696540426bbf2146075ac6779801cf3dac54aa21ec69b14ab78217b5be70d083d075f06443a6f84ed6a61c924a4467b76eb35b41cf0d9e88be8c98734aec87ba7e9a6e8b9bec45627edbba2ea285f4907811ff94a01b6b1a90d88d303fbb60f62c094a65f5739fc6e46e06924040cd54c2a990483aa25eb4a7a35c0b77ef42f0c06fe1b00a8ca038939d22cc136de862a3bb5ba4a14f211e31d1380cf26fa3d6b268f6a4be47e3926a5d83ca20ae0108379b0d940c0e8a5a9cf7d24a6222305520ce6b507e3f7515e2d0203010001\n```\n\n----------------------------------------\n\nTITLE: Running Datascope Source Connector Commands via Docker - Bash\nDESCRIPTION: These snippets illustrate how to execute the Datascope source connector in various operational modes inside a Docker container. Each command runs a different connector action (spec, check, discover, read), with relevant file mounts and flags for local development or testing. Input parameters typically include configuration and catalog files mounted from the host, while outputs may be processed downstream or for test validation. Credentials and integration test files are expected in specified mounted volumes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datascope/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-datascope:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-datascope:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-datascope:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-datascope:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Test Suite for Prestashop Source Connector\nDESCRIPTION: Command to run the full test suite for the Prestashop source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-prestashop/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-prestashop test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the LinkedIn Pages connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-pages/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linkedin-pages test\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Drive Service Account Key Authentication in Airbyte OSS (JSON)\nDESCRIPTION: Provides the expected JSON structure for the Service Account Key when configuring the Google Drive source connector in Airbyte Open Source. This JSON object, obtained from Google Cloud Console after creating a service account and generating a key, contains necessary credentials like project ID and private key for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-drive.md#2025-04-23_snippet_0\n\nLANGUAGE: js\nCODE:\n```\n{ \"type\": \"service_account\", \"project_id\": \"YOUR_PROJECT_ID\", \"private_key_id\": \"YOUR_PRIVATE_KEY\", ... }\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-webflow build\n```\n\n----------------------------------------\n\nTITLE: Building Gainsight PX Connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Gainsight PX source connector Docker image using airbyte-ci. After execution, an image will be available on the host with the tag airbyte/source-gainsight-px:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gainsight-px/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gainsight-px build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Microsoft Teams source connector using airbyte-ci locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-teams/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-teams test\n```\n\n----------------------------------------\n\nTITLE: Running Pardot Connector CI Tests (Bash)\nDESCRIPTION: Executes the full CI test suite for the Pardot source connector locally using the 'airbyte-ci' tool. This command requires 'airbyte-ci' to be installed and helps verify that code changes pass all required quality and integration checks before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pardot/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pardot test\n```\n\n----------------------------------------\n\nTITLE: Formatting Airbyte Trace Message in JSON\nDESCRIPTION: This snippet shows the structure of an Airbyte trace message. It includes a stream status object with a stream descriptor and status, along with an emission timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_object_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"object_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Defining Daily Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema example for the daily_active_users stream in Google Analytics, showing the count of 1-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_1dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated User for Airbyte in Oracle SQL\nDESCRIPTION: These SQL commands create a new Oracle database user named 'airbyte' with a specified password and grant it the basic privilege to establish a connection (CREATE SESSION). Creating a dedicated user is recommended for security and auditing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte IDENTIFIED BY <your_password_here>;\nGRANT CREATE SESSION TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Data Type Migration Table for Version 2.0.0\nDESCRIPTION: Table showing field type changes from string/integer to standardized formats including object, number, timestamp with tz, and date types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bing-ads-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream field                | Current Airbyte Type | New Airbyte Type  |\n| --------------------------- | -------------------- | ----------------- |\n| LinkedAgencies              | string               | object            |\n| BiddingScheme.MaxCpc.Amount | string               | number            |\n| CostPerConversion           | integer              | number            |\n| Modified Time               | string               | timestamp with tz |\n| Date                        | string               | date              |\n| TimePeriod                  | string               | timestamp with tz |\n```\n\n----------------------------------------\n\nTITLE: Access Token Request Body Format\nDESCRIPTION: JSON body structure required when requesting an access token. The request must include the client_id and client_secret values obtained from creating an application.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/api-access-config.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{ \"client_id\": \"\", \"client_secret\": \"\" }\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands with Poetry - Bash\nDESCRIPTION: Provides example Poetry-based commands to execute spec, check, discover, and read operations for the source-zendesk-support connector. These commands run Airbyte’s CLI tool for connectors, passing configuration files as needed. Dependencies: Poetry environment must be initialized, and config files (like secrets/config.json and integration_tests/configured_catalog.json) must exist. Outputs are displayed in the terminal, executing respective connector entrypoints or tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-zendesk-support spec\npoetry run source-zendesk-support check --config secrets/config.json\npoetry run source-zendesk-support discover --config secrets/config.json\npoetry run source-zendesk-support read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bing-ads test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Svix Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Svix source connector using airbyte-ci. It verifies the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-svix/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-svix test\n```\n\n----------------------------------------\n\nTITLE: Migrating Data from Old Raw Table to New Raw Table in Oracle\nDESCRIPTION: SQL script to create a new raw table in the airbyte_internal schema and migrate data from the old raw table. This script assumes the database was 'PUBLIC' and requires replacing {{stream_name}} with the actual stream name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/oracle-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE airbyte_internal.public_raw__stream_{{stream_name}}\n(\n    _airbyte_raw_id VARCHAR(64) PRIMARY KEY,\n    _airbyte_data JSON,\n    _airbyte_extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    _airbyte_loaded_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\n    _airbyte_meta JSON\n);\n\nINSERT INTO airbyte_internal.default_raw__stream_{{stream_name}}\n    SELECT\n        _airbyte_ab_id AS \"_airbyte_raw_id\",\n        _airbyte_data AS \"_airbyte_data\"\n        _airbyte_emitted_at AS \"_airbyte_extracted_at\",\n        NULL AS \"_airbyte_loaded_at\",\n        NULL AS \"_airbyte_meta\",\n    FROM default._airbyte_raw_{{stream_name}};\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Recurly Source Connector\nDESCRIPTION: Standard commands to run the source-recurly connector in Docker for various operations including spec, check, discover, and read. These commands mount local directories to provide configuration and receive output.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recurly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-recurly:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recurly:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recurly:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-recurly:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping for Instatus Connector\nDESCRIPTION: This table shows the mapping between Instatus data types and Airbyte data types for the source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instatus.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type    | Airbyte Type | Notes |\n| :------------------ | :----------- | :---- |\n| `string`            | `string`     |       |\n| `integer`, `number` | `number`     |       |\n| `array`             | `array`      |       |\n| `object`            | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Platform with Gradle (Bash)\nDESCRIPTION: Executes the Gradle wrapper script to run the `build` task for the entire Airbyte platform project. This compiles Java/Kotlin code, runs unit tests, and generates necessary artifacts like JAR files and Docker images tagged as `:dev`. Requires Java 21 and is run from the `airbyte-platform` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew build\n```\n\n----------------------------------------\n\nTITLE: Granting Granular Read Access to SingleStore User (SQL)\nDESCRIPTION: Demonstrates how to grant SELECT (read-only) permissions to the 'airbyte' user on specific tables, potentially across different databases. This method allows for more precise control over data access compared to granting access to an entire database.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/singlestore.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON \"<database_a>\".\"<table_1>\" TO airbyte;\nGRANT SELECT ON \"<database_b>\".\"<table_2>\" TO airbyte;\n```\n\n----------------------------------------\n\nTITLE: Airbyte Stream Records with Special Data Types\nDESCRIPTION: Example JSON records showing streams with binary data fields and various edge cases like special characters and reserved keywords.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/edge_case_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_with_binary_data\", \"emitted_at\": 1602637589500, \"data\": { \"some_id\" : 303, \"binary_field_name\":\"dGVzdA==\" }}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Debug Options in Docker Compose (YAML)\nDESCRIPTION: Defines environment variables for the `worker` service in `docker-compose.debug.yaml`. The `DEBUG_CONTAINER_IMAGE` variable (set externally via the `docker compose` command) specifies the target container image to debug, and `DEBUG_CONTAINER_JAVA_OPTS` holds the JDWP settings that the worker will inject as `JAVA_TOOL_OPTIONS` into the target container if its image name matches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nworker:\n  environment:\n    - DEBUG_CONTAINER_IMAGE=${DEBUG_CONTAINER_IMAGE}\n    - DEBUG_CONTAINER_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005\n```\n\n----------------------------------------\n\nTITLE: Running Tempo Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Tempo source connector using Docker. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tempo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tempo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tempo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tempo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tempo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Documenting Squarespace Connector Changelog in Markdown\nDESCRIPTION: This snippet presents the version history of the Squarespace connector, detailing updates and changes made in each release.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/squarespace.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.20 | 2025-04-19 | [58441](https://github.com/airbytehq/airbyte/pull/58441) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57991](https://github.com/airbytehq/airbyte/pull/57991) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57407](https://github.com/airbytehq/airbyte/pull/57407) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56843](https://github.com/airbytehq/airbyte/pull/56843) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56279](https://github.com/airbytehq/airbyte/pull/56279) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55642](https://github.com/airbytehq/airbyte/pull/55642) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [54543](https://github.com/airbytehq/airbyte/pull/54543) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [54035](https://github.com/airbytehq/airbyte/pull/54035) | Update dependencies |\n| 0.0.12 | 2025-02-08 | [53545](https://github.com/airbytehq/airbyte/pull/53545) | Update dependencies |\n| 0.0.11 | 2025-02-01 | [53095](https://github.com/airbytehq/airbyte/pull/53095) | Update dependencies |\n| 0.0.10 | 2025-01-25 | [52425](https://github.com/airbytehq/airbyte/pull/52425) | Update dependencies |\n| 0.0.9 | 2025-01-18 | [51969](https://github.com/airbytehq/airbyte/pull/51969) | Update dependencies |\n| 0.0.8 | 2025-01-11 | [51390](https://github.com/airbytehq/airbyte/pull/51390) | Update dependencies |\n| 0.0.7 | 2024-12-28 | [50788](https://github.com/airbytehq/airbyte/pull/50788) | Update dependencies |\n| 0.0.6 | 2024-12-21 | [50336](https://github.com/airbytehq/airbyte/pull/50336) | Update dependencies |\n| 0.0.5 | 2024-12-14 | [49781](https://github.com/airbytehq/airbyte/pull/49781) | Update dependencies |\n| 0.0.4 | 2024-12-12 | [49386](https://github.com/airbytehq/airbyte/pull/49386) | Update dependencies |\n| 0.0.3 | 2024-11-04 | [48233](https://github.com/airbytehq/airbyte/pull/48233) | Update dependencies |\n| 0.0.2 | 2024-10-29 | [47806](https://github.com/airbytehq/airbyte/pull/47806) | Update dependencies |\n| 0.0.1 | 2024-10-10 | | Initial release by [@avirajsingh7](https://github.com/avirajsingh7) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Emitting Data Record in Airbyte JSON Format\nDESCRIPTION: This JSON object represents a RECORD message in Airbyte's data protocol. It includes the record type, stream name, emission timestamp, and the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/namespace_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"data_stream\", \"emitted_at\": 1602637589000, \"data\": { \"field1\" : true }}}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Commands via Docker (Bash)\nDESCRIPTION: Executes standard Airbyte connector commands (`spec`, `check`, `discover`, `read`) within a Docker container using the previously built `airbyte/source-google-sheets:dev` image. It mounts local directories (`secrets`, `integration_tests`) using `-v` flags to provide necessary configuration (`config.json`) and catalog (`configured_catalog.json`) files to the containerized connector. Requires Docker and the built connector image (`airbyte/source-google-sheets:dev`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-sheets:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-sheets:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-sheets:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-sheets:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from Guardian API\nDESCRIPTION: Sample response from the Guardian API showing an article object with its publication date, which is used as the cursor field for incremental syncing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"business/live/2023/apr/15/uk-bosses-more-optimistic-energy-prices-fall-ai-spending-boom-economics-business-live\",\n  \"type\": \"liveblog\",\n  \"sectionId\": \"business\",\n  \"sectionName\": \"Business\",\n  \"webPublicationDate\": \"2023-04-15T07:30:58Z\",\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands for running the full test suite, unit tests, and integration tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-chroma test\npoetry run pytest -s unit_tests\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Modifying Static Schema in Python Stream Class\nDESCRIPTION: Example showing how to override the get_json_schema method to dynamically modify a static schema by adding new properties to the base schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/schemas.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema\n```\n\n----------------------------------------\n\nTITLE: Example SQS Output with Message Body Key\nDESCRIPTION: This JSON example shows the resulting SQS message when using the Message Body Key feature to extract just the 'parent_with_child' object from the input record.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/amazon-sqs.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"child\": \"child_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte Helm Values for GCS Storage in YAML\nDESCRIPTION: Specifies Helm chart values under `global.storage` to configure Airbyte to use Google Cloud Storage (GCS) for log, state, and workload output storage. It sets the storage `type` to `GCS`, references the Kubernetes secret (`secretName`) containing the service account JSON, defines the target GCS bucket name(s) under `bucket`, and specifies the Google Cloud `projectId` and the path within the pod to the mounted credentials file (`credentialsPath`). Requires a pre-existing Kubernetes secret (e.g., `airbyte-config-secrets`) with the GCS credentials under the `gcp.json` key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/storage.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"GCS\"\n    secretName: airbyte-config-secrets\n    bucket: ## GCS bucket names that you've created. We recommend storing the following all in one bucket.\n      log: airbyte-bucket\n      state: airbyte-bucket\n      workloadOutput: airbyte-bucket\n    gcs:\n      projectId: <project-id>\n      credentialsPath: /secrets/gcs-log-creds/gcp.json\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Snapshot Isolation\nDESCRIPTION: SQL command to enable snapshot isolation on the database, which is required for CDC initial sync to avoid table locks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER DATABASE {database name}\n  SET ALLOW_SNAPSHOT_ISOLATION ON;\n```\n\n----------------------------------------\n\nTITLE: Basic HTTP Authenticator Schema and Examples\nDESCRIPTION: Schema and examples for Basic HTTP authentication using username/password credentials encoded in base64.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nBasicHttpAuthenticator:\n  type: object\n  additionalProperties: true\n  required:\n    - username\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    username:\n      type: string\n    password:\n      type: string\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: \"BasicHttpAuthenticator\"\n  username: \"hello\"\n  password: \"world\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: \"BasicHttpAuthenticator\"\n  username: \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Running Firebase-Realtime-Database Connector as Docker Container\nDESCRIPTION: Commands to run the connector's various functions as a Docker container, mounting necessary volumes for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-firebase-realtime-database:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-firebase-realtime-database:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-firebase-realtime-database:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-firebase-realtime-database:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Metabase API Authentication Response\nDESCRIPTION: Example JSON response from the Metabase authentication API containing the session token ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/metabase.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"id\":\"38f4939c-ad7f-4cbe-ae54-30946daf8593\"}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Standard commands for running the Mailersend source connector container, including spec, check, discover, and read operations with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailersend/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailersend:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailersend:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailersend:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailersend:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Source Scaffold Java JDBC connector Docker image\nDESCRIPTION: Command to build the connector Docker image using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scaffold-java-jdbc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-scaffold-java-jdbc:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Connector Metadata Service\nDESCRIPTION: Command to execute the test suite for the Connector Metadata Service library using pytest through Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using Docker CLI\nDESCRIPTION: This command builds a Docker image for the Couchbase source connector using the Dockerfile in the current directory. The image is tagged as 'airbyte/source-couchbase:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\ndocker build . -t airbyte/source-couchbase:dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Datasets Connector in Markdown\nDESCRIPTION: This snippet defines the configuration parameters for the Hugging Face Datasets connector, including dataset name, subsets, and splits.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hugging-face-datasets.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `dataset_name` | `string` | Dataset Name.  |  |\n| `dataset_subsets` | `array` | Dataset Subsets. Dataset Subsets to import. Will import all of them if nothing is provided (see https://huggingface.co/docs/dataset-viewer/en/configs_and_splits for more details) |  |\n| `dataset_splits` | `array` | Dataset Splits. Splits to import. Will import all of them if nothing is provided (see https://huggingface.co/docs/dataset-viewer/en/configs_and_splits for more details) |  |\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Read-Only User\nDESCRIPTION: SQL commands to create a dedicated read-only user for data replication with necessary permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER <user_name> IDENTIFIED BY 'your_password_here';\n```\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO <user_name>;\n```\n\n----------------------------------------\n\nTITLE: Running Commcare Connector Locally with Poetry (Bash)\nDESCRIPTION: Executes various commands for the Commcare source connector using `poetry run`. This includes retrieving the specification (`spec`), checking the connection configuration (`check`), discovering the schema (`discover`), and reading data (`read`). Requires a `secrets/config.json` file for check, discover, and read operations, and a catalog file for the read operation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-commcare spec\npoetry run source-commcare check --config secrets/config.json\npoetry run source-commcare discover --config secrets/config.json\npoetry run source-commcare read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Chargify Source Connector Commands via Docker\nDESCRIPTION: These commands demonstrate how to run standard Airbyte source connector operations (spec, check, discover, read) using the locally built Docker image (`airbyte/source-chargify:dev`). The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing `config.json` using `-v $(pwd)/secrets:/secrets`. The `read` command also requires mounting an `integration_tests` directory containing `configured_catalog.json` using `-v $(pwd)/integration_tests:/integration_tests`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-chargify:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chargify:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chargify:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-chargify:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: YNAB API Streams Schema\nDESCRIPTION: Table defining the available data streams and their properties including primary keys, pagination support, and sync capabilities\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/you-need-a-budget-ynab.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Budgets | id | No pagination | ✅ |  ❌  |\n| Accounts | id | No pagination | ✅ |  ❌  |\n| Categories |  | No pagination | ✅ |  ❌  |\n| Payees |  | No pagination | ✅ |  ❌  |\n| Transactions | id | No pagination | ✅ |  ❌  |\n| Category Groups |  | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Region Creation Response in JSON\nDESCRIPTION: The successful response from the region creation API call, containing the region ID, name, organization ID, status, and timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"regionId\": \"uuid-string\",\n  \"name\": \"region-name\",\n  \"organizationId\": \"org-uuid-string\",\n  \"enabled\": true,\n  \"createdAt\": \"timestamp-string\",\n  \"updatedAt\": \"timestamp-string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Data Examples for New Parquet Union Representation\nDESCRIPTION: Provides examples of how input data maps to the new typed disjoint record structure for unions in Parquet. An integer input results in a record with 'type' set to 'integer' and the 'integer' field populated. An object input results in a record with 'type' set to 'object' and the 'object' field populated with the nested record.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Integer)\n1\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data (Parquet Structure)\n{ \"type\": \"integer\", \"integer\": 1 }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Object)\n{\"id\": 10, \"name\": \"Alice\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data (Parquet Structure)\n{ \"type\": \"object\", \"object\": { \"id\": 10, \"name\": \"Alice\" } }\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with Array Data Types in JSON\nDESCRIPTION: An example of an Airbyte RECORD message containing arrays of different numeric types (numbers, floats, and integers). The message contains metadata including stream name and emission timestamp, followed by the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/number_data_type_array_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"array_number\" : [-12345.678, 100000000000000000.1234],\"array_float\" : [-12345.678, 0, 1000000000000000000000000000000000000000000000000000.1234], \"array_integer\" : [42, 0, 12345]}}}\n```\n\n----------------------------------------\n\nTITLE: Declaring OAuth Spec: Access and Refresh Token Nested in Data Placeholders - YAML\nDESCRIPTION: This snippet demonstrates specifying deeply nested access_token and refresh_token properties inside their respective placeholders under a data root. The path_in_oauth_response arrays define the key hierarchies required. Inputs must have both tokens inside distinct nested objects for correct parsing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_70\n\nLANGUAGE: yaml\nCODE:\n```\n# advanced_auth\\n\\noauth_config_specification:\\n  oauth_connector_input_specification:\\n    consent_url: >-\\n      https://yourconnectorservice.com/oauth/consent?{{client_id_param}}&{{redirect_uri_param}}&{{state_param}}\\n    access_token_url: >-\\n      https://yourconnectorservice.com/oauth/token?{{client_id_param}}&{{client_secret_param}}&{{auth_code_param}}\\n  complete_oauth_output_specification:\\n    required:\\n      - access_token\\n      - refresh_token\\n    properties:\\n      access_token:\\n        type: string\\n        path_in_connector_config:\\n          - access_token\\n        path_in_oauth_response:\\n          - data\\n          - access_token_placeholder\\n          - access_token\\n      refresh_token:\\n        type: string\\n        path_in_connector_config:\\n          - refresh_token\\n        path_in_oauth_response:\\n          - data\\n          - refresh_token_placeholder\\n          - refresh_token\\n\\n  # Other common properties are omitted, see the `More common use-cases` description\\n\n```\n\n----------------------------------------\n\nTITLE: Running Docker commands for Source Scaffold Java JDBC connector\nDESCRIPTION: Various Docker commands to execute connector operations such as spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scaffold-java-jdbc/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-scaffold-java-jdbc:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-scaffold-java-jdbc:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-scaffold-java-jdbc:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-scaffold-java-jdbc:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-dataverse build\n```\n\n----------------------------------------\n\nTITLE: Configuring Error Handler to Ignore 404 HTTP Status in YAML\nDESCRIPTION: Demonstrates configuration to ignore HTTP 404 responses in custom error handling, using a response filter that sets the action to IGNORE for matching HTTP codes. Intended for use in Airbyte's HTTP request error handling configuration block; simplifies silent handling of certain error conditions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: IGNORE\n```\n\n----------------------------------------\n\nTITLE: Running Faker connector in Docker container\nDESCRIPTION: Commands to run Faker connector operations (spec, check, discover, read) in a Docker container with appropriate volume mounts for configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-faker:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-faker:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-faker:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-faker:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Selecting a Field from the Response - YAML\nDESCRIPTION: YAML selector configuration for extracting records from a nested field in an HTTP response, specifically from the 'data' field. The extractor field_path is [\"data\"]. The input should be a JSON object containing a 'data' array; the selector will yield all elements within that array.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  extractor:\n    field_path: [\"data\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring WaitTimeFromHeader Backoff Strategy Example in YAML\nDESCRIPTION: Demonstrates error handler configuration to use the value of the 'wait_time' HTTP header for backoff intervals after an error occurs. This YAML block can be extended with additional options as needed for custom handling in Airbyte's requester.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"\n```\n\n----------------------------------------\n\nTITLE: Defining CompositeErrorHandler Schema for Sequential Error Handlers in YAML\nDESCRIPTION: Schema for CompositeErrorHandler, which mandates an array of subordinate error handlers. Supports additional arbitrary parameters, enabling the composition of complex retry rules that are applied in sequence when handling HTTP errors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nCompositeErrorHandler:\n  type: object\n  required:\n    - error_handlers\n  additionalProperties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    error_handlers:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/ErrorHandler\"\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteControlMessage Schema in YAML\nDESCRIPTION: Schema definition for the AirbyteControlMessage, which enables connectors to signal to the Airbyte Platform that an action with a side-effect should be taken. Includes required properties and references to specialized message types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteControlMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - type\n    - emitted_at\n  properties:\n    type:\n      title: orchestrator type\n      description: \"the type of orchestrator message\"\n      type: string\n      enum:\n        - CONNECTOR_CONFIG\n    emitted_at:\n      description: \"the time in ms that the message was emitted\"\n      type: number\n    connectorConfig:\n      description: \"connector config orchestrator message: the updated config for the platform to store for this connector\"\n      \"$ref\": \"#/definitions/AirbyteControlConnectorConfigMessage\"\n```\n\n----------------------------------------\n\nTITLE: Running the Source-Gitlab Connector in Docker Containers in Bash\nDESCRIPTION: These Bash snippets demonstrate how to execute various connector commands (spec, check, discover, read) in isolated Docker containers using the previously built image. Configuration and catalog files are mounted as Docker volumes to provide required input data. The commands require a built Docker image ('airbyte/source-gitlab:dev'), available config files, and a running Docker daemon. Outputs and logs are shown on the console. The pattern allows for clean execution of the connector outside the development environment, suitable for testing and integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gitlab:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gitlab:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gitlab:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gitlab:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Bucket Policy in JSON\nDESCRIPTION: This JSON snippet defines an AWS IAM policy that grants necessary permissions for the S3 Data Lake connector to interact with the specified S3 bucket. It includes actions for listing buckets, and managing objects within the bucket.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-data-lake.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\",\n        \"s3:GetObject*\",\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket*\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Avro Source Connector in Airbyte\nDESCRIPTION: YAML configuration for setting up an AWS S3 source connector for Avro files in Airbyte. This example demonstrates the required parameters for bucket access, authentication, and Avro-specific configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/config/vocabularies/Airbyte/reject.txt#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Accept the defaults where they make sense for your data\ndataset: s3_data\npath_pattern: \"*.avro\"\ndata_interval: P1D\n\n# S3 bucket\nbucket: airbyte-bucket\naws_access_key_id: \"{{ secrets.AWS_ACCESS_KEY_ID }}\"\naws_secret_access_key: \"{{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n\n# Avro configuration\nformat: avro\n\n# Schema inference\nschemas:\n  - name: users \n    format: avro\n    globs: [\"**\"]\n    filetype: avro\n    legacy_mode: False\n    validation_policy: Emit Record\n    parsing_mode: default\n```\n\n----------------------------------------\n\nTITLE: AWS S3 Policy for Airbyte\nDESCRIPTION: JSON policy to allow Airbyte cluster to communicate with AWS S3 storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"*\" },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:ListBucket\", \"s3:GetBucketLocation\"],\n      \"Resource\": \"arn:aws:s3:::YOUR-S3-BUCKET-NAME\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\",\n        \"s3:GetObject\",\n        \"s3:GetObjectAcl\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::YOUR-S3-BUCKET-NAME/*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Exception Example for Hot Standby Conflict\nDESCRIPTION: Sample error message that occurs when a query is terminated due to conflict with recovery in a PostgreSQL hot standby server configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nCaused by: org.postgresql.util.PSQLException: FATAL: terminating connection due to conflict with recovery\n    Detail: User query might have needed to see row versions that must be removed.\n    Hint: In a moment you should be able to reconnect to the database and repeat your command.\n```\n\n----------------------------------------\n\nTITLE: Configuring Airbyte to Use AWS Secrets Manager - YAML\nDESCRIPTION: This YAML configuration block configures Airbyte's global secrets management setting to use AWS Secrets Manager with a given Kubernetes secret holding the credentials. Specify the manager type (awsSecretManager), secretName (referencing created Kubernetes secret), AWS region, authenticationType (either \\'credentials\\' for explicit keys or \\'instanceProfile\\' for EC2/IAM attachement), optional tags for secret categorization, and optionally a KMS ARN for encryption/decryption. Prerequisite: AWS credentials secret named airbyte-config-secrets. Inputs: AWS region, secretName, authenticationType, optional tags and KMS ARN. Outputs: Configured Airbyte deployment for AWS Secret Manager integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\\n  secretsManager:\\n    type: awsSecretManager\\n    secretName: \\\"airbyte-config-secrets\\\" # Name of your Kubernetes secret.\\n    awsSecretManager:\\n      region: <aws-region>\\n      authenticationType: credentials ## Use \\\"credentials\\\" or \\\"instanceProfile\\\"\\n      tags: ## Optional - You may add tags to new secrets created by Airbyte.\\n        - key: ## e.g. team\\n          value: ## e.g. deployments\\n        - key: business-unit\\n          value: engineering\\n      kms: ## Optional - ARN for KMS Decryption.\\n\n```\n\n----------------------------------------\n\nTITLE: Running Smartsheets Connector as Docker Container\nDESCRIPTION: Series of commands to run various functions of the Smartsheets connector as a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-smartsheets:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smartsheets:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smartsheets:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-smartsheets:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-microsoft-dataverse:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-dataverse:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-dataverse:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-microsoft-dataverse:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Clickhouse Integration Tests with Gradle\nDESCRIPTION: Command to run acceptance and custom integration tests for the Clickhouse connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-clickhouse/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-clickhouse:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Decoding Text Iterable API Response to JSON Records (Airbyte, Text)\nDESCRIPTION: Exhibits text responses where each line represents a logical record (e.g., timestamps). Airbyte transforms each line into a JSON object under a 'record' key, producing an array of these objects. Inputs are newline-separated text; outputs are JSON objects, one per line. This is used for simple log or event streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n2021-04-14 16:52:18 +00:00\n2021-04-14 16:52:23 +00:00\n2021-04-14 16:52:21 +00:00\n2021-04-14 16:52:23 +00:00\n2021-04-14 16:52:27 +00:00\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"record\": \"2021-04-14 16:52:18 +00:00\"\n  },\n  {\n    \"record\": \"2021-04-14 16:52:23 +00:00\"\n  },\n  {\n    \"record\": \"2021-04-14 16:52:21 +00:00\"\n  },\n  {\n    \"record\": \"2021-04-14 16:52:23 +00:00\"\n  },\n  {\n    \"record\": \"2021-04-14 16:52:27 +00:00\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailchimp:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailchimp:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailchimp:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailchimp:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Source Connector\nDESCRIPTION: Command to execute acceptance tests for the source-airbyte connector to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airbyte/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-airbyte test\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Testing Chameleon Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Chameleon source connector using airbyte-ci testing framework.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chameleon/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chameleon test\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with Various Date/Time Formats\nDESCRIPTION: Sample Airbyte RECORD message showing different date and time format representations. Includes date, datetime with and without timezone specification, and time fields with different timezone handling approaches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/every_time_type_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"every_time_type_test_1\", \"emitted_at\": 1721428633000, \"data\": { \"date\": \"2024-07-25\", \"datetime_unspecified\": \"2024-07-25T10:30:00Z\", \"datetime_with_timezone\": \"2024-07-25T10:30:00+02:00\", \"datetime_without_timezone\": \"2024-07-25T10:30:00\", \"time_unspecified\": \"10:30:00Z\", \"time_with_timezone\": \"10:30:00+02:00\", \"time_without_timezone\": \"10:30:00\" }}}\n```\n\n----------------------------------------\n\nTITLE: Airbyte TRACE Message for Stream Status Reporting\nDESCRIPTION: TRACE message with STREAM_STATUS information indicating the completion status of a data stream. Used for monitoring and reporting the progress of data synchronization in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/every_time_type_messages.txt#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"every_time_type_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1721428636000}}\n```\n\n----------------------------------------\n\nTITLE: Building the Source CircleCI Connector Locally using Bash\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image tagged as `source-circleci:dev`. This image can then be used for testing the connector locally. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-circleci/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-circleci build\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Squarespace Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Squarespace connector named 'source-squarespace:dev' for local testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-squarespace/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-squarespace build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container, including volume mounts for secrets and test configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-monday:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-monday:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-monday:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-monday:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Microsoft Entra ID Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Microsoft Entra ID connector using airbyte-ci. Validates the connector's functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-entra-id/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-entra-id test\n```\n\n----------------------------------------\n\nTITLE: Running the spec command for Outreach Connector via Docker\nDESCRIPTION: Executes the `spec` command within a temporary Docker container using the previously built `airbyte/source-outreach:dev` image. This command outputs the connector's specification (spec), defining the required configuration parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-outreach:dev spec\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linkedin-ads test\n```\n\n----------------------------------------\n\nTITLE: Structuring Airbyte Stream Records in JSONL\nDESCRIPTION: This snippet showcases multiple data records formatted as JSON Lines (JSONL), characteristic of output from systems like Airbyte. Each line is an independent JSON object representing a record from the 'Sheet6-2000-rows' stream. Key fields include 'stream' (identifying the data source), 'data' (containing the record payload with 'ID' and 'Name'), and 'emitted_at' (a Unix timestamp indicating when the record was generated).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1526\",\"Name\":\"RuKDwlkiS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1527\",\"Name\":\"UPgwzcYAp\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1528\",\"Name\":\"eaccHNDPK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1529\",\"Name\":\"gUwcqoeal\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1530\",\"Name\":\"fvmyFemxS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1531\",\"Name\":\"CaAnuZbEK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1532\",\"Name\":\"dlTUbaYOf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1533\",\"Name\":\"dfgMhdbMp\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1534\",\"Name\":\"oDdDkAixf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1535\",\"Name\":\"tTOeXirOQ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1536\",\"Name\":\"anZsAKIhr\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1537\",\"Name\":\"DwYwPizjZ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1538\",\"Name\":\"AeQQCWCNz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1539\",\"Name\":\"oKJiWSphf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1540\",\"Name\":\"EpwTRUHON\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1541\",\"Name\":\"cIcscaGPO\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1542\",\"Name\":\"WfCpYkoxk\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1543\",\"Name\":\"TSNAgTxcm\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1544\",\"Name\":\"KJhlRWVrx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1545\",\"Name\":\"DvPMxhxBL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1546\",\"Name\":\"etryAwvVr\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1547\",\"Name\":\"HnujJakzf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1548\",\"Name\":\"KGBWjsCtf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1549\",\"Name\":\"GgDvlPtiz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1550\",\"Name\":\"TyWDsGnCY\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1551\",\"Name\":\"BoOyrNuKa\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1552\",\"Name\":\"XoFFIcIYA\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1553\",\"Name\":\"RwzIWSuVh\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1554\",\"Name\":\"VobCDildL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1555\",\"Name\":\"phlEAszNT\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1556\",\"Name\":\"hUAbkzLFV\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1557\",\"Name\":\"SOisZsHtq\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1558\",\"Name\":\"btKMarFQZ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1559\",\"Name\":\"lTwWtAZMt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1560\",\"Name\":\"WYuhtcvcS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1561\",\"Name\":\"WrrYWfKDC\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1562\",\"Name\":\"HNMhLVrYA\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1563\",\"Name\":\"MnffaxwOT\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1564\",\"Name\":\"sAoPANban\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1565\",\"Name\":\"UeKIRyxJM\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1566\",\"Name\":\"bDiquWMfm\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1567\",\"Name\":\"YeaxWpGeJ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1568\",\"Name\":\"ndLXqMbGS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1569\",\"Name\":\"iVeCmogAu\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1570\",\"Name\":\"lOTXToxUW\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1571\",\"Name\":\"CckkxVXwx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1572\",\"Name\":\"FisxvwFqV\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1573\",\"Name\":\"hHmuBQhzL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1574\",\"Name\":\"VhGmRoCos\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1575\",\"Name\":\"WmUkamrpM\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1576\",\"Name\":\"cwioJmqwE\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1577\",\"Name\":\"iIHDNULrr\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1578\",\"Name\":\"LKQnDjUjX\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1579\",\"Name\":\"FRMyOAoyI\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1580\",\"Name\":\"DGOxKUBLG\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1581\",\"Name\":\"lZOBAnlpv\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1582\",\"Name\":\"MaIboFxHu\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1583\",\"Name\":\"BsdtZLaLS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1584\",\"Name\":\"tmeRlzJwR\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1585\",\"Name\":\"nYzDiiYDN\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1586\",\"Name\":\"DNDVNJXIx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1587\",\"Name\":\"UBWdLOMXy\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1588\",\"Name\":\"GVjvGmhei\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1589\",\"Name\":\"pmaZaFHQC\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1590\",\"Name\":\"RuplVPlwt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1591\",\"Name\":\"NYlsgnTEx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1592\",\"Name\":\"IvCJDkIti\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1593\",\"Name\":\"waNubHkxJ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1594\",\"Name\":\"sMfeTgrJH\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1595\",\"Name\":\"HUHJirbFD\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1596\",\"Name\":\"DoFmcTobK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1597\",\"Name\":\"OCzbLpxHw\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1598\",\"Name\":\"OVUSQtUeL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1599\",\"Name\":\"gluLDWhyx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1600\",\"Name\":\"aezNiwCjL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1601\",\"Name\":\"htsElUgol\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1602\",\"Name\":\"aXVUgEdkJ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1603\",\"Name\":\"QijlDlLvC\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1604\",\"Name\":\"WibRMVUlu\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1605\",\"Name\":\"OKidTaZga\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1606\",\"Name\":\"SrDhBcZwb\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1607\",\"Name\":\"krmZsHuMg\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1608\",\"Name\":\"vzvrLGTZH\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1609\",\"Name\":\"hQTyIKtHW\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1610\",\"Name\":\"CoeftnBoH\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1611\",\"Name\":\"LsxByUjcr\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1612\",\"Name\":\"qrKGSdBEV\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1613\",\"Name\":\"VnSjUihoX\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1614\",\"Name\":\"nMyuMQLQo\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1615\",\"Name\":\"UaONcZETp\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1616\",\"Name\":\"VgGNGIEvM\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1617\",\"Name\":\"MLyOVQhmi\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1618\",\"Name\":\"pEpAmaagz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1619\",\"Name\":\"vrHGUULfW\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1620\",\"Name\":\"rXgQkQDRE\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1621\",\"Name\":\"yPVWSvVss\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1622\",\"Name\":\"LswiPQDbo\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1623\",\"Name\":\"lCsZNduNZ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1624\",\"Name\":\"MRocFayZn\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1625\",\"Name\":\"YmeWdRJEp\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1626\",\"Name\":\"fkHTewQOd\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1627\",\"Name\":\"dGPeAVjbP\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1628\",\"Name\":\"aiupwecpc\"},\"emitted_at\":1673989570000}\n```\n\n----------------------------------------\n\nTITLE: Testing Algolia Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Algolia source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-algolia/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-algolia test\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_definition_config_injection` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_definition_config_injection` table, used to inject specific configurations into actor definitions based on a path. It stores the JSON configuration to inject, the path for injection, the target actor definition ID, and timestamps. The primary key is composite (`actor_definition_id`, `injection_path`), with a foreign key linking to `actor_definition`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name         | Datatype    | Description                                     |\n| ------------------ | ----------- | ----------------------------------------------- |\n| json_to_inject    | JSONB       | JSON configuration to inject.                   |\n| injection_path    | VARCHAR     | Path where the injection applies.               |\n| actor_definition_id | UUID        | Foreign key referencing `actor_definition(id)`.|\n| created_at        | TIMESTAMP   | Timestamp when the record was created.          |\n| updated_at        | TIMESTAMP   | Timestamp when the record was last modified.    |\n\n#### Indexes and Constraints\n\n- Primary Key: (`actor_definition_id`, `injection_path`)\n- Foreign Key: `actor_definition_id` references `actor_definition(id)`\n```\n\n----------------------------------------\n\nTITLE: Defining StockData Connector Streams in Markdown\nDESCRIPTION: Markdown table outlining the available data streams in the StockData connector, including their primary keys, pagination methods, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stockdata.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| news_feeds_per_symbol | uuid | DefaultPaginator | ✅ |  ✅  |\n| news_feeds_per_industry | uuid | DefaultPaginator | ✅ |  ✅  |\n| eod_data | date.ticker | No pagination | ✅ |  ✅  |\n| intraday_unadjusted_data | date.ticker | No pagination | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running Kyve Connector Commands in Docker Container\nDESCRIPTION: Commands to run various Kyve connector operations using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-kyve:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kyve:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kyve:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-kyve:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Destination Connector Interfaces in Pseudocode\nDESCRIPTION: Defines the standard interfaces for an Airbyte destination connector using language-agnostic pseudocode. It includes `spec` to return the connector's configuration specification, `check` to validate a configuration, and `write` to consume a stream of Airbyte messages from standard input and output state messages.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol-docker.md#2025-04-23_snippet_2\n\nLANGUAGE: pseudocode\nCODE:\n```\n```\nspec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\nwrite(Config, AirbyteCatalog, Stream<AirbyteMessage>(stdin)) -> Stream<AirbyteStateMessage>\n```\n```\n\n----------------------------------------\n\nTITLE: Running Langchain Connector Locally in Python\nDESCRIPTION: Commands to run various operations of the Langchain connector locally using Python.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-langchain/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Example for Custom Connector Build - Dockerfile\nDESCRIPTION: Provides a sample Dockerfile for building a custom Airbyte source connector image based on the official connector. It copies integration code into the image and installs it with pip. The image automatically inherits the entrypoint and environment variables from the base image unless overridden. Inputs are local code files and outputs are a runnable Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-google-analytics-v4-service-account-only:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Formatting Multiple JSON Objects in Airbyte Data Stream\nDESCRIPTION: Example of multiple JSON objects in an array from an Airbyte data source, each containing user information with nested name fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/gcs.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running TMDB Source Connector Docker Commands\nDESCRIPTION: Examples of running standard source connector commands using the TMDB source connector docker image. These commands demonstrate how to run spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tmdb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tmdb:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tmdb:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tmdb:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tmdb:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Redis Destination Connector Docker Commands\nDESCRIPTION: Examples of running various connector commands using the built Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redis/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/destination-redis:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-redis:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-redis:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-redis:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands as Docker Containers - bash\nDESCRIPTION: Runs the Github source connector in Docker containers using various Airbyte commands. Mounts local 'secrets' and 'integration_tests' directories as volumes when necessary. The '--rm' flag ensures containers are removed after execution. Requires a built connector image ('airbyte/source-github:dev'). Command parameters specify which configuration and catalog files to use. Each line is a different functional command: 'spec', 'check', 'discover', or 'read'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-github:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-github:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-github:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-github:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Base Image in YAML Metadata\nDESCRIPTION: Example of how to declare the base image in the connector's metadata.yaml file using the connectorBuildOptions.baseImage key with a specific SHA256 digest for build reproducibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/migration-to-base-image.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnectorBuildOptions:\n  baseImage: docker.io/airbyte/python-connector-base:3.0.0@sha256:1a0845ff2b30eafa793c6eee4e8f4283c2e52e1bbd44eed6cb9e9abd5d34d844\n```\n\n----------------------------------------\n\nTITLE: Building Redis Destination Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the Redis destination connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redis/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-redis:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple GA4 Custom Reports\nDESCRIPTION: Example showing how to configure multiple custom reports with different dimensions and metrics for separate analysis streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\"ga:date\", \"ga:pagePath\"],\n    \"metrics\": [\"ga:screenPageViews\", \"ga:totalUsers\"]\n  },\n  {\n    \"name\": \"sessions_by_region\",\n    \"dimensions\": [\"ga:date\", \"ga:region\"],\n    \"metrics\": [\"ga:totalUsers\", \"ga:sessions\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Applying Helm Configuration with values.yaml in Bash\nDESCRIPTION: This Bash command shows how to apply configuration from one or more `values.yaml` files during an Airbyte installation or upgrade using Helm. The `helm upgrade --install` command uses the `-f` flag to specify configuration files, enabling features like unsafe custom components defined within those files. Note the example shows applying the same file twice, which is valid for layering configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/custom-components.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n```bash\nhelm upgrade --install airbyte airbyte/airbyte -f values.yaml -f values.yaml\n```\n```\n\n----------------------------------------\n\nTITLE: Formatting Airbyte State Message in JSON\nDESCRIPTION: This snippet demonstrates the structure of an Airbyte state message. It includes a global state object with a shared state containing a start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_object_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}} }\n```\n\n----------------------------------------\n\nTITLE: Connection Status Table Definition\nDESCRIPTION: Markdown table defining connection status icons and their meanings for monitoring connection health in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/review-connection-status.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Icon                                               | Status      | Description                                                         |\n| -------------------------------------------------- | ----------- | ------------------------------------------------------------------- |\n| ![Healthy](./assets/connection_synced.png)         | **Healthy** | The most recent sync for this connection succeeded                  |\n| ![Failed](./assets/connection_action_required.png) | **Failed**  | The most recent sync for this connection failed                     |\n| ![Running](./assets/connection_syncing.png)        | **Running** | The connection is currently actively syncing                        |\n| ![Paused](./assets/connection_disabled.png)        | **Paused**  | The connection is disabled and isn't scheduled to run automatically |\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for specification, configuration checking, discovery, and data reading\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-appsflyer spec\npoetry run source-appsflyer check --config secrets/config.json\npoetry run source-appsflyer discover --config secrets/config.json\npoetry run source-appsflyer read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Stream Slicing Interface Description for Python\nDESCRIPTION: Stream slices must be implemented by returning a list of dictionaries from the Stream.stream_slices() method. Each dictionary describes a slice and is passed to the read_stream method for processing that slice.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/stream-slices.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nStream.stream_slices() -> List[dict]\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with pytest and Poetry (Bash)\nDESCRIPTION: Explains how to execute unit tests for the connector module using pytest via Poetry from the connector directory. This ensures that tests are run in the controlled Python environment as defined by Poetry, using all declared dependencies. It requires the 'unit_tests' directory to exist and contain valid test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Testing Mendeley Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Mendeley source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mendeley/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mendeley test\n```\n\n----------------------------------------\n\nTITLE: Enabling Unsafe Code via abctl values.yaml in YAML\nDESCRIPTION: This YAML configuration snippet updates the `values.yaml` file for Airbyte deployments managed by `abctl`. It sets the `AIRBYTE_ENABLE_UNSAFE_CODE` environment variable to `true` for both the `workload-launcher` and `connector-builder-server` services, enabling the use of custom Python components. This feature is marked as unsafe and experimental.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/custom-components.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml title=\"values.yaml\"\nworkload-launcher:\n  extraEnv:\n    AIRBYTE_ENABLE_UNSAFE_CODE: true\nconnector-builder-server:\n  extraEnv:\n    AIRBYTE_ENABLE_UNSAFE_CODE: true\n```\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for specification, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-linkedin-ads spec\npoetry run source-linkedin-ads check --config secrets/config.json\npoetry run source-linkedin-ads discover --config secrets/config.json\npoetry run source-linkedin-ads read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Connector Image with airbyte-ci - bash\nDESCRIPTION: This snippet shows how to use the airbyte-ci tool to build the Outbrain Amplify connector Docker image. After running the command, a developer will have a docker image of the connector in the local registry. Prerequisites are the airbyte-ci tool and the Airbyte codebase. The image is named airbyte/source-outbrain-amplify:dev by default.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-outbrain-amplify build\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Read Command via Docker (Bash)\nDESCRIPTION: Executes the `read` command within a temporary Docker container. It mounts the local `secrets` directory (for `config.json`) and the `integration_tests` directory (for `configured_catalog.json`) into the container to read data according to the provided configuration and catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-coingecko-coins:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Two Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema for the two_weekly_active_users stream in the Google Analytics connector. It includes the count of 14-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_14dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Markdown Version History Table\nDESCRIPTION: A markdown table containing version history, dates, pull request references and descriptions of changes made to the Amplitude connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amplitude.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| 0.3.13     | 2024-06-22 | [40108](https://github.com/airbytehq/airbyte/pull/40108) | Update dependencies                                                                                                                                                    |\n| 0.3.12     | 2024-06-06 | [39103](https://github.com/airbytehq/airbyte/pull/39103) | Use `CheckpointMixin` for state management                                                                                                                             |\n| 0.3.11     | 2024-06-04 | [38988](https://github.com/airbytehq/airbyte/pull/38988) | [autopull] Upgrade base image to v1.2.1                                                                                                                                |\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with Negative Timezone Offset\nDESCRIPTION: Another Airbyte RECORD message example with a different set of date/time values, featuring a negative timezone offset (-05:00) in the datetime and time fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/every_time_type_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"every_time_type_test_1\", \"emitted_at\": 1721428634000, \"data\": { \"date\": \"2024-07-26\", \"datetime_unspecified\": \"2024-07-26T15:45:30Z\", \"datetime_with_timezone\": \"2024-07-26T15:45:30-05:00\", \"datetime_without_timezone\": \"2024-07-26T15:45:30\", \"time_unspecified\": \"15:45:30Z\", \"time_with_timezone\": \"15:45:30-05:00\", \"time_without_timezone\": \"15:45:30\" }}}\n```\n\n----------------------------------------\n\nTITLE: Querying All Documents in a Collection using FQL\nDESCRIPTION: This FQL query paginates through all documents in the specified collection. It's used for full syncs to export all data from a Fauna collection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: FQL\nCODE:\n```\nPaginate(Documents(Collection(\"collection-name\")))\n```\n\n----------------------------------------\n\nTITLE: Running Surveycto Connector Commands Locally\nDESCRIPTION: Series of commands to run various connector operations locally, including spec, check, discover, and read functions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-surveycto spec\npoetry run source-surveycto check --config secrets/config.json\npoetry run source-surveycto discover --config secrets/config.json\npoetry run source-surveycto read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record for Message Body Key Configuration\nDESCRIPTION: This JSON example demonstrates the structure of an input record that can be used with the Message Body Key feature, which allows referencing a specific key within the input record to use as the SQS message body.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/amazon-sqs.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parent_with_child\": {\n    \"child\": \"child_value\"\n  },\n  \"parent\": \"parent_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Self-Managed Airbyte\nDESCRIPTION: Configuration setting to disable telemetry data collection in self-managed Airbyte instances by modifying the values.yaml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/telemetry.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nTRACKING_STRATEGY=logging\n```\n\n----------------------------------------\n\nTITLE: Docker Container Operations\nDESCRIPTION: Commands for running the connector operations in a Docker container including spec, check, discover and read functions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mixpanel:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mixpanel:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mixpanel:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mixpanel:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Specifying Scoped Impact for Breaking Changes (YAML)\nDESCRIPTION: This snippet demonstrates the use of the optional `scopedImpact` property within the `breakingChanges` section. It shows how to limit the scope of a breaking change to specific streams (e.g., only the `users` stream) using `scopeType: stream` and listing the `impactedScopes`. This informs users that other streams are unaffected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreleases:\n  breakingChanges:\n    1.0.0:\n      message: \"This version changes the cursor for the `Users` stream. After upgrading, please reset the stream.\"\n      upgradeDeadline: \"2023-12-31\"\n      scopedImpact:\n        - scopeType: stream\n          impactedScopes: [\"users\"]\n```\n\n----------------------------------------\n\nTITLE: Running Duckdb Connector Test Suite\nDESCRIPTION: Command to run the full test suite for the Duckdb connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-duckdb test\n```\n\n----------------------------------------\n\nTITLE: Defining ConnectorSpecification Schema in YAML\nDESCRIPTION: YAML schema definition for ConnectorSpecification that describes connector requirements and capabilities including connection parameters, supported features, and protocol version. This schema is used to validate connector configurations and render user interfaces.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nConnectorSpecification:\n  description: Specification of a connector (source/destination)\n  type: object\n  required:\n    - connectionSpecification\n  additionalProperties: true\n  properties:\n    # General Properties (Common to all connectors)\n    protocol_version:\n      description: \"the Airbyte Protocol version supported by the connector. Protocol versioning uses SemVer.\"\n      type: string\n    documentationUrl:\n      type: string\n      format: uri\n    changelogUrl:\n      type: string\n      format: uri\n    connectionSpecification:\n      description: ConnectorDefinition specific blob. Must be a valid JSON string.\n      type: object\n      existingJavaType: com.fasterxml.jackson.databind.JsonNode\n    # Connector Type Properties (Common to all connectors from same type)\n    # Source Connectors Properties\n    supportsIncremental:\n      description: (deprecated) If the connector supports incremental mode or not.\n      type: boolean\n    # Destination Connectors Properties\n    # Normalization is currently implemented using dbt, so it requires `supportsDBT` to be true for this to be true.\n    supportsNormalization:\n      description: If the connector supports normalization or not.\n      type: boolean\n      default: false\n    supportsDBT:\n      description: If the connector supports DBT or not.\n      type: boolean\n      default: false\n    supported_destination_sync_modes:\n      description: List of destination sync modes supported by the connector\n      type: array\n      items:\n        \"$ref\": \"#/definitions/DestinationSyncMode\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image Using Airbyte CI - Bash\nDESCRIPTION: This command uses the Airbyte CI tool to build a Docker image for the source-google-analytics-data-api connector. The '--name' parameter specifies the connector, and upon success, an image tagged as 'airbyte/source-google-analytics-data-api:dev' will be available locally. Prerequisites include the installation of airbyte-ci and Docker. The resulting image facilitates subsequent containerized execution or deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-data-api build\n```\n\n----------------------------------------\n\nTITLE: Airbyte State Message\nDESCRIPTION: JSON state message format for Airbyte showing state tracking with a start date. Used for maintaining sync state between source and destination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/nan_type_test_message.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"data\": {\"start_date\": \"2022-02-14\"}}}\n```\n\n----------------------------------------\n\nTITLE: Customizing Service Account in values.yaml\nDESCRIPTION: YAML configuration to specify a custom service account name in the values.yaml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\nserviceAccount:\n  name:\n```\n\n----------------------------------------\n\nTITLE: Migrating Data from Old to New Raw Tables in Clickhouse\nDESCRIPTION: SQL script for migrating data from the old raw table structure to the new Destinations V2 format in Clickhouse. It creates a new table in the airbyte_internal database and copies data from the old raw table, mapping the column names to the new schema while preserving the original data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/clickhouse-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- assumes your database was 'default'\n-- replace `{{stream_name}}` with replace your stream name\n\nCREATE TABLE airbyte_internal.default_raw__stream_{{stream_name}}\n(\n    `_airbyte_raw_id` String,\n    `_airbyte_extracted_at` DateTime64(3, 'GMT') DEFAULT now(),\n    `_airbyte_loaded_at` DateTime64(3, 'GMT') NULL,\n    `_airbyte_data` String,\n    PRIMARY KEY(`_airbyte_raw_id`)\n)\nENGINE = MergeTree;\n\nINSERT INTO `airbyte_internal`.`default_raw__stream_{{stream_name}}`\n    SELECT\n        `_airbyte_ab_id` AS \"_airbyte_raw_id\",\n        `_airbyte_emitted_at` AS \"_airbyte_extracted_at\",\n        NULL AS \"_airbyte_loaded_at\",\n        _airbyte_data AS \"_airbyte_data\"\n    FROM default._airbyte_raw_{{stream_name}};\n```\n\n----------------------------------------\n\nTITLE: Listing Files in Airbyte Job Directory (Inside Container)\nDESCRIPTION: Demonstrates the `ls` command used within a container (like the BusyBox container started previously) to list the files inside a specific Airbyte job attempt directory (e.g., `/data/9/2`). This directory corresponds to the mounted `airbyte_workspace` volume.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nls /data/9/2/\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for various operations including spec generation, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-airtable spec\npoetry run source-airtable check --config secrets/config.json\npoetry run source-airtable discover --config secrets/config.json\npoetry run source-airtable read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Applying values.yaml Changes with abctl (Shell)\nDESCRIPTION: Shows the `abctl` command used to apply configuration changes specified in a `values.yaml` file (e.g., disabling authentication) to a local Airbyte deployment managed by `abctl`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --values ./values.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Glassflow Connector as Docker Container\nDESCRIPTION: Commands to run various Glassflow connector operations using the built Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-glassflow:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-glassflow:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/destination-glassflow:dev write --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Airbyte Persona Connector - Bash\nDESCRIPTION: This Bash snippet executes acceptance tests for the Persona connector using the airbyte-ci CLI. It assumes that the source-persona connector has already been built and that the airbyte-ci tool is installed. Acceptance tests validate integration and functional correctness; test output should be reviewed for errors or failures indicating issues with the connector implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-persona/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-persona test\n```\n\n----------------------------------------\n\nTITLE: Requesting Partitioned Order Notes via Path Parameter - Woocommerce API - curl - Bash\nDESCRIPTION: Illustrates how to fetch partitioned order notes from the Woocommerce API using curl, with order IDs injected into the URL path. This pattern is used when user-supplied or config-driven arrays provide the set of partitions to iterate over. The endpoint requires one request per order ID (partition).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/123/notes\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/456/notes\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://example.com/wp-json/wc/v3/orders/789/notes\n```\n\n----------------------------------------\n\nTITLE: Querying Airbyte Docker Image List with abctl - Bash\nDESCRIPTION: This Bash snippet runs the abctl command to fetch a manifest of all Airbyte Docker images for the latest version. abctl is a CLI tool that interfaces with Airbyte deployments and lists available image names and tags. The command outputs a list to standard output; abctl must be installed and authenticated against the Airbyte instance for this to succeed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nabctl images manifest\n```\n\n----------------------------------------\n\nTITLE: Running Typeform Source Connector Commands\nDESCRIPTION: Execute standard source connector commands for Typeform using Docker. These commands include spec, check, discover, and read operations, with appropriate volume mounting for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-typeform/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-typeform:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-typeform:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-typeform:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-typeform:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Parameter Reference in YAML Configuration\nDESCRIPTION: Demonstrates how to reference parameters in YAML configuration using Jinja2 templating. The example shows nested objects with parameter interpolation where 'name' parameter is referenced in a string template.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/string-interpolation.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsome_object:\n  $parameters:\n    name: \"airbyte\"\n  inner_object:\n    key: \"Hello {{ parameters.name }}\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Public Token in Plaid Sandbox\nDESCRIPTION: This curl command creates a public token in the Plaid sandbox environment, which is a prerequisite for generating an access token. It requires your client ID and sandbox API key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/plaid.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://sandbox.plaid.com/sandbox/public_token/create' \\\n    --header 'Content-Type: application/json;charset=UTF-16' \\\n    --data-raw '{\n        \"client_id\": \"<your-client-id>\",\n        \"secret\": \"<your-sandbox-api-key>\",\n        \"institution_id\": \"ins_127287\",\n        \"initial_products\": [\"auth\", \"transactions\"]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Running Connector with State for Partial Failure Testing\nDESCRIPTION: Command to test the connector's handling of partial failures by running it with a state file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py read --config examples/config_localhost.json --catalog examples/configured_catalog.json --state examples/sample_state_full_sync.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-airtable test\n```\n\n----------------------------------------\n\nTITLE: Building the OpenFDA Connector Image with Airbyte-CI (Bash)\nDESCRIPTION: This bash snippet demonstrates how to use the airbyte-ci command-line tool to build a development Docker image for the source-openfda connector. The build process will create a dev image tagged as source-openfda:dev, which is suitable for local testing and development. airbyte-ci should be installed and available in the environment, and the current directory should have the appropriate project files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openfda/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nairbyte-ci connectors --name=source-openfda build\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Connector Operations via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to run standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-goldcast:dev` Docker image. The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing `config.json`, and the `read` command also requires mounting an `integration_tests` directory with `configured_catalog.json`. Each command runs in a temporary, auto-removed container (`--rm`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-goldcast/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-goldcast:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-goldcast:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-goldcast:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-goldcast:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Breezometer connector in Docker, including spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-breezometer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-breezometer:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-breezometer:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-breezometer:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-breezometer:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, and data writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-couchbase spec\npoetry run destination-couchbase check --config secrets/config.json\npoetry run destination-couchbase write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Smartreach Connector in Airbyte\nDESCRIPTION: Command to run acceptance tests for the Smartreach connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartreach/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartreach test\n```\n\n----------------------------------------\n\nTITLE: Ignoring Errors Based on Error Message Content in YAML\nDESCRIPTION: Shows how to configure an error handling response filter to ignore errors when the error message contains a specific substring (here, 'ignorethisresponse'). This behavior is useful for customizing error handling logic based on response payloads in Airbyte error handlers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - error_message_contains: \"ignorethisresponse\"\n          action: IGNORE\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-adjust test\n```\n\n----------------------------------------\n\nTITLE: Displaying Supported Features for Hellobaton API in Markdown\nDESCRIPTION: This code snippet shows a markdown table that outlines the supported features of the Hellobaton API connector in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hellobaton.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| Namespaces        | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands for Aircall Connector\nDESCRIPTION: Standard docker commands for running the Aircall source connector container with various operations including spec, check, discover, and read functionalities\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aircall/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-aircall:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aircall:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aircall:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-aircall:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Increasing Worker Replicas in Airbyte\nDESCRIPTION: YAML configuration for scaling the number of worker replicas in an Airbyte instance. This allows handling more concurrent syncs by increasing the number of available workers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nworker:\n  replicaCount: ## e.g. 2\n```\n\n----------------------------------------\n\nTITLE: Example JSON with Nested Arrays and Records - JSON\nDESCRIPTION: A JSON object where the 'data' field is an array and each element has a nested 'record' object. Used to demonstrate extraction with wildcards in the field_path selector of Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"record\": {\n        \"id\": \"1\"\n      }\n    },\n    {\n      \"record\": {\n        \"id\": \"2\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying abctl Installation\nDESCRIPTION: Command to check if abctl is properly installed by displaying its version. This works on all platforms.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nabctl version\n```\n\n----------------------------------------\n\nTITLE: Visualizing Airbyte Attempt Status State Machine using Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the state machine for a single Airbyte Job Attempt. It shows the two possible terminal states for an attempt that starts in the 'running' state: it can either transition to 'succeeded' or 'failed'. This provides a simplified view focused on the outcome of a single execution attempt within a larger job lifecycle. This visualization requires a Mermaid renderer to be displayed correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/jobs.md#2025-04-23_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ntitle: Attempt Status State Machine\n---\nstateDiagram-v2\n    direction LR\n    running --> succeeded\n    running --> failed\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Postmarkapp Connector with airbyte-ci\nDESCRIPTION: Command to build the docker image for the source-postmarkapp connector using airbyte-ci tool. This creates an image tagged as airbyte/source-postmarkapp:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postmarkapp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-postmarkapp build\n```\n\n----------------------------------------\n\nTITLE: Running Qonto Connector Commands in Docker\nDESCRIPTION: Commands to run standard source connector operations using the built Docker image, including spec retrieval, connection check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qonto/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-qonto:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-qonto:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-qonto:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-qonto:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing 7shifts Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the 7shifts source connector using airbyte-ci tool. Validates connector functionality through predefined test cases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-7shifts/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-7shifts test\n```\n\n----------------------------------------\n\nTITLE: Building source-open-data-dc Connector Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image tagged as `source-open-data-dc:dev`. This image can then be used for testing the connector locally. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-data-dc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-open-data-dc build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Paperform Connector using airbyte-ci (bash)\nDESCRIPTION: Executes the standard Airbyte acceptance tests against the locally built Paperform source connector using the `airbyte-ci` tool. This command requires `airbyte-ci` to be installed and a development image (typically built using the 'build' command) to be available locally. It verifies the connector's compliance with Airbyte protocols.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paperform/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paperform test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Standard commands for running the Lokalise source connector container with various operations including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lokalise/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-lokalise:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lokalise:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-lokalise:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-lokalise:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Python Dependencies via Poetry (Python Bash Command)\nDESCRIPTION: Adds a new dependency to the connector's Python project via Poetry. Replace '<package-name>' with the desired package. This updates the 'pyproject.toml' and 'poetry.lock' files accordingly. Must be run from the project root; outputs file modifications and new dependencies in the virtual environment. Changes to dependency files should be committed to version control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Processing Sheet6 JSON Stream Records\nDESCRIPTION: Stream of JSON records containing sequential ID numbers and randomly generated names. Each record includes a stream identifier, data payload with ID and Name fields, and an emitted_at timestamp of 1673989569000.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1217\",\"Name\":\"IavgETcWE\"},\"emitted_at\":1673989569000}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Genesys Connector\nDESCRIPTION: Command to build a Docker image for the Genesys connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-genesys build\n```\n\n----------------------------------------\n\nTITLE: Using format Filter in Jinja2\nDESCRIPTION: Demonstrates the `format` filter in Jinja2, which interpolates values into a string using Python's string formatting syntax. The example inserts 'world' into 'Hello %s'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_26\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'Hello %s'|format('world') }}\n```\n\n----------------------------------------\n\nTITLE: Running Pinecone Connector Docker Commands\nDESCRIPTION: Commands to run various Pinecone connector operations using the Docker image, including specification, configuration check, and data writing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-pinecone:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-pinecone:dev check --config /secrets/config.json\n# messages.jsonl is a file containing line-separated JSON representing AirbyteMessages\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-pinecone:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Streams Table in Markdown\nDESCRIPTION: Table listing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-campaign.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| recent_campaigns | campaign_key | DefaultPaginator | ✅ |  ❌  |\n| campaign_recipients | contactid, sent_time | DefaultPaginator | ✅ |  ❌  |\n| campaign_details | campaign_name | No pagination | ✅ |  ❌  |\n| campaign_reports | campaign_name | No pagination | ✅ |  ❌  |\n| recent_sent_campaigns | campaign_key | DefaultPaginator | ✅ |  ❌  |\n| mailing_lists | listunino | DefaultPaginator | ✅ |  ❌  |\n| subscribers | contact_email |  DefaultPaginator | ✅ |  ❌  |\n| lists | listkey | No pagination | ✅ |  ❌  |\n| total_contacts |  | No pagination | ✅ |  ❌  |\n| topics | topicId | No pagination | ✅ |  ❌  |\n| all_tags |  | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Key Pair for Oracle Connection\nDESCRIPTION: Command to generate an RSA private key in PEM format for SSH tunnel authentication. The generated key pair includes a private key for Airbyte configuration and a public key for the bastion host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Lokalise connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lokalise/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lokalise build\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Suites with Secrets in YAML\nDESCRIPTION: This YAML configuration enables unit tests and acceptance tests for a connector. It specifies that acceptance tests require a secret named SECRET_SOURCE-FAKER_CREDS, which should be stored in config.json within the connector's secrets directory, using Google Secret Manager as the secret store.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-metadata-file.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n  connectorTestSuitesOptions:\n    - suite: unitTests\n    # This will enable acceptanceTests for this connector\n    # It declares that this test suite requires a secret named SECRET_SOURCE-FAKER_CREDS\n    # In our secret store, and that the secret should be stored in the connector secret folder in a file named config.json\n    - suite: acceptanceTests\n      testSecrets:\n        - name: SECRET_SOURCE-FAKER_CREDS\n          fileName: config.json\n          secretStore:\n            type: GSM\n            alias: airbyte-connector-testing-secret-store\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-only Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This bash snippet demonstrates how to build the Docker image for the source-chargebee connector using the airbyte-ci CLI tool. The airbyte-ci tool must be installed beforehand, and it utilizes the connector name as a parameter. Running this command outputs a local Docker image tagged as airbyte/source-chargebee:dev. Ensure you are in the appropriate project directory and that airbyte-ci dependencies are met.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargebee/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargebee build\n```\n\n----------------------------------------\n\nTITLE: Testing Miro Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Miro source connector using airbyte-ci. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-miro/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-miro test\n```\n\n----------------------------------------\n\nTITLE: Generating ConfiguredAirbyteCatalog from Connector Discovery\nDESCRIPTION: Command to run a connector's discover operation and pipe the output to schema_generator to create a configured catalog file, which is necessary for the schema generation process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/schema_generator/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ../../airbyte-integrations/connectors/<your-connector> # you need to use the tool at the root folder of a connector\n$ docker run --rm -v $(pwd)/secrets:/secrets airbyte/<your-connector-image-name>:dev discover --config /secrets/config.json | schema_generator --configure-catalog\n```\n\n----------------------------------------\n\nTITLE: cURL Request for Next Incremental Sync Based on State - Bash\nDESCRIPTION: This cURL command shows how to construct the next incremental sync API request using the last seen record's webPublicationDate value as the new from-date. This approach ensures only new or updated articles are fetched after the previous cutoff. Dependencies include cURL and internet connectivity. The to-date is set to the current datetime; this request pattern should be repeated on each subsequent sync to enable efficient data ingestion.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/incremental-sync.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl 'https://content.guardianapis.com/search?from-date=2023-04-15T07:30:58Z&to-date={`<now>`}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Image Registry in values.yaml\nDESCRIPTION: YAML configuration to set a custom image registry for Airbyte in the values.yaml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  image:\n    registry: ghcr.io/NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Check Command via Docker (Bash)\nDESCRIPTION: Executes the `check` command within a temporary Docker container using the locally built image. It mounts the local `secrets` directory containing `config.json` into the container and uses it to validate the connection configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coingecko-coins:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Starburst Galaxy Connector with Gradle\nDESCRIPTION: Builds the Docker image for the Starburst Galaxy connector using Gradle. The image name and tag are derived from Dockerfile labels.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-starburst-galaxy/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-starburst-galaxy:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Defining Azure Key Vault Credentials Secret for Kubernetes - YAML\nDESCRIPTION: This Kubernetes Secret manifest defines Opaque secret values for Airbyte on Azure, specifically for Azure Key Vault credentials. The azure-key-vault-client-id and azure-key-vault-client-secret fields are required in the stringData section to allow Airbyte access to Azure Key Vault for secrets storage and retrieval. The secret is named airbyte-config-secrets and must be deployed in advance of using Azure secret management in Airbyte. Inputs: Azure Key Vault client ID and secret. Outputs: A Kubernetes Secret.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/secrets.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: airbyte-config-secrets\\ntype: Opaque\\nstringData:\\n  azure-key-vault-client-id: ## 3fc863e9-4740-4871-bdd4-456903a04d4e\\n  azure-key-vault-client-secret: ## KWP6egqixiQeQoKqFZuZq2weRbYoVxMH\\n\n```\n\n----------------------------------------\n\nTITLE: Building MongoDB Connector Docker Image\nDESCRIPTION: Command to build the connector's Docker image using Gradle. Creates an image tagged as 'airbyte/source-mongodb-v2:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mongodb-v2/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mongodb-v2:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building Campaign Monitor Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Campaign Monitor source connector using airbyte-ci tooling. Creates a dev image tagged as source-campaign-monitor:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-campaign-monitor/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-campaign-monitor build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-mailchimp spec\npoetry run source-mailchimp check --config secrets/config.json\npoetry run source-mailchimp discover --config secrets/config.json\npoetry run source-mailchimp read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Output Schema for The Guardian API Content Items in YAML\nDESCRIPTION: This YAML snippet defines the structure of each content item (news article) returned by The Guardian API. It includes fields such as id, type, sectionId, webPublicationDate, and others.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/the-guardian-api.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{\n    \"id\": \"string\",\n    \"type\": \"string\"\n    \"sectionId\": \"string\"\n    \"sectionName\": \"string\"\n    \"webPublicationDate\": \"string\"\n    \"webTitle\": \"string\"\n    \"webUrl\": \"string\"\n    \"apiUrl\": \"string\"\n    \"isHosted\": \"boolean\"\n    \"pillarId\": \"string\"\n    \"pillarName\": \"string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands inside Docker (Bash)\nDESCRIPTION: Executes various connector commands (`spec`, `check`, `discover`, `read`) within a Docker container using the `airbyte/source-commcare:dev` image. Requires Docker to be installed. Volumes are mounted (`-v`) to provide access to the configuration file (`secrets/config.json`) and catalog file (`integration_tests/configured_catalog.json`) from the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-commcare:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-commcare:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-commcare:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-commcare:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Handling Arrays Stream in JSON\nDESCRIPTION: This JSON record demonstrates handling of different array types in a stream. It includes an array of strings with null values and a nested array within a parent object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"arrays\",\"emitted_at\":1602638599000,\"data\":{\"array_of_strings\":[\"string1\",null,\"string2\",\"string3\"],\"nested_array_parent\":{\"nested_array\":[\"string1\",null,\"string2\"]}}}}\n```\n\n----------------------------------------\n\nTITLE: Building Poplar Connector Development Image using airbyte-ci (Bash)\nDESCRIPTION: This Bash command utilizes the `airbyte-ci` tool to build a local development Docker image tagged `source-poplar:dev`. This image allows developers to test changes to the connector locally before integration or further testing. Requires `airbyte-ci` to be installed and configured in the environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-poplar/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-poplar build\n```\n\n----------------------------------------\n\nTITLE: Running Pagerduty Source Connector as Docker Container (Bash)\nDESCRIPTION: These Bash/Docker commands demonstrate how to interact with the built Pagerduty source connector using standard Airbyte source commands: spec, check, discover, and read. Dependencies include Docker and a valid configuration JSON file generated as per the manifest spec. The read command also expects a configured_catalog.json. Bind mounts are used to pass secrets and integration test catalogs from the host system. Each command outputs the connector's response to stdout and can be chained or executed separately.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pagerduty/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pagerduty:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pagerduty:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pagerduty:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pagerduty:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Snapchat Marketing Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Snapchat Marketing source connector using Docker. They include spec, check, discover, and read operations, with appropriate volume mappings for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-snapchat-marketing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-snapchat-marketing:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-snapchat-marketing:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-snapchat-marketing:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-snapchat-marketing:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Operations Locally\nDESCRIPTION: Commands to run various connector operations like spec, check, discover and read locally\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Hiding Content from Self-Managed using Custom Markdown Extension - Markdown\nDESCRIPTION: Encloses documentation content within `<!-- env:cloud --> ... <!-- /env:cloud -->` tags, ensuring it only appears in Airbyte Cloud and not in Self-Managed UI. No external dependencies are required, but the markdown rendering system must process these environment markers. Key parameter: content visibility. Limitations: Only recognized by Airbyte-aware markdown tooling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- env:cloud -->\\n\\nOnly Cloud builds of the Airbyte UI will render this content.\\n\\n<!-- /env:cloud -->\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Snowflake Column Naming Behavior and V2 Changes in SQL\nDESCRIPTION: This snippet illustrates how Snowflake handles column name casing with and without quotes, and demonstrates the changes in querying columns with special characters after migrating to Airbyte Destinations V2, where column names are explicitly uppercased.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/upgrading_to_destinations_v2.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Snowflake will implicitly uppercase column names which are not quoted\n-- These three queries are equivalent\nSELECT my_column from my_table;\nSELECT MY_COLUMN from MY_TABLE;\nSELECT \"MY_COLUMN\" from MY_TABLE;\n\n-- However, this query is different, and requires a lowercase column name\nSELECT \"my_column\" from my_table;\n\n-- Because we are explicitly upper-casing column names, column names containing special characters (like a space)\n-- should now also be uppercase\n\n-- Before v2\nSELECT \"my column\" from my_table;\n-- After v2\nSELECT \"MY COLUMN\" from my_table;\n```\n\n----------------------------------------\n\nTITLE: Installing Hardcoded Records Connector with Poetry\nDESCRIPTION: Command to install the Hardcoded Records connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Pyenv with Homebrew\nDESCRIPTION: Commands to install pyenv using Homebrew package manager on macOS.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew update\nbrew install pyenv\n```\n\n----------------------------------------\n\nTITLE: Running Connector as Docker Container - Bash\nDESCRIPTION: This set of 'docker run' commands executes the Okta connector's CLI operations within a pre-built Docker image. Volumes are mounted for configuration and test files as needed, and files are referenced via container paths. The '--rm' flag ensures the container is removed after completion. Proper local file paths and Docker permissions are prerequisites.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-okta:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-okta:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-okta:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-okta:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Missive Configuration Table\nDESCRIPTION: Configuration parameters table showing required inputs for the Missive connector including API key, limit, start date and kind parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/missive.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `limit` | `string` | Limit. Max records per page limit | 50 |\n| `start_date` | `string` | Start date.  |  |\n| `kind` | `string` | Kind. Kind parameter for `contact_groups` stream | group |\n```\n\n----------------------------------------\n\nTITLE: Workspace Creation Response in JSON\nDESCRIPTION: The successful response from the workspace creation API call, containing the workspace ID, name, data residency settings, and notification configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"workspaceId\": \"uuid-string\",\n  \"name\": \"workspace-name\",\n  \"dataResidency\": \"auto\",\n  \"notifications\": {\n    \"failure\": {},\n    \"success\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Google Cloud Service Account Key Structure in JSON\nDESCRIPTION: This snippet shows the structure of a Google Cloud service account key file in JSON format. It includes fields such as type, project_id, private_key, client_email, and various URIs required for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/google-sheets.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"PROJECT_ID\",\n  \"private_key_id\": \"KEY_ID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"SERVICE_ACCOUNT_EMAIL\",\n  \"client_id\": \"CLIENT_ID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\"\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Airbyte Job Directory Files from Host Shell via Docker\nDESCRIPTION: Provides an alternative command to list files within the Airbyte job directory (`/data/9/2`) inside the `airbyte_workspace` volume without entering an interactive container shell. It runs `ls` within a temporary BusyBox container and prints the output directly to the host terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --volume airbyte_workspace:/data busybox ls /data/9/2\n```\n\n----------------------------------------\n\nTITLE: Running Onesignal Connector Commands via Docker - Bash\nDESCRIPTION: This set of commands demonstrates how to run various Airbyte source-onesignal connector operations using Docker CLI. Key commands include 'spec' (display configuration schema), 'check' (validate credentials), 'discover' (list available streams), and 'read' (sync data). The commands mount local secrets and integration test catalogs via Docker volumes for secure configuration management. Users must have valid credentials and configuration files available at the referenced locations. Outputs vary by command: configuration spec, connection check result, discovered schemas, or data dumps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onesignal/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-onesignal:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-onesignal:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-onesignal:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-onesignal:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airbyte Credentials\nDESCRIPTION: Command to get the default credentials for accessing your Airbyte instance, including email, password, client-id, and client-secret.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nabctl local credentials\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Airbyte DBT Source Connector - Bash\nDESCRIPTION: This command executes the Airbyte acceptance tests for the `source-dbt` connector using the `airbyte-ci` CLI. The tests validate the connector's functionality and compliance with platform requirements. The command assumes that all environment dependencies (such as test fixtures and required configuration) are properly set up, and that the `airbyte-ci` tool is installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dbt/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dbt test\n```\n\n----------------------------------------\n\nTITLE: Configuring Tinyemail API Connector in Markdown\nDESCRIPTION: Defines the configuration parameter for the Tinyemail API connector. It requires an API key for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tinyemail.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running the Connector CI Test Suite using airbyte-ci (Bash)\nDESCRIPTION: Uses the `airbyte-ci` command-line tool to execute the complete test suite for the `source-google-pagespeed-insights` connector locally. Requires `airbyte-ci` and Docker to be installed. This command runs various tests including build, spec, check, discover, and integration tests, reporting the results.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-pagespeed-insights test\n```\n\n----------------------------------------\n\nTITLE: Resulting XML Message in SQS\nDESCRIPTION: This XML example shows the resulting SQS message when using the Message Body Key feature to extract just the XML content from the 'my_xml_field' property of the input record.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/amazon-sqs.md#2025-04-23_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<something>value</something>\n```\n\n----------------------------------------\n\nTITLE: Disabling Secure Cookies in Airbyte via values.yaml (YAML)\nDESCRIPTION: Provides the YAML configuration snippet for `values.yaml` to disable the `Secure` attribute on Airbyte authentication cookies by setting `global.auth.cookieSecureSetting` to `\"false\"`. This modification is necessary when running Airbyte on a non-localhost domain without HTTPS, but it reduces security.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  auth:\n    cookieSecureSetting: \"false\"\n```\n\n----------------------------------------\n\nTITLE: Former Parquet Representation of Airbyte Union Types\nDESCRIPTION: Illustrates the old method of representing Airbyte union types (`oneOf`) in Parquet format. This involved using anonymous disjoint records where each possible type was a member (e.g., member0, member1), making it harder to determine the actual type present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n// Airbyte Union Type (Integer or String)\n{\"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] }\n```\n\nLANGUAGE: text\nCODE:\n```\n// Old Parquet Type\nOptional Record { member0: Optional Int32, member1: Optional String }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Airbyte Union Type (Boolean or Object)\n{\"oneOf\": [ {\"type\": \"boolean\"}, {\"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" } } } ] }\n```\n\nLANGUAGE: text\nCODE:\n```\n// Old Parquet Type\nOptional Record { member0: Optional Boolean, member1: Optional Record { id: Optional Uint32, name: Optional String } }\n```\n\n----------------------------------------\n\nTITLE: Checking Account Network Policy in Snowflake (SQL)\nDESCRIPTION: SQL command to check if a network policy is currently set at the Snowflake account level. This is useful for verifying if network policies might interfere with Airbyte Cloud connections.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/snowflake.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARAMETERS LIKE 'network_policy' IN ACCOUNT;\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Defines the mapping between Microsoft Teams API data types and Airbyte data types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-teams.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type                 |\n| :--------------- | :--------------------------- |\n| `string`         | `string`                     |\n| `number`         | `number`                     |\n| `date`           | `date`                       |\n| `datetime`       | `timestamp_without_timezone` |\n| `array`          | `array`                      |\n| `object`         | `object`                     |\n```\n\n----------------------------------------\n\nTITLE: Example GAQL Custom Query for Google Ads Source\nDESCRIPTION: This GAQL query demonstrates how to select specific fields (campaign name, total conversions, and conversions by date) from the `ad_group` resource in the Google Ads API. It's intended for use as a custom query within the Airbyte Google Ads source configuration to create a tailored data stream. Note that `segments.date`, if included, is automatically added to the `WHERE` clause, resulting in daily syncing. The query must be valid for all configured customer IDs to avoid skipping accounts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-ads.md#2025-04-23_snippet_0\n\nLANGUAGE: gaql\nCODE:\n```\nSELECT\n    campaign.name,\n    metrics.conversions,\n    metrics.conversions_by_conversion_date\nFROM ad_group\n```\n\n----------------------------------------\n\nTITLE: Describing Schema Changes for TMDb API v1.0.0 in Markdown\nDESCRIPTION: Details the schema changes in version 1.0.0 of the TMDb API integration. It specifies modifications to the 'search_people' and 'search_tv_shows' schemas, including changes to the 'poster_path' and 'overview' properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tmdb-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# TMDb Migration Guide\n\n## Upgrading to 1.0.0\n\nVersion 1.0.0 has a schema change.\n\nThe search_people schema has been changed it's 'type' in schema['properties']['results']['items']['properties']['known_for']['items']['properties']['poster_path'] to be optionally empty\nThe search_tv_shows schema has been changed it's pattern in schema['properties']['results']['items']['properties']['overview'] to contain both strings and spaces.\n```\n\n----------------------------------------\n\nTITLE: Constructing Snapchat OAuth2 Authorization URL\nDESCRIPTION: This code snippet demonstrates how to construct the OAuth2 authorization URL for Snapchat Marketing API. It includes placeholders for client ID and redirect URI, which need to be replaced with actual values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/snapchat-marketing.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://accounts.snapchat.com/login/oauth2/authorize?response_type=code&client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=snapchat-marketing-api&state=wmKkg0TWgppW8PTBZ20sldUwerf-m\n```\n\n----------------------------------------\n\nTITLE: Decoding JSON API Response to Normalized JSON (Airbyte, JSON)\nDESCRIPTION: Demonstrates a typical JSON response body from an API. This is handled natively by Airbyte connectors without any special decoding, as the data is already in the required normalized JSON format for further processing. Key parameters are direct fields of the JSON root or nested elements. No dependencies are required beyond supporting JSON parsing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"cod\": \"200\",\n  \"message\": 0,\n  \"cnt\": 40,\n  \"list\": [\n    {\n      \"dt\": 1728604800,\n      \"main\": {\n        \"temp\": 283.51,\n        \"feels_like\": 283.21,\n        \"temp_min\": 283.51,\n        \"temp_max\": 285.11,\n        \"pressure\": 1014,\n        \"sea_level\": 1014,\n        \"grnd_level\": 982,\n        \"humidity\": 100,\n        \"temp_kf\": -1.6\n      }\n    },\n    {\n      \"dt\": 1728615600,\n      \"main\": {\n        \"temp\": 283.55,\n        \"feels_like\": 283.13,\n        \"temp_min\": 283.55,\n        \"temp_max\": 283.63,\n        \"pressure\": 1014,\n        \"sea_level\": 1014,\n        \"grnd_level\": 983,\n        \"humidity\": 95,\n        \"temp_kf\": -0.08\n      }\n    },\n    ...\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Weaviate Connector Tests\nDESCRIPTION: These commands show how to run the full test suite, unit tests, and integration tests for the Weaviate connector using airbyte-ci and pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-weaviate test\npoetry run pytest -s unit_tests\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for GCS Configuration via kubectl\nDESCRIPTION: This bash command creates a Kubernetes secret for Airbyte configuration with GCS storage using kubectl. It sets various credentials including license key, data plane credentials, AWS/S3 access keys, and GCP credentials file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic airbyte-config-secrets \\\n  --from-literal=license-key='' \\\n  --from-literal=data_plane_client_id='' \\\n  --from-literal=data_plane_client_secret='' \\\n  --from-literal=s3-access-key-id='' \\\n  --from-literal=s3-secret-access-key='' \\\n  --from-literal=aws-secret-manager-access-key-id='' \\\n  --from-literal=aws-secret-manager-secret-access-key='' \\\n  --from-file=gcp.json\n  --namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Running Hubplanner Source Connector Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Hubplanner source connector, including spec, check, discover, and read. These commands demonstrate how to use the connector with configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubplanner/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-hubplanner:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hubplanner:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hubplanner:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-hubplanner:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Default Facebook Insights Query Implementation\nDESCRIPTION: Shows the default implementation for querying Facebook Insights data from an AdAccount, selecting all possible fields with date filtering and grouping by ad and breakdown.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/source_facebook_marketing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect <all possible fields> from AdAccount(me) where start_date = …. and end_date = …. group by ad, <breakdown>\n```\n\n----------------------------------------\n\nTITLE: Running Senseforce connector commands in Docker\nDESCRIPTION: Series of Docker commands for running standard connector operations: spec, check, discover, and read. These commands use the built Docker image and can access local files via volume mounts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-senseforce/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-senseforce:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-senseforce:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-senseforce:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-senseforce:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running SingleStore Connector Docker Commands\nDESCRIPTION: Commands to run various connector operations using the built Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-singlestore/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/destination-singlestore:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-singlestore:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-singlestore:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-singlestore:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Genesys Connector Commands Locally in Python\nDESCRIPTION: Series of commands to run the Genesys connector locally for spec, check, discover, and read operations using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-genesys spec\npoetry run source-genesys check --config secrets/config.json\npoetry run source-genesys discover --config secrets/config.json\npoetry run source-genesys read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Google Webfonts Source Connector\nDESCRIPTION: Series of docker commands to run standard source connector operations including spec, check, discover, and read. These commands use the locally built image and mount necessary directories for configuration and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-webfonts/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-webfonts:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-webfonts:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-webfonts:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-webfonts:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Freshsales Connector Docker Commands\nDESCRIPTION: Standard commands for running the Freshsales source connector in different modes including spec, check, discover, and read. These commands require mounting configuration and test files as volumes when appropriate.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshsales/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-freshsales:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshsales:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshsales:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-freshsales:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running S3 Connector as Docker Container\nDESCRIPTION: Commands to run the S3 connector as a Docker container, mounting local directories for secrets and test configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-s3:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-s3:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-s3:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-s3:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining a DefaultErrorHandler Schema in YAML\nDESCRIPTION: Specifies the configuration for a default error handler, mandating a 'max_retries' field and supporting arrays of 'response_filters' and 'backoff_strategies'. Additional properties can be included, with documented defaults for retries and strategies. Used to instruct how errors and retry logic are processed by Airbyte requesters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nDefaultErrorHandler:\n  type: object\n  required:\n    - max_retries\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    response_filters:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/HttpResponseFilter\"\n    max_retries:\n      type: integer\n      default: 5\n    backoff_strategies:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/BackoffStrategy\"\n      default: []\n```\n\n----------------------------------------\n\nTITLE: Configuring Zoho Bigin Connection Parameters\nDESCRIPTION: Configuration table showing required input parameters for establishing connection with Zoho Bigin, including data center location, OAuth credentials, and refresh token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-bigin.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `data_center` | `string` | Data Center. The data center where the Bigin account's resources are hosted | com |\n| `client_id` | `string` | OAuth Client ID.  |  |\n| `client_secret` | `string` | OAuth Client Secret.  |  |\n| `client_refresh_token` | `string` | Refresh token.  |  |\n```\n\n----------------------------------------\n\nTITLE: Zoho Invoice Data Streams Table\nDESCRIPTION: Comprehensive table listing all available data streams with their primary keys, pagination details, and sync capabilities for the Zoho Invoice integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-invoice.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| items | item_id | DefaultPaginator | ✅ |  ❌  |\n| users |  | DefaultPaginator | ✅ |  ❌  |\n| contacts | contact_id | DefaultPaginator | ✅ |  ❌  |\n| invoices | invoice_id | DefaultPaginator | ✅ |  ❌  |\n| recurring_invoices | recurring_invoice_id | DefaultPaginator | ✅ |  ❌  |\n| customer_payments | payment_id | DefaultPaginator | ✅ |  ❌  |\n| credit notes | creditnote_id | DefaultPaginator | ✅ |  ❌  |\n| expenses | expense_id | DefaultPaginator | ✅ |  ❌  |\n| taxes | tax_id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Creating Airbyte User in Teradata - SQL\nDESCRIPTION: This SQL snippet creates a new user named 'airbyte_user' with a specified permanent space quota and assigns a password, followed by granting all permissions on the 'dbc' database to the new user. It requires access to Teradata with administrative privileges to execute user creation and grant statements. Replace '<password>' with the desired user password, and ensure the 'perm' parameter matches your storage and policy constraints. This is necessary for Airbyte's Teradata connector to have sufficient permissions to create tables, write data, and create schemas.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/teradata.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte_user  as perm=10e6, PASSWORD=<password>;\nGRANT ALL on dbc to airbyte_user;\n\n```\n\n----------------------------------------\n\nTITLE: Running Connector Read Command via Docker (Bash)\nDESCRIPTION: Executes the `read` command within a temporary Docker container (`--rm`) using the `airbyte/source-open-exchange-rates:dev` image. It mounts the local `secrets` directory to `/secrets` and the `integration_tests` directory to `/integration_tests`. The command reads data from the source based on the provided `--config` file and the `--catalog` file (specifying which streams and fields to sync). Requires Docker, the built connector image, a valid `secrets/config.json`, and an `integration_tests/configured_catalog.json` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-open-exchange-rates:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Bearer Token Authenticator Schema and Implementation\nDESCRIPTION: Schema and example for Bearer token authentication, a specialized version of API Key authentication that uses the Authorization header.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nBearerAuthenticator:\n  type: object\n  additionalProperties: true\n  required:\n    - api_token\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    api_token:\n      type: string\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator:\n  type: \"BearerAuthenticator\"\n  api_token: \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Creating GCS Configuration Secrets in Kubernetes\nDESCRIPTION: YAML manifest for creating a Kubernetes secret containing sensitive credentials for Airbyte with Google Cloud Storage (GCS), including license key, database credentials, admin credentials, and GCP service account credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\nstringData:\n  # Enterprise License Key\n  license-key: ## e.g. xxxxx.yyyyy.zzzzz\n\n  # Database Secrets\n  database-host: ## e.g. database.internal\n  database-port: ## e.g. 5432\n  database-name: ## e.g. airbyte\n  database-user: ## e.g. airbyte\n  database-password: ## e.g. password\n\n  # Instance Admin Credentials\n  instance-admin-email: ## e.g. admin@company.example\n  instance-admin-password: ## e.g. password\n\n  # SSO OIDC Credentials\n  client-id: ## e.g. e83bbc57-1991-417f-8203-3affb47636cf\n  client-secret: ## e.g. wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n  # GCP Secrets\n  gcp.json: <CREDENTIALS_JSON_BLOB>\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Locally with Poetry (Bash)\nDESCRIPTION: Executes the `source-declarative-manifest` connector commands locally using `poetry run`. This includes fetching the specification (`spec`), checking the configuration (`check`), discovering the schema (`discover`), and reading data (`read`). Requires a valid `config.json` file (e.g., copied from `integration_tests/pokeapi_config.json` to `secrets/config.json`) and potentially a `configured_catalog.json` (e.g., `sample_files/configured_catalog.json`) for the `read` command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-declarative-manifest spec\npoetry run source-declarative-manifest check --config secrets/config.json\npoetry run source-declarative-manifest discover --config secrets/config.json\npoetry run source-declarative-manifest read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding pg_cron Log Table to Airbyte Publication on Primary PostgreSQL\nDESCRIPTION: SQL command to add the `periodic_log` table (used by the `pg_cron` heartbeat job) to the `airbyte_publication` on the primary PostgreSQL instance. This ensures that the inserts generated by the scheduled job on the primary are included in the publication and replicated, thus advancing the LSN on the replica for CDC.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nALTER PUBLICATION airbyte_publication ADD TABLE periodic_log;\n```\n\n----------------------------------------\n\nTITLE: Example OAuth Token Refresh Response (JSON)\nDESCRIPTION: Shows an example JSON response received from an OAuth token refresh endpoint after successfully exchanging credentials or a refresh token. This response contains the short-lived `access_token` needed for subsequent API requests and its expiry information, here represented by `expires_at`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n {\"access_token\":\"<access-token>\", \"expires_at\": \"2023-12-12T00:00:00\"}\n```\n\n----------------------------------------\n\nTITLE: Running SpaceX API Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the SpaceX API source connector using Docker. They include running spec, check, discover, and read operations with appropriate volume mounts for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-spacex-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-spacex-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-spacex-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-spacex-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-spacex-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the containerized connector for various operations including spec, check, and write operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-chroma:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-chroma:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-chroma:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Sendinblue connector commands in Docker\nDESCRIPTION: Set of Docker commands for running the standard source connector operations including spec, check, discover, and read. These commands allow testing the connector functionality with local configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendinblue/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sendinblue:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sendinblue:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sendinblue:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sendinblue:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector's spec, check, and write operations using Docker\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pgvector/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-pgvector:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-pgvector:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-pgvector:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running BigQuery Destination Connector Docker Commands\nDESCRIPTION: Examples of running various connector commands using the built Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-bigquery:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-bigquery:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-bigquery:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-bigquery:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Aha connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aha/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-aha:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aha:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aha:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-aha:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Dwolla Source Connector (Bash)\nDESCRIPTION: This Bash command uses the `airbyte-ci` tool to run the acceptance tests defined for the Dwolla source connector. This helps ensure the connector functions correctly according to Airbyte's standards. Requires `airbyte-ci` to be installed and the connector source code to be available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dwolla/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dwolla test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Employment Hero Connector\nDESCRIPTION: This command executes the acceptance tests for the Employment Hero connector to verify its functionality and compliance with Airbyte's connector standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-employment-hero/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-employment-hero test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-couchbase test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Gorgias Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Gorgias source connector using the airbyte-ci tool. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gorgias/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gorgias test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Primetric Source Connector in Bash\nDESCRIPTION: Command to run the full test suite for the Primetric source connector using airbyte-ci locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-primetric/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-primetric test\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite Locally using Airbyte-CI - Bash\nDESCRIPTION: Executes the full CI test suite for the source-google-drive connector using airbyte-ci. Ensures comprehensive testing and validation before pushing changes. Requires airbyte-ci to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-drive test\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: Builds the Docker image for the `source-google-sheets` connector using the `airbyte-ci` tool. This command automates the Docker build process based on the connector's configuration. Requires `airbyte-ci` to be installed. The resulting image will be tagged locally as `airbyte/source-google-sheets:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-sheets build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Firebase-Realtime-Database Connector\nDESCRIPTION: Command to execute unit tests for the connector using Poetry and pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Example Array of JSON Objects for S3 Connector\nDESCRIPTION: Example array of JSON objects that would be processed by the Airbyte S3 connector. This shows multiple records with user_id and nested name fields that would be converted to the output format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Building SFTP Connector via Gradle in Java\nDESCRIPTION: Command to build the SFTP source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-sftp:build\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-Only Connector Docker Image with airbyte-ci (bash)\nDESCRIPTION: This command builds the Plausible source connector Docker image locally using the airbyte-ci tool. Requires the airbyte-ci CLI to be installed and available in the system PATH. After execution, a Docker image tagged as airbyte/source-plausible:dev will be present on your host. Input: none; Output: built Docker image for the connector. Limitations: Requires airbyte-ci setup and sufficient system permissions for Docker.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plausible/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-plausible build\n```\n\n----------------------------------------\n\nTITLE: Example UnlimitedCallRatePolicy Usage in YAML\nDESCRIPTION: This YAML snippet shows an example of using the UnlimitedCallRatePolicy within an HTTPAPIBudget. It configures the policy to apply to any GET request directed at 'https://api.example.com' with a path starting with '/sandbox', effectively removing rate limits for these specific requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napi_budget:\n  type: \"HTTPAPIBudget\"\n  policies:\n    - type: \"UnlimitedCallRatePolicy\"\n      # For any GET request on https://api.example.com/sandbox\n      matchers:\n        - method: \"GET\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/sandbox\"\n```\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite with airbyte-ci\nDESCRIPTION: Command to run the full CI test suite for the Slack connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-slack test\n```\n\n----------------------------------------\n\nTITLE: Testing the Recruitee Connector with airbyte-ci in Bash\nDESCRIPTION: Command to run the full test suite for the Recruitee source connector using airbyte-ci. This validates that the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recruitee/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recruitee test\n```\n\n----------------------------------------\n\nTITLE: Generating QA Checks Documentation\nDESCRIPTION: Command to generate documentation for QA checks into a markdown file\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa generate-documentation qa_checks.md\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run various connector operations within a Docker container, including specification, checking, discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-notion:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-notion:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-notion:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-notion:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Freshchat Connector in Bash\nDESCRIPTION: Command to build the source-freshchat connector locally using airbyte-ci, creating a dev image that can be used for testing the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshchat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshchat build\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Manifest-Only Connector using airbyte-ci in Bash\nDESCRIPTION: This snippet demonstrates how to use the airbyte-ci tool to build a Docker image for the source-pendo manifest-only connector. It requires that the airbyte-ci CLI is installed and available in the environment. The command compiles the connector and creates a Docker image tagged as airbyte/source-pendo:dev, which is ready for local development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pendo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pendo build\n```\n\n----------------------------------------\n\nTITLE: Building Cal.com Connector with Airbyte CI\nDESCRIPTION: Command to build the Cal.com connector locally, creating a dev image named 'source-cal-com:dev' for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cal-com/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cal-com build\n```\n\n----------------------------------------\n\nTITLE: Configuring Exchange Rate Stream Tests in Airbyte\nDESCRIPTION: This snippet outlines the setup for testing simple non-nested streams using exchange rate data. It describes two different destination sync modes: incremental + overwrite and incremental + append_dedup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Exchange Rate\n\nThis test suite is focusing on testing a simple stream (non-nested) of data similar to `source-exchangerates` using two different\n`destination_sync_modes`:\n\n- `incremental` + `overwrite` with stream `exchange_rate`\n- `incremental` + `append_dedup` with stream `dedup_exchange_rate`\n\nTo do so, we've setup two streams in the catalog.json and are using the exact same record messages data in both.\n\nNote that we are also making sure that one of the column used as primary key is of type `float` as this could be\nan edge case using it as partition key on certain destinations.\n```\n\n----------------------------------------\n\nTITLE: Airbyte Exchange Rate Record and State Data Format in JSON\nDESCRIPTION: Collection of JSON objects demonstrating Airbyte's data protocol format. Shows exchange rate records with currency conversion data, state messages for tracking synchronization progress, and a trace message indicating stream completion. Each line represents a separate message in the data stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/exchange_rate_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637589000, \"data\": { \"id\": \"1\", \"currency\": \"USD\", \"date\": \"2020-08-29T00:00:00Z\", \"NZD\": \"0.12\", \"HKD\": \"2.13\"}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-08-31\"}}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637689100, \"data\": { \"id\": \"1\", \"currency\": \"USD\", \"date\": \"2020-08-30T00:00:00Z\", \"NZD\": \"1.14\", \"HKD\": \"7.15\"}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-09-01\"}}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637789200, \"data\": { \"id\": \"2\", \"currency\": \"EUR\", \"date\": \"2020-08-31T00:00:00Z\", \"NZD\": \"1.14\", \"HKD\": \"7.15\", \"USD\": \"10.16\"}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637889300, \"data\": { \"id\": \"2\", \"currency\": \"EUR\", \"date\": \"2020-08-31T00:00:00Z\", \"NZD\": \"1.14\", \"HKD\": \"7.99\", \"USD\": \"10.99\"}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637989400, \"data\": { \"id\": \"2\", \"currency\": \"EUR\", \"date\": \"2020-09-01T00:00:00Z\", \"NZD\": \"1.14\", \"HKD\": \"7.15\", \"USD\": \"10.16\"}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-09-02\"}}}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"exchange_rate\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Building the Fastly Source Connector in Bash\nDESCRIPTION: Command to build the source-fastly connector locally, creating a dev image (source-fastly:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fastly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fastly build\n```\n\n----------------------------------------\n\nTITLE: Available Streams Table in Markdown\nDESCRIPTION: Table documenting the available data streams, their primary keys, pagination details, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/youtube-data.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| videos |  | DefaultPaginator | ✅ |  ❌  |\n| video_details |  | DefaultPaginator | ✅ |  ❌  |\n| channels | id | DefaultPaginator | ✅ |  ❌  |\n| comments |  | DefaultPaginator | ✅ |  ❌  |\n| channel_comments | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Configuring GCS External Log Storage for Airbyte in YAML\nDESCRIPTION: This YAML configuration sets up Google Cloud Storage (GCS) as external log storage for Airbyte. It's added to the values.yaml file under the global section.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"GCS\"\n    storageSecretName: airbyte-config-secrets\n    bucket: ## GCS bucket names that you've created. We recommend storing the following all in one bucket.\n      log: airbyte-bucket\n      state: airbyte-bucket\n      workloadOutput: airbyte-bucket\n    gcs:\n      projectId: <project-id>\n```\n\n----------------------------------------\n\nTITLE: Building the Oracle Source Connector Docker Image with Gradle\nDESCRIPTION: This shell command utilizes the Gradle wrapper (`./gradlew`) from the Airbyte repository root to run the `buildConnectorImage` task for the `source-oracle` module. This task builds a Docker image named `airbyte/source-oracle` with the tag `dev`, packaging the connector for containerized deployment. Docker must be installed and running.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oracle/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-oracle:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building the Oracle Source Connector with Gradle\nDESCRIPTION: This shell command uses the Gradle wrapper (`./gradlew`) from the Airbyte repository root to execute the `build` task specifically for the `source-oracle` connector module. This compiles the Java source code and packages the connector artifacts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oracle/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-oracle:build\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Tasks Source Connector in Markdown\nDESCRIPTION: Markdown table showing the configuration parameters for the Google Tasks source connector, including API key, start date, and records limit.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-tasks.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `records_limit` | `string` | Records Limit. The maximum number of records to be returned per request | 50 |\n```\n\n----------------------------------------\n\nTITLE: Configuring StockData Connector Parameters in Markdown\nDESCRIPTION: Markdown table defining the configuration parameters for the StockData connector, including API key, symbols, industries, entity filtering, and start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stockdata.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `symbols` | `array` | Symbols.  |  |\n| `industries` | `array` | Industries. Specify the industries of entities which have been identified within the article. |  |\n| `filter_entities` | `boolean` | By default all entities for each article are returned - by setting this to true, only the relevant entities to your query will be returned with each article. For example, if you set symbols=TSLA and filter_entities=true, only \"TSLA\" entities will be returned with the articles.  | false |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - Bash\nDESCRIPTION: This bash command installs all required dependencies for the connector project using Poetry, including development dependencies, as specified in the project's pyproject.toml file. Poetry must be installed prior to running this command, and it should be executed from the root directory of the connector. No input parameters are needed. Successful execution sets up the project's virtual environment and dependencies, which are prerequisites for running and testing the connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: NASA API Feature Support Matrix\nDESCRIPTION: Markdown table showing supported features of the NASA connector including sync modes and connection types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nasa.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n| SSL connection    | No         |\n| Namespaces        | No         |\n```\n\n----------------------------------------\n\nTITLE: Running Kyriba Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-kyriba spec\npoetry run source-kyriba check --config secrets/config.json\npoetry run source-kyriba discover --config secrets/config.json\npoetry run source-kyriba read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the connector Docker image using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-aws-datalake build\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the `source-goldcast` connector. It requires `airbyte-ci` to be installed. Upon successful execution, a Docker image tagged `airbyte/source-goldcast:dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-goldcast/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-goldcast build\n```\n\n----------------------------------------\n\nTITLE: Running the Initialization Script for Template GitHub Repository - Bash\nDESCRIPTION: This snippet demonstrates how to execute the 'run.sh' shell script to bootstrap a new template GitHub repository. It assumes that the user has already copied the 'github-filler' content to a fresh directory without git history. Upon running, the script will prompt for credentials needed to connect to GitHub. Dependencies include bash, the actual 'run.sh' script, and accessible GitHub API credentials. The user is expected to provide interactive input during execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/fixtures/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./run.sh\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source Connector Commands in Docker - Bash\nDESCRIPTION: These bash commands demonstrate how to execute standard Airbyte source connector operations (spec, check, discover, read) by running the generated Docker image. The commands mount host directories for secrets and configuration files, and issue the desired Airbyte command. Prerequisites include local presence of the previously built Docker image, valid configuration files, and any required integration test catalogs. Outputs are the command-line results for each operation, commonly used in development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convertkit/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-convertkit:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-convertkit:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-convertkit:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-convertkit:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Revolut Merchant Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the source-revolut-merchant connector using airbyte-ci. This creates a dev image (source-revolut-merchant:dev) that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-revolut-merchant/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-revolut-merchant build\n```\n\n----------------------------------------\n\nTITLE: Running Klaviyo Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Klaviyo source connector, including spec, check, discover, and read. These commands use the built Docker image and mount local directories for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaviyo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-klaviyo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klaviyo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klaviyo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-klaviyo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Dixa Source Connector Commands in Docker - Bash\nDESCRIPTION: These Bash snippets illustrate running standard Airbyte source connector commands (`spec`, `check`, `discover`, and `read`) with the locally built `source-dixa` Docker image. Depending on the command, volumes are mounted to provide secure credentials (`/secrets`) and test catalogs (`/integration_tests`). Replace the mounted JSON files with your own configurations as specified in the documentation. Ensures connector operations utilize prepared credentials and configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dixa/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-dixa:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dixa:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dixa:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-dixa:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Date Format for Facebook Marketing API in Markdown\nDESCRIPTION: Example of the date format pattern required for Start Date and End Date fields when configuring a Facebook Marketing source connection. The format follows the ISO 8601 standard.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/facebook-marketing.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nYYYY-MM-DDTHH:mm:ssZ\n```\n\n----------------------------------------\n\nTITLE: Querying Workspace Information via Airbyte API\nDESCRIPTION: This bash command demonstrates how to use curl to query workspace information from the Airbyte API. It includes the necessary headers for authentication and content type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"https://example.com/api/public/v1/workspaces/{workspaceId}\" \\\n  -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Weaviate Connector Build Process\nDESCRIPTION: This Python code snippet shows how to customize the build process by adding pre and post-install steps to modify the base image and connector container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Running Smaily connector commands in Docker\nDESCRIPTION: Set of commands to run various operations with the source-smaily Docker container, including spec, check, discover, and read operations. These commands mount local directories for configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smaily/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-smaily:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smaily:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-smaily:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-smaily:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Source with Definition ID in Terraform\nDESCRIPTION: Creates a custom source resource using definition_id to specify the connector type in Terraform. This approach is necessary when using a custom connector, where the definition ID is extracted from the connector's URL in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_4\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_source_custom\" \"my_source_stripe\" {\n    configuration = jsonencode({\n        \"account_id\" : \"YOUR_STRIPE_ACCOUNT_ID\",\n        \"client_secret\" : \"YOUR_STRIPE_CLIENT_SECRET\",\n        \"start_date\" : \"2023-07-01T00:00:00Z\",\n        \"lookback_window_days\" : 0,\n        \"slice_range\" : 365\n        })\n    name = \"Stripe\"\n    workspace_id = var.workspace_id\n    // highlight-next-line\n    definition_id = \"e094cb9a-26de-4645-8761-65c0c425d1de\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Recharge connector as Docker container\nDESCRIPTION: Commands to run the Recharge connector in a Docker container for various operations including specs, connection check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-recharge:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recharge:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recharge:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-recharge:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Retrieving Logs from a Specific Pod in abctl Cluster (Shell)\nDESCRIPTION: Fetches and displays the logs for a specific pod (e.g., `airbyte-abctl-server-74465db7fd-gk25q`) within the `airbyte-abctl` namespace using `kubectl`. The pod name needs to be replaced with the actual name obtained from `kubectl get pods`. Requires `kubectl` configured for the cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl logs -n airbyte-abctl airbyte-abctl-server-74465db7fd-gk25q\n```\n\n----------------------------------------\n\nTITLE: Building the Eventee connector with airbyte-ci\nDESCRIPTION: Command to build the Eventee connector locally, creating a dev image (source-eventee:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventee/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventee build\n```\n\n----------------------------------------\n\nTITLE: Paginated API Requests for User Records - shell\nDESCRIPTION: These shell command snippets illustrate paginated REST API requests, retrieving user data in pages from a hypothetical user endpoint. Parameters: Replace 'api.com' with the API host and 'page' values as needed. Inputs: page number; outputs: JSON user records. No additional dependencies required, but API authentication or rate limiting may apply. Limitation: Only demonstrates the request pattern; parsing/handling responses is not included.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/resumability.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -x GET api.com/v1/users?page=1\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -x GET api.com/v1/users?page=2\n```\n\n----------------------------------------\n\nTITLE: Building Pagerduty Source Connector Image with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet builds the Pagerduty connector Docker image using the airbyte-ci utility. The prerequisite is having airbyte-ci installed (see referenced Airbyte documentation). The main parameter is the --name switch to specify the connector. Running this command yields a Docker image tagged as airbyte/source-pagerduty:dev on the local machine. No additional arguments are required; ensure you are in the appropriate directory and have Docker available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pagerduty/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pagerduty build\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 JSON Source Connector in Airbyte\nDESCRIPTION: YAML configuration for setting up an AWS S3 source connector for JSON files in Airbyte. This example shows all required parameters including bucket details, authentication credentials, and JSON-specific settings like decoder options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/config/vocabularies/Airbyte/reject.txt#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Accept the defaults where they make sense for your data\ndataset: s3_data\npath_pattern: \"*.json\"\ndata_interval: P1D\n\n# S3 bucket\nbucket: airbyte-bucket\naws_access_key_id: \"{{ secrets.AWS_ACCESS_KEY_ID }}\"\naws_secret_access_key: \"{{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n\n# JSON configuration\nformat: jsonl\n\n# Decoder options\ndecode_initial_buffer_size_bytes: 1048576\ndecode_buffer_growth_factor: 2.0\ndecode_buffer_max_size_bytes: 104857600\nmax_record_size_bytes: 104857600\nskip_field_names_validation: false\n\n# Schema inference\nschemas:\n  - name: users \n    format: jsonl\n    globs: [\"**\"]\n    filetype: jsonl\n    legacy_mode: False\n    validation_policy: Emit Record\n    parsing_mode: default\n    columns:\n      - name: user_id\n        data_type: integer\n        constraints:\n          nullable: false\n      - name: name\n        data_type: string\n      - name: date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Schema Diff Example: Problematic Type Mismatch (subject)\nDESCRIPTION: Shows a potentially problematic difference between the declared and detected schemas for the `subject` field. The declared schema expects a `number`, but the test data contained `string` values, indicating a likely incompatibility that could cause errors during syncs if not corrected in the declared schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_15\n\nLANGUAGE: diff\nCODE:\n```\n     \"subject\": {\n-      \"type\": \"number\"\n+      \"type\": \"string\"\n     }\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Data Types in Airbyte Streams\nDESCRIPTION: Sample JSON records showing different data type handling in Airbyte streams including strings, numbers, integers, booleans, and binary data. Also includes state tracking and stream status traces with completion notifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_basic_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"foo bar\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"some random special characters: ࠈൡሗ\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"56.78\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"number_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"-12345.678\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"42\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"-12345\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"boolean_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : true }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"binary_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"dGVzdA==\" }}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"string_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"number_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"integer_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"boolean_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"binary_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Viewing Airbyte Catalog File Content via Docker\nDESCRIPTION: Demonstrates how to display the contents of the `catalog.json` file located within a specific job attempt directory (`/data/9/2`) inside the `airbyte_workspace` volume. It uses `cat` executed within a temporary BusyBox container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --volume airbyte_workspace:/data busybox cat /data/9/2/catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the source-easypost Connector Image - Bash\nDESCRIPTION: This snippet shows how to use the Airbyte CI command-line tool to build a development Docker image for the source-easypost connector. It requires the airbyte-ci tool to be installed and accessible in your environment. The --name parameter specifies the connector to build, generating a dev image which facilitates local testing and development workflows.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-easypost/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-easypost build\n```\n\n----------------------------------------\n\nTITLE: Building Appfigures Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Appfigures source connector using airbyte-ci tool. Creates a source-appfigures:dev image for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appfigures/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appfigures build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests with `airbyte-ci` (Bash)\nDESCRIPTION: The preferred command for executing unit, integration, and acceptance tests for a specific Airbyte connector using the `airbyte-ci` tool. Replace `<name-of-your-connector>` with the actual connector name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=<name-of-your-connector></name-of-your-connector> test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Freshdesk Connector\nDESCRIPTION: Command to run the full test suite for the Freshdesk source connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshdesk/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshdesk test\n```\n\n----------------------------------------\n\nTITLE: Defining Tremendous API Streams in Markdown\nDESCRIPTION: Markdown table listing the available data streams for the Tremendous connector, including their primary keys, pagination method, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tremendous.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| orders | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ❌  |\n| funding_sources | id | DefaultPaginator | ✅ |  ❌  |\n| account_members | id | DefaultPaginator | ✅ |  ❌  |\n| campaigns | id | DefaultPaginator | ✅ |  ❌  |\n| exchange_rates |  | DefaultPaginator | ✅ |  ❌  |\n| organizations | id | DefaultPaginator | ✅ |  ❌  |\n| balance_transactions |  | DefaultPaginator | ✅ |  ❌  |\n| rewards | id | DefaultPaginator | ✅ |  ❌  |\n| invoices | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Trustpilot Source Connector Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Trustpilot source connector, including spec, check, discover, and read. These commands use the locally built Docker image and require configuration files in the secrets directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trustpilot/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-trustpilot:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-trustpilot:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-trustpilot:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-trustpilot:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: CDC Excluded Records JSON\nDESCRIPTION: Change Data Capture records with deletion tracking and LSN (Log Sequence Number) handling. Tests deletion behavior and CDC functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_incremental.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"dedup_cdc_excluded\",\"data\":{\"id\":5,\"name\":\"vw\",\"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\",\"_ab_cdc_updated_at\":1623849314663,\"_ab_cdc_lsn\":26975264,\"_ab_cdc_deleted_at\":null},\"emitted_at\":1623860160}}\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests using Pytest and Poetry in Bash\nDESCRIPTION: Executes the integration tests located in the `unit_tests/integration` directory using `pytest`. It assumes the project uses `poetry` for dependency management and environment activation. This command is used to verify the test implementation against the mocked API behavior.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests/integration\n```\n\n----------------------------------------\n\nTITLE: Testing Campaign Monitor Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Campaign Monitor source connector using airbyte-ci tooling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-campaign-monitor/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-campaign-monitor test\n```\n\n----------------------------------------\n\nTITLE: Running Source Dremio Connector Commands in Docker - Bash\nDESCRIPTION: This sequence of Bash commands demonstrates how to run standard Airbyte source connector operations such as spec, check, discover, and read, inside a Docker container. These commands assume that the airbyte/source-dremio:dev image is built and that configuration files exist locally under the secrets and integration_tests folders. The commands use host directory mounting with -v for config and catalog file access. Each command cleans up the container after execution with --rm.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dremio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-dremio:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dremio:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dremio:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-dremio:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Starting Airbyte Server with Debugging Enabled (Bash)\nDESCRIPTION: Command to launch Airbyte services using Docker Compose, specifically enabling remote JVM debugging for the `server` container. It sets the `DEBUG_SERVER_JAVA_OPTIONS` environment variable with JDWP agent parameters and instructs Docker Compose to use both `docker-compose.yaml` and the debug-specific `docker-compose.debug.yaml` file. The `VERSION=dev` variable is also passed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDEBUG_SERVER_JAVA_OPTIONS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005\" VERSION=\"dev\" docker compose -f docker-compose.yaml -f docker-compose.debug.yaml up\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Detecting Outdated Records in Snowflake Iceberg Tables\nDESCRIPTION: SQL query to identify outdated records in Snowflake Iceberg tables by creating a flag based on row numbering over primary key partitions ordered by cursor and extraction timestamp. This helps mitigate Snowflake's limitation with native Iceberg row-level deletes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-data-lake.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nrow_number() over (partition by {primary_key} order by {cursor}, _airbyte_extracted_at)) != 1 OR _ab_cdc_deleted_at IS NOT NULL as is_outdated;\n```\n\n----------------------------------------\n\nTITLE: Copying Catalog File from Airbyte Container to Host\nDESCRIPTION: Shows how to use the `docker cp` command to copy the `catalog.json` file from a specific path (`/tmp/workspace/9/2/catalog.json`) within the running `airbyte-server` container to the current directory (`.`) on the host machine. The subsequent `cat` command displays the copied file's content on the host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/workspace/9/2/catalog.json .\ncat catalog.json\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Shows the mapping between Zendesk Chat data types and their corresponding Airbyte types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-chat.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Inspecting Docker Container IP Addresses (Bash)\nDESCRIPTION: A `docker inspect` command used to retrieve the names, images, and IP addresses of all currently running Docker containers. This is particularly useful on macOS (when using `docker-mac-net-connect`) for finding the correct IP address needed to configure the remote debugger connection in IntelliJ for a specific container like `airbyte-server`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ docker inspect $(docker ps -q ) --format='{{ printf \"%-50s\" .Name}} {{printf \"%-50s\" .Config.Image}} {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n/airbyte-proxy                                     airbyte/proxy:dev                                  172.18.0.10172.19.0.4\n/airbyte-server                                    airbyte/server:dev                                 172.18.0.9\n/airbyte-worker                                    airbyte/worker:dev                                 172.18.0.8\n/airbyte-source                                    sha256:5eea76716a190d10fd866f5ac6498c8306382f55c6d910231d37a749ad305960 172.17.0.2\n/airbyte-connector-builder-server                  airbyte/connector-builder-server:dev               172.18.0.6\n/airbyte-webapp                                    airbyte/webapp:dev                                 172.18.0.7\n/airbyte-cron                                      airbyte/cron:dev                                   172.18.0.5\n/airbyte-temporal                                  airbyte/temporal:dev                               172.18.0.2\n/airbyte-db                                        airbyte/db:dev                                     172.18.0.4172.19.0.3\n/airbyte-temporal-ui                               temporalio/web:1.13.0                              172.18.0.3172.19.0.2\n```\n\n----------------------------------------\n\nTITLE: Running Twitter Source Connector Commands\nDESCRIPTION: Demonstrates how to run standard source connector commands using the built Docker image. Includes examples for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twitter/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-twitter:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twitter:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twitter:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-twitter:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Bamboo HR Connector Image using airbyte-ci in Bash\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the `source-bamboo-hr` connector locally. It requires `airbyte-ci` to be installed and assumes the command is run from a context where the tool can locate the connector's source. The resulting Docker image will be tagged as `airbyte/source-bamboo-hr:dev` on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bamboo-hr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bamboo-hr build\n```\n\n----------------------------------------\n\nTITLE: Building Docker image with airbyte-ci\nDESCRIPTION: Command to build a Docker image for the connector using the airbyte-ci tool, which results in an image tagged as 'airbyte/source-gcs:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gcs build\n```\n\n----------------------------------------\n\nTITLE: Exiting and Reconnecting to EC2 Instance - Shell\nDESCRIPTION: Logs out of the current SSH session and reconnects to the EC2 instance using a private key and appropriate username. Replace 'ec2-user-key.pem' and the IP with your actual key file and instance address. This step is needed for updated group permissions to take effect.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexit\nssh -i ec2-user-key.pem ec2-user@1.2.3.4\n```\n\n----------------------------------------\n\nTITLE: Tracking Connector Versions and Changes in Markdown Changelog\nDESCRIPTION: The changelog Markdown table documents version history, release dates, associated pull requests, and change summaries for the Linear connector. This enables users and maintainers to track updates, bug fixes, and new features. The structured format ensures transparency and historical record-keeping in the Airbyte platform context.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linear.md#2025-04-23_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\\n|---------|------|--------------|---------|\\n| 0.0.3 | 2025-04-19 | [58215](https://github.com/airbytehq/airbyte/pull/58215) | Update dependencies |\\n| 0.0.2 | 2025-04-12 | [57669](https://github.com/airbytehq/airbyte/pull/57669) | Update dependencies |\\n| 0.0.1 | 2025-04-11 | [#57586](https://github.com/airbytehq/airbyte/pull/57586) | Initial release by [@natikgadzhi](https://github.com/natikgadzhi) |\n```\n\n----------------------------------------\n\nTITLE: Building Kafka Connector Docker Image\nDESCRIPTION: Command to build the connector docker image using Gradle, resulting in an image tagged as airbyte/destination-kafka:dev\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kafka/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-kafka:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building the Paystack Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the Paystack source connector. It requires `airbyte-ci` to be installed and generates a local Docker image tagged as `airbyte/source-paystack:dev` for development purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paystack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paystack build\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Vector Store with LangChain in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a LangChain QA chain using Pinecone as the vector store. It sets up the necessary components including OpenAI embeddings, Pinecone index, and the RetrievalQA chain.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/langchain.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import OpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Pinecone\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pinecone\nimport os\n\nembeddings = OpenAIEmbeddings()\npinecone.init(api_key=os.environ[\"PINECONE_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\nindex = pinecone.Index(\"<your pinecone index name>\")\nvector_store = Pinecone(index, embeddings.embed_query, \"text\")\n\nqa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever())\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Snapchat Marketing Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Snapchat Marketing source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-snapchat-marketing/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-snapchat-marketing test\n```\n\n----------------------------------------\n\nTITLE: Group-Permission Mapping Configuration in YAML\nDESCRIPTION: YAML configuration that defines what Airbyte permissions each company group should receive, including workspace and scope specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/role-mapping.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{\n  \"companyGroup1\": [\n    {\n      \"scope\": \"workspace\",\n      \"scopeId\": \"11111111-11111111-11111111-11111111\",\n      \"permissionType\": \"workspace_admin\"\n    },\n    {\n      \"scope\": \"workspace\",\n      \"scopeId\": \"22222222-22222222-22222222-22222222\",\n      \"permissionType\": \"workspace_reader\"\n    }\n  ],\n  \"companyGroup2\": [\n    {\n      \"scope\": \"workspace\",\n      \"scopeId\": \"33333333-33333333-33333333-33333333\",\n      \"permissionType\": \"workspace_reader\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite locally using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zapier-supported-storage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zapier-supported-storage test\n```\n\n----------------------------------------\n\nTITLE: Running Wrike Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running spec, check, discover and read operations using the Wrike connector docker container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wrike/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-wrike:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-wrike:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-wrike:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-wrike:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to RSS Source Connector\nDESCRIPTION: Command to add a new dependency to the RSS source connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Testing Partnerize Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet demonstrates how to run acceptance tests for the 'source-partnerize' connector using the airbyte-ci command-line tool. It assumes airbyte-ci is installed and the build step has been completed. The command runs tests on the specified connector and outputs test results to the console, allowing developers to verify correctness and compatibility before further deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-partnerize/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-partnerize test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Survey Sparrow Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Survey Sparrow source connector. The resulting image will be available on the host with the tag 'airbyte/source-survey-sparrow:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-survey-sparrow/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-survey-sparrow build\n```\n\n----------------------------------------\n\nTITLE: Installing Convex Connector Dependencies with Poetry\nDESCRIPTION: Command to install the Convex connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Auto-Merge Package Locally\nDESCRIPTION: Commands for installing the auto-merge package using Poetry and running it locally. It requires setting the GITHUB_TOKEN environment variable and optionally the AUTO_MERGE_PRODUCTION variable for actual merging.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/auto_merge/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\nexport GITHUB_TOKEN=<your_github_token>\n# By default no merge will be done, you need to set the AUTO_MERGE_PRODUCTION environment variable to true to actually merge the PRs\npoetry run auto-merge\n```\n\n----------------------------------------\n\nTITLE: Building the Mock Source Connector with Gradle - Shell\nDESCRIPTION: This snippet demonstrates how to build the mock source connector using Gradle from the Airbyte repository root. It requires Gradle to be installed and assumes the working directory is the repository root. The build command compiles the Java connector, creates required artifacts, and prepares the connector for local development and testing. Successful execution results in a built Java-based source connector inside the Airbyte ecosystem.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n```\n./gradlew :airbyte-integrations:connectors:source-e2e-test:build\n```\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source Connector Operations via Docker (Bash)\nDESCRIPTION: Demonstrates the execution of standard Airbyte commands for the 'source-polygon-stock-api' Docker image, including 'spec', 'check', 'discover', and 'read'. Requires the image to exist locally (tagged ':dev'), and depending on the command, a secrets/config.json file mapped into the container. Some commands also mount a configured_catalog.json for read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-polygon-stock-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-polygon-stock-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-polygon-stock-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-polygon-stock-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-polygon-stock-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Querying Book Information from Gutendex API in JSON\nDESCRIPTION: This snippet shows the JSON structure returned by the Gutendex API when querying book information. It includes the total count, pagination URLs, and an array of book objects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gutendex.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"count\": <number>,\n    \"next\": <string or null>,\n    \"previous\": <string or null>,\n    \"results\": <array of Books>\n}\n```\n\n----------------------------------------\n\nTITLE: Converting JSON oneOf Constraint to Avro Union Type\nDESCRIPTION: Illustrates how a JSON schema `oneOf` constraint (allowing `string` or `integer`) is converted into a less stringent Avro union type. The resulting Avro schema allows the field to be `null`, a `string`, or an `int`, encompassing the original JSON types plus nullability.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": [\"null\", \"string\", \"int\"]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Docker Image Registry for Airbyte (YAML, Airbyte)\nDESCRIPTION: This snippet configures Airbyte to pull all images, including both platform and connector images, from a specified custom image registry in the `values.yaml` file. Setting `global.image.registry` to a custom URL (e.g., `ghcr.io/NAMESPACE`) allows organizations to use private or organization-specific registries. Make sure the referenced registry contains all the required Airbyte images for successful deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.4.md#2025-04-23_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nglobal:\n  image:\n    registry: ghcr.io/NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Running Teradata Connector Docker Commands\nDESCRIPTION: Examples of running various connector commands using the built Docker image. These commands demonstrate how to run spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-teradata/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/destination-teradata:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-teradata:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-teradata:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-teradata:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Formbricks Connector with airbyte-ci\nDESCRIPTION: Command to build the Formbricks source connector locally, creating a development image called 'source-formbricks:dev' for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-formbricks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-formbricks build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector operations locally including specification, connection checks, discover schema, and reading data from the source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config sample_files/config.json\npython main.py discover --config sample_files/config.json\npython main.py read --config sample_files/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Avni Source Connector Unit Tests Using Gradle - Shell\nDESCRIPTION: Runs all unit tests for the Avni source connector using Gradle. Should be executed from the Airbyte project's root directory. Produces test reports indicating the health and correctness of the connector codebase before any integration or acceptance testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-avni:unitTest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Microsoft Teams source connector docker image using airbyte-ci. Creates an image tagged as airbyte/source-microsoft-teams:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-teams/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-teams build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations within a Docker container, including volume mounting for configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-amazon-sqs:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-sqs:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-sqs:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-amazon-sqs:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Zendesk Chat Migration\nDESCRIPTION: Structured documentation explaining the migration process for Zendesk Chat connector version 1.0.0, including the new subdomain requirement and step-by-step upgrade instructions for both Airbyte Open Source and Cloud platforms.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-chat-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Zendesk Chat Migration Guide\n\n## Upgrading to 1.0.0\n\nThe Live Chat API [changed its URL structure](https://developer.zendesk.com/api-reference/live-chat/introduction/) to use the Zendesk subdomain.\nThe `subdomain` field of the connector configuration is now required. \nYou can find your Zendesk subdomain by following instructions [here](https://support.zendesk.com/hc/en-us/articles/4409381383578-Where-can-I-find-my-Zendesk-subdomain).\n\n### For Airbyte Open Source: Update the local connector image\n\nAirbyte Open Source users must manually update the connector image in their local registry before proceeding with the migration. To do so:\n\n1. Select **Settings** in the main navbar.\n    - Select **Sources**.\n2. Find Zendesk Chat in the list of connectors.\n\n:::note\nYou will see two versions listed, the current in-use version and the latest version available.\n:::\n\n3. Select **Change** to update your OSS version to the latest available version.\n\n### For Airbyte Cloud: Update the connector version\n\n1. Select **Sources** in the main navbar.\n2. Select the instance of the connector you wish to upgrade.\n\n:::note\nEach instance of the connector must be updated separately. If you have created multiple instances of a connector, updating one will not affect the others.\n:::\n\n3. Select **Upgrade**\n    - Follow the prompt to confirm you are ready to upgrade to the new version.\n```\n\n----------------------------------------\n\nTITLE: Running Paystack Connector Commands via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to execute standard Airbyte connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-paystack:dev` Docker image. The `check`, `discover`, and `read` commands require mounting a secrets directory containing `config.json`, and the `read` command also needs a configured catalog file mounted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paystack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-paystack:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-paystack:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-paystack:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-paystack:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating a Read-Only Role for Multiple Collections in Fauna\nDESCRIPTION: This FQL query creates a role with read permissions for multiple collections ('users' and 'products'). Used when you need to sync data from multiple collections.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true },\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"users\"),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"products\"),\n      actions: { read: true },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Lead Forms Time Range Query Parameter\nDESCRIPTION: Example of LinkedIn API time range query parameter format used for Lead Forms, showing limitation with incremental sync implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsubmittedAtTimeRange=(start:1711407600000,end:1711494000000)\n```\n\n----------------------------------------\n\nTITLE: Running the Connector as a Docker Container (Docker Bash Commands)\nDESCRIPTION: Executes the Google-Directory source connector within a Docker container, supporting multiple operational commands (spec, check, discover, read) with optional volume mounts for configuration and integration testing. Assumes that a Docker image 'airbyte/source-google-directory:dev' exists locally. Environment paths and config files must be mapped as required; output is produced as command-line execution results or data dumps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-directory:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-directory:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-directory:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-directory:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Mailgun Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the Mailgun connector in Docker, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailgun/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailgun:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailgun:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailgun:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailgun:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example Record without Timezone for Ambiguous Schema in JSON\nDESCRIPTION: An example record showing a timestamp string *without* timezone information. This format is also valid for a standard JSON schema `{\"type\": \"string\", \"format\": \"date-time\"}`, further demonstrating the ambiguity resolved by Airbyte's `airbyte_type` field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"created_at\": \"2021-11-22T01:23:45\"}\n```\n\n----------------------------------------\n\nTITLE: Running Omnisend Source Connector via Docker - Bash\nDESCRIPTION: These bash commands demonstrate how to execute various Airbyte connector operations ('spec', 'check', 'discover', 'read') using the built Docker image for the Omnisend source. They map the required secrets and (optionally) integration test directories as Docker volumes, ensuring that configuration and test data are accessible. Each command uses different flags to reference configs and catalogs, and all commands assume prior creation of a valid secrets/config.json file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-omnisend/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-omnisend:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-omnisend:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-omnisend:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-omnisend:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Granting replication privileges for CDC in PostgreSQL\nDESCRIPTION: SQL command to grant the REPLICATION privilege to a user for enabling Change Data Capture functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER <user_name> REPLICATION;\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpoint Intervals\nDESCRIPTION: Example showing how to configure interval-based state checkpointing in a stream class. Sets the checkpoint interval to save state every 100 records.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/incremental-stream.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nclass MyAmazingStream(Stream):\n  # Save the state every 100 records\n  state_checkpoint_interval = 100\n```\n\n----------------------------------------\n\nTITLE: Running connector as Docker container\nDESCRIPTION: Commands to run the connector's various operations (spec, check, discover, read) as a Docker container, mounting local directories for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gcs:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gcs:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gcs:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gcs:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Source Configuration Specification\nDESCRIPTION: Example reference to Exchange Rates source configuration specification file that defines required connection parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/basic-concepts.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Example from source-exchange-rates/spec.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Gutendex Source Connector Commands in Docker\nDESCRIPTION: Commands to run various operations for the Gutendex source connector in a Docker container. These include specifying the connector, checking the configuration, discovering the schema, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gutendex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gutendex:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gutendex:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gutendex:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gutendex:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example S3 Staging Path Usage in Airbyte Documentation (Text)\nDESCRIPTION: Demonstrates the file path structure used for storing temporary Iceberg staging tables on S3. This sample path helps users understand where interim data is located before being committed to the Starburst Galaxy catalog. Inputs represent placeholder parameters for bucket name, bucket path, source schema/namespace, temp table name, and stream name. Output is a well-formed S3 URI. No dependencies besides S3 and a correct Airbyte sync configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/starburst-galaxy.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ns3://<bucket-name>/<bucket-path>/<namespace/schema>/<temp Iceberg table name {_airbyte_tmp_random-three-chars_stream-name}>\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record and State Message Format Examples\nDESCRIPTION: Example JSON messages showing Airbyte's record format for infinity values and state tracking. Includes record messages with Infinity/-Infinity values and a state message with start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/infinity_type_test_message.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"infinity\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"Infinity\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"infinity\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"-Infinity\" }}}\n{\"type\": \"STATE\", \"state\": { \"data\": {\"start_date\": \"2022-02-14\"}}}\n```\n\n----------------------------------------\n\nTITLE: Making GET Request to List Workspaces\nDESCRIPTION: Example API request to list all workspaces in the Airbyte instance. This request requires the access token to be included as a Bearer Token in the authorization header.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/api-access-config.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nGET <YOUR_WEBAPP_URL>/api/public/v1/workspaces\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Source-Gitlab Connector with Airbyte CI in Bash\nDESCRIPTION: This Bash snippet builds the Docker image for the Airbyte Gitlab source connector using the airbyte-ci tool. The command takes the connector name as a parameter and triggers the build process, resulting in a Docker image tagged as 'airbyte/source-gitlab:dev'. Dependencies include a working installation of airbyte-ci and relevant source files. It must be run from the connector directory or with paths correctly set. Outputs are the built Docker image, with logs visible on the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gitlab build\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - Bash\nDESCRIPTION: Installs all required dependencies for the Zendesk-Support source connector’s development environment using Poetry's dev option. This command ensures that all standard and development dependencies specified in pyproject.toml are installed in an isolated environment. Python (≈3.9) and Poetry (≈1.7) must be pre-installed for this to work. No input or output files are directly involved, and the result is a prepared development environment with all dependencies resolved.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Defining StreamDescriptor Schema in YAML\nDESCRIPTION: YAML schema definition for a StreamDescriptor, which contains information required to identify a Stream including name (required) and namespace (optional). This structure is used when referring to streams across the Airbyte platform.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nStreamDescriptor:\n  type: object\n  additionalProperties: true\n  required:\n    - name\n  properties:\n    name:\n      type: string\n    namespace:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: Aircall API Request Example\nDESCRIPTION: Example of a GET request to the Aircall API endpoint for retrieving numbers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aircall.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.aircall.io/v1/numbers\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Tyntec SMS source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tyntec-sms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tyntec-sms build\n```\n\n----------------------------------------\n\nTITLE: Running CI test suite for Faker connector\nDESCRIPTION: Command to run the full test suite for the Faker connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-faker test\n```\n\n----------------------------------------\n\nTITLE: Managing Dependencies with Poetry - bash\nDESCRIPTION: Adds a new package dependency to the Github source connector project using Poetry. Replace '<package-name>' with the desired dependency name. Updates the project's dependency files ('pyproject.toml' and 'poetry.lock'). Should be run from the root of the connector directory after activating the appropriate environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Emailoctopus Source Connector Commands in Docker\nDESCRIPTION: Commands to run the standard source connector operations (spec, check, discover, read) on the Emailoctopus connector Docker container. These commands mount local directories to access configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-emailoctopus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-emailoctopus:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-emailoctopus:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-emailoctopus:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-emailoctopus:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Upgrading to the Latest Open Source Community Release using Helm\nDESCRIPTION: Command to upgrade an existing Airbyte deployment to the latest Open Source Community release using Helm, which is a prerequisite for upgrading to Self-Managed Enterprise.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrading-from-community.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade [RELEASE_NAME] airbyte/airbyte\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Read Command via Docker (Bash)\nDESCRIPTION: Executes the `read` command within a Docker container to read data from the source based on the provided configuration (`/secrets/config.json`) and catalog (`/integration_tests/configured_catalog.json`). Requires Docker, the connector image (`airbyte/source-google-pagespeed-insights:dev`), a valid config file, and a configured catalog file, mounted from local directories (`secrets` and `integration_tests`). Outputs Airbyte messages (records, state).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-pagespeed-insights:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Tremendous API Connector in Markdown\nDESCRIPTION: Markdown table defining the configuration parameters for the Tremendous connector. It specifies the API key input and environment selection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tremendous.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. You can generate an API key through the Tremendous dashboard under Team Settings &gt; Developers. Save the key once you've generated it. |  |\n| `environment` | `string` | Environment.  |  |\n```\n\n----------------------------------------\n\nTITLE: Creating User and Granting Permissions in PostgreSQL\nDESCRIPTION: SQL commands to create a user with necessary permissions for Airbyte in PostgreSQL. This includes creating a user, granting database privileges, and enabling the vector extension.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/pgvector.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte_user WITH PASSWORD '<password>';\nGRANT CREATE, TEMPORARY ON DATABASE <database> TO airbyte_user;\n```\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION vector;\n```\n\n----------------------------------------\n\nTITLE: SQL Query to Filter Valid Rows Without Changes in Postgres\nDESCRIPTION: Example SQL query for Postgres that selects only rows without any changes recorded in the _airbyte_meta column, which is useful for downstream processing of clean data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/core-concepts/typing-deduping.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- postgres syntax\nSELECT COUNT(*) FROM _table_ WHERE json_array_length(_airbyte_meta ->> changes) = 0\n```\n\n----------------------------------------\n\nTITLE: Running CI test suite for Shopify connector\nDESCRIPTION: Command to execute the full CI test suite for the Shopify connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shopify test\n```\n\n----------------------------------------\n\nTITLE: Parent Stream Reading Records Concurrently Within Stream Slices in Python\nDESCRIPTION: The `stream_slices` generator yields all parent records by iterating through slices and reading each parent record for the substream. It is a workaround for substreams in a partially concurrent setup. Relies on the parent stream's ability to generate slices and records and expects Airbyte `SyncMode`. Limitation: full concurrency for substreams is not enabled with this technique.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    def stream_slices(self, stream_state: Mapping[str, Any] = None, **kwargs) -> Iterable[Optional[Mapping[str, any]]]:\\n        for _slice in self._parent_stream.stream_slices():\\n            for parent_record in self._parent_stream.read_records(sync_mode=SyncMode.full_refresh, stream_slice=_slice):\\n                yield parent_record\n```\n\n----------------------------------------\n\nTITLE: Dynamically Modifying Static JSON Schemas in Python\nDESCRIPTION: This code snippet demonstrates how to override the get_json_schema method to modify a statically defined schema with dynamically determined properties. It first loads the schema using the parent class implementation, then adds a new property before returning the modified schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/source_python_http_tutorial/schemas/TODO.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema\n```\n\n----------------------------------------\n\nTITLE: Example JSON Object Structure for S3 Connector\nDESCRIPTION: Example of a JSON object that would be processed by the Airbyte S3 connector. This shows a typical record with a user_id and nested name field containing first and last name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running MySQL Connector Integration Tests\nDESCRIPTION: Command to execute acceptance and custom integration tests for the MySQL connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mysql:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Error Handling Filters in YAML\nDESCRIPTION: Demonstrates configuring multiple response filters in an error handler, where different HTTP codes (e.g., 404 and 429) are assigned distinct actions (IGNORE or RETRY). This snippet exemplifies multi-condition error handling strategies for HTTP requesters in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: IGNORE\n        - http_codes: [ 429 ]\n          action: RETRY\n```\n\n----------------------------------------\n\nTITLE: Defining AirbyteTraceMessage Schema in YAML\nDESCRIPTION: Schema definition for the AirbyteTraceMessage, which allows Actors to emit metadata about runtime, including errors or estimates. The schema includes required properties like type and emitted_at, and optional properties for specific message types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/airbyte-protocol.md#2025-04-23_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nAirbyteTraceMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - type\n    - emitted_at\n  properties:\n    type:\n      title: \"trace type\" # this title is required to avoid python codegen conflicts with the \"type\" parameter in AirbyteMessage. See https://github.com/airbytehq/airbyte/pull/12581\n      description: \"the type of trace message\"\n      type: string\n      enum:\n        - ERROR\n        - ESTIMATE\n    emitted_at:\n      description: \"the time in ms that the message was emitted\"\n      type: number\n    error:\n      description: \"error trace message: the error object\"\n      \"$ref\": \"#/definitions/AirbyteErrorTraceMessage\"\n    estimate:\n      description: \"Estimate trace message: a guess at how much data will be produced in this sync\"\n      \"$ref\": \"#/definitions/AirbyteEstimateTraceMessage\"\nAirbyteErrorTraceMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - message\n  properties:\n    message:\n      description: A user-friendly message that indicates the cause of the error\n      type: string\n    internal_message:\n      description: The internal error that caused the failure\n      type: string\n    stack_trace:\n      description: The full stack trace of the error\n      type: string\n    failure_type:\n      description: The type of error\n      type: string\n      enum:\n        - system_error\n        - config_error\nAirbyteEstimateTraceMessage:\n  type: object\n  additionalProperties: true\n  required:\n    - name\n    - type\n  properties:\n    name:\n      description: The name of the stream\n      type: string\n    type:\n      description: The type of estimate\n      type: string\n      enum:\n        - STREAM\n        - SYNC\n    namespace:\n      description: The namespace of the stream\n      type: string\n    row_estimate:\n      description: The estimated number of rows to be emitted by this sync for this stream\n      type: integer\n    byte_estimate:\n      description: The estimated number of bytes to be emitted by this sync for this stream\n      type: integer\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Python Virtual Environment - Python\nDESCRIPTION: These shell commands illustrate how to set up and activate a Python 3.7+ virtual environment using the built-in venv module, then install dependencies via pip based on requirements.txt. Ensure Python is installed and available on your PATH. After executing, your shell session will use the local .venv for all Python commands, and package installations are isolated from the system environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Promoting Connector Version to Latest in Registry\nDESCRIPTION: Command to promote a specific connector version to be the latest version in the registry. Useful for mocking a registry where a prerelease connector is treated as published. Warning: removes files in the latest folder that aren't in the versioned folder.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTARGET_BUCKET=<YOUR-DEV_BUCKET> CONNECTOR=\"airbyte/source-stripe\" VERSION=\"3.17.0-dev.ea013c8741\" poetry run poe promote-connector-to-latest\n```\n\n----------------------------------------\n\nTITLE: Running the Discover Operation via Docker (Bash)\nDESCRIPTION: Executes the 'discover' command for the Gocardless source connector within a Docker container using the 'airbyte/source-gocardless:dev' image. Similar to the 'check' command, it mounts the 'secrets' directory and uses the 'config.json' file to discover the available data schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gocardless:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-Only Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the Gnews manifest-only source connector using the airbyte-ci command-line tool. It assumes airbyte-ci is installed and your working directory is properly set. The key parameter is --name, which specifies the connector to build. The output is a locally available Docker image tagged as airbyte/source-gnews:dev. Ensure your environment is set up for Docker builds and you have required privileges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gnews/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gnews build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Gutendex Source Connector\nDESCRIPTION: Command to build the Docker image for the Gutendex source connector using airbyte-ci. This creates an image tagged as airbyte/source-gutendex:dev on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gutendex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gutendex build\n```\n\n----------------------------------------\n\nTITLE: Building Gorgias Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Gorgias source connector for local testing. It uses the airbyte-ci tool to create a Docker image tagged as 'source-gorgias:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gorgias/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gorgias build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for The Guardian API Source Connector\nDESCRIPTION: Command to build the docker image for the source-the-guardian-api connector using airbyte-ci. This creates an image with the tag airbyte/source-the-guardian-api:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-the-guardian-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-the-guardian-api build\n```\n\n----------------------------------------\n\nTITLE: Building BoldSign Connector with Airbyte CI\nDESCRIPTION: Command to build a development version of the BoldSign source connector using airbyte-ci. Creates a dev image tagged as source-boldsign:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-boldsign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-boldsign build\n```\n\n----------------------------------------\n\nTITLE: Building the source-countercyclical Connector using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-countercyclical` connector. This is necessary for local testing and development. Requires `airbyte-ci` to be installed. The resulting image will be tagged as `source-countercyclical:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-countercyclical/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-countercyclical build\n```\n\n----------------------------------------\n\nTITLE: Building Lemlist Connector Docker Image using Bash\nDESCRIPTION: This command uses the 'airbyte-ci' tool to build the Docker image for the 'source-lemlist' connector. It requires 'airbyte-ci' to be installed. Upon successful execution, a Docker image tagged 'airbyte/source-lemlist:dev' will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lemlist/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lemlist build\n```\n\n----------------------------------------\n\nTITLE: Testing the FireHydrant Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the FireHydrant connector to validate its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firehydrant/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firehydrant test\n```\n\n----------------------------------------\n\nTITLE: Building Zoom Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Zoom connector docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoom/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoom build\n```\n\n----------------------------------------\n\nTITLE: Building Luma Source Connector in Bash\nDESCRIPTION: Command to build a development image of the Luma source connector using airbyte-ci. Creates a dev image tagged as source-luma:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-luma/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-luma build\n```\n\n----------------------------------------\n\nTITLE: Building Clickhouse Connector Docker Image\nDESCRIPTION: Command to build the connector Docker image using Gradle. Creates an image tagged as airbyte/destination-clickhouse:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-clickhouse/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-clickhouse:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building the Oura Source Connector Docker Image with Airbyte-CI - Bash\nDESCRIPTION: This Bash command builds the Docker image for the manifest-only `source-oura` connector through the airbyte-ci tool. It assumes that `airbyte-ci` is installed and available in the environment. The resulting Docker image will be tagged as `airbyte/source-oura:dev` for usage in subsequent development and testing steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oura/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oura build\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Rendering Database-Specific Migration Generators in JSX\nDESCRIPTION: This JSX snippet renders custom React components, likely defined in './destinations_v2.js', within the MDX documentation page. Each component (`SnowflakeMigrationGenerator`, `BigQueryMigrationGenerator`, `RedshiftMigrationGenerator`, `PostgresMigrationGenerator`) is responsible for displaying content specific to upgrading the corresponding database destination to Airbyte Destinations V2. This might include code examples, configuration steps, or migration scripts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/upgrading_to_destinations_v2.md#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<SnowflakeMigrationGenerator />\n<BigQueryMigrationGenerator />\n<RedshiftMigrationGenerator />\n<PostgresMigrationGenerator />\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Redis Destination Connector\nDESCRIPTION: Gradle command to run acceptance and custom integration tests for the Redis destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redis/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-redis:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running connector commands locally with Poetry\nDESCRIPTION: Series of commands to run the connector locally for specification, configuration checking, schema discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-gcs spec\npoetry run source-gcs check --config secrets/config.json\npoetry run source-gcs discover --config secrets/config.json\npoetry run source-gcs read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Salesloft connector commands in Docker\nDESCRIPTION: Standard commands for running the Salesloft source connector as a Docker container, including spec, check, discover, and read operations. These commands show how to mount local directories and pass configuration files to the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesloft/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-salesloft:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-salesloft:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-salesloft:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-salesloft:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Destination with Strongly Typed Syntax in Terraform\nDESCRIPTION: Creates a BigQuery destination using strongly typed configuration in Terraform. This approach uses a specialized resource type with jsonencode() to handle the service account credentials more cleanly, while using native HCL syntax for the rest of the configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_7\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_destination_bigquery\" \"my_destination_bigquery\" {\n    configuration = {\n        destination_type       = \"BigQuery\"\n        credentials_json       = jsonencode({\n            \"type\"                        = \"service_account\",\n            \"project_id\"                  = \"YOUR_PROJECT_ID\",\n            \"private_key_id\"              = \"YOUR_PRIVATE_KEY_ID\",\n            \"private_key\"                 = \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\",\n            \"client_email\"                = \"you@example.iam.gserviceaccount.com\",\n            \"client_id\"                   = \"YOUR_CLIENT_ID\",\n            \"auth_uri\"                    = \"https://accounts.google.com/o/oauth2/auth\",\n            \"token_uri\"                   = \"https://oauth2.googleapis.com/token\",\n            \"auth_provider_x509_cert_url\" = \"https://www.googleapis.com/oauth2/v1/certs\",\n            \"client_x509_cert_url\"        = \"https://www.googleapis.com/robot/v1/metadata/x509/you@example.iam.gserviceaccount.com\"\n        })\n        dataset_id              = \"YOUR_DATASET_ID\"\n        dataset_location        = \"us-central1\"\n        loading_method          = {\n            batched_standard_inserts = {}\n        }\n        project_id              = \"YOUR_PROJECT_ID\"\n        transformation_priority = \"batch\"\n    }\n    name         = \"BigQuery\"\n    workspace_id = var.workspace_id\n}\n```\n\n----------------------------------------\n\nTITLE: Testing the OpenFDA Connector with Airbyte-CI (Bash)\nDESCRIPTION: This bash snippet shows how to run acceptance tests for the source-openfda connector using the airbyte-ci tool. The command executes the relevant test suite, verifying the connector's compliance and functional requirements. Ensure that dependencies and test environments are correctly configured before execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openfda/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nairbyte-ci connectors --name=source-openfda test\n```\n\n----------------------------------------\n\nTITLE: Building the Freightview Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the source-freightview connector using airbyte-ci. This creates a dev image (source-freightview:dev) that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freightview/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freightview build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running the connector locally to test specification, configuration checks, and write operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Instatus Source Connector\nDESCRIPTION: Command to build the Docker image for the Instatus source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-instatus:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instatus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-instatus build\n```\n\n----------------------------------------\n\nTITLE: Building Height Source Connector with airbyte-ci\nDESCRIPTION: This command builds a dev image (source-height:dev) for local testing of the connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-height/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-height build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Hubplanner Source Connector\nDESCRIPTION: Command to run the full test suite for the Hubplanner source connector using airbyte-ci. This is used to verify changes and ensure the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubplanner/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hubplanner test\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Desk Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zoho Desk connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-desk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-desk test\n```\n\n----------------------------------------\n\nTITLE: Running Pardot Connector Commands via Docker (Bash)\nDESCRIPTION: Demonstrates how to run standard Airbyte source connector commands ('spec', 'check', 'discover', 'read') using the built Docker image ('airbyte/source-pardot:dev'). The 'check', 'discover', and 'read' commands require mounting a local 'secrets' directory (containing 'config.json') to '/secrets' inside the container. The 'read' command additionally mounts an 'integration_tests' directory (containing 'configured_catalog.json') for the catalog file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pardot/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pardot:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pardot:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pardot:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pardot:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Defines the mapping between Microsoft Dataverse data types and Airbyte types, showing how different data types are converted during synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-dataverse.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type   | Airbyte Type              | Notes                 |\n| :----------------- | :------------------------ | :-------------------- |\n| `String`           | `string`                  |                       |\n| `UniqueIdentifier` | `string`                  |                       |\n| `DateTime`         | `timestamp with timezone` |                       |\n| `Integer`          | `integer`                 |                       |\n| `BigInt`           | `integer`                 |                       |\n| `Money`            | `number`                  |                       |\n| `Boolean`          | `boolean`                 |                       |\n| `Double`           | `number`                  |                       |\n| `Decimal`          | `number`                  |                       |\n| `Status`           | `integer`                 |                       |\n| `State`            | `integer`                 |                       |\n| `Virtual`          | None                      | We skip virtual types |\n```\n\n----------------------------------------\n\nTITLE: Testing the Connector Using Airbyte CI CLI - Shell\nDESCRIPTION: This command runs the Airbyte-provided CI test suite for the connector 'source-e2e-test'. It ensures the connector passes Airbyte's standardized compliance, integration, and acceptance tests before being considered for release. Requires the Airbyte CLI tooling and all relevant test dependencies to be available in the environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n```\nairbyte-ci connectors --name=source-e2e-test test\n```\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Omnisend Connector with airbyte-ci - Bash\nDESCRIPTION: This bash command runs the complete test suite for the Omnisend source connector using airbyte-ci. It requires airbyte-ci installation and a developed Omnisend connector in the current environment. The --name flag targets the specific connector, ensuring that only its tests are executed. Output from this command verifies build integrity and functionality before contribution or release.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-omnisend/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-omnisend test\n```\n\n----------------------------------------\n\nTITLE: Running Confluence Connector Commands via Docker (Bash)\nDESCRIPTION: Executes standard Airbyte connector commands (`spec`, `check`, `discover`, `read`) within ephemeral Docker containers using the locally built `airbyte/source-confluence:dev` image. The `check`, `discover`, and `read` commands require mounting a local `secrets` directory (containing `config.json`) into the container. The `read` command additionally requires mounting an `integration_tests` directory (containing `configured_catalog.json`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-confluence/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-confluence:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-confluence:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-confluence:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-confluence:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Former Handling of Typeless Arrays in Airbyte\nDESCRIPTION: Illustrates the previous behavior for arrays defined without an `items` field (`\"type\": \"array\"`). Airbyte formerly attempted to infer a schema (often defaulting to string) and converted all array elements to strings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"array\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Schema\n{ \"type\": \"array\", \"items\": [ \"null\", \"string\" ] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Data\n[\"1\", \"Alice\"]\n```\n\n----------------------------------------\n\nTITLE: Running Clazar Source Connector Commands via Docker using Bash\nDESCRIPTION: Demonstrates running standard Airbyte source connector commands (`spec`, `check`, `discover`, `read`) using the previously built `airbyte/source-clazar:dev` Docker image. The `check`, `discover`, and `read` commands require mounting a local `secrets` directory containing `config.json` to `/secrets` in the container. The `read` command also requires mounting an `integration_tests` directory containing `configured_catalog.json`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clazar/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-clazar:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clazar:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clazar:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-clazar:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image via Gradle - Bash\nDESCRIPTION: This snippet shows the command to build the Docker image for the DynamoDB connector using Gradle. The resulting Docker image will be named 'airbyte/source-dynamodb:dev' on the local system. Users must have both Docker and Gradle installed in their environment before running this command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-dynamodb:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Defining SparkPost Data Streams in Markdown\nDESCRIPTION: Lists the available data streams for the SparkPost connector, including their primary keys, pagination details, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sparkpost.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| message_events | event_id | DefaultPaginator | ✅ |  ❌  |\n| sending_domains | domain | No pagination | ✅ |  ❌  |\n| ab_test | id | No pagination | ✅ |  ❌  |\n| templates | id | No pagination | ✅ |  ❌  |\n| recipients | id | No pagination | ✅ |  ❌  |\n| subaccounts | id | DefaultPaginator | ✅ |  ❌  |\n| snippets | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Greenhouse Connector as Docker Container\nDESCRIPTION: These commands demonstrate how to run various Greenhouse connector operations using the Docker container, including spec retrieval, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-greenhouse:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-greenhouse:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-greenhouse:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-greenhouse:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Ip2whois Source Connector\nDESCRIPTION: Command to build the docker image for the Ip2whois source connector using airbyte-ci. This creates an image tagged as airbyte/source-ip2whois:dev on the host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ip2whois/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ip2whois build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for the Source-Gitlab Connector with Pytest in Bash\nDESCRIPTION: This Bash command executes the unit tests for the Airbyte Gitlab source connector within the Poetry-managed environment. It uses 'pytest' to discover and run all test cases found in the 'unit_tests' directory. Dependencies include the presence of all test files, properly configured test dependencies via Poetry, and a suitable Python environment. Input is optional and test results are outputted to the console. There are no special limitations but it should be run from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-airtable build\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands for Recreation\nDESCRIPTION: Commands to run the standard source connector operations (spec, check, discover, read) using the Recreation connector Docker image with mounted configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recreation/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-recreation:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recreation:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recreation:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-recreation:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Height Connector Streams in Markdown\nDESCRIPTION: This snippet lists the available streams for the Height connector, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/height.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| workspace | id | No pagination | ✅ |  ✅  |\n| lists | id | No pagination | ✅ |  ✅  |\n| tasks | id | No pagination | ✅ |  ✅  |\n| activities | id | No pagination | ✅ |  ✅  |\n| field_templates | id | No pagination | ✅ |  ❌  |\n| users | id | No pagination | ✅ |  ✅  |\n| groups | id | No pagination | ✅ |  ✅  |\n| search | id | No pagination | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running Connector Check Command via Docker (Bash)\nDESCRIPTION: Executes the `check` command within a temporary Docker container (`--rm`) using the `airbyte/source-open-exchange-rates:dev` image. It mounts the local `secrets` directory (containing `config.json`) to `/secrets` inside the container and uses the `--config` flag to provide the configuration file path. This command validates the connection credentials provided in `config.json`. Requires Docker, the built connector image, and a valid `secrets/config.json` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-open-exchange-rates:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Creating Read-Only User in MongoDB\nDESCRIPTION: MongoDB command to create a read-only user with specific database access permissions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nadmin> db.createUser({user: \"READ_ONLY_USER\", pwd: \"READ_ONLY_PASSWORD\", roles: [{role: \"read\", db: \"TARGET_DATABASE\"}]})\n```\n\n----------------------------------------\n\nTITLE: Building the Freshsales Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Freshsales source connector docker image using airbyte-ci tool. This creates an image tagged as airbyte/source-freshsales:dev on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshsales/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshsales build\n```\n\n----------------------------------------\n\nTITLE: Building Kafka Connector with Gradle\nDESCRIPTION: Command to build the Kafka destination connector using Gradle from the Airbyte repository root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kafka/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-kafka:build\n```\n\n----------------------------------------\n\nTITLE: Defining Mux API Stream Specifications\nDESCRIPTION: Table defining the available data streams from Mux API, including their primary keys, pagination details, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mux.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| video_assets | id | DefaultPaginator | ✅ |  ✅  |\n| video_live-streams | id | DefaultPaginator | ✅ |  ✅  |\n| video_playbacks | id | DefaultPaginator | ✅ |  ❌  |\n| system_signin-keys | id | DefaultPaginator | ✅ |  ✅  |\n| video_playback-restrictions | id | DefaultPaginator | ✅ |  ✅  |\n| video_transcription-vocabularies | id | DefaultPaginator | ✅ |  ✅  |\n| video_uploads | id | DefaultPaginator | ✅ |  ❌  |\n| video_signing-keys | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record Without Partition Value\nDESCRIPTION: A sample JSON record from the Woocommerce API showing an order note without the order ID included in the record. This demonstrates the need for adding partition values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"id\": 999, \"author\": \"Jon Doe\", \"note\": \"Great product!\" }\n```\n\n----------------------------------------\n\nTITLE: Running Faker connector commands locally\nDESCRIPTION: Commands to run the Faker connector operations locally including spec, check, discover, and read operations with appropriate configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-faker spec\npoetry run source-faker check --config secrets/config.json\npoetry run source-faker discover --config secrets/config.json\npoetry run source-faker read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for S3 Configuration\nDESCRIPTION: This YAML snippet defines a Kubernetes secret for Airbyte configuration with S3 storage. It includes fields for license key, data plane credentials, and AWS/S3 access keys.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: airbyte-config-secrets\ntype: Opaque\ndata:\n  # Enterprise License Key\n  license-key: your-airbyte-license-key\n\n  # Insert the data plane credentials received in step 2\n  DATA_PLANE_CLIENT_ID: your-data-plane-client-id\n  DATA_PLANE_CLIENT_SECRET: your-data-plane-client-id\n  \n  # Only set these values if they are also set on your control plane\n  AWS_SECRET_MANAGER_ACCESS_KEY_ID: your-aws-secret-manager-access-key\n  AWS_SECRET_MANAGER_SECRET_ACCESS_KEY: your-aws-secret-manager-secret-key\n  S3_ACCESS_KEY_ID: your-s3-access-key\n  S3_SECRET_ACCESS_KEY: your-s3-secret-key\n```\n\n----------------------------------------\n\nTITLE: Enabling Basic Authentication in abctl - Shell\nDESCRIPTION: This snippet highlights the usage of authentication flags in the `abctl` command-line tool, specifically for enabling basic authentication and migrating to secure configurations. Requires v0.11.0 or newer of `abctl`. Users can pass command-line flags for non-secure or low-resource environments. Inputs are OS shell commands, and outputs depend on successful CLI execution. Limitations: Only available starting v0.11.0; ensure your platform version meets this requirement.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/aug_2024.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n`abctl` (v0.11.0) now also supports basic authentication. Flags for both non-secure and low resource environments were also added to enable migration paths.\n```\n\n----------------------------------------\n\nTITLE: Using `airbyte-ci connectors bump-version` Command Example (Shell)\nDESCRIPTION: Shell example showing how to use the `airbyte-ci connectors bump-version` command to increment the version of a specific connector (`source-openweather`). It demonstrates bumping the patch version and providing a changelog entry message.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-openweather bump-version patch \"<changelog-entry>\"\n```\n\n----------------------------------------\n\nTITLE: Running Snowflake Cortex Connector Commands Locally\nDESCRIPTION: Demonstrates how to run various connector commands locally using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python main.py spec\npoetry run python main.py check --config secrets/config.json\ncat examples/messages.jsonl | poetry run python main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Convex Connector Operations Locally with Poetry (Bash)\nDESCRIPTION: Executes various Airbyte connector operations (spec, check, discover, read) locally using `poetry run`. Requires a `secrets/config.json` file for configuration-dependent commands and `sample_files/configured_catalog.json` for the read command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-convex spec\npoetry run source-convex check --config secrets/config.json\npoetry run source-convex discover --config secrets/config.json\npoetry run source-convex read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated SingleStore User for Airbyte (SQL)\nDESCRIPTION: Provides the SQL command to create a new user named 'airbyte' in SingleStore, identified by a specified password. This is a recommended step for managing permissions and auditing Airbyte's access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/singlestore.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE\nUSER airbyte IDENTIFIED BY <your_password_here>;\n```\n\n----------------------------------------\n\nTITLE: Making API Request to OSS Workspaces Endpoint\nDESCRIPTION: Example of how to make an API request to retrieve workspaces list in Airbyte OSS edition using the required /api/public/v1 path prefix.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/api-documentation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/api/public/v1/workspaces\n```\n\n----------------------------------------\n\nTITLE: Populating Postgres Benchmark Database via SQL Scripts (Shell)\nDESCRIPTION: Executes a sequence of SQL scripts against a specified Postgres instance to generate a test dataset for performance benchmarks. This setup requires manual adjustment of 3-run-script.sql according to TODO comments to specify table and record counts. The commands use psql to connect to the target host/database as a specified user/port, running the create-copy-tables-procedure, insert-rows-to-table-procedure, and run-script scripts in order. Prerequisites: psql CLI installed, valid authentication to the Postgres DB, and the Airbyte source-postgres connector directory checked out.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postgres/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd airbyte-integrations/connectors/source-postgres\npsql -h <host> -d <db-name> -U <username> -p <port> -a -q -f src/test-performance/sql/1-create-copy-tables-procedure.sql\npsql -h <host> -d <db-name> -U <username> -p <port> -a -q -f src/test-performance/sql/2-create-insert-rows-to-table-procedure.sql\npsql -h <host> -d <db-name> -U <username> -p <port> -a -q -f src/test-performance/sql/3-run-script.sql\n```\n\n----------------------------------------\n\nTITLE: Testing the Eventzilla connector\nDESCRIPTION: Command to run acceptance tests for the source-eventzilla connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventzilla/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventzilla test\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Engine - Shell\nDESCRIPTION: Installs the Docker engine using Amazon Linux's package manager via a non-interactive command. No additional dependencies are required beyond default system repositories. Takes no parameters and outputs installation logs to the terminal. Should be run with root privileges.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install -y docker\n```\n\n----------------------------------------\n\nTITLE: Building the Gong Source Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This snippet shows how to build a manifest-only Airbyte connector Docker image using the airbyte-ci command-line tool. It requires the airbyte-ci tool to be installed and available in the environment. The --name parameter specifies the connector name (source-gong), and the resulting image will be tagged as airbyte/source-gong:dev on the local Docker host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gong/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gong build\n```\n\n----------------------------------------\n\nTITLE: YNAB API Configuration Schema\nDESCRIPTION: Configuration schema showing required API key parameter for YNAB integration\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/you-need-a-budget-ynab.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Commands in Docker - Bash\nDESCRIPTION: Executes Airbyte source connector commands via Docker, mounting necessary secrets and integration test configurations as volumes. Each command runs the container with specific config/catalog files mapped from the host, ensuring credentials and integration tests are available to the connector. Outputs appear on stdout; files must be present and correctly formatted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-analytics-v4-service-account-only:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-v4-service-account-only:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-analytics-v4-service-account-only:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-analytics-v4-service-account-only:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for RD Station Marketing Connector with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the RD Station Marketing source connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rd-station-marketing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rd-station-marketing build\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Teradata Connector with Gradle\nDESCRIPTION: Executes acceptance and custom integration tests for the Teradata destination connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-teradata/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-teradata:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Conflict Stream Scalar Record Structure\nDESCRIPTION: Illustrates the structure of records in the 'conflict_stream_scalar' stream. It contains a simple key-value pair where the key name matches the stream name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"RECORD\",\n  \"record\":{\n    \"stream\":\"conflict_stream_scalar\",\n    \"data\":{\n      \"id\":1,\n      \"conflict_stream_scalar\": 2\n    },\n    \"emitted_at\":1623861660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Stripe Connector\nDESCRIPTION: This command adds a new dependency to the Stripe connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airbyte OSS Credentials using abctl Command Line\nDESCRIPTION: This shell command uses the Airbyte control utility (`abctl`) version 0.11.0 or newer to retrieve the randomly generated native authentication credentials (email and password) for a local Airbyte Open Source instance. This is part of the feature ensuring Airbyte OSS instances are secure by default.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/july_2024.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nabctl local credentials\n```\n\n----------------------------------------\n\nTITLE: Adding abctl to PATH on Linux\nDESCRIPTION: Command to move the abctl executable to a directory in the system PATH on Linux, allowing it to be run from any location.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo mv abctl /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Running Standard Airbyte Source Connector Docker Commands (Bash)\nDESCRIPTION: This snippet contains standard docker commands to operate the Airbyte Orb source connector locally. It includes commands for running the spec, check, discover, and read operations, with appropriate mounts for secrets and test catalogs. These require prior creation of a valid secrets/config.json file and, for read operations, a pre-configured catalog. Outputs and side effects depend on Airbyte's internal logic and may create or use local files for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-orb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-orb:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-orb:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-orb:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-orb:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Configuration Table in Markdown\nDESCRIPTION: This snippet shows the configuration options for the GreytHR Connector using a markdown table. It includes input parameters, their types, descriptions, and default values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/greythr.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `domain` | `string` | Host URL. Your GreytHR Host URL |  |\n| `base_url` | `string` | Base URL. https://api.greythr.com |  |\n| `password` | `string` | Password.  |  |\n| `username` | `string` | Username.  |  |\n```\n\n----------------------------------------\n\nTITLE: Testing the Salesflare Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Salesflare connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesflare/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesflare test\n```\n\n----------------------------------------\n\nTITLE: Installing Weaviate Connector Dependencies with Poetry\nDESCRIPTION: This command installs the necessary dependencies for the Weaviate connector using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Available Streams Table in Markdown\nDESCRIPTION: Markdown table listing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/ninjaone-rmm.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| organizations | id | DefaultPaginator | ✅ |  ❌  |\n| policies | id | No pagination | ✅ |  ✅  |\n| activities | id | DefaultPaginator | ✅ |  ✅  |\n| automation_scripts | id | No pagination | ✅ |  ✅  |\n| groups | id | No pagination | ✅ |  ✅  |\n| locations | id | DefaultPaginator | ✅ |  ❌  |\n| roles | id | No pagination | ✅ |  ✅  |\n| software_products | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building the Sentry connector Docker image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Sentry source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sentry/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sentry build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Exchange Rates Connector\nDESCRIPTION: Command for running the full test suite for the Exchange Rates connector using airbyte-ci. This is useful for validating changes before submitting a pull request.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-exchange-rates/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-exchange-rates test\n```\n\n----------------------------------------\n\nTITLE: Building Taboola Source Connector in Airbyte\nDESCRIPTION: This command builds a development image of the Taboola source connector using airbyte-ci. The resulting image (source-taboola:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-taboola/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-taboola build\n```\n\n----------------------------------------\n\nTITLE: Running Docuseal Connector Tests with airbyte-ci (bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the acceptance tests defined for the `source-docuseal` connector. Requires `airbyte-ci` to be installed and the connector source code to be available. It verifies the connector's functionality against defined test cases, typically running against the previously built development image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-docuseal/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-docuseal test\n```\n\n----------------------------------------\n\nTITLE: Running Convex Connector as Docker Container\nDESCRIPTION: Series of commands to run various Convex connector operations as a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-convex:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-convex:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-convex:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-convex:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Guru Connector Streams in Markdown\nDESCRIPTION: Markdown table listing the available streams in the Guru connector, including their primary keys, pagination methods, and support for full and incremental sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/guru.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| teams | id | DefaultPaginator | ✅ |  ✅  |\n| groups | id | DefaultPaginator | ✅ |  ✅  |\n| group_collection_access |  | DefaultPaginator | ✅ |  ✅  |\n| group_members |  | DefaultPaginator | ✅ |  ✅  |\n| members | id | DefaultPaginator | ✅ |  ✅  |\n| team_analytics |  | DefaultPaginator | ✅ |  ❌  |\n| collections | id | DefaultPaginator | ✅ |  ✅  |\n| folders | id | DefaultPaginator | ✅ |  ❌  |\n| folder_items | id | DefaultPaginator | ✅ |  ❌  |\n| folders_parent |  | DefaultPaginator | ✅ |  ❌  |\n| search_cardmgr | id | DefaultPaginator | ✅ |  ✅  |\n| tag_categories |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Teamwork Connector\nDESCRIPTION: This command executes the acceptance tests for the Teamwork connector using airbyte-ci. It helps ensure the connector functions correctly before deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teamwork/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-teamwork test\n```\n\n----------------------------------------\n\nTITLE: Testing Brevo Source Connector with Airbyte-CI\nDESCRIPTION: Command to run acceptance tests for the Brevo source connector using airbyte-ci tool. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-brevo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-brevo test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Gradle\nDESCRIPTION: Command to execute unit tests for the MSSQL connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-mssql-v2:test\n```\n\n----------------------------------------\n\nTITLE: Zoho Billing Available Data Streams\nDESCRIPTION: Table defining the available data streams, their primary keys, pagination type, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-billing.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Products | product_id | DefaultPaginator | ✅ |  ❌  |\n| plans | plan_code | DefaultPaginator | ✅ |  ❌  |\n| addons | addon_code | DefaultPaginator | ✅ |  ❌  |\n| coupons | coupon_code | DefaultPaginator | ✅ |  ❌  |\n| customers | customer_id | DefaultPaginator | ✅ |  ❌  |\n| Quotes | estimate_id | DefaultPaginator | ✅ |  ❌  |\n| invoices | invoice_id | DefaultPaginator | ✅ |  ❌  |\n| expenses | expense_id | DefaultPaginator | ✅ |  ❌  |\n| subscriptions | customer_id | DefaultPaginator | ✅ |  ❌  |\n| taxes | tax_id | DefaultPaginator | ✅ |  ❌  |\n| transactions | transaction_id | DefaultPaginator | ✅ |  ❌  |\n| recurring expenses | recurring_expense_id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building Split-io Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Split-io source connector using airbyte-ci. The resulting image is tagged as 'source-split-io:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-split-io/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-split-io build\n```\n\n----------------------------------------\n\nTITLE: Running Pinecone Connector Tests\nDESCRIPTION: Commands to run the full test suite, unit tests, and integration tests for the Pinecone connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-pinecone test\n\n# Unit Tests\npoetry run pytest -s unit_tests\n\n# Integration Tests\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Breezometer connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-breezometer/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-breezometer test\n```\n\n----------------------------------------\n\nTITLE: Testing Canny Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Canny source connector using airbyte-ci tool. Validates the connector's functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-canny/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-canny test\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Invoice Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Zoho Invoice source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-invoice/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-invoice test\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Connector Orchestrator\nDESCRIPTION: Command to run pytest for the Connector Orchestrator project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Gmail Connector with Airbyte CI - Bash\nDESCRIPTION: This Bash snippet executes acceptance tests for the Gmail connector using the 'airbyte-ci' tool. The command validates the connector's compliance and reliability in a development environment. It requires the same prerequisites as the build command: 'airbyte-ci' must be installed, and the Gmail connector specified via '--name=source-gmail'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gmail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gmail test\n```\n\n----------------------------------------\n\nTITLE: Running the Airbyte Source-Gitlab CI Test Suite with Airbyte CI in Bash\nDESCRIPTION: This Bash command triggers the full continuous integration (CI) test suite for the Airbyte Gitlab source connector using the airbyte-ci tool. The connector name is specified as a parameter to target this specific connector. Dependencies include a working installation of airbyte-ci and all relevant test dependencies configured. Expected output includes test logs and pass/fail results. This command is essential before publishing changes to ensure the connector satisfies all automated test criteria.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gitlab test\n\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally with Poetry - Bash\nDESCRIPTION: This sequence of Poetry commands executes the Airbyte Okta source connector's various CLI operations (spec, check, discover, read) using configuration files. These commands require Poetry and the Okta connector installed in the local Python environment. The --config flag specifies the config file and the --catalog parameter is needed for the read operation. Outputs vary depending on the operation and configuration provided.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-okta spec\npoetry run source-okta check --config secrets/config.json\npoetry run source-okta discover --config secrets/config.json\npoetry run source-okta read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI for Pinterest Connector (Bash)\nDESCRIPTION: This bash command builds a Docker image for the source-pinterest connector using Airbyte's specialized CLI tool, airbyte-ci. Requires global or virtual environment installation of airbyte-ci and a functioning Docker environment. Outputs an image tagged as airbyte/source-pinterest:dev on the local Docker host. Users should ensure name-parameter consistency with the target connector and fulfill all CI-related prerequisites.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pinterest build\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table for LinkedIn API Integration\nDESCRIPTION: Structured changelog table showing version history, dates, pull request references and change descriptions for the LinkedIn API integration component\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-pages.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                              |\n|:--------|:-----------| :------------------------------------------------------- | :--------------------------------------------------- |\n| 1.1.22 | 2025-04-19 | [58174](https://github.com/airbytehq/airbyte/pull/58174) | Update dependencies |\n| 1.1.21 | 2025-04-12 | [57721](https://github.com/airbytehq/airbyte/pull/57721) | Update dependencies |\n| 1.1.20 | 2025-04-05 | [56679](https://github.com/airbytehq/airbyte/pull/56679) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Exchange Rates Connector Commands in Docker\nDESCRIPTION: Standard commands for running the Exchange Rates source connector in a Docker container. These commands allow you to run the spec, check, discover, and read operations with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-exchange-rates/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-exchange-rates:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-exchange-rates:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-exchange-rates:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-exchange-rates:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing the S3 Connector with Poetry\nDESCRIPTION: Command to install the S3 connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Fastbill Source Connector\nDESCRIPTION: Command to build the docker image for the Fastbill source connector using airbyte-ci. This creates an image with the tag 'airbyte/source-fastbill:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fastbill/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fastbill build\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source-Chargebee Standard Commands (Docker, Bash)\nDESCRIPTION: This series of bash/docker commands illustrates standard operations (spec, check, discover, read) for the source-chargebee connector in Airbyte. The commands use the locally built Docker image, and many utilize Docker volume mounts for passing secrets and catalog configuration files. Required prerequisites are a configured secrets/config.json and, for 'read', an integration_tests/configured_catalog.json. The output and behavior depend on each subcommand ('spec', 'check', 'discover', 'read'). Adjust volume mounts as necessary for your environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargebee/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-chargebee:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chargebee:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-chargebee:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-chargebee:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing MixMax Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the MixMax source connector using airbyte-ci tool. Validates connector functionality through test suite.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixmax/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mixmax test\n```\n\n----------------------------------------\n\nTITLE: Building Connector with airbyte-ci\nDESCRIPTION: Command to build the connector using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-typesense build\n```\n\n----------------------------------------\n\nTITLE: Running Weatherstack Source Connector Commands\nDESCRIPTION: Docker commands to run various operations for the Weatherstack source connector, including spec, check, discover, and read. These commands use the built Docker image and require configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-weatherstack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-weatherstack:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-weatherstack:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-weatherstack:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-weatherstack:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Stream Settings Configuration\nDESCRIPTION: Table showing the available stream-specific settings and their descriptions for Airbyte data transfers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/configuring-connections.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Setting | Description             |\n| --------- | ----------- |\n| [Stream selection](/using-airbyte/configuring-schema.md) | Determine if the stream syncs to your destination     |\n| [Sync mode](/using-airbyte/core-concepts/sync-modes/README.md) | Configure how Airbyte reads data from the source and writes it     |\n| [Cursor selection](/using-airbyte/configuring-schema.md) | Select what field the stream uses to incrementally read from the source     |\n| [Primary key selection](/using-airbyte/configuring-schema.md) | Select what field the stream uses to determine uniqueness of a record     |\n| [Field selection](/using-airbyte/configuring-schema.md) | (Optional) Disable a partial set of fields Airbyte should not sync to the destination     |\n```\n\n----------------------------------------\n\nTITLE: Example Output of Listing Job Directory Files (Host Shell Method)\nDESCRIPTION: Shows sample output for the command that lists job directory contents from the host shell using a temporary Docker container. This output might include additional files or directories compared to the basic example, such as `singer_rendered_catalog.json` and the `normalize` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ncatalog.json                 singer_rendered_catalog.json\nlogs.log                     tap_config.json\nnormalize                    target_config.json\n```\n\n----------------------------------------\n\nTITLE: JSON Record with Mixed Data Types\nDESCRIPTION: A comprehensive JSON record showing various data structures including schemaless objects, typed unions, arrays, and nested objects with different property patterns.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_disjoint_union_messages_out.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"schemaless_object\":\"{\\\"uuid\\\":\\\"38F52396-736D-4B23-B5B4-F504D8894B97\\\",\\\"probability\\\":1.5}\",\"schematized_object\":{\"id\":1,\"name\":\"Joe\"},\"combined_type\":{\"type\":\"string\",\"string\":\"string1\",\"integer\":null},\"union_type\":{\"type\":\"integer\",\"string\":null,\"integer\":10},\"schemaless_array\":\"[10,\\\"foo\\\",null,{\\\"bar\\\":\\\"qua\\\"}]\",\"mixed_array_integer_and_schemaless_object\":[{\"type\":\"integer\",\"integer\":15,\"object\":null},null,{\"type\":\"object\",\"integer\":null,\"object\":\"{\\\"hello\\\":\\\"world\\\"}\"}],\"array_of_union_integer_and_schemaless_array\":[{\"type\":\"integer\",\"integer\":25,\"object\":null},null,{\"type\":\"object\",\"integer\":null,\"object\":\"[\\\"goodbye\\\",\\\"cruel world\\\"]\"}],\"union_of_objects_with_properties_identical\":{\"id\":10,\"name\":\"Joe\"},\"union_of_objects_with_properties_overlapping\":{\"id\":20,\"name\":\"Jane\",\"flagged\":true},\"union_of_objects_with_properties_nonoverlapping\":{\"id\":30,\"name\":\"Phil\",\"flagged\":false,\"description\":\"Very Phil\"},\"union_of_objects_with_properties_contradicting\":{\"id\":{\"type\":\"integer\",\"integer\":1,\"string\":null},\"name\":\"Jenny\"},\"empty_object\":\"{}\",\"object_with_null_properties\":\"{}\",\"combined_with_null\":\"foobar\",\"union_with_null\":\"barfoo\",\"combined_nulls\":null,\"compact_union\":{\"type\":\"object\",\"object\":{\"id\":10,\"name\":\"Tyler\"},\"integer\":null}}\n```\n\n----------------------------------------\n\nTITLE: Defining Humanitix Data Streams in Markdown\nDESCRIPTION: Table describing the available data streams for the Humanitix connector, including primary keys, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/humanitix.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| events | _id | DefaultPaginator | ✅ |  ❌  |\n| orders | _id | DefaultPaginator | ✅ |  ❌  |\n| tickets | _id | DefaultPaginator | ✅ |  ❌  |\n| tags | _id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Tiktok-Marketing Connector as Docker Container\nDESCRIPTION: These commands demonstrate how to run various operations of the Tiktok-Marketing connector using a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tiktok-marketing:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tiktok-marketing:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tiktok-marketing:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tiktok-marketing:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Self-hosted Sign-in Redirect URI Configuration\nDESCRIPTION: The redirect URI pattern required for self-hosted Airbyte SSO integration with Okta. Contains placeholders for domain and app integration name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/okta.md#2025-04-23_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n<your-airbyte-domain>/auth/realms/airbyte/broker/<app-integration-name>/endpoint\n```\n\n----------------------------------------\n\nTITLE: Listing Connectors with Multiple Filters\nDESCRIPTION: This command demonstrates using multiple filters to list connectors, combining language and support level criteria.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --language=low-code --support-level=certified list\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands for running the connector in Docker container, including spec, check, discover, and read operations with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-sms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailjet-sms:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailjet-sms:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailjet-sms:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailjet-sms:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using striptags Filter in Jinja2\nDESCRIPTION: Demonstrates the `striptags` filter in Jinja2, which removes all HTML/XML tags from a string. The example removes the `<div>` and `</div>` tags from '<div>hello</div>'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_46\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '<div>hello</div>'|striptags }}\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Table for Airbyte CI Tools in Markdown\nDESCRIPTION: A markdown table listing the directories within the Airbyte CI folder and their descriptions. It covers various tools for building images, managing credentials, running QA, generating metadata, and executing CI pipelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Directory                                          | Description                                                                                                                   |\n| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| [`base_images`](connectors/base_images)            | A set of tools to build and publish Airbyte base connector images.                                                            |\n| [`ci_credentials`](connectors/ci_credentials)      | A CLI tool to fetch connector secrets from GCP Secrets Manager.                                                               |\n| [`connector_ops`](connectors/connector_ops)        | A python package with utils reused in internal packages.                                                                      |\n| [`connectors_qa`](connectors/connectors_qa/)       | A tool to verify connectors have sounds assets and metadata.                                                                  |\n| [`metadata_service`](connectors/metadata_service/) | Tools to generate connector metadata and registry.                                                                            |\n| [`pipelines`](connectors/pipelines/)               | Airbyte CI pipelines, including formatting, linting, building, testing connectors, etc. Connector acceptance tests live here. |\n| [`auto_merge`](connectors/auto_merge/)             | A tool to automatically merge connector pull requests.                                                                        |\n```\n\n----------------------------------------\n\nTITLE: Running Slack Connector as Docker Container\nDESCRIPTION: Commands to run the Slack connector as a Docker container for different operations, mounting necessary volumes for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-slack:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-slack:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-slack:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-slack:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Elasticsearch Connector\nDESCRIPTION: Command to run unit tests for the Elasticsearch source connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticsearch/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-elasticsearch:unitTest\n```\n\n----------------------------------------\n\nTITLE: GET Request Example for Gainsight API\nDESCRIPTION: An example of a GET request to the Gainsight API endpoint for retrieving accounts data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gainsight-px.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.aptrinsic.com/v1/accounts\n```\n\n----------------------------------------\n\nTITLE: Running Connector Test Suite with airbyte-ci - Bash\nDESCRIPTION: This snippet illustrates how to invoke the full test suite for the source-close-com connector using airbyte-ci. The only required dependency is that airbyte-ci is installed, and the source-close-com connector build environment is properly set up. This command orchestrates all connector tests to validate changes before publishing; the outcome is test feedback provided in the terminal or CI logs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-close-com/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-close-com test\n```\n\n----------------------------------------\n\nTITLE: Getting IP Address Command\nDESCRIPTION: Command to obtain the IP address of the machine running Airbyte instance, used for IP whitelisting in Marketo\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/marketo.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl ifconfig.io\n```\n\n----------------------------------------\n\nTITLE: MongoDB Configuration for Authentication\nDESCRIPTION: YAML configuration for enabling authentication and setting bind IP in MongoDB\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnet:\n  bindIp: 0.0.0.0\n\nsecurity:\n  authorization: enabled\n```\n\n----------------------------------------\n\nTITLE: Testing Campayn Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Campayn connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-campayn/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-campayn test\n```\n\n----------------------------------------\n\nTITLE: Installing dbt via pip\nDESCRIPTION: Command to install dbt using pip package manager within a virtual environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install dbt\n```\n\n----------------------------------------\n\nTITLE: Building Datadog Source Connector Docker Image with Airbyte-CI (Bash)\nDESCRIPTION: This shell command builds the Docker image for the manifest-only Datadog source connector using the Airbyte-CI tool. It requires Airbyte-CI to be installed and run from the root of the Airbyte repository. The resulting image will be tagged as 'airbyte/source-datadog:dev' and is required for local development or testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datadog/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-datadog build\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pgvector/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev.\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Airbyte CI\nDESCRIPTION: Command to reinstall Airbyte CI using the Makefile when the update command fails.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# from the root of the airbyte repository\nmake tools.airbyte-ci.install\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Time Format\nDESCRIPTION: Represents a JSON schema definition for a field intended to store a time of day. This schema uses the standard `string` type with the `time` format specifier.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"string\",\n  \"format\": \"time\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Teradata Source Connector in Java\nDESCRIPTION: This Gradle command runs acceptance and custom integration tests for the Teradata source connector from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teradata/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-teradata:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running Tplcentral Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-tplcentral spec\npoetry run source-tplcentral check --config secrets/config.json\npoetry run source-tplcentral discover --config secrets/config.json\npoetry run source-tplcentral read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Bigin Connector Using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zoho Bigin connector using airbyte-ci tool. Executes the test suite to verify connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-bigin/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-bigin test\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Docker Commands\nDESCRIPTION: Commands to run the Firebolt connector Docker image for specification, configuration check, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-firebolt:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-firebolt:dev check --config /secrets/config.json\ncat integration_tests/messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-firebolt:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Tplcentral Connector Commands in Docker\nDESCRIPTION: Commands to run various connector operations using the Docker container, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tplcentral:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tplcentral:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tplcentral:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tplcentral:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Table in Markdown\nDESCRIPTION: This snippet shows a changelog table for the GreytHR Connector, listing version updates, dates, pull request numbers, and subjects of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/greythr.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.19 | 2025-04-19 | [58188](https://github.com/airbytehq/airbyte/pull/58188) | Update dependencies |\n| 0.0.18 | 2025-04-12 | [57712](https://github.com/airbytehq/airbyte/pull/57712) | Update dependencies |\n| 0.0.17 | 2025-04-05 | [57061](https://github.com/airbytehq/airbyte/pull/57061) | Update dependencies |\n| 0.0.16 | 2025-03-29 | [56696](https://github.com/airbytehq/airbyte/pull/56696) | Update dependencies |\n| 0.0.15 | 2025-03-22 | [56077](https://github.com/airbytehq/airbyte/pull/56077) | Update dependencies |\n| 0.0.14 | 2025-03-08 | [55476](https://github.com/airbytehq/airbyte/pull/55476) | Update dependencies |\n| 0.0.13 | 2025-03-01 | [54785](https://github.com/airbytehq/airbyte/pull/54785) | Update dependencies |\n| 0.0.12 | 2025-02-22 | [53820](https://github.com/airbytehq/airbyte/pull/53820) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53270](https://github.com/airbytehq/airbyte/pull/53270) | Update dependencies |\n| 0.0.10 | 2025-02-03 | [52620](https://github.com/airbytehq/airbyte/pull/52620) | Bug fixes with pagination |\n| 0.0.9 | 2025-02-01 | [52782](https://github.com/airbytehq/airbyte/pull/52782) | Update dependencies |\n| 0.0.8 | 2025-01-25 | [52256](https://github.com/airbytehq/airbyte/pull/52256) | Update dependencies |\n| 0.0.7 | 2025-01-18 | [51794](https://github.com/airbytehq/airbyte/pull/51794) | Update dependencies |\n| 0.0.6 | 2025-01-11 | [51176](https://github.com/airbytehq/airbyte/pull/51176) | Update dependencies |\n| 0.0.5 | 2024-12-28 | [50603](https://github.com/airbytehq/airbyte/pull/50603) | Update dependencies |\n| 0.0.4 | 2024-12-21 | [50126](https://github.com/airbytehq/airbyte/pull/50126) | Update dependencies |\n| 0.0.3 | 2024-12-14 | [49621](https://github.com/airbytehq/airbyte/pull/49621) | Update dependencies |\n| 0.0.2 | 2024-12-12 | [48920](https://github.com/airbytehq/airbyte/pull/48920) | Update dependencies |\n| 0.0.1 | 2024-11-29 | | Initial release by [@bhushan-dhwaniris](https://github.com/bhushan-dhwaniris) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Rendering Academy Course Cards with React Grid Component\nDESCRIPTION: This code renders a responsive grid layout of course cards for Airbyte Academy. It uses React components to display available and upcoming courses with icons, descriptions, and call-to-action links.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/academy.md#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Grid columns=\"2\">\n    <CardWithIcon title=\"Cloud fundamentals\" description=\"In this course you will create a data pipeline using Airbyte Cloud, find existing Connectors in the Connector Marketplace, and learn best practices to create streams and move data between sources and destinations. In addition, you will learn how, using the new AI Assistant, you can create connections to any endpoint with no code.\" ctaText=\"Start learning Cloud\" ctaLink=\"https://airbyteacademy.thinkific.com/products/courses/101-cloud-fundamentals?utm_source=airbytedocs\" icon=\"fa-cloud\" />\n    <CardWithIcon title=\"Build an AI chatbot\" description=\"In this course, you will use Stripe, Airbyte, Supabase, PGVector, and OpenAI to build an AI-powered chatbot to allow users to interact with e-commerce data. They will be able to ask natural language questions to uncover insights in the data.\" ctaText =\"Start building AI apps\" ctaLink=\"https://airbyteacademy.thinkific.com/courses/ai-chatbot?utm_source=airbytedocs\" icon=\"fa-robot\" />\n</Grid>\n```\n\n----------------------------------------\n\nTITLE: Granting Container Access for CDC in Oracle Multitenant SQL\nDESCRIPTION: This SQL command grants the 'SET CONTAINER' privilege to the 'airbyte' user, specifically required when using CDC incremental syncs on an Oracle multitenant Container Database (CDB) instance. The `CONTAINER=ALL` clause ensures the user can operate across containers, which is necessary for Debezium in this architecture.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-oracle-enterprise.md#2025-04-23_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SET CONTAINER TO airbyte CONTAINER=ALL;\n```\n\n----------------------------------------\n\nTITLE: Enabling CDC on MSSQL Table\nDESCRIPTION: SQL command to enable Change Data Capture on a specific table. Requires schema name and table name as parameters, with optional role_name parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql/mssql-troubleshooting.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sys.sp_cdc_enable_table\n    @source_schema = N'<schema>',\n    @source_name   = N'<table>',\n    @role_name     = NULL\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Facebook Insights Basic Structure\nDESCRIPTION: Demonstrates the basic SQL-like query structure for fetching Facebook Insights data, showing how to select fields from an edge object with filtering and grouping capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/source_facebook_marketing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect <fields> from <edge_object> where <filter> group by <level>, <breakdowns>;\n```\n\n----------------------------------------\n\nTITLE: Setting Airbyte Public URL in YAML Configuration\nDESCRIPTION: This YAML snippet configures the public facing URL for the Airbyte instance. It's added to the values.yaml file under the global section.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nairbyteUrl: # e.g. https://airbyte.company.example\n```\n\n----------------------------------------\n\nTITLE: Redeploying Airbyte via abctl with Custom Values (Shell)\nDESCRIPTION: Instructs `abctl` to perform a local installation (or upgrade) of Airbyte, applying configuration overrides specified in the `values.yaml` file. This is used to deploy changes, such as using a locally built image specified in the values file. Requires `abctl` and the `values.yaml` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --values values.yaml\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the News API connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-news-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-news-api test\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Values for Airbyte Installation in Shell\nDESCRIPTION: This command applies custom values defined in a values.yaml file to the Airbyte installation. The path to the values.yaml file should be adjusted as needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --values ./values.yaml\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Access Token Nested Under Data - JSON\nDESCRIPTION: This sample JSON illustrates a token response where the access_token is found inside the data key, matching the nested configuration in the YAML spec. Airbyte will extract the token using the specified key path. Input and output must preserve this hierarchy.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_65\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"data\\\": {\\n    \\\"access_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\"\\n  }\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests Directly with Pytest for Debugging (Bash)\nDESCRIPTION: Provides the sequence of bash commands to run the acceptance test suite directly using pytest from the `connector-acceptance-test` directory. This allows for local debugging and requires Poetry to be installed. It involves changing directory, installing dependencies with Poetry, and running pytest with the specific plugin and configuration file path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-integrations/bases/connector-acceptance-test/\npoetry install\npoetry run pytest -p connector_acceptance_test.plugin --acceptance-test-config=../../connectors/<your-connector> --pdb\n```\n\n----------------------------------------\n\nTITLE: Advanced User Engagement Report with Cohort and Pivot Configuration\nDESCRIPTION: Enhanced custom report configuration that includes cohort specification for a 7-day analysis period and pivot table settings to analyze the top 50 cities. This implementation showcases advanced features like date ranges and metric aggregations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-data-api.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"User Engagement Report\",\n    \"dimensions\": [\"city\"],\n    \"metrics\": [\"sessions\", \"bounceRate\"],\n    \"cohortSpec\": {\n      \"cohorts\": [\n        {\n          \"name\": \"Last 7 Days\",\n          \"dateRange\": {\n            \"startDate\": \"2023-07-27\",\n            \"endDate\": \"2023-08-03\"\n          }\n        }\n      ],\n      \"cohortReportSettings\": {\n        \"accumulate\": true\n      }\n    },\n    \"pivots\": [\n      {\n        \"fieldNames\": [\"city\"],\n        \"limit\": 50,\n        \"metricAggregations\": [\"TOTAL\"]\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Fetching Specific Data from Firebase Realtime Database\nDESCRIPTION: Demonstrates the JSON data returned when fetching the `/my-data/dinosaurs.json` path from the example Firebase Realtime Database via its REST API. The result contains only the data under the specified path, excluding parent keys.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/bootstrap.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lambeosaurus\": {\n    \"height\": 2.1,\n    \"length\": 12.5,\n    \"weight\": 5000\n  },\n  \"stegosaurus\": {\n    \"height\": 4,\n    \"length\": 9,\n    \"weight\": 2500\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX Ingress for Airbyte Webapp (Kubernetes YAML)\nDESCRIPTION: This Kubernetes YAML manifest defines an Ingress resource to expose the Airbyte web application service using the NGINX Ingress Controller. It routes traffic from the specified host (defaulting to 'localhost') to the Airbyte webapp service (`airbyte-airbyte-webapp-svc`) on port 80. SSL redirection is explicitly disabled via annotations. An NGINX Ingress Controller must be pre-installed in the cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/ingress.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: airbyte-ingress # ingress name, example: airbyte-production-ingress\n  annotations:\n    ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: localhost # host, example: airbyte.company.example\n      http:\n        paths:\n          - backend:\n              service:\n                # format is ${RELEASE_NAME}-airbyte-webapp-svc\n                name: airbyte-airbyte-webapp-svc\n                port:\n                  number: 80 # service port, example: 8080\n            path: /\n            pathType: Prefix\n```\n\n----------------------------------------\n\nTITLE: Generating Nullable Avro Schema for Timestamp-Micros Logical Type\nDESCRIPTION: Illustrates the final Avro schema generated for a JSON field originally defined with `type: \"string\"` and `format: \"date-time\"`. It defines a union type allowing either `null` or an Avro `long` annotated with the `timestamp-micros` logical type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"long\",\n      \"logicalType\": \"timestamp-micros\"\n    }\n  ]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Locally Running Airbyte Connector Commands - Python\nDESCRIPTION: These examples run Airbyte's source connector in various operational modes (spec, check, discover, read) via Python, directing it to the appropriate configuration files. Requires that all dependencies are installed and the virtual environment is activated. Input files such as secrets/config.json and integration_tests/configured_catalog.json provide credentials and catalog definitions, respectively. Outputs include operation results printed to stdout, with errors if files are missing or invalid.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py check --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py discover --config secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS ALB Ingress for Airbyte Webapp (Kubernetes YAML)\nDESCRIPTION: This Kubernetes YAML manifest defines an Ingress resource configured for the AWS Load Balancer Controller, creating an Application Load Balancer (ALB) to expose the Airbyte webapp service. It includes annotations to specify the ALB type (`kubernetes.io/ingress.class: \"alb\"`), enable SSL redirection, set the scheme to internal, reference an ACM certificate ARN for HTTPS, and define an idle timeout. Optional annotations for subnets and security groups are commented out. The AWS Load Balancer Controller must be installed with the required IAM permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/ingress.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: airbyte-ingress # ingress name, e.g. airbyte-production-ingress\n  annotations:\n    # Specifies that the Ingress should use an AWS ALB.\n    kubernetes.io/ingress.class: \"alb\"\n    # Redirects HTTP traffic to HTTPS.\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    # Creates an internal ALB, which is only accessible within your VPC or through a VPN.\n    alb.ingress.kubernetes.io/scheme: internal\n    # Specifies the ARN of the SSL certificate managed by AWS ACM, essential for HTTPS.\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-x:xxxxxxxxx:certificate/xxxxxxxxx-xxxxx-xxxx-xxxx-xxxxxxxxxxx\n    # Sets the idle timeout value for the ALB.\n    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=30\n    # [If Applicable] Specifies the VPC subnets and security groups for the ALB\n    # alb.ingress.kubernetes.io/subnets: '' e.g. 'subnet-12345, subnet-67890'\n    # alb.ingress.kubernetes.io/security-groups: <SECURITY_GROUP>\nspec:\n  rules:\n    - host: localhost # e.g. airbyte.company.example\n      http:\n        paths:\n          - backend:\n              service:\n                name: airbyte-airbyte-webapp-svc\n                port:\n                  number: 80\n            path: /\n            pathType: Prefix\n```\n\n----------------------------------------\n\nTITLE: Defining a User Schema with Complex Types for GCS Source (JSON)\nDESCRIPTION: Another example of a user-defined JSON schema for the Airbyte GCS source connector. This demonstrates defining columns with basic ('username' as string) and complex ('friends' as array, 'information' as object) data types. User schemas are specified as a map of column names to valid data types, allowing control over data structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gcs.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"username\": \"string\", \"friends\": \"array\", \"information\": \"object\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Test Dependencies using Pip\nDESCRIPTION: This shell command uses pip to install the additional dependencies required for running tests. These dependencies are defined under the 'tests' extra in the 'setup.py' file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install .[tests]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Bucket Access Policy in JSON\nDESCRIPTION: This JSON snippet defines an AWS IAM policy that grants full S3 access to a specific bucket and its contents. It's necessary for the S3 destination connector to function properly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for SFTP-Bulk Connector\nDESCRIPTION: Command to run the complete test suite for the connector using Airbyte's CI tools.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sftp-bulk test\n```\n\n----------------------------------------\n\nTITLE: Installing Hubspot Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector dependencies using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment for Airbyte Connector Development - Python\nDESCRIPTION: This snippet demonstrates how to create a dedicated Python virtual environment for developing the Outbrain Amplify Airbyte connector. The process isolates dependencies, requiring Python version 3.9.0 or higher, and generates the .venv directory for package management. No additional inputs are required beyond ensuring the correct Python version is installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Documenting Structured Changelog Table - Markdown\nDESCRIPTION: This snippet presents a markdown table describing the changelog of the Airbyte connector project. Each row includes version, date, pull request link, and a brief change subject. The table requires no external dependencies but uses standard markdown syntax for tables and github-style URLs. Columns are aligned using colons and dashes for improved readability; links are provided inline in markdown format. Inputs include release data and change details, and the output is a formatted table interpretable by markdown viewers. This snippet is constrained by markdown formatting and may not render advanced features unless viewed in a compatible markdown viewer.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/notion.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                                              |\\n|:--------|:-----------|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|\\n| 3.0.7 | 2025-01-11 | [43832](https://github.com/airbytehq/airbyte/pull/43832) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\\n| 3.0.6 | 2024-06-25 | [40498](https://github.com/airbytehq/airbyte/pull/40498) | Fix Pydantic error - add missing type annotation for `max_cursor_time` |\\n| 3.0.5 | 2024-06-04 | [38871](https://github.com/airbytehq/airbyte/pull/38871) | Refactor: use `client_side_incremental` feature |\\n| 3.0.4 | 2024-06-06 | [38798](https://github.com/airbytehq/airbyte/pull/38798) | Implement CheckpointMixin for state handling |\\n| 3.0.3 | 2024-06-06 | [39204](https://github.com/airbytehq/airbyte/pull/39204) | [autopull] Upgrade base image to v1.2.2 |\\n| 3.0.2 | 2024-05-20 | [38266](https://github.com/airbytehq/airbyte/pull/38266) | Replace AirbyteLogger with logging.Logger |\\n| 3.0.1 | 2024-04-24 | [36653](https://github.com/airbytehq/airbyte/pull/36653) | Schema descriptions and CDK 0.80.0 |\\n| 3.0.0   | 2024-04-12 | [35794](https://github.com/airbytehq/airbyte/pull/35974) | Migrate to low-code CDK (python CDK for Blocks stream)                                               |\\n| 2.2.0   | 2024-04-08 | [36890](https://github.com/airbytehq/airbyte/pull/36890) | Unpin CDK version                                                                                    |\\n| 2.1.0   | 2024-02-19 | [35409](https://github.com/airbytehq/airbyte/pull/35409) | Update users stream schema with bot type info fields and block schema with mention type info fields. |\\n| 2.0.9   | 2024-02-12 | [35155](https://github.com/airbytehq/airbyte/pull/35155) | Manage dependencies with Poetry.                                                                     |\\n| 2.0.8   | 2023-11-01 | [31899](https://github.com/airbytehq/airbyte/pull/31899) | Fix `table_row.cells` property in `Blocks` stream                                                    |\\n| 2.0.7   | 2023-10-31 | [32004](https://github.com/airtybehq/airbyte/pull/32004) | Reduce page_size on 504 errors                                                                       |\\n| 2.0.6   | 2023-10-25 | [31825](https://github.com/airbytehq/airbyte/pull/31825) | Increase max_retries on retryable errors                                                             |\\n| 2.0.5   | 2023-10-23 | [31742](https://github.com/airbytehq/airbyte/pull/31742) | Add 'synced_block' property to Blocks schema                                                         |\\n| 2.0.4   | 2023-10-19 | [31625](https://github.com/airbytehq/airbyte/pull/31625) | Fix check_connection method                                                                          |\\n| 2.0.3   | 2023-10-19 | [31612](https://github.com/airbytehq/airbyte/pull/31612) | Add exponential backoff for 500 errors                                                               |\\n| 2.0.2   | 2023-10-19 | [31599](https://github.com/airbytehq/airbyte/pull/31599) | Base image migration: remove Dockerfile and use the python-connector-base image                      |\\n| 2.0.1   | 2023-10-17 | [31507](https://github.com/airbytehq/airbyte/pull/31507) | Add start_date validation checks                                                                     |\\n| 2.0.0   | 2023-10-09 | [30587](https://github.com/airbytehq/airbyte/pull/30587) | Source-wide schema update                                                                            |\\n| 1.3.0   | 2023-10-09 | [30324](https://github.com/airbytehq/airbyte/pull/30324) | Add `Comments` stream                                                                                |\\n| 1.2.2   | 2023-10-09 | [30780](https://github.com/airbytehq/airbyte/pull/30780) | Update Start Date in config to optional field                                                        |\\n| 1.2.1   | 2023-10-08 | [30750](https://github.com/airbytehq/airbyte/pull/30750) | Add availability strategy                                                                            |\\n| 1.2.0   | 2023-10-04 | [31053](https://github.com/airbytehq/airbyte/pull/31053) | Add undeclared fields for blocks and pages streams                                                   |\\n| 1.1.2   | 2023-08-30 | [29999](https://github.com/airbytehq/airbyte/pull/29999) | Update error handling during connection check                                                        |\\n| 1.1.1   | 2023-06-14 | [26535](https://github.com/airbytehq/airbyte/pull/26535) | Migrate from deprecated `authSpecification` to `advancedAuth`                                        |\\n| 1.1.0   | 2023-06-08 | [27170](https://github.com/airbytehq/airbyte/pull/27170) | Fix typo in `blocks` schema                                                                          |\\n| 1.0.9   | 2023-06-08 | [27062](https://github.com/airbytehq/airbyte/pull/27062) | Skip streams with `invalid_start_cursor` error                                                       |\\n| 1.0.8   | 2023-06-07 | [27073](https://github.com/airbytehq/airbyte/pull/27073) | Add empty results handling for stream `Blocks`                                                       |\\n| 1.0.7   | 2023-06-06 | [27060](https://github.com/airbytehq/airbyte/pull/27060) | Add skipping 404 error in `Blocks` stream                                                            |\\n| 1.0.6   | 2023-05-18 | [26286](https://github.com/airbytehq/airbyte/pull/26286) | Add `parent` field to `Blocks` stream                                                                |\\n| 1.0.5   | 2023-05-01 | [25709](https://github.com/airbytehq/airbyte/pull/25709) | Fixed `ai_block is unsupported by API` issue, while fetching `Blocks` stream                         |\\n| 1.0.4   | 2023-04-11 | [25041](https://github.com/airbytehq/airbyte/pull/25041) | Improve error handling for API /search                                                               |\\n| 1.0.3   | 2023-03-02 | [22931](https://github.com/airbytehq/airbyte/pull/22931) | Specified date formatting in specification                                                           |\\n| 1.0.2   | 2023-02-24 | [23437](https://github.com/airbytehq/airbyte/pull/23437) | Add retry for 400 error (validation_error)                                                           |\\n| 1.0.1   | 2023-01-27 | [22018](https://github.com/airbytehq/airbyte/pull/22018) | Set `AvailabilityStrategy` for streams explicitly to `None`                                          |\\n| 1.0.0   | 2022-12-19 | [20639](https://github.com/airbytehq/airbyte/pull/20639) | Fix `Pages` stream schema                                                                            |\\n| 0.1.10  | 2022-09-28 | [17298](https://github.com/airbytehq/airbyte/pull/17298) | Use \"Retry-After\" header for backoff                                                                 |\\n| 0.1.9   | 2022-09-16 | [16799](https://github.com/airbytehq/airbyte/pull/16799) | Migrate to per-stream state                                                                          |\\n| 0.1.8   | 2022-09-05 | [16272](https://github.com/airbytehq/airbyte/pull/16272) | Update spec description to include working timestamp example                                         |\\n| 0.1.7   | 2022-07-26 | [15042](https://github.com/airbytehq/airbyte/pull/15042) | Update `additionalProperties` field to true from shared schemas                                      |\\n| 0.1.6   | 2022-07-21 | [14924](https://github.com/airbytehq/airbyte/pull/14924) | Remove `additionalProperties` field from schemas and spec                                            |\\n| 0.1.5   | 2022-07-14 | [14706](https://github.com/airbytehq/airbyte/pull/14706) | Added OAuth2.0 authentication                                                                        |\\n| 0.1.4   | 2022-07-07 | [14505](https://github.com/airbytehq/airbyte/pull/14505) | Fixed bug when normalization didn't run through                                                      |\\n| 0.1.3   | 2022-04-22 | [11452](https://github.com/airbytehq/airbyte/pull/11452) | Use pagination for User stream                                                                       |\\n| 0.1.2   | 2022-01-11 | [9084](https://github.com/airbytehq/airbyte/pull/9084)   | Fix documentation URL                                                                                |\\n| 0.1.1   | 2021-12-30 | [9207](https://github.com/airbytehq/airbyte/pull/9207)   | Update connector fields title/description                                                            |\\n| 0.1.0   | 2021-10-17 | [7092](https://github.com/airbytehq/airbyte/pull/7092)   | Initial Release                                                                                      |\n```\n\n----------------------------------------\n\nTITLE: Building the Simplesat Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Simplesat connector using airbyte-ci. This creates a dev image named 'source-simplesat:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simplesat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simplesat build\n```\n\n----------------------------------------\n\nTITLE: Building NY Times Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-nytimes` connector. It requires `airbyte-ci` to be installed. Upon successful execution, a Docker image tagged `airbyte/source-nytimes:dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nytimes/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nytimes build\n```\n\n----------------------------------------\n\nTITLE: Running Pipedrive Connector Check Command via Docker (Bash)\nDESCRIPTION: Executes the `check` command within a temporary Docker container using the `airbyte/source-pipedrive:dev` image. It mounts the local `secrets` directory containing `config.json` into the container and uses it to verify the connection credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pipedrive:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Amazon SQS Connector\nDESCRIPTION: Command to run the full test suite for the Amazon SQS connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nairbyte-ci connectors --name=destination-amazon-sqs test\n```\n\n----------------------------------------\n\nTITLE: Setting Up Fauna Database for Testing\nDESCRIPTION: Command to initialize the Fauna database with test data using a provided FQL script.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfauna eval \"$(cat examples/setup_database.fql)\" --domain localhost --port 8443 --scheme http --secret secret\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the Freshcaller connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-ads test\n```\n\n----------------------------------------\n\nTITLE: Running Pipedrive Connector Discover Command via Docker (Bash)\nDESCRIPTION: Executes the `discover` command within a temporary Docker container using the `airbyte/source-pipedrive:dev` image. It mounts the local `secrets` directory with `config.json` to discover the schema available from the Pipedrive source based on the provided configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pipedrive:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for BigQuery Destination Connector\nDESCRIPTION: Command to build the Docker image for the BigQuery destination connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-bigquery:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Plaid Source Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This Bash command uses the airbyte-ci CLI to build the manifest-only Plaid source connector Docker image. It requires airbyte-ci to be installed from the given GitHub repository and run inside the connector's directory. Upon execution, it creates an image tagged as 'airbyte/source-plaid:dev' on the host. No special parameters are required beyond the connector name; ensure network connectivity for image dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plaid/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-plaid build\n```\n\n----------------------------------------\n\nTITLE: Copying SSH Public Key using ssh-copy-id in Shell\nDESCRIPTION: This command copies the user's public SSH key to the specified remote server to enable key-based authentication for SFTP access. Replace `<username>` and `<server_ip_address>` with the actual username and server IP address. This step is part of setting up SSH key pair authentication if password authentication is not used.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nssh-copy-id <username>@<server_ip_address>\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Greenhouse Connector\nDESCRIPTION: This command runs the full CI test suite for the Greenhouse connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-greenhouse test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands for running the connector container with different operations including spec, check, discover, and read operations. Each command mounts necessary volumes for secrets and test configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-us-census/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-us-census:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-us-census:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-us-census:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-us-census:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Delighted Source Connector Commands via Docker using Bash\nDESCRIPTION: These Bash commands demonstrate how to run standard Airbyte source connector operations (spec, check, discover, read) using the previously built `airbyte/source-delighted:dev` Docker image. The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing a `config.json` file using `-v $(pwd)/secrets:/secrets`. The `read` command also requires mounting an `integration_tests` directory with a `configured_catalog.json` file using `-v $(pwd)/integration_tests:/integration_tests`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-delighted/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-delighted:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-delighted:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-delighted:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-delighted:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the manifest-only Chartmogul source connector using the Airbyte project\\'s CLI tool, `airbyte-ci`. The command `airbyte-ci connectors --name=source-chartmogul build` packages the connector according to its manifest for local development. Requires having `airbyte-ci` installed and available in your environment. Upon success, it creates a Docker image tagged as `airbyte/source-chartmogul:dev`. Input parameters include the connector name, and no direct outputs except the built image on the host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chartmogul/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chartmogul build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector as a Docker container for different operations including spec generation, config checking, discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-microsoft-sharepoint:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-sharepoint:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-sharepoint:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-microsoft-sharepoint:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit and Integration Tests with Gradle - Shell\nDESCRIPTION: This section shows how to execute unit and integration tests for the connector using Gradle from the Airbyte project root. These commands require a properly configured Java and Gradle environment. Unit tests are placed under the standard test directory, while integration and acceptance tests follow Airbyte's conventions. Outputs indicate test success or failure, and passing all tests is required for connector approval.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n```\n./gradlew :airbyte-integrations:connectors:sources-e2e-test:unitTest\n```\n```\n\nLANGUAGE: shell\nCODE:\n```\n```\n./gradlew :airbyte-integrations:connectors:sources-e2e-test:integrationTest\n```\n```\n\n----------------------------------------\n\nTITLE: Streams Table\nDESCRIPTION: Table showing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mention.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| alert | id | DefaultPaginator | ✅ |  ❌  |\n| mention | id | DefaultPaginator | ✅ |  ❌  |\n| mention_children | id | DefaultPaginator | ✅ |  ❌  |\n| account_me | id | No pagination | ✅ |  ❌  |\n| account | id | No pagination | ✅ |  ❌  |\n| alert_tag | id | No pagination | ✅ |  ❌  |\n| alert_author |  | DefaultPaginator | ✅ |  ❌  |\n| alert_tasks | id | DefaultPaginator | ✅ |  ❌  |\n| statistics |  | No pagination | ✅ |  ✅  |\n| mention_tasks | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Postmarkapp Connector\nDESCRIPTION: Standard commands for running the source connector operations including spec, check, discover, and read. These commands mount local directories to access configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postmarkapp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-postmarkapp:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-postmarkapp:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-postmarkapp:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-postmarkapp:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing the RSS Source Connector with Poetry\nDESCRIPTION: Command to install the RSS source connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Pocket Source Connector Commands in Docker - Bash\nDESCRIPTION: These Bash commands demonstrate how to run the main Airbyte source connector operations (spec, check, discover, read) inside a Docker container built for development. Dependencies include the previously built Docker image (airbyte/source-pocket:dev), proper secrets/config.json, and optionally a configured catalog for the read command. The commands mount local directories for configuration files and run the appropriate source connector command. Outputs vary based on the operation, such as specification details, connection checks, discovery of schema, or synchronized data. Limitations include requiring mounted volumes and Docker installation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pocket/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pocket:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pocket:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pocket:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pocket:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Younium connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-younium/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-younium test\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the Google Search Console source connector. Requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-google-search-console:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-search-console build\n```\n\n----------------------------------------\n\nTITLE: Running Amazon SQS Connector as Docker Container\nDESCRIPTION: Commands to run various connector operations using the built Docker image. These include spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/destination-amazon-sqs:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-amazon-sqs:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-amazon-sqs:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-amazon-sqs:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Connection Timeline Markdown Table - Status Definitions\nDESCRIPTION: A markdown table defining the different status types that can appear in the Connection Timeline for syncs, refreshes, and clears.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/review-connection-timeline.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Status     | Description                                                                                                                            |\n| ---------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| Succeeded  | Airbyte extracted 100% of the data and loaded to the destination.                                                                      |\n| Incomplete | The sync encountered an error, and should resolve itself. Airbyte loaded a subset or none of the data to the destination.              |\n| Failed     | The sync encountered a fatal error, and needs intervention to resolve. Airbyte loaded a subset or none of the data to the destination. |\n| Cancelled  | Someone cancelled the sync before it finished.                                                                                          |\n| Running    | The sync is currently running.                                                                                                         |\n```\n\n----------------------------------------\n\nTITLE: Running Recruitee Connector Commands as Docker Container in Bash\nDESCRIPTION: Standard commands for running the Recruitee source connector in Docker. These commands allow you to get the connector spec, check a connection, discover available streams, and read data using the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recruitee/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-recruitee:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recruitee:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-recruitee:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-recruitee:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Generating Airbyte Contributor List using GitHub API\nDESCRIPTION: This shell script queries the GitHub API to fetch all contributors to the Airbyte repository. It iterates through paginated results, formats each contributor's information, and sorts the final list alphabetically.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/CONTRIBUTORS.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\np=1;\nwhile true; do\n    s=$(curl \"https://api.github.com/repos/airbytehq/airbyte/contributors?page=$p\") || break\n    [ \"0\" = $(echo $s | jq length) ] && break\n    echo $s | jq -r '.[] | \"* [\" + .login + \"](\" + .html_url + \")\"'\n    p=$((p+1))\ndone | sort -f\n```\n\n----------------------------------------\n\nTITLE: Example Output of abctl local credentials Command (Shell)\nDESCRIPTION: Provides an example JSON output from the `abctl local credentials` command, displaying the Airbyte password, client ID, and client secret.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n{\n  \"password\": \"password\",\n  \"client-id\": \"client_id\",\n  \"client-secret\": \"client_secret\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Databricks Connector Docker Commands\nDESCRIPTION: Commands to run various connector operations using the built Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databricks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/destination-databricks:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-databricks:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-databricks:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-databricks:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring HoorayHR Source Connector in Markdown\nDESCRIPTION: Markdown table describing the configuration inputs for the HoorayHR source connector. It specifies the required username and password fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hoorayhr.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input              | Type     | Description        | Default Value |\n| ------------------ | -------- | ------------------ | ------------- |\n| `hoorayhrusername` | `string` | HoorayHR Username. |               |\n| `hoorayhrpassword` | `string` | HoorayHR Password. |               |\n```\n\n----------------------------------------\n\nTITLE: Validating Connector Metadata Files\nDESCRIPTION: Command to validate a connector's metadata file against the ConnectorMetadataDefinitionV0 schema and its documentation file. Both file paths must be provided as arguments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run metadata_service validate tests/fixtures/metadata_validate/valid/metadata_simple.yaml tests/fixtures/doc.md\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Commands to run the connector's Docker container for various operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-motherduck:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-motherduck:dev check --config /secrets/config.json\ncat integration_tests/messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-motherduck:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining a Mocked HTTP Test Method Structure in Python\nDESCRIPTION: Shows the core structure of a test method (`test_read_a_single_page`) within the test class, decorated with `@HttpMocker`. It outlines how `http_mocker.get` is used to register a mocked HTTP GET request (with placeholders for URL) and its expected response (with placeholders for body and status code). The method then calls the read operation and asserts the number of records returned.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    @HttpMocker()\n    def test_read_a_single_page(self, http_mocker: HttpMocker) -> None:\n\n        http_mocker.get(\n            HttpRequest(url=),\n            HttpResponse(body=, status_code=)\n        )\n\n        output = self._read(_A_CONFIG, _configured_catalog(<TODO>, SyncMode.full_refresh))\n\n        assert len(output.records) == 2\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Lokalise connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lokalise/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lokalise test\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands for Freshdesk\nDESCRIPTION: Set of Docker commands to run standard operations like spec, check, discover, and read for the Freshdesk source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshdesk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-freshdesk:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshdesk:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshdesk:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-freshdesk:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Documenting Solarwinds Service Desk Streams in Markdown\nDESCRIPTION: This snippet provides a table of supported streams for the Solarwinds Service Desk connector, including their properties such as primary key, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/solarwinds-service-desk.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| hardwares | id | DefaultPaginator | ✅ |  ✅  |\n| risks | id | DefaultPaginator | ✅ |  ✅  |\n| audits | uuid | DefaultPaginator | ✅ |  ✅  |\n| vendors | id | DefaultPaginator | ✅ |  ❌  |\n| purchase_orders | id | DefaultPaginator | ✅ |  ✅  |\n| contracts | id | DefaultPaginator | ✅ |  ✅  |\n| assets | id | DefaultPaginator | ✅ |  ✅  |\n| mobiles | id | DefaultPaginator | ✅ |  ❌  |\n| categories | id | DefaultPaginator | ✅ |  ❌  |\n| groups | id | DefaultPaginator | ✅ |  ❌  |\n| roles | id | DefaultPaginator | ✅ |  ❌  |\n| departments | id | DefaultPaginator | ✅ |  ❌  |\n| sites | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ✅  |\n| configuration_items | id | DefaultPaginator | ✅ |  ✅  |\n| solutions | id | DefaultPaginator | ✅ |  ✅  |\n| releases | id | DefaultPaginator | ✅ |  ✅  |\n| change_catalogs | id | DefaultPaginator | ✅ |  ✅  |\n| changes | id | DefaultPaginator | ✅ |  ✅  |\n| incidents | id | DefaultPaginator | ✅ |  ✅  |\n| catalog_items | id | DefaultPaginator | ✅ |  ✅  |\n| problems | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running Harness Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Harness source connector, including spec retrieval, configuration check, schema discovery, and data reading. These commands assume the existence of a config.json file in a 'secrets' directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harness/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-harness:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-harness:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-harness:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-harness:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A markdown table documenting version history with dates, pull request references, and descriptions of changes made to the Google Ads connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-ads.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version   | Date       | Pull Request                                             | Subject                                                                                                                              |\n|:----------|:-----------|:---------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|\n| 3.7.10 | 2025-01-11 | [47090](https://github.com/airbytehq/airbyte/pull/47090) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n```\n\n----------------------------------------\n\nTITLE: Configuring Zoho Billing API Authentication Parameters\nDESCRIPTION: Configuration table showing the required authentication parameters for connecting to the Zoho Billing API, including region, client ID, client secret, and refresh token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-billing.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `region` | `string` | Region.  |  |\n| `client_id` | `string` | OAuth Client ID.  |  |\n| `client_secret` | `string` | OAuth Client Secret.  |  |\n| `refresh_token` | `string` | OAuth Refresh Token.  |  |\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Metabase API using cURL\nDESCRIPTION: Command to authenticate with Metabase API and obtain a session token by sending a POST request with username and password credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/metabase.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \\\n-H \"Content-Type: application/json\" \\\n-d '{\"username\": \"person@metabase.com\", \"password\": \"fakepassword\"}' \\\nhttp://localhost:3000/api/session\n```\n\n----------------------------------------\n\nTITLE: Creating TiDB User for Airbyte (SQL)\nDESCRIPTION: SQL command to create a dedicated user named 'airbyte' in TiDB, intended for use by the Airbyte connector. Replace 'your_password_here' with a secure password. This user is configured with '%' host access, allowing connections from any host, which might need adjustment based on security policies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/tidb.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to S3 Connector\nDESCRIPTION: Command to add a new dependency to the S3 connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Markdown table defining the required configuration parameters for the NinjaOne RMM integration, including API key and start date specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/ninjaone-rmm.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. Token could be generated natively by authorize section of NinjaOne swagger documentation `https://app.ninjarmm.com/apidocs/?links.active=authorization` |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Testing SimFin Connector in Airbyte\nDESCRIPTION: This command runs the acceptance tests for the SimFin connector using airbyte-ci to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simfin/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simfin test\n```\n\n----------------------------------------\n\nTITLE: Using abctl for Airbyte Deployment\nDESCRIPTION: Introduces `abctl`, presented as the easiest and quickest method for deploying Airbyte Self-managed instances. It links to quickstart documentation for detailed usage instructions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.0.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n`abctl`\n```\n\n----------------------------------------\n\nTITLE: Testing the Shutterstock Source Connector\nDESCRIPTION: Command to run acceptance tests for the Shutterstock connector using airbyte-ci. This validates that the connector meets Airbyte's quality standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shutterstock/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shutterstock test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Pytest via Poetry - bash\nDESCRIPTION: This snippet shows the command to execute unit tests for the Commercetools connector using Pytest within the Poetry environment. It relies on the presence of properly configured test files under 'unit_tests'. The command ensures that all Python dependencies and environment variables are correctly sourced by Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input for Nested AddFields Transformation - JSON\nDESCRIPTION: Illustrates the JSON structure of a record before applying a nested AddFields transformation (contains key 'id' and a nested 'data' object). Used as input for demonstrating transformation effects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 0,\n  \"data\":\n  {\n    \"field0\": \"some_data\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests with Airbyte-CI (Bash)\nDESCRIPTION: Shows how to execute acceptance tests for the source-finage connector using Airbyte's CI command. Requires prior setup of the airbyte-ci CLI tool and assumes that a dev image for source-finage is available. This tests the connector's conformance to required specifications, ensuring expected integration and functionality within Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finage test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands via Docker - Bash\nDESCRIPTION: Executes connector commands (spec, check, discover, read) within a Docker container. Includes volume mounts for configuration ('secrets') and, for reading, mounting an integration tests directory for catalog input. Requires the built image 'airbyte/source-azure-table:dev' and that the configuration and catalog files exist at the specified mount paths.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-azure-table:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-azure-table:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-azure-table:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-azure-table:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Checking installed pipx packages\nDESCRIPTION: Command to list all packages installed with pipx to verify if airbyte-ci is installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\npipx list\n```\n\n----------------------------------------\n\nTITLE: Local Connector Operations\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-microsoft-dataverse spec\npoetry run source-microsoft-dataverse check --config secrets/config.json\npoetry run source-microsoft-dataverse discover --config secrets/config.json\npoetry run source-microsoft-dataverse read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: Markdown table showing version history and changes made to the connector over time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/ninjaone-rmm.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.4 | 2025-04-19 | [58525](https://github.com/airbytehq/airbyte/pull/58525) | Update dependencies |\n| 0.0.3 | 2025-04-12 | [57868](https://github.com/airbytehq/airbyte/pull/57868) | Update dependencies |\n| 0.0.2 | 2025-04-05 | [57325](https://github.com/airbytehq/airbyte/pull/57325) | Update dependencies |\n| 0.0.1 | 2025-04-04 | [57013](https://github.com/airbytehq/airbyte/pull/57013) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry - bash\nDESCRIPTION: Runs the unit tests for the Github source connector using Poetry to invoke pytest. All test modules inside the 'unit_tests' directory are executed. Requires test dependencies to be installed; outputs the test results in the terminal. Typically used to validate connector logic before further testing or build steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Cortex Connector Dependencies with Poetry\nDESCRIPTION: Installs the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev.\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Weatherstack Source Connector\nDESCRIPTION: Command to build the Docker image for the Weatherstack source connector using airbyte-ci. This creates an image tagged as airbyte/source-weatherstack:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-weatherstack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-weatherstack build\n```\n\n----------------------------------------\n\nTITLE: Specifying JDBC URL Parameters for Teradata Connection - Markdown\nDESCRIPTION: This documentation code snippet demonstrates the format for custom JDBC URL parameters to be used with the Airbyte Teradata connector. The parameters are expressed as key-value pairs separated by '&' symbols. Placeholders like 'key1', 'key2', etc., represent actual configuration keys and values needed for your JDBC connection string. These parameters are optional but allow for granular control of the connection, and will be appended to the end of the JDBC URL used by Airbyte to connect to Teradata.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/teradata.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nkey1=value1&key2=value2&key3=value3\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte CI with Poetry\nDESCRIPTION: Installation and activation of Airbyte CI using Poetry dependency management from the pipelines directory. Creates an isolated Python environment\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# From airbyte-ci/connectors/pipelines\npoetry install\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Smartsheets Connector\nDESCRIPTION: Command to execute unit tests for the Smartsheets connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Testing Picqer Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This shell command executes acceptance tests for the 'source-picqer' connector using the airbyte-ci CLI. The '--name' flag specifies the relevant connector, and 'test' initiates the testing suite. 'airbyte-ci' must be installed and configured beforehand. The command should be executed in a terminal on the local machine where development occurs. The expected output is the result of the test suite, which validates connector functionality and integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-picqer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-picqer test\n```\n\n----------------------------------------\n\nTITLE: Publishing Updated Link Checker Docker Image\nDESCRIPTION: Command to update the Docker image used by the link checker tool. This would typically rebuild and publish the Docker image to a registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/site/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./tools/site/link_checker.sh publish\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table in Markdown\nDESCRIPTION: Outlines the supported features of the Microsoft Dataverse connector, including sync capabilities and connection options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-dataverse.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | Supported?\\(Yes/No\\) | Notes                                                      |\n| :---------------------------- | :------------------- | :--------------------------------------------------------- |\n| Full Refresh Sync             | Yes                  |                                                            |\n| Incremental Sync              | Yes                  |                                                            |\n| CDC                           | Yes                  | Not all entities support it. Deleted data only have the ID |\n| Replicate Incremental Deletes | Yes                  |                                                            |\n| SSL connection                | Yes                  |                                                            |\n| Namespaces                    | No                   |                                                            |\n```\n\n----------------------------------------\n\nTITLE: Building Pipedrive Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the Pipedrive source connector. It requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-pipedrive:dev` locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pipedrive build\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for SingleStore Connector via Gradle\nDESCRIPTION: Command to run acceptance and custom integration tests for the SingleStore destination connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-singlestore/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-singlestore:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated User for TiDB in SQL\nDESCRIPTION: SQL commands to create a dedicated read-only user for Airbyte in TiDB. This includes creating the user and granting SELECT permissions on the relevant database.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tidb.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';\n```\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON <database name>.* TO 'airbyte'@'%';\n```\n\n----------------------------------------\n\nTITLE: Running the Connector as a Docker Container\nDESCRIPTION: These commands show how to run the connector operations using the Docker container, including mounting local directories for configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-rki-covid:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rki-covid:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rki-covid:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-rki-covid:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Missive Connector with airbyte-ci\nDESCRIPTION: Command to build a development version of the Missive source connector. Creates a dev image tagged as 'source-missive:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-missive/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-missive build\n```\n\n----------------------------------------\n\nTITLE: Rendering ManifestYamlDefinitions Component - React (JavaScript)\nDESCRIPTION: This JSX snippet renders the 'ManifestYamlDefinitions' React component, which is expected to display YAML manifest definitions on the documentation page. There are no incoming parameters in this usage. Its output is a rendered component within the page structure, and its rendering may depend on external data or props not shown here.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/reference.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n<ManifestYamlDefinitions />\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Dev Null Connector Docker Commands\nDESCRIPTION: Various Docker commands for running the Dev Null connector operations including spec, check, discover, and read functions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dev-null/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-dev-null:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-dev-null:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-dev-null:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-dev-null:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: GitHub Branch Format Examples\nDESCRIPTION: Examples of valid branch format specifications for the GitHub connector configuration, showing how to specify repositories and their branches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/invalid_links.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte/master airbytehq/airbyte/my-branch\n```\n\n----------------------------------------\n\nTITLE: Building Mendeley Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Mendeley source connector using airbyte-ci. Creates a dev image tagged as source-mendeley:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mendeley/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mendeley build\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies for Airbyte Connector - bash\nDESCRIPTION: This snippet outlines activation of a Python virtual environment and the installation of required dependencies for development and testing of the Outbrain Amplify Airbyte connector. It assumes the .venv directory exists and includes installing from requirements.txt and adding test dependencies. Prerequisites are a valid virtual environment and the requirements file. The output is a development-ready isolated Python setup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\npip install '.[tests]'\n```\n\n----------------------------------------\n\nTITLE: Running Trello Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run standard source connector operations using the Trello source connector docker image. They include operations for spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trello/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-trello:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-trello:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-trello:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-trello:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using filesizeformat Filter in Jinja2\nDESCRIPTION: Demonstrates the `filesizeformat` filter in Jinja2, which formats a number of bytes into a human-readable file size (e.g., KB, MB, GB). The example formats 123456789 bytes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_22\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 123456789|filesizeformat }}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector's main functions locally including spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Yellowbrick Connector\nDESCRIPTION: Various Docker commands for running the Yellowbrick connector, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-yellowbrick/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-yellowbrick:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-yellowbrick:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-yellowbrick:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-yellowbrick:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the connector using Poetry package manager, which will update the pyproject.toml and poetry.lock files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Source Close-com Connector Commands via Docker - Bash\nDESCRIPTION: This sequence of bash commands shows how to run the source-close-com connector using Docker, executing typical lifecycle commands (spec, check, discover, read). Standard input parameters include configuration file mounting (for check, discover, read), and optional mounting of integration test catalogs. The dependencies are a previously built airbyte/source-close-com:dev image, and the presence of configuration and (optionally) test catalog files in local directories. Expected outputs include command results for connector specification, connection checking, schema discovery, and data reading; errors may occur if the required files do not exist or are malformatted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-close-com/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-close-com:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-close-com:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-close-com:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-close-com:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Invoice Ninja Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Invoice Ninja source connector using airbyte-ci. It's used to verify the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-invoiceninja/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-invoiceninja test\n```\n\n----------------------------------------\n\nTITLE: Building Bugsnag Source Connector using airbyte-ci\nDESCRIPTION: Command to build a development Docker image of the Bugsnag source connector using airbyte-ci. Creates an image tagged as source-bugsnag:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bugsnag/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bugsnag build\n```\n\n----------------------------------------\n\nTITLE: Running Getlago Connector Commands in Docker\nDESCRIPTION: Series of commands to run the standard source connector operations (spec, check, discover, read) using the Getlago connector Docker image. These commands mount local directories for configuration and test data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-getlago/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-getlago:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-getlago:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-getlago:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-getlago:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally using Poetry - Bash\nDESCRIPTION: Executes various Airbyte connector commands (spec, check, discover, read) locally via Poetry. Requires a properly configured config.json in the secrets directory. These commands test the connector's integration with Google Drive and produce configuration specifications, check authentication, discover available streams, and start data reading based on a catalog file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-google-drive spec\npoetry run source-google-drive check --config secrets/config.json\npoetry run source-google-drive discover --config secrets/config.json\npoetry run source-google-drive read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile Definition\nDESCRIPTION: Example Dockerfile for building a custom version of the connector by extending the base image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-zoho-crm:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container - Bash\nDESCRIPTION: Shows examples of running the Airbyte source-zendesk-support connector inside a Docker container. These commands map configuration and test files from the host to the container using -v, and specify the connector operation to perform. Dependencies include the previously built docker image (airbyte/source-zendesk-support:dev) and valid config/catalog files. Outputs are streamed to the terminal for operational feedback.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zendesk-support:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-support:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-support:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zendesk-support:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Factorial API Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-factorial connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-factorial/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-factorial test\n```\n\n----------------------------------------\n\nTITLE: Adding a Package Dependency to the Project with Poetry in Bash\nDESCRIPTION: This Bash command uses Poetry to add a new dependency to the project. The '<package-name>' placeholder should be replaced with the desired Python package. Upon execution, Poetry updates the project's dependency list and lockfile. Prerequisites include a functional Poetry installation and write access to 'pyproject.toml' and 'poetry.lock'. The command produces no standard output but updates project configuration files. It must not be run outside the project’s directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using Poetry - Bash\nDESCRIPTION: Installs the Python dependencies for the Google Drive connector using Poetry. Requires Python (~=3.9) and Poetry (~=1.7) to be installed. The '--with dev' flag installs development dependencies as defined in pyproject.toml. This command should be run from the connector directory and will set up a local development environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands via Docker Container\nDESCRIPTION: These commands execute the connector operations ('spec', 'check', 'discover', 'read') within a Docker container using the previously built 'airbyte/source-couchbase:dev' image. Volumes are mounted to provide access to the configuration file in the 'secrets' directory and the catalog/test files in the 'integration_tests' directory from the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\ndocker run --rm airbyte/source-couchbase:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-couchbase:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-couchbase:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-couchbase:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Statuspage Source Connector Docker Commands\nDESCRIPTION: Commands to run various operations of the Statuspage source connector using Docker. These include displaying the connector spec, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-statuspage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-statuspage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-statuspage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-statuspage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-statuspage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Illumina Basespace Data Streams in Markdown\nDESCRIPTION: Markdown table listing the available data streams for the Illumina Basespace connector. It includes information on primary keys, pagination, and support for full and incremental syncs for each stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/illumina-basespace.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| projects | Id | DefaultPaginator | ✅ |  ❌  |\n| runs | Id | DefaultPaginator | ✅ |  ❌  |\n| samples | Id | DefaultPaginator | ✅ |  ❌  |\n| sample_files | Id | DefaultPaginator | ✅ |  ❌  |\n| run_files | Id | DefaultPaginator | ✅ |  ❌  |\n| appsessions | Id | DefaultPaginator | ✅ |  ❌  |\n| appresults | Id | DefaultPaginator | ✅ |  ❌  |\n| appresults_files | Id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Installing and Building Docusaurus with pnpm\nDESCRIPTION: Instructions for installing pnpm package manager and building the Docusaurus documentation site. The commands install pnpm via Homebrew, navigate to the docusaurus directory, install dependencies, and build the site.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docusaurus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install pnpm\n\ncd docusaurus\npnpm install\npnpm build\n```\n\n----------------------------------------\n\nTITLE: Building and Running Custom Duckdb Connector Docker Image\nDESCRIPTION: Commands to build a custom Docker image for the Duckdb connector and run various connector commands using the Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-duckdb:dev .\ndocker run --rm airbyte/destination-duckdb:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-duckdb:dev check --config /secrets/config.json\ncat integration_tests/messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-duckdb:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zenloop:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zenloop:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zenloop:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zenloop:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Spec Command via Docker (Bash)\nDESCRIPTION: Executes the `spec` command within a Docker container using the previously built `airbyte/source-google-pagespeed-insights:dev` image. This command outputs the connector's specification, detailing its configuration parameters. Requires Docker and the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-pagespeed-insights:dev spec\n```\n\n----------------------------------------\n\nTITLE: Updating Default Service Account Permissions with kubectl\nDESCRIPTION: Command to patch the default airbyte-admin-role to include 'secrets' in the resources list using kubectl.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrade-service-account.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n <namespace> patch role airbyte-admin-role --type='json' -p='[{\"op\": \"replace\", \"path\": \"/rules/0/resources\", \"value\": [\"jobs\", \"pods\", \"pods/log\", \"pods/exec\", \"pods/attach\", \"secrets\"]}]'\n```\n\n----------------------------------------\n\nTITLE: Running Intercom Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Intercom source connector using Docker. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intercom/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-intercom:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-intercom:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-intercom:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-intercom:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Desk Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Zoho Desk connector using airbyte-ci. Creates a dev image tagged as source-zoho-desk:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-desk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-desk build\n```\n\n----------------------------------------\n\nTITLE: Triggering Postgres Source Connector Performance Test in Pull Request (Shell)\nDESCRIPTION: Triggers the performance test for the source-postgres connector directly from a pull request comment. The command optionally takes '--cpulimit' to constrain the CPU count (minimum 2) and '--memorylimit' to restrict memory usage (minimum 6MB, specify units). Used in continuous integration environments to benchmark connector performance within the PR lifecycle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postgres/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n/test-performance connector=connectors/source-postgres [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Building the Secoda Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Secoda source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-secoda:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-secoda/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-secoda build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Google Webfonts Source Connector\nDESCRIPTION: Command to run the full test suite for the Google Webfonts source connector using airbyte-ci. This is used to validate changes before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-webfonts/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-webfonts test\n```\n\n----------------------------------------\n\nTITLE: Installing Vale on Different Platforms\nDESCRIPTION: Commands for installing Vale, a prose linting tool, on Windows, Mac, and Linux systems using their respective package managers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nchoco install vale\n```\n\nLANGUAGE: bash\nCODE:\n```\nbrew install vale\n```\n\nLANGUAGE: bash\nCODE:\n```\nsnap install vale\n```\n\n----------------------------------------\n\nTITLE: Building source-elasticemail Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the source-elasticemail connector. This creates a dev image tagged as 'source-elasticemail:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticemail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-elasticemail build\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests with Gradle (Shell)\nDESCRIPTION: Executes the integration and acceptance tests for the e2e test source connector using the Gradle wrapper script. Note that the path 'sources-e2e-test' might refer to a shared test module rather than the specific 'source-e2e-test-cloud' module. This command should be run from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:sources-e2e-test:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Testing Survicate Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Survicate connector using airbyte-ci tool. Executes the test suite to verify connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-survicate/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-survicate test\n```\n\n----------------------------------------\n\nTITLE: Testing CDC with Position Logging in JSON\nDESCRIPTION: This snippet shows CDC records with additional position logging (_ab_cdc_log_pos). It includes examples of updates, deletions, and insertions to test various CDC scenarios.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"pos_dedup_cdcx\",\n    \"data\": {\n      \"id\": 1,\n      \"name\": \"mazda\",\n      \"_ab_cdc_updated_at\": 1623849130530,\n      \"_ab_cdc_lsn\": 26971624,\n      \"_ab_cdc_log_pos\": 33274,\n      \"_ab_cdc_deleted_at\": null\n    },\n    \"emitted_at\": 1623859926\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Local Airbyte Instance on Port 8001 via abctl (Bash)\nDESCRIPTION: Uses `abctl` to install and run a local Airbyte instance, specifically configuring the service to be accessible on TCP port 8001 instead of the default 8000. This is typically done to run the backend separately for frontend (webapp) development. Requires `abctl`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nabctl local install --port 8001\n```\n\n----------------------------------------\n\nTITLE: YAML Nested Value Reference\nDESCRIPTION: Shows how to reference nested values within YAML structures using path notation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/references.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndict:\n  limit: 50\nlimit_ref: \"#/dict/limit\"\n```\n\n----------------------------------------\n\nTITLE: Formatting GitHub Repository Names\nDESCRIPTION: Examples showing the correct format for specifying GitHub repositories, including single repository, multiple repositories, and organization-wide repository access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/correct.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte            # single repository\nairbytehq/airbyte airbytehq/another-repo  # multiple repositories\nairbytehq/*                                 # all repositories in organization\n```\n\n----------------------------------------\n\nTITLE: Cleaning Airbyte CI Installation\nDESCRIPTION: Command to clean the Airbyte CI installation, which can be useful for troubleshooting or removing it completely.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nmake tools.airbyte-ci.clean\n```\n\n----------------------------------------\n\nTITLE: Building Databricks Connector via Gradle in Java\nDESCRIPTION: Command to build the Databricks destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databricks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-databricks:build\n```\n\n----------------------------------------\n\nTITLE: Documenting Invoiceninja Connector Changelog in Markdown\nDESCRIPTION: Markdown table showing the version history and changes for the Invoiceninja connector. It includes version numbers, dates, pull request links, and descriptions of updates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiceninja.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.20 | 2025-04-19 | [58207](https://github.com/airbytehq/airbyte/pull/58207) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57742](https://github.com/airbytehq/airbyte/pull/57742) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57107](https://github.com/airbytehq/airbyte/pull/57107) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56637](https://github.com/airbytehq/airbyte/pull/56637) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56052](https://github.com/airbytehq/airbyte/pull/56052) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55470](https://github.com/airbytehq/airbyte/pull/55470) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [54827](https://github.com/airbytehq/airbyte/pull/54827) | Update dependencies |\n| 0.0.13 | 2025-02-22 | [54317](https://github.com/airbytehq/airbyte/pull/54317) | Update dependencies |\n| 0.0.12 | 2025-02-15 | [53798](https://github.com/airbytehq/airbyte/pull/53798) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53289](https://github.com/airbytehq/airbyte/pull/53289) | Update dependencies |\n| 0.0.10 | 2025-02-01 | [52764](https://github.com/airbytehq/airbyte/pull/52764) | Update dependencies |\n| 0.0.9 | 2025-01-25 | [52273](https://github.com/airbytehq/airbyte/pull/52273) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [51780](https://github.com/airbytehq/airbyte/pull/51780) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51179](https://github.com/airbytehq/airbyte/pull/51179) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50664](https://github.com/airbytehq/airbyte/pull/50664) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50134](https://github.com/airbytehq/airbyte/pull/50134) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49605](https://github.com/airbytehq/airbyte/pull/49605) | Update dependencies |\n| 0.0.3 | 2024-12-12 | [49234](https://github.com/airbytehq/airbyte/pull/49234) | Update dependencies |\n| 0.0.2 | 2024-12-11 | [48917](https://github.com/airbytehq/airbyte/pull/48917) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.1 | 2024-11-07 | | Initial release by [@ombhardwajj](https://github.com/ombhardwajj) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Airbyte State Message Format Example\nDESCRIPTION: Example of an Airbyte STATE message format used for tracking synchronization state. Shows the structure for maintaining incremental sync state with a start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/namespace_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"data\": {\"start_date\": \"2022-08-17\"}}}\n```\n\n----------------------------------------\n\nTITLE: Selecting Records in Nested Arrays Using Wildcards - YAML\nDESCRIPTION: YAML selector configuration demonstrating use of the '*' wildcard to iterate and select all 'record' objects nested under each element in a 'data' array. The field_path [\"data\", \"*\", \"record\"] allows dynamic extraction from varying API responses where sub-objects are stored in array elements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  extractor:\n    field_path: [\"data\", \"*\", \"record\"]\n```\n\n----------------------------------------\n\nTITLE: Running S3 Connector Unit Tests\nDESCRIPTION: Command to execute unit tests for the S3 connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Creating a Read-Only Role for Incremental Sync in Fauna\nDESCRIPTION: This FQL query creates a role with permissions needed for incremental sync, including read access to collections, indexes, a specific collection, and the incremental sync index.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true },\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"COLLECTION_NAME\"),\n      actions: { read: true },\n    },\n    {\n      resource: Index(\"INDEX_NAME\"),\n      actions: { read: true },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Four Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema for the four_weekly_active_users stream in the Google Analytics connector. It includes the count of 28-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_28dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Running SFTP-Bulk Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for testing and debugging. These commands handle specification, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-sftp-bulk spec\npoetry run source-sftp-bulk check --config secrets/config.json\npoetry run source-sftp-bulk discover --config secrets/config.json\npoetry run source-sftp-bulk read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Encoding OAuth Header with Base64 Credentials - Diff/YAML\nDESCRIPTION: This diff example modifies an Airbyte manifest to encode client ID and client secret together in the SECRETHEADER using base64 encoding. Instead of sending the client_secret directly, it concatenates client_id and client_secret with a colon and applies the b64encode filter. Requires Jinja2-style template support and the b64encode filter in the YAML rendering engine. The output is a manifest supporting basic-like encoded header authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n--- secret_header_manifest.yml\n+++ secret_header_manifest.yml\n      spec:\n           https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n           redirect_uri_value }}&state={{ state }}\n         access_token_url: >-\n-          https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n+          https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&code={{auth_code_value}}\n+        access_token_headers:\n-          SECRETHEADER: \"{{ client_secret_value }}\"\n+          SECRETHEADER: \"{{ (client_id_value ~ ':' ~ client_secret_value) | b64encode }}\"\n       complete_oauth_output_specification:\n         required:\n```\n\n----------------------------------------\n\nTITLE: Obtaining Yandex Metrica API Key via GET Request (HTTP)\nDESCRIPTION: This snippet shows the structure of an HTTP GET request used to obtain an OAuth API key (token) from Yandex. It requires substituting 'YOUR_CLIENT_ID' with the actual Client ID obtained from the Yandex app settings. Accessing this URL in a browser or via an HTTP client will initiate the OAuth flow and return the API key upon successful authorization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/yandex-metrica.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttps://oauth.yandex.com/authorizE?response_type=token&client_id=YOUR_CLIENT_ID\n```\n\n----------------------------------------\n\nTITLE: API Endpoint Example for Merge\nDESCRIPTION: Example GET request endpoint for accessing account details through the Merge API v1.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/merge.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.merge.dev/api/ats/v1/account-details\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Tokens Nested Under Data Placeholders - JSON\nDESCRIPTION: A JSON response where both access_token and refresh_token are respectively nested under 'access_token_placeholder' and 'refresh_token_placeholder', which are themselves inside a root data object. The nested structure is critical for demonstration of advanced response parsing and mapping in Airbyte configs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_71\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"data\\\": {\\n    \\\"access_token_placeholder\\\": {\\n      \\\"access_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\"\\n    },\\n    \\\"refresh_token_placeholder\\\" {\\n      \\\"refresh_token\\\": \\\"YOUR_REFRESH_TOKEN_123\\\"\\n    }\\n  }\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Creating GCS Configuration Secrets Using kubectl Command\nDESCRIPTION: Bash command to create Kubernetes secrets for Airbyte with Google Cloud Storage (GCS) using kubectl. This method creates the same secret as the YAML manifest but via CLI and references a GCP credentials file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic airbyte-config-secrets \\\n  --from-literal=license-key='' \\\n  --from-literal=database-host='' \\\n  --from-literal=database-port='' \\\n  --from-literal=database-name='' \\\n  --from-literal=database-user='' \\\n  --from-literal=database-password='' \\\n  --from-literal=instance-admin-email='' \\\n  --from-literal=instance-admin-password='' \\\n  --from-file=gcp.json\n  --namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Docker image for the News API source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-news-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-news-api build\n```\n\n----------------------------------------\n\nTITLE: Example XML Input Record for Message Body Key\nDESCRIPTION: This JSON example shows an input record containing an XML string that can be used with the Message Body Key feature to send pure XML messages to SQS.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/amazon-sqs.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"my_xml_field\": \"<something>value</something>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Using Poetry - Bash\nDESCRIPTION: Installs the Python dependencies for the Airbyte File Source Connector using Poetry in the development environment. Requires Poetry (~=1.7) and Python (~=3.9). Outputs installed dependencies listed in pyproject.toml and poetry.lock. Use from the connector's root directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Testing the Pretix Connector Locally\nDESCRIPTION: Command to run acceptance tests for the Pretix connector using airbyte-ci. This validates the connector's functionality against defined acceptance criteria.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pretix/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pretix test\n```\n\n----------------------------------------\n\nTITLE: Zoho Invoice Configuration Parameters Table\nDESCRIPTION: Configuration table showing required input parameters for authenticating with the Zoho Invoice API, including client credentials, organization ID and region settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-invoice.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id` | `string` | Client ID.  |  |\n| `client_secret` | `string` | Client secret.  |  |\n| `client_refresh_token` | `string` | Refresh token.  |  |\n| `organization_id` | `string` | Organization ID. TO be provided if a user belongs to multiple organizations |  |\n| `region` | `string` | Region.  |  |\n```\n\n----------------------------------------\n\nTITLE: Resetting Airbyte Environment\nDESCRIPTION: Series of commands to reset the local Airbyte development environment, including removing Docker volumes and rebuilding the project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nabctl local uninstall --persisted\nrm -rf ~/.airbyte/\n./gradlew clean build\nabctl local install --values values.yaml\n```\n\n----------------------------------------\n\nTITLE: Maximum Rate Limit Exceeded Message\nDESCRIPTION: Error message displayed when maximum API request capacity is reached after multiple attempts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"Max try rate limit exceeded...\"\n```\n\n----------------------------------------\n\nTITLE: Running Surveymonkey Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally using Poetry, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-surveymonkey spec\npoetry run source-surveymonkey check --config secrets/config.json\npoetry run source-surveymonkey discover --config secrets/config.json\npoetry run source-surveymonkey read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Zendesk Sell connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sell/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-sell test\n```\n\n----------------------------------------\n\nTITLE: Available Data Streams Configuration\nDESCRIPTION: Table defining the available data streams in the Blogger connector, including their primary keys, pagination details, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/blogger.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| blogs | id | DefaultPaginator | ✅ |  ❌  |\n| posts |  | DefaultPaginator | ✅ |  ❌  |\n| pages | id | DefaultPaginator | ✅ |  ❌  |\n| comments | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Customizing Build Process Example\nDESCRIPTION: Example Python code showing how to customize the build process using pre and post install hooks\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from dagger import Container\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for Connector\nDESCRIPTION: Example Dockerfile for building a custom version of the connector based on the official image\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pgvector/README.md#2025-04-23_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-pgvector:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role Trust Relationship for Airbyte\nDESCRIPTION: This JSON policy establishes a trust relationship between the IAM role and Airbyte's AWS account. It includes an external ID condition to prevent the 'confused deputy' problem. The external ID should be set to the Airbyte workspace ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-data-lake.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::094410056844:user/delegated_access_user\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"{your-airbyte-workspace-id}\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Updating CDC Replication Configuration\nDESCRIPTION: SQL query to update source configurations in the actor table for connections using CDC replication method after upgrading from version 0.6.8 to 0.6.9.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql/mysql-troubleshooting.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"CDC\"}', true)\nWHERE actor_definition_id ='435bb9a5-7887-4809-aa58-28c27df0d7ad' AND (configuration->>'replication_method' = 'CDC');\n```\n\n----------------------------------------\n\nTITLE: Running Sonar Cloud Source Connector Docker Commands\nDESCRIPTION: Various Docker commands to run the Sonar Cloud source connector for different operations such as spec, check, discover, and read. These commands use the built Docker image and require configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sonar-cloud/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sonar-cloud:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sonar-cloud:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sonar-cloud:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sonar-cloud:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Listing GCP Service Account Permissions for GCS Destination Testing\nDESCRIPTION: This snippet shows the permissions assigned to the GCP service account used for testing the GCS destination connector. It includes the required permissions for multipart uploads and basic object operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-gcs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstorage.multipartUploads.abort\nstorage.multipartUploads.create\nstorage.objects.create\nstorage.objects.delete\nstorage.objects.get\nstorage.objects.list\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-only Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet demonstrates how to build the Docker image for the `source-dixa` manifest-only connector using the `airbyte-ci` command-line interface. Prerequisites include installing the `airbyte-ci` tool and having access to the relevant project directory. The command tags the resulting Docker image as `airbyte/source-dixa:dev`, which is referenced in subsequent execution commands.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dixa/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dixa build\n```\n\n----------------------------------------\n\nTITLE: Running Harvest Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Harvest source connector using Docker. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harvest/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-harvest:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-harvest:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-harvest:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-harvest:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running AWS Cloudtrail Connector as Docker Container - Bash\nDESCRIPTION: This Bash snippet demonstrates running the AWS Cloudtrail source connector Docker image with different Airbyte source commands (spec, check, discover, read). It requires a built image tagged as airbyte/source-aws-cloudtrail:dev, valid credentials in a config file, and mount points for secrets/catalog as necessary. Each line executes a different connector operation, with the read command additionally mounting an integration_tests directory and using a configured catalog. Outputs depend on the type of Airbyte command executed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aws-cloudtrail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-aws-cloudtrail:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aws-cloudtrail:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-aws-cloudtrail:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-aws-cloudtrail:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Duckdb Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally, including spec, check, discover, and write.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config integration_tests/config.json\npython main.py discover --config integration_tests/config.json\ncat integration_tests/messages.jsonl| python main.py write --config integration_tests/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Dockerhub Source Connector as Docker Container (bash)\nDESCRIPTION: These bash commands demonstrate standard execution patterns for the 'source-dockerhub' Airbyte connector via Docker. Each command uses 'docker run' to perform a distinct Airbyte command (spec, check, discover, read), with the image previously built or pulled. Some commands mount local directories holding secrets/configuration or catalog files into the container. Required prerequisites include a valid Docker image ('airbyte/source-dockerhub:dev') and correct configuration files in the specified mounting paths. Commands output specification details, validate source configuration, list schema/catalog, or perform source data reading. Errors may result from misconfigured paths or missing files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dockerhub/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-dockerhub:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dockerhub:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-dockerhub:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-dockerhub:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Avni Source Connector Integration and Acceptance Tests Using Gradle - Shell\nDESCRIPTION: Executes all integration and acceptance tests for the connector via Gradle. These tests validate the connector's interactions with external systems and ensure it meets acceptance criteria. Must be run from the project's root and requires that the integration test configuration files are correctly set up.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-avni:integrationTest\n```\n\n----------------------------------------\n\nTITLE: MixMax Configuration Schema\nDESCRIPTION: Configuration parameters required for setting up the MixMax connector. Includes API key for authentication and start date for data synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mixmax.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and integration tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-youtube-analytics:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-youtube-analytics:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-youtube-analytics:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-youtube-analytics:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Iterable Connector\nDESCRIPTION: Uses Poetry to add a new package dependency to the Iterable connector project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Orb Source Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This snippet builds the manifest-only Orb source connector docker image using airbyte-ci. The command expects airbyte-ci to be installed and available on your PATH. It takes the connector name as an argument and outputs a development docker image tagged as airbyte/source-orb:dev. Users must ensure all prerequisites for Airbyte container builds are satisfied.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-orb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-orb build\n```\n\n----------------------------------------\n\nTITLE: Running Local S3 Connector Commands\nDESCRIPTION: Series of commands to run the S3 connector locally, including spec generation, configuration checking, schema discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-s3 spec\npoetry run source-s3 check --config secrets/config.json\npoetry run source-s3 discover --config secrets/config.json\npoetry run source-s3 read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Alpaca Broker API Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Alpaca Broker API connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-alpaca-broker-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-alpaca-broker-api test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Northpass LMS connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-northpass-lms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-northpass-lms build\n```\n\n----------------------------------------\n\nTITLE: Running Secoda Source Connector Commands in Docker\nDESCRIPTION: Standard commands for running the Secoda source connector in Docker. These commands cover specification output, configuration checking, schema discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-secoda/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-secoda:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-secoda:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-secoda:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-secoda:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands for running the connector operations in a Docker container, including volume mounting for secrets and test configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-microsoft-onedrive:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-onedrive:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-onedrive:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-microsoft-onedrive:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Snowflake Cortex Connector\nDESCRIPTION: Example Dockerfile to build a custom version of the Snowflake Cortex connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-snowflake-cortex:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for various operations like spec generation, configuration checking, source discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-microsoft-sharepoint spec\npoetry run source-microsoft-sharepoint check --config secrets/config.json\npoetry run source-microsoft-sharepoint discover --config secrets/config.json\npoetry run source-microsoft-sharepoint read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various connector operations using the Docker image. They include spec, check, discover, and read operations, with appropriate volume mounts for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kafka/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-kafka:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kafka:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kafka:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-kafka:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands via Docker (Shell)\nDESCRIPTION: Executes standard Airbyte connector commands (spec, check, discover, read) using the locally built 'airbyte/source-e2e-test-cloud:dev' Docker image. The 'check', 'discover', and 'read' commands require mounting local directories containing configuration files ('secrets') and potentially catalog files ('integration_tests').\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm airbyte/source-e2e-test-cloud:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-e2e-test-cloud:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-e2e-test-cloud:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-e2e-test-cloud:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the AWS Cloudtrail Connector CI Test Suite with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet provides the required command to execute the full local test suite for the AWS Cloudtrail connector via Airbyte's CI tools. It depends on the airbyte-ci utility, which runs the relevant tests for the specified connector. Outputs will be test results indicating pass/failure for the connector as implemented and built locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aws-cloudtrail/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aws-cloudtrail test\n```\n\n----------------------------------------\n\nTITLE: Setup Steps for Airbyte Cloud - Markdown List\nDESCRIPTION: Numbered list of configuration steps for setting up Amazon Seller Partner source in Airbyte Cloud\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-seller-partner.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n1. [Log into your Airbyte Cloud](https://cloud.airbyte.com/workspaces) account.\n2. Click Sources and then click + New source.\n3. On the Set up the source page, select Amazon Seller Partner from the Source type dropdown.\n4. Enter a name for the Amazon Seller Partner connector.\n5. Click `Authenticate your account`.\n6. Log in and Authorize to your Amazon Seller Partner account.\n7. For `Start Date`, enter the date in `YYYY-MM-DD` format. The data added on and after this date will be replicated. This field is optional - if not provided or older than 2 years ago from today, the date 2 years ago from today will be used.\n8. For `End Date`, enter the date in `YYYY-MM-DD` format. Any data after this date will not be replicated. This field is optional - if not provided, today's date will be used.\n9. **Financial Events Step Size**: Select the time window size for fetching financial events data.\n10. You can specify report options for each stream using **Report Options** section.\n11. For `Wait between requests to avoid fatal statuses in reports`, enable if you want to use wating time between requests to avoid fatal statuses in report based streams.\n12. Click `Set up source`.\n```\n\n----------------------------------------\n\nTITLE: Running the Fauna Connector Commands\nDESCRIPTION: Series of commands to test the Fauna connector functionality. This includes checking the specification, verifying the connection, discovering available schemas, and reading data using the configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/examples/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config examples/config_localhost.json\npython main.py discover --config examples/config_localhost.json\npython main.py read --config examples/config_localhost.json --catalog examples/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte STATE Message with Global State in JSON\nDESCRIPTION: An example of an Airbyte STATE message with global state information. This message contains a start_date parameter in the shared_state object, which is typically used for incremental synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/number_data_type_array_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Teamwork Source Connector Parameters\nDESCRIPTION: This table outlines the configuration parameters required for setting up the Teamwork source connector. It includes the username, password, site name, and start date for data synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/teamwork.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n| `site_name` | `string` | Site Name. The teamwork site name which appears on the url |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Cloning Forked Airbyte Platform Repository (Bash)\nDESCRIPTION: Clones the user's fork of the `airbytehq/airbyte-platform` repository using SSH and then navigates into the newly created `airbyte-platform` directory. `{YOUR_USERNAME}` should be replaced with the user's GitHub username. Requires `git` and SSH setup for GitHub.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:{YOUR_USERNAME}/airbyte-platform.git\ncd airbyte-platform\n```\n\n----------------------------------------\n\nTITLE: Running Avni Source Connector Commands via Docker - Shell\nDESCRIPTION: Runs various commands (spec, check, discover, read) for the Avni source connector using Docker. Each command mounts required directories (such as 'secrets' and 'integration_tests') as volumes, passes configuration files as parameters, and ensures the connector image is removed after execution. Designed for both testing connector configurations and extracting data as per the Airbyte protocol.\n\n- `spec`: Retrieves the connector specification.\n- `check`: Validates the connector configuration file.\n- `discover`: Discovers available streams based on configuration.\n- `read`: Reads data using provided configuration and catalog files.\n\nRequires the Docker image to be built and requisite config files to be present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm airbyte/source-avni:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-avni:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-avni:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-avni:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Sample Connection Status Error Response - JSON\nDESCRIPTION: This JSON object represents a typical failed CONNECTION_STATUS response from an Airbyte connector when a required configuration is missing. It expects a structure with 'type', 'connectionStatus.status', and 'connectionStatus.message'. Limitations: This is only an example for when validation fails on missing properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"type\\\": \\\"CONNECTION_STATUS\\\",\\n  \\\"connectionStatus\\\": {\\n    \\\"status\\\": \\\"FAILED\\\",\\n    \\\"message\\\": \\\"Config validation error: 'TODO' is a required property\\\"\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to run the full test suite locally for the `source-coingecko-coins` connector. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coingecko-coins test\n```\n\n----------------------------------------\n\nTITLE: Available Data Streams Configuration\nDESCRIPTION: Table defining available data streams with their primary keys, pagination support, and sync capabilities for the Zoho Bigin connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-bigin.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| modules | id | No pagination | ✅ |  ❌  |\n| organizations | id | No pagination | ✅ |  ❌  |\n| roles | id | No pagination | ✅ |  ❌  |\n| notes | id | DefaultPaginator | ✅ |  ❌  |\n| tags | id | No pagination | ✅ |  ❌  |\n| companies | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| tasks |  | DefaultPaginator | ✅ |  ❌  |\n| events | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Features Support Table in Markdown\nDESCRIPTION: Describes the supported features of the Microsoft Teams connector including sync capabilities and connection settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-teams.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Compose Instance\nDESCRIPTION: Stops all services defined in the `docker-compose.yml` file for the existing Airbyte instance. This is a prerequisite step before migrating data using `abctl` to ensure data consistency and avoid conflicts during the migration process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/migrating-from-docker-compose.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose stop\n```\n\n----------------------------------------\n\nTITLE: Running Insightly Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run various standard source connector operations using the built Docker image. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-insightly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-insightly:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-insightly:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-insightly:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-insightly:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Sendinblue connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Sendinblue source connector using the airbyte-ci tool. This creates a Docker image with the tag airbyte/source-sendinblue:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendinblue/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendinblue build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Height Source Connector\nDESCRIPTION: This command runs the acceptance tests for the Height source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-height/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-height test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Hardcoded Records Connector\nDESCRIPTION: Command to run the full test suite for the Hardcoded Records connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hardcoded-records test\n```\n\n----------------------------------------\n\nTITLE: Enabling Corepack and Installing PNPM\nDESCRIPTION: Commands to enable Node.js corepack and install the required pnpm package manager version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncorepack enable && corepack install\n```\n\n----------------------------------------\n\nTITLE: Illustrative Directory Structure for Path Pattern Examples\nDESCRIPTION: This text block shows an example directory structure used to illustrate how different glob path patterns select files within a root folder ('MyFolder') configured in the Airbyte connector. It helps visualize the effects of patterns like `**/part*.csv` or `*table_files/*.csv`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-sharepoint.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMyFolder\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the My Hours connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-my-hours/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-my-hours:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-my-hours:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-my-hours:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-my-hours:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Activecampaign source connector Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-activecampaign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-activecampaign build\n```\n\n----------------------------------------\n\nTITLE: Running Glassflow Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Glassflow connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-glassflow test\n```\n\n----------------------------------------\n\nTITLE: Example Output of Catalog File Content\nDESCRIPTION: Provides sample JSON content from an Airbyte `catalog.json` file, typically displayed by the previous `cat` command. This file defines the schema and sync settings for the streams involved in the connection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n{\"streams\":[{\"stream\":{\"name\":\"exchange_rate\",\"json_schema\":{\"type\":\"object\",\"properties\":{\"CHF\":{\"type\":\"number\"},\"HRK\":{\"type\":\"number\"},\"date\":{\"type\":\"string\"},\"MXN\":{\"type\":\"number\"},\"ZAR\":{\"type\":\"number\"},\"INR\":{\"type\":\"number\"},\"CNY\":{\"type\":\"number\"},\"THB\":{\"type\":\"number\"},\"NZD\":{\"type\":\"number\"},\"BRL\":{\"type\":\"number\"}}},\"supported_sync_modes\":[\"full_refresh\"],\"default_cursor_field\":[]},\"sync_mode\":\"full_refresh\",\"cursor_field\":[]}]}\n```\n\n----------------------------------------\n\nTITLE: Running the CI test suite for Smaily connector\nDESCRIPTION: Command to execute the full test suite for the source-smaily connector using the airbyte-ci tool, which verifies the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smaily/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smaily test\n```\n\n----------------------------------------\n\nTITLE: Configuration Table Structure in Markdown\nDESCRIPTION: Version history table showing changelog entries for the Appfollow connector, including version numbers, dates, pull request references, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appfollow.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                 |\n| :------ | :--------- | :------------------------------------------------------- | :-------------------------------------- |\n| 1.1.18 | 2025-04-19 | [58283](https://github.com/airbytehq/airbyte/pull/58283) | Update dependencies |\n| 1.1.17 | 2025-04-12 | [57641](https://github.com/airbytehq/airbyte/pull/57641) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Loading Local Server Image into abctl Kind Cluster (Shell)\nDESCRIPTION: Uses the `kind` CLI to load the locally built `airbyte/server:dev` Docker image into the nodes of the Kubernetes cluster named `airbyte-abctl`. This allows testing changes made to the Airbyte server component within the `abctl` environment. Requires `kind` and the `airbyte/server:dev` image built via `./gradlew build`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nkind load docker-image airbyte/server:dev --name airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Airbyte Open Source Configuration Steps\nDESCRIPTION: Step-by-step instructions for configuring the Amazon Ads connector in Airbyte Open Source environment with detailed parameter explanations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n1. **Client ID** of your Amazon Ads developer application.\n2. **Client Secret** of your Amazon Ads developer application.\n3. **Refresh Token**.\n4. Select **Region** to pull data from **North America (NA)**, **Europe (EU)**, **Far East (FE)**.\n5. **Start Date (Optional)** in YYYY-MM-DD format.\n6. **Profile IDs (Optional)** for data filtering.\n7. **Marketplace IDs (Optional)** for data filtering.\n```\n\n----------------------------------------\n\nTITLE: Building MySQL Connector via Gradle\nDESCRIPTION: Command to build the MySQL source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mysql:build\n```\n\n----------------------------------------\n\nTITLE: Example of R2 Output File Path\nDESCRIPTION: This example demonstrates a specific instance of an R2 output file path, showing how the different components are populated with actual values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntesting_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz\n```\n\n----------------------------------------\n\nTITLE: Creating an Index for Incremental Sync in Fauna\nDESCRIPTION: This FQL query creates an index that enables incremental syncing for a specific collection. The index sorts documents by timestamp ('ts') and reference ('ref').\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nCreateIndex({\n  name: \"INDEX_NAME\",\n  source: Collection(\"COLLECTION_NAME\"),\n  terms: [],\n  values: [{ field: \"ts\" }, { field: \"ref\" }],\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Gorgias Source Connector Parameters\nDESCRIPTION: This table outlines the configuration parameters required for the Gorgias source connector, including username, password, domain name, and start date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gorgias.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n| `domain_name` | `string` | Domain name. Domain name given for gorgias, found as your url prefix for accessing your website |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Salesforce Connector\nDESCRIPTION: Command to add a new dependency to the Salesforce connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Datadog Source Connector Test Suite Locally with Airbyte-CI (Bash)\nDESCRIPTION: This shell command runs the complete test suite for the Datadog source connector using Airbyte-CI. It expects Airbyte-CI to be installed and executed from the appropriate directory with necessary development dependencies configured. The results of the tests will indicate whether the connector is functioning as expected, which is crucial before publishing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datadog/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-datadog test\n```\n\n----------------------------------------\n\nTITLE: Markdown Version History Table\nDESCRIPTION: A detailed changelog table showing version history, dates, pull request references, and changes made to the connector over time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/babelforce.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                     |\n| :------ | :--------- | :------------------------------------------------------- | :-------------------------- |\n| 0.3.21 | 2025-04-19 | [58239](https://github.com/airbytehq/airbyte/pull/58239) | Update dependencies |\n| 0.3.20 | 2025-04-12 | [57613](https://github.com/airbytehq/airbyte/pull/57613) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests Locally using Poetry (Bash)\nDESCRIPTION: This command executes the connector's unit tests located in the `unit_tests` directory using `pytest`, managed via Poetry. It should be run from the connector's root directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Feature Support Matrix\nDESCRIPTION: Defines supported features for the Aha API connector including full refresh and incremental sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aha.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n```\n\n----------------------------------------\n\nTITLE: Building Exchange Rates Connector with airbyte-ci\nDESCRIPTION: Command for building the Exchange Rates source connector docker image using airbyte-ci. The resulting image will be available on the host with the tag 'airbyte/source-exchange-rates:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-exchange-rates/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-exchange-rates build\n```\n\n----------------------------------------\n\nTITLE: Running Local Tests with Poetry\nDESCRIPTION: Commands for running Python tests and static type checking using Poetry package manager. Includes pytest for unit tests and mypy for type checking.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/base_images/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n# Static typing checks\npoetry run mypy base_images --check-untyped-defs\n```\n\n----------------------------------------\n\nTITLE: Using `airbyte-ci connectors upgrade-cdk` Command Examples (Shell)\nDESCRIPTION: Shell examples illustrating the usage of the `airbyte-ci connectors upgrade-cdk` command. It shows how to update the Airbyte CDK version for all Python connectors, pin a specific connector (`source-openweather`) to an exact CDK version, or update modified connectors within a specific major version range.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --language=python upgrade-cdk\n```\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-openweather upgrade-cdk \"3.0.0\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --modified upgrade-cdk \"<4\"\n```\n\n----------------------------------------\n\nTITLE: Supported Streams Table in Markdown\nDESCRIPTION: Markdown table listing all supported data streams with their properties including primary keys, pagination type, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/akeneo.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| products | uuid | DefaultPaginator | ✅ |  ❌  |\n| categories  | code | DefaultPaginator | ✅ |  ❌  |\n| families | code | DefaultPaginator | ✅ |  ❌  |\n| family_variants | code | DefaultPaginator | ✅ |  ❌  |\n| attributes | code | DefaultPaginator | ✅ |  ❌  |\n| attribute_groups | code | DefaultPaginator | ✅ |  ❌  |\n| association_types | code | DefaultPaginator | ✅ |  ❌  |\n| channels | code | DefaultPaginator | ✅ |  ❌  |\n| locales |  | DefaultPaginator | ✅ |  ❌  |\n| currencies | code | DefaultPaginator | ✅ |  ❌  |\n| measure_families | code | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Displaying Information Block in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display an information block about optional migration steps for incremental syncing of the Refunds stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stripe-migrations.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n:::info\nThe following migration steps are relevant for those who would like to sync `Refunds` incrementally. These migration steps can be skipped if you prefer to sync using `Full Refresh`. \n:::\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Disabling Basic Authentication in Airbyte OSS via .env\nDESCRIPTION: Provides example environment variable settings within the `.env` file to disable the Nginx Basic Authentication feature in Airbyte Open Source. Setting `BASIC_AUTH_USERNAME` and `BASIC_AUTH_PASSWORD` to empty strings bypasses the default login prompt (username 'airbyte', password 'password'). This configuration is sourced when the Airbyte server starts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/october_2022.md#2025-04-23_snippet_0\n\nLANGUAGE: dotenv\nCODE:\n```\nBASIC_AUTH_USERNAME=\"\"\nBASIC_AUTH_PASSWORD=\"\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Go High Level Connector Changelog in Markdown\nDESCRIPTION: Markdown table displaying the version history of the Go High Level connector, including version numbers, dates, pull request links, and update descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/high-level.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n|---------|------|--------------|----------|\n| 0.0.19 | 2025-04-05 | [57062](https://github.com/airbytehq/airbyte/pull/57062) | Update dependencies |\n| 0.0.18 | 2025-03-29 | [56644](https://github.com/airbytehq/airbyte/pull/56644) | Update dependencies |\n| 0.0.17 | 2025-03-22 | [56079](https://github.com/airbytehq/airbyte/pull/56079) | Update dependencies |\n| 0.0.16 | 2025-03-08 | [55477](https://github.com/airbytehq/airbyte/pull/55477) | Update dependencies |\n| 0.0.15 | 2025-03-01 | [54757](https://github.com/airbytehq/airbyte/pull/54757) | Update dependencies |\n| 0.0.14 | 2025-02-22 | [54342](https://github.com/airbytehq/airbyte/pull/54342) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [53813](https://github.com/airbytehq/airbyte/pull/53813) | Update dependencies |\n| 0.0.12 | 2025-02-08 | [53250](https://github.com/airbytehq/airbyte/pull/53250) | Update dependencies |\n| 0.0.11 | 2025-02-01 | [52756](https://github.com/airbytehq/airbyte/pull/52756) | Update dependencies |\n| 0.0.10 | 2025-01-25 | [52267](https://github.com/airbytehq/airbyte/pull/52267) | Update dependencies |\n| 0.0.9 | 2025-01-18 | [51813](https://github.com/airbytehq/airbyte/pull/51813) | Update dependencies |\n| 0.0.8 | 2025-01-11 | [51211](https://github.com/airbytehq/airbyte/pull/51211) | Update dependencies |\n| 0.0.7 | 2024-12-28 | [50637](https://github.com/airbytehq/airbyte/pull/50637) | Update dependencies |\n| 0.0.6 | 2024-12-21 | [50093](https://github.com/airbytehq/airbyte/pull/50093) | Update dependencies |\n| 0.0.5 | 2024-12-14 | [49640](https://github.com/airbytehq/airbyte/pull/49640) | Update dependencies |\n| 0.0.4 | 2024-12-12 | [49237](https://github.com/airbytehq/airbyte/pull/49237) | Update dependencies |\n| 0.0.3 | 2024-12-11 | [48901](https://github.com/airbytehq/airbyte/pull/48901) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.2 | 2024-10-28 | [47472](https://github.com/airbytehq/airbyte/pull/47472) | Update dependencies |\n| 0.0.1 | 2024-08-23 | | Initial release by [@Stockotaco](https://github.com/stockotaco) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Updating AzureBlobStorageFormatConfigs in Java\nDESCRIPTION: Modify the AzureBlobStorageFormatConfigs class to construct a config for the new format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-azure-blob-storage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class AzureBlobStorageFormatConfigs {\n    // Add method to construct config for new format\n}\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Testing Zonka Feedback Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zonka Feedback source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zonka-feedback/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zonka-feedback test\n```\n\n----------------------------------------\n\nTITLE: Building the source-gologin Connector Using Airbyte CI (Bash)\nDESCRIPTION: This code snippet demonstrates how to build the 'source-gologin' connector for local development using Airbyte CI. The 'airbyte-ci' tool is required as a dependency and should be properly installed beforehand. The command creates a development Docker image ('source-gologin:dev') which can be used for local connector testing. The main parameter is '--name=source-gologin' specifying the target connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gologin/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gologin build\n```\n\n----------------------------------------\n\nTITLE: Running Standard Copper Connector Commands via Docker\nDESCRIPTION: Executes standard Airbyte source connector commands (spec, check, discover, read) using the locally built Docker image 'airbyte/source-copper:dev'. The 'check', 'discover', and 'read' commands require mounting a 'secrets' directory containing 'config.json', and 'read' also requires mounting an 'integration_tests' directory with 'configured_catalog.json'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-copper/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-copper:dev spec\ndocker run --rm -v \\$(pwd)/secrets:/secrets airbyte/source-copper:dev check --config /secrets/config.json\ndocker run --rm -v \\$(pwd)/secrets:/secrets airbyte/source-copper:dev discover --config /secrets/config.json\ndocker run --rm -v \\$(pwd)/secrets:/secrets -v \\$(pwd)/integration_tests:/integration_tests airbyte/source-copper:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Facebook Pages Connector\nDESCRIPTION: Example Dockerfile to patch the Facebook Pages connector by extending the official image and adding custom code. This allows for customization while maintaining the base functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-facebook-pages:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Tiktok-Marketing Connector\nDESCRIPTION: This command adds a new dependency to the Tiktok-Marketing connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Elasticsearch Connector Tests\nDESCRIPTION: Gradle commands for running unit tests and integration tests for the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-elasticsearch/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-elasticsearch:unitTest\n./gradlew :airbyte-integrations:connectors:destination-elasticsearch:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Apptivo Data Streams Schema\nDESCRIPTION: Available data streams in the Apptivo connector with their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/apptivo.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| customers | customerId | DefaultPaginator | ✅ |  ❌  |\n| contacts | contactId | DefaultPaginator | ✅ |  ❌  |\n| cases |  | No pagination | ✅ |  ❌  |\n| leads | id | DefaultPaginator | ✅ |  ❌  |\n| opportunities | opportunityId | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Google Sheets Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally using Poetry. Includes specifying connector details, checking configuration, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-google-sheets spec\npoetry run destination-google-sheets check --config secrets/config.json\npoetry run destination-google-sheets write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-typesense test\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Discover Operation - Airbyte CLI (bash)\nDESCRIPTION: This command demonstrates invoking the 'discover' operation for an Airbyte source connector. It requires an accessible connector executable and config file, outputting catalog discovery results or errors to the terminal. Limitations: Fails if required config properties are missing, as described in the context.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/1-environment-setup.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo discover --config secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Running Survey Sparrow Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Survey Sparrow source connector using Docker. They include running the spec, check, discover, and read operations with appropriate configurations and volume mounts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-survey-sparrow/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-survey-sparrow:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-survey-sparrow:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-survey-sparrow:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-survey-sparrow:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for Connector\nDESCRIPTION: Example Dockerfile for building a custom version of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-motherduck:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Running Visma Economic Source Connector Docker Commands\nDESCRIPTION: Series of docker commands to run the Visma Economic source connector for various operations like spec, check, discover, and read. These commands use the built docker image and assume the presence of a config.json file in the secrets directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-visma-economic/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-visma-economic:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-visma-economic:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-visma-economic:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-visma-economic:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Calendly Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Calendly source connector using airbyte-ci. Creates a dev image tagged as source-calendly:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-calendly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-calendly build\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite for Zendesk Talk Connector using airbyte-ci (Bash)\nDESCRIPTION: Executes the full Airbyte connector integration test suite locally for the `source-zendesk-talk` connector using the `airbyte-ci` tool. This command requires `airbyte-ci` to be installed and configured, along with necessary credentials (likely via `secrets/config.json`) for the tests to run against the actual API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-talk test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Thinkific Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Thinkific source connector using airbyte-ci. It helps ensure the connector meets the required standards and functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-thinkific/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-thinkific test\n```\n\n----------------------------------------\n\nTITLE: Running Kyve Connector Commands Locally with Poetry\nDESCRIPTION: Commands to run various Kyve connector operations locally using Poetry, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-kyve spec\npoetry run source-kyve check --config secrets/config.json\npoetry run source-kyve discover --config secrets/config.json\npoetry run source-kyve read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running different connector operations locally including spec, check, discover, and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-adjust spec\npoetry run source-adjust check --config secrets/config.json\npoetry run source-adjust discover --config secrets/config.json\npoetry run source-adjust read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Split-io Data Streams in Markdown\nDESCRIPTION: Markdown table listing available data streams for the Split-io connector, including their properties such as primary key, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/split-io.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| changeRequests | id | DefaultPaginator | ✅ |  ❌  |\n| workspaces | id | DefaultPaginator | ✅ |  ❌  |\n| flagSets | id | DefaultPaginator | ✅ |  ✅  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| segments | name | DefaultPaginator | ✅ |  ✅  |\n| segments_keys | uid | DefaultPaginator | ✅ |  ❌  |\n| rolloutStatuses | id | DefaultPaginator | ✅ |  ❌  |\n| environments | id | DefaultPaginator | ✅ |  ❌  |\n| trafficTypes | id | DefaultPaginator | ✅ |  ❌  |\n| groups | id | DefaultPaginator | ✅ |  ❌  |\n| feature_flags | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Importing Tabs Components in React/JSX\nDESCRIPTION: Imports the Tabs and TabItem components from the @theme/Tabs module for creating tabbed interface in the documentation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/salesforce.md#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n```\n\n----------------------------------------\n\nTITLE: AgileCRM Streams Table\nDESCRIPTION: Detailed overview of supported data streams, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/agilecrm.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| companies | id | DefaultPaginator | ✅ |  ❌  |\n| deals | id | DefaultPaginator | ✅ |  ❌  |\n| notes | id | No pagination | ✅ |  ❌  |\n| tasks | id | No pagination | ✅ |  ❌  |\n| milestone | id | No pagination | ✅ |  ❌  |\n| campaigns | id | DefaultPaginator | ✅ |  ❌  |\n| documents | id | No pagination | ✅ |  ❌  |\n| ticket_filters | id | No pagination | ✅ |  ❌  |\n| tickets |  | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Weaviate Connector\nDESCRIPTION: This Dockerfile example shows how to create a custom build based on the latest version of the Weaviate connector image, useful for patching or modifying the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-weaviate:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Disabling Authentication Configuration in YAML\nDESCRIPTION: YAML configuration to disable authentication in Airbyte by setting the global auth enabled flag to false.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  auth:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Retrieving Airbyte Credentials using abctl (Shell)\nDESCRIPTION: Shows the `abctl` command used to display the automatically generated Airbyte password, client ID, and client secret for local deployments managed by `abctl`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nabctl local credentials\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-braintree spec\npoetry run source-braintree check --config secrets/config.json\npoetry run source-braintree discover --config secrets/config.json\npoetry run source-braintree read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment and install dependencies for local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: Version history table showing release dates, pull request references, and change descriptions for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/assemblyai.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.2 | 2025-04-19 | [57655](https://github.com/airbytehq/airbyte/pull/57655) | Update dependencies |\n| 0.0.1 | 2025-04-05 | [57210](http://github.com/airbytehq/airbyte/pull/57210) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for Connector\nDESCRIPTION: Example Dockerfile for building a custom version of the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-astra:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Defining Two Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema example for the two_weekly_active_users stream in Google Analytics, showing the count of 14-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_14dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Testing the Rocketlane Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Rocketlane source connector. This validates the connector's functionality against Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rocketlane/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rocketlane test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Newsdata connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-newsdata/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-newsdata test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Genesys Connector\nDESCRIPTION: Command to run the full CI test suite for the Genesys connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-genesys test\n```\n\n----------------------------------------\n\nTITLE: Building Omnisend Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This bash command builds the Omnisend source connector Docker image using the airbyte-ci CLI tool. It requires airbyte-ci to be installed, which manages building connectors for the Airbyte platform. The key parameter --name specifies the connector identifier. Running this command will generate a Docker image tagged as airbyte/source-omnisend:dev, usable for local testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-omnisend/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-omnisend build\n```\n\n----------------------------------------\n\nTITLE: Using sum Filter in Jinja2\nDESCRIPTION: Demonstrates the `sum` filter in Jinja2, which calculates the sum of all numeric items in a sequence. The example sums the list `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_47\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|sum }}\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Python Virtual Environment\nDESCRIPTION: Commands to create a Python virtual environment and install the required dependencies for local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running compare_versions.sh to Compare Connector Outputs\nDESCRIPTION: Script for comparing records output between two versions of a connector. It requires configuration files in the config_files directory and prompts for connector name and versions to compare.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/internal/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./tools/internal/compare_versions.sh # to run script\n```\n\n----------------------------------------\n\nTITLE: Defining SurveyMonkey Connector Specification in YAML\nDESCRIPTION: This YAML snippet defines the connection specification for the SurveyMonkey connector, including the required access token property.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ndocumentationUrl: https://docsurl.com\nconnectionSpecification:\n  $schema: http://json-schema.org/draft-07/schema#\n  title: Survey Monkey Demo Spec\n  type: object\n  required:\n    - access_token\n  properties:\n    access_token:\n      type: string\n      description: \"Access token for Survey Monkey API\"\n      order: 0\n      airbyte_secret: true\n```\n\n----------------------------------------\n\nTITLE: Deploying Airbyte Data Plane using Helm\nDESCRIPTION: These bash commands demonstrate how to deploy the Airbyte data plane using Helm. Two examples are provided: one for deploying in the default namespace and another for creating or using a specific namespace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airbyte-enterprise airbyte/airbyte-data-plane --version 1.6.0 --values values.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install airbyte-enterprise airbyte/airbyte-data-plane --version 1.6.0 -n airbyte-dataplane --create-namespace --values values.yaml\n```\n\n----------------------------------------\n\nTITLE: Running CI tests for Fullstory connector in Bash\nDESCRIPTION: Command to run the full test suite for the Fullstory source connector using the airbyte-ci tool, which validates the connector's functionality according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fullstory/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fullstory test\n```\n\n----------------------------------------\n\nTITLE: Available Connector Commands\nDESCRIPTION: Lists the available commands for working with Airbyte connectors, including testing, building Docker images, and publishing to DockerHub.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n- `airbyte-ci connectors test`: Run tests for one or multiple connectors.\n- `airbyte-ci connectors build`: Build docker images for one or multiple connectors.\n- `airbyte-ci connectors publish`: Publish a connector to Airbyte's DockerHub.\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Global Connector Test Suite with airbyte-ci CLI - Bash\nDESCRIPTION: This bash command invokes the global CI test suite for a specified Airbyte connector via the `airbyte-ci` CLI. It requires prior installation of the CLI and the correct setting of the `<connector_name>` parameter to specify the target connector. Running this collects and executes all relevant connector tests available, including QA checks, unit, integration, and acceptance tests, depending on config and credentials. The command outputs the results directly to the console and may require provisions such as credentials for certain test types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=<connector_name> test\n```\n\n----------------------------------------\n\nTITLE: Documenting Solarwinds Service Desk Connector Changelog in Markdown\nDESCRIPTION: This snippet presents the changelog for the Solarwinds Service Desk connector, showing version history, dates, pull requests, and subjects of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/solarwinds-service-desk.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.21 | 2025-04-19 | [58404](https://github.com/airbytehq/airbyte/pull/58404) | Update dependencies |\n| 0.0.20 | 2025-04-12 | [57981](https://github.com/airbytehq/airbyte/pull/57981) | Update dependencies |\n| 0.0.19 | 2025-04-05 | [57437](https://github.com/airbytehq/airbyte/pull/57437) | Update dependencies |\n| 0.0.18 | 2025-03-29 | [56889](https://github.com/airbytehq/airbyte/pull/56889) | Update dependencies |\n| 0.0.17 | 2025-03-22 | [56286](https://github.com/airbytehq/airbyte/pull/56286) | Update dependencies |\n| 0.0.16 | 2025-03-08 | [55643](https://github.com/airbytehq/airbyte/pull/55643) | Update dependencies |\n| 0.0.15 | 2025-03-01 | [55102](https://github.com/airbytehq/airbyte/pull/55102) | Update dependencies |\n| 0.0.14 | 2025-02-22 | [54520](https://github.com/airbytehq/airbyte/pull/54520) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [54063](https://github.com/airbytehq/airbyte/pull/54063) | Update dependencies |\n| 0.0.12 | 2025-02-08 | [53519](https://github.com/airbytehq/airbyte/pull/53519) | Update dependencies |\n| 0.0.11 | 2025-02-01 | [53096](https://github.com/airbytehq/airbyte/pull/53096) | Update dependencies |\n| 0.0.10 | 2025-01-25 | [52410](https://github.com/airbytehq/airbyte/pull/52410) | Update dependencies |\n| 0.0.9 | 2025-01-18 | [51977](https://github.com/airbytehq/airbyte/pull/51977) | Update dependencies |\n| 0.0.8 | 2025-01-11 | [51409](https://github.com/airbytehq/airbyte/pull/51409) | Update dependencies |\n| 0.0.7 | 2025-01-04 | [50748](https://github.com/airbytehq/airbyte/pull/50748) | Update dependencies |\n| 0.0.6 | 2024-12-21 | [50360](https://github.com/airbytehq/airbyte/pull/50360) | Update dependencies |\n| 0.0.5 | 2024-12-14 | [49771](https://github.com/airbytehq/airbyte/pull/49771) | Update dependencies |\n| 0.0.4 | 2024-12-12 | [49418](https://github.com/airbytehq/airbyte/pull/49418) | Update dependencies |\n| 0.0.3 | 2024-12-11 | [49111](https://github.com/airbytehq/airbyte/pull/49111) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.2 | 2024-10-29 | [47855](https://github.com/airbytehq/airbyte/pull/47855) | Update dependencies |\n| 0.0.1 | 2024-10-10 | [46707](https://github.com/airbytehq/airbyte/pull/46707) | Initial release by [@gemsteam](https://github.com/gemsteam) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Installing the Slack Connector with Poetry\nDESCRIPTION: Command to install the Slack connector and its development dependencies using Poetry package manager from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Displaying Streams Table in Markdown\nDESCRIPTION: This snippet presents a table of available streams in the GreytHR Connector, including stream names, primary keys, pagination methods, and support for full and incremental sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/greythr.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Employees | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employees Categories | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employees Profile | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employees Personal Details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employees Work Details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employee Separation Details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employee Statutory Details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employee Bank Details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employee PF &amp; ESI details | employeeId | DefaultPaginator | ✅ |  ❌  |\n| Employee Qualifications Details |  | DefaultPaginator | ✅ |  ❌  |\n| Users List |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Stream Status Table Definition\nDESCRIPTION: Markdown table defining stream status icons and their meanings for monitoring individual stream health in Airbyte connections.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/review-connection-status.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Icon                                                            | Status                   | Description                                                                                                                                        |\n| --------------------------------------------------------------- | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ![Synced](./assets/connection_synced.png)                       | **Synced**               | The stream's last sync was successful.                                                                                                             |\n| ![Syncing](./assets/connection_syncing.png)                     | **Syncing**              | The stream is currently actively syncing. Airbyte highlights the stream in grey to indicate the sync is actively extracting or loading data. |\n| ![Queued](./assets/connection_not_yet_synced.png)               | **Queued**               | The stream hasn't synced yet, and is going to sync in the current ongoing sync                                                           |\n| ![Queued for next sync](./assets/connection_not_yet_synced.png) | **Queued for next sync** | The stream hasn't synced yet, and is going to sync in the next scheduled sync                                                            |\n| ![Error](./assets/connection_incomplete.png)                    | **Error**                | The connection didn't succeed on its most recent sync, but Airbyte expects it to recover on the next one                                                 |\n| ![Action Required](./assets/connection_action_required.png)     | **Action Required**      | A breaking change related to the source or destination requires attention to resolve                                                               |\n```\n\n----------------------------------------\n\nTITLE: Defining Invoiced Data Streams in Markdown\nDESCRIPTION: Markdown table listing available data streams from the Invoiced API, including their primary keys, pagination methods, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiced.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| customers | id | DefaultPaginator | ✅ |  ❌  |\n| invoices | id | DefaultPaginator | ✅ |  ❌  |\n| payments | id | DefaultPaginator | ✅ |  ❌  |\n| credit_balance_adjustments | id | DefaultPaginator | ✅ |  ❌  |\n| credit_notes | id | DefaultPaginator | ✅ |  ❌  |\n| subscriptions |  | DefaultPaginator | ✅ |  ❌  |\n| estimates | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| items | id | DefaultPaginator | ✅ |  ❌  |\n| tax_rates | id | DefaultPaginator | ✅ |  ❌  |\n| plans | id | DefaultPaginator | ✅ |  ❌  |\n| coupons | id | DefaultPaginator | ✅ |  ❌  |\n| events | id | DefaultPaginator | ✅ |  ❌  |\n| notes | id | DefaultPaginator | ✅ |  ❌  |\n| tasks | id | DefaultPaginator | ✅ |  ❌  |\n| metered_billings | id | DefaultPaginator | ✅ |  ❌  |\n| payment_sources | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte with Insecure Cookies - Shell\nDESCRIPTION: Performs an Airbyte local installation with abctl, specifying both a host and the --insecure-cookies flag. This allows Airbyte to serve over HTTP and disables secure cookies, suitable only if you understand security implications. Replace [HOSTNAME] with your server address. Should not be used in production or untrusted environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nabctl local install --host [HOSTNAME] --insecure-cookies\n```\n\n----------------------------------------\n\nTITLE: Airbyte STATE Message for Synchronization Tracking\nDESCRIPTION: STATE message used to track synchronization progress in the Airbyte protocol. Contains a global shared state with a start_date parameter that indicates the beginning of the data extraction period.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/every_time_type_messages.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2024-07-25\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Streaming Sheet6-2000-rows Data in JSON Format\nDESCRIPTION: Each JSON object represents a row from Sheet6-2000-rows, containing an ID, Name, and emission timestamp. The data is streamed with a consistent structure for each entry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1011\",\"Name\":\"NwMUHUOUz\"},\"emitted_at\":1673989569000}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Adding Python Dependencies with Poetry - Bash\nDESCRIPTION: Adds a new dependency to the project's Python environment and updates 'pyproject.toml' and 'poetry.lock'. Requires Poetry and that '<package-name>' is replaced with the desired package identifier. The updated lockfiles should be committed to version control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Displaying Supported Features for Toggl API Connector in Markdown\nDESCRIPTION: This snippet shows a markdown table listing the supported features of the Toggl API connector, including full refresh sync and incremental sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/toggl.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Configuring shared_preload_libraries for pg_cron on Google Cloud SQL\nDESCRIPTION: Example configuration value for the `shared_preload_libraries` flag in Google Cloud SQL PostgreSQL settings, specifying only `pg_cron`. This ensures the `pg_cron` extension is preloaded at startup. A database restart is required after setting this flag.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nshared_preload_libraries = 'pg_cron'\n```\n\n----------------------------------------\n\nTITLE: Building Kafka Source Connector Docker Image\nDESCRIPTION: This command builds the Docker image for the Kafka source connector using Gradle. The resulting image will be tagged as 'airbyte/source-kafka:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kafka/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-kafka:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Temporarily Modifying Page Size for Testing in Python\nDESCRIPTION: This Python code snippet temporarily changes the value of the `_PAGE_SIZE` constant to 1. This is done to force the connector to make multiple API requests even if there are few records, making it easier to observe and test the pagination logic during a `read` operation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    _PAGE_SIZE: int = 1\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Google Classroom Connector (Bash)\nDESCRIPTION: This command executes the acceptance test suite for the `source-google-classroom` connector using the `airbyte-ci` tool. It requires the connector's development image to have been built previously. Running these tests helps ensure the connector functions correctly according to Airbyte standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-classroom/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-classroom test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Intercom Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Intercom source connector. The resulting image will be tagged as 'airbyte/source-intercom:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intercom/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-intercom build\n```\n\n----------------------------------------\n\nTITLE: Running Kyriba Connector as Docker Container\nDESCRIPTION: Commands to run various connector operations using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-kyriba:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kyriba:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-kyriba:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-kyriba:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Feature Support Table\nDESCRIPTION: A markdown table showing supported features of the Babelforce connector including sync modes and connection capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/babelforce.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | Supported?  |\n| :---------------------------- | :---------- |\n| Full Refresh Sync             | Yes         |\n| Incremental Sync              | Yes         |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection                | Yes         |\n| Namespaces                    | No          |\n```\n\n----------------------------------------\n\nTITLE: Running SingleStore Connector Docker Commands\nDESCRIPTION: Commands to run the SingleStore connector Docker container for various operations including spec, check, discover, and read data using configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-singlestore/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-singlestore:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-singlestore:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-singlestore:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-singlestore:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Elasticsearch Connector\nDESCRIPTION: Command to run acceptance and custom integration tests for the Elasticsearch source connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticsearch/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-elasticsearch:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for TVmaze Schedule Source Connector\nDESCRIPTION: Command to run the full test suite for the TVmaze Schedule source connector using airbyte-ci. This ensures all changes pass the necessary tests before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tvmaze-schedule/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tvmaze-schedule test\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Connector Tests with Gradle\nDESCRIPTION: Commands for running unit tests and integration tests using Gradle from the project root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kafka/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-kafka:unitTest\n./gradlew :airbyte-integrations:connectors:destination-kafka:integrationTest\n```\n\n----------------------------------------\n\nTITLE: CSV Escape Character Example\nDESCRIPTION: Sample CSV data showing how escape characters are used to handle quotes within field values\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-sharepoint-enterprise.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-microsoft-onedrive spec\npoetry run source-microsoft-onedrive check --config secrets/config.json\npoetry run source-microsoft-onedrive discover --config secrets/config.json\npoetry run source-microsoft-onedrive read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Tests\nDESCRIPTION: Command to run the full test suite for the Firebolt connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-firebolt test\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Finnhub Source Connector with airbyte-ci - Bash\nDESCRIPTION: Builds the development Docker image for the Finnhub source connector using the airbyte-ci CLI tool. Requires airbyte-ci to be installed and available in the environment. The '--name=source-finnhub' flag selects the specific connector to build, and the 'build' command initiates the build process. Outputs a dev image named 'source-finnhub:dev' for local testing. No external parameters beyond the connector name and command are needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finnhub/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finnhub build\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Table showing the configuration parameters required for the YouTube Data API connector, including API key and channel ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/youtube-data.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `channel_id` | `string` | channel_id.  |  |\n```\n\n----------------------------------------\n\nTITLE: Sample Output: Airbyte Image List - Bash\nDESCRIPTION: This code block is an example output from the abctl images manifest command, showing the names and tags of various Docker images Airbyte uses, such as core platform components and connectors. No execution is required for this snippet; it is purely illustrative and may change with newer Airbyte versions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte/bootloader:1.3.1\nairbyte/connector-builder-server:1.3.1\nairbyte/connector-sidecar:1.3.1\nairbyte/container-orchestrator:1.3.1\nairbyte/cron:1.3.1\nairbyte/db:1.3.1\nairbyte/mc:latest\nairbyte/server:1.3.1\nairbyte/webapp:1.3.1\nairbyte/worker:1.3.1\nairbyte/workload-api-server:1.3.1\nairbyte/workload-init-container:1.3.1\nairbyte/workload-launcher:1.3.1\nbitnami/kubectl:1.28.9\nbusybox:1.35\nbusybox:latest\ncurlimages/curl:8.1.1\nminio/minio:RELEASE.2023-11-20T22-40-07Z\ntemporalio/auto-setup:1.23.0\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Various commands for running the XKCD connector container, including spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xkcd/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-xkcd:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-xkcd:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-xkcd:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-xkcd:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Yahoo Finance Price source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yahoo-finance-price/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yahoo-finance-price build\n```\n\n----------------------------------------\n\nTITLE: Building Planhat Source Connector Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-planhat` connector, tagging it as `source-planhat:dev`. This is essential for local testing and development, requiring `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-planhat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-planhat build\n```\n\n----------------------------------------\n\nTITLE: Testing the Formbricks Connector with airbyte-ci\nDESCRIPTION: Command to run the acceptance tests for the Formbricks source connector, validating its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-formbricks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-formbricks test\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Connector Commands Locally\nDESCRIPTION: Commands to run the connector's main operations (spec, check, write) locally using Python. These are used for testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Live Connection\nDESCRIPTION: Command to run connector tests using a live connection where connection objects will be fetched automatically, specifying connector image, target version, and PR URL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/live-tests/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n poetry run pytest src/live_tests \\\n --connector-image=airbyte/source-faker \\\n --target-version=dev \\\n  --pr-url=<PR-URL> # The URL of the PR you are testing\n```\n\n----------------------------------------\n\nTITLE: Configuring Split-io Source Connector in Markdown\nDESCRIPTION: Markdown table showing the configuration parameters for the Split-io source connector. It includes the API key and start date as input fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/split-io.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Original Hubspot Object Structure Before Unnesting (JSON)\nDESCRIPTION: Shows an example JSON object representing data fetched from Hubspot *before* Airbyte applies the unnesting logic (introduced in version 1.5.0). Note the nested `properties` field containing specific attributes like `hs_note_body` and `hs_created_by`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hubspot.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"updatedAt\": \"2020-01-01\",\n  \"properties\": {\n    \"hs_note_body\": \"World's best boss\",\n    \"hs_created_by\": \"Michael Scott\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Connectors QA Package\nDESCRIPTION: Command to install the Connectors QA package locally using pipx from the current directory\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install .\n```\n\n----------------------------------------\n\nTITLE: Creating Vertica User for Airbyte Integration (SQL)\nDESCRIPTION: This SQL script creates a new user named 'airbyte_user' in Vertica, assigns a password, and grants essential permissions (`CREATE`, `TEMPORARY`) on a specified database. This user is intended for Airbyte to connect and write data to the Vertica destination. Replace '<password>' and '<database>' with the actual password and database name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/vertica.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER airbyte_user WITH PASSWORD '<password>';\nGRANT CREATE, TEMPORARY ON DATABASE <database> TO airbyte_user;\n```\n\n----------------------------------------\n\nTITLE: Trace Records with Stream Status in Airbyte JSON Format\nDESCRIPTION: Sample trace records in Airbyte's JSON format. Contains stream status information indicating completion of various test streams with timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"string_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"date_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"datetime_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"datetime_test_2\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"number_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"bignumber_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"integer_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"boolean_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Duckdb Connector\nDESCRIPTION: Example Dockerfile to create a custom build of the Duckdb connector based on the official image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-duckdb:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Configuring TicketTailor API Key in Markdown\nDESCRIPTION: Markdown table showing the configuration input for the TicketTailor connector. It specifies the API key input, its type, description, and where to find it.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tickettailor.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it at https://www.getdrip.com/user/edit |  |\n```\n\n----------------------------------------\n\nTITLE: Updating Standard Replication Configuration\nDESCRIPTION: SQL query to update source configurations in the actor table for connections using Standard replication method after upgrading from version 0.6.8 to 0.6.9.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql/mysql-troubleshooting.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"STANDARD\"}', true)\nWHERE actor_definition_id ='435bb9a5-7887-4809-aa58-28c27df0d7ad' AND (configuration->>'replication_method' = 'STANDARD');\n```\n\n----------------------------------------\n\nTITLE: Running SFTP Connector Docker Commands in Bash\nDESCRIPTION: Series of Docker commands to run various operations of the SFTP source connector, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/source-sftp:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sftp:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sftp:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sftp:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte Stream State and Trace Messages\nDESCRIPTION: Example JSON messages showing state tracking and stream status traces. Includes global state configuration and completion status for various streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/edge_case_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-09-02\"}}}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"streamWithCamelCase\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-Only Airbyte Connector with airbyte-ci - Bash\nDESCRIPTION: This bash command builds a development image (source-phyllo:dev) for the Phyllo manifest-only source connector. It depends on the airbyte-ci command-line tool being installed and configured. The main parameter, --name=source-phyllo, specifies the connector by name. This action prepares the local environment for further connector testing and development steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-phyllo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-phyllo build\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Campaign Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the Zoho Campaign connector using airbyte-ci tool. Creates a dev image tagged as source-zoho-campaign:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-campaign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-campaign build\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Inventory Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Zoho Inventory connector using airbyte-ci. Creates a dev image tagged as source-zoho-inventory:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-inventory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-inventory build\n```\n\n----------------------------------------\n\nTITLE: Testing Capsule CRM Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Capsule CRM connector using airbyte-ci. Executes the test suite to verify connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-capsule-crm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-capsule-crm test\n```\n\n----------------------------------------\n\nTITLE: Configuring Thinkific Courses Connector in Markdown\nDESCRIPTION: This snippet defines the configuration parameters required for the Thinkific Courses connector. It specifies the API key and subdomain inputs needed for authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific-courses.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `X-Auth-Subdomain` | `string` | subdomain.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building the source-pingdom Connector Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-pingdom` connector. The resulting image will be tagged as `source-pingdom:dev` and can be used for local testing. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pingdom/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pingdom build\n```\n\n----------------------------------------\n\nTITLE: Adding Python Dependencies via Poetry (Bash)\nDESCRIPTION: This command uses Poetry to add a new Python package dependency to the project's environment, updating pyproject.toml and poetry.lock accordingly. Substitute <package-name> with the desired library name. The command must be run from within the connector's directory, and users must have Poetry installed. Dependencies with optional extras or version specifiers can also be specified. Always verify resulting files are committed to version control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running CAT with airbyte-ci\nDESCRIPTION: Command to run connector acceptance tests using the airbyte-ci pipeline, which executes tests in the same environment as CI/CD.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=<connector-name> test\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte CI\nDESCRIPTION: Command to build the Fauna connector using the airbyte-ci tool, which creates a Docker image in the local registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-fauna build\n```\n\n----------------------------------------\n\nTITLE: Installing Pipx for Python Package Management\nDESCRIPTION: Commands to install pipx using pip and set up the necessary paths.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-ci/connectors/ci_credentials/\npyenv install # ensure you have the correct python version\npython -m pip install --user pipx\npython -m pipx ensurepath\n```\n\n----------------------------------------\n\nTITLE: Building the Lever Hiring Connector Docker Image using airbyte-ci\nDESCRIPTION: Builds the Docker image for the Lever Hiring source connector locally using the `airbyte-ci` tool. This command requires `airbyte-ci` to be installed and accessible in the environment. The resulting image will be tagged as `airbyte/source-lever-hiring:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lever-hiring/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lever-hiring build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using the Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailchimp build\n```\n\n----------------------------------------\n\nTITLE: Formatting JSON Records for Stream Test SCD Drop in JSON\nDESCRIPTION: These JSON objects represent data records for a stream named 'stream_test_scd_drop'. Each record contains various fields including id, date, timestamp, datetime conversions, and numeric values. The records demonstrate different formats for dates, times, and numbers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_reset_scd_overwrite/data_input/test_drop_scd_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_test_scd_drop\",\n    \"emitted_at\": 1602637589000,\n    \"data\": {\n      \"id\": 1,\n      \"date\": \"2022-08-29\",\n      \"timestamp_col\": \"2020-08-29T00:00:00.000000-0000\",\n      \"datetime_to_string\": \"2022-10-01T01:04:04-04:00\",\n      \"string_to_dt\": \"2022-11-01T02:03:04-07:00\",\n      \"number_to_int\": 1,\n      \"int_to_number\": 10\n    }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_test_scd_drop\",\n    \"emitted_at\": 1602637689100,\n    \"data\": {\n      \"id\": 2,\n      \"date\": \"2022-08-30\",\n      \"timestamp_col\": \"2020-08-30T00:00:00.000-00\",\n      \"datetime_to_string\": \"2022-10-02T01:04:04-04:00\",\n      \"string_to_dt\": \"2022-11-02T03:04:05-07:00\",\n      \"number_to_int\": 10,\n      \"int_to_number\": 11\n    }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_test_scd_drop\",\n    \"emitted_at\": 1602637789200,\n    \"data\": {\n      \"id\": 3,\n      \"date\": \"2022-08-31\",\n      \"timestamp_col\": \"2020-08-31T00:00:00+00\",\n      \"datetime_to_string\": \"2022-10-03T01:04:04-04:00\",\n      \"string_to_dt\": \"2022-11-03T03:04:06-07:00\",\n      \"number_to_int\": 11,\n      \"int_to_number\": 12\n    }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_test_scd_drop\",\n    \"emitted_at\": 1602637889300,\n    \"data\": {\n      \"id\": 4,\n      \"date\": \"2022-09-01\",\n      \"timestamp_col\": \"2020-08-31T00:00:00+0000\",\n      \"datetime_to_string\": \"2022-10-04T01:04:04-04:00\",\n      \"string_to_dt\": \"2022-11-04T03:04:07-07:00\",\n      \"number_to_int\": 111,\n      \"int_to_number\": 133\n    }\n  }\n}\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"stream_test_scd_drop\",\n    \"emitted_at\": 1602637989400,\n    \"data\": {\n      \"id\": 5,\n      \"date\": \"2022-09-02\",\n      \"timestamp_col\": \"2020-09-01T00:00:00Z\",\n      \"datetime_to_string\": \"2022-10-05T01:04:04-04:00\",\n      \"string_to_dt\": \"2022-11-05T03:04:08-12:00\",\n      \"number_to_int\": 1010,\n      \"int_to_number\": 1300\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Airbyte State Message Format in JSON\nDESCRIPTION: This snippet shows the structure of an Airbyte state message. It includes the message type and a data payload containing a start_date field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/integration_tests/test_data/messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"data\": {\"start_date\": \"2022-04-15\"}}}\n```\n\n----------------------------------------\n\nTITLE: Defining FixedWindowCallRatePolicy Schema in YAML\nDESCRIPTION: This YAML snippet defines the schema for the FixedWindowCallRatePolicy object. This policy enforces a limit on the number of calls ('call_limit') allowed within a specific, non-overlapping time window ('period'). It requires 'type', 'period' (ISO 8601 duration), 'call_limit', and 'matchers' properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nFixedWindowCallRatePolicy:\n  type: object\n  title: Fixed Window Call Rate Policy\n  description: A policy that allows a fixed number of calls within a specific time window.\n  required:\n    - type\n    - period\n    - call_limit\n    - matchers\n  properties:\n    type:\n      type: string\n      enum: [FixedWindowCallRatePolicy]\n    period:\n      type: string\n      format: duration\n    call_limit:\n      type: integer\n    matchers:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/HttpRequestRegexMatcher\"\n    additionalProperties: true\n```\n```\n\n----------------------------------------\n\nTITLE: Running Firestore Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Available Streams Table in Markdown\nDESCRIPTION: Table defining the available data streams, their primary keys, pagination methods, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zapsign.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| templates | token | DefaultPaginator | ✅ |  ✅  |\n| documents | token | DefaultPaginator | ✅ |  ✅  |\n| signer | token | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Checking PostgreSQL Version (Google Cloud pg_cron Prerequisite)\nDESCRIPTION: SQL query to retrieve the current PostgreSQL version. This is used to verify if the version meets the minimum requirement (version 10 or higher) for enabling the `pg_cron` extension on Google Cloud SQL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT version();\n```\n\n----------------------------------------\n\nTITLE: Airbyte TRACE Message with Stream Status\nDESCRIPTION: A JSON example of an Airbyte TRACE message containing stream status information. This message indicates the completion status of a specific stream and includes the emission timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"array_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Building the Less Annoying CRM Connector Image (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-less-annoying-crm` connector. The resulting image is tagged as `source-less-annoying-crm:dev` and can be used for local testing and development. Requires `airbyte-ci` to be installed and access to the connector's source code.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-less-annoying-crm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-less-annoying-crm build\n```\n\n----------------------------------------\n\nTITLE: Defining Standard 'Supported sync modes' Section Content in Markdown\nDESCRIPTION: This Markdown snippet specifies the standard template for the 'Supported sync modes' section in Airbyte connector documentation. It includes a standard introductory sentence mentioning the `CONNECTOR_NAME_FROM_METADATA` (placeholder for the connector name) and linking to the general Airbyte documentation about sync modes. This check enforces a uniform description format for sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n\nThe CONNECTOR_NAME_FROM_METADATA source connector supports the following [sync modes](https://docs.airbyte.com/cloud/core-concepts/#connection-sync-modes):\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci\nDESCRIPTION: Command to build a Docker image for the Freshcaller connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshcaller build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Klaviyo Source Connector\nDESCRIPTION: Command to run the full test suite for the Klaviyo source connector using airbyte-ci. This ensures all changes pass the required tests before contribution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaviyo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klaviyo test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands for running the source connector interfaces in a Docker container. Includes commands for spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenefits/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zenefits:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zenefits:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zenefits:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zenefits:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Chargify Source Connector Test Suite using airbyte-ci\nDESCRIPTION: This command uses the `airbyte-ci` tool to execute the full Continuous Integration (CI) test suite locally for the `source-chargify` connector. This helps ensure the connector functions correctly before contributing changes. It requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargify/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargify test\n```\n\n----------------------------------------\n\nTITLE: Building TicketTailor Source Connector in Bash\nDESCRIPTION: This command uses airbyte-ci to build a development image of the TicketTailor source connector. The resulting image is tagged as 'source-tickettailor:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tickettailor/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tickettailor build\n```\n\n----------------------------------------\n\nTITLE: Testing Manifest-Only Airbyte Source Connector with airbyte-ci in Bash\nDESCRIPTION: The following bash command runs the full test suite for the source-coin-api manifest-only connector using the airbyte-ci CLI tool. Dependency on airbyte-ci and appropriate project setup is required. The connector name is specified as a parameter, and results of the tests are output to the console; this is essential before publishing new versions of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coin-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coin-api test\n```\n\n----------------------------------------\n\nTITLE: Documentation of Source Harness Performance Tool\nDESCRIPTION: A markdown documentation file explaining the purpose of the source-harness component which is used to test throughput performance of source connectors via GitHub Actions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors-performance/source-harness/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# source-harness\n\nPerformance harness for source connectors.\n\nThis component is used by the `/connector-performance` GitHub action and is used in order to test throughput of\nsource connectors on a number of datasets.\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency with Poetry - Bash\nDESCRIPTION: Adds a new Python package dependency to the connector project using Poetry’s add command. This updates pyproject.toml and poetry.lock accordingly, ensuring proper dependency tracking and reproducibility. The user must specify the package name. The resulting configuration files should be committed to version control after running the command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running the Salesloft connector test suite with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Salesloft source connector using the airbyte-ci tool. This validates that any changes made to the connector work properly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesloft/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesloft test\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Greenhouse Connector\nDESCRIPTION: This command adds a new dependency to the Greenhouse connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite with airbyte-ci - Bash\nDESCRIPTION: This command runs the complete CI test suite for the Gnews source connector locally via the airbyte-ci tool. The --name parameter specifies which connector to test. Ensure airbyte-ci is installed and all dependencies for running connector tests are satisfied. Expected output is test logs and results, verifying connector functionality prior to publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gnews/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gnews test\n```\n\n----------------------------------------\n\nTITLE: Displaying Max Rate Limit Exceeded Message in LinkedIn Pages Source\nDESCRIPTION: Shows an example log message indicating that the Airbyte LinkedIn Pages source has exceeded the maximum number of retry attempts (5) for API requests, likely due to persistent rate limiting (HTTP 429). The sync operation will stop.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-pages.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"Max try rate limit exceeded...\"\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite for Convex Connector with airbyte-ci (Bash)\nDESCRIPTION: Executes the complete Continuous Integration (CI) test suite for the `source-convex` connector locally using the `airbyte-ci` tool. This typically includes integration and acceptance tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-convex test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Mailerlite source connector docker container to perform spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailerlite/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-mailerlite:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailerlite:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-mailerlite:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-mailerlite:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Planhat Source Connector Tests using airbyte-ci (Bash)\nDESCRIPTION: Executes the acceptance tests for the `source-planhat` connector using the `airbyte-ci` tool. This step validates the connector's functionality against Airbyte standards, typically performed after building the connector locally. Requires `airbyte-ci` installation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-planhat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-planhat test\n```\n\n----------------------------------------\n\nTITLE: Building Cisco Meraki Connector with Airbyte CI - Bash\nDESCRIPTION: This snippet demonstrates how to build the development image for the Cisco Meraki connector using the airbyte-ci command-line tool. The build command creates a Docker development image tagged as source-cisco-meraki:dev, which is used for local testing and development. Dependencies include the airbyte-ci CLI and a properly configured environment; ensure that airbyte-ci is installed and in your PATH. The main parameter is --name, which specifies the connector to build. The command outputs the built Docker image to your local registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cisco-meraki/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cisco-meraki build\n```\n\n----------------------------------------\n\nTITLE: Running CI Tests for Clazar Source Connector using Bash\nDESCRIPTION: Executes the complete CI test suite for the source-clazar connector locally using the `airbyte-ci` tool. This command verifies the connector's functionality and adherence to Airbyte standards. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clazar/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clazar test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Callrail source connector container for different operations including spec, check, discover, and read\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-callrail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-callrail:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-callrail:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-callrail:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-callrail:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Full CI Test Suite for Dixa Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash command runs the complete CI (continuous integration) test suite locally for the `source-dixa` connector using the `airbyte-ci` tool. This ensures all recent changes are verified before publishing new versions or submitting pull requests. The command requires a proper environment with `airbyte-ci` installed and relevant test dependencies configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dixa/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dixa test\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Azure Table Storage Output\nDESCRIPTION: JSON schema definition for the generic output structure of Azure Table Storage data. The schema includes a data property that contains all row values and an additionalProperties flag.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-table.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"data\": {\n            \"type\": \"object\"\n        },\n        \"additionalProperties\": {\n            \"type\": \"boolean\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Column Name Conflicts in JSON\nDESCRIPTION: This snippet shows a record with multiple variations of similar column names (e.g., 'User Id', 'user_id', 'UserId'). It's used to test how the system handles conflicting or similar column names in the same record.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"multiple_column_names_conflicts\",\n    \"data\": {\n      \"id\": 1,\n      \"User Id\": \"chris\",\n      \"user_id\": 42,\n      \"User id\": 300,\n      \"user id\": 102,\n      \"UserId\": 101\n    },\n    \"emitted_at\": 1623959926\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for Milvus Connector\nDESCRIPTION: Example Dockerfile for building a custom version of the Milvus connector based on the official image\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/README.md#2025-04-23_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-milvus:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Helm Chart Installation Timeout Error\nDESCRIPTION: Error message when the Helm chart installation times out waiting for a condition to be met.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nunable to install airbyte chart: unable to install helm: failed pre-install:\n 1 error occurred: * timed out waiting for the condition\n```\n\n----------------------------------------\n\nTITLE: Running Waiteraid Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the Waiteraid source connector using Docker. They include getting the spec, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-waiteraid/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-waiteraid:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-waiteraid:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-waiteraid:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-waiteraid:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Fastbill Source Connector\nDESCRIPTION: Standard commands for running the Fastbill source connector as a docker container. These commands allow you to get the spec, check the configuration, discover data sources, and read data from the configured source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fastbill/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-fastbill:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fastbill:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fastbill:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-fastbill:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Self-Managed Redirect URI Configuration\nDESCRIPTION: The redirect URI format required for Airbyte Self-Managed Enterprise SSO integration with Microsoft Entra ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/azure-entra-id.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n<your-airbyte-domain>/auth/realms/airbyte/broker/<app-integration-name>/endpoint\n```\n\n----------------------------------------\n\nTITLE: Building PayFit Connector with Airbyte CI - Bash\nDESCRIPTION: This snippet demonstrates how to build the PayFit connector locally using the Airbyte CI tool. It requires that 'airbyte-ci' is installed and available in your environment. Running the command creates a development Docker image ('source-payfit:dev') for testing purposes; you must specify the connector name as a parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-payfit/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-payfit build\n```\n\n----------------------------------------\n\nTITLE: Building Mailgun Connector Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Mailgun connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailgun/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailgun build\n```\n\n----------------------------------------\n\nTITLE: Defining Monthly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema example for the monthly_active_users stream in Google Analytics, showing the count of 30-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_30dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: SSH Key Generation Command\nDESCRIPTION: Command to generate an RSA private key in PEM format for SSH tunneling authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for the DynamoDB Connector via Gradle - Bash\nDESCRIPTION: This Gradle command runs all unit tests for the DynamoDB source connector in the Airbyte project. It should be executed from the Airbyte project root with Gradle installed. The command triggers only the unit tests located within the 'src/test' directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-dynamodb:unitTest\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Slack Connector\nDESCRIPTION: Command to execute unit tests for the Slack connector using pytest from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Alpaca Broker API Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Alpaca Broker API connector using airbyte-ci. Creates a dev image tagged as source-alpaca-broker-api:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-alpaca-broker-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-alpaca-broker-api build\n```\n\n----------------------------------------\n\nTITLE: State Record in Airbyte JSON Format\nDESCRIPTION: Sample state record in Airbyte's JSON format. Contains global state information with a start date that can be used for incremental synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Executing Credential Generation Script in Shell\nDESCRIPTION: This shell command executes the `get_credentials.sh` script located in the current directory (`connectors/google-search-console/credentials`). This script likely performs the Google OAuth 2.0 flow using the details provided in `credentials.json` to generate and output a `refresh_token`. It requires `credentials.json` to be correctly populated with the `CLIENT_ID`, `CLIENT_SECRET`, and `REDIRECTED_URI` obtained from Google Cloud Console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/credentials/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./get_credentials.sh\n```\n\n----------------------------------------\n\nTITLE: GET Request Example for Pexels Curated Endpoint - bash\nDESCRIPTION: This snippet demonstrates how to perform a GET request to the Pexels-API curated endpoint, fetching one curated photo object as an example. It requires an API key to be included in the request header or as a parameter, aligning with previous described prerequisites. The request highlights the 'per_page' parameter, which limits the number of resources returned, providing users with a reference format for querying the curated photos endpoint. The output is in JSON, and users should adjust 'per_page' or other query parameters as needed for their integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/pexels-api.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nGET https://api.pexels.com/v1/curated?per_page=1\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency with Poetry (Bash)\nDESCRIPTION: Adds a new Python package dependency to the project using Poetry. Replace `<package-name>` with the actual name of the package. This command updates the `pyproject.toml` and `poetry.lock` files, which should then be committed to version control. Requires Poetry to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte CI Using Makefile\nDESCRIPTION: Command to install Airbyte CI using the project's Makefile from the root of the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# from the root of the airbyte repository\nmake tools.airbyte-ci.install\n```\n\n----------------------------------------\n\nTITLE: Building the Destination Connector with Gradle\nDESCRIPTION: Compiles the Java code and builds the Docker image for the specified destination connector using the Gradle wrapper. This command must be executed from the root directory of the Airbyte project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Must be run from the Airbyte project root\n./gradlew :airbyte-integrations:connectors:destination-<name>:build\n```\n\n----------------------------------------\n\nTITLE: Querying TMDb API for Movie Alternative Titles\nDESCRIPTION: Example of how to make a GET request to the TMDb API to retrieve alternative titles for a specific movie. Requires an API key and movie ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tmdb.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.themoviedb.org/3/movie/{movie_id}/alternative_titles?api_key={api_key}\n```\n\n----------------------------------------\n\nTITLE: Injecting Variables into GraphQL Request - JSON\nDESCRIPTION: This JSON example shows how a parameterized GraphQL request should look after injecting a variable (e.g., limit) into the nested variables object. There are no dependencies beyond the correct GraphQL schema and variable structure. Key parameters are query and variables; the output is a correctly formatted JSON request body for GraphQL APIs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": \"query($limit: Int) { users(limit: $limit) { id name } }\",\n  \"variables\": {\n    \"limit\": 10\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte Self-Managed Enterprise with Helm (Shell)\nDESCRIPTION: This 'sh' command installs the Airbyte Self-Managed Enterprise stack using Helm. It specifies the target namespace, a custom values.yaml for configuration overrides, and chart release names. Helm 3.x or newer is required as a prerequisite, along with 'airbyte/airbyte' repository added and updated. The YAML file referenced must be populated with required configuration fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nhelm install \\\n--namespace airbyte \\\n--values ./values.yaml \\\nairbyte-enterprise \\\nairbyte/airbyte\n\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Intruder Source Connector\nDESCRIPTION: Command to run the full test suite for the Intruder source connector using airbyte-ci. This is used to verify changes and ensure compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intruder/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-intruder test\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Configuration for Airbyte Web UI\nDESCRIPTION: Series of commands to set up port forwarding to access the Airbyte web interface after upgrade. Gets the webapp pod name and container port, then establishes port forwarding on port 8080.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/upgrading-airbyte.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport POD_NAME=$(kubectl get pods -l \"app.kubernetes.io/name=webapp\" -o jsonpath=\"{.items[0].metadata.name}\")\nexport CONTAINER_PORT=$(kubectl get pod  $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\necho \"Visit http://127.0.0.1:8080 to use your application\"\nkubectl  port-forward $POD_NAME 8080:$CONTAINER_PORT\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests\nDESCRIPTION: Command to run custom integration tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: GitHub Source Connector Installation Steps - Markdown\nDESCRIPTION: Step-by-step instructions for installing and configuring the GitHub source connector in both Airbyte Cloud and Open Source environments. Includes configuration parameters like repository selection and start date settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/unit_tests/data/docs/correct_all_description_exist.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# GitHub\n\n<HideInUI>\n\nThis page contains the setup guide and reference information for the [GitHub](https://www.github.com) source connector.\n\n</HideInUI>\n\n## For Airbyte Cloud:\n\n1. [Log into your Airbyte Cloud](https://cloud.airbyte.com/workspaces) account.\n2. Click Sources and then click + New source/destination.\n3. On the Set up the source page, select GitHub from the Source type dropdown.\n4. Enter a name for the GitHub connector.\n5. Add list of GitHub repositories you want to sync.\n6. Add Start Date from with data will be replicated.\n\n## For Airbyte Open Source:\n\n1. Navigate to the Airbyte Open Source dashboard.\n2. Click Sources and then click + New source/destination.\n3. On the Set up the source page, select GitHub from the Source type dropdown.\n4. Enter a name for the GitHub connector.\n5. Add list of GitHub repositories you want to sync.\n6. Add Start Date from with data will be replicated.\n```\n\n----------------------------------------\n\nTITLE: Creating a Region with Airbyte API in Bash\nDESCRIPTION: API request to create a new region in Airbyte by sending a POST request to the /v1/regions endpoint. The region represents the geographical area where a data plane will operate.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url https://example.com/api/public/v1/regions \\\n  --header 'authorization: Bearer $TOKEN' \\\n  --header 'content-type: application/json' \\\n  --data '{\n  \"name\": \"aws-us-east-1\",\n  \"organizationId\": \"00000000-0000-0000-0000-000000000000\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Basic User Engagement Report Configuration in GA4\nDESCRIPTION: A simple custom report configuration that tracks sessions and bounce rate metrics segmented by city dimension. This basic implementation demonstrates the minimal required fields for creating a custom report.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-data-api.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"User Engagement Report\",\n    \"dimensions\": [\"city\"],\n    \"metrics\": [\"sessions\", \"bounceRate\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example SFTP Server Directory Structure\nDESCRIPTION: Provides a visual representation of a potential directory structure on an SFTP server. This example is used to explain how the `Folder Path` setting in the Airbyte SFTP Bulk connector configuration targets specific directories (e.g., `/logs/2022`) for file replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp-bulk.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nRoot\n| - logs\n|   | - 2021\n|   | - 2022\n|\n| - files\n|   | - 2021\n|   | - 2022\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands via Docker (Bash)\nDESCRIPTION: These commands show how to run the connector's operations (`spec`, `check`, `discover`, `read`) within a Docker container using the previously built image (`airbyte/source-google-search-console:dev`). It demonstrates volume mounting (`-v`) to provide configuration files and catalogs from the host machine to the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-google-search-console:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-search-console:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-search-console:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-google-search-console:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-xata build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands for running the connector container with different operations including spec, check, discover, and read operations. Each command mounts necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wikipedia-pageviews/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-wikipedia-pageviews:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-wikipedia-pageviews:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-wikipedia-pageviews:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-wikipedia-pageviews:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Deepset Connector as Docker Container\nDESCRIPTION: Commands to run the Deepset connector as a Docker container for specification, configuration check, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-deepset:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-deepset:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-deepset:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally with Poetry - Bash\nDESCRIPTION: Executes various Airbyte connector commands (spec, check, discover, read) locally using Poetry, allowing for checking configuration, discovering schemas, and reading data. The '--config' and '--catalog' flags specify configuration and catalog files respectively. Assumes the presence of 'secrets/config.json' and, for the read command, 'sample_files/configured_catalog.json'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-azure-table spec\npoetry run source-azure-table check --config secrets/config.json\npoetry run source-azure-table discover --config secrets/config.json\npoetry run source-azure-table read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands to run the connector in different modes including spec, check, discover, and read operations using Docker.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sunshine/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zendesk-sunshine:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-sunshine:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-sunshine:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zendesk-sunshine:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Jira Connector Commands Locally\nDESCRIPTION: Series of commands to run various Jira connector operations locally, including spec generation, configuration check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-jira spec\npoetry run source-jira check --config secrets/config.json\npoetry run source-jira discover --config secrets/config.json\npoetry run source-jira read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Datadog Source Connector Docker Commands (Bash)\nDESCRIPTION: These shell commands demonstrate how to execute common operations with the Datadog source connector using Docker. Commands include running the 'spec', 'check', 'discover', and 'read' operations. The commands require a valid configuration file in a 'secrets' directory and (for 'read') an integration test catalog. Volume mounting is used to inject secrets and test data. Outputs vary by subcommand: connector specification, configuration check output, discovery schema, or read results.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datadog/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-datadog:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-datadog:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-datadog:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-datadog:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Discover Command for an Airbyte Connector (Bash)\nDESCRIPTION: This command uses `poetry run` to execute the `discover` method of the `source-survey-monkey-demo` Airbyte connector. It requires a configuration file specified by the `--config` flag (e.g., `secrets/config.json`). This operation queries the source system to determine the available data streams and their schemas, outputting an AirbyteCatalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/5-discover.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo discover --config secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 External Log Storage for Airbyte in YAML\nDESCRIPTION: This YAML configuration sets up S3 as external log storage for Airbyte, disabling the default internal Minio storage. It's added to the values.yaml file under the global section.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  storage:\n    type: \"S3\"\n    storageSecretName: airbyte-config-secrets # Name of your Kubernetes secret.\n    bucket: ## S3 bucket names that you've created. We recommend storing the following all in one bucket.\n      log: airbyte-bucket\n      state: airbyte-bucket\n      workloadOutput: airbyte-bucket\n    s3:\n      region: \"\" ## e.g. us-east-1\n      authenticationType: credentials ## Use \"credentials\" or \"instanceProfile\"\n```\n\n----------------------------------------\n\nTITLE: Building MSSQL Connector with Airbyte CI\nDESCRIPTION: Command to build the MSSQL destination connector using the Airbyte CI CLI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=destination-mssql-v2 build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Harness Source Connector using airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Harness source connector using airbyte-ci tool. This creates an image tagged as 'airbyte/source-harness:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harness/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-harness build\n```\n\n----------------------------------------\n\nTITLE: Building the StockData Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the StockData connector using airbyte-ci. The resulting image is tagged as 'source-stockdata:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stockdata/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-stockdata build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Sonar Cloud Source Connector\nDESCRIPTION: Command to build the Docker image for the Sonar Cloud source connector using airbyte-ci. This creates an image tagged as airbyte/source-sonar-cloud:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sonar-cloud/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sonar-cloud build\n```\n\n----------------------------------------\n\nTITLE: Building Docuseal Connector Image with airbyte-ci (bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image tagged `source-docuseal:dev` for the Docuseal source connector. Requires `airbyte-ci` to be installed and the connector source code to be present in the expected directory structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-docuseal/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-docuseal build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Schema for RemoveFields Transformation - YAML\nDESCRIPTION: YAML schema definition for RemoveFields, enabling deletion of fields from records during data ingestion. The 'field_pointers' parameter is an array of field paths (arrays of strings) to target fields for removal. This configuration is referenced within transformation steps in Airbyte stream definitions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nRemoveFields:\n  type: object\n  required:\n    - field_pointers\n  additionalProperties: true\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    field_pointers:\n      type: array\n      items:\n        \"$ref\": \"#/definitions/FieldPointer\"\n```\n\n----------------------------------------\n\nTITLE: Defining Go High Level Streams in Markdown\nDESCRIPTION: Markdown table listing the available streams in the Go High Level connector, including their primary keys, pagination methods, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/high-level.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Contacts | id | DefaultPaginator | ✅ |  ❌  |\n| Payments | _id | DefaultPaginator | ✅ |  ❌  |\n| Form Submissions | id | DefaultPaginator | ✅ |  ❌  |\n| Custom Fields | id | No pagination | ✅ |  ❌  |\n| Transactions | _id | DefaultPaginator | ✅ |  ❌  |\n| Invoices | _id | DefaultPaginator | ✅ |  ❌  |\n| Opportunities | id | DefaultPaginator | ✅ |  ❌  |\n| Pipelines | id | No pagination | ✅ |  ❌  |\n| Subscriptions | _id | DefaultPaginator | ✅ |  ❌  |\n| Orders | _id | DefaultPaginator | ✅ |  ❌  |\n| Order | _id | No pagination | ✅ |  ❌  |\n| Contact Search | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Describing Milvus Destination Connector Functionality in Markdown\nDESCRIPTION: This markdown snippet outlines the three main functions of the Milvus destination connector: splitting records, embedding text data, and storing data in Milvus. It also mentions the use of LangChain for text splitting and the CDK for embedding generation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Milvus Destination Connector Bootstrap\n\nThis destination does three things:\n\n- Split records into chunks and separates metadata from text data\n- Embeds text data into an embedding vector\n- Stores the metadata and embedding vector in a vector database\n\nThe record processing is using the text split components from https://python.langchain.com/docs/modules/data_connection/document_transformers/.\n\nThere are various possible providers for generating embeddings, delivered as part of the CDK (`airbyte_cdk.destinations.vector_db_based`).\n\nEmbedded documents are stored in the Milvus vector database.\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Circa Source Connector - Bash\nDESCRIPTION: This bash snippet runs acceptance tests for the Airbyte Circa source connector via the airbyte-ci tool. Dependencies include the airbyte-ci tool and any test configurations required for acceptance tests. The command ensures the connector meets required standards and functions as expected; results and error output will be shown in the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-circa/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-circa test\n\n```\n\n----------------------------------------\n\nTITLE: Building Timely Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Timely source connector using airbyte-ci. This creates an image tagged as airbyte/source-timely:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-timely/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-timely build\n```\n\n----------------------------------------\n\nTITLE: String Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for string data tests in Airbyte's JSON record format. Includes examples with regular text, empty strings, and special characters with timestamps and stream identifiers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"foo bar\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"string_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"some random special characters: ࠈൡሗ\" }}}\n```\n\n----------------------------------------\n\nTITLE: Listing Affected Analytics Streams for LinkedIn Ads v2.0.0 Primary Key Change (Text)\nDESCRIPTION: Lists all the analytics-related streams affected by the primary key change introduced in version 2.0.0 of the LinkedIn Ads connector. The change involved converting the primary key `pivotValues` (array of strings) to `string_of_pivot_values` (string) for better destination compatibility, requiring a data refresh.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n- \"ad_campaign_analytics\"\n- \"ad_creative_analytics\"\n- \"ad_impression_device_analytics\"\n- \"ad_member_company_size_analytics\"\n- \"ad_member_country_analytics\"\n- \"ad_member_job_function_analytics\"\n- \"ad_member_job_title_analytics\"\n- \"ad_member_industry_analytics\"\n- \"ad_member_seniority_analytics\"\n- \"ad_member_region_analytics\"\n- \"ad_member_company_analytics\"\n```\n\n----------------------------------------\n\nTITLE: Extracting abctl Archive on Linux\nDESCRIPTION: Command to extract the downloaded abctl archive file on Linux, creating a directory with the executable and required files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntar -xvzf {name-of-file-downloaded.linux-*.tar.gz}\n```\n\n----------------------------------------\n\nTITLE: Building 7shifts Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the 7shifts source connector using airbyte-ci tool. Creates a dev image tagged as source-7shifts:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-7shifts/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-7shifts build\n```\n\n----------------------------------------\n\nTITLE: Building Weaviate Connector with airbyte-ci\nDESCRIPTION: This command builds the Weaviate connector using the airbyte-ci tool, creating a Docker image tagged as 'dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-weaviate build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Convex Connector\nDESCRIPTION: Command to add a new dependency to the Convex connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Firebolt Connector Docker Image\nDESCRIPTION: Commands to build the Docker image for the Firebolt connector using airbyte-ci or docker build.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-firebolt build\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-firebolt:dev .\n```\n\n----------------------------------------\n\nTITLE: Building MongoDB Connector with Gradle\nDESCRIPTION: Command to build the MongoDB source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mongodb-v2/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mongodb-v2:build\n```\n\n----------------------------------------\n\nTITLE: ServiceNow Stream Configuration List - CMDB\nDESCRIPTION: List of supported Configuration Management Database (CMDB) streams that can be synced using the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-service-now.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- cmdb_ci_wap_network\n- cmdb_ci_ip_router\n- cmdb_ci_ip_switch\n- cmdb_ci_lb_bigip\n- cmdb_ci_ip_firewall\n- cmdb_ci_printer\n- cmdb_ci_scanner\n- cmdb_ci_linux_server\n- cmdb_ci_comm\n- cmdb_ci_win_server\n- cmdb_ci_ucs_chassis\n- cmdb_ci_storage_switch\n- cmdb_ci_pc_hardware\n- cmdb_ci_esx_server\n- cmdb_ci_aix_server\n- cmdb_ci_solaris_server\n- cmdb_ci_chassis_server\n- cmdb_ci_server\n- cmdb_ci_net_app_server\n```\n\n----------------------------------------\n\nTITLE: Running Smartsheets Connector Commands Locally\nDESCRIPTION: Series of commands to run various functions of the Smartsheets connector locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-smartsheets spec\npoetry run source-smartsheets check --config secrets/config.json\npoetry run source-smartsheets discover --config secrets/config.json\npoetry run source-smartsheets read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Sonar Cloud Source Connector\nDESCRIPTION: Command to run the full test suite for the Sonar Cloud source connector using airbyte-ci. This is used to verify changes and ensure compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sonar-cloud/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sonar-cloud test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands to run the Ashby source connector docker container for various operations including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ashby/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-ashby:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ashby:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ashby:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-ashby:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite for the Partnerstack Connector using airbyte-ci\nDESCRIPTION: This command executes the full Airbyte connector integration test suite locally for the `source-partnerstack` connector. It uses the `airbyte-ci` tool, which must be installed beforehand. Running these tests helps ensure the connector functions correctly and meets Airbyte's quality standards before contributing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-partnerstack/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-partnerstack test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Zendesk Chat connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-chat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zendesk-chat:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-chat:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-chat:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zendesk-chat:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Orchestrator Dependencies with Poetry\nDESCRIPTION: Commands to install dependencies using Poetry and create an environment file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\ncp .env.template .env\n```\n\n----------------------------------------\n\nTITLE: Building Toggl Source Connector Docker Image with airbyte-ci\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Toggl source connector. The resulting image will be tagged as 'airbyte/source-toggl:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-toggl/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-toggl build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Source Dremio Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash command runs the complete integration test suite for the source-dremio connector using the airbyte-ci tool. The command should be executed from the project root and requires airbyte-ci to be installed. The --name parameter targets the specific connector. Output includes test results for local or CI workflows.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dremio/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dremio test\n```\n\n----------------------------------------\n\nTITLE: Building Firebolt Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Firebolt source connector using the recommended airbyte-ci tool, which creates an image tagged as airbyte/source-firebolt:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firebolt build\n```\n\n----------------------------------------\n\nTITLE: Testing Brex Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Brex source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-brex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-brex test\n```\n\n----------------------------------------\n\nTITLE: Running Salesforce Connector Unit Tests\nDESCRIPTION: Command to run unit tests for the Salesforce connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency using Poetry (Bash)\nDESCRIPTION: This command uses Poetry to add a new Python package dependency to the project. It automatically updates the `pyproject.toml` and `poetry.lock` files, which should then be committed to version control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Local Connector Operations\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and write operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-cumulio spec\npoetry run destination-cumulio check --config secrets/config.json\npoetry run destination-cumulio discover --config secrets/config.json\npoetry run destination-cumulio write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker image for Shopify connector\nDESCRIPTION: Command to build a Docker image for the Shopify source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shopify build\n```\n\n----------------------------------------\n\nTITLE: Running the Spec Operation for Zendesk Talk Connector (Bash)\nDESCRIPTION: Executes the `spec` command within a temporary Docker container using the locally built `airbyte/source-zendesk-talk:dev` image. This command outputs the connector's specification, detailing the required configuration parameters. It requires Docker and the built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zendesk-talk:dev spec\n```\n\n----------------------------------------\n\nTITLE: Building Teradata Connector Docker Image with Gradle\nDESCRIPTION: Builds the Docker image for the Teradata destination connector using Gradle. The resulting image will be tagged as 'airbyte/destination-teradata:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-teradata/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-teradata:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building the Coinmarketcap Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This Bash command builds the Coinmarketcap source connector Docker image using the airbyte-ci tool. It requires that airbyte-ci is installed and available on the system as per the prerequisites. The --name flag specifies the connector to build, resulting in a Docker image tagged as airbyte/source-coinmarketcap:dev on the local host. Input is handled by passing the connector name as an argument, and there are no direct output artifacts except for the generated image. No additional configuration is needed beyond having airbyte-ci installed and project files present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coinmarketcap/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coinmarketcap build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Yotpo connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yotpo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yotpo test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, and writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-xata spec\npoetry run destination-xata check --config secrets/config.json\npoetry run destination-xata write --config secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Rootly Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Rootly source connector using airbyte-ci. This validates that the connector meets Airbyte's acceptance criteria.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rootly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rootly test\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Support Table for GlassFlow Destination in Markdown\nDESCRIPTION: This table shows the supported features of the GlassFlow destination, including full refresh sync, incremental append sync, and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/glassflow.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Supported?\\(Yes/No\\) | Notes |\n| :----------------------------- | :------------------- | :---- |\n| Full Refresh Sync              | Yes                  |       |\n| Incremental - Append Sync      | Yes                  |       |\n| Incremental - Append + Deduped | No                   |       |\n| Namespaces                     | Yes                  |       |\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands in Docker\nDESCRIPTION: Commands to run the Fauna connector operations using Docker with volume mounts for configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-fauna:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fauna:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fauna:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-fauna:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Auth0 Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the Auth0 connector in Docker. Includes commands for spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-auth0/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-auth0:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-auth0:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-auth0:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-auth0:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Convex Connector Operations via Docker (Bash)\nDESCRIPTION: Executes the standard Airbyte connector commands (spec, check, discover, read) using the pre-built `airbyte/source-convex:dev` Docker image. It utilizes volume mounts (`-v`) to provide configuration and catalog files from the host machine to the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-convex:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-convex:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-convex:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-convex:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry - Bash\nDESCRIPTION: This command allows for adding a new package dependency to the project, updating pyproject.toml and poetry.lock accordingly. Replace '<package-name>' with the actual dependency. After execution, these changes should be committed to version control. Poetry must be installed, and the command should be run from the project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Testing the Productboard Source Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Productboard connector using airbyte-ci. This validates that the connector works as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-productboard/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-productboard test\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Table of Supported Features\nDESCRIPTION: A markdown table showing the supported features and sync modes for the YouTube Analytics connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/youtube-analytics.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature               | Supported?  |\n| :-------------------- | :---------- |\n| Full Refresh Sync     | Yes         |\n| Incremental Sync      | Yes         |\n| SSL connection        | Yes         |\n| Channel Reports       | Yes         |\n| Content Owner Reports | Coming soon |\n| YouTube Data API      | Coming soon |\n```\n\n----------------------------------------\n\nTITLE: Testing Web Scrapper Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Web Scrapper connector using airbyte-ci tool. Validates the connector's functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-web-scrapper/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-web-scrapper test\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image with Gradle - Shell\nDESCRIPTION: This code snippet shows how to build a Docker image for the source connector using Gradle from the Airbyte project root. It assumes Docker and Gradle are both installed and configured. Running this command produces a Docker image tagged as 'airbyte/source-e2e-test:dev' on the local machine. This image is essential for running integration tests or deploying the connector in Airbyte workflows.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n```\n./gradlew :airbyte-integrations:connectors:source-e2e-test:buildConnectorImage\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Incident.io Streams in Markdown\nDESCRIPTION: Markdown table displaying the available streams in the Incident.io source connector, including their names, primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/incident-io.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| actions | id | No pagination | ✅ |  ❌  |\n| catalog_types | id | No pagination | ✅ |  ❌  |\n| custom_fields | id | No pagination | ✅ |  ❌  |\n| follow-ups | id | No pagination | ✅ |  ❌  |\n| incident_roles | id | No pagination | ✅ |  ❌  |\n| incident_timestamps | id | No pagination | ✅ |  ❌  |\n| incident_updates | id | DefaultPaginator | ✅ |  ❌  |\n| incident_statuses | id | No pagination | ✅ |  ❌  |\n| workflows | id | No pagination | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| severities | id | No pagination | ✅ |  ❌  |\n| schedules | id | DefaultPaginator | ✅ |  ❌  |\n| incidents | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building MixMax Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the MixMax source connector using airbyte-ci tool. Creates a dev image tagged as source-mixmax:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixmax/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mixmax build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Firebase-Realtime-Database Connector\nDESCRIPTION: Command to build a Docker image for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firebase-realtime-database build\n```\n\n----------------------------------------\n\nTITLE: Building Mailtrap Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Mailtrap source connector using airbyte-ci. Creates a dev image tagged as source-mailtrap:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailtrap/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailtrap build\n```\n\n----------------------------------------\n\nTITLE: Mapping Source Integration Types to Airbyte Types (Markdown Table)\nDESCRIPTION: This table outlines the direct mapping between common data types encountered in source files (`string`, `number`, `array`, `object`) and their corresponding representation within the Airbyte type system during the sync process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-onedrive.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog for Hellobaton API Connector in Markdown\nDESCRIPTION: This code snippet shows a markdown table that provides a detailed changelog for the Hellobaton API connector, including version numbers, dates, pull request links, and subjects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hellobaton.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                             |\n| :------ | :--------- | :------------------------------------------------------- | :---------------------------------- |\n| 0.3.23 | 2025-04-19 | [58157](https://github.com/airbytehq/airbyte/pull/58157) | Update dependencies |\n| 0.3.22 | 2025-04-12 | [57705](https://github.com/airbytehq/airbyte/pull/57705) | Update dependencies |\n| 0.3.21 | 2025-04-05 | [57098](https://github.com/airbytehq/airbyte/pull/57098) | Update dependencies |\n| 0.3.20 | 2025-03-29 | [56661](https://github.com/airbytehq/airbyte/pull/56661) | Update dependencies |\n| 0.3.19 | 2025-03-22 | [56050](https://github.com/airbytehq/airbyte/pull/56050) | Update dependencies |\n| 0.3.18 | 2025-03-08 | [55434](https://github.com/airbytehq/airbyte/pull/55434) | Update dependencies |\n| 0.3.17 | 2025-03-01 | [54771](https://github.com/airbytehq/airbyte/pull/54771) | Update dependencies |\n| 0.3.16 | 2025-02-22 | [54347](https://github.com/airbytehq/airbyte/pull/54347) | Update dependencies |\n| 0.3.15 | 2025-02-15 | [53806](https://github.com/airbytehq/airbyte/pull/53806) | Update dependencies |\n| 0.3.14 | 2025-02-08 | [53286](https://github.com/airbytehq/airbyte/pull/53286) | Update dependencies |\n| 0.3.13 | 2025-02-01 | [52772](https://github.com/airbytehq/airbyte/pull/52772) | Update dependencies |\n| 0.3.12 | 2025-01-25 | [52292](https://github.com/airbytehq/airbyte/pull/52292) | Update dependencies |\n| 0.3.11 | 2025-01-18 | [51810](https://github.com/airbytehq/airbyte/pull/51810) | Update dependencies |\n| 0.3.10 | 2025-01-11 | [51203](https://github.com/airbytehq/airbyte/pull/51203) | Update dependencies |\n| 0.3.9 | 2025-01-04 | [50657](https://github.com/airbytehq/airbyte/pull/50657) | Update dependencies |\n| 0.3.8 | 2024-12-21 | [50122](https://github.com/airbytehq/airbyte/pull/50122) | Update dependencies |\n| 0.3.7 | 2024-12-14 | [49643](https://github.com/airbytehq/airbyte/pull/49643) | Update dependencies |\n| 0.3.6 | 2024-12-12 | [49245](https://github.com/airbytehq/airbyte/pull/49245) | Update dependencies |\n| 0.3.5 | 2024-12-11 | [48981](https://github.com/airbytehq/airbyte/pull/48981) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.3.4 | 2024-11-05 | [48359](https://github.com/airbytehq/airbyte/pull/48359) | Revert to source-declarative-manifest v5.17.0 |\n| 0.3.3 | 2024-11-05 | [48320](https://github.com/airbytehq/airbyte/pull/48320) | Update dependencies |\n| 0.3.2 | 2024-10-22 | [47236](https://github.com/airbytehq/airbyte/pull/47236) | Update dependencies |\n| 0.3.1 | 2024-08-16 | [44196](https://github.com/airbytehq/airbyte/pull/44196) | Bump source-declarative-manifest version |\n| 0.3.0 | 2024-08-15 | [44142](https://github.com/airbytehq/airbyte/pull/44142) | Refactor connector to manifest-only format |\n| 0.2.14 | 2024-08-12 | [43779](https://github.com/airbytehq/airbyte/pull/43779) | Update dependencies |\n| 0.2.13 | 2024-08-10 | [43465](https://github.com/airbytehq/airbyte/pull/43465) | Update dependencies |\n| 0.2.12 | 2024-08-03 | [43233](https://github.com/airbytehq/airbyte/pull/43233) | Update dependencies |\n| 0.2.11 | 2024-07-27 | [42678](https://github.com/airbytehq/airbyte/pull/42678) | Update dependencies |\n| 0.2.10 | 2024-07-20 | [42232](https://github.com/airbytehq/airbyte/pull/42232) | Update dependencies |\n| 0.2.9 | 2024-07-13 | [41888](https://github.com/airbytehq/airbyte/pull/41888) | Update dependencies |\n| 0.2.8 | 2024-07-10 | [41538](https://github.com/airbytehq/airbyte/pull/41538) | Update dependencies |\n| 0.2.7 | 2024-07-09 | [41277](https://github.com/airbytehq/airbyte/pull/41277) | Update dependencies |\n| 0.2.6 | 2024-07-06 | [40838](https://github.com/airbytehq/airbyte/pull/40838) | Update dependencies |\n| 0.2.5 | 2024-06-26 | [40445](https://github.com/airbytehq/airbyte/pull/40445) | Update dependencies |\n| 0.2.4 | 2024-06-22 | [40195](https://github.com/airbytehq/airbyte/pull/40195) | Update dependencies |\n| 0.2.3 | 2024-06-15 | [39113](https://github.com/airbytehq/airbyte/pull/39113) | Make compatible with builder |\n| 0.2.2 | 2024-06-06 | [39189](https://github.com/airbytehq/airbyte/pull/39189) | [autopull] Upgrade base image to v1.2.2 |\n| 0.2.1 | 2024-05-21 | [38507](https://github.com/airbytehq/airbyte/pull/38507) | [autopull] base image + poetry + up_to_date |\n| 0.2.0 | 2023-08-19 | [29490](https://github.com/airbytehq/airbyte/pull/29490) | Migrate CDK from Python to Low Code |\n| 0.1.0 | 2022-01-14 | [8461](https://github.com/airbytehq/airbyte/pull/8461) | 🎉 New Source: Hellobaton |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI - Bash\nDESCRIPTION: Builds a Docker image for the source-file connector using airbyte-ci. Assumes airbyte-ci is installed and run from the connector's root. Produces a Docker image tagged as airbyte/source-file:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-file build\n```\n\n----------------------------------------\n\nTITLE: Configuring Jamf Pro Connection Parameters in Markdown\nDESCRIPTION: Defines the configuration parameters required for connecting to Jamf Pro. It includes the subdomain, username, and password fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/jamf-pro.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `subdomain` | `string` | Subdomain. The unique subdomain for your Jamf Pro instance. |  |\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building Zonka Feedback Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Zonka Feedback source connector using airbyte-ci. Creates a dev image tagged as source-zonka-feedback:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zonka-feedback/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zonka-feedback build\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Spec Command via Docker (Bash)\nDESCRIPTION: Executes the `spec` command within a temporary Docker container using the locally built `airbyte/source-coingecko-coins:dev` image. This command outputs the connector's specification (spec).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-coingecko-coins:dev spec\n```\n\n----------------------------------------\n\nTITLE: Building Breezy HR Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Breezy HR source connector using airbyte-ci. Creates a dev image tagged as source-breezy-hr:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-breezy-hr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-breezy-hr build\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Surveymonkey Connector\nDESCRIPTION: Command to add a new dependency to the Surveymonkey connector using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Tests for ERD Tools\nDESCRIPTION: This snippet shows how to run tests for the ERD tools using Poetry and pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/erd/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Docker image for the Captain Data source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-captain-data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-captain-data build\n```\n\n----------------------------------------\n\nTITLE: Building Vectara Connector with airbyte-ci\nDESCRIPTION: Command to build the Vectara connector using the airbyte-ci tool, which creates a Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-vectara build\n```\n\n----------------------------------------\n\nTITLE: Example Output of Listing Files Inside Kubernetes Pod\nDESCRIPTION: Illustrates the expected prompt and sample output after successfully executing `kubectl exec` and running `ls` inside the pod's `/config` directory. It shows typical files like `FINISHED_UPLOADING`, `destination_catalog.json`, and `destination_config.json`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nroot@destination-bigquery-worker-3607-0-chlle:/config# ls\nFINISHED_UPLOADING  destination_catalog.json  destination_config.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Server Debug Options in Docker Compose (YAML)\nDESCRIPTION: Defines the environment configuration for the `server` service within `docker-compose.debug.yaml`. This snippet enables JVM debugging by mapping the host's `DEBUG_SERVER_JAVA_OPTIONS` environment variable to the `JAVA_TOOL_OPTIONS` variable inside the `server` container, allowing a remote debugger to attach.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  environment:\n    - JAVA_TOOL_OPTIONS=${DEBUG_SERVER_JAVA_OPTIONS}\n```\n\n----------------------------------------\n\nTITLE: Example Dataset for Page Increment Pagination (JSON)\nDESCRIPTION: Illustrates a sample JSON dataset containing product information used to demonstrate the Page Increment pagination method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"id\": 1, \"name\": \"Product A\"},\n  {\"id\": 2, \"name\": \"Product B\"},\n  {\"id\": 3, \"name\": \"Product C\"},\n  {\"id\": 4, \"name\": \"Product D\"},\n  {\"id\": 5, \"name\": \"Product E\"},\n  {\"id\": 6, \"name\": \"Product F\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom State Length in Airbyte Declarative OAuth (YAML Diff)\nDESCRIPTION: Shows how to modify the `oauth_config_specification` in a YAML file to define minimum (10) and maximum (27) lengths for the auto-generated `state` parameter used in the OAuth consent URL. This change is applied using a diff format between `base_oauth.yml` and `base_oauth_with_custom_state.yml`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_11\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ base_oauth_with_custom_state.yml\n@@ -80,10 +80,10 @@ spec:\n     oauth_config_specification:\n       oauth_connector_input_specification:\n         consent_url: >-\n-          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n-          redirect_uri_value }}&state={{ state_value }}\n+          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{ redirect_uri_value }}&state={{ state_value }}\n         access_token_url: >-\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n+        state: {\n+          min: 10,\n+          max: 27\n+        }\n       complete_oauth_output_specification:\n         required:\n           - access_token\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-rabbitmq build\n```\n\n----------------------------------------\n\nTITLE: Running Tests for the Ebay Finance Connector\nDESCRIPTION: Command to run the acceptance tests for the Ebay Finance connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ebay-finance/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ebay-finance test\n```\n\n----------------------------------------\n\nTITLE: Documenting Incident.io Connector Changelog in Markdown\nDESCRIPTION: Markdown table showing the version history of the Incident.io source connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/incident-io.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.20 | 2025-04-19 | [58204](https://github.com/airbytehq/airbyte/pull/58204) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57724](https://github.com/airbytehq/airbyte/pull/57724) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57094](https://github.com/airbytehq/airbyte/pull/57094) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56650](https://github.com/airbytehq/airbyte/pull/56650) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56006](https://github.com/airbytehq/airbyte/pull/56006) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55475](https://github.com/airbytehq/airbyte/pull/55475) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [54780](https://github.com/airbytehq/airbyte/pull/54780) | Update dependencies |\n| 0.0.13 | 2025-02-22 | [54316](https://github.com/airbytehq/airbyte/pull/54316) | Update dependencies |\n| 0.0.12 | 2025-02-15 | [53844](https://github.com/airbytehq/airbyte/pull/53844) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53299](https://github.com/airbytehq/airbyte/pull/53299) | Update dependencies |\n| 0.0.10 | 2025-02-01 | [52718](https://github.com/airbytehq/airbyte/pull/52718) | Update dependencies |\n| 0.0.9 | 2025-01-25 | [52221](https://github.com/airbytehq/airbyte/pull/52221) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [51782](https://github.com/airbytehq/airbyte/pull/51782) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51186](https://github.com/airbytehq/airbyte/pull/51186) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50648](https://github.com/airbytehq/airbyte/pull/50648) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50137](https://github.com/airbytehq/airbyte/pull/50137) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49218](https://github.com/airbytehq/airbyte/pull/49218) | Update dependencies |\n| 0.0.3 | 2024-12-11 | [48989](https://github.com/airbytehq/airbyte/pull/48989) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.2 | 2024-11-04 | [47842](https://github.com/airbytehq/airbyte/pull/47842) | Update dependencies |\n| 0.0.1 | 2024-10-03 | | Initial release by [@aazam-gh](https://github.com/aazam-gh) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Building the Flexmail Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Flexmail source connector using airbyte-ci. This creates a dev image named 'source-flexmail:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flexmail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flexmail build\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAQ Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This command executes the acceptance tests defined for the `source-openaq` connector using the `airbyte-ci` tool. It verifies the connector's functionality against standard Airbyte test suites. Requires `airbyte-ci` to be installed and typically requires a connector image (like the one built with the build command) to be available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openaq/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-openaq test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-asana build\n```\n\n----------------------------------------\n\nTITLE: Installing Gridly Connector Dependencies with Poetry\nDESCRIPTION: Command to install the Gridly connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Source-100ms Connector with Airbyte-CI\nDESCRIPTION: Command to build a development image of the source-100ms connector using airbyte-ci. Creates a dev image tagged as 'source-100ms:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-100ms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-100ms build\n```\n\n----------------------------------------\n\nTITLE: Building the Railz connector docker image using airbyte-ci\nDESCRIPTION: This command builds the docker image for the Railz source connector using the airbyte-ci tool. The image will be available on the host with the tag 'airbyte/source-railz:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-railz/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-railz build\n```\n\n----------------------------------------\n\nTITLE: Building the Fulcrum Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Fulcrum source connector using airbyte-ci. This creates a 'source-fulcrum:dev' image for local testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fulcrum/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fulcrum build\n```\n\n----------------------------------------\n\nTITLE: Displaying Keen Destination Changelog in Markdown\nDESCRIPTION: A markdown table showing the version history and changes made to the Keen destination connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/keen.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                      |\n| :------ | :--------- | :------------------------------------------------------- | :--------------------------------------------------------------------------- |\n| 0.2.4   | 2022-08-04 | [15291](https://github.com/airbytehq/airbyte/pull/15291) | Update Keen destination to use outputRecordCollector to properly store state |\n| 0.2.3   | 2022-06-17 | [13864](https://github.com/airbytehq/airbyte/pull/13864) | Updated stacktrace format for any trace message errors                       |\n| 0.2.1   | 2021-12-30 | [8809](https://github.com/airbytehq/airbyte/pull/8809)   | Update connector fields title/description                                    |\n| 0.2.0   | 2021-09-10 | [5973](https://github.com/airbytehq/airbyte/pull/5973)   | Fix timestamp inference for complex schemas                                  |\n| 0.1.0   | 2021-08-18 | [5339](https://github.com/airbytehq/airbyte/pull/5339)   | Keen Destination Release!                                                    |\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table in Markdown\nDESCRIPTION: A markdown table defining the supported features of the ActiveCampaign connector, specifically indicating support for full refresh sync and incremental sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/activecampaign.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Building the FreshBooks Connector Locally with airbyte-ci\nDESCRIPTION: Command to build the FreshBooks connector locally, creating a dev image (source-freshbooks:dev) for testing purposes. This requires having airbyte-ci installed as a prerequisite.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshbooks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshbooks build\n```\n\n----------------------------------------\n\nTITLE: Using wordcount Filter in Jinja2\nDESCRIPTION: Demonstrates the `wordcount` filter in Jinja2, which counts the number of words in a string (splitting by whitespace). The example counts the words in 'hello world'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_54\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello world'|wordcount }}\n```\n\n----------------------------------------\n\nTITLE: Building and Running a Custom Docker Image\nDESCRIPTION: Commands to build a custom Docker image for the connector and run its spec command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-fauna:dev .\n# Running the spec command against your patched connector\ndocker run airbyte/source-fauna:dev spec\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector's various operations within a Docker container, including spec generation, configuration checking, discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-airtable:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-airtable:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-airtable:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-airtable:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Sage HR Connector\nDESCRIPTION: This command executes the acceptance tests for the Sage HR connector using airbyte-ci. It's used to verify the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sage-hr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sage-hr test\n```\n\n----------------------------------------\n\nTITLE: Displaying Height Connector Changelog in Markdown\nDESCRIPTION: This snippet shows the changelog for the Height connector, detailing version history, dates, pull requests, and changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/height.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n| ------------------ | ------------ | ---- | ---------------- |\n| 0.0.23 | 2025-04-19 | [58219](https://github.com/airbytehq/airbyte/pull/58219) | Update dependencies |\n| 0.0.22 | 2025-04-12 | [57733](https://github.com/airbytehq/airbyte/pull/57733) | Update dependencies |\n| 0.0.21 | 2025-04-05 | [57076](https://github.com/airbytehq/airbyte/pull/57076) | Update dependencies |\n| 0.0.20 | 2025-03-29 | [56710](https://github.com/airbytehq/airbyte/pull/56710) | Update dependencies |\n| 0.0.19 | 2025-03-22 | [56065](https://github.com/airbytehq/airbyte/pull/56065) | Update dependencies |\n| 0.0.18 | 2025-03-08 | [55435](https://github.com/airbytehq/airbyte/pull/55435) | Update dependencies |\n| 0.0.17 | 2025-03-01 | [54801](https://github.com/airbytehq/airbyte/pull/54801) | Update dependencies |\n| 0.0.16 | 2025-02-22 | [54288](https://github.com/airbytehq/airbyte/pull/54288) | Update dependencies |\n| 0.0.15 | 2025-02-15 | [53793](https://github.com/airbytehq/airbyte/pull/53793) | Update dependencies |\n| 0.0.14 | 2025-02-08 | [53247](https://github.com/airbytehq/airbyte/pull/53247) | Update dependencies |\n| 0.0.13 | 2025-02-01 | [52741](https://github.com/airbytehq/airbyte/pull/52741) | Update dependencies |\n| 0.0.12 | 2025-01-25 | [52264](https://github.com/airbytehq/airbyte/pull/52264) | Update dependencies |\n| 0.0.11 | 2025-01-18 | [51836](https://github.com/airbytehq/airbyte/pull/51836) | Update dependencies |\n| 0.0.10 | 2025-01-11 | [51154](https://github.com/airbytehq/airbyte/pull/51154) | Update dependencies |\n| 0.0.9 | 2024-12-28 | [50610](https://github.com/airbytehq/airbyte/pull/50610) | Update dependencies |\n| 0.0.8 | 2024-12-21 | [50094](https://github.com/airbytehq/airbyte/pull/50094) | Update dependencies |\n| 0.0.7 | 2024-12-14 | [49597](https://github.com/airbytehq/airbyte/pull/49597) | Update dependencies |\n| 0.0.6 | 2024-12-12 | [49225](https://github.com/airbytehq/airbyte/pull/49225) | Update dependencies |\n| 0.0.5 | 2024-12-11 | [48977](https://github.com/airbytehq/airbyte/pull/48977) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.4 | 2024-11-04 | [48158](https://github.com/airbytehq/airbyte/pull/48158) | Update dependencies |\n| 0.0.3 | 2024-10-29 | [47790](https://github.com/airbytehq/airbyte/pull/47790) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47615](https://github.com/airbytehq/airbyte/pull/47615) | Update dependencies |\n| 0.0.1 | 2024-08-31 | [45065](https://github.com/airbytehq/airbyte/pull/45065) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Kvdb Connector\nDESCRIPTION: This command runs the unit tests for the Kvdb connector from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Configuring Nutshell CRM Connector Authentication\nDESCRIPTION: Configuration parameters required for authenticating with the Nutshell CRM API. Requires username and API token credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nutshell.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | API Token.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building Source-Customerly Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet builds the development Docker image for the source-customerly connector using the airbyte-ci command-line tool. The 'build' subcommand is used with the '--name=source-customerly' flag to specify the target connector. Requires airbyte-ci to be installed and accessible in the PATH. The resulting image is tagged as 'source-customerly:dev', which can be used for local testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-customerly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-customerly build\n```\n\n----------------------------------------\n\nTITLE: Building the Quickbooks Connector with airbyte-ci\nDESCRIPTION: Command to build the Quickbooks source connector docker image using airbyte-ci. This creates an image with the tag 'airbyte/source-quickbooks:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-quickbooks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-quickbooks build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Ip2whois Source Connector\nDESCRIPTION: Command to run the full test suite for the Ip2whois source connector using airbyte-ci. This is used to ensure changes pass all tests before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ip2whois/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ip2whois test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Snowflake Cortex Connector Tests\nDESCRIPTION: Commands for running unit tests, integration tests, and the full test suite using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-snowflake-cortex test\npoetry run pytest -s unit_tests\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Displaying Pulsar Destination Features in Markdown\nDESCRIPTION: A markdown table showing the supported features of the Pulsar destination connector, including sync types and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/pulsar.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Supported?\\(Yes/No\\) | Notes |\n| :----------------------------- | :------------------- | :---- |\n| Full Refresh Sync              | No                   |       |\n| Incremental - Append Sync      | Yes                  |       |\n| Incremental - Append + Deduped | No                   |       |\n| Namespaces                     | Yes                  |       |\n```\n\n----------------------------------------\n\nTITLE: Building TiDB Source Connector via Gradle\nDESCRIPTION: Command to build the TiDB source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tidb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-tidb:build\n```\n\n----------------------------------------\n\nTITLE: Running standard source connector commands in Docker\nDESCRIPTION: Commands to run the Everhour source connector in Docker for various operations including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-everhour/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-everhour:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-everhour:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-everhour:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-everhour:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: References to Gradle Build Files and GitHub Workflow\nDESCRIPTION: The document references several code files including 'extract', 'extract-jdbc', 'bulk-cdk-core-base', and a GitHub workflow file for publishing, but does not contain actual code snippets to document.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/bulk/README.md#2025-04-23_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Testing Connector with Airbyte-CI - Bash\nDESCRIPTION: This snippet describes running the acceptance tests for the 'source-clockodo' Airbyte connector via the airbyte-ci tool. Executing the command ensures the connector passes predefined test suites, validating correct behavior before deployment or usage. It requires 'airbyte-ci' to be installed, and the output will detail the results of the test execution for the specified connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clockodo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clockodo test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Zenefits connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenefits/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zenefits test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests Locally with Pytest (Bash)\nDESCRIPTION: This command executes all unit tests for the connector using the pytest test runner within the Poetry environment. It targets the unit_tests directory and assumes that all test dependencies and test files are correctly configured. The command provides pass/fail output for test validation and is limited by the correctness of test setup within the given directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-northpass-lms/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-northpass-lms test\n```\n\n----------------------------------------\n\nTITLE: Displaying Keen Destination Features Table in Markdown\nDESCRIPTION: A markdown table showing the supported features of the Keen destination connector, including full refresh sync, incremental append sync, and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/keen.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Supported?\\(Yes/No\\) | Notes |\n| :----------------------------- | :------------------- | :---- |\n| Full Refresh Sync              | Yes                  |       |\n| Incremental - Append Sync      | Yes                  |       |\n| Incremental - Append + Deduped | No                   |       |\n| Namespaces                     | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Former Handling of Union-Typed Arrays in Airbyte\nDESCRIPTION: Demonstrates the previous behavior for arrays defined with union types (`oneOf`). Airbyte formerly coerced all elements into strings, regardless of whether the input data type matched one of the union types or not.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema (Union: Integer or String)\n{ \"type\": \"array\", \"items\": { \"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Matching)\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Schema\n{ \"type\": \"array\", \"items\": [ \"null\", \"string\" ] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Data\n[\"1\", \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema (Union: Integer or String)\n{ \"type\": \"array\", \"items\": { \"oneOf\": [ {\"type\": \"integer\"}, {\"type\": \"string\"} ] } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data (Mismatched - boolean)\n[1, false]\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Schema\n{ \"type\": \"array\", \"items\": [ \"null\", \"string\" ] }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Old Output Data\n[\"1\", \"false\"]\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations within a Docker container including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-cart:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-cart:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-cart:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-cart:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Set of commands to run the connector locally using Poetry for specification, configuration checking, schema discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-facebook-marketing spec\npoetry run source-facebook-marketing check --config secrets/config.json\npoetry run source-facebook-marketing discover --config secrets/config.json\npoetry run source-facebook-marketing read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Slack Connector Commands Locally\nDESCRIPTION: Series of commands to run the Slack connector locally for various operations including checking specifications, configuration validation, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-slack spec\npoetry run source-slack check --config secrets/config.json\npoetry run source-slack discover --config secrets/config.json\npoetry run source-slack read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding dependencies to Faker connector with Poetry\nDESCRIPTION: Command to add a new dependency to the Faker connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the connector's Docker image using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-ads build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Vitally connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vitally/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vitally test\n```\n\n----------------------------------------\n\nTITLE: Building Elasticsearch Connector Docker Image\nDESCRIPTION: Command to build the connector's Docker image using Gradle. Creates an image tagged as airbyte/destination-elasticsearch:dev\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-elasticsearch/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-elasticsearch:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MSSQL Connector\nDESCRIPTION: Command to build the Docker image for local execution of the MSSQL destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-mssql-v2:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Pinterest Source Connector as a Docker Container (Bash)\nDESCRIPTION: This multi-step snippet shows how to run the built Docker image for the Pinterest connector, invoking different Airbyte subcommands for spec, config checking, schema discovery, and data reading. It utilizes Docker volume mapping to provide local secrets and integration test configuration to the container. Replace $(pwd) with the present working directory. Proper population of /secrets and /integration_tests is required for success; missing files will cause errors or incomplete operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pinterest:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pinterest:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pinterest:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pinterest:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for GreytHR Source Connector\nDESCRIPTION: This command executes the acceptance tests for the GreytHR source connector using airbyte-ci. It's used to validate the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greythr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-greythr test\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing All Airbyte Images to Custom Registry - Bash\nDESCRIPTION: This Bash one-liner fetches all Airbyte image names using abctl images manifest, tags each image to the custom registry (ghcr.io/NAMESPACE), and then pushes the newly tagged images. It uses xargs and docker tag/push commands in a pipeline. Docker CLI and abctl must be installed, and authentication to both the source and destination registries must be configured before execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/custom-image-registries.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nabctl images manifest | xargs -L1 -I{} docker tag {} ghcr.io/NAMESPACE/{} && docker push ghcr.io/NAMESPACE/{}\n```\n\n----------------------------------------\n\nTITLE: Running Vectara Connector Docker Commands\nDESCRIPTION: Commands to run the Vectara connector Docker image for spec, check, and write operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-vectara:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-vectara:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-vectara:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Tempo Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Tempo source connector. The resulting image will be tagged as 'airbyte/source-tempo:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tempo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tempo build\n```\n\n----------------------------------------\n\nTITLE: Testing Source-Front Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-front connector using airbyte-ci. This validates that the connector functions properly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-front/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-front test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Trello Source Connector using airbyte-ci\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Trello source connector. The resulting image will be available on the host with the tag 'airbyte/source-trello:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trello/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-trello build\n```\n\n----------------------------------------\n\nTITLE: Building Papersign Connector Docker Image using Bash\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-papersign` connector. The resulting image will be tagged as `source-papersign:dev` and can be used for local testing purposes. This step requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-papersign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-papersign build\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite for Outreach Connector using airbyte-ci\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to run the full connector test suite locally for the `source-outreach` connector. This helps ensure the connector meets Airbyte's standards before publishing. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outreach/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-outreach test\n```\n\n----------------------------------------\n\nTITLE: Configuring High Availability for Airbyte Core Components\nDESCRIPTION: YAML configuration for enabling high availability in Airbyte by increasing replica counts for worker and server pods. This setup helps minimize downtime by ensuring multiple instances are running.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nworker:\n  replicaCount: 2\n\nserver:\n  replicaCount: 2\n```\n\n----------------------------------------\n\nTITLE: Running NY Times Connector CI Tests using airbyte-ci (Bash)\nDESCRIPTION: This command executes the complete continuous integration test suite locally for the `source-nytimes` connector using the `airbyte-ci` tool. It ensures the connector passes all required checks before contributing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nytimes/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nytimes test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Manifest-only Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash command builds the Docker image for the Dremio source connector using the airbyte-ci tool. The airbyte-ci CLI must be installed in advance. The --name parameter specifies the connector to build. The output is a Docker image tagged as airbyte/source-dremio:dev on the local host. Requires airbyte-ci and Docker to be available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dremio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dremio build\n```\n\n----------------------------------------\n\nTITLE: Building Kissmetrics Source Connector with Airbyte CI\nDESCRIPTION: This command builds a development image of the Kissmetrics source connector using airbyte-ci. The resulting image is tagged as 'source-kissmetrics:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kissmetrics/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kissmetrics build\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record Message Format Example\nDESCRIPTION: Example of an Airbyte RECORD message format containing stream data. Shows the standard structure with type, record metadata, and data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/namespace_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"data_stream\", \"emitted_at\": 1602637589000, \"data\": { \"field1\" : true }}}\n```\n\n----------------------------------------\n\nTITLE: Building Rocket Chat Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Rocket Chat source connector docker image using the airbyte-ci tool. This creates an image tagged as 'airbyte/source-rocket-chat:dev' on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rocket-chat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rocket-chat build\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of TrackHS API Streams\nDESCRIPTION: Tabular documentation of TrackHS API streams showing stream names, primary keys, pagination details, sync capabilities and API documentation links.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/track-pms.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental | API Docs |\n|-------------|-------------|------------|---------------------|----------------------|----------------------|\n| accounting_accounts | id | DefaultPaginator | ✅ |  ❌  | [Link](https://developer.trackhs.com/reference/getledgeraccounts) |\n| accounting_bills | id | DefaultPaginator | ✅ |  ❌  | [Link](https://developer.trackhs.com/reference/getbillscollection) |\n```\n\n----------------------------------------\n\nTITLE: Running Iceberg V2 Connector Tests with airbyte-ci CLI\nDESCRIPTION: Command to run both unit and acceptance tests for the Iceberg V2 destination connector using the Airbyte CI CLI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=destination-iceberg-v2 test\n```\n\n----------------------------------------\n\nTITLE: Using xmlattr Filter in Jinja2\nDESCRIPTION: Demonstrates the `xmlattr` filter in Jinja2, which creates a string of XML/HTML attributes from a dictionary. Keys become attribute names, and values become attribute values (properly escaped). The example creates attributes from a dictionary.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_56\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ {'class': 'my-class', 'id': 'my-id'}|xmlattr }}\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry Dependencies\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the GCS source connector using Poetry\nDESCRIPTION: This command installs the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Source Mention Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the source-mention connector locally. Creates a dev image tagged as source-mention:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mention/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mention build\n```\n\n----------------------------------------\n\nTITLE: Financial Events Step Size Options - Markdown List\nDESCRIPTION: List of available time window size options for fetching financial events data from Amazon Seller Partner API\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-seller-partner.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- 1\n- 7\n- 14\n- 30\n- 60\n- 90\n- 180 (default)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Zenefits connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-zenefits:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenefits/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zenefits build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using the Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-marketo build\n```\n\n----------------------------------------\n\nTITLE: Algolia Available Streams Configuration\nDESCRIPTION: Table describing the available data streams in the Algolia connector, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/algolia.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| indices | name | DefaultPaginator | ✅ |  ✅  |\n| indexes_query |  | No pagination | ✅ |  ❌  |\n| available_languages |  | No pagination | ✅ |  ❌  |\n| logs | sha1 | No pagination | ✅ |  ✅  |\n| indexes_settings |  | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building Wasabi Stats API Source Connector in Bash\nDESCRIPTION: This command builds a development image of the Wasabi stats API source connector using airbyte-ci. The resulting image is tagged as 'source-wasabi-stats-api:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wasabi-stats-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wasabi-stats-api build\n```\n\n----------------------------------------\n\nTITLE: Running Fullstory connector commands in Docker\nDESCRIPTION: Commands to run the Fullstory source connector within a Docker container, including operations for specification, configuration checking, discovering available streams, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fullstory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-fullstory:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fullstory:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fullstory:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-fullstory:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: CLI Commands for Listing Migrated Connectors\nDESCRIPTION: Bash commands using airbyte-ci to list both migrated and non-migrated certified connectors based on metadata queries.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/migration-to-base-image.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --support-level=certified --metadata-query=\"data.connectorBuildOptions.baseImage is not None\" list\n```\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --metadata-query=\"data.supportLevel == 'certified' and 'connectorBuildOptions' not in data.keys()\" list\n```\n\n----------------------------------------\n\nTITLE: Former Handling of Typed Arrays with Mismatched Data in Airbyte\nDESCRIPTION: Shows the previous behavior when input data did not conform to the specified array item type (e.g., a string \"Alice\" in an integer array). This mismatch formerly caused the synchronization to fail.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"array\", \"items\": { \"type\": \"integer\" } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n[1, \"Alice\"]\n```\n\nLANGUAGE: text\nCODE:\n```\n// Old Output Data\n[SYNC FAILED]\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Source-Nylas Connector with airbyte-ci - Bash\nDESCRIPTION: This snippet runs the acceptance test suite for the source-nylas connector using the airbyte-ci tool. It invokes the connectors subcommand to specify which connector should be tested. Requires a configured development environment with airbyte-ci installed; outputs test results based on the current state of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nylas/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nylas test\n```\n\n----------------------------------------\n\nTITLE: Building Snowflake Cortex Connector with airbyte-ci\nDESCRIPTION: Uses the airbyte-ci tool to build the connector Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-snowflake-cortex build\n```\n\n----------------------------------------\n\nTITLE: Running the Full CI Test Suite with airbyte-ci (Bash)\nDESCRIPTION: Executes the complete Airbyte test suite (including integration and acceptance tests) for the `source-declarative-manifest` connector locally using the `airbyte-ci` tool. This command simulates the tests run in the CI environment. Requires `airbyte-ci` to be installed and Docker to be running.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-declarative-manifest test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-sharepoint test\n```\n\n----------------------------------------\n\nTITLE: Customizing Duckdb Connector Build Process\nDESCRIPTION: Example of a Python module to customize the build process for the Duckdb connector, allowing for environment variable setting and image modification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from dagger import Container\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Pulling, Tagging, and Pushing Connector Image to Custom Registry\nDESCRIPTION: Bash commands to pull an Airbyte connector image from Docker, tag it, and push it to a custom GitHub registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull airbyte/destination-google-sheets:latest\ndocker tag airbyte/desination-google-sheets:latest ghcr.io/NAMESPACE/desination-google-sheets:latest\ndocker push ghcr.io/NAMESPACE/destination-google-sheets:latest\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table for Amazon Advertising API\nDESCRIPTION: Mapping table showing the correspondence between integration data types and Airbyte data types used in the connector implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type         | Airbyte Type |\n|:-------------------------|:-------------|\n| `string`                 | `string`     |\n| `int`, `float`, `number` | `number`     |\n| `date`                   | `date`       |\n| `datetime`               | `datetime`   |\n| `array`                  | `array`      |\n| `object`                 | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Building the Avni Source Connector Using Gradle - Shell\nDESCRIPTION: This shell command compiles and builds the Avni source connector within the Airbyte repository using Gradle. Requires Gradle to be installed and assumes the current directory is the root of the Airbyte repository. On successful execution, it produces build artifacts for the connector, necessary for further development or deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-avni:build\n```\n\n----------------------------------------\n\nTITLE: Running CI test suite for Recharge connector\nDESCRIPTION: Command to run the full test suite for the Recharge connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recharge test\n```\n\n----------------------------------------\n\nTITLE: Displaying Data Type Mapping for Hellobaton API in Markdown\nDESCRIPTION: This code snippet shows a markdown table that maps Hellobaton API data types to Airbyte data types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hellobaton.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Testing VWO Source Connector with airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the VWO source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vwo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vwo test\n```\n\n----------------------------------------\n\nTITLE: Note about data reset prompting in Airbyte\nDESCRIPTION: A note explaining that depending on the destination type, users may not be prompted to reset their data during the migration process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-ads-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nDepending on destination type you may not be prompted to reset your data.\n```\n```\n\n----------------------------------------\n\nTITLE: Illustrating Escape Character Usage in CSV Data (Text)\nDESCRIPTION: Provides an example of CSV data where a backslash (`\\`) is used as an escape character before a double quote (`\"`) within a quoted field. This demonstrates how the 'Escape Character' setting in the Airbyte File connector allows correct parsing of literal quote characters embedded in CSV values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-onedrive.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Running Local Connector Commands\nDESCRIPTION: Commands for running different connector operations locally including spec, check, discover and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-youtube-analytics spec\npoetry run source-youtube-analytics check --config secrets/config.json\npoetry run source-youtube-analytics discover --config secrets/config.json\npoetry run source-youtube-analytics read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with Airbyte-CI - Bash\nDESCRIPTION: Runs the complete test suite for the source-file connector using airbyte-ci, which may include unit, integration, and acceptance tests depending on the Airbyte CI pipeline. Requires airbyte-ci to be installed and run from the connector's root directory. Outputs aggregated test results.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-file test\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing the Firebase-Realtime-Database Connector using Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Example Self-Hosted Couchbase Connection String\nDESCRIPTION: An example connection string format for connecting Airbyte to a self-hosted Couchbase server running on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/couchbase.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ncouchbase://localhost\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Twitter Source Connector\nDESCRIPTION: Executes the full test suite for the Twitter source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twitter/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twitter test\n```\n\n----------------------------------------\n\nTITLE: Building Qdrant Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Qdrant connector using the airbyte-ci tool. This is the recommended method for building the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-qdrant build\n```\n\n----------------------------------------\n\nTITLE: Presenting Doris Destination Changelog in Markdown Table\nDESCRIPTION: A markdown table displaying the changelog for the Doris destination connector, showing version, date, pull request, and subject information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/doris.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject        |\n| :------ | :--------- | :------------------------------------------------------- | :------------- |\n| 0.1.0   | 2022-11-14 | [17884](https://github.com/airbytehq/airbyte/pull/17884) | Initial Commit |\n```\n\n----------------------------------------\n\nTITLE: Running standard source connector commands for Sentry\nDESCRIPTION: A set of Docker commands to run the standard operations for the Sentry source connector: spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sentry/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sentry:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sentry:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sentry:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sentry:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Nexiopay Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Nexiopay source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nexiopay/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nexiopay test\n```\n\n----------------------------------------\n\nTITLE: Literal Value Instantiation Example\nDESCRIPTION: Demonstrates how literal values are handled by returning them as-is without modification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/object-instantiation.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n3\n```\n\nLANGUAGE: python\nCODE:\n```\n3\n```\n\n----------------------------------------\n\nTITLE: Running Facebook Pages Connector Docker Commands\nDESCRIPTION: Various Docker commands to run the Facebook Pages connector with different operations like spec, check, discover, and read, mounting necessary configuration files and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-facebook-pages:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-facebook-pages:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-facebook-pages:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-facebook-pages:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running K6 Cloud Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various operations for the K6 Cloud source connector using Docker. They include specifying the connector, checking the configuration, discovering available data, and reading data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-k6-cloud/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-k6-cloud:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-k6-cloud:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-k6-cloud:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-k6-cloud:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Giphy Source Connector in Bash\nDESCRIPTION: Command to build the Giphy source connector locally using airbyte-ci. This creates a development image named 'source-giphy:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-giphy/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-giphy build\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Table defining the configuration parameters required for the Navan connector including client_id, client_secret, and start_date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/navan.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id` | `string` | OAuth Client ID.  |  |\n| `client_secret` | `string` | OAuth Client Secret.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Testing Source-Customerly Connector with airbyte-ci Acceptance Tests - Bash\nDESCRIPTION: This Bash snippet runs acceptance tests for the source-customerly connector using the airbyte-ci command-line tool. The 'test' subcommand, combined with the '--name=source-customerly' flag, targets the specific connector for testing. It assumes airbyte-ci is installed and configured, and runs the predefined test suite to validate the connector's behavior against Airbyte's requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-customerly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-customerly test\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - bash\nDESCRIPTION: This snippet demonstrates how to install all Python dependencies for the Commercetools source connector project using Poetry, including development dependencies. Poetry (~=1.7) and Python (~=3.9) must be installed beforehand. Upon execution from the connector directory, all dependencies are resolved and prepared for development, ensuring project reproducibility and environment isolation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Source-Nutshell Connector via airbyte-ci in Bash\nDESCRIPTION: This Bash command builds the source-nutshell connector locally by invoking the airbyte-ci tool with the build command. It creates a development Docker image tagged as source-nutshell:dev, which can be used for local testing of the connector. Requires airbyte-ci to be installed and available in the PATH.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nutshell/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nutshell build\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite Locally using airbyte-ci (Bash)\nDESCRIPTION: This command executes the complete Continuous Integration (CI) test suite for the Google Search Console source connector locally using the `airbyte-ci` tool. This helps ensure the connector meets quality standards before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-search-console test\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Commands Locally\nDESCRIPTION: Commands to run the Firebolt source connector's various operations locally, including spec generation, connection checking, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Stream Data for Sheet6-2000-rows\nDESCRIPTION: This JSON object represents a single row of data from the 'Sheet6-2000-rows' stream. It includes an ID, a randomly generated Name, and an emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_16\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1835\",\"Name\":\"weuhSDkfT\"},\"emitted_at\":1673989570000}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests\nDESCRIPTION: Command to execute the test suite using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Testing Taboola Source Connector in Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Taboola source connector using airbyte-ci. It ensures that the connector meets the required functionality and performance standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-taboola/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-taboola test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Google Sheets Connector\nDESCRIPTION: Command to build a Docker image for the Google Sheets connector using Airbyte's CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-google-sheets build\n```\n\n----------------------------------------\n\nTITLE: Handling Conflict Stream with Array Values in JSON\nDESCRIPTION: These JSON records demonstrate handling of array values in conflict streams. The 'conflict_stream_array' field contains nested arrays of objects with 'id' fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_array\",\"data\":{\"id\":1, \"conflict_stream_array\": {\"conflict_stream_array\": [{\"id\": 1}, {\"id\": 2}, {\"id\": 3}]}}, \"emitted_at\":1623861660}}\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_array\",\"data\":{\"id\":2, \"conflict_stream_array\": {\"conflict_stream_array\": [{\"id\": 4}, {\"id\": 5}, {\"id\": 6}]}}, \"emitted_at\":1623861860}}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Convex Connector\nDESCRIPTION: Command to build a Docker image for the Convex connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-convex build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the N8n connector in a Docker container. Includes commands for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-n8n/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-n8n:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-n8n:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-n8n:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-n8n:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Wufoo Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Wufoo connector using airbyte-ci. Creates a dev image tagged as source-wufoo:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wufoo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wufoo build\n```\n\n----------------------------------------\n\nTITLE: Teamwork Source Connector Streams Configuration\nDESCRIPTION: This table lists all available data streams for the Teamwork source connector, including their primary keys, pagination methods, and support for full and incremental syncs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/teamwork.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| latestactivity | id | DefaultPaginator | ✅ |  ✅  |\n| projects | id | DefaultPaginator | ✅ |  ✅  |\n| projects_active | value | DefaultPaginator | ✅ |  ❌  |\n| projects_billable | name | DefaultPaginator | ✅ |  ❌  |\n| companies | id | DefaultPaginator | ✅ |  ✅  |\n| me_timers | id | DefaultPaginator | ✅ |  ✅  |\n| time_entries | id | DefaultPaginator | ✅ |  ✅  |\n| timelog_totals |  | DefaultPaginator | ✅ |  ❌  |\n| dashboards | id | DefaultPaginator | ✅ |  ✅  |\n| forms | id | DefaultPaginator | ✅ |  ✅  |\n| milestones | id | DefaultPaginator | ✅ |  ✅  |\n| milestones_deadlines |  | DefaultPaginator | ✅ |  ❌  |\n| notebooks | id | DefaultPaginator | ✅ |  ✅  |\n| people | id | DefaultPaginator | ✅ |  ❌  |\n| notebooks_comments.json | id | DefaultPaginator | ✅ |  ✅  |\n| projectcategories | id | DefaultPaginator | ✅ |  ❌  |\n| tasklists | id | DefaultPaginator | ✅ |  ✅  |\n| tasks | id | DefaultPaginator | ✅ |  ✅  |\n| tags | id | DefaultPaginator | ✅ |  ❌  |\n| timesheets |  | DefaultPaginator | ✅ |  ❌  |\n| workload_planners |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Building Akeneo Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Akeneo connector using airbyte-ci tool. Creates a dev image tagged as 'source-akeneo:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-akeneo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-akeneo build\n```\n\n----------------------------------------\n\nTITLE: NASA Connector Version History\nDESCRIPTION: Detailed changelog table showing version history, dates, pull requests, and changes made to the NASA connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nasa.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                    |\n| :------ | :--------- | :------------------------------------------------------- | :----------------------------------------- |\n| 0.3.22 | 2025-04-19 | [58504](https://github.com/airbytehq/airbyte/pull/58504) | Update dependencies |\n| 0.3.21 | 2025-04-12 | [57921](https://github.com/airbytehq/airbyte/pull/57921) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Testing the Airbyte Insightful Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet shows how to execute acceptance tests for the 'source-insightful' Airbyte connector using the airbyte-ci tool. It requires airbyte-ci to be installed and specifies the connector name with '--name=source-insightful'. The 'test' command triggers the full test suite to verify the connector's compliance and functionality. Input and output will depend on your test environment configuration, and successful execution is necessary to validate any changes prior to deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-insightful/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-insightful test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Sendgrid Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Sendgrid source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendgrid/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendgrid test\n```\n\n----------------------------------------\n\nTITLE: Building Buildkite Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Buildkite source connector using airbyte-ci. Creates a dev image tagged as source-buildkite:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-buildkite/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-buildkite build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Captain Data source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-captain-data/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-captain-data test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the My Hours connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-my-hours/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-my-hours test\n```\n\n----------------------------------------\n\nTITLE: JSON State Message\nDESCRIPTION: Global state message containing shared state information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_messages_in.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Zendesk Chat connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-chat/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-chat test\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command executes the complete CI test suite for the `source-configcat` connector locally using the `airbyte-ci` tool. It requires `airbyte-ci` to be installed and typically depends on a working Docker environment for test execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-configcat/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-configcat test\n```\n\n----------------------------------------\n\nTITLE: Running Amazon SQS Connector Commands Locally\nDESCRIPTION: Commands to run various connector operations locally using Poetry. These include spec, check, discover, and read operations with configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run destination-amazon-sqs spec\npoetry run destination-amazon-sqs check --config secrets/config.json\npoetry run destination-amazon-sqs discover --config secrets/config.json\npoetry run destination-amazon-sqs read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building NASA Connector with Airbyte-CI\nDESCRIPTION: Command to build the NASA source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nasa/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nasa build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for SpaceX API Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the SpaceX API source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-spacex-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-spacex-api test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Apify dataset connector in a Docker container. Includes commands for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apify-dataset/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-apify-dataset:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-apify-dataset:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-apify-dataset:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-apify-dataset:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Eventbrite Source Connector with airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the Eventbrite connector using the airbyte-ci tool to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventbrite/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventbrite test\n```\n\n----------------------------------------\n\nTITLE: Installing Docs Dependencies - pnpm - Bash\nDESCRIPTION: This snippet installs project dependencies required for building Airbyte documentation locally. It assumes you have pnpm installed and are inside the 'docusaurus' directory. The command downloads all necessary node modules as defined in the project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd docusaurus\\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Apple Search Ads connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apple-search-ads/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-apple-search-ads:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-apple-search-ads:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-apple-search-ads:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-apple-search-ads:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running SQLite Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-sqlite spec\npoetry run destination-sqlite check --config secrets/config.json\npoetry run destination-sqlite discover --config secrets/config.json\npoetry run destination-sqlite read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally with Poetry - bash\nDESCRIPTION: Executes the Github source connector's specification, configuration validation, discovery, and sync reading commands locally, utilizing Poetry's runtime environment. These commands require proper setup, including credentials in 'secrets/config.json', and optionally a configured catalog. Output varies based on the subcommand ('spec', 'check', 'discover', 'read'). These commands should be run individually from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-github spec\npoetry run source-github check --config secrets/config.json\npoetry run source-github discover --config secrets/config.json\npoetry run source-github read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Tiktok-Marketing Connector\nDESCRIPTION: This command runs the full CI test suite for the Tiktok-Marketing connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tiktok-marketing test\n```\n\n----------------------------------------\n\nTITLE: Building Vercel Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Vercel source connector using airbyte-ci. The resulting image (source-vercel:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vercel/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vercel build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci - bash\nDESCRIPTION: This Bash command leverages the 'airbyte-ci' tool to build a Docker image for the Commercetools source connector. It assumes 'airbyte-ci' is installed on the system and will generate a local image with the tag 'airbyte/source-commercetools:dev'. The resulting image can be run as a Docker container for further integration testing or deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-commercetools build\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table\nDESCRIPTION: Mapping table showing the correspondence between Zapier integration types and Airbyte data types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zapier-supported-storage.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n| `boolean`        | `boolean`    |       |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Greenhouse Connector\nDESCRIPTION: This command builds a Docker image for the Greenhouse connector using airbyte-ci, tagging it as 'airbyte/source-greenhouse:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-greenhouse build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Jotform Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Jotform source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jotform/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jotform test\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Invoiced Connector with Airbyte CI\nDESCRIPTION: This command builds a development image of the Invoiced connector using airbyte-ci. The resulting image is tagged as 'source-invoiced:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-invoiced/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-invoiced build\n```\n\n----------------------------------------\n\nTITLE: Building Katana Source Connector with airbyte-ci\nDESCRIPTION: This command builds a dev image of the Katana source connector for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-katana/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-katana build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Klaus API Source Connector\nDESCRIPTION: Command to run the full test suite for the Klaus API source connector using airbyte-ci. This is used to validate changes before contributing or publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaus-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klaus-api test\n```\n\n----------------------------------------\n\nTITLE: Testing the Google Calendar Connector Locally using airbyte-ci (Bash)\nDESCRIPTION: This command runs the acceptance tests defined for the `source-google-calendar` connector using the `airbyte-ci` tool. It helps verify the connector's functionality during local development. Requires `airbyte-ci` to be installed and potentially the connector to be built first.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-calendar/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-calendar test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Manifest-only Airbyte Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet builds the Docker image for the manifest-only Airbyte source-pexels-api connector using the airbyte-ci tooling. It requires airbyte-ci to be installed and available in your shell environment. The main parameter is the connector name specified by --name. The output is a Docker image tagged as airbyte/source-pexels-api:dev, available on the local Docker host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pexels-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pexels-api build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Workable connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workable/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workable test\n```\n\n----------------------------------------\n\nTITLE: Example API Request with Offset Increment (HTTP)\nDESCRIPTION: Shows an example HTTP GET request to an API endpoint demonstrating the use of 'limit' and 'offset' query parameters for Offset Increment pagination. This request asks for 2 records starting from offset 3.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.example.com/products?limit=2&offset=3\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Surveymonkey Connector\nDESCRIPTION: Command to run the full test suite for the Surveymonkey connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-surveymonkey test\n```\n\n----------------------------------------\n\nTITLE: Using dateformat Filter in Jinja2\nDESCRIPTION: Demonstrates the `dateformat` filter in Jinja2 (requires Babel library), which formats a date object according to a specific format string (using strftime codes). The example assumes a variable `date` holds a date object and formats it as 'YYYY-MM-DD'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_57\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ date|dateformat('%Y-%m-%d') }}\n```\n\n----------------------------------------\n\nTITLE: Example Google Drive Folder Structure for Path Pattern Matching (Text)\nDESCRIPTION: Illustrates a sample directory structure within a Google Drive folder (`MyFolder`) to demonstrate how path patterns function in the Airbyte Google Drive connector. This example hierarchy includes nested folders and CSV files, serving as a reference for crafting patterns to include or exclude specific files during synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-drive.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nMyFolder\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Building Redis Destination Connector with Gradle\nDESCRIPTION: Command to build the Redis destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redis/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-redis:build\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-Only Connector with airbyte-ci (Bash)\nDESCRIPTION: Uses the airbyte-ci command line interface to build the manifest-only Clockify source connector image. Requires airbyte-ci to be installed; see Airbyte documentation for prerequisites. The flag --name specifies the connector (source-clockify), and the build subcommand builds the Docker image tagged as airbyte/source-clockify:dev. Input: None. Output: Docker image locally tagged and available for further use. Assumes valid manifest files are present and required dependencies are installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clockify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clockify build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-firestore test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Genesys Connector in Python\nDESCRIPTION: Command to run unit tests for the Genesys connector using pytest through Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Converting Empty String Fields to Null - Airbyte Mailchimp Migration - Markdown\nDESCRIPTION: This Markdown snippet visually illustrates the transformation in JSON format where empty string values (e.g., \\\"last_opened\\\": \\\"\\\") are replaced by null (\\\"last_opened\\\": null) during the migration to a newer schema in the Airbyte Mailchimp connector. This transformation is essential for ensuring data consistency and correct type handling across destinations after upgrading. No specific dependencies are required to understand the snippet, but the depicted logic must be implemented during connector processing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mailchimp-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n```md\\n{\"id\": \"record_id\", \"last_opened\": \"\"} -> {\"id\": \"record_id\", \"last_opened\": null}\\n```\n```\n\n----------------------------------------\n\nTITLE: Building the Chargify Source Connector Docker Image using airbyte-ci\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-chargify` connector. It requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-chargify:dev` on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargify build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-sharepoint/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Hardcoded Records Connector in Docker\nDESCRIPTION: Series of Docker commands to run various operations of the Hardcoded Records connector, including spec retrieval, configuration check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-hardcoded-records:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hardcoded-records:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hardcoded-records:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-hardcoded-records:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Fleetio Connector Docker Commands\nDESCRIPTION: Standard commands for running the Fleetio source connector Docker container in different modes including spec, check, discover, and read. These commands demonstrate how to mount local directories for configuration and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fleetio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-fleetio:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fleetio:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-fleetio:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-fleetio:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Usage for `airbyte-ci connectors up-to-date` Command (Shell)\nDESCRIPTION: Shell output showing the usage instructions and available options for the `airbyte-ci connectors up-to-date` command. This command is used to update connector dependencies, potentially bump versions, update changelogs, and create pull requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nUsage: airbyte-ci connectors up-to-date [OPTIONS]\n\nOptions:\n  --no-bump    Don't bump the version or changelog.\n  --dep TEXT  Give a specific set of `poetry add` dependencies to update. For\n              example: --dep airbyte-cdk==0.80.0 --dep pytest@^6.2\n  --open-reports    Auto open reports in the browser.\n  --create-prs      Create pull requests for each updated connector.\n  --auto-merge    Set the auto-merge label on created PRs.\n  --help      Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Rendering Source Connectors Registry Component in JSX\nDESCRIPTION: Uses the ConnectorRegistry component to render a list of all source connectors available in Airbyte. The 'type' prop is set to 'source' to specifically display connectors that pull data from external systems.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/README.md#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<ConnectorRegistry type=\"source\"/>\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Firebase-Realtime-Database Connector\nDESCRIPTION: Command to execute the full CI test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firebase-realtime-database test\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Facebook Pages Connector\nDESCRIPTION: Command to run the full test suite for the Facebook Pages connector using the airbyte-ci tool, which executes various tests to ensure connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-facebook-pages test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Zendesk Sunshine source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sunshine/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-sunshine build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-cumulio test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the N8n connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-n8n/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-n8n test\n```\n\n----------------------------------------\n\nTITLE: JSON Record with All Null Values\nDESCRIPTION: A third JSON record where all fields are set to null, demonstrating how the structure handles null values across all fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_disjoint_union_messages_out.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"schemaless_object\":null,\"schematized_object\":null,\"combined_type\":null,\"union_type\":null,\"schemaless_array\":null,\"mixed_array_integer_and_schemaless_object\":null,\"array_of_union_integer_and_schemaless_array\":null,\"union_of_objects_with_properties_identical\":null,\"union_of_objects_with_properties_overlapping\":null,\"union_of_objects_with_properties_nonoverlapping\":null,\"union_of_objects_with_properties_contradicting\":null,\"empty_object\":null,\"object_with_null_properties\":null,\"combined_with_null\":null,\"union_with_null\":null,\"combined_nulls\":null,\"compact_union\":null}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-looker test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Ashby source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ashby/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ashby test\n```\n\n----------------------------------------\n\nTITLE: Running Glassflow Connector Tests\nDESCRIPTION: Commands to run unit and integration tests for the Glassflow connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit-tests\npoetry run pytest integration-tests\n```\n\n----------------------------------------\n\nTITLE: Building the Sharetribe Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Sharetribe source connector. This creates a dev image (source-sharetribe:dev) that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sharetribe/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sharetribe build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the WorkRamp source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workramp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workramp build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands for running the connector operations within a Docker container with mounted volumes for secrets and tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-asana:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-asana:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-asana:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-asana:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Aviationstack Connector using Bash\nDESCRIPTION: This command executes the acceptance tests for the `source-aviationstack` connector using the `airbyte-ci` tool. It is intended to be run after building the connector image locally and requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aviationstack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aviationstack test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci - Bash\nDESCRIPTION: Uses Airbyte’s airbyte-ci tool to build a Docker image for the Zendesk-Support source connector. This process bundles code, dependencies, and Entrypoint for deployment or publication. Requires airbyte-ci to be installed and run from the correct directory. The resulting image is tagged as airbyte/source-zendesk-support:dev and available locally for running or further use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-support build\n```\n\n----------------------------------------\n\nTITLE: Inspecting Docker Volume Mountpoint on Linux\nDESCRIPTION: Shows the `docker volume inspect` command used on Linux systems to get detailed information about a named Docker volume. The `Mountpoint` field in the output reveals the directory on the host filesystem where the volume's data is stored, allowing direct access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_14\n\nLANGUAGE: text\nCODE:\n```\ndocker volume inspect <volume_name>\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full Freshcaller connector test suite using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshcaller test\n```\n\n----------------------------------------\n\nTITLE: Running Hardcoded Records Connector Commands\nDESCRIPTION: Series of commands to run various operations of the Hardcoded Records connector using Poetry, including spec retrieval, configuration check, schema discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-hardcoded-records spec\npoetry run source-hardcoded-records check --config secrets/config.json\npoetry run source-hardcoded-records discover --config secrets/config.json\npoetry run source-hardcoded-records read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Jina AI Reader Connector\nDESCRIPTION: Command to run the test suite for the Jina AI Reader connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Building SingleStore Connector via Gradle in Java\nDESCRIPTION: Command to build the SingleStore destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-singlestore/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-singlestore:build\n```\n\n----------------------------------------\n\nTITLE: Migrating Connector to Manifest-Only Format by Name\nDESCRIPTION: Example of how to migrate a specific connector (source-pokeapi) to the manifest-only format using the Airbyte CI connectors command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pokeapi migrate-to-manifest-only\n```\n\n----------------------------------------\n\nTITLE: Building Ubidots Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Ubidots source connector using airbyte-ci. The resulting image (source-ubidots:dev) can be used for local testing of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ubidots/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ubidots build\n```\n\n----------------------------------------\n\nTITLE: Building SparkPost Connector for Airbyte (Bash)\nDESCRIPTION: This command builds a development image of the SparkPost connector using airbyte-ci. The resulting image is tagged as 'source-sparkpost:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sparkpost/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sparkpost build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest through Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Harness Source Connector\nDESCRIPTION: Command to run the full test suite for the Harness source connector using airbyte-ci tool. This executes all tests associated with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harness/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-harness test\n```\n\n----------------------------------------\n\nTITLE: Running Deepset Connector Commands Locally\nDESCRIPTION: Commands to run the Deepset connector locally for specification, configuration check, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-deepset spec\npoetry run destination-deepset check --config secrets/config.json\npoetry run destination-deepset write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Braze source connector using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braze/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-braze build\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table\nDESCRIPTION: Lists the supported features of the Appstore connector including sync types and namespaces.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appstore.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | no                   |       |\n| Incremental Sync  | yes                  |       |\n| Namespaces        | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Testing the Shippo Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Shippo source connector using airbyte-ci. This validates that the connector functions properly according to Airbyte's specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shippo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shippo test\n```\n\n----------------------------------------\n\nTITLE: Building the Invoice Ninja Source Connector in Airbyte\nDESCRIPTION: This command uses airbyte-ci to build a development image of the Invoice Ninja source connector. The resulting image is tagged as 'source-invoiceninja:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-invoiceninja/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-invoiceninja build\n```\n\n----------------------------------------\n\nTITLE: Running the Check Operation for Zendesk Talk Connector (Bash)\nDESCRIPTION: Executes the `check` command within a temporary Docker container using the `airbyte/source-zendesk-talk:dev` image. It validates the connection settings provided in the configuration file. Requires Docker, the built connector image, and a valid `config.json` file located in the `secrets` directory mounted into the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-talk:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building the Fullstory connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Fullstory source connector using the airbyte-ci tool, which creates a Docker image tagged as airbyte/source-fullstory:dev on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fullstory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fullstory build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Unleash Source Connector\nDESCRIPTION: Command to run the full test suite for the Unleash source connector using airbyte-ci. This helps ensure that changes made to the connector pass all tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-unleash/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-unleash test\n```\n\n----------------------------------------\n\nTITLE: Using replace Filter in Jinja2\nDESCRIPTION: Demonstrates the `replace` filter in Jinja2, which replaces occurrences of a substring within a string with another substring. The example replaces all 'l's with 'x's in 'hello'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_39\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello'|replace('l', 'x') }}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source-Pivotal-Tracker Connector Commands in Docker (bash)\nDESCRIPTION: These snippets show how to execute core Airbyte source connector commands (spec, check, discover, read) for the source-pivotal-tracker connector as Docker containers. Each command mounts required volumes for configuration and test data, and executes the appropriate subcommand. Required prerequisites include a built Docker image, existing configuration in '/secrets/config.json' (and optionally a catalog file). Inputs include credentials and configuration files, while outputs are the connector's CLI results. These commands are intended for developers running connector operations locally or in CI contexts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pivotal-tracker/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pivotal-tracker:dev spec\n\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pivotal-tracker:dev check --config /secrets/config.json\n\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pivotal-tracker:dev discover --config /secrets/config.json\n\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pivotal-tracker:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n\n```\n\n----------------------------------------\n\nTITLE: Building MySQL Connector Docker Image\nDESCRIPTION: Command to build the connector Docker image using Gradle, resulting in an image tagged as airbyte/source-mysql:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-mysql:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Whisky Hunter Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Whisky Hunter source connector. The resulting image will be available on the host with the tag 'airbyte/source-whisky-hunter:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-whisky-hunter/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-whisky-hunter build\n```\n\n----------------------------------------\n\nTITLE: Customizing Pinecone Connector Build Process\nDESCRIPTION: Example of a Python module to customize the build process for the Pinecone connector, including pre and post-installation steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Glassflow Connector Commands Locally\nDESCRIPTION: Series of commands to run the Glassflow connector locally for specification, configuration check, and data writing operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-glassflow spec\npoetry run destination-glassflow check --config secrets/config.json\npoetry run destination-glassflow write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Example\nDESCRIPTION: Example showing how Marketo data types map to Airbyte data types, presented in markdown table format\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/marketo.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes                                                                           |\n| :--------------- | :----------- | :------------------------------------------------------------------------------ |\n| `array`          | `array`      | primitive arrays are converted into arrays of the types described in this table |\n| `int`, `long`    | `number`     |                                                                                 |\n| `object`         | `object`     |                                                                                 |\n| `string`         | `string`     | \\`\\`                                                                            |\n| Namespaces       | No           |                                                                                 |\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-sqs test\n```\n\n----------------------------------------\n\nTITLE: Testing the Freightview Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-freightview connector using airbyte-ci. This validates that the connector meets Airbyte's requirements and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freightview/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freightview test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci\nDESCRIPTION: Command to build the connector Docker image using the recommended airbyte-ci tool, which creates an image tagged as airbyte/source-python-http-tutorial:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-python-http-tutorial build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Smartsheets Connector\nDESCRIPTION: Command to run the full test suite for the Smartsheets connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartsheets test\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte CI Test Command with Modified Packages\nDESCRIPTION: Example of how to run Poe tasks on modified internal packages in the current branch using the Airbyte CI test command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci test --modified\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Records from Sheet6-2000-rows in Airbyte\nDESCRIPTION: This JSON data represents streamed records from Airbyte's 'Sheet6-2000-rows' stream. Each record contains a numerical ID, a Name field with a 9-character alphanumeric string, and an emitted_at timestamp (1673989570000) which represents when the record was processed by Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1629\",\"Name\":\"scOjWyzyD\"},\"emitted_at\":1673989570000}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Apple Search Ads connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apple-search-ads/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apple-search-ads build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for StockData Connector\nDESCRIPTION: This command executes the acceptance tests for the StockData connector using airbyte-ci. It helps ensure the connector functions correctly before deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stockdata/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-stockdata test\n```\n\n----------------------------------------\n\nTITLE: Building SingleStore Connector via Gradle\nDESCRIPTION: Command to build the SingleStore connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-singlestore/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-singlestore:build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Iterable Connector\nDESCRIPTION: Executes unit tests for the Iterable connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Getlago Docker Image with airbyte-ci in Bash\nDESCRIPTION: Command to build the Getlago source connector docker image using the airbyte-ci tool. This builds the image with the tag 'airbyte/source-getlago:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-getlago/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-getlago build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's docker image using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-onedrive build\n```\n\n----------------------------------------\n\nTITLE: Testing the SignNow Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the SignNow connector using airbyte-ci. This validates that the connector meets Airbyte's requirements and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-signnow/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-signnow test\n```\n\n----------------------------------------\n\nTITLE: Running DynamoDB Connector Docker Commands\nDESCRIPTION: Various Docker commands for running the connector's spec, check, discover, and read operations with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dynamodb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-dynamodb:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-dynamodb:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-dynamodb:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-dynamodb:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Buzzsprout Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Buzzsprout source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-buzzsprout/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-buzzsprout test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Vitally source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vitally/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vitally build\n```\n\n----------------------------------------\n\nTITLE: Building ShopWired Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the ShopWired source connector using airbyte-ci. The resulting image (source-shopwired:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopwired/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shopwired build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Flexport Connector\nDESCRIPTION: Command to run the full test suite for the Flexport source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flexport/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flexport test\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Glassflow Connector\nDESCRIPTION: Command to add a new dependency to the Glassflow connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Twitter Source Connector\nDESCRIPTION: Uses airbyte-ci to build the docker image for the Twitter source connector. The resulting image will be tagged as 'airbyte/source-twitter:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twitter/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twitter build\n```\n\n----------------------------------------\n\nTITLE: Building Bitly Source Connector\nDESCRIPTION: Command to build a development image of the Bitly source connector using airbyte-ci tool. Creates a dev image tagged as source-bitly:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bitly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bitly build\n```\n\n----------------------------------------\n\nTITLE: Testing Source Mention Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-mention connector using the airbyte-ci testing framework.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mention/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mention test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tyntec-sms/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tyntec-sms test\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with Gradle (Shell)\nDESCRIPTION: Builds the Docker image for the source-e2e-test-cloud connector using the Gradle wrapper script. The resulting image will be tagged as 'airbyte/source-e2e-test-cloud:dev'. This command should be executed from the root of the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-e2e-test-cloud:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Installing Vectara Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Tplcentral Connector\nDESCRIPTION: Command to run unit tests for the Tplcentral connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Testing Calendly Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Calendly source connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-calendly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-calendly test\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Docker Commands (Bash/Docker)\nDESCRIPTION: Runs various Docker commands for interacting with the built Clockify source connector image. These commands perform spec retrieval, configuration checking, schema discovery, and reading with sample configs. They mount local directories (such as secrets and integration_tests) for configuration files, using --rm for ephemeral containers. Requires Docker, the built airbyte/source-clockify:dev image, and correctly formatted config files as specified in the manifest.yaml spec. Outputs are CLI responses for each operation. Limitations: paths must exist and configs must match required schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clockify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-clockify:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clockify:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-clockify:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-clockify:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Navan Source Connector in Airbyte\nDESCRIPTION: Command to build a development image of the Navan source connector using airbyte-ci. Creates a dev image tagged as source-navan:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-navan/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-navan build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Unleash Source Connector\nDESCRIPTION: Command to build the Docker image for the Unleash source connector using airbyte-ci. This creates an image tagged as airbyte/source-unleash:dev on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-unleash/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-unleash build\n```\n\n----------------------------------------\n\nTITLE: Defining WaitUntilTimeFromHeader Backoff Schema in YAML\nDESCRIPTION: Establishes a YAML schema for a backoff strategy that waits until a time value found (optionally via regex) in a response header. Can enforce a 'min_wait' threshold and allows additional parameters. Intended for scenarios where a server instructs a precise retry time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nWaitUntilTimeFromHeader:\n  type: object\n  additionalProperties: true\n  required:\n    - header\n  properties:\n    \"$parameters\":\n      \"$ref\": \"#/definitions/$parameters\"\n    header:\n      type: string\n    regex:\n      type: string\n    min_wait:\n      type: number\n```\n\n----------------------------------------\n\nTITLE: Schema-less Mode Document Structure\nDESCRIPTION: Example JSON structure showing the format of documents when schema enforcement is disabled\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_id\": <document id>,\n  \"data\": {<a JSON cotaining the entire set of fields found in document>}\n}\n```\n\n----------------------------------------\n\nTITLE: Running Firebolt Connector Tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Firebolt source connector using the airbyte-ci tool, which executes various acceptance tests defined in the configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firebolt test\n```\n\n----------------------------------------\n\nTITLE: Creating MongoDB Credentials JSON for Testing\nDESCRIPTION: This JSON structure defines the necessary credentials for connecting to a MongoDB instance for testing purposes. It includes fields for the database name, user, password, cluster URL, host, and port.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mongodb-strict-encrypt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"database\": \"database_name\",\n   \"user\": \"user\",\n   \"password\": \"password\",\n   \"cluster_url\": \"cluster_url\",\n   \"host\": \"host\",\n   \"port\": \"port\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Timeout Extensions for Schema Discovery\nDESCRIPTION: YAML configuration for extending timeout limits when dealing with large database sources that have many tables and columns. This prevents timeouts during schema discovery operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  extraEnvs:\n    - name: HTTP_IDLE_TIMEOUT\n      value: 20m\n    - name: READ_TIMEOUT\n      value: 30m\n```\n\n----------------------------------------\n\nTITLE: Building a Custom Airbyte Connector Docker Image - bash\nDESCRIPTION: This bash snippet shows how to build and run a custom Docker image for the Outbrain Amplify connector. It uses docker build to create an image tagged for development, and docker run to invoke the spec command, validating the build. It requires a Dockerfile (as previously shown) and a compatible source code base.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-outbrain-amplify:dev .\n# Running the spec command against your patched connector\ndocker run airbyte/source-outbrain-amplify:dev spec\n```\n\n----------------------------------------\n\nTITLE: Using max Filter in Jinja2\nDESCRIPTION: Demonstrates the `max` filter in Jinja2, which returns the largest item from a sequence. The example finds the maximum value in the list `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_36\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|max }}\n```\n\n----------------------------------------\n\nTITLE: Using kubectl with abctl-installed Airbyte\nDESCRIPTION: Example command to interact with Kubernetes resources in an Airbyte instance installed via abctl using kubectl with the correct kubeconfig and namespace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl --kubeconfig ~/.airbyte/abctl/abctl.kubeconfig --namespace airbyte-abctl get pods\n```\n\n----------------------------------------\n\nTITLE: Configuring SendGrid Contact Export Polling Step (UI)\nDESCRIPTION: Details the UI configuration within the 'Polling' tab for checking the status of a SendGrid contacts export job. It specifies the polling URL using the `{{ creation_response['id'] }}` variable to reference the job ID from the creation step, the GET HTTP method, the field path (`status`) for extracting the job status, the mapping of API status values (`ready`, `failed`, `pending`, `timeout`) to standard connector statuses, and the field path (`urls`) for extracting the download target.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/async-streams.md#2025-04-23_snippet_1\n\nLANGUAGE: Configuration/Templating\nCODE:\n```\n- **URL** field: `https://api.sendgrid.com/v3/marketing/contacts/exports/{{creation_response['id']}}`\n- **HTTP Method** dropdown: `GET`\n- In the **Status Extractor** section:\n  - Set the **Field Path** to: `status`\n- In the **Status Mapping** section:\n  - Set **Completed** to: `ready`\n  - Set **Failed** to: `failed`\n  - Set **Running** to: `pending`\n  - Set **Timeout** to: `timeout`\n- In the **Download Target Extractor** section:\n  - Set the **Field Path** to: `urls`\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Kissmetrics Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Kissmetrics source connector using airbyte-ci. It helps ensure the connector meets the required standards and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kissmetrics/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kissmetrics test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Vantage connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vantage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vantage test\n```\n\n----------------------------------------\n\nTITLE: Testing the Source CircleCI Connector Locally using Bash\nDESCRIPTION: This command executes the acceptance tests defined for the `source-circleci` connector using the `airbyte-ci` tool. It requires the `airbyte-ci` tool to be installed and typically runs against a previously built development image or the existing connector definition.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-circleci/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-circleci test\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Connector Test Suite with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Qdrant connector using the airbyte-ci tool. This includes unit tests, integration tests, and acceptance tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-qdrant test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the source-gologin Connector Using Airbyte CI (Bash)\nDESCRIPTION: This snippet provides the command for executing acceptance tests for the 'source-gologin' connector using Airbyte CI. Before executing, ensure that Airbyte CI is installed and configured correctly. The 'test' subcommand runs a suite of tests to verify the connector's compliance and functionality locally, also targeting the connector by its name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gologin/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gologin test\n```\n\n----------------------------------------\n\nTITLE: Displaying Statuspage.io API Connector Changelog\nDESCRIPTION: A markdown table showing the version history and changes made to the Statuspage.io API connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/statuspage.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                              | Subject                                         |\n|:--------|:-----------| :-------------------------------------------------------- | :---------------------------------------------- |\n| 0.2.19 | 2025-04-19 | [58388](https://github.com/airbytehq/airbyte/pull/58388) | Update dependencies |\n| 0.2.18 | 2025-04-12 | [57935](https://github.com/airbytehq/airbyte/pull/57935) | Update dependencies |\n| 0.2.17 | 2025-04-05 | [57421](https://github.com/airbytehq/airbyte/pull/57421) | Update dependencies |\n| 0.2.16 | 2025-03-29 | [56897](https://github.com/airbytehq/airbyte/pull/56897) | Update dependencies |\n| 0.2.15 | 2025-03-22 | [56253](https://github.com/airbytehq/airbyte/pull/56253) | Update dependencies |\n| 0.2.14 | 2025-03-08 | [55625](https://github.com/airbytehq/airbyte/pull/55625) | Update dependencies |\n| 0.2.13 | 2025-03-01 | [55120](https://github.com/airbytehq/airbyte/pull/55120) | Update dependencies |\n| 0.2.12 | 2025-02-22 | [54481](https://github.com/airbytehq/airbyte/pull/54481) | Update dependencies |\n| 0.2.11 | 2025-02-15 | [54104](https://github.com/airbytehq/airbyte/pull/54104) | Update dependencies |\n| 0.2.10 | 2025-02-08 | [53588](https://github.com/airbytehq/airbyte/pull/53588) | Update dependencies |\n| 0.2.9 | 2025-02-01 | [53092](https://github.com/airbytehq/airbyte/pull/53092) | Update dependencies |\n| 0.2.8 | 2025-01-25 | [52459](https://github.com/airbytehq/airbyte/pull/52459) | Update dependencies |\n| 0.2.7 | 2025-01-18 | [51971](https://github.com/airbytehq/airbyte/pull/51971) | Update dependencies |\n| 0.2.6 | 2025-01-11 | [51401](https://github.com/airbytehq/airbyte/pull/51401) | Update dependencies |\n| 0.2.5 | 2025-01-04 | [50749](https://github.com/airbytehq/airbyte/pull/50749) | Update dependencies |\n| 0.2.4 | 2024-12-21 | [50348](https://github.com/airbytehq/airbyte/pull/50348) | Update dependencies |\n| 0.2.3 | 2024-12-14 | [49782](https://github.com/airbytehq/airbyte/pull/49782) | Update dependencies |\n| 0.2.2 | 2024-12-12 | [49426](https://github.com/airbytehq/airbyte/pull/49426) | Update dependencies |\n| 0.2.1 | 2024-08-16 | [44196](https://github.com/airbytehq/airbyte/pull/44196) | Bump source-declarative-manifest version |\n| 0.2.0 | 2024-08-14 | [44061](https://github.com/airbytehq/airbyte/pull/44061) | Refactor connector to manifest-only format |\n| 0.1.13 | 2024-08-12 | [43866](https://github.com/airbytehq/airbyte/pull/43866) | Update dependencies |\n| 0.1.12 | 2024-08-10 | [43525](https://github.com/airbytehq/airbyte/pull/43525) | Update dependencies |\n| 0.1.11 | 2024-08-03 | [43208](https://github.com/airbytehq/airbyte/pull/43208) | Update dependencies |\n| 0.1.10 | 2024-07-27 | [42596](https://github.com/airbytehq/airbyte/pull/42596) | Update dependencies |\n| 0.1.9 | 2024-07-20 | [42324](https://github.com/airbytehq/airbyte/pull/42324) | Update dependencies |\n| 0.1.8 | 2024-07-13 | [41828](https://github.com/airbytehq/airbyte/pull/41828) | Update dependencies |\n| 0.1.7 | 2024-07-10 | [41413](https://github.com/airbytehq/airbyte/pull/41413) | Update dependencies |\n| 0.1.6 | 2024-07-09 | [41290](https://github.com/airbytehq/airbyte/pull/41290) | Update dependencies |\n| 0.1.5 | 2024-07-06 | [40902](https://github.com/airbytehq/airbyte/pull/40902) | Update dependencies |\n| 0.1.4 | 2024-06-26 | [40182](https://github.com/airbytehq/airbyte/pull/40182) | Update dependencies |\n| 0.1.3   | 2024-06-20 | [#38662](https://github.com/airbytehq/airbyte/pull/38662) | Make connector compatible with Builder          |\n| 0.1.2   | 2024-06-04 | [39064](https://github.com/airbytehq/airbyte/pull/39064) | [autopull] Upgrade base image to v1.2.1 |\n| 0.1.1   | 2024-05-20 | [38451](https://github.com/airbytehq/airbyte/pull/38451) | [autopull] base image + poetry + up_to_date |\n| 0.1.0   | 2022-10-30 | [#18664](https://github.com/airbytehq/airbyte/pull/18664) | 🎉 New Source: Statuspage.io API [low-code CDK] |\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests Against External Airbyte Deployment (Bash)\nDESCRIPTION: Executes the Gradle acceptance test task (`:oss:airbyte-tests:acceptanceTests`) with the `USE_EXTERNAL_DEPLOYMENT` environment variable set to `true`. This directs the tests to target an already running Airbyte instance instead of managing one via test containers. Run from the `airbyte-platform` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nUSE_EXTERNAL_DEPLOYMENT=true ./gradlew :oss:airbyte-tests:acceptanceTests\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Connector\nDESCRIPTION: Example Dockerfile to create a custom build of the connector based on the latest version of the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-aws-datalake:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Running Coinmarketcap Source Connector Commands in Docker (Bash)\nDESCRIPTION: These Bash commands execute standard Airbyte source connector actions using Docker, including spec, check, discover, and read, for the Coinmarketcap connector. Dependencies include Docker and a built connector image. Key parameters are --config for specifying configuration JSON, --catalog for providing the data catalog, and mounted volumes for accessing secrets and integration tests. Inputs require properly formatted config and catalog files, while outputs include command results (such as discovery or data reads). Limitations involve ensuring host paths exist and that the Docker image is built as instructed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coinmarketcap/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-coinmarketcap:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coinmarketcap:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coinmarketcap:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-coinmarketcap:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Managing Sync Progression with Airbyte State File - JSON\nDESCRIPTION: This snippet illustrates the structure and setup of an Airbyte state file (state.json) in JSON format. It contains an array of stream states for incremental data synchronization, including user, purchase, and product streams, each with tracking IDs. The state file should be updated after each chunked sync by incrementing 'id' and 'user_id' fields, helping Airbyte continue from the last checkpoint. This file must match the stream configuration and is required for resuming exports in large-scale data scenarios.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/integration_tests/csv_export/README.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\\n  {\\n    \\\"type\\\": \\\"STREAM\\\",\\n    \\\"stream\\\": {\\n      \\\"stream_state\\\": {\\n        \\\"id\\\": 0\\n      },\\n      \\\"stream_descriptor\\\": {\\n        \\\"name\\\": \\\"users\\\"\\n      }\\n    }\\n  },\\n  {\\n    \\\"type\\\": \\\"STREAM\\\",\\n    \\\"stream\\\": {\\n      \\\"stream_state\\\": {\\n        \\\"id\\\": 0,\\n        \\\"user_id\\\": 0\\n      },\\n      \\\"stream_descriptor\\\": {\\n        \\\"name\\\": \\\"purchases\\\"\\n      }\\n    }\\n  },\\n  {\\n    \\\"type\\\": \\\"STREAM\\\",\\n    \\\"stream\\\": {\\n      \\\"stream_state\\\": {\\n        \\\"id\\\": 0\\n      },\\n      \\\"stream_descriptor\\\": {\\n        \\\"name\\\": \\\"products\\\"\\n      }\\n    }\\n  }\\n]\n```\n\n----------------------------------------\n\nTITLE: Testing Wufoo Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Wufoo connector using airbyte-ci. Validates the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wufoo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wufoo test\n```\n\n----------------------------------------\n\nTITLE: Displaying JSON Object Example for Azure Blob Storage\nDESCRIPTION: Example JSON object showing user data structure as it appears from a source before being written to Azure Blob Storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/azure-blob-storage.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Coda Connector Test Suite using airbyte-ci - Bash\nDESCRIPTION: This command runs the complete integration and unit test suite for the source-coda connector using airbyte-ci. It ensures that recent changes do not break functionality or integrations, a prerequisite for contribution or publishing a new connector version. The command assumes a proper local setup with airbyte-ci installed and is typically run prior to pull requests or releases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coda/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coda test\n```\n\n----------------------------------------\n\nTITLE: Installing Greenhouse Connector Dependencies with Poetry\nDESCRIPTION: This command installs the necessary dependencies for the Greenhouse connector using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Source-Pivotal-Tracker Docker Image with airbyte-ci (bash)\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the Airbyte source-pivotal-tracker connector using the airbyte-ci command-line tool. The prerequisite is that 'airbyte-ci' must be installed according to the project's documentation. Running this command results in a Docker image tagged as 'airbyte/source-pivotal-tracker:dev' locally. This step is essential before running or testing the connector as a Docker container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pivotal-tracker/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pivotal-tracker build\n\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Pytest and Poetry (Bash)\nDESCRIPTION: Executes the unit tests located in the `unit_tests` directory using `pytest`, invoked via `poetry run`. Requires Poetry and development dependencies to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Qualaroo Source Connector Docker Container Commands\nDESCRIPTION: Standard source connector commands to run the Qualaroo connector as a docker container. These commands allow you to get the connector specification, check connection configuration, discover available data streams, and read data from the source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qualaroo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-qualaroo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-qualaroo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-qualaroo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-qualaroo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Updating Airbyte CI\nDESCRIPTION: Command to update the Airbyte CI tool to the latest version. This is an alternative to reinstalling it from scratch.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nairbyte-ci update\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog in Markdown\nDESCRIPTION: A markdown table showing the version history of the Airbyte Destination E2E/Dev-Null connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/dev-null.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version     | Date       | Pull Request                                             | Subject                                                                                       |\n|:------------|:-----------|:---------------------------------------------------------|:----------------------------------------------------------------------------------------------|\n| 0.7.20      | 2025-03-21 | [55906](https://github.com/airbytehq/airbyte/pull/55906) | CDK: Pass DestinationRecordRaw around instead of DestinationRecordAirbyteValue                |\n| 0.7.19      | 2025-03-13 | [55737](https://github.com/airbytehq/airbyte/pull/55737) | CDK: Pass DestinationRecordRaw around instead of DestinationRecordAirbyteValue                |\n| 0.7.18      | 2025-02-25 | [54179](https://github.com/airbytehq/airbyte/pull/54179) | Use new CDK interface; perf improvements, skip initial staging                                |\n| 0.7.17      | 2025-01-24 | [51600](https://github.com/airbytehq/airbyte/pull/51600) | Internal refactor                                                                             |\n| 0.7.16      | 2024-12-19 | [52076](https://github.com/airbytehq/airbyte/pull/52076) | Test improvements                                                                             |\n| 0.7.15      | 2024-12-19 | [49899](https://github.com/airbytehq/airbyte/pull/49931) | Non-functional CDK changes                                                                    |\n| 0.7.14      | 2024-12-20 | [49974](https://github.com/airbytehq/airbyte/pull/49974) | Non-functional CDK changes                                                                    |\n| 0.7.13      | 2024-12-18 | [49899](https://github.com/airbytehq/airbyte/pull/49899) | Use a base image: airbyte/java-connector-base:1.0.0                                           |\n| 0.7.12      | 2024-12-04 | [48794](https://github.com/airbytehq/airbyte/pull/48794) | Promoting release candidate 0.7.12-rc.2 to a main version.                                    |\n| 0.7.12-rc.2 | 2024-11-26 | [48693](https://github.com/airbytehq/airbyte/pull/48693) | Update for testing progressive rollout                                                        |\n| 0.7.12-rc.1 | 2024-11-25 | [48693](https://github.com/airbytehq/airbyte/pull/48693) | Update for testing progressive rollout                                                        |\n| 0.7.11      | 2024-11-18 | [48468](https://github.com/airbytehq/airbyte/pull/48468) | Implement File CDk                                                                            |\n| 0.7.10      | 2024-11-08 | [48429](https://github.com/airbytehq/airbyte/pull/48429) | Bugfix: correctly handle state ID field                                                       |\n| 0.7.9       | 2024-11-07 | [48417](https://github.com/airbytehq/airbyte/pull/48417) | Only pass through the state ID field, not all additional properties                           |\n| 0.7.8       | 2024-11-07 | [48416](https://github.com/airbytehq/airbyte/pull/48416) | Bugfix: global state correclty sends additional properties                                    |\n| 0.7.7       | 2024-10-17 | [46692](https://github.com/airbytehq/airbyte/pull/46692) | Internal code changes                                                                         |\n| 0.7.6       | 2024-10-08 | [46683](https://github.com/airbytehq/airbyte/pull/46683) | Bugfix: pick up checkpoint safety check fix                                                   |\n| 0.7.5       | 2024-10-08 | [46683](https://github.com/airbytehq/airbyte/pull/46683) | Bugfix: checkpoints in order, all checkpoints processed before shutdown                       |\n| 0.7.4       | 2024-10-08 | [46650](https://github.com/airbytehq/airbyte/pull/46650) | Internal code changes                                                                         |\n| 0.7.3       | 2024-10-01 | [46559](https://github.com/airbytehq/airbyte/pull/46559) | From load CDK: async improvements, stream incomplete, additionalProperties on state messages  |\n| 0.7.2       | 2024-10-01 | [45929](https://github.com/airbytehq/airbyte/pull/45929) | Internal code changes                                                                         |\n| 0.7.1       | 2024-09-30 | [46276](https://github.com/airbytehq/airbyte/pull/46276) | Upgrade to latest bulk CDK                                                                    |\n| 0.7.0       | 2024-09-20 | [45704](https://github.com/airbytehq/airbyte/pull/45704) |                                                                                               |\n| 0.6.1       | 2024-09-20 | [45715](https://github.com/airbytehq/airbyte/pull/45715) | add destination to cloud registry                                                             |\n| 0.6.0       | 2024-09-18 | [45651](https://github.com/airbytehq/airbyte/pull/45651) | merge destination-e2e(OSS) and destination-dev-null(cloud)                                    |\n| 0.5.0       | 2024-09-18 | [45650](https://github.com/airbytehq/airbyte/pull/45650) | upgrade cdk                                                                                   |\n| 0.4.1       | 2024-09-18 | [45649](https://github.com/airbytehq/airbyte/pull/45649) | convert test code to kotlin                                                                   |\n| 0.4.0       | 2024-09-18 | [45648](https://github.com/airbytehq/airbyte/pull/45648) | convert production code to kotlin                                                             |\n| 0.3.6       | 2024-05-09 | [38097](https://github.com/airbytehq/airbyte/pull/38097) | Support dedup                                                                                 |\n| 0.3.5       | 2024-04-29 | [37366](https://github.com/airbytehq/airbyte/pull/37366) | Support refreshes                                                                             |\n| 0.3.4       | 2024-04-16 | [37366](https://github.com/airbytehq/airbyte/pull/37366) | Fix NPE                                                                                       |\n| 0.3.3       | 2024-04-16 | [37366](https://github.com/airbytehq/airbyte/pull/37366) | Fix Log trace messages                                                                        |\n| 0.3.2       | 2024-02-14 | [36812](https://github.com/airbytehq/airbyte/pull/36812) | Log trace messages                                                                            |\n| 0.3.1       | 2024-02-14 | [35278](https://github.com/airbytehq/airbyte/pull/35278) | Adopt CDK 0.20.6                                                                              |\n| 0.3.0       | 2023-05-08 | [25776](https://github.com/airbytehq/airbyte/pull/25776) | Standardize spec and change property field to non-keyword                                     |\n| 0.2.4       | 2022-06-17 | [13864](https://github.com/airbytehq/airbyte/pull/13864) | Updated stacktrace format for any trace message errors                                        |\n| 0.2.3       | 2022-02-14 | [10256](https://github.com/airbytehq/airbyte/pull/10256) | Add `-XX:+ExitOnOutOfMemoryError` JVM option                                                  |\n| 0.2.2       | 2022-01-29 | [\\#9745](https://github.com/airbytehq/airbyte/pull/9745) | Integrate with Sentry.                                                                        |\n| 0.2.1       | 2021-12-19 | [\\#8824](https://github.com/airbytehq/airbyte/pull/8905) | Fix documentation URL.                                                                        |\n| 0.2.0       | 2021-12-16 | [\\#8824](https://github.com/airbytehq/airbyte/pull/8824) | Add multiple logging modes.                                                                   |\n| 0.1.0       | 2021-05-25 | [\\#3290](https://github.com/airbytehq/airbyte/pull/3290) | Create initial version.                                                                       |\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Gutendex Source Connector\nDESCRIPTION: Command to run the full test suite for the Gutendex source connector using airbyte-ci. This executes all tests associated with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gutendex/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gutendex test\n```\n\n----------------------------------------\n\nTITLE: Testing Jamf Pro Source Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Jamf Pro source connector using airbyte-ci. It verifies the functionality of the connector against predefined test cases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jamf-pro/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jamf-pro test\n```\n\n----------------------------------------\n\nTITLE: Building the Ringcentral connector with airbyte-ci\nDESCRIPTION: Command to build the Ringcentral source connector using the airbyte-ci tool, which creates a Docker image tagged as airbyte/source-ringcentral:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ringcentral/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ringcentral build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Google Tasks Connector\nDESCRIPTION: Command to execute acceptance tests for the Google Tasks connector using airbyte-ci. This runs the test suite to ensure the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-tasks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-tasks test\n```\n\n----------------------------------------\n\nTITLE: Uploading Local Metadata Files for Testing\nDESCRIPTION: Commands to upload local metadata files to a GCS bucket for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ../lib\nexport GCS_CREDENTIALS=`cat /path/to/gcs_credentials.json`\npoetry run metadata_service upload <PATH TO METADATA FILE> <NAME OF YOUR BUCKET>\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table in Markdown\nDESCRIPTION: Table outlining the supported features of the BigCommerce connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bigcommerce.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                   | Supported?\\(Yes/No\\) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | Yes                  |       |\n| Namespaces                | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Hardcoded Records Connector\nDESCRIPTION: Command to build a Docker image for the Hardcoded Records connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hardcoded-records build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Whisky Hunter Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Whisky Hunter source connector locally. It ensures that all changes are passing the test suite before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-whisky-hunter/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-whisky-hunter test\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run various connector operations within a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-xata:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-xata:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-xata:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-xata:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Keka Source Connector using airbyte-ci\nDESCRIPTION: This command builds a development image of the Keka source connector for local testing. It uses the airbyte-ci tool to create a Docker image tagged as 'source-keka:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-keka/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-keka build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Zendesk Chat source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-chat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-chat build\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: Uses the `airbyte-ci` command-line tool to build the Docker image for the `source-google-pagespeed-insights` connector. Requires `airbyte-ci` to be installed. The resulting image will be tagged `airbyte/source-google-pagespeed-insights:dev` locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-pagespeed-insights build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Customizing Connector Build Process with Python\nDESCRIPTION: Example Python module that demonstrates how to customize the build process for the connector by defining pre_connector_install and post_connector_install functions to modify the base image and connector container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Testing SparkPost Connector for Airbyte (Bash)\nDESCRIPTION: This command runs the acceptance tests for the SparkPost connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sparkpost/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sparkpost test\n```\n\n----------------------------------------\n\nTITLE: Building the Rocketlane Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Rocketlane source connector. This creates a Docker image tagged as 'source-rocketlane:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rocketlane/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rocketlane build\n```\n\n----------------------------------------\n\nTITLE: Building Mailosaur Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Mailosaur source connector using airbyte-ci. Creates a dev image tagged as source-mailosaur:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailosaur/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailosaur build\n```\n\n----------------------------------------\n\nTITLE: Building Capsule CRM Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Capsule CRM connector using airbyte-ci. Creates a dev image tagged as source-capsule-crm:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-capsule-crm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-capsule-crm build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Xero source connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-xero:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xero/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xero build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Newsdata source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-newsdata/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-newsdata build\n```\n\n----------------------------------------\n\nTITLE: Building Source-Front Connector Locally with airbyte-ci\nDESCRIPTION: Command to build a development image of the source-front connector using airbyte-ci. This creates a dev image named 'source-front:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-front/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-front build\n```\n\n----------------------------------------\n\nTITLE: Building JustCall Source Connector for Airbyte\nDESCRIPTION: This command builds a development image for the JustCall source connector using airbyte-ci. The resulting image (source-justcall:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-justcall/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-justcall build\n```\n\n----------------------------------------\n\nTITLE: Running the Freshsales Connector Test Suite\nDESCRIPTION: Command to execute the full test suite for the Freshsales source connector using airbyte-ci tool. This validates that the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshsales/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshsales test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Commands to run the connector using Docker for various operations like spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-netsuite:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-netsuite:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-netsuite:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-netsuite:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building WordPress Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the WordPress source connector using airbyte-ci. Creates a dev image tagged as source-wordpress:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wordpress/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wordpress build\n```\n\n----------------------------------------\n\nTITLE: Building the FireHydrant Connector with airbyte-ci\nDESCRIPTION: Command to build the FireHydrant connector locally, creating a dev image (source-firehydrant:dev) for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firehydrant/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-firehydrant build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Oura Source Connector with Airbyte-CI - Bash\nDESCRIPTION: This Bash command runs the full test suite for the `source-oura` connector using the Airbyte CI toolkit. It assumes that `airbyte-ci` has already been installed and is accessible in the working environment. This helps ensure code changes do not break existing functionality before contributing or publishing updates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oura/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oura test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running The Guardian API Source Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the source-the-guardian-api connector in a Docker container. These commands include spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-the-guardian-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-the-guardian-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-the-guardian-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-the-guardian-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-the-guardian-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Lemlist Connector CI Tests using Bash\nDESCRIPTION: This command utilizes the 'airbyte-ci' tool to run the full test suite locally for the 'source-lemlist' connector. It requires 'airbyte-ci' to be installed and helps ensure code changes pass all necessary checks before contribution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lemlist/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lemlist test\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Connecting to MongoDB Cluster via Shell\nDESCRIPTION: Command to launch MongoDB shell and connect to a cluster with admin permissions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n> mongosh <connection string to cluster> --username <user with admin permissions>;\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests\nDESCRIPTION: Command to run the full test suite using Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-crm test\n```\n\n----------------------------------------\n\nTITLE: Building the Pabbly Connector Development Image using Bash\nDESCRIPTION: This command utilizes the 'airbyte-ci' tool to build a local development Docker image tagged as 'source-pabbly-subscriptions-billing:dev'. This image is essential for running local tests and verifying changes made during development. Requires 'airbyte-ci' to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pabbly-subscriptions-billing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pabbly-subscriptions-billing build\n```\n\n----------------------------------------\n\nTITLE: Building a Drip Source Connector with Airbyte CI - Bash\nDESCRIPTION: This command uses the airbyte-ci CLI tool to build a development Docker image for the source-drip connector, tagging it as source-drip:dev. The airbyte-ci tool must be installed prior to running this command. The --name parameter specifies the connector to build. Input is provided directly via CLI arguments, and the main output is a locally available Docker image for testing or further development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-drip/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-drip build\n```\n\n----------------------------------------\n\nTITLE: Building the Chargedesk Source Connector with airbyte-ci - Bash\nDESCRIPTION: This bash snippet executes the build process for the Chargedesk source connector by invoking the airbyte-ci CLI tool. It requires the airbyte-ci command-line utility to be installed and available in your PATH. The '--name' parameter specifies the connector's identifier ('source-chargedesk'), while the 'build' argument triggers the construction of a development image ('source-chargedesk:dev'). The output is a Docker image suitable for local testing. Ensure prerequisite dependencies (like Docker) are installed, and that you have access to the source-chargedesk directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargedesk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargedesk build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for SFTP-Bulk Connector\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Using random Filter in Jinja2\nDESCRIPTION: Demonstrates the `random` filter in Jinja2, which returns a random item from a sequence. The example selects a random element from `[1, 2, 3]`. Note: The output shown (2) is just one possibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_38\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|random }}\n```\n\n----------------------------------------\n\nTITLE: Testing the Shortcut Connector with airbyte-ci\nDESCRIPTION: Command to run the acceptance tests for the Shortcut connector, validating its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shortcut/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shortcut test\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block - Data Reset\nDESCRIPTION: Markdown code block noting potential data reset prompts based on destination type\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nDepending on destination type you may not be prompted to reset your data.\n```\n```\n\n----------------------------------------\n\nTITLE: Copying Output File to Host Machine in Bash\nDESCRIPTION: This command copies the output file from the Airbyte server container to the current working directory on the host machine. It uses Docker CP to transfer the file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/local-json.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path}/{filename}.jsonl .\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector operations within a Docker container, including specification, checking, discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-instagram:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-instagram:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-instagram:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-instagram:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Blogger Source Connector with Airbyte-CI\nDESCRIPTION: Command to run acceptance tests for the Blogger source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-blogger/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-blogger test\n```\n\n----------------------------------------\n\nTITLE: Building Hellobaton Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Hellobaton source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-hellobaton:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hellobaton/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hellobaton build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Hardcoded Records Connector\nDESCRIPTION: Command to execute unit tests for the Hardcoded Records connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Minimal `acceptance-test-config.yml` Configuration (YAML)\nDESCRIPTION: Provides a basic example of the `acceptance-test-config.yml` file required for running standard acceptance tests. It specifies the connector's Docker image and the path to the connector's spec file for the `spec` test suite.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnector_image: airbyte/source-some-connector:dev\nacceptance-tests:\n  spec:\n    tests:\n      - spec_path: \"some_folder/spec.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Building Illumina Basespace Source Connector in Bash\nDESCRIPTION: This command uses airbyte-ci to build a development image of the Illumina Basespace source connector. The resulting image is tagged as 'source-illumina-basespace:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-illumina-basespace/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-illumina-basespace build\n```\n\n----------------------------------------\n\nTITLE: Testing the RevenueCat Source Connector with airbyte-ci\nDESCRIPTION: Runs the acceptance tests for the RevenueCat source connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-revenuecat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-revenuecat test\n```\n\n----------------------------------------\n\nTITLE: Building Help Scout Source Connector in Bash\nDESCRIPTION: This command uses airbyte-ci to build a development image of the Help Scout source connector. The resulting image is tagged as 'source-help-scout:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-help-scout/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-help-scout build\n```\n\n----------------------------------------\n\nTITLE: Testing Bunny Inc Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Bunny Inc source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bunny-inc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bunny-inc test\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to activate the virtual environment and install required dependencies from requirements.txt.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: A markdown table showing the version history, dates, pull request references and descriptions of changes for the Mailchimp connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mailchimp.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                    |\n|---------|------------|----------------------------------------------------------|----------------------------------------------------------------------------|\n| 2.0.34 | 2025-04-12 | [57719](https://github.com/airbytehq/airbyte/pull/57719) | Update dependencies |\n| 2.0.33 | 2025-04-05 | [57043](https://github.com/airbytehq/airbyte/pull/57043) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running airbyte-ci with debug logs\nDESCRIPTION: Command to run airbyte-ci with the --show-dagger-logs option for detailed debugging information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\n$ airbyte-ci --show-dagger-logs connectors --name=source-pokeapi test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-rabbitmq test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for SingleStore Connector\nDESCRIPTION: Command to build the Docker image for the SingleStore connector using Gradle, which creates an image named 'airbyte/source-singlestore:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-singlestore/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-singlestore:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building Sage HR Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Sage HR connector using airbyte-ci. The resulting image (source-sage-hr:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sage-hr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sage-hr build\n```\n\n----------------------------------------\n\nTITLE: Building Tavus Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Tavus source connector using airbyte-ci. The resulting image is tagged as 'source-tavus:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tavus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tavus build\n```\n\n----------------------------------------\n\nTITLE: Testing source-elasticemail Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-elasticemail connector. This validates that the connector meets the required standards and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticemail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-elasticemail test\n```\n\n----------------------------------------\n\nTITLE: Building Docker image for Recharge connector\nDESCRIPTION: Command to build a Docker image for the Recharge connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recharge build\n```\n\n----------------------------------------\n\nTITLE: Running Source-Pendo Commands with Docker in Bash\nDESCRIPTION: These commands illustrate how to interact with the source-pendo Docker image by running Airbyte standard operations: spec, check, discover, and read. These Docker commands mount credential and test data directories as needed and execute specific Airbyte subcommands. They assume that the required config files (such as /secrets/config.json and /integration_tests/configured_catalog.json) are present in the appropriate mounted directories.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pendo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pendo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pendo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pendo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pendo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Public-Apis Connector as Docker Container\nDESCRIPTION: Commands to run the connector operations inside a Docker container, mounting necessary volumes for configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-public-apis:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-public-apis:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-public-apis:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-public-apis:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Databricks Connector Docker Image via Gradle\nDESCRIPTION: Command to build the Docker image for the Databricks destination connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databricks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-databricks:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Local Connector Operations\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-rabbitmq spec\npoetry run destination-rabbitmq check --config secrets/config.json\npoetry run destination-rabbitmq discover --config secrets/config.json\npoetry run destination-rabbitmq read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Expense Connector with Airbyte-CI\nDESCRIPTION: Command to run acceptance tests for the Zoho Expense source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-expense/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-expense test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Vantage connector in a Docker container, including spec, check, discover, and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vantage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-vantage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-vantage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-vantage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-vantage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Intruder Source Connector\nDESCRIPTION: Command to build the Docker image for the Intruder source connector using airbyte-ci. This creates an image tagged as airbyte/source-intruder:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intruder/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-intruder build\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: Detailed version history table showing changes and updates to the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-sell.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                        |\n| :------ | :--------- | :------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| 0.3.18 | 2025-04-19 | [58570](https://github.com/airbytehq/airbyte/pull/58570) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Airbyte CI\nDESCRIPTION: Command to run the full test suite using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-netsuite test\n```\n\n----------------------------------------\n\nTITLE: Testing Oncehub Connector Using airbyte-ci - Bash\nDESCRIPTION: This Bash snippet runs acceptance tests against the source-oncehub connector using the airbyte-ci tool. It verifies the connector's compatibility and correct behavior according to Airbyte's standards. Requires airbyte-ci installed; the --name flag specifies which connector to test. Outputs test results to confirm connector validity or identify issues.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oncehub/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oncehub test\n```\n\n----------------------------------------\n\nTITLE: Streams Configuration Table in Markdown\nDESCRIPTION: Table showing available data streams with their properties including primary keys and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/newsdata-io.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| latest_news | `article_id` | DefaultPaginator | ✅ |  ❌  |\n| historical_news | `article_id` | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Installing Schema Generator with Poetry\nDESCRIPTION: Commands to navigate to the schema_generator directory from the Airbyte project root and install the tool using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/schema_generator/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd tools/schema_generator # assumes you are starting from the root of the Airbyte project.\n$ poetry install\n```\n\n----------------------------------------\n\nTITLE: Building Paperform Connector Development Image using airbyte-ci (bash)\nDESCRIPTION: Builds a local development Docker image tagged as 'source-paperform:dev' for the Paperform connector using the `airbyte-ci` tool. This command requires `airbyte-ci` to be installed and configured in the environment. It compiles the connector code and packages it into a runnable image for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paperform/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paperform build\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Coda Source Connector Commands in Docker - Bash\nDESCRIPTION: This series of Docker commands demonstrates running the Coda source connector in different operational modes: spec, check, discover, and read. Inputs such as configuration and catalog files are mounted from local directories, with secrets placed in a git-ignored folder. The expected outputs vary by mode, from returning the spec schema to streaming records for reading. Environment prerequisites include the previously built Docker image (airbyte/source-coda:dev) and proper secrets/configuration files, with outputs flowing to standard output or error streams depending on operation success.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coda/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-coda:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coda:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coda:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-coda:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Chargedesk Source Connector with airbyte-ci - Bash\nDESCRIPTION: This bash snippet runs acceptance tests for the Chargedesk connector using the airbyte-ci CLI tool. It requires airbyte-ci to be installed and correctly configured. The '--name=source-chargedesk' parameter targets the specific connector, and the 'test' argument launches the testing process, ensuring the connector meets acceptance criteria. Results provide validation for local developments. Prerequisites include prior successful build of the connector, and any dependencies (test data, credentials) must be configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargedesk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargedesk test\n```\n\n----------------------------------------\n\nTITLE: Building the SafetyCulture Connector in Bash\nDESCRIPTION: Command to build a development image of the SafetyCulture connector using airbyte-ci. This creates a dev image named 'source-safetyculture:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-safetyculture/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-safetyculture build\n```\n\n----------------------------------------\n\nTITLE: Changing Airbyte Password\nDESCRIPTION: Command to set a custom password for your Airbyte instance. This will restart the Airbyte server to apply the change.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nabctl local credentials --password YourStrongPasswordExample\n```\n\n----------------------------------------\n\nTITLE: Testing the Retail Express by Maropost Connector using airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the Retail Express by Maropost connector. It validates that the connector is functioning correctly according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-retailexpress-by-maropost/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-retailexpress-by-maropost test\n```\n\n----------------------------------------\n\nTITLE: Testing the Reddit Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-reddit connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-reddit/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-reddit test\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Fauna Container with Docker\nDESCRIPTION: Command to start a local Fauna database container using Docker. This exposes port 8443 for connecting to the database instance.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/examples/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --name faunadb -p 8443:8443 fauna/faunadb\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A markdown table containing version history details including version numbers, dates, pull request links, and change descriptions for the Hubspot connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hubspot.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Version    | Date       | Pull Request                                             | Subject                                                                                                                                                                          |\n|:-----------|:-----------|:---------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 4.6.4      | 2025-04-22 | [58138](https://github.com/airbytehq/airbyte/pull/58138) | Use short-hand custom object type name path for custom object streams.                                                                                                                                                       |\n| 4.6.3      | 2025-04-19 | [58226](https://github.com/airbytehq/airbyte/pull/58226) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Kafka Source Connector\nDESCRIPTION: This command runs the integration tests for the Kafka source connector using Gradle. It should be executed from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kafka/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-kafka:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appsflyer test\n```\n\n----------------------------------------\n\nTITLE: Building the Salesflare Connector with airbyte-ci\nDESCRIPTION: Command to build the Salesflare connector locally, creating a dev image (source-salesflare:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesflare/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesflare build\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Hardcoded Records Connector\nDESCRIPTION: Command to add a new dependency to the Hardcoded Records connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hardcoded-records/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Activecampaign source connector Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-activecampaign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-activecampaign:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-activecampaign:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-activecampaign:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-activecampaign:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Key Pair for Tunnel Authentication\nDESCRIPTION: Command to generate an RSA key pair in PEM format for SSH tunnel authentication\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nssh-keygen -t rsa -m PEM -f myuser_rsa\n```\n\n----------------------------------------\n\nTITLE: Changelog Table (Markdown)\nDESCRIPTION: Detailed version history and changelog in markdown table format showing updates and dependencies changes\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/newsdata.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                         |\n| :------ | :--------- | :------------------------------------------------------- | :------------------------------------------------------------------------------ |\n| 0.2.6 | 2025-03-29 | [56672](https://github.com/airbytehq/airbyte/pull/56672) | Update dependencies |\n| 0.2.5 | 2025-03-08 | [55469](https://github.com/airbytehq/airbyte/pull/55469) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Testing Akeneo Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Akeneo connector using airbyte-ci tool. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-akeneo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-akeneo test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Iterable Connector\nDESCRIPTION: Uses airbyte-ci to build a Docker image for the Iterable source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-iterable build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Mailersend source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailersend/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailersend build\n```\n\n----------------------------------------\n\nTITLE: Testing the Less Annoying CRM Connector (Bash)\nDESCRIPTION: This command executes the acceptance tests for the `source-less-annoying-crm` connector using the `airbyte-ci` tool. It verifies the connector's functionality against the standard Airbyte test suite. Requires `airbyte-ci` to be installed and the connector source code available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-less-annoying-crm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-less-annoying-crm test\n```\n\n----------------------------------------\n\nTITLE: Listing Pod Status in abctl Kubernetes Cluster (Shell)\nDESCRIPTION: Uses `kubectl` to retrieve and display the status of all pods within the `airbyte-abctl` namespace of the Kubernetes cluster managed by `abctl`. This helps verify that Airbyte components are running correctly. Requires `kubectl` configured for the cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Changelog Table - Version History\nDESCRIPTION: Detailed version history table showing updates and changes to the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nytimes.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                         |\n| :------ | :--------- | :------------------------------------------------------- | :------------------------------------------------------------------------------ |\n| 0.2.2 | 2024-10-29 | [47728](https://github.com/airbytehq/airbyte/pull/47728) | Update dependencies |\n| 0.2.1 | 2024-10-28 | [47634](https://github.com/airbytehq/airbyte/pull/47634) | Update dependencies |\n| 0.2.0 | 2024-08-22 | [44555](https://github.com/airbytehq/airbyte/pull/44555) | Refactor connector to manifest-only format |\n```\n\n----------------------------------------\n\nTITLE: Finding the latest airbyte-ci version in project file\nDESCRIPTION: Command to check the version specified in the project's pyproject.toml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\n$ cat airbyte-ci/connectors/pipelines/pyproject.toml | grep version\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Statsig Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Statsig source connector using airbyte-ci. It verifies that the connector meets the required specifications and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-statsig/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-statsig test\n```\n\n----------------------------------------\n\nTITLE: Managing Project Dependencies with Poetry - bash\nDESCRIPTION: This command shows how to add a new Python package as a dependency to the project using Poetry. After adding the package, developers should commit the changes to 'pyproject.toml' and 'poetry.lock' to ensure version control of dependencies. Poetry manages and locks all requirements, enhancing reproducibility and dependency transparency for all contributors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for TMDB Source Connector\nDESCRIPTION: Uses airbyte-ci to build the docker image for the TMDB source connector. This command creates an image tagged as 'airbyte/source-tmdb:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tmdb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tmdb build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Mailerlite source connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-mailerlite:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailerlite/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailerlite build\n```\n\n----------------------------------------\n\nTITLE: Running Convex Connector Commands Locally\nDESCRIPTION: Series of commands to run various Convex connector operations locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-convex spec\npoetry run destination-convex check --config secrets/config.json\npoetry run destination-convex discover --config secrets/config.json\npoetry run destination-convex read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Salesloft connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Salesloft source connector using the airbyte-ci tool. This creates a Docker image tagged as 'airbyte/source-salesloft:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesloft/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-salesloft build\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Connector Docker Commands\nDESCRIPTION: Set of docker run commands for executing various connector operations including spec, check, discover, and read\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kafka/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-kafka:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-kafka:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-kafka:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-kafka:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing Systeme Source Connector in Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Systeme source connector using airbyte-ci. It verifies the functionality of the connector against predefined test cases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-systeme/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-systeme test\n```\n\n----------------------------------------\n\nTITLE: Running the CI test suite for Everhour connector\nDESCRIPTION: Command to run the full test suite for the Everhour source connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-everhour/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-everhour test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Aha connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aha/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aha test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-webflow test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Callrail connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-callrail/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-callrail test\n```\n\n----------------------------------------\n\nTITLE: Running NASA Connector Test Suite\nDESCRIPTION: Command to execute the full test suite for the NASA source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nasa/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nasa test\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Analytics Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zoho Analytics metadata connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-analytics-metadata-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-analytics-metadata-api test\n```\n\n----------------------------------------\n\nTITLE: Running Standard Connector Commands for RD Station Marketing\nDESCRIPTION: Commands to run the standard source connector operations (spec, check, discover, read) for the RD Station Marketing connector using Docker.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rd-station-marketing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-rd-station-marketing:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rd-station-marketing:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rd-station-marketing:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-rd-station-marketing:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Role Trust Relationship Policy\nDESCRIPTION: JSON policy for setting up a trust relationship for an IAM role. It allows the specified AWS account to assume the role, with an optional external ID condition for enhanced security.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::{your-aws-account-id}:user/{your-username}\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"{your-external-id}\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cart build\n```\n\n----------------------------------------\n\nTITLE: Running Glassfrog Connector Commands via Docker\nDESCRIPTION: These commands demonstrate how to run standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-glassfrog:dev` Docker image. The `check`, `discover`, and `read` commands require mounting a `secrets` directory containing a `config.json` file, and the `read` command also requires an `integration_tests` directory with a `configured_catalog.json`. These commands interact with the connector to retrieve its specification, validate credentials, discover the schema, and read data, respectively.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-glassfrog/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-glassfrog:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-glassfrog:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-glassfrog:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-glassfrog:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Tests for Fastbill Source Connector\nDESCRIPTION: Command to run the full test suite for the Fastbill source connector using airbyte-ci. This validates that the connector is working correctly before publishing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fastbill/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fastbill test\n```\n\n----------------------------------------\n\nTITLE: Running Unleash Source Connector Commands\nDESCRIPTION: Standard commands for running the Unleash source connector as a Docker container. These commands demonstrate how to run spec, check, discover, and read operations with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-unleash/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-unleash:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-unleash:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-unleash:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-unleash:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Available Data Streams Table in Markdown\nDESCRIPTION: Lists all available data streams in the NoCRM connector with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nocrm.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| steps | id | No pagination | ✅ |  ❌  |\n| pipelines | id | No pagination | ✅ |  ❌  |\n| clients_folders | id | No pagination | ✅ |  ❌  |\n| categories | id | No pagination | ✅ |  ❌  |\n| predefined_tags | id | No pagination | ✅ |  ❌  |\n| fields | id | No pagination | ✅ |  ❌  |\n| leads | id | DefaultPaginator | ✅ |  ❌  |\n| follow_ups | id | No pagination | ✅ |  ❌  |\n| users | id | No pagination | ✅ |  ❌  |\n| teams | id | No pagination | ✅ |  ❌  |\n| webhooks | id | No pagination | ✅ |  ❌  |\n| webhook_events | id | No pagination | ✅ |  ❌  |\n| activities | id | No pagination | ✅ |  ❌  |\n| prospecting_lists | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running ERD Generation Script\nDESCRIPTION: This snippet demonstrates how to run the ERD generation script using Poetry. It requires setting the GENAI_API_KEY environment variable and specifies the source path and technical name as parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/erd/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run erd --source-path <source path> --source-technical-name <for example, 'source-facebook-marketing'>\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests Using Poetry and Pytest - Bash\nDESCRIPTION: Executes the suite of unit tests for the connector with pytest inside the Poetry environment. Tests source code under unit_tests to ensure functionality before deployment. Requires that poetry dependencies are already installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image Manually\nDESCRIPTION: Alternative command to build the Docker image directly using docker build command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-chroma:dev .\n```\n\n----------------------------------------\n\nTITLE: Testing Wasabi Stats API Source Connector in Bash\nDESCRIPTION: This command runs the acceptance tests for the Wasabi stats API source connector using airbyte-ci. It verifies the functionality and compatibility of the connector with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wasabi-stats-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wasabi-stats-api test\n```\n\n----------------------------------------\n\nTITLE: Building the Youtube Data Connector Image (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-youtube-data` connector. The resulting image is tagged as `source-youtube-data:dev`, allowing for local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-youtube-data build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Alpha Vantage source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-alpha-vantage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-alpha-vantage build\n```\n\n----------------------------------------\n\nTITLE: Building the RevenueCat Source Connector with airbyte-ci\nDESCRIPTION: Creates a development image (source-revenuecat:dev) that can be used for testing the connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-revenuecat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-revenuecat build\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte Message Mapping to Xata Table - JSON\nDESCRIPTION: Demonstrates a sample Airbyte message object that will be mapped to a Xata table according to the destination connector's conventions. This JSON object includes typical fields such as name, date, and driver, intended to match corresponding columns in the target Xata table. No additional dependencies are required for this static example, but the actual implementation expects table columns and types to match exactly with the JSON keys; unmatched schema will result in insertion failure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/xata.md#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\\n    \\\"name\\\": \\\"Yellow Cab, co\\\",\\n    \\\"date\\\": \\\"2022-05-15\\\",\\n    \\\"driver\\\": \\\"Joe Doe\\\"\\n}\n```\n\n----------------------------------------\n\nTITLE: ServiceNow Stream Configuration List - SAM\nDESCRIPTION: List of supported Software Asset Management (SAM) streams that can be synced using the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-service-now.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- cmdb_model_category\n- sam_sw_product_lifecycle\n- alm_license\n```\n\n----------------------------------------\n\nTITLE: Testing Mailosaur Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Mailosaur source connector using airbyte-ci, validating the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailosaur/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailosaur test\n```\n\n----------------------------------------\n\nTITLE: Version History Table in Markdown\nDESCRIPTION: A markdown table showing the version history of the ActiveCampaign connector, including version numbers, dates, pull request references, and change descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/activecampaign.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject        |\n| :------ | :--------- | :------------------------------------------------------- | :------------- |\n| 0.2.10 | 2025-04-19 | [58273](https://github.com/airbytehq/airbyte/pull/58273) | Update dependencies |\n| 0.2.9 | 2025-04-12 | [57654](https://github.com/airbytehq/airbyte/pull/57654) | Update dependencies |\n| 0.2.8 | 2025-04-05 | [57168](https://github.com/airbytehq/airbyte/pull/57168) | Update dependencies |\n| 0.2.7 | 2025-03-29 | [56626](https://github.com/airbytehq/airbyte/pull/56626) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Visma Economic Source Connector\nDESCRIPTION: Command to run the full test suite for the Visma Economic source connector using airbyte-ci. This is used to verify changes and ensure the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-visma-economic/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-visma-economic test\n```\n\n----------------------------------------\n\nTITLE: Building Source High Level Connector with airbyte-ci\nDESCRIPTION: This command builds a development image (source-high-level:dev) for local testing of the connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-high-level/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-high-level build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite locally using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-merge/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-merge test\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating to Repository - Git CLI - Bash\nDESCRIPTION: This snippet demonstrates how to clone the Airbyte project repository from GitHub using SSH and navigate into the cloned project folder. It requires Git to be installed on your local machine. Replace {YOUR_USERNAME} with your GitHub username. The command outputs a local directory with the full repository content.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:{YOUR_USERNAME}/airbyte.git\\ncd airbyte\n```\n\n----------------------------------------\n\nTITLE: Testing Source-Referralhero Connector with Airbyte-CI\nDESCRIPTION: Command to run acceptance tests for the Referral Hero source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-referralhero/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-referralhero test\n```\n\n----------------------------------------\n\nTITLE: Testing Care Quality Commission Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Care Quality Commission source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-care-quality-commission/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-care-quality-commission test\n```\n\n----------------------------------------\n\nTITLE: Adding dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies to the connector using Poetry dependency management.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for ShopWired Source Connector\nDESCRIPTION: This command executes the acceptance tests for the ShopWired source connector using airbyte-ci. It's used to verify the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopwired/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shopwired test\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte CI\nDESCRIPTION: Command to build the connector using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-linnworks build\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Table in Markdown\nDESCRIPTION: This snippet shows a markdown table that lists the version history of the Help Scout connector, including version numbers, dates, pull request links, and brief descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/help-scout.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request                                         | Subject        |\n|------------------|-------------------|------------------------------------------------------|----------------|\n| 0.0.12 | 2025-04-19 | [58214](https://github.com/airbytehq/airbyte/pull/58214) | Update dependencies |\n| 0.0.11 | 2025-04-12 | [57681](https://github.com/airbytehq/airbyte/pull/57681) | Update dependencies |\n| 0.0.10 | 2025-04-05 | [57031](https://github.com/airbytehq/airbyte/pull/57031) | Update dependencies |\n| 0.0.9 | 2025-03-29 | [56669](https://github.com/airbytehq/airbyte/pull/56669) | Update dependencies |\n| 0.0.8 | 2025-03-22 | [56000](https://github.com/airbytehq/airbyte/pull/56000) | Update dependencies |\n| 0.0.7 | 2025-03-08 | [55474](https://github.com/airbytehq/airbyte/pull/55474) | Update dependencies |\n| 0.0.6 | 2025-03-01 | [54798](https://github.com/airbytehq/airbyte/pull/54798) | Update dependencies |\n| 0.0.5 | 2025-02-22 | [54330](https://github.com/airbytehq/airbyte/pull/54330) | Update dependencies |\n| 0.0.4 | 2025-02-15 | [53833](https://github.com/airbytehq/airbyte/pull/53833) | Update dependencies |\n| 0.0.3 | 2025-02-08 | [53281](https://github.com/airbytehq/airbyte/pull/53281) | Update dependencies |\n| 0.0.2 | 2025-02-01 | [52784](https://github.com/airbytehq/airbyte/pull/52784) | Update dependencies |\n| 0.0.1 | 2025-01-28 | [52614](https://github.com/airbytehq/airbyte/pull/52614) | Initial release by [@pabloescoder](https://github.com/pabloescoder) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Building e-conomic Connector with airbyte-ci in Bash\nDESCRIPTION: This Bash command builds a development image called \"source-e-conomic:dev\" for the connector using the airbyte-ci tool. It is used during local development to prepare the connector for testing or further editing. Dependency: Requires the airbyte-ci CLI to be installed. The key parameter is --name, which should match the connector name. The expected output is a Docker image for the connector. This command must be run from a terminal in the project context.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e-conomic/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-e-conomic build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using Airbyte CI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-notion test\n```\n\n----------------------------------------\n\nTITLE: Building the Glassfrog Connector Docker Image using airbyte-ci\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-glassfrog` connector. It requires `airbyte-ci` to be installed. Upon successful execution, a Docker image tagged `airbyte/source-glassfrog:dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-glassfrog/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-glassfrog build\n```\n\n----------------------------------------\n\nTITLE: Building JobNimbus Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the JobNimbus connector named 'source-jobnimbus:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jobnimbus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jobnimbus build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the US Census source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-us-census/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-us-census build\n```\n\n----------------------------------------\n\nTITLE: Installing Iterable Connector Dependencies with Poetry\nDESCRIPTION: Uses Poetry to install the connector dependencies, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Microsoft Entra ID Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Microsoft Entra ID connector using airbyte-ci. Creates a dev image tagged as 'source-microsoft-entra-id:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-entra-id/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-entra-id build\n```\n\n----------------------------------------\n\nTITLE: Checking Airbyte CI Installation\nDESCRIPTION: Command to verify that Airbyte CI is installed correctly on your system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmake tools.airbyte-ci.check\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte CI\nDESCRIPTION: Command to build the connector using Airbyte's CI tool which creates a Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-zoho-crm build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci (bash)\nDESCRIPTION: This bash command builds the manifest-only 'source-dockerhub' connector Docker image using the airbyte-ci CLI tool. It requires the airbyte-ci utility to be installed and run from the project root. The '--name=source-dockerhub' flag specifies which connector to build. On success, a Docker image tagged as 'airbyte/source-dockerhub:dev' becomes available locally for further testing and use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dockerhub/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dockerhub build\n```\n\n----------------------------------------\n\nTITLE: Defining Taboola API Streams in Markdown\nDESCRIPTION: This snippet outlines the available data streams from the Taboola API, including their names, primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/taboola.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| account | id | No pagination | ✅ |  ❌  |\n| campaigns | id | No pagination | ✅ |  ❌  |\n| campaign_items | id | No pagination | ✅ |  ❌  |\n| audience_rules | id | No pagination | ✅ |  ❌  |\n| conversion_rules | id | No pagination | ✅ |  ❌  |\n| motion_ads | id | No pagination | ✅ |  ❌  |\n| audiences | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Gridly Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Gridly connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gridly test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Discover Command via Docker (Bash)\nDESCRIPTION: Executes the `discover` command within a temporary Docker container (`--rm`) using the `airbyte/source-open-exchange-rates:dev` image. It mounts the local `secrets` directory to `/secrets` and uses the `--config` flag to specify the configuration file. This command connects to the source using the provided config and outputs the available data streams (catalog). Requires Docker, the built connector image, and a valid `secrets/config.json` file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-open-exchange-rates:dev discover --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Testing UpPromote Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the UpPromote source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-uppromote/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-uppromote test\n```\n\n----------------------------------------\n\nTITLE: Testing the Flexmail Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Flexmail source connector using airbyte-ci. This executes the standard test suite to validate connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flexmail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flexmail test\n```\n\n----------------------------------------\n\nTITLE: Running SFTP-JSON Connector as Docker Container\nDESCRIPTION: Series of commands to run connector operations using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-sftp-json:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-sftp-json:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-sftp-json:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-sftp-json:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Fauna Connector Docker Image\nDESCRIPTION: Command to build a Docker image for the Fauna connector for integration testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -t airbyte/source-fauna:dev\n```\n\n----------------------------------------\n\nTITLE: Building the Clazar Source Connector Docker Image using Bash\nDESCRIPTION: Builds the Docker image for the source-clazar connector using the airbyte-ci tool. This command assumes `airbyte-ci` is installed and available in the environment. It creates a local image tagged as `airbyte/source-clazar:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clazar/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clazar build\n```\n\n----------------------------------------\n\nTITLE: Verifying shared_preload_libraries Configuration in PostgreSQL\nDESCRIPTION: Command used in a PostgreSQL client (like psql) to display the current value of the `shared_preload_libraries` configuration parameter. This helps verify that `pg_cron` has been correctly added and is recognized by the running instance after configuration changes and a restart.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nshow shared_preload_libraries\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for SFTP-JSON Connector\nDESCRIPTION: Command to build a Docker image for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-sftp-json build\n```\n\n----------------------------------------\n\nTITLE: Building Picqer Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This shell command builds the 'source-picqer' connector locally by creating a development image tagged as 'source-picqer:dev' using the airbyte-ci command-line interface. Prerequisites include having 'airbyte-ci' installed as described in its documentation. The '--name' flag specifies which connector to build. The command is intended to be run in a terminal from the local development environment. Output is a local Docker image that can be used for testing the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-picqer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-picqer build\n```\n\n----------------------------------------\n\nTITLE: Listing Required Google Cloud Storage Roles - Text\nDESCRIPTION: This snippet enumerates the Google Cloud IAM roles needed for Airbyte installations requiring access to Google Cloud Storage. No dependencies are needed beyond Google Cloud IAM. Users must assign the 'roles/storage.objectCreator' and 'roles/storage.admin' permissions to their service account. This enables Airbyte to create and administer storage objects required for integrations on GCP. The credentials for accessing these roles should be added to your Airbyte deployment configuration, with permissions scoped to appropriate resources.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/infrastructure/gcp.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nroles/storage.objectCreator\\nroles/storage.admin\n```\n\n----------------------------------------\n\nTITLE: Airbyte JSON Record Format for Customer Data\nDESCRIPTION: Shows the structure of Airbyte's JSON Line format for customer records. Each record includes metadata (type, stream, emitted_at, time_extracted) and the actual customer data with fields like id, email, and account details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/stripe_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637450, \"data\": {\"id\": \"cus_I1l0XHrjLYwLR2\", \"object\": \"customer\", \"account_balance\": \"0\", \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 3\", \"discount\": null, \"email\": \"customer3@test.com\", \"invoice_prefix\": \"EC156D8F\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637460, \"data\": {\"id\": \"cus_I1l0INzfeSf2MM\", \"object\": \"customer\", \"account_balance\": \"0\", \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 2\", \"discount\": null, \"email\": \"customer2@test.com\", \"invoice_prefix\": \"D4564D22\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": { \"stream\": \"customers\", \"emitted_at\": 1602637470, \"data\": {\"id\": \"cus_I1l0cRVFy4ZhwQ\", \"object\": \"customer\", \"account_balance\": \"0\", \"created\": \"2020-09-15T16:58:52.000000Z\", \"currency\": null, \"default_source\": null, \"delinquent\": false, \"description\": \"Customer 1\", \"discount\": null, \"email\": \"customer1@test.com\", \"invoice_prefix\": \"92A8C396\", \"livemode\": false, \"metadata\": {}, \"shipping\": null, \"sources\": [], \"subscriptions\": null, \"tax_info\": null, \"tax_info_verification\": null, \"updated\": \"2020-09-15T16:58:52.000000Z\"}, \"time_extracted\": \"2020-09-15T18:01:23.634272Z\"}}\n```\n\n----------------------------------------\n\nTITLE: Defining Locations Stream Schema in JSON\nDESCRIPTION: JSON schema for the locations stream in the Google Analytics connector. It includes geographical metrics such as continent, country, region, and city.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_continent\":\"Americas\",\"ga_subContinent\":\"Northern America\",\"ga_country\":\"United States\",\"ga_region\":\"Iowa\",\"ga_metro\":\"Des Moines-Ames IA\",\"ga_city\":\"Des Moines\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":1,\"ga_sessionsPerUser\":1.0,\"ga_avgSessionDuration\":29.0,\"ga_pageviews\":7,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.666666666666667,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Custom Build Process Configuration\nDESCRIPTION: Example Python module showing how to customize the build process by adding pre and post install hooks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from dagger import Container\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Selected Records Output from Nested Array Wildcards - JSON\nDESCRIPTION: Shows the JSON array resulting from extracting all nested 'record' objects inside each entry of the 'data' array in the previous example, using field_path wildcards in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1\n  },\n  {\n    \"id\": 2\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Split-io Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Split-io source connector using airbyte-ci. It ensures that the connector meets the required functionality and compatibility standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-split-io/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-split-io test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Tempo Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Tempo source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tempo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tempo test\n```\n\n----------------------------------------\n\nTITLE: Displaying JSONL Output Example with Root Level Flattening\nDESCRIPTION: Example showing how data is stored in Azure Blob Storage using JSONL format with root level flattening, where source data is promoted to the root of the object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/azure-blob-storage.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n{ \"_airbyte_raw_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_extracted_at\": \"1622135805000\", \"_airbyte_generation_id\": \"11\", \"_airbyte_meta\": { \"changes\": [], \"sync_id\": 10111 }, \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_extracted_at\": \"1631948170000\", \"_airbyte_generation_id\": \"12\", \"_airbyte_meta\": { \"changes\": [], \"sync_id\": 10112 }, \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } }\n```\n\n----------------------------------------\n\nTITLE: Testing Basecamp Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Basecamp source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-basecamp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-basecamp test\n```\n\n----------------------------------------\n\nTITLE: Running Poplar Connector Acceptance Tests using airbyte-ci (Bash)\nDESCRIPTION: This Bash command executes the acceptance test suite for the `source-poplar` connector using the `airbyte-ci` tool. It validates the connector's functionality against predefined test cases, ensuring it meets Airbyte standards. Requires `airbyte-ci` and a testable connector environment (often involving the image built using the `build` command).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-poplar/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-poplar test\n```\n\n----------------------------------------\n\nTITLE: Testing the source-pingdom Connector using airbyte-ci (Bash)\nDESCRIPTION: This command executes the acceptance tests defined for the `source-pingdom` connector using the `airbyte-ci` tool. It typically runs these tests against the development Docker image (`source-pingdom:dev`) built in the previous step. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pingdom/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pingdom test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-braintree test\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Deepset Connector\nDESCRIPTION: Command to run tests for the Deepset connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Testing source-open-data-dc Connector using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the standard acceptance tests for the `source-open-data-dc` connector. This helps verify the connector's functionality during local development. Requires `airbyte-ci` to be installed and likely depends on a built connector image or source code.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-data-dc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-open-data-dc test\n```\n\n----------------------------------------\n\nTITLE: Installing Google Sheets Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector dependencies using Poetry package manager. It includes development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-youtube-analytics test\n```\n\n----------------------------------------\n\nTITLE: BigMailer Configuration Table\nDESCRIPTION: Configuration parameters required for the BigMailer connector, including API key details.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bigmailer.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. You can create and find it on the API key management page in your BigMailer account. |  |\n```\n\n----------------------------------------\n\nTITLE: Markdown Configuration Table\nDESCRIPTION: Configuration table showing required input parameters for the AssemblyAI integration, including API key and date settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/assemblyai.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. Your AssemblyAI API key. You can find it in the AssemblyAI dashboard at https://www.assemblyai.com/app/api-keys. |  |\n| `start_date` | `string` | Start date.  |  |\n| `subtitle_format` | `string` | Subtitle format. The subtitle format for transcript_subtitle stream | srt |\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_catalog_fetch_event` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_catalog_fetch_event` table, logging events related to fetching actor catalogs. It links an actor and its fetched catalog, storing the configuration hash, actor version at the time of fetching, and timestamps. Foreign keys connect to `actor_catalog` and `actor`. Indexes are placed on `actor_catalog_id` and `actor_id`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name        | Datatype     | Description                                              |\n| ------------------ | ------------ | -------------------------------------------------------- |\n| id                 | UUID         | Primary key. Unique identifier for the fetch event.      |\n| actor_catalog_id   | UUID         | Foreign key referencing `actor_catalog(id)`.             |\n| actor_id           | UUID         | Foreign key referencing `actor(id)`.                     |\n| config_hash        | VARCHAR(32)  | Hash of the configuration at the time of the fetch.      |\n| actor_version      | VARCHAR(256) | Version of the actor definition when the fetch occurred. |\n| created_at         | TIMESTAMP    | Timestamp when the record was created.                   |\n| modified_at        | TIMESTAMP    | Timestamp when the record was last modified.             |\n\n#### `Indexes and Constraints`\n\n- Primary Key: (`id`)\n- Foreign Key: `actor_catalog_id` references `actor_catalog(id)`\n- Foreign Key: `actor_id` references `actor(id)`\n- Index: `actor_catalog_fetch_event_actor_catalog_id_idx` on (`actor_catalog_id`)\n- Index: `actor_catalog_fetch_event_actor_id_idx` on (`actor_id`)\n```\n\n----------------------------------------\n\nTITLE: Running the Read Operation via Docker (Bash)\nDESCRIPTION: Executes the 'read' command for the Gocardless source connector within a Docker container. It mounts both the 'secrets' directory (for 'config.json') and the 'integration_tests' directory (containing 'configured_catalog.json') into the container. It uses the specified configuration and catalog files to read data from the source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gocardless:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Customizing Build Process with Python\nDESCRIPTION: Example of a build_customization.py module to customize the connector build process by adding environment variables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Initial Airbyte Catalog Output from Discover (JSON)\nDESCRIPTION: This JSON object represents an example AirbyteCatalog returned by the `discover` command, potentially with an incorrect schema. It defines a single stream named \"surveys\", specifies its basic JSON schema (properties: id, name, signup_date), indicates support only for \"full_refresh\" sync mode, and defines a source-defined primary key.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/5-discover.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"CATALOG\",\n  \"catalog\": {\n    \"streams\": [\n      {\n        \"name\": \"surveys\",\n        \"json_schema\": {\n          \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n          \"type\": \"object\",\n          \"properties\": {\n            \"id\": { \"type\": [\"null\", \"string\"] },\n            \"name\": { \"type\": [\"null\", \"string\"] },\n            \"signup_date\": { \"type\": [\"null\", \"string\"], \"format\": \"date-time\" }\n          }\n        },\n        \"supported_sync_modes\": [\"full_refresh\"],\n        \"source_defined_primary_key\": [[\"id\"]]\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Connector Commands in Airbyte CI\nDESCRIPTION: Examples of using the airbyte-ci build command to build single or multiple connectors with various options including custom tags and multi-architecture builds.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pokeapi build\nairbyte-ci connectors --name=source-pokeapi build --tag=my-custom-tag\nairbyte-ci connectors --name=source-pokeapi build --architecture=linux/amd64 --architecture=linux/arm64\nairbyte-ci connectors --name=source-pokeapi --name=source-bigquery build\nairbyte-ci connectors --support-level=certified build\nairbyte-ci connectors --modified build\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Scopes for Google Search Console API\nDESCRIPTION: This code snippet shows the OAuth scope required for granting a service account access to the Google Search Console API when using domain-wide delegation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-search-console.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.googleapis.com/auth/webmasters.readonly\n```\n\n----------------------------------------\n\nTITLE: Running Standard Connector Commands with Docker\nDESCRIPTION: Example commands for running the Quickbooks source connector in a Docker container. These commands allow you to view the connector specification, check configurations, discover available data, and read data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-quickbooks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-quickbooks:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-quickbooks:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-quickbooks:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-quickbooks:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests\nDESCRIPTION: Command to execute the connector's test suite using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for source-shortio with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Shortio source connector using airbyte-ci tooling. This creates an image tagged as airbyte/source-shortio:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shortio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shortio build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zenloop test\n```\n\n----------------------------------------\n\nTITLE: Testing the Freshchat Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the source-freshchat connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshchat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshchat test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sunshine/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-sunshine test\n```\n\n----------------------------------------\n\nTITLE: Testing Linear Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Linear source connector using airbyte-ci. Validates connector functionality and integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linear/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linear test\n```\n\n----------------------------------------\n\nTITLE: Testing BigMailer Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the BigMailer source connector using airbyte-ci tool to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bigmailer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bigmailer test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Source Connector using Airbyte CI - Bash\nDESCRIPTION: This bash command runs the complete Airbyte CI test suite for the source-convertkit connector. It validates connector changes in a local or CI environment before submission or publishing. The key dependency is the airbyte-ci CLI with all local test resources prepared. Outputs are standard test result logs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convertkit/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-convertkit test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Public-Apis Connector\nDESCRIPTION: Command to run the full CI test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-public-apis test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for RD Station Marketing Connector\nDESCRIPTION: Command to run the CI test suite for the RD Station Marketing source connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rd-station-marketing/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rd-station-marketing test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-seller-partner test\n```\n\n----------------------------------------\n\nTITLE: Creating a Read-Only Role for Full Sync in Fauna\nDESCRIPTION: This FQL query creates a role named 'airbyte-readonly' with read permissions for all collections, indexes, and a specific collection. This role is required for performing a full sync from Fauna.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/fauna.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true },\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true },\n    },\n    {\n      resource: Collection(\"COLLECTION_NAME\"),\n      actions: { read: true },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: JSON Data Exchange Format for Airbyte\nDESCRIPTION: A collection of JSON records demonstrating Airbyte's data exchange format. The records include exchange rate data, state tracking for incremental syncs, and trace information for stream status. This format is used for communication between Airbyte components during data extraction and loading processes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/exchange_rate_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637589000, \"data\": { \"id\": 1, \"currency\": \"USD\", \"date\": \"2020-08-29T00:00:00Z\", \"NZD\": 0.12, \"HKD\": 2.13}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-08-31\"}}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637689100, \"data\": { \"id\": 1, \"currency\": \"USD\", \"date\": \"2020-08-30T00:00:00Z\", \"NZD\": 1.14, \"HKD\": 7.15}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-09-01\"}}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637789200, \"data\": { \"id\": 2, \"currency\": \"EUR\", \"date\": \"2020-08-31T00:00:00Z\", \"NZD\": 1.14, \"HKD\": 7.15, \"USD\": 10.16}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637889300, \"data\": { \"id\": 2, \"currency\": \"EUR\", \"date\": \"2020-08-31T00:00:00Z\", \"NZD\": 1.14, \"HKD\": 7.99, \"USD\": 10.99}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637989400, \"data\": { \"id\": 2, \"currency\": \"EUR\", \"date\": \"2020-09-01T00:00:00Z\", \"NZD\": 1.14, \"HKD\": 7.15, \"USD\": 10.16}}}\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2020-09-02\"}}}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"exchange_rate\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Defining RequestOption Injection Schema - YAML\nDESCRIPTION: This schema describes the RequestOption object, which specifies how and where a value should be inserted into an API request. It supports multiple injection targets such as request_parameter, header, body_data, and body_json. Requires understanding of schema definitions and how Airbyte components interface with API requests. Key parameters include type, inject_into, and optionally field_name or field_path, especially for nested JSON injections. Outputs conforming objects enable structured value injection according to schema rules.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nRequestOption:\n  description: A component that specifies the key field and where in the request a component's value should be inserted into.\n  type: object\n  required:\n    - type\n    - inject_into\n  properties:\n    type:\n      type: string\n      enum: [RequestOption]\n    inject_into:\n      enum:\n        - request_parameter\n        - header\n        - body_data\n        - body_json\n  oneOf:\n    - properties:\n      field_name:\n        type: string\n        description: The key where the value will be injected. Used for non-nested injection\n      field_path:\n        type: array\n          items: \n            type: string\n          description: For body_json injection, specifies the nested path to the inject values. Particularly useful for GraphQL queries where values need to be injected into the variables object.\n```\n\n----------------------------------------\n\nTITLE: Configuring Onfleet API Connection Parameters\nDESCRIPTION: Configuration table showing the required input parameters for connecting to the Onfleet API. Includes API key and password fields with their descriptions and default values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/onfleet.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use for authenticating requests. You can create and manage your API keys in the API section of the Onfleet dashboard. |  |\n| `password` | `string` | Placeholder Password. Placeholder for basic HTTP auth password - should be set to empty string | x |\n```\n\n----------------------------------------\n\nTITLE: Derived JSON Schema from Test Records\nDESCRIPTION: Displays the JSON Schema automatically derived by the Airbyte Connector Builder based on the provided test records. This schema defines the structure, fields, and data types inferred from the sample data and serves as the 'Detected Schema'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$schema\": \"http://json-schema.org/schema#\",\n  \"properties\": {\n    \"created_at\": {\n      \"type\": \"string\"\n    },\n    \"id\": {\n      \"type\": \"string\"\n    },\n    \"sent_at\": {\n      \"type\": \"string\"\n    },\n    \"status\": {\n      \"type\": \"string\"\n    },\n    \"subject\": {\n      \"type\": \"string\"\n    }\n  },\n  \"type\": \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Statsig API Integration in Markdown\nDESCRIPTION: Describes the configuration parameters required for the Statsig API integration, including the API key and date range for data retrieval.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/statsig.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `end_date` | `string` | End date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running Toggl Source Connector Test Suite with airbyte-ci\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Toggl source connector. It's used to ensure that changes made to the connector pass all tests before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-toggl/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-toggl test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Teradata Source Connector in Java\nDESCRIPTION: This Gradle command builds the Docker image for the Teradata source connector. The resulting image will be tagged as 'airbyte/source-teradata:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teradata/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-teradata:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Retently Source Connector Using airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Retently source connector using the airbyte-ci tool. This creates an image with the tag 'airbyte/source-retently:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-retently/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-retently build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Microsoft Teams connector in Docker. Includes commands for spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-teams/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-microsoft-teams:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-teams:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-microsoft-teams:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-microsoft-teams:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Example S3 Bucket Directory Structure\nDESCRIPTION: Shows a sample directory structure for an S3 bucket to illustrate glob pattern matching examples.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/s3.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmyBucket\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Todoist Source Connector\nDESCRIPTION: Command to run the full test suite for the Todoist source connector using airbyte-ci. This executes all tests locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-todoist/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-todoist test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Watchmode Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Watchmode source connector using airbyte-ci. It validates the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-watchmode/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-watchmode test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Commands to run the connector operations using Docker\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-typesense:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-typesense:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-typesense:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry - Bash\nDESCRIPTION: Installs all dependencies, including development requirements, for the Azure-Table source connector project using Poetry. Requires Poetry to be installed and run from the connector directory. Expected input is none; output is installed packages in the Python environment managed by Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Importing GCP Service Account Credentials\nDESCRIPTION: Bash command to import GCP service account credentials into an environment variable.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GCS_CREDENTIALS=`cat /path/to/credentials.json`\n```\n\n----------------------------------------\n\nTITLE: Installing Tplcentral Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Teradata Destination Connector with Gradle\nDESCRIPTION: Builds the Teradata destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-teradata/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-teradata:build\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Datascope Connector Docker Image - Bash\nDESCRIPTION: This snippet demonstrates building the Docker image for the manifest-only Datascope source connector using the airbyte-ci tool. The command requires airbyte-ci to be installed and expects the --name parameter to match the connector. The resulting Docker image will be tagged as airbyte/source-datascope:dev locally. No input parameters beyond those shown are required, and the output is a Docker image available on the host for further commands.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datascope/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-datascope build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Amplitude connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amplitude/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amplitude test\n```\n\n----------------------------------------\n\nTITLE: Installing Current Directory in Development Mode with pip\nDESCRIPTION: This directive instructs pip to install the current directory (.) in editable/development mode (-e flag). Development mode allows changes to the package to be immediately reflected without reinstallation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-e .\n```\n\n----------------------------------------\n\nTITLE: Building SpotlerCRM Source Connector in Bash\nDESCRIPTION: This command builds a development image of the SpotlerCRM source connector using airbyte-ci. The resulting image is tagged as 'source-spotlercrm:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-spotlercrm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-spotlercrm build\n```\n\n----------------------------------------\n\nTITLE: Testing the Flowlu Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Flowlu source connector. This validates that the connector functions correctly according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flowlu/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flowlu test\n```\n\n----------------------------------------\n\nTITLE: Building the Docker Image with Airbyte CI - Bash\nDESCRIPTION: This Bash command uses Airbyte's custom CI tooling ('airbyte-ci') to build a Docker image for the Okta source connector. 'airbyte-ci' must be installed and configured per project guidelines. After execution, a Docker image tagged 'airbyte/source-okta:dev' will be available locally for containerized deployment and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-okta build\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-only Docker Image with Airbyte CI - Bash\nDESCRIPTION: This bash command uses Airbyte's CLI tool, airbyte-ci, to build the Docker image for the manifest-only Convertkit source connector. Dependencies include having airbyte-ci installed and the source-convertkit manifest available. The expected output is a Docker image tagged as airbyte/source-convertkit:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convertkit/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-convertkit build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Drift Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This bash command runs the full continuous integration test suite for the Drift source connector using airbyte-ci. It validates recent changes by executing defined test pipelines, and is essential before publishing connector updates or submitting pull requests. The command assumes airbyte-ci is installed and configured properly, and that test infrastructure (including environment variables and secrets) is set up as required by Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-drift/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-drift test\n\n```\n\n----------------------------------------\n\nTITLE: Building Confluence Connector Image with airbyte-ci (Bash)\nDESCRIPTION: Uses the `airbyte-ci` tool to build the Docker image for the `source-confluence` connector. This command assumes `airbyte-ci` is installed. The resulting image will be available locally with the tag `airbyte/source-confluence:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-confluence/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-confluence build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Slack Connector\nDESCRIPTION: Command to build a Docker image for the Slack connector using the airbyte-ci tool, which creates an image tagged as airbyte/source-slack:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-slack build\n```\n\n----------------------------------------\n\nTITLE: Running Rocket Chat Connector Docker Commands\nDESCRIPTION: Standard commands for interacting with the Rocket Chat source connector docker container. These commands allow viewing specifications, checking configurations, discovering available data, and reading data using the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rocket-chat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-rocket-chat:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rocket-chat:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rocket-chat:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-rocket-chat:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Available Onfleet Data Streams\nDESCRIPTION: Table describing the available data streams from the Onfleet API, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/onfleet.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| workers | id | No pagination | ✅ |  ❌  |\n| administrators | id | No pagination | ✅ |  ❌  |\n| teams | id | No pagination | ✅ |  ❌  |\n| hubs | id | No pagination | ✅ |  ❌  |\n| tasks | id | DefaultPaginator | ✅ |  ❌  |\n| containers | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Testing the Encharge Source Connector in Airbyte\nDESCRIPTION: Command to run acceptance tests for the Encharge source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-encharge/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-encharge test\n```\n\n----------------------------------------\n\nTITLE: Testing Navan Source Connector in Airbyte\nDESCRIPTION: Command to run acceptance tests for the Navan source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-navan/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-navan test\n```\n\n----------------------------------------\n\nTITLE: Testing Apptivo Connector with Airbyte CI\nDESCRIPTION: Command to execute acceptance tests for the Apptivo source connector using airbyte-ci tooling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apptivo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apptivo test\n```\n\n----------------------------------------\n\nTITLE: Running Standard Source Connector Commands in Docker\nDESCRIPTION: Commands to run standard source connector operations (spec, check, discover, read) for the Flexport connector as a Docker container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flexport/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-flexport:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-flexport:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-flexport:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-flexport:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing abctl via Homebrew for Mac\nDESCRIPTION: Commands to install abctl using Homebrew package manager on Mac. This includes adding the Airbyte tap and installing the tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew tap airbytehq/tap\nbrew install abctl\n```\n\n----------------------------------------\n\nTITLE: Running Vale from Command Line\nDESCRIPTION: Commands for navigating to the Docusaurus directory and running Vale lint checks on specific files or folders.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncd docusaurus\n```\n\nLANGUAGE: bash\nCODE:\n```\nvale ../docs/myfolder/myfile.md\n```\n\nLANGUAGE: bash\nCODE:\n```\nvale ../docs/myfolder/\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Databricks Connector via Gradle\nDESCRIPTION: Command to run acceptance and custom integration tests for the Databricks destination connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databricks/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-databricks:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Testing Mux Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Mux source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mux/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mux test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Serpstat Source Connector using airbyte-ci\nDESCRIPTION: Command to build the Docker image for the source-serpstat connector using the airbyte-ci tool. This creates an image with the tag airbyte/source-serpstat:dev on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-serpstat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-serpstat build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry\nDESCRIPTION: Command to execute unit tests for the connector using pytest through Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Qualaroo Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Qualaroo source connector docker image using the airbyte-ci tool. This creates an image with the tag 'airbyte/source-qualaroo:dev' on your local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qualaroo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-qualaroo build\n```\n\n----------------------------------------\n\nTITLE: Testing Manifest-Only Airbyte Connector with airbyte-ci - Bash\nDESCRIPTION: This bash command executes the acceptance tests for the Phyllo source connector. It uses the airbyte-ci tool to run standardized tests, ensuring the connector meets expected criteria before use or publication. The command requires airbyte-ci to be installed and the connector's name to be specified with --name=source-phyllo. Outputs include test results and any relevant validation errors for diagnostic purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-phyllo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-phyllo test\n```\n\n----------------------------------------\n\nTITLE: Building the Oveit Connector Image using airbyte-ci\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a development Docker image for the `source-oveit` connector. The resulting image will be tagged as `source-oveit:dev` and can be used for local testing and development purposes. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oveit/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oveit build\n```\n\n----------------------------------------\n\nTITLE: Identifying Affected Stream for LinkedIn Ads v5.0.0 Account Users PK Change (Text)\nDESCRIPTION: Specifies the `account_users` stream as being affected by primary key changes in the LinkedIn Ads connector version 5.0.0. This stream requires migration steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n- `account_users`\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Response for Workspace API Query\nDESCRIPTION: This JSON snippet shows a sample response from the Airbyte API when querying workspace information. It includes the workspace ID, name, and data residency setting.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"workspaceId\": \"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826\",\n  \"name\": \"Acme Company\",\n  \"dataResidency\": \"auto\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running the connector locally to perform specification, configuration checking, source discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-twilio spec\npoetry run source-twilio check --config secrets/config.json\npoetry run source-twilio discover --config secrets/config.json\npoetry run source-twilio read --config secrets/config.json --catalog integration_tests/constant_records_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Weatherstack Source Connector\nDESCRIPTION: Command to run the full test suite for the Weatherstack source connector using airbyte-ci. This ensures all changes pass the required tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-weatherstack/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-weatherstack test\n```\n\n----------------------------------------\n\nTITLE: Running the Spec Operation via Docker (Bash)\nDESCRIPTION: Executes the 'spec' command for the Gocardless source connector within a Docker container. It uses the previously built 'airbyte/source-gocardless:dev' image. The '--rm' flag ensures the container is removed after execution. This command outputs the connector's specification.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gocardless:dev spec\n```\n\n----------------------------------------\n\nTITLE: Testing the SendPulse Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the SendPulse connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendpulse/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendpulse test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Track PMS Connector\nDESCRIPTION: This command executes the acceptance tests for the Track PMS connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-track-pms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-track-pms test\n```\n\n----------------------------------------\n\nTITLE: Building Thinkific Source Connector with Airbyte CI\nDESCRIPTION: This command builds a development image of the Thinkific source connector using airbyte-ci. The resulting image is tagged as 'source-thinkific:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-thinkific/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-thinkific build\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Kyriba Connector\nDESCRIPTION: Command to add a new dependency to the Kyriba connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Updating Secrets in GSM\nDESCRIPTION: Command to upload updated configurations to GSM for a specific connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=dev ci_credentials source-bing-ads update-secrets\n```\n\n----------------------------------------\n\nTITLE: Ensuring pipx bin directory is in PATH\nDESCRIPTION: Command to add the pipx binary directory to the system PATH.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\npipx ensurepath\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Tplcentral Connector\nDESCRIPTION: Command to add a new dependency to the Tplcentral connector project using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Yellowbrick Connector\nDESCRIPTION: Command to run unit tests for the Yellowbrick connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-yellowbrick/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-yellowbrick:unitTest\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-coingecko-coins` connector. Requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-coingecko-coins:dev` locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coingecko-coins/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coingecko-coins build\n```\n\n----------------------------------------\n\nTITLE: Building Jotform Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Jotform source connector using airbyte-ci. The resulting image (source-jotform:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jotform/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jotform build\n```\n\n----------------------------------------\n\nTITLE: Executing MsSQL Benchmark Database Setup Script\nDESCRIPTION: Command to run the SQL script that creates and populates benchmark tables in MsSQL. Must be executed after creating a new database and customizing the script parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mssql/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-integrations/connectors/source-mssql\nsqlcmd -S Serverinstance -E -i src/test-performance/sql/create_mssql_benchmarks.sql\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Insightly Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Insightly source connector locally. It's useful for verifying changes and ensuring the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-insightly/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-insightly test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Google Sheets Connector\nDESCRIPTION: Command to execute unit tests for the Google Sheets connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Testing Xsolla Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Xsolla connector using airbyte-ci tool. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xsolla/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xsolla test\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Source-Coassemble Connector - Bash\nDESCRIPTION: This snippet builds the source-coassemble development image using the airbyte-ci tool. It requires the airbyte-ci CLI to be pre-installed as described in the referenced documentation. The --name parameter is set to 'source-coassemble', and the build subcommand creates a dev image tagged as source-coassemble:dev, which can then be used for local testing. No inputs are required beyond the command line, and output is the built Docker image. Ensure Docker and airbyte-ci are installed before running.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coassemble/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coassemble build\n```\n\n----------------------------------------\n\nTITLE: Building the Docker Image Using airbyte-ci - Bash\nDESCRIPTION: This snippet builds the connector's Docker image using the airbyte-ci CLI. airbyte-ci must be installed beforehand. Upon successful execution, a Docker image tagged as airbyte/source-azure-blob-storage:dev is available on the host, enabling containerized usage and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-azure-blob-storage build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Typeform Source Connector\nDESCRIPTION: Use airbyte-ci to build the Docker image for the Typeform source connector. This command creates an image tagged as 'airbyte/source-typeform:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-typeform/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-typeform build\n```\n\n----------------------------------------\n\nTITLE: Replicating Production Data to Development Bucket\nDESCRIPTION: Command to replicate production data to a development bucket for testing. Requires gsutil to be installed and authenticated. Warning: this will remove any existing files in the destination bucket.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nTARGET_BUCKET=<YOUR-DEV_BUCKET> poetry run poe replicate-prod\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Sendgrid Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Sendgrid source connector. The resulting image will be available on the host with the tag 'airbyte/source-sendgrid:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendgrid/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendgrid build\n```\n\n----------------------------------------\n\nTITLE: Defining Statsig Data Streams in Markdown\nDESCRIPTION: Lists available data streams from the Statsig API, including their primary keys, pagination methods, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/statsig.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| audit_logs | id | DefaultPaginator | ✅ |  ❌  |\n| autotunes | id | DefaultPaginator | ✅ |  ✅  |\n| dynamic_configs | id | DefaultPaginator | ✅ |  ✅  |\n| dynamic_configs_versions | id.version | DefaultPaginator | ✅ |  ✅  |\n| dynamic_configs_rules | id | DefaultPaginator | ✅ |  ❌  |\n| events |  | DefaultPaginator | ✅ |  ❌  |\n| events_metrics | id | DefaultPaginator | ✅ |  ✅  |\n| experiments | id | DefaultPaginator | ✅ |  ✅  |\n| gates | id | DefaultPaginator | ✅ |  ✅  |\n| gates_rules | id | DefaultPaginator | ✅ |  ❌  |\n| holdouts | id | DefaultPaginator | ✅ |  ✅  |\n| ingestion_status |  | No pagination | ✅ |  ✅  |\n| ingestion_runs | runID | DefaultPaginator | ✅ |  ✅  |\n| layers | id | DefaultPaginator | ✅ |  ✅  |\n| metrics | id | DefaultPaginator | ✅ |  ❌  |\n| metrics_values |  | DefaultPaginator | ✅ |  ❌  |\n| segments | id | DefaultPaginator | ✅ |  ✅  |\n| segments_ids |  | DefaultPaginator | ✅ |  ❌  |\n| tags | id | DefaultPaginator | ✅ |  ❌  |\n| target_apps | id | DefaultPaginator | ✅ |  ❌  |\n| users |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Manifest-Only Connector with airbyte-ci (Bash)\nDESCRIPTION: Runs the acceptance tests for the source-pennylane connector using the airbyte-ci CLI tool. Assumes the airbyte-ci CLI is installed and configured and that acceptance test configuration is present for the connector. The --name flag specifies the connector to test. Input: None; Output: Test results in console/logs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pennylane/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pennylane test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci\nDESCRIPTION: Command to build the docker image for the source-merge connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-merge/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-merge build\n```\n\n----------------------------------------\n\nTITLE: Documenting TicketTailor Connector Changelog in Markdown\nDESCRIPTION: Markdown table displaying the version history of the TicketTailor connector. It includes version numbers, dates, pull request links, and descriptions of changes made in each release.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tickettailor.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.19 | 2025-04-19 | [58450](https://github.com/airbytehq/airbyte/pull/58450) | Update dependencies |\n| 0.0.18 | 2025-04-12 | [57962](https://github.com/airbytehq/airbyte/pull/57962) | Update dependencies |\n| 0.0.17 | 2025-04-05 | [57409](https://github.com/airbytehq/airbyte/pull/57409) | Update dependencies |\n| 0.0.16 | 2025-03-29 | [56864](https://github.com/airbytehq/airbyte/pull/56864) | Update dependencies |\n| 0.0.15 | 2025-03-22 | [56303](https://github.com/airbytehq/airbyte/pull/56303) | Update dependencies |\n| 0.0.14 | 2025-03-08 | [55578](https://github.com/airbytehq/airbyte/pull/55578) | Update dependencies |\n| 0.0.13 | 2025-03-01 | [55115](https://github.com/airbytehq/airbyte/pull/55115) | Update dependencies |\n| 0.0.12 | 2025-02-22 | [54519](https://github.com/airbytehq/airbyte/pull/54519) | Update dependencies |\n| 0.0.11 | 2025-02-15 | [54068](https://github.com/airbytehq/airbyte/pull/54068) | Update dependencies |\n| 0.0.10 | 2025-02-08 | [53544](https://github.com/airbytehq/airbyte/pull/53544) | Update dependencies |\n| 0.0.9 | 2025-02-01 | [53055](https://github.com/airbytehq/airbyte/pull/53055) | Update dependencies |\n| 0.0.8 | 2025-01-25 | [52404](https://github.com/airbytehq/airbyte/pull/52404) | Update dependencies |\n| 0.0.7 | 2025-01-18 | [51952](https://github.com/airbytehq/airbyte/pull/51952) | Update dependencies |\n| 0.0.6 | 2025-01-11 | [51414](https://github.com/airbytehq/airbyte/pull/51414) | Update dependencies |\n| 0.0.5 | 2024-12-28 | [50750](https://github.com/airbytehq/airbyte/pull/50750) | Update dependencies |\n| 0.0.4 | 2024-12-21 | [50327](https://github.com/airbytehq/airbyte/pull/50327) | Update dependencies |\n| 0.0.3 | 2024-12-14 | [49784](https://github.com/airbytehq/airbyte/pull/49784) | Update dependencies |\n| 0.0.2 | 2024-12-12 | [49374](https://github.com/airbytehq/airbyte/pull/49374) | Update dependencies |\n| 0.0.1 | 2024-11-06 | | Initial release by [@parthiv11](https://github.com/parthiv11) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for Python Firebolt Connector\nDESCRIPTION: Commands to create a virtual environment, activate it, and install dependencies for the Firebolt connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte CI with Pipx\nDESCRIPTION: Installation of Airbyte CI using pipx package manager with editable mode from the pipelines directory\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# From airbyte-ci/connectors/pipelines:\npipx install --editable --force .\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Standard commands for running the Alpha Vantage source connector in Docker, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-alpha-vantage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-alpha-vantage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-alpha-vantage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-alpha-vantage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-alpha-vantage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies with Poetry\nDESCRIPTION: Command to install the connector with development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Executing Iceberg V2 Connector Docker Commands\nDESCRIPTION: Series of Docker commands to run the Iceberg V2 destination connector image for various operations such as spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm airbyte/destination-iceberg-v2:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-iceberg-v2:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-iceberg-v2:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-iceberg-v2:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for TVmaze Schedule Source Connector\nDESCRIPTION: Command to build the Docker image for the TVmaze Schedule source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-tvmaze-schedule:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tvmaze-schedule/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tvmaze-schedule build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mixpanel build\n```\n\n----------------------------------------\n\nTITLE: Customizing Build Process with Python\nDESCRIPTION: Example of build customization module for modifying the build process with pre and post install hooks\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with airbyte-ci - Bash\nDESCRIPTION: This command executes the complete test suite for the Google-Ads connector using Airbyte's airbyte-ci tool. All relevant tests (unit, integration, and acceptance) are launched. Prerequisites include installing airbyte-ci and ensuring test dependencies and environments are configured. The command will output pass/fail status for all tests, supporting connector quality before release.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-ads test\n```\n\n----------------------------------------\n\nTITLE: Running the Delighted Source Connector Test Suite using Bash\nDESCRIPTION: This Bash command executes the full CI test suite for the `source-delighted` connector locally using the `airbyte-ci` tool. It requires `airbyte-ci` to be installed and configured. This command runs various tests to ensure the connector functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-delighted/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-delighted test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Wrike connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wrike/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wrike test\n```\n\n----------------------------------------\n\nTITLE: Running Hubspot Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Hubspot connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hubspot test\n```\n\n----------------------------------------\n\nTITLE: Testing Mailtrap Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Mailtrap source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailtrap/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailtrap test\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Source-Coassemble Connector - Bash\nDESCRIPTION: This snippet executes acceptance tests for the source-coassemble connector using airbyte-ci. Dependencies include a working airbyte-ci CLI installation and any further testing configuration required by Airbyte. The command targets 'source-coassemble' by name and runs test routines to validate the connector's compliance and correctness. Successful execution provides feedback on test outcomes, while failures indicate errors or compliance issues. All prerequisites such as environment variables and test configuration should be met beforehand.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coassemble/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coassemble test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Test Suite with airbyte-ci (Shell)\nDESCRIPTION: Runs the standard Airbyte connector test suite for the 'source-e2e-test-cloud' connector using the 'airbyte-ci' tool. This is typically used as part of the CI/CD process before publishing a new version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=source-e2e-test-cloud test\n```\n\n----------------------------------------\n\nTITLE: Configuring Metabase API Session Token Request Body in JSON\nDESCRIPTION: Example of configuring the JSON request body for retrieving a session token from the Metabase API. The configuration uses User Inputs to pass username and password credentials through key-value pairs in the request body.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\nKey: `username`, Value: `{{ config['username'] }}`\nKey: `password`, Value: `{{ config['password'] }}`\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-box-data-extract build\n```\n\n----------------------------------------\n\nTITLE: Running the Concurrent SurveyMonkey Source via Poetry in Bash\nDESCRIPTION: This command executes the newly-implemented concurrent SurveyMonkey connector using the poetry toolchain. It triggers a `read` operation using specified configuration and catalog files, exercising all concurrency and error handling features implemented in Python. Requirements: Poetry, `source-survey-monkey-demo` package installed, config/catalog files present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/8-concurrency.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running the Check Operation via Docker (Bash)\nDESCRIPTION: Executes the 'check' command for the Gocardless source connector within a Docker container, using the 'airbyte/source-gocardless:dev' image. It mounts the local 'secrets' directory (containing 'config.json') to '/secrets' inside the container and uses the '--config' flag to specify the path to the configuration file. This command validates the provided credentials.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gocardless:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Building a Custom Docker Image for the Connector - Dockerfile\nDESCRIPTION: This Dockerfile example demonstrates how to build a custom connector image based on the existing Outbrain Amplify image, copying integration code and installing it with pip. The snippet is not optimized for production and assumes all runtime dependencies are present in the base image. It avoids changing entrypoint or environment variables, as those are pre-set in the parent image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_5\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM airbyte/source-outbrain-amplify:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Customizing Connector Build with Dagger - Python\nDESCRIPTION: Defines two asynchronous Python functions ('pre_connector_install' and 'post_connector_install') for build customization using Dagger SDK's Container API. Should be placed in a 'build_customization.py' module within the connector directory. These hooks allow mutation of the Docker build context before and after connector dependencies are installed, such as setting environment variables. Prerequisites include the Dagger Python SDK and adherence to async function definitions. Inputs are Container objects; outputs are mutated Container objects. Limitations: functions must be async and import types conditionally for type checking.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Available Data Streams Configuration\nDESCRIPTION: Table defining available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zonka-feedback.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| responses | id | DefaultPaginator | ✅ |  ❌  |\n| workspaces | id | DefaultPaginator | ✅ |  ❌  |\n| surveys | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | emailAddress | DefaultPaginator | ✅ |  ❌  |\n| tasks | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| locations | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Iceberg V2 Connector Acceptance Tests with Gradle\nDESCRIPTION: Gradle command to run acceptance tests for the Iceberg V2 destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-iceberg-v2:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Custom Build Process Example\nDESCRIPTION: Example Python module for customizing the connector build process with pre and post install hooks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from dagger import Container\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Testing Teamtailor Source Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Teamtailor source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teamtailor/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-teamtailor test\n```\n\n----------------------------------------\n\nTITLE: Testing SSH Connection with Private Key in Shell\nDESCRIPTION: This command tests the SSH connection to the remote server using the previously configured private key, verifying that key pair authentication is set up correctly. Replace `<username>` and `<server_ip_address>` with the appropriate values. A successful connection without a password prompt indicates the key pair is working.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nssh <username>@<server_ip_address>\n```\n\n----------------------------------------\n\nTITLE: Example Couchbase Capella Connection String\nDESCRIPTION: An example connection string format for connecting Airbyte to a Couchbase Capella cluster, typically found in the cluster's 'Connect' tab. Replace `<your-cluster-id>` with the actual cluster ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/couchbase.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncouchbases://cb.<your-cluster-id>.cloud.couchbase.com\n```\n\n----------------------------------------\n\nTITLE: Running Connector Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the full test suite for the `source-open-exchange-rates` connector. It verifies the connector's functionality against predefined test cases, ensuring it meets Airbyte standards. Requires `airbyte-ci` to be installed and potentially configured connector credentials in the environment or secrets manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-open-exchange-rates test\n```\n\n----------------------------------------\n\nTITLE: Testing Smartwaiver Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Smartwaiver connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartwaiver/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartwaiver test\n```\n\n----------------------------------------\n\nTITLE: Installing SQLite Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Defining HoorayHR Data Streams in Markdown\nDESCRIPTION: Markdown table listing the available data streams from the HoorayHR API, including their primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hoorayhr.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination    | Supports Full Sync | Supports Incremental |\n| ----------- | ----------- | ------------- | ------------------ | -------------------- |\n| sick-leaves | id          | No pagination | ✅                 | ❌                   |\n| time-off    | id          | No pagination | ✅                 | ❌                   |\n| leave-types | id          | No pagination | ✅                 | ❌                   |\n| users       | id          | No pagination | ✅                 | ❌                   |\n```\n\n----------------------------------------\n\nTITLE: Custom Service Account Role Configuration with Secrets Access\nDESCRIPTION: Example YAML configuration for a custom Kubernetes role with 'secrets' access, necessary for Airbyte 1.6+ operation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrade-service-account.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: roleName    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: pre-install\n    helm.sh/hook-weight: \"-5\"\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"jobs\", \"pods\", \"pods/log\", \"pods/exec\", \"pods/attach\", \"secrets\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n```\n\n----------------------------------------\n\nTITLE: Building the Rootly Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Rootly source connector locally using airbyte-ci. This creates a development image named 'source-rootly:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rootly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rootly build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Generating Registry for Development\nDESCRIPTION: Commands to start Dagster and open the UI to run the generate_registry job.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run dagster dev\nopen http://localhost:3000\n```\n\n----------------------------------------\n\nTITLE: Checking airbyte-ci version\nDESCRIPTION: Command to display the current version of the installed airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\n$ airbyte-ci --version\nairbyte-ci, version 0.1.0\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Wrike connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-wrike:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wrike/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wrike build\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Circa Source Connector - Bash\nDESCRIPTION: This bash snippet builds the development image for the Airbyte Circa source connector using the airbyte-ci utility. It requires the airbyte-ci tool to be installed and available in your system environment. The command creates a dev image tagged as source-circa:dev, which can then be used for local testing or further development steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-circa/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-circa build\n\n```\n\n----------------------------------------\n\nTITLE: Building the Facebook Pages Connector with airbyte-ci\nDESCRIPTION: Command to build the Facebook Pages source connector using the airbyte-ci tool, which creates a Docker image tagged as dev in the local registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-pages/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-facebook-pages build\n```\n\n----------------------------------------\n\nTITLE: Installing Salesforce Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector dependencies using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Testing Castor EDC Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Castor EDC connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-castor-edc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-castor-edc test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Recreation Source Connector\nDESCRIPTION: Command to run the full test suite for the Recreation source connector using airbyte-ci, which executes all configured tests for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recreation/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recreation test\n```\n\n----------------------------------------\n\nTITLE: Running unit tests with Poetry and pytest\nDESCRIPTION: Command to execute unit tests for the connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gcs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Scryfall connector with airbyte-ci in Bash\nDESCRIPTION: Command to build a development image of the Scryfall connector using airbyte-ci. This creates a dev image named 'source-scryfall:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scryfall/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-scryfall build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-couchbase build\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte CI Test Suite for Connector (Bash)\nDESCRIPTION: This command invokes the airbyte-ci test suite for the source-pexels-api connector. It ensures code changes or configuration updates do not break compatibility. Requires airbyte-ci installed and configured. The key parameter is --name for selecting the connector. Outputs test results to the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pexels-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pexels-api test\n```\n\n----------------------------------------\n\nTITLE: Building Onesignal Connector Image with Airbyte CI - Bash\nDESCRIPTION: This snippet shows how to build the Docker image for the manifest-only Onesignal connector using the airbyte-ci tool. Requires airbyte-ci to be installed and available in your PATH. The command downloads dependencies, builds the connector, and tags the resulting image as airbyte/source-onesignal:dev. No additional parameters are required. Output is a locally available Docker image for development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onesignal/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onesignal build\n```\n\n----------------------------------------\n\nTITLE: Building Workflowmax Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Workflowmax source connector using airbyte-ci tool. Creates a dev image tagged as source-workflowmax:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workflowmax/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workflowmax build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the WooCommerce connector in a Docker container. Includes commands for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-woocommerce/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-woocommerce:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-woocommerce:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-woocommerce:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-woocommerce:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Nylas Data Streams Table\nDESCRIPTION: Detailed table showing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nylas.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| calendars | id | DefaultPaginator | ✅ |  ❌  |\n| connectors |  | No pagination | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| contact_groups | id | DefaultPaginator | ✅ |  ❌  |\n| credentials |  | No pagination | ✅ |  ❌  |\n| drafts | id | DefaultPaginator | ✅ |  ✅  |\n| events | id | DefaultPaginator | ✅ |  ✅  |\n| folders | id | DefaultPaginator | ✅ |  ❌  |\n| grants | id | DefaultPaginator | ✅ |  ❌  |\n| messages | id | DefaultPaginator | ✅ |  ✅  |\n| scheduled_messages | schedule_id | No pagination | ✅ |  ❌  |\n| threads | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the CIMIS Connector using Bash\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the acceptance test suite against the `source-cimis` connector. Running these tests ensures the connector meets Airbyte's standards and functions correctly. This step typically follows the build process and requires the `airbyte-ci` tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cimis/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cimis test\n```\n\n----------------------------------------\n\nTITLE: Running Opsgenie Connector Docker Commands - Bash\nDESCRIPTION: This set of commands illustrates running the Opsgenie source connector Docker container in various operational modes, including 'spec', 'check', 'discover', and 'read'. It assumes the presence of a valid configuration file in the 'secrets' directory and, for some commands, an integration tests catalog. Required dependencies include Docker and the previously built 'airbyte/source-opsgenie:dev' image. The '-v' flags mount local directories containing secrets and integration test files into the container. Key outputs include configuration validation and data discovery or reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-opsgenie/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-opsgenie:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-opsgenie:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-opsgenie:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-opsgenie:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog in Markdown\nDESCRIPTION: This code snippet shows how to structure a changelog using markdown syntax. It includes a collapsible section and a table format for presenting version history, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-sheets.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Changelog\n\n<details>\n  <summary>Expand to review</summary>\n\n| Version    | Date       | Pull Request                                             | Subject                                                                                                                                                                |\n|------------|------------|----------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0.9.4 | 2025-03-01 | [54989](https://github.com/airbytehq/airbyte/pull/54989) | Update dependencies |\n| 0.9.3 | 2025-02-22 | [54434](https://github.com/airbytehq/airbyte/pull/54434) | Update dependencies |\n| 0.9.2 | 2025-02-15 | [53720](https://github.com/airbytehq/airbyte/pull/53720) | Update dependencies |\n| 0.9.1 | 2025-02-08 | [51696](https://github.com/airbytehq/airbyte/pull/51696) | Update dependencies |\n| 0.9.0 | 2025-02-04 | [53154](https://github.com/airbytehq/airbyte/pull/53154) | Promoting release candidate 0.9.0-rc.3 to a main version. |\n| 0.9.0-rc.3 | 2025-01-31 | [52682](https://github.com/airbytehq/airbyte/pull/52682) | Fix stream name typing                                                                                                                                                 |\n| 0.9.0-rc.2 | 2025-01-31 | [52671](https://github.com/airbytehq/airbyte/pull/52671) | Fix sheet id encoding                                                                                                                                                  |\n| 0.9.0-rc.1 | 2025-01-30 | [50843](https://github.com/airbytehq/airbyte/pull/50843) | Migrate to low-code                                                                                                                                                    |\n| 0.8.5      | 2025-01-11 | [44270](https://github.com/airbytehq/airbyte/pull/44270) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.8.4      | 2024-12-09 | [48835](https://github.com/airbytehq/airbyte/pull/48835) | Implementing integration tests                                                                                                                                         |\n| 0.7.4      | 2024-09-09 | [45108](https://github.com/airbytehq/airbyte/pull/45108) | Google Sheets API errors now cause syncs to fail                                                                                                                       |\n| 0.7.3      | 2024-08-12 | [43921](https://github.com/airbytehq/airbyte/pull/43921) | Update dependencies                                                                                                                                                    |\n| 0.7.2      | 2024-08-10 | [43544](https://github.com/airbytehq/airbyte/pull/43544) | Update dependencies                                                                                                                                                    |\n| 0.7.1      | 2024-08-03 | [43290](https://github.com/airbytehq/airbyte/pull/43290) | Update dependencies                                                                                                                                                    |\n| 0.7.0      | 2024-08-02 | [42975](https://github.com/airbytehq/airbyte/pull/42975) | Migrate to CDK v4.3.0                                                                                                                                                  |\n| 0.6.3      | 2024-07-27 | [42826](https://github.com/airbytehq/airbyte/pull/42826) | Update dependencies                                                                                                                                                    |\n| 0.6.2      | 2024-07-22 | [41993](https://github.com/airbytehq/airbyte/pull/41993) | Avoid syncs with rate limits being considered successful                                                                                                               |\n| 0.6.1      | 2024-07-20 | [42376](https://github.com/airbytehq/airbyte/pull/42376) | Update dependencies                                                                                                                                                    |\n| 0.6.0      | 2024-07-17 | [42071](https://github.com/airbytehq/airbyte/pull/42071) | Migrate to CDK v3.9.0                                                                                                                                                  |\n| 0.5.11     | 2024-07-13 | [41527](https://github.com/airbytehq/airbyte/pull/41527) | Update dependencies                                                                                                                                                    |\n| 0.5.10     | 2024-07-09 | [41273](https://github.com/airbytehq/airbyte/pull/41273) | Update dependencies                                                                                                                                                    |\n| 0.5.9      | 2024-07-06 | [41005](https://github.com/airbytehq/airbyte/pull/41005) | Update dependencies                                                                                                                                                    |\n| 0.5.8      | 2024-06-28 | [40587](https://github.com/airbytehq/airbyte/pull/40587) | Replaced deprecated AirbyteLogger with logging.Logger                                                                                                                  |\n| 0.5.7      | 2024-06-25 | [40560](https://github.com/airbytehq/airbyte/pull/40560) | Catch an auth error during discover and raise a config error                                                                                                           |\n| 0.5.6      | 2024-06-26 | [40533](https://github.com/airbytehq/airbyte/pull/40533) | Update dependencies                                                                                                                                                    |\n| 0.5.5      | 2024-06-25 | [40505](https://github.com/airbytehq/airbyte/pull/40505) | Update dependencies                                                                                                                                                    |\n| 0.5.4      | 2024-06-22 | [40129](https://github.com/airbytehq/airbyte/pull/40129) | Update dependencies                                                                                                                                                    |\n| 0.5.3      | 2024-06-06 | [39225](https://github.com/airbytehq/airbyte/pull/39225) | [autopull] Upgrade base image to v1.2.2                                                                                                                                |\n| 0.5.2      | 2024-06-02 | [38851](https://github.com/airbytehq/airbyte/pull/38851) | Emit state message at least once per stream                                                                                                                            |\n| 0.5.1      | 2024-04-11 | [35404](https://github.com/airbytehq/airbyte/pull/35404) | Add `row_batch_size` parameter more granular control read records                                                                                                      |\n| 0.5.0      | 2024-03-26 | [36515](https://github.com/airbytehq/airbyte/pull/36515) | Resolve poetry dependency conflict, add record counts to state messages                                                                                                |\n| 0.4.0      | 2024-03-19 | [36267](https://github.com/airbytehq/airbyte/pull/36267) | Pin airbyte-cdk version to `^0`                                                                                                                                        |\n| 0.3.17     | 2024-02-29 | [35722](https://github.com/airbytehq/airbyte/pull/35722) | Add logic to emit stream statuses                                                                                                                                      |\n| 0.3.16     | 2024-02-12 | [35136](https://github.com/airbytehq/airbyte/pull/35136) | Fix license in `pyproject.toml`.                                                                                                                                       |\n| 0.3.15     | 2024-02-07 | [34944](https://github.com/airbytehq/airbyte/pull/34944) | Manage dependencies with Poetry.                                                                                                                                       |\n| 0.3.14     | 2024-01-23 | [34437](https://github.com/airbytehq/airbyte/pull/34437) | Fix header cells filtering                                                                                                                                             |\n| 0.3.13     | 2024-01-19 | [34376](https://github.com/airbytehq/airbyte/pull/34376) | Fix names conversion                                                                                                                                                   |\n| 0.3.12     | 2023-12-14 | [33414](https://github.com/airbytehq/airbyte/pull/33414) | Prepare for airbyte-lib                                                                                                                                                |\n| 0.3.11     | 2023-10-19 | [31599](https://github.com/airbytehq/airbyte/pull/31599) | Base image migration: remove Dockerfile and use the python-connector-base image                                                                                        |\n```\n\n----------------------------------------\n\nTITLE: Building the OpinionStage Airbyte Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet demonstrates how to build the OpinionStage Airbyte connector locally using the airbyte-ci command-line tool. The command generates a development Docker image ('source-opinion-stage:dev') for use in local testing. airbyte-ci must be installed beforehand, and the 'source-opinion-stage' name specifies the target connector. No code customization is needed; running the command in the connector root directory is sufficient.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-opinion-stage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-opinion-stage build\n```\n\n----------------------------------------\n\nTITLE: Building the Google Classroom Connector Docker Image (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a development Docker image for the `source-google-classroom` connector. The resulting image will be tagged as `source-google-classroom:dev`, enabling local testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-classroom/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-classroom build\n```\n\n----------------------------------------\n\nTITLE: Generating JSON QA Report\nDESCRIPTION: Command to run QA checks on all connectors and generate a JSON report file\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa run --connector-directory=airbyte-integrations/connectors --report-path=qa_report.json\n```\n\n----------------------------------------\n\nTITLE: Running SFTP Connector Integration Tests via Gradle in Java\nDESCRIPTION: Command to run acceptance and custom integration tests for the SFTP source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-sftp:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Enabling System-Wide JSON Support in Yellowbrick (SQL)\nDESCRIPTION: This SQL command enables full JSON support (`enable_full_json`) globally for the entire Yellowbrick system. This change requires a configuration reload using `SELECT pg_reload_conf();` afterwards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/yellowbrick.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER SYSTEM SET enable_full_json TO TRUE;\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mixpanel test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Pytest via Poetry - Bash\nDESCRIPTION: This snippet runs the unit tests for the connector using Pytest through Poetry. All test modules located in unit_tests will be executed. Poetry, pytest, and all test dependencies must be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-braintree build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-dataverse test\n```\n\n----------------------------------------\n\nTITLE: Installing CI Credentials in Development Mode\nDESCRIPTION: Command to install ci_credentials in editable mode for development purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npipx install --editable airbyte-ci/connectors/ci_credentials/\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Pull Secrets in values.yaml\nDESCRIPTION: YAML configuration to add image pull secrets for authentication with a private registry in the values.yaml file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  image:\n    registry: ghcr.io/NAMESPACE\n  imagePullSecrets:\n    - name: regcred\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the News API source connector in Docker, including spec, check, discover, and read operations with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-news-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-news-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-news-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-news-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-news-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Auth0 Connector with Airbyte-CI\nDESCRIPTION: Command to build the Auth0 connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-auth0:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-auth0/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-auth0 build\n```\n\n----------------------------------------\n\nTITLE: Example RingCentral API Request URL\nDESCRIPTION: A sample GET request URL for the RingCentral API that retrieves business hours information for an extension. This endpoint demonstrates the use of the tilde (~) operator which represents the authenticated user's account and extension IDs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/ringcentral.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://platform.devtest.ringcentral.com/restapi/v1.0/account/~/extension/~/business-hours\n```\n\n----------------------------------------\n\nTITLE: Running Freshservice Connector Tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Freshservice source connector using airbyte-ci. This validates that all connector functionality is working correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshservice/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshservice test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Testrail Source Connector\nDESCRIPTION: This command runs the acceptance tests for the Testrail source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-testrail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-testrail test\n```\n\n----------------------------------------\n\nTITLE: Running TiDB Source Connector Integration Tests with Gradle\nDESCRIPTION: Command to run acceptance and custom integration tests for the TiDB source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tidb/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-tidb:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Building the Smaily connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the source-smaily connector using the airbyte-ci tool, which creates a Docker image tagged as airbyte/source-smaily:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smaily/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smaily build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Google Sheets Connector\nDESCRIPTION: Command to add a new dependency to the Google Sheets connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up abctl Configuration\nDESCRIPTION: Command to remove all remaining configuration files created by abctl after uninstalling Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nrm -rf ~/.airbyte/abctl\n```\n\n----------------------------------------\n\nTITLE: Using timesince Filter in Jinja2\nDESCRIPTION: Demonstrates the `timesince` filter in Jinja2 (requires Babel library), which returns a human-readable string representing the time difference between a date/datetime object and the current time. The example assumes a variable `date` holds a past date object.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_60\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ date|timesince }}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the XKCD source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xkcd/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xkcd build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Finnworlds Source Connector (Bash)\nDESCRIPTION: This command executes the acceptance tests defined for the `source-finnworlds` connector using the `airbyte-ci` tool. It typically runs against the locally built `dev` image to ensure the connector functions correctly according to Airbyte's standards. This requires `airbyte-ci` and a successful build of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finnworlds/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finnworlds test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Hubplanner Source Connector\nDESCRIPTION: Command to build the Docker image for the Hubplanner source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-hubplanner:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubplanner/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hubplanner build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the docker image for the Zapier Supported Storage connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zapier-supported-storage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zapier-supported-storage build\n```\n\n----------------------------------------\n\nTITLE: Testing Ubidots Source Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Ubidots source connector using airbyte-ci. It ensures that the connector meets the required functionality and performance standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ubidots/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ubidots test\n```\n\n----------------------------------------\n\nTITLE: Executing Convex Connector Unit Tests with Poetry (Bash)\nDESCRIPTION: Runs the unit tests for the Convex source connector using Pytest, executed via `poetry run`. Assumes tests are located in the `unit_tests` directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for SQLite Connector\nDESCRIPTION: Command to build the Docker image for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-sqlite build\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite with airbyte-ci - bash\nDESCRIPTION: This snippet demonstrates running the full CI test suite for the Commercetools connector using the 'airbyte-ci' tool. It ensures that the connector passes all integration and acceptance tests as configured and is critical for validation prior to publishing or merging changes. The command expects 'airbyte-ci' to be installed and properly configured for the repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commercetools/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-commercetools test\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite with airbyte-ci (bash)\nDESCRIPTION: This bash command runs the full test suite for the 'source-dockerhub' Airbyte connector using the airbyte-ci CLI, specifying the connector by name. It requires that airbyte-ci is installed and all test dependencies/configuration are available locally. The command executes automated CI tests to verify connector functionality prior to release or review; all outputs and errors are printed to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dockerhub/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dockerhub test\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the Faker connector with Poetry\nDESCRIPTION: Command to install the Faker connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the Public-Apis Connector with Poetry\nDESCRIPTION: Command to install the connector and its dependencies using Poetry. Includes the development dependencies for local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Survicate Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Survicate connector using airbyte-ci tool. Creates a dev image tagged as source-survicate:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-survicate/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-survicate build\n```\n\n----------------------------------------\n\nTITLE: Service Account Configuration in values.yaml\nDESCRIPTION: Example YAML configuration showing how to define a custom service account in the values.yaml file used for Airbyte deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrade-service-account.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nserviceAccount:\n  name:\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch - Git CLI - Bash\nDESCRIPTION: This snippet shows how to create a new feature or bugfix branch in Git. You must be inside your cloned repository, and {YOUR_USERNAME} and {FEATURE/BUG} should be replaced appropriately. This sets up version control context for your changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b {YOUR_USERNAME}/{FEATURE/BUG}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Onesignal Connector with Airbyte CI - Bash\nDESCRIPTION: This airbyte-ci command executes the full connector test suite for source-onesignal locally. Dependencies include airbyte-ci installed and a local copy of the connector. The command validates implementation correctness and compatibility with Airbyte standards. No input parameters are required besides the connector name. Output consists of test results reporting pass/fail status for integration and unit tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onesignal/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onesignal test\n```\n\n----------------------------------------\n\nTITLE: Building OpenAQ Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a development Docker image for the `source-openaq` connector. The resulting image is tagged as `source-openaq:dev` and can be used for local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openaq/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-openaq build\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: Markdown table showing version history with release dates, pull request references and change descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/README.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| 0.0.2      | 2023-08-21 | [\\#28687](https://github.com/airbytehq/airbyte/pull/28687)  | Version bump only (no other changes).                                                                                                                          |\n| 0.0.1      | 2023-08-08 | [\\#28687](https://github.com/airbytehq/airbyte/pull/28687)  | Initial release for testing.                                                                                                                                   |\n```\n\n----------------------------------------\n\nTITLE: Running QA Checks on All Connectors\nDESCRIPTION: Command to run QA checks on all connectors in the specified directory\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa run --connector-directory=airbyte-integrations/connectors\n```\n\n----------------------------------------\n\nTITLE: Changing to the Connector Directory\nDESCRIPTION: Command to navigate to the Fauna connector directory within the Airbyte project structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/examples/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-integrations/connectors/source-fauna\n```\n\n----------------------------------------\n\nTITLE: Creating a New Workspace with Region Association in Bash\nDESCRIPTION: API request to create a new workspace with data residency settings by sending a POST request to /v1/workspaces/. This associates the workspace with a specific region's data plane.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://example.com/api/public/v1/workspaces\" \\\n  -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"My New Workspace\",\n    \"dataResidency\": \"auto\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Building Beamer Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Beamer source connector using airbyte-ci. Creates a dev image tagged as source-beamer:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-beamer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-beamer build\n```\n\n----------------------------------------\n\nTITLE: Building the Incident.io Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Incident.io source connector for local testing. It uses the airbyte-ci tool to create a Docker image tagged as 'source-incident:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-incident-io/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-incident build\n```\n\n----------------------------------------\n\nTITLE: Testing When I Work Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the When I Work connector using airbyte-ci tool. Verifies the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-when-i-work/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-when-i-work test\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector dependencies including development packages using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Using Escape Characters in CSV Files for GCS Source (CSV)\nDESCRIPTION: Illustrates the use of an escape character (backslash '\\') within a CSV file intended for the Airbyte GCS source. The backslash escapes the double quote before '34\\\"', ensuring it's treated as part of the 'Description' field ('Navy Blue, Bootcut, 34\"') rather than the closing quote. This functionality is controlled by the 'Escape Character' setting in the CSV configuration, which defaults to disabled (blank).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gcs.md#2025-04-23_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI for Aircall Connector\nDESCRIPTION: Command to build the Aircall connector docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aircall/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aircall build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Mailjet Mail source connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-mailjet-mail:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-mail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailjet-mail build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Smartsheets Connector\nDESCRIPTION: Command to build a Docker image for the Smartsheets connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartsheets build\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Insightful Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet demonstrates how to build a development image for the 'source-insightful' Airbyte connector using the airbyte-ci tool. It requires airbyte-ci to be installed and available in your PATH. The parameter '--name=source-insightful' specifies the connector, and 'build' triggers the creation of a local development image tagged 'source-insightful:dev'. The command outputs an image usable for local connector testing; ensure the name matches a configured Airbyte connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-insightful/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-insightful build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Snapchat Marketing Connector using airbyte-ci\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Snapchat Marketing source connector. The resulting image will be tagged as 'airbyte/source-snapchat-marketing:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-snapchat-marketing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-snapchat-marketing build\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: Uses the `airbyte-ci` tool to build the Docker image for the `source-declarative-manifest` connector. The resulting image will be tagged as `airbyte/source-declarative-manifest:dev`. Requires `airbyte-ci` to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-declarative-manifest build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-sqs build\n```\n\n----------------------------------------\n\nTITLE: Building the Ebay Finance Connector with airbyte-ci\nDESCRIPTION: Command to build the connector locally, creating a dev image (source-ebay-finance:dev) for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ebay-finance/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ebay-finance build\n```\n\n----------------------------------------\n\nTITLE: Building Glassflow Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the Glassflow connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-glassflow build\n```\n\n----------------------------------------\n\nTITLE: Nested Object Instantiation Example\nDESCRIPTION: Shows how nested objects with type definitions are instantiated, demonstrating the recursive nature of the factory's instantiation process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/object-instantiation.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": TopLevel\n  \"param\":\n    {\n      \"type\": \"ParamType\"\n      \"k\": \"v\"\n    }\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nTopLevel(param=ParamType(k=\"v\"))\n```\n\n----------------------------------------\n\nTITLE: Building VWO Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the VWO source connector using airbyte-ci. The resulting image is tagged as 'source-vwo:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vwo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vwo build\n```\n\n----------------------------------------\n\nTITLE: Handling Conflict Stream Names in JSON\nDESCRIPTION: These JSON records demonstrate handling of conflicting stream names. The data includes nested structures with the same name as the parent, custom fields, and group information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_name\",\"data\":{\"id\":1,\"conflict_stream_name\":{\"conflict_stream_name\": {\"groups\": \"1\", \"custom_fields\": [{\"id\":1, \"value\":3}, {\"id\":2, \"value\":4}], \"conflict_stream_name\": 3}}},\"emitted_at\":1623861660}}\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_name\",\"data\":{\"id\":2,\"conflict_stream_name\":{\"conflict_stream_name\": {\"groups\": \"2\", \"custom_fields\": [{\"id\":1, \"value\":3}, {\"id\":2, \"value\":4}], \"conflict_stream_name\": 3}}},\"emitted_at\":1623861660}}\n```\n\n----------------------------------------\n\nTITLE: Installing PyAirbyte with pip\nDESCRIPTION: Command to install the PyAirbyte package using pip package manager. This is the first step to start using Airbyte in Python projects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/pyairbyte/getting-started.mdx#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install airbyte\n```\n\n----------------------------------------\n\nTITLE: AWS Secret Manager Policy for Airbyte\nDESCRIPTION: JSON policy to allow Airbyte to interact with AWS Secret Manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:TagResource\",\n                \"secretsmanager:UpdateSecret\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Condition\": {\n                \"ForAllValues:StringEquals\": {\n                    \"secretsmanager:ResourceTag/AirbyteManaged\": \"true\"\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Spec Command via Docker (Bash)\nDESCRIPTION: Executes the `spec` command within a temporary Docker container (`--rm`) using the locally built `airbyte/source-open-exchange-rates:dev` image. This command outputs the connector's specification (spec), defining its required configuration parameters and their structure. Requires Docker and the built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-open-exchange-rates:dev spec\n```\n\n----------------------------------------\n\nTITLE: MongoDB Oplog Status Output Example\nDESCRIPTION: Example output from MongoDB's oplog status check command showing configuration details including size, duration, and event timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mongodb-v2/mongodb-v2-troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfigured oplog size: 10.10546875MB\nlog length start to end: 94400 (26.22hrs)\noplog first event time: Mon Mar 19 2012 13:50:38 GMT-0400 (EDT)\noplog last event time: Wed Oct 03 2012 14:59:10 GMT-0400 (EDT)\nnow: Wed Oct 03 2012 15:00:21 GMT-0400 (EDT)\n```\n\n----------------------------------------\n\nTITLE: Upgrading MSSQL Source with Standard Replication Method\nDESCRIPTION: SQL query to update Microsoft SQL Source configurations in the Airbyte database for connections using the Standard replication method when upgrading from connector version 0.4.17 to 0.4.18+.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"STANDARD\"}', true)\nWHERE actor_definition_id ='b5ea17b1-f170-46dc-bc31-cc744ca984c1' AND (configuration->>'replication_method' = 'STANDARD');\n```\n\n----------------------------------------\n\nTITLE: Installing the Connector with Poetry\nDESCRIPTION: Command to install the Freshcaller connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - Bash\nDESCRIPTION: This snippet demonstrates how to install all development dependencies for the Okta source connector in the current directory using Poetry. Poetry (>=1.7) and Python (>=3.9) must be pre-installed. The 'dev' group of dependencies will be included, preparing the environment for local development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the Genesys Connector with Poetry in Python\nDESCRIPTION: Command to install the Genesys connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building TiDB Source Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the TiDB source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tidb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-tidb:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Clickhouse Unit Tests with Gradle\nDESCRIPTION: Command to execute unit tests for the Clickhouse connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-clickhouse/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-clickhouse:unitTest\n```\n\n----------------------------------------\n\nTITLE: Modifying Cookie SameSite Setting in Airbyte via values.yaml (YAML)\nDESCRIPTION: Shows the YAML configuration snippet for `values.yaml` to change the `SameSite` attribute for Airbyte authentication cookies to `\"None\"` by setting `global.auth.cookieSameSiteSetting`. The default is `\"Strict\"`. Changing to `\"None\"` allows cookies in cross-site contexts but increases CSRF risk and should be used cautiously.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  auth:\n    cookieSameSiteSetting: \"None\"\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Standard / Nested Access Token Field in OAuth Response - Diff/YAML\nDESCRIPTION: This diff illustrates how to update an OAuth manifest so that a non-standard, nested field (super_duper_access_token) is used to extract the access token from the response's data property. The manifest's specification is modified to set the required field, the property definition (with path_in_oauth_response indicating nested access), for correct extraction. Requires Airbyte manifest familiarity and knowledge of the expected API response structure. Input is a YAML manifest; output is proper access token extraction from a nested JSON response.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ different_access_token_field.yml\n    spec:\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n       complete_oauth_output_specification:\n         required:\n-          - access_token\n+          - super_duper_access_token\n         properties:\n-          access_token:\n+          super_duper_access_token:\n             type: string\n             path_in_connector_config:\n               - access_token\n+            path_in_oauth_response:\n+              - data\n+              - super_duper_access_token\n       complete_oauth_server_input_specification:\n```\n\n----------------------------------------\n\nTITLE: Testing the Eventee connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Eventee connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventee/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventee test\n```\n\n----------------------------------------\n\nTITLE: Running Hubspot Connector Unit Tests\nDESCRIPTION: Command to run unit tests for the Hubspot connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Organizing Changelog in Collapsible Details Block - HTML\nDESCRIPTION: This snippet wraps the markdown changelog table within an HTML <details> element, allowing users to expand or collapse the entire changelog section for better readability and navigation efficiency. It uses standard HTML tags <details> and <summary>, compatible with GitHub and various markdown viewers. The block requires that the enclosed markdown is parsed correctly inside the HTML tags. Inputs are the summary label and the changelog table, and the output is an interactive, collapsible block. This snippet does not enforce any constraints beyond HTML compliance.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/notion.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<details>\\n  <summary>Expand to review</summary>\\n\\n| Version | Date       | Pull Request                                             | Subject                                                                                              |\\n|:--------|:-----------|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|\\n| 3.0.7 | 2025-01-11 | [43832](https://github.com/airbytehq/airbyte/pull/43832) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\\n| 3.0.6 | 2024-06-25 | [40498](https://github.com/airbytehq/airbyte/pull/40498) | Fix Pydantic error - add missing type annotation for `max_cursor_time` |\\n| 3.0.5 | 2024-06-04 | [38871](https://github.com/airbytehq/airbyte/pull/38871) | Refactor: use `client_side_incremental` feature |\\n| 3.0.4 | 2024-06-06 | [38798](https://github.com/airbytehq/airbyte/pull/38798) | Implement CheckpointMixin for state handling |\\n| 3.0.3 | 2024-06-06 | [39204](https://github.com/airbytehq/airbyte/pull/39204) | [autopull] Upgrade base image to v1.2.2 |\\n| 3.0.2 | 2024-05-20 | [38266](https://github.com/airbytehq/airbyte/pull/38266) | Replace AirbyteLogger with logging.Logger |\\n| 3.0.1 | 2024-04-24 | [36653](https://github.com/airbytehq/airbyte/pull/36653) | Schema descriptions and CDK 0.80.0 |\\n| 3.0.0   | 2024-04-12 | [35794](https://github.com/airbytehq/airbyte/pull/35974) | Migrate to low-code CDK (python CDK for Blocks stream)                                               |\\n| 2.2.0   | 2024-04-08 | [36890](https://github.com/airbytehq/airbyte/pull/36890) | Unpin CDK version                                                                                    |\\n| 2.1.0   | 2024-02-19 | [35409](https://github.com/airbytehq/airbyte/pull/35409) | Update users stream schema with bot type info fields and block schema with mention type info fields. |\\n| 2.0.9   | 2024-02-12 | [35155](https://github.com/airbytehq/airbyte/pull/35155) | Manage dependencies with Poetry.                                                                     |\\n| 2.0.8   | 2023-11-01 | [31899](https://github.com/airbytehq/airbyte/pull/31899) | Fix `table_row.cells` property in `Blocks` stream                                                    |\\n| 2.0.7   | 2023-10-31 | [32004](https://github.com/airtybehq/airbyte/pull/32004) | Reduce page_size on 504 errors                                                                       |\\n| 2.0.6   | 2023-10-25 | [31825](https://github.com/airbytehq/airbyte/pull/31825) | Increase max_retries on retryable errors                                                             |\\n| 2.0.5   | 2023-10-23 | [31742](https://github.com/airbytehq/airbyte/pull/31742) | Add 'synced_block' property to Blocks schema                                                         |\\n| 2.0.4   | 2023-10-19 | [31625](https://github.com/airbytehq/airbyte/pull/31625) | Fix check_connection method                                                                          |\\n| 2.0.3   | 2023-10-19 | [31612](https://github.com/airbytehq/airbyte/pull/31612) | Add exponential backoff for 500 errors                                                               |\\n| 2.0.2   | 2023-10-19 | [31599](https://github.com/airbytehq/airbyte/pull/31599) | Base image migration: remove Dockerfile and use the python-connector-base image                      |\\n| 2.0.1   | 2023-10-17 | [31507](https://github.com/airbytehq/airbyte/pull/31507) | Add start_date validation checks                                                                     |\\n| 2.0.0   | 2023-10-09 | [30587](https://github.com/airbytehq/airbyte/pull/30587) | Source-wide schema update                                                                            |\\n| 1.3.0   | 2023-10-09 | [30324](https://github.com/airbytehq/airbyte/pull/30324) | Add `Comments` stream                                                                                |\\n| 1.2.2   | 2023-10-09 | [30780](https://github.com/airbytehq/airbyte/pull/30780) | Update Start Date in config to optional field                                                        |\\n| 1.2.1   | 2023-10-08 | [30750](https://github.com/airbytehq/airbyte/pull/30750) | Add availability strategy                                                                            |\\n| 1.2.0   | 2023-10-04 | [31053](https://github.com/airbytehq/airbyte/pull/31053) | Add undeclared fields for blocks and pages streams                                                   |\\n| 1.1.2   | 2023-08-30 | [29999](https://github.com/airbytehq/airbyte/pull/29999) | Update error handling during connection check                                                        |\\n| 1.1.1   | 2023-06-14 | [26535](https://github.com/airbytehq/airbyte/pull/26535) | Migrate from deprecated `authSpecification` to `advancedAuth`                                        |\\n| 1.1.0   | 2023-06-08 | [27170](https://github.com/airbytehq/airbyte/pull/27170) | Fix typo in `blocks` schema                                                                          |\\n| 1.0.9   | 2023-06-08 | [27062](https://github.com/airbytehq/airbyte/pull/27062) | Skip streams with `invalid_start_cursor` error                                                       |\\n| 1.0.8   | 2023-06-07 | [27073](https://github.com/airbytehq/airbyte/pull/27073) | Add empty results handling for stream `Blocks`                                                       |\\n| 1.0.7   | 2023-06-06 | [27060](https://github.com/airbytehq/airbyte/pull/27060) | Add skipping 404 error in `Blocks` stream                                                            |\\n| 1.0.6   | 2023-05-18 | [26286](https://github.com/airbytehq/airbyte/pull/26286) | Add `parent` field to `Blocks` stream                                                                |\\n| 1.0.5   | 2023-05-01 | [25709](https://github.com/airbytehq/airbyte/pull/25709) | Fixed `ai_block is unsupported by API` issue, while fetching `Blocks` stream                         |\\n| 1.0.4   | 2023-04-11 | [25041](https://github.com/airbytehq/airbyte/pull/25041) | Improve error handling for API /search                                                               |\\n| 1.0.3   | 2023-03-02 | [22931](https://github.com/airbytehq/airbyte/pull/22931) | Specified date formatting in specification                                                           |\\n| 1.0.2   | 2023-02-24 | [23437](https://github.com/airbytehq/airbyte/pull/23437) | Add retry for 400 error (validation_error)                                                           |\\n| 1.0.1   | 2023-01-27 | [22018](https://github.com/airbytehq/airbyte/pull/22018) | Set `AvailabilityStrategy` for streams explicitly to `None`                                          |\\n| 1.0.0   | 2022-12-19 | [20639](https://github.com/airbytehq/airbyte/pull/20639) | Fix `Pages` stream schema                                                                            |\\n| 0.1.10  | 2022-09-28 | [17298](https://github.com/airbytehq/airbyte/pull/17298) | Use \"Retry-After\" header for backoff                                                                 |\\n| 0.1.9   | 2022-09-16 | [16799](https://github.com/airbytehq/airbyte/pull/16799) | Migrate to per-stream state                                                                          |\\n| 0.1.8   | 2022-09-05 | [16272](https://github.com/airbytehq/airbyte/pull/16272) | Update spec description to include working timestamp example                                         |\\n| 0.1.7   | 2022-07-26 | [15042](https://github.com/airbytehq/airbyte/pull/15042) | Update `additionalProperties` field to true from shared schemas                                      |\\n| 0.1.6   | 2022-07-21 | [14924](https://github.com/airbytehq/airbyte/pull/14924) | Remove `additionalProperties` field from schemas and spec                                            |\\n| 0.1.5   | 2022-07-14 | [14706](https://github.com/airbytehq/airbyte/pull/14706) | Added OAuth2.0 authentication                                                                        |\\n| 0.1.4   | 2022-07-07 | [14505](https://github.com/airbytehq/airbyte/pull/14505) | Fixed bug when normalization didn't run through                                                      |\\n| 0.1.3   | 2022-04-22 | [11452](https://github.com/airbytehq/airbyte/pull/11452) | Use pagination for User stream                                                                       |\\n| 0.1.2   | 2022-01-11 | [9084](https://github.com/airbytehq/airbyte/pull/9084)   | Fix documentation URL                                                                                |\\n| 0.1.1   | 2021-12-30 | [9207](https://github.com/airbytehq/airbyte/pull/9207)   | Update connector fields title/description                                                            |\\n| 0.1.0   | 2021-10-17 | [7092](https://github.com/airbytehq/airbyte/pull/7092)   | Initial Release                                                                                      |\\n\\n</details>\n```\n\n----------------------------------------\n\nTITLE: Configuring Taboola API Source in Markdown\nDESCRIPTION: This snippet describes the configuration parameters required for the Taboola API source connector. It includes the client_id, account_id, and client_secret fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/taboola.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id` | `string` | Client ID.  |  |\n| `account_id` | `string` | Account ID. The ID associated with your taboola account |  |\n| `client_secret` | `string` | Client secret.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Trello Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Trello source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trello/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-trello test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Trustpilot Source Connector\nDESCRIPTION: Command to build the Docker image for the Trustpilot source connector using airbyte-ci. This creates an image tagged as airbyte/source-trustpilot:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trustpilot/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-trustpilot build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry - Bash\nDESCRIPTION: This Bash command runs all unit tests for the Google-Ads source connector project using Poetry to invoke pytest within the virtual environment. The command must be executed from the connector's root directory. Output will detail test results for all Python tests under the unit_tests directory. This requires that pytest and test dependencies have already been installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Displaying Inline API Parameter Names and Values - Markdown\nDESCRIPTION: Displays API parameter names, bearer tokens, and example values using Markdown inline code formatting for greater clarity in the documentation. No dependencies or prerequisites are required as this is purely for documentation and readability. Inputs are strings representing parameter names and values; outputs are inline code snippets visible in the rendered README. This is not intended for execution and acts solely as a documentation aid.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/workflowmax.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`source-workflowmax`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`https://app.swaggerhub.com/apis-docs/WorkflowMax-BlueRock/WorkflowMax-BlueRock-OpenAPI3/0.1#/`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`Workflowmax`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`access token`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`Bearer`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`Authorization code`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`https://oauth.workflowmax2.com/oauth/authorize`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`https://oauth.workflowmax2.com/oauth/token`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`openid profile email workflowmax offline_access`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`1`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`Send as Basic Auth Header`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`Get New Access Token`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`The Source`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`api_key_2`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`string`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`account_id`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`start_date`\n```\n\n----------------------------------------\n\nTITLE: Running Timely Source Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Timely source connector using airbyte-ci. This ensures all changes pass the required tests before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-timely/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-timely test\n```\n\n----------------------------------------\n\nTITLE: Building the Shortcut Connector with airbyte-ci\nDESCRIPTION: Command to build the Shortcut connector locally, creating a dev image (source-shortcut:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shortcut/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shortcut build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations using Docker containers\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-linnworks:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linnworks:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linnworks:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-linnworks:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Allowed Hosts in YAML\nDESCRIPTION: YAML configuration for specifying allowed hosts in the connector metadata file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  allowedHosts:\n    hosts:\n      - \"api.github.com\"\n      - \"*.hubspot.com\"\n```\n\n----------------------------------------\n\nTITLE: Using groupby Filter in Jinja2\nDESCRIPTION: Demonstrates the `groupby` filter in Jinja2, which groups items from a sequence based on a common attribute. The example groups a list of dictionaries by the 'name' attribute.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_27\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [{'name': 'a'}, {'name': 'b'}]|groupby('name') }}\n```\n\n----------------------------------------\n\nTITLE: Building Care Quality Commission Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Care Quality Commission source connector using airbyte-ci tool. Creates a dev image tagged as source-care-quality-commission:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-care-quality-commission/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-care-quality-commission build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Chartmogul Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet shows how to execute the complete set of tests for the Chartmogul source connector using the Airbyte project CI tool. The command `airbyte-ci connectors --name=source-chartmogul test` runs the test suite defined for the connector, helping to validate changes and ensure connector functionality before publishing. This depends on the `airbyte-ci` CLI tool and relevant test configurations in the Airbyte codebase. The primary output is the result of the test run presented on the command line.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chartmogul/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chartmogul test\n```\n\n----------------------------------------\n\nTITLE: Installing Surveymonkey Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for S3 Configuration via kubectl\nDESCRIPTION: This bash command creates a Kubernetes secret for Airbyte configuration using kubectl. It sets various credentials including license key, data plane credentials, and AWS/S3 access keys.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic airbyte-config-secrets \\\n  --from-literal=license-key='' \\\n  --from-literal=data_plane_client_id='' \\\n  --from-literal=data_plane_client_secret='' \\\n  --from-literal=s3-access-key-id='' \\\n  --from-literal=s3-secret-access-key='' \\\n  --from-literal=aws-secret-manager-access-key-id='' \\\n  --from-literal=aws-secret-manager-secret-access-key='' \\\n  --namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Building Duckdb Connector with airbyte-ci\nDESCRIPTION: Command to build the Duckdb connector using the airbyte-ci tool, which creates a Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-duckdb build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Kyve Connector CI Test Suite with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Kyve connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kyve test\n```\n\n----------------------------------------\n\nTITLE: Running Specific QA Checks on Selected Connectors\nDESCRIPTION: Command to run specific QA checks (icon and Python base image) on selected connectors\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa run --name=source-faker --name=source-google-sheets --check=CheckConnectorIconIsAvailable --check=CheckConnectorUsesPythonBaseImage\n```\n\n----------------------------------------\n\nTITLE: Building Java CDK with Gradle\nDESCRIPTION: Command to build and test the Java CDK using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-cdk:java:airbyte-cdk:build\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte DBT Source Connector via CLI - Bash\nDESCRIPTION: This snippet demonstrates how to build a development image for the `source-dbt` connector using the `airbyte-ci` CLI. The CLI command compiles the connector code and packages it as a Docker image tagged `source-dbt:dev`, which can then be used for local development and manual testing. Requires `airbyte-ci` to be installed and configured; the `--name=source-dbt` flag targets the manifest-only connector defined in this directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dbt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dbt build\n```\n\n----------------------------------------\n\nTITLE: Building Apptivo Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Apptivo source connector using airbyte-ci tooling. Creates a dev image tagged as source-apptivo:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apptivo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apptivo build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands as Docker Container\nDESCRIPTION: Standard commands for running the Pypi source connector in a Docker container. These commands allow you to view the connector specification, check the connection configuration, discover available data, and read data using a configured catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pypi/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pypi:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pypi:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pypi:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pypi:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuration Table\nDESCRIPTION: Configuration parameters table showing required inputs including API key and statistics-related parameters for the Mention connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mention.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `stats_interval` | `string` | Statistics Interval. Periodicity of statistics returned. it may be daily(P1D), weekly(P1W) or monthly(P1M). | P1D |\n| `stats_start_date` | `string` | Statistics Start Date.  |  |\n| `stats_end_date` | `string` | Statistics End Date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Managing Dependencies with Poetry - Bash\nDESCRIPTION: This command adds a new package dependency to the project using Poetry. Replace '<package-name>' with the actual desired package. The command updates 'pyproject.toml' and 'poetry.lock', ensuring that dependency management adheres to project standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Testing ZapSign Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the ZapSign source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zapsign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zapsign test\n```\n\n----------------------------------------\n\nTITLE: Installing Surveycto Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building the source-getgist connector locally using airbyte-ci\nDESCRIPTION: Command to build a development image of the source-getgist connector for local testing purposes. Creates a source-getgist:dev image that can be used for development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-getgist/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-getgist build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Ashby source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ashby/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ashby build\n```\n\n----------------------------------------\n\nTITLE: Workday Changelog Details\nDESCRIPTION: Markdown details section containing version history of the connector showing versions 0.2.0 and 0.1.0.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-workday.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<details>\n  <summary>Expand to review</summary>\n\nThe connector is still incubating, this section only exists to satisfy Airbyte's QA checks.\n\n- 0.2.0\n- 0.1.0\n\n</details>\n```\n\n----------------------------------------\n\nTITLE: Testing the Lightspeed Retail Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet shows how to execute acceptance tests for the Lightspeed Retail connector using the airbyte-ci CLI. It assumes airbyte-ci is installed and the connector exists locally. The command runs the test suite specifically for the connector named source-lightspeed-retail, enabling developers to verify functionality and correctness before release. The major prerequisite is having the correct environment and dependencies for the test suite; output consists of the test results as reported by airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lightspeed-retail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lightspeed-retail test\n```\n\n----------------------------------------\n\nTITLE: Changelog Version Table in Markdown\nDESCRIPTION: A markdown table displaying version history with dates, pull request references, and change descriptions spanning from version 0.3.14 to 0.7.11.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amplitude.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version    | Date       | Pull Request                                             | Subject                                                                                                                                                                |\n|:-----------|:-----------| :------------------------------------------------------- |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0.7.11 | 2025-04-19 | [58264](https://github.com/airbytehq/airbyte/pull/58264) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Installs the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Leadfeeder Source Connector for Local Development\nDESCRIPTION: This command builds a development image of the Leadfeeder source connector using airbyte-ci. The resulting image (source-leadfeeder:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-leadfeeder/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-leadfeeder build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Thinkific Courses Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Thinkific Courses source connector using airbyte-ci. It helps ensure the connector's functionality meets the required standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-thinkific-courses/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-thinkific-courses test\n```\n\n----------------------------------------\n\nTITLE: Building Guru Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Guru source connector using airbyte-ci. The resulting image is tagged as 'source-guru:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-guru/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-guru build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Container\nDESCRIPTION: Commands to run various connector operations using the Docker container, mounting local files for configuration and catalogs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-python-http-tutorial:dev spec\ndocker run --rm -v $(pwd)/sample_files:/sample_files airbyte/source-python-http-tutorial:dev check --config /sample_files/config.json\ndocker run --rm -v $(pwd)/sample_files:/sample_files airbyte/source-python-http-tutorial:dev discover --config /sample_files/config.json\ndocker run --rm -v $(pwd)/sample_files:/sample_files -v $(pwd)/sample_files:/sample_files airbyte/source-python-http-tutorial:dev read --config /sample_files/config.json --catalog /sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example HTTPAPIBudget Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure an 'api_budget' using the HTTPAPIBudget type. It specifies custom header names for rate limit information ('X-RateLimit-Reset', 'X-RateLimit-Remaining'), defines a status code (429) that signifies a rate limit hit, and includes examples of Unlimited, Fixed Window (1000 calls/hour for GET /users), and Moving Window (100 calls/minute for POST /users) policies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/rate-limit-api-budget.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napi_budget:\n  type: \"HTTPAPIBudget\"\n  ratelimit_reset_header: \"X-RateLimit-Reset\"\n  ratelimit_remaining_header: \"X-RateLimit-Remaining\"\n  status_codes_for_ratelimit_hit: [ 429 ]\n  policies:\n    - type: \"UnlimitedCallRatePolicy\"\n      matchers: []\n    - type: \"FixedWindowCallRatePolicy\"\n      period: \"PT1H\"\n      call_limit: 1000\n      matchers:\n        - method: \"GET\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/users\"\n    - type: \"MovingWindowCallRatePolicy\"\n      rates:\n        - limit: 100\n          interval: \"PT1M\"\n      matchers:\n        - method: \"POST\"\n          url_base: \"https://api.example.com\"\n          url_path_pattern: \"^/users\"\n```\n```\n\n----------------------------------------\n\nTITLE: Making abctl Executable on Linux\nDESCRIPTION: Command to make the extracted abctl executable accessible on Linux by setting execution permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x abctl/abctl\n```\n\n----------------------------------------\n\nTITLE: Documenting HoorayHR Connector Changelog in Markdown\nDESCRIPTION: Markdown table detailing the version history of the HoorayHR connector, including dates, pull request references, and descriptions of changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/hoorayhr.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request | Subject                                                                                             |\n| ------- | ---------- | ------------ | --------------------------------------------------------------------------------------------------- |\n| 0.1.16 | 2025-04-19 | [58169](https://github.com/airbytehq/airbyte/pull/58169) | Update dependencies |\n| 0.1.15 | 2025-04-12 | [57735](https://github.com/airbytehq/airbyte/pull/57735) | Update dependencies |\n| 0.1.14 | 2025-04-05 | [57070](https://github.com/airbytehq/airbyte/pull/57070) | Update dependencies |\n| 0.1.13 | 2025-03-29 | [56652](https://github.com/airbytehq/airbyte/pull/56652) | Update dependencies |\n| 0.1.12 | 2025-03-22 | [56047](https://github.com/airbytehq/airbyte/pull/56047) | Update dependencies |\n| 0.1.11 | 2025-03-08 | [55439](https://github.com/airbytehq/airbyte/pull/55439) | Update dependencies |\n| 0.1.10 | 2025-03-01 | [54756](https://github.com/airbytehq/airbyte/pull/54756) | Update dependencies |\n| 0.1.9 | 2025-02-22 | [54350](https://github.com/airbytehq/airbyte/pull/54350) | Update dependencies |\n| 0.1.8 | 2025-02-15 | [53840](https://github.com/airbytehq/airbyte/pull/53840) | Update dependencies |\n| 0.1.7 | 2025-02-08 | [53294](https://github.com/airbytehq/airbyte/pull/53294) | Update dependencies |\n| 0.1.6 | 2025-02-01 | [52762](https://github.com/airbytehq/airbyte/pull/52762) | Update dependencies |\n| 0.1.5 | 2025-01-25 | [52250](https://github.com/airbytehq/airbyte/pull/52250) | Update dependencies |\n| 0.1.4 | 2025-01-18 | [51784](https://github.com/airbytehq/airbyte/pull/51784) | Update dependencies |\n| 0.1.3 | 2025-01-11 | [51151](https://github.com/airbytehq/airbyte/pull/51151) | Update dependencies |\n| 0.1.2 | 2024-12-28 | [50598](https://github.com/airbytehq/airbyte/pull/50598) | Update dependencies |\n| 0.1.1 | 2024-12-21 | [50110](https://github.com/airbytehq/airbyte/pull/50110) | Update dependencies |\n| 0.1.0   | 2024-12-17 |              | Added some more documentation and icon for HoorayHR by [@JoeriSmits](https://github.com/JoeriSmits) |\n| 0.0.1   | 2024-12-17 |              | Initial release by [@JoeriSmits](https://github.com/JoeriSmits) via Connector Builder               |\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Build Process Module\nDESCRIPTION: Example of a build_customization.py module that can be used to customize the build process for the connector by adding environment variables.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Configuring PyPI Publishing in Connector Metadata (YAML)\nDESCRIPTION: Example YAML configuration snippet demonstrating how to enable PyPI publishing for an Airbyte connector within its `metadata.yaml` file. The `remoteRegistries.pypi` section specifies `enabled: true` and the desired `packageName` for the PyPI registry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nremoteRegistries:\n  pypi:\n    enabled: true\n    packageName: airbyte-source-pokeapi\n```\n\n----------------------------------------\n\nTITLE: Handling Scopes as OAuth Consent Query Parameters - Diff/YAML\nDESCRIPTION: This diff snippet shows the necessary changes to add a scope parameter to the OAuth consent_url in the manifest, both in the consent URL and as a manifest property, to ensure the end-user is presented with and can approve the proper access scopes. Requires familiarity with the OAuth standard flow and Airbyte manifest customization. Inputs are manifest YAML and scope strings; outputs are manifests enabling scope approval in consent screens.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n--- base_oauth.yml\n+++ scopes.yml\n@@ -80,10 +80,10 @@ spec:\n     oauth_config_specification:\n       oauth_connector_input_specification:\n         consent_url: >-\n-          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{\n-          redirect_uri_value }}&state={{ state_value }}\n+          https://yourconnectorservice.com/oauth/consent?client_id={{client_id_value}}&redirect_uri={{ redirect_uri_value }}&state={{ state_value }}&scope={{scope_value}}\n         access_token_url: >-\n           https://yourconnectorservice.com/oauth/token?client_id={{client_id_value}}&client_secret={{client_secret_value}}&code={{auth_code_value}}\n+        scope: my_scope_A:read,my_scope_B:read\n       complete_oauth_output_specification:\n         required:\n           - access_token\n```\n\n----------------------------------------\n\nTITLE: Building the Onfleet Connector with Airbyte CI - Bash\nDESCRIPTION: This Bash command compiles or builds a development image of the Onfleet connector using the Airbyte CI tool. It is intended for developers who want to test or modify the connector locally before deploying it. The command requires the airbyte-ci tool to be installed and takes the connector name as a flag, outputting a Docker image tagged as 'source-onfleet:dev'. No arguments beyond the connector's name are required.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onfleet/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onfleet build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for SQLite Connector\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cart test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wikipedia-pageviews/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wikipedia-pageviews test\n```\n\n----------------------------------------\n\nTITLE: Testing Ticketmaster Source Connector in Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Ticketmaster source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ticketmaster/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ticketmaster test\n```\n\n----------------------------------------\n\nTITLE: Building Source Inflowinventory Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the source-inflowinventory connector using airbyte-ci. The resulting image (source-inflowinventory:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-inflowinventory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-inflowinventory build\n```\n\n----------------------------------------\n\nTITLE: Running Vectara Connector Tests\nDESCRIPTION: Commands to run the full test suite, unit tests, and integration tests for the Vectara connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-vectara test\npoetry run pytest -s unit_tests\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Displaying Multiple JSON Objects Example for Azure Blob Storage\nDESCRIPTION: Example showing two JSON objects with user data as they appear from a source before being written to Azure Blob Storage using JSONL format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/azure-blob-storage.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}\n{\n  \"user_id\": 456,\n  \"name\": {\n    \"first\": \"Jane\",\n    \"last\": \"Roe\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Mapping JSON Time Format to Avro Logical Type\nDESCRIPTION: Shows the direct mapping of the JSON `time` format to an Avro `long` type with the `time-micros` logical type. The long represents the number of microseconds after midnight. This is an intermediate representation before the final nullable schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"long\",\n  \"logicalType\": \"time-micros\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-looker/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Pocket Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet runs the Airbyte connector test suite for the Pocket source using airbyte-ci. It assumes airbyte-ci is installed and the environment is configured appropriately. The key parameter is the connector's name (--name=source-pocket), and running this command executes all automated tests defined for the connector, returning pass/fail results in the console. This command ensures code quality before updates or publication but requires test dependencies as defined by the Airbyte platform.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pocket/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pocket test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Ruddr Connector with airbyte-ci\nDESCRIPTION: Command to build the Ruddr connector using airbyte-ci, which creates a development image called 'source-ruddr:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ruddr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ruddr build\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Dockerfile for the Connector\nDESCRIPTION: Example Dockerfile to create a custom build of the Fauna connector based on the official image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_11\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-fauna:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Building Faker connector Docker image with airbyte-ci\nDESCRIPTION: Command to build a Docker image for the Faker connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-faker build\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Environment Variables in YAML\nDESCRIPTION: YAML configuration for setting custom environment variables for connector testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nconnector_image: \"airbyte/source-pokeapi\"\ncustom_environment_variables:\n  my_custom_environment_variable: value\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with airbyte-ci (Bash)\nDESCRIPTION: Executes the complete Airbyte continuous integration test suite locally for the `source-commcare` connector using the `airbyte-ci` tool. This helps ensure the connector meets Airbyte standards before publishing. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-commcare test\n```\n\n----------------------------------------\n\nTITLE: Starting Docusaurus Development Server\nDESCRIPTION: Command to run the Docusaurus development server locally. This enables live-reloading so changes to documentation source files are immediately visible in the browser.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docusaurus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm start # any changes will automatically be reflected in your browser!\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to SFTP-JSON Connector\nDESCRIPTION: Command to add a new package dependency using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building DynamoDB Connector Docker Image\nDESCRIPTION: Command to build the connector's Docker image using Gradle. Creates an image tagged as 'airbyte/destination-dynamodb:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dynamodb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dynamodb:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Importing ConnectorRegistry Component in JavaScript\nDESCRIPTION: Imports the ConnectorRegistry React component from the site components directory to display available source connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/README.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport ConnectorRegistry from '@site/src/components/ConnectorRegistry';\n```\n\n----------------------------------------\n\nTITLE: Building the Everhour connector Docker image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Everhour source connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-everhour/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-everhour build\n```\n\n----------------------------------------\n\nTITLE: Configuring SpotlerCRM Connector in Markdown\nDESCRIPTION: Defines the configuration parameters for the SpotlerCRM connector, specifically the access token required for API authentication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/spotlercrm.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `access_token` | `string` | Access Token. Access Token to authenticate API requests. Generate it by logging into your CRM system, navigating to Settings / Integrations / API V4, and clicking &#39;generate new key&#39;. |  |\n```\n\n----------------------------------------\n\nTITLE: AWS CloudTrail Features Support\nDESCRIPTION: Markdown table outlining the supported features of the AWS CloudTrail connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aws-cloudtrail.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| Namespaces        | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Running Metabase Source Connector Commands via Docker (Bash)\nDESCRIPTION: These commands demonstrate how to execute standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built `airbyte/source-metabase:dev` Docker image. The commands map local directories (`secrets`, `integration_tests`) containing configuration and catalog files into the container. Ensure the `secrets/config.json` file exists and conforms to the connector's spec.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-metabase/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-metabase:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-metabase:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-metabase:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-metabase:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Klaus API Source Connector Commands\nDESCRIPTION: Commands to run standard source connector operations such as spec, check, discover, and read using the Klaus API source connector Docker image. These commands assume the existence of a secrets directory with configuration files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaus-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-klaus-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klaus-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-klaus-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-klaus-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte State and Stream Status Records\nDESCRIPTION: Examples of state and trace records used for tracking sync state and stream status. Includes a global state record with a start date and trace records indicating completion status for each of the data streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_test_messages.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"int_test\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589301}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"float_test\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589301}}\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"default_number_test\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589301}}\n```\n\n----------------------------------------\n\nTITLE: Note about data reset and fresh sync in Airbyte\nDESCRIPTION: A note informing users that saving the connection with the reset option will reset the data in the destination and initiate a fresh synchronization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-ads-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nThis will reset the data in your destination and initiate a fresh sync.\n```\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-box-data-extract test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Klaus API Source Connector\nDESCRIPTION: Command to build the Docker image for the Klaus API source connector using airbyte-ci. This creates an image tagged as airbyte/source-klaus-api:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaus-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klaus-api build\n```\n\n----------------------------------------\n\nTITLE: Testing Luma Source Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the Luma source connector using airbyte-ci. Executes the test suite to verify connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-luma/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-luma test\n```\n\n----------------------------------------\n\nTITLE: Building Dev Image with Airbyte-CI (Bash)\nDESCRIPTION: Demonstrates the Bash command to build a development Docker image for the source-finage connector using Airbyte's CI tool. Requires the airbyte-ci CLI to be installed and available in the environment. The command takes the connector name as an argument to produce a new local dev image (source-finage:dev), which is critical for local testing and development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finage build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Convex Connector\nDESCRIPTION: Command to run unit tests for the Convex connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Google Tasks Connector with airbyte-ci\nDESCRIPTION: Command to build a dev image of the Google Tasks connector using airbyte-ci. This creates a development image named 'source-google-tasks:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-tasks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-tasks build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Public-Apis Connector\nDESCRIPTION: Command to build a Docker image for the connector using airbyte-ci tool. Creates an image tagged as airbyte/source-public-apis:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-public-apis build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Iterable Connector\nDESCRIPTION: Executes the full test suite for the Iterable connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-iterable test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry - Bash\nDESCRIPTION: Runs all unit tests for the Zendesk-Support source connector using pytest within the Poetry environment. This helps verify the functionality and stability of the code before any further actions such as deployment or integration. Prerequisites include all development dependencies being installed and existence of tests in the unit_tests directory. Outputs results to the terminal, showing test outcomes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests using Pytest\nDESCRIPTION: This command executes the unit tests located in the 'unit_tests' directory using the pytest framework. It should be run from the connector's root directory within the activated virtual environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython -m pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Markdown Template for Connector Changelog Section\nDESCRIPTION: A snippet showing the required format for the Changelog section in connector documentation. The format includes a collapsible details section that users can expand to view the changelog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n<details>\n  <summary>Expand to review</summary>\n</details>\n```\n\n----------------------------------------\n\nTITLE: Documenting Tremendous Connector Changelog in Markdown\nDESCRIPTION: Markdown table detailing the version history of the Tremendous connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tremendous.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.20 | 2025-04-19 | [58452](https://github.com/airbytehq/airbyte/pull/58452) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57971](https://github.com/airbytehq/airbyte/pull/57971) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57447](https://github.com/airbytehq/airbyte/pull/57447) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56899](https://github.com/airbytehq/airbyte/pull/56899) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56311](https://github.com/airbytehq/airbyte/pull/56311) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55597](https://github.com/airbytehq/airbyte/pull/55597) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [55108](https://github.com/airbytehq/airbyte/pull/55108) | Update dependencies |\n| 0.0.13 | 2025-02-22 | [54467](https://github.com/airbytehq/airbyte/pull/54467) | Update dependencies |\n| 0.0.12 | 2025-02-15 | [54057](https://github.com/airbytehq/airbyte/pull/54057) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53566](https://github.com/airbytehq/airbyte/pull/53566) | Update dependencies |\n| 0.0.10 | 2025-02-01 | [53065](https://github.com/airbytehq/airbyte/pull/53065) | Update dependencies |\n| 0.0.9 | 2025-01-25 | [52446](https://github.com/airbytehq/airbyte/pull/52446) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [51945](https://github.com/airbytehq/airbyte/pull/51945) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51406](https://github.com/airbytehq/airbyte/pull/51406) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50768](https://github.com/airbytehq/airbyte/pull/50768) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50363](https://github.com/airbytehq/airbyte/pull/50363) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49750](https://github.com/airbytehq/airbyte/pull/49750) | Update dependencies |\n| 0.0.3 | 2024-12-12 | [49375](https://github.com/airbytehq/airbyte/pull/49375) | Update dependencies |\n| 0.0.2 | 2024-12-11 | [49128](https://github.com/airbytehq/airbyte/pull/49128) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.1 | 2024-10-29 | | Initial release by [@bishalbera](https://github.com/bishalbera) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Running Firebase-Realtime-Database Connector Commands Locally\nDESCRIPTION: Series of commands to run the connector locally for spec generation, configuration check, schema discovery, and data reading using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-firebase-realtime-database spec\npoetry run source-firebase-realtime-database check --config secrets/config.json\npoetry run source-firebase-realtime-database discover --config secrets/config.json\npoetry run source-firebase-realtime-database read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Kyve Connector Unit Tests with Poetry\nDESCRIPTION: Command to run unit tests for the Kyve connector using Poetry and pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Source PostHog - Bash\nDESCRIPTION: Uses the 'airbyte-ci' tool to build a Docker image for the PostHog source connector. Requires 'airbyte-ci' to be installed as described in the referenced documentation. This creates a local Docker image tagged 'airbyte/source-posthog:dev' for use in further testing, development, or deployment steps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-posthog/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-posthog build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image via airbyte-ci\nDESCRIPTION: Command to build the connector's Docker image using the recommended airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-chroma build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Databricks Connector via Gradle\nDESCRIPTION: Command to run unit tests for the Databricks destination connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databricks/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-databricks:unitTest\n```\n\n----------------------------------------\n\nTITLE: Testing Workflowmax Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Workflowmax source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workflowmax/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workflowmax test\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Pinterest Connector Locally (Bash)\nDESCRIPTION: This multi-line snippet demonstrates various local invocations for the source-pinterest connector using Poetry. It covers fetching the connector's specs, performing config validation, schema discovery, and reading data using specified config files and Airbyte catalogs. Requires correct setup of secrets/config.json and integration_tests/configured_catalog.json files. Outputs correspond to connector CLI subcommands, and invalid or missing configurations may result in errors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-pinterest spec\npoetry run source-pinterest check --config secrets/config.json\npoetry run source-pinterest discover --config secrets/config.json\npoetry run source-pinterest read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using float Filter in Jinja2\nDESCRIPTION: Demonstrates the `float` filter in Jinja2, which converts a value (typically a string or integer) into a floating-point number. The example converts the string '42' to 42.0.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_24\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '42'|float }}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Zoom Connector\nDESCRIPTION: Command to run the full test suite for the Zoom connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoom/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoom test\n```\n\n----------------------------------------\n\nTITLE: Building Hubspot Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the Hubspot connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hubspot build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Documenting Huntr Connector Changelog in Markdown\nDESCRIPTION: Markdown table showing the version history of the Huntr connector. It includes version numbers, release dates, and brief descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/huntr.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date       | Subject        |\n|------------------|------------|----------------|\n| 0.1.0 | 2025-01-29 | Add new streams |\n| 0.0.1 | 2025-01-15 | Initial release by [@krokrob](https://github.com/krokrob) via Connector Builder|\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Version Changelog\nDESCRIPTION: Detailed version history table showing updates and changes to the connector over time, including version numbers, dates, pull request references, and change descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aha.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                 |\n|:--------|:-----------| :------------------------------------------------------- |:------------------------------------------------------------------------|\n| 0.4.21 | 2025-04-19 | [57608](https://github.com/airbytehq/airbyte/pull/57608) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Klaviyo Source Connector\nDESCRIPTION: Command to build the Docker image for the Klaviyo source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-klaviyo:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klaviyo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klaviyo build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for JustCall Source Connector\nDESCRIPTION: This command executes the acceptance tests for the JustCall source connector using airbyte-ci. It's used to verify the connector's functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-justcall/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-justcall test\n```\n\n----------------------------------------\n\nTITLE: Running Local Tests with Pytest\nDESCRIPTION: Command to execute the test suite using pytest in the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Building Dev Null Connector via Gradle\nDESCRIPTION: Command to build the Dev Null destination connector using Gradle from the Airbyte repository root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dev-null/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dev-null:build\n```\n\n----------------------------------------\n\nTITLE: Executing the CI Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command runs the complete connector integration test suite locally for `source-goldcast` using the `airbyte-ci` tool. It requires `airbyte-ci` to be installed and properly configured. This is used to verify connector functionality before submitting changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-goldcast/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-goldcast test\n```\n\n----------------------------------------\n\nTITLE: Running test suite for Senseforce connector in Bash\nDESCRIPTION: Command to run the full test suite for the Senseforce source connector using the airbyte-ci tool. This verifies functionality before submitting contributions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-senseforce/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-senseforce test\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the Connector with Poetry - Bash\nDESCRIPTION: This snippet installs the connector and its development dependencies using Poetry. Poetry (version ~1.7) must be installed beforehand. Running this command from the connector directory will set up all necessary Python dependencies according to the configuration in pyproject.toml and poetry.lock.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Creating Database Scoped Credential in MSSQL\nDESCRIPTION: SQL command to create a database scoped credential in MSSQL. This credential grants access to Azure Blob Storage using a Shared Access Signature (SAS) token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mssql.md#2025-04-23_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE DATABASE SCOPED CREDENTIAL <credential_name>\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n     SECRET = '<your_sas_token>';\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A markdown table documenting version history with dates, pull request references, and change descriptions. Includes version numbers from 0.1.0 to 0.9.0 spanning from 2021 to 2025.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bamboo-hr.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                         |\n| :------ | :--------- | :------------------------------------------------------- | :------------------------------------------------------------------------------ |\n| 0.9.0 | 2025-04-14 | [57587](https://github.com/airbytehq/airbyte/pull/57587) | change how to collect custom fields for custom reports |\n| 0.8.3 | 2025-04-12 | [57604](https://github.com/airbytehq/airbyte/pull/57604) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the project using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Custom Docker Build Configuration\nDESCRIPTION: Dockerfile for custom builds of the connector based on the official image\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-linnworks:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-configcat` connector. It requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-configcat:dev` on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-configcat/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-configcat build\n```\n\n----------------------------------------\n\nTITLE: Running Kvdb Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various connector operations using the Docker image, including spec, check, and write.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-kvdb:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-kvdb:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-kvdb:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Humanitix API Key in Markdown\nDESCRIPTION: Configuration table for the Humanitix connector, specifying the API key input parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/humanitix.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: A version history table showing changes made to the Okta connector over time, including version numbers, dates, pull request references, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/okta.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                        |\n|:--------|:-----------|:---------------------------------------------------------|:-------------------------------------------------------------------------------|\n| 0.3.21 | 2025-02-24 | [54167](https://github.com/airbytehq/airbyte/pull/54167) | Remove stream_state interpolation |\n| 0.3.20 | 2025-02-01 | [52728](https://github.com/airbytehq/airbyte/pull/52728) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Strava Source Connector Test Suite with airbyte-ci\nDESCRIPTION: Command to execute the full test suite for the Strava source connector using airbyte-ci. This is used to validate changes and ensure compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-strava/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-strava test\n```\n\n----------------------------------------\n\nTITLE: Running unit tests for the Faker connector\nDESCRIPTION: Command to run unit tests for the Faker connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Segment Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Segment source connector using airbyte-ci. The resulting image is tagged as 'source-segment:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-segment/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-segment build\n```\n\n----------------------------------------\n\nTITLE: Testing Squarespace Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the Squarespace connector to ensure its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-squarespace/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-squarespace test\n```\n\n----------------------------------------\n\nTITLE: Default Number Stream Records in Airbyte JSON Format\nDESCRIPTION: Examples of JSON records for a default number data stream named 'default_number_test' with different values including a very large decimal number, zero, and a negative decimal. Each record includes a timestamp in the emitted_at field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"10000000000000000000000.1234\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"default_number_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"-12345.678\" }}}\n```\n\n----------------------------------------\n\nTITLE: Building the source-clarif-ai Connector Image (Bash)\nDESCRIPTION: This Bash command utilizes the `airbyte-ci` tool to build a development Docker image for the `source-clarif-ai` connector. Running this command creates an image tagged as `source-clarif-ai:dev`, which can be used for local testing and development. Requires `airbyte-ci` to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clarif-ai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clarif-ai build\n```\n\n----------------------------------------\n\nTITLE: Displaying Note on Schema Changes in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display a note about reviewing schema changes during the refresh process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stripe-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nAny detected schema changes will be listed for your review.\n```\n```\n\n----------------------------------------\n\nTITLE: Building UserVoice Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the UserVoice connector using airbyte-ci tool. Creates a dev image tagged as source-uservoice:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-uservoice/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-uservoice build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Prestashop Source Connector\nDESCRIPTION: Command to build the Docker image for the Prestashop source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-prestashop/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-prestashop build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Kisi Source Connector\nDESCRIPTION: This command runs the acceptance tests for the Kisi source connector using Airbyte CI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kisi/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kisi test\n```\n\n----------------------------------------\n\nTITLE: Testing Bugsnag Source Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Bugsnag source connector using airbyte-ci framework.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bugsnag/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bugsnag test\n```\n\n----------------------------------------\n\nTITLE: Running Gainsight PX Connector Tests with airbyte-ci in Bash\nDESCRIPTION: Command to run the full test suite for the Gainsight PX connector using airbyte-ci. This verifies the connector's functionality before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gainsight-px/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gainsight-px test\n```\n\n----------------------------------------\n\nTITLE: Adding a New Python Dependency using Poetry - Bash\nDESCRIPTION: Adds a new Python package dependency to the project using Poetry. The command should be run with '<package-name>' replaced by the desired package. Modifies pyproject.toml and poetry.lock, which must both be committed to version control.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Invoiced API Key in Markdown\nDESCRIPTION: Markdown table showing the configuration input for the Invoiced connector. It specifies the API key as a required string parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiced.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it at https://invoiced.com/account |  |\n```\n\n----------------------------------------\n\nTITLE: Using slice Filter in Jinja2\nDESCRIPTION: Demonstrates the `slice` filter in Jinja2, which slices a sequence into a specified number of sub-sequences (slices). The example slices the list `[1, 2, 3, 4]` into 2 slices.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_43\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3, 4]|slice(2) }}\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog Table in Markdown\nDESCRIPTION: This code snippet shows a portion of the changelog table in markdown format, listing version updates, dates, pull request links, and subjects for the Sonar Cloud API connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sonar-cloud.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date                                                                  | Pull Request                                              | Subject                                                                         |\n| :------ | :-------------------------------------------------------------------- | :-------------------------------------------------------- | :------------------------------------------------------------------------------ |\n| 0.2.19 | 2025-04-19 | [58392](https://github.com/airbytehq/airbyte/pull/58392) | Update dependencies |\n| 0.2.18 | 2025-04-12 | [57953](https://github.com/airbytehq/airbyte/pull/57953) | Update dependencies |\n| 0.2.17 | 2025-04-05 | [57461](https://github.com/airbytehq/airbyte/pull/57461) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Querying Google Webfonts API with GET Request\nDESCRIPTION: This code snippet demonstrates how to make a GET request to the Google Webfonts API. It includes the API endpoint, API key parameter, and optional query parameters for sorting, pretty printing, and specifying the response format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-webfonts.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://webfonts.googleapis.com/v1/webfonts?key=<1234567>&sort=SORT_UNDEFINED&prettyPrint=true&alt=json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the XKCD connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xkcd/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xkcd test\n```\n\n----------------------------------------\n\nTITLE: Running TVmaze Schedule Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the TVmaze Schedule source connector, including spec, check, discover, and read. These commands use the locally built Docker image and mount necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tvmaze-schedule/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-tvmaze-schedule:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tvmaze-schedule:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-tvmaze-schedule:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-tvmaze-schedule:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using round Filter in Jinja2\nDESCRIPTION: Demonstrates the `round` filter in Jinja2, which rounds a number to a given precision (defaulting to 0 decimal places, rounding to the nearest integer). The example rounds 2.7 to 3.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_41\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 2.7|round }}\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Zendesk Sell connector container with various operations including spec, check, discover, and read configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sell/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zendesk-sell:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-sell:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zendesk-sell:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zendesk-sell:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Querying Latest Launch Data from SpaceX API\nDESCRIPTION: This code snippet demonstrates how to make a GET request to the SpaceX API to retrieve information about the latest launch. It uses the v5 endpoint for launches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/spacex-api.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.spacexdata.com/v5/launches/latest\n```\n\n----------------------------------------\n\nTITLE: Displaying Pulsar Destination Changelog in Markdown\nDESCRIPTION: A markdown table showing the version history and changes made to the Pulsar destination connector, including version numbers, dates, and descriptions of updates.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/pulsar.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                        |\n| :------ | :--------- | :------------------------------------------------------- | :----------------------------------------------------------------------------- |\n| 0.1.3   | 2022-08-05 | [15349](https://github.com/airbytehq/airbyte/pull/15349) | Update Pulsar destination to use outputRecordCollector to properly store state |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Emailoctopus Source Connector\nDESCRIPTION: Command to build the Emailoctopus source connector Docker image using airbyte-ci. This creates an image with the tag 'airbyte/source-emailoctopus:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-emailoctopus/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-emailoctopus build\n```\n\n----------------------------------------\n\nTITLE: Running Tests for the Simplesat Connector\nDESCRIPTION: Command to execute acceptance tests for the Simplesat connector using airbyte-ci. This validates that the connector works correctly with the Simplesat API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simplesat/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simplesat test\n```\n\n----------------------------------------\n\nTITLE: Running dbt for table creation\nDESCRIPTION: Command to execute dbt to build tables in the configured data warehouse.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Building Starburst Galaxy Destination with Gradle in Java\nDESCRIPTION: Builds the Starburst Galaxy destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-starburst-galaxy/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-starburst-galaxy:build\n```\n\n----------------------------------------\n\nTITLE: Building the Shipstation Connector with airbyte-ci\nDESCRIPTION: Command to build the Shipstation connector locally, which creates a development image called 'source-shipstation:dev' for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shipstation/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shipstation build\n```\n\n----------------------------------------\n\nTITLE: Testing e-conomic Connector with airbyte-ci in Bash\nDESCRIPTION: This Bash command triggers the acceptance tests for the e-conomic manifest-only connector via the airbyte-ci CLI tool. It validates the connector implementation against Airbyte's testing suite. Dependency: airbyte-ci must be installed. The --name parameter specifies the connector, and test selection is based on connector naming. Expected output includes test results for functionality and compliance. Run this from your terminal in the appropriate project directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e-conomic/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-e-conomic test\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for Python\nDESCRIPTION: Commands to create a virtual environment, activate it, and install dependencies for local development of the AWS Datalake connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Testing Appcues Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Appcues source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appcues/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appcues test\n```\n\n----------------------------------------\n\nTITLE: Disabling CDC on MSSQL Table\nDESCRIPTION: SQL command to disable Change Data Capture on a specific table. Requires schema name, table name, and capture instance name as parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql/mssql-troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nEXEC sys.sp_cdc_disable_table\n    @source_schema = N'<schema>',\n    @source_name   = N'<table>',\n    @capture_instance = N'<capture instance (typically schema_table)>'\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Kyriba Connector\nDESCRIPTION: Command to run the full test suite for the Kyriba connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kyriba test\n```\n\n----------------------------------------\n\nTITLE: Available Streams Table in Markdown\nDESCRIPTION: Table describing the available data streams in the Navan connector, including their properties like primary key, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/navan.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| bookings | uuid | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Surveycto Connector\nDESCRIPTION: Command to add a new dependency to the Surveycto connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building the Partnerstack Connector Docker Image using airbyte-ci\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image specifically for the `source-partnerstack` connector. It requires `airbyte-ci` to be installed locally. Upon successful execution, a Docker image tagged `airbyte/source-partnerstack:dev` will be created and made available on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-partnerstack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-partnerstack build\n```\n\n----------------------------------------\n\nTITLE: Configuring Mailosaur Authentication Parameters\nDESCRIPTION: Configuration table showing the required authentication parameters for the Mailosaur connector: username (API) and password (API Key).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mailosaur.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username. Enter API here |  |\n| `password` | `string` | Password. Enter your API Key here |  |\n```\n\n----------------------------------------\n\nTITLE: Configuration Table - Feature Support\nDESCRIPTION: Markdown table showing supported features of the New York Times connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nytimes.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n```\n\n----------------------------------------\n\nTITLE: Cloud Sign-in Redirect URI Configuration\nDESCRIPTION: The redirect URI pattern required for Airbyte Cloud SSO integration with Okta. Contains a placeholder for company identifier.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/okta.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://cloud.airbyte.com/auth/realms/<your-company-identifier>/broker/default/endpoint\n```\n\n----------------------------------------\n\nTITLE: Build Customization Module Example\nDESCRIPTION: Example Python module for customizing the connector build process with pre and post install hooks\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pgvector/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Building SFTP Connector Docker Image via Gradle in Java\nDESCRIPTION: Command to build the Docker image for the SFTP source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-sftp:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: JSON Stream Trace Message\nDESCRIPTION: Trace message indicating stream status completion with stream descriptor and timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_array_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"array_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589301}}\n```\n\n----------------------------------------\n\nTITLE: Boolean Test Record in Airbyte JSON Format\nDESCRIPTION: Sample record for boolean data test in Airbyte's JSON record format. Contains a simple boolean value (true) with timestamp and stream identifier.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"boolean_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : true }}}\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte CI\nDESCRIPTION: Command to build the connector using Airbyte's CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-astra build\n```\n\n----------------------------------------\n\nTITLE: Building Mux Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Mux source connector using airbyte-ci. Creates a dev image tagged as source-mux:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mux/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mux build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Recurly Source Connector\nDESCRIPTION: Command to run the full test suite for the source-recurly connector using airbyte-ci. This validates that all connector functionality is working as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recurly/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recurly test\n```\n\n----------------------------------------\n\nTITLE: Building Bluetally Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Bluetally source connector. Creates a dev image tagged as source-bluetally:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bluetally/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bluetally build\n```\n\n----------------------------------------\n\nTITLE: Checking PostgreSQL Version (Amazon RDS pg_cron Prerequisite)\nDESCRIPTION: SQL query to retrieve the current PostgreSQL version. This is used to verify if the version meets the minimum requirement (version 12.5 or later) for enabling the `pg_cron` extension on Amazon RDS for PostgreSQL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT version();\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Gridly Connector\nDESCRIPTION: Command to add a new dependency to the Gridly connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building UpPromote Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the UpPromote source connector using airbyte-ci tool. Creates a dev image tagged as source-uppromote:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-uppromote/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-uppromote build\n```\n\n----------------------------------------\n\nTITLE: Running the Spec Operation via Docker\nDESCRIPTION: Shows the sequence of commands to first build the destination connector's Docker image using Gradle, and then execute its 'spec' operation within a Docker container. This command retrieves the connector's configuration specification defined in `spec.json`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/building-a-java-destination.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# First build the connector\n./gradlew :airbyte-integrations:connectors:destination-<name>:build\n\n# Run the spec operation\ndocker run --rm airbyte/destination-<name>:dev spec\n```\n\n----------------------------------------\n\nTITLE: Building ZapSign Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the ZapSign source connector using airbyte-ci tool. Creates a dev image tagged as source-zapsign:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zapsign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zapsign build\n```\n\n----------------------------------------\n\nTITLE: Building BigQuery Destination Connector with Gradle\nDESCRIPTION: Command to build the BigQuery destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-bigquery:build\n```\n\n----------------------------------------\n\nTITLE: Building Kvdb Destination Connector via Gradle\nDESCRIPTION: This command builds the Kvdb destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-kvdb:build\n```\n\n----------------------------------------\n\nTITLE: Building Elasticsearch Connector Docker Image via Gradle\nDESCRIPTION: Command to build the Docker image for the Elasticsearch source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticsearch/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-elasticsearch:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Decoding CSV API Response to Normalized JSON Array (Airbyte, CSV)\nDESCRIPTION: Shows a response in CSV format, which Airbyte decodes by using the header row as field names and converting each subsequent row into a JSON object. Assumes the default delimiter is a comma unless specified. Each output array element represents a record with all fields as strings, handling standard CSV parsing nuances.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/record-processing.mdx#2025-04-23_snippet_4\n\nLANGUAGE: csv\nCODE:\n```\nid,name,email,created_at\n1,John Doe,john.doe@example.com,2023-01-15T09:30:00Z\n2,Jane Smith,jane.smith@example.com,2023-01-16T14:20:00Z\n3,Bob Johnson,bob.johnson@example.com,2023-01-17T11:45:00Z\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": \"1\",\n    \"name\": \"John Doe\",\n    \"email\": \"john.doe@example.com\",\n    \"created_at\": \"2023-01-15T09:30:00Z\"\n  },\n  {\n    \"id\": \"2\",\n    \"name\": \"Jane Smith\",\n    \"email\": \"jane.smith@example.com\",\n    \"created_at\": \"2023-01-16T14:20:00Z\"\n  },\n  {\n    \"id\": \"3\",\n    \"name\": \"Bob Johnson\",\n    \"email\": \"bob.johnson@example.com\",\n    \"created_at\": \"2023-01-17T11:45:00Z\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Building Nebius AI Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Nebius AI source connector using airbyte-ci tool. Creates a dev image tagged as source-nebius-ai:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nebius-ai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nebius-ai build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for the Connector\nDESCRIPTION: Command to execute unit tests for the connector using pytest within the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Airbyte JSON Stream Records\nDESCRIPTION: Example JSON records showing the standard Airbyte data format. Each record contains a stream identifier, data payload with id and name fields, and an emitted_at timestamp. The format is consistent across multiple records with different values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/integration_tests/expected_records.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"1\", \"name\": \"one\"}, \"emitted_at\": 999999}\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"2\", \"name\": \"two\"}, \"emitted_at\": 999999}\n{\"stream\": \"id_and_name_cat\", \"data\": {\"id\": \"3\", \"name\": \"three\"}, \"emitted_at\": 999999}\n```\n\n----------------------------------------\n\nTITLE: Branch Specification Format\nDESCRIPTION: Examples demonstrating how to specify GitHub repository branches for commit retrieval, including the default branch and custom branches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/correct.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte/master              # default branch\nairbytehq/airbyte/master airbytehq/airbyte/my-branch  # multiple branches\n```\n\n----------------------------------------\n\nTITLE: Oncehub Configuration Table in Markdown\nDESCRIPTION: Markdown table showing the configuration parameters required for the Oncehub connector, including API key and start date settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/oncehub.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Find it in your OnceHub account under the API & Webhooks Integration page. |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-timeplus build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running the Asana connector locally to perform spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-asana spec\npoetry run source-asana check --config secrets/config.json\npoetry run source-asana discover --config secrets/config.json\npoetry run source-asana read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using lower Filter in Jinja2\nDESCRIPTION: Demonstrates the `lower` filter in Jinja2, which converts a string to all lowercase characters. The example converts 'HELLO' to 'hello'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_34\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'HELLO'|lower }}\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Tremendous Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Tremendous source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tremendous/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tremendous test\n```\n\n----------------------------------------\n\nTITLE: Listing Limited GCP Service Account Permissions for Failure Testing\nDESCRIPTION: This snippet shows the limited permissions assigned to a GCP service account used for testing failure scenarios in the GCS destination connector. It lacks multipart upload permissions to test error detection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-gcs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nstorage.objects.create\nstorage.objects.delete\nstorage.objects.get\nstorage.objects.list\n```\n\n----------------------------------------\n\nTITLE: Customizing Snowflake Cortex Connector Build Process\nDESCRIPTION: Example of a build_customization.py module to customize the connector build process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Authentication Example - Multiple API Tokens\nDESCRIPTION: Example showing how to input multiple GitHub API tokens for load balancing API quota consumption.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/unit_tests/data/docs/correct.md#2025-04-23_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntoken1,token2,token3\n```\n\n----------------------------------------\n\nTITLE: Version History Table Format in Markdown\nDESCRIPTION: A markdown table showing the version history of the Apple Ads connector, including version numbers, dates, pull request references, and change descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/apple-search-ads.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                              |\n|:--------|:-----------|:---------------------------------------------------------|:-------------------------------------------------------------------------------------|\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Tiktok-Marketing Connector\nDESCRIPTION: This command runs the unit tests for the Tiktok-Marketing connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Paddle Connector - Bash\nDESCRIPTION: This bash command uses the airbyte-ci tool to build a development Docker image for the Paddle source connector (named source-paddle:dev). The command requires airbyte-ci to be installed and accessible in the environment. It takes the connector name as an argument and executes the build process, enabling local testing and development of the connector image. No input is required other than the correct connector name; output is the built Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paddle/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paddle build\n```\n\n----------------------------------------\n\nTITLE: Removing Boilerplate Unit Test Files using Bash\nDESCRIPTION: Removes default or placeholder unit test files (`test_incremental_streams.py`, `test_source.py`, `test_streams.py`) located in the `unit_tests` directory. This cleanup step is often performed when focusing on integration tests or when replacing default tests with custom ones.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrm unit_tests/test_incremental_streams.py unit_tests/test_source.py unit_tests/test_streams.py\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Docker image for the Mailjet SMS source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-sms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailjet-sms build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Jina AI Reader Connector\nDESCRIPTION: Command to add a new dependency to the Jina AI Reader connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Unscheduling a pg_cron Job in PostgreSQL\nDESCRIPTION: SQL command using the `cron.unschedule` function to remove a previously scheduled `pg_cron` job. This example shows how to unschedule the job named 'periodic_logger', effectively stopping the periodic inserts into the `periodic_log` table.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT cron.unschedule('periodic_logger');\n```\n\n----------------------------------------\n\nTITLE: Building Humanitix Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Humanitix source connector using airbyte-ci. The resulting image is tagged as 'source-humanitix:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-humanitix/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-humanitix build\n```\n\n----------------------------------------\n\nTITLE: Deduplicated Exchange Rate Records\nDESCRIPTION: Deduplicated version of exchange rate records with the same structure but in a separate stream for deduplication purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"dedup_exchange_rate\", \"emitted_at\": 1602637589000, \"data\": { \"id\": 1, \"currency\": \"USD\", \"date\": \"2020-08-29\", \"timestamp_col\": \"2020-08-29T00:00:00.000000-0000\", \"NZD\": 1.14, \"HKD@spéçiäl & characters\": 2.13, \"HKD_special___characters\": \"column name collision?\", \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\" }}}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketo/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-marketo test\n```\n\n----------------------------------------\n\nTITLE: Defining TicketTailor Data Streams in Markdown\nDESCRIPTION: Markdown table listing available data streams for the TicketTailor connector. It includes stream names, primary keys, pagination method, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/tickettailor.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| events_series | id | DefaultPaginator | ✅ |  ❌  |\n| events | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ❌  |\n| vouchers | id | DefaultPaginator | ✅ |  ❌  |\n| discounts | id | DefaultPaginator | ✅ |  ❌  |\n| check_ins | id | DefaultPaginator | ✅ |  ❌  |\n| issued_tickets | id | DefaultPaginator | ✅ |  ❌  |\n| orders | id | DefaultPaginator | ✅ |  ❌  |\n| waitlists | id | DefaultPaginator | ✅ |  ❌  |\n| vouchers_codes | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Defining Testrail Streams in Markdown\nDESCRIPTION: Lists the available streams in the Testrail connector, including their primary keys, pagination type, and support for full and incremental sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/testrail.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| projects | id | DefaultPaginator | ✅ |  ❌  |\n| priorities | id | DefaultPaginator | ✅ |  ❌  |\n| plans | id | DefaultPaginator | ✅ |  ✅  |\n| roles | id | DefaultPaginator | ✅ |  ❌  |\n| runs | id | DefaultPaginator | ✅ |  ✅  |\n| result_fields | id | DefaultPaginator | ✅ |  ❌  |\n| suites | id | DefaultPaginator | ✅ |  ❌  |\n| templates | id | DefaultPaginator | ✅ |  ❌  |\n| runs_tests | id | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n| sections | id | DefaultPaginator | ✅ |  ❌  |\n| case_statuses | case_status_id | DefaultPaginator | ✅ |  ❌  |\n| milestones | id | DefaultPaginator | ✅ |  ✅  |\n| datasets | id | DefaultPaginator | ✅ |  ❌  |\n| configs | id | DefaultPaginator | ✅ |  ❌  |\n| case_types | id | DefaultPaginator | ✅ |  ❌  |\n| case_fields | id | DefaultPaginator | ✅ |  ❌  |\n| cases | id | DefaultPaginator | ✅ |  ✅  |\n```\n\n----------------------------------------\n\nTITLE: Running Weaviate Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run the connector locally for specification, configuration checking, and writing data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Content Block for Product Specification\nDESCRIPTION: YAML-style markdown header specifying that this documentation applies to all Airbyte products.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/community/getting-support.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nproducts: all\n---\n```\n\n----------------------------------------\n\nTITLE: Aviationstack Configuration Table in Markdown\nDESCRIPTION: Markdown table defining the configuration parameters for the Aviationstack API connection, including access key and start date fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aviationstack.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `access_key` | `string` | Access Key. Your unique API key for authenticating with the Aviation API. You can find it in your Aviation account dashboard at https://aviationstack.com/dashboard |  |\n| `start_date` | `string` | Start date. |  |\n```\n\n----------------------------------------\n\nTITLE: Documenting Humanitix Connector Changelog in Markdown\nDESCRIPTION: Detailed version history table for the Humanitix connector, showing updates and dependencies changes over time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/humanitix.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.18 | 2025-04-19 | [58181](https://github.com/airbytehq/airbyte/pull/58181) | Update dependencies |\n| 0.0.17 | 2025-04-12 | [57727](https://github.com/airbytehq/airbyte/pull/57727) | Update dependencies |\n| 0.0.16 | 2025-04-05 | [57079](https://github.com/airbytehq/airbyte/pull/57079) | Update dependencies |\n| 0.0.15 | 2025-03-29 | [56671](https://github.com/airbytehq/airbyte/pull/56671) | Update dependencies |\n| 0.0.14 | 2025-03-22 | [56071](https://github.com/airbytehq/airbyte/pull/56071) | Update dependencies |\n| 0.0.13 | 2025-03-08 | [55509](https://github.com/airbytehq/airbyte/pull/55509) | Update dependencies |\n| 0.0.12 | 2025-03-01 | [54778](https://github.com/airbytehq/airbyte/pull/54778) | Update dependencies |\n| 0.0.11 | 2025-02-22 | [53822](https://github.com/airbytehq/airbyte/pull/53822) | Update dependencies |\n| 0.0.10 | 2025-02-08 | [53307](https://github.com/airbytehq/airbyte/pull/53307) | Update dependencies |\n| 0.0.9 | 2025-02-01 | [52754](https://github.com/airbytehq/airbyte/pull/52754) | Update dependencies |\n| 0.0.8 | 2025-01-25 | [52293](https://github.com/airbytehq/airbyte/pull/52293) | Update dependencies |\n| 0.0.7 | 2025-01-18 | [51785](https://github.com/airbytehq/airbyte/pull/51785) | Update dependencies |\n| 0.0.6 | 2025-01-11 | [51210](https://github.com/airbytehq/airbyte/pull/51210) | Update dependencies |\n| 0.0.5 | 2024-12-28 | [50646](https://github.com/airbytehq/airbyte/pull/50646) | Update dependencies |\n| 0.0.4 | 2024-12-21 | [50074](https://github.com/airbytehq/airbyte/pull/50074) | Update dependencies |\n| 0.0.3 | 2024-12-14 | [49618](https://github.com/airbytehq/airbyte/pull/49618) | Update dependencies |\n| 0.0.2 | 2024-12-12 | [49261](https://github.com/airbytehq/airbyte/pull/49261) | Update dependencies |\n| 0.0.1 | 2024-10-31 | | Initial release by [@ombhardwajj](https://github.com/ombhardwajj) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Importing ConnectorRegistry Component in JSX\nDESCRIPTION: Imports the ConnectorRegistry React component which is used to display the catalog of available connectors on the documentation page. This component will render either source or destination connectors based on the 'type' prop.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/README.md#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport ConnectorRegistry from '@site/src/components/ConnectorRegistry';\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the WooCommerce connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-woocommerce:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-woocommerce/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-woocommerce build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the WorkRamp source connector container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workramp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-workramp:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-workramp:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-workramp:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-workramp:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Airbyte Data Record Structure in JSON\nDESCRIPTION: This snippet shows the structure of an Airbyte data record. It includes the record type, stream name, actual data payload, and emission timestamp. The data payload contains an ID and an updated_at field, demonstrating a sparse nested structure.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_sparse_nested_streams/data_input/messages2.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"sparse_nested_stream\", \"data\": {\"id\": 2, \"updated_at\": 101}, \"emitted_at\": 1672568200}}\n```\n\n----------------------------------------\n\nTITLE: Testing Cloudbeds Source Connector Locally (Bash)\nDESCRIPTION: This command executes the acceptance tests for the `source-cloudbeds` connector using the `airbyte-ci` tool. It typically runs against the development image created by the build command. Requires `airbyte-ci` to be installed and the connector image to be built.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cloudbeds/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cloudbeds test\n```\n\n----------------------------------------\n\nTITLE: Referencing Table Structure in BigQuery\nDESCRIPTION: Demonstrates the naming convention for raw data tables in BigQuery, showing how JSON blob data is stored in tables prefixed with _airbyte_raw_* before transformation and normalization.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-bigquery/BOOTSTRAP.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n_airbyte_raw_*\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for SFTP-JSON Connector\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Source Connector\nDESCRIPTION: Command to build a development image of the source-airbyte connector. Creates a dev image tagged as source-airbyte:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airbyte/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-airbyte build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Shortio Connector\nDESCRIPTION: Command to run the full test suite for the Shortio source connector using airbyte-ci. This ensures that any changes made to the connector pass all tests before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shortio/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shortio test\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Billing Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Zoho Billing connector using airbyte-ci tool. Creates a dev image tagged as source-zoho-billing:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-billing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-billing build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Tplcentral Connector\nDESCRIPTION: Command to build the Docker image for the Tplcentral connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tplcentral build\n```\n\n----------------------------------------\n\nTITLE: Version History Table\nDESCRIPTION: Detailed changelog showing version history, dates, pull request references, and changes made to the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zapier-supported-storage.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n|:--------|:-----------|:---------------------------------------------------------| |\n| 0.2.21 | 2025-04-19 | [58555](https://github.com/airbytehq/airbyte/pull/58555) | Update dependencies |\n| 0.2.20 | 2025-04-13 | [58049](https://github.com/airbytehq/airbyte/pull/58049) | Update dependencies |\n[...additional version history entries...]\n```\n\n----------------------------------------\n\nTITLE: Running the Connector CI Test Suite Locally with airbyte-ci (Bash)\nDESCRIPTION: This bash command runs the complete test suite for the source-chargebee connector using the airbyte-ci CLI tool. Prior setup should include installing airbyte-ci and having all required test files/configuration. The command outputs test results directly to the terminal and is suitable for local development environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chargebee/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chargebee test\n```\n\n----------------------------------------\n\nTITLE: Error Log When Upgrading Without Updated Permissions\nDESCRIPTION: The error message displayed in the Bootloader pod logs when attempting to upgrade to Airbyte 1.6 without proper service account permissions for secrets access.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/upgrade-service-account.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n2025-04-08 21:40:25,960 [main]  INFO    i.a.b.Bootloader(load):73 - Initializing auth secrets...\n2025-04-08 21:40:26,880 [main]  ERROR   i.a.b.ApplicationKt(main):28 - Unable to bootstrap Airbyte environment.\njava.lang.IllegalStateException: Upgrade to version AirbyteVersion{version='1.6.0-alpha-35dc75a5941', major='1', minor='6', patch='0'} failed. As of version 1.6 of the Airbyte Platform, we require your Service Account permissions to include access to the \"secrets\" resource. To learn more, please visit our documentation page at https://docs.airbyte.com/enterprise-setup/upgrade-service-account.\n        at io.airbyte.bootloader.AuthKubernetesSecretInitializer.checkAccessToSecrets(AuthKubernetesSecretInitializer.kt:57)\n        at io.airbyte.bootloader.Bootloader.load(Bootloader.kt:74)\n        at io.airbyte.bootloader.ApplicationKt.main(Application.kt:25)\n```\n\n----------------------------------------\n\nTITLE: Configuring Track PMS Airbyte Source Parameters in Markdown\nDESCRIPTION: This snippet defines the configuration parameters required for the Track PMS Airbyte source connector. It includes the customer domain, API key, and API secret as input fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/track-pms.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `customer_domain` | `string` | Customer Domain.  |  |\n| `api_key` | `string` | API Key.  |  |\n| `api_secret` | `string` | API Secret.  |  |\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for the connector development\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Testing TicketTailor Source Connector in Bash\nDESCRIPTION: This command uses airbyte-ci to run acceptance tests for the TicketTailor source connector. It ensures that the connector meets the required functionality and integration standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tickettailor/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tickettailor test\n```\n\n----------------------------------------\n\nTITLE: Running Performance Tests in Pull Request\nDESCRIPTION: Command to trigger performance tests in a pull request context with optional resource limits. Supports the same CPU and memory limitation parameters as the Gradle command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mssql/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n/test-performance connector=connectors/source-mssql [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zapier-supported-storage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zapier-supported-storage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zapier-supported-storage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zapier-supported-storage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zapier-supported-storage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies in requirements.txt - Python\nDESCRIPTION: This snippet specifies the Python package dependencies for a component in the Airbyte project using the requirements.txt format. It pins psycopg2 to version 2.9.9 for stable PostgreSQL database connectivity and includes pytz for time zone calculations. This file is intended to be processed by pip to install or manage the listed packages; it does not contain executable code, but acts as a manifest outlining prerequisites for the Python environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postgres/integration_tests/seed/requirement.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npsycopg2==2.9.9\\npytz\n```\n\n----------------------------------------\n\nTITLE: JSON Stream Data Records from Sheet6\nDESCRIPTION: Stream of JSON records containing ID-Name pairs from a spreadsheet. Each record has a stream identifier, data object with ID and Name fields, and an emitted timestamp in milliseconds.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"179\",\"Name\":\"ijsAHEZTk\"},\"emitted_at\":1673989567000}\n```\n\n----------------------------------------\n\nTITLE: Installing ERD Tools with Poetry\nDESCRIPTION: This snippet shows how to install the ERD tools using Poetry package manager. It assumes the user is in the Airbyte repository root directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/erd/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-ci/connectors/erd\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Building Mode Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Mode source connector using airbyte-ci. Creates a dev image tagged as 'source-mode:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mode/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mode build\n```\n\n----------------------------------------\n\nTITLE: Sample Airbyte input state (for incremental read) in JSON\nDESCRIPTION: Presents an example state list with a stream's name and its last known cursor value. Used to pass state input when rerunning incremental syncs to fetch only new records. All fields are required by Airbyte state management.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/6-incremental-reads.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"type\": \"STREAM\",\n    \"stream\": {\n      \"stream_descriptor\": {\n        \"name\": \"surveys\"\n      },\n      \"stream_state\": {\n        \"date_modified\": 1711753326\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Cloning with HTTPS and Navigating - Git CLI - Bash\nDESCRIPTION: This snippet is an alternative to cloning the repository, using HTTPS instead of SSH. Similar requirements apply: Git must be installed and {YOUR_USERNAME} should be your GitHub username. It will download the repo and enter the project directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/{YOUR_USERNAME}/airbyte.git\\ncd airbyte\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-databend spec\npoetry run destination-databend check --config secrets/config.json\npoetry run destination-databend discover --config secrets/config.json\npoetry run destination-databend read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Docker Run Command Failure\nDESCRIPTION: Sample error output when the Docker run command fails during Kind cluster creation. The error shows the complete command that failed with exit status 125.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nunable to create kind cluster: command \"docker run --name airbyte-abctl-control-plane\n--hostname airbyte-abctl-control-plane --label io.x-k8s.kind.role=control-plane --privileged\n--security-opt seccomp=unconfined --security-opt apparmor=unconfined --tmpfs /tmp --tmpfs /run\n--volume /var --volume /lib/modules:/lib/modules:ro -e KIND_EXPERIMENTAL_CONTAINERD_SNAPSHOTTER\n--detach --tty --label io.x-k8s.kind.cluster=airbyte-abctl --net kind --restart=on-failure:1\n--init=false --cgroupns=private --volume /dev/mapper:/dev/mapper\n--volume=/home/chang.kim/.airbyte/abctl/data:/var/local-path-provisioner --publish=0.0.0.0:8000:80/TCP\n--publish=127.0.0.1:44776:6443/TCP -e KUBECONFIG=/etc/kubernetes/admin.conf kindest/node:v1.29.\n4@sha256:3abb816a5b1061fb15c6e9e60856ec40d56b7b52bcea5f5f1350bc6e2320b6f8\"\nfailed with error: exit status 125\n```\n\n----------------------------------------\n\nTITLE: Listing Airbyte Images with abctl\nDESCRIPTION: Command to get a list of all Airbyte images for the latest version using abctl.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nabctl images manifest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Callrail source connector docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-callrail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-callrail build\n```\n\n----------------------------------------\n\nTITLE: Configuring Bluetally API Parameters in Markdown\nDESCRIPTION: Configuration table showing required input parameters for the Bluetally connector: API key for authentication and start date for data fetching.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bluetally.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. Your API key to authenticate with the BlueTally API. You can generate it by navigating to your account settings, selecting &#39;API Keys&#39;, and clicking &#39;Create API Key&#39;. |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-monday test\n```\n\n----------------------------------------\n\nTITLE: Structuring JSON Source Object for Airbyte Output - JSON\nDESCRIPTION: This snippet shows an example JSON object from a data source prior to exporting with Airbyte. The object contains a numeric user_id and a nested name field (with first and last). Airbyte connectors will process such objects, converting them according to selected output and normalization formats. Inputs are user data in JSON form, outputs may vary based on downstream use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/r2.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"user_id\\\": 123,\\n  \\\"name\\\": {\\n    \\\"first\\\": \\\"John\\\",\\n    \\\"last\\\": \\\"Doe\\\"\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Markdown table showing the mapping between Zendesk Sell integration types and Airbyte data types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-sell.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Using title Filter in Jinja2\nDESCRIPTION: Demonstrates the `title` filter in Jinja2, which converts a string into title case (first letter of each word capitalized, others lowercase). The example converts 'hello world' to 'Hello World'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_48\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello world'|title }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Go High Level Proxy Connector in Markdown\nDESCRIPTION: Markdown table defining the configuration parameters for the Go High Level proxy connector. It specifies the required inputs: location_id, api_key, and start_date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/high-level.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `location_id` | `string` | Location ID.  |  |\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Inspecting Docker Containers for IP Addresses\nDESCRIPTION: This bash command lists all running Docker containers, displaying their names, images, and IP addresses. It's used to find the IP address of the destination-postgres container for debugging purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/debugging-docker.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ docker inspect $(docker ps -q ) --format='{{ printf \"%-50s\" .Name}} {{printf \"%-50s\" .Config.Image}} {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n```\n\n----------------------------------------\n\nTITLE: Defining State Management for Incremental Faker Data Syncs in JSON\nDESCRIPTION: State file that tracks progress across multiple streams (users, purchases, products). This allows for incremental data loading by updating the ID values after each sync to generate the next batch of data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/csv_export/README.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"type\": \"STREAM\",\n    \"stream\": {\n      \"stream_state\": {\n        \"id\": 0\n      },\n      \"stream_descriptor\": {\n        \"name\": \"users\"\n      }\n    }\n  },\n  {\n    \"type\": \"STREAM\",\n    \"stream\": {\n      \"stream_state\": {\n        \"id\": 0,\n        \"user_id\": 0\n      },\n      \"stream_descriptor\": {\n        \"name\": \"purchases\"\n      }\n    }\n  },\n  {\n    \"type\": \"STREAM\",\n    \"stream\": {\n      \"stream_state\": {\n        \"id\": 0\n      },\n      \"stream_descriptor\": {\n        \"name\": \"products\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Constructing a Basic Slack Notification JSON Payload\nDESCRIPTION: This example shows the structure of a basic Slack notification payload with text content. The JSON includes a main text field that supports markdown formatting and will be displayed in the Slack channel.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/Google/vocab.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"Hi there, I'm a notification from Airbyte!\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Dev Null Connector\nDESCRIPTION: Gradle command to execute unit tests for the Dev Null connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dev-null/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dev-null:unitTest\n```\n\n----------------------------------------\n\nTITLE: Displaying Note on Fresh Sync in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display a note about resetting data and initiating a fresh sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stripe-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nThis will reset the data in your destination and initiate a fresh sync.\n```\n```\n\n----------------------------------------\n\nTITLE: Building Clickhouse Connector via Gradle\nDESCRIPTION: Command to build the Clickhouse destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-clickhouse/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-clickhouse:build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Source Inflowinventory Connector\nDESCRIPTION: This command executes the acceptance tests for the source-inflowinventory connector using airbyte-ci. It helps ensure the connector functions correctly before deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-inflowinventory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-inflowinventory test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-xata test\n```\n\n----------------------------------------\n\nTITLE: Running Freshservice Connector Operations with Docker\nDESCRIPTION: Standard Docker commands for running the Freshservice source connector's main operations: spec, check, discover, and read. These commands use mounted volumes to access configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshservice/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-freshservice:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshservice:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshservice:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-freshservice:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Teradata Source Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run various connector operations using the Docker image. They include spec, check, discover, and read operations with appropriate configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teradata/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run --rm airbyte/source-teradata:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-teradata:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-teradata:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-teradata:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run the Freshcaller connector within a Docker container for specification retrieval, connection checking, data discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-freshcaller:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshcaller:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-freshcaller:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-freshcaller:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running SAP Fieldglass Connector Docker Commands\nDESCRIPTION: Standard commands for running the SAP Fieldglass source connector as a docker container. Includes commands for specification, connection check, schema discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sap-fieldglass/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-sap-fieldglass:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sap-fieldglass:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-sap-fieldglass:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-sap-fieldglass:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source Connector Commands via Docker in Bash\nDESCRIPTION: This group of commands demonstrates how to run various lifecycle operations (spec, check, discover, read) for the Airbyte source-coin-api connector inside Docker containers. These commands require Docker, a built airbyte/source-coin-api:dev image, and local credentials/configuration files. Key parameters include mounting host directories for secrets and integration tests as needed. Commands output standard Airbyte connector CLI results and operate in isolated containers that are removed after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coin-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-coin-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coin-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-coin-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-coin-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Amazon SQS Connector\nDESCRIPTION: Command to execute unit tests for the Amazon SQS connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Dev Null Connector\nDESCRIPTION: Command to build the Docker image for the Dev Null connector using Gradle\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dev-null/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dev-null:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-webflow spec\npoetry run source-webflow check --config secrets/config.json\npoetry run source-webflow discover --config secrets/config.json\npoetry run source-webflow read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Deepset Connector\nDESCRIPTION: Command to add a new dependency to the Deepset connector using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: New Handling of Typeless Arrays in Airbyte\nDESCRIPTION: Illustrates the new behavior for typeless arrays (`\"type\": \"array\"` without `items`). Instead of converting elements to strings, the entire array is now serialized into a single JSON array string. The output schema type becomes 'string'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"array\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n[1, \"Alice\"]\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Schema\n{ \"type\": \"string\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// New Output Data\n\"[1, \\\"Alice\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci\nDESCRIPTION: Command to build a Docker image for the connector using the airbyte-ci tool, which creates an image tagged as 'airbyte/source-facebook-marketing:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-facebook-marketing build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Yellowbrick Connector\nDESCRIPTION: Command to build the Docker image for the Yellowbrick connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-yellowbrick/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-yellowbrick:airbyteDocker\n```\n\n----------------------------------------\n\nTITLE: Building Iceberg V2 Connector with Gradle\nDESCRIPTION: Gradle command to build the Iceberg V2 destination connector. This should be run from the root of the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-iceberg-v2:build\n```\n\n----------------------------------------\n\nTITLE: Listing Certified Connectors\nDESCRIPTION: This command filters and lists only the certified connectors using the support-level option.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --support-level=certified list\n```\n\n----------------------------------------\n\nTITLE: Configuring SFTP File Source in Markdown\nDESCRIPTION: Example table showing how to configure the File source connector for SFTP data sources with specific reader options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/file.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Dataset Name | Storage | User | Password | Host            | URL                     | Reader Options                                                            | Description                                                                                                                       |\n| ------------ | ------- | ---- | -------- | --------------- | ----------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n| Test Rebext  | SFTP    | demo | password | test.rebext.net | /pub/example/readme.txt | `{\"sep\": \"\\r\\n\", \"header\": null, \"names\": [\"text\"], \"engine\": \"python\"}` | We use `python` engine for `read_csv` in order to handle delimiter of more than 1 character while providing our own column names. |\n```\n\n----------------------------------------\n\nTITLE: Running Drift Source Connector Docker Commands (Bash)\nDESCRIPTION: This block provides several docker run commands for interacting with the built Drift source connector image. It uses standard source connector commands such as spec, check, discover, and read, mounting local directories for configuration and integration tests. These commands require a valid Docker installation and the airbyte/source-drift:dev image built previously. Input parameters include paths to configuration and catalog JSON files located in mounted volumes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-drift/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-drift:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-drift:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-drift:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-drift:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n\n```\n\n----------------------------------------\n\nTITLE: Running Docker Commands for Prestashop Source Connector\nDESCRIPTION: Standard Docker commands for running the Prestashop source connector, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-prestashop/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-prestashop:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-prestashop:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-prestashop:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-prestashop:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Traffic Sources Stream Schema in JSON\nDESCRIPTION: JSON schema example for the traffic_sources stream in Google Analytics, including metrics related to traffic sources and user engagement.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_source\":\"(direct)\",\"ga_medium\":\"(none)\",\"ga_socialNetwork\":\"(not set)\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Modifying AzureBlobStorageFormat Enum in Java\nDESCRIPTION: Add a new enum in the AzureBlobStorageFormat class to support a new output format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-azure-blob-storage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic enum AzureBlobStorageFormat {\n    // Add new enum here\n}\n```\n\n----------------------------------------\n\nTITLE: Describing the `active_declarative_manifest` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `active_declarative_manifest` table, which links an actor definition to its active manifest version. It includes columns for the actor definition ID (primary and foreign key), the manifest version, and timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name           | Datatype  | Description                                           |\n| --------------------- | --------- | ----------------------------------------------------- |\n| actor_definition_id   | UUID      | Primary key. References the `actor_definition` table. |\n| version              | BIGINT    | Version of the manifest.                              |\n| created_at           | TIMESTAMP | Timestamp when the record was created.               |\n| updated_at           | TIMESTAMP | Timestamp when the record was last updated.          |\n\n#### Indexes and Constraints\n\n- Primary Key: (`actor_definition_id`)\n- Foreign Key: `actor_definition_id` references `actor_definition(id)`\n```\n\n----------------------------------------\n\nTITLE: Running Connector Test Pipeline in Airbyte CI\nDESCRIPTION: This async function runs the connector test pipeline. It computes the steps to run, executes them concurrently according to their dependencies, and generates a ConnectorReport from the results. The function also handles static analysis steps if required.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def run_connector_test_pipeline(context: ConnectorContext, semaphore: anyio.Semaphore) -> ConnectorReport:\n    \"\"\"\n    Compute the steps to run for a connector test pipeline.\n    \"\"\"\n    all_steps_to_run: STEP_TREE = []\n\n    all_steps_to_run += get_test_steps(context)\n\n    if not context.code_tests_only:\n        static_analysis_steps_to_run = [\n            [\n                StepToRun(id=CONNECTOR_TEST_STEP_ID.VERSION_INC_CHECK, step=VersionIncrementCheck(context)),\n                StepToRun(id=CONNECTOR_TEST_STEP_ID.QA_CHECKS, step=QaChecks(context)),\n            ]\n        ]\n        all_steps_to_run += static_analysis_steps_to_run\n\n    async with semaphore:\n        async with context:\n            result_dict = await run_steps(\n                runnables=all_steps_to_run,\n                options=context.run_step_options,\n            )\n\n            results = list(result_dict.values())\n            report = ConnectorReport(context, steps_results=results, name=\"TEST RESULTS\")\n            context.report = report\n\n        return report\n```\n\n----------------------------------------\n\nTITLE: Example Response from List Workspaces Endpoint\nDESCRIPTION: Sample JSON response from the List Workspaces API endpoint showing the structure of returned workspace data, including workspace IDs, names, and data residency settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/api-access-config.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"workspaceId\": \"b5367aab-9d68-4fea-800f-0000000000\",\n      \"name\": \"Finance Team\",\n      \"dataResidency\": \"auto\"\n    },\n    {\n      \"workspaceId\": \"b5367aab-9d68-4fea-800f-0000000001\",\n      \"name\": \"Analytics Team\",\n      \"dataResidency\": \"auto\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Locations Stream Schema in JSON\nDESCRIPTION: JSON schema example for the locations stream in Google Analytics, including geographical metrics and user engagement data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_continent\":\"Americas\",\"ga_subContinent\":\"Northern America\",\"ga_country\":\"United States\",\"ga_region\":\"Iowa\",\"ga_metro\":\"Des Moines-Ames IA\",\"ga_city\":\"Des Moines\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":1,\"ga_sessionsPerUser\":1.0,\"ga_avgSessionDuration\":29.0,\"ga_pageviews\":7,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.666666666666667,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Installing CAT Dependencies\nDESCRIPTION: Commands to navigate to the CAT directory and install required dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-integrations/bases/connector-acceptance-test/\n\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Kvdb Connector\nDESCRIPTION: This command runs the custom integration tests for the Kvdb connector from the connector root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest integration_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Reddit Source Connector with airbyte-ci\nDESCRIPTION: Command to build the source-reddit connector using airbyte-ci, which creates a dev image (source-reddit:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-reddit/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-reddit build\n```\n\n----------------------------------------\n\nTITLE: Building the Printify Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Printify source connector locally, which creates a dev image (source-printify:dev) that can be used for testing the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-printify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-printify build\n```\n\n----------------------------------------\n\nTITLE: Building FreeAgent Source Connector in Bash\nDESCRIPTION: Command to build the FreeAgent source connector using airbyte-ci, which creates a dev image (source-free-agent-connector:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-free-agent-connector/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-free-agent-connector build\n```\n\n----------------------------------------\n\nTITLE: Building Dwolla Source Connector Image (Bash)\nDESCRIPTION: This Bash command utilizes the `airbyte-ci` tool to build a development Docker image tagged as `source-dwolla:dev`. This image can then be used for local testing of the Dwolla source connector. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dwolla/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dwolla build\n```\n\n----------------------------------------\n\nTITLE: Building the SimpleCast Connector in Bash\nDESCRIPTION: Command to build the SimpleCast connector locally, creating a dev image (source-simplecast:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simplecast/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simplecast build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Insightly Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Insightly source connector. The resulting image will be tagged as 'airbyte/source-insightly:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-insightly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-insightly build\n```\n\n----------------------------------------\n\nTITLE: Building the SendPulse Connector with airbyte-ci\nDESCRIPTION: Command to build the SendPulse connector locally, creating a development image called 'source-sendpulse:dev' that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendpulse/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendpulse build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-seller-partner/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amazon-seller-partner build\n```\n\n----------------------------------------\n\nTITLE: Selected Records Array Output - JSON\nDESCRIPTION: Demonstrates the expected output after extracting a single JSON object as a list of records using Airbyte’s record selector. Input is a JSON object, output is a single-element array containing the object. No dependencies. Often used for validation or illustration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Standard Airbyte Connector Commands via Docker for Partnerstack\nDESCRIPTION: These commands demonstrate how to execute standard Airbyte source connector operations (`spec`, `check`, `discover`, `read`) using the locally built Docker image (`airbyte/source-partnerstack:dev`). The `spec` command retrieves the connector specification. The `check`, `discover`, and `read` commands require mounting a local `secrets` directory (containing `config.json`) into the container; `read` also requires mounting `integration_tests` for the catalog file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-partnerstack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-partnerstack:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-partnerstack:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-partnerstack:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-partnerstack:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Kyriba Connector\nDESCRIPTION: Command to run unit tests for the Kyriba connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for PandaDoc Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the acceptance test suite for the `source-pandadoc` connector. It's typically used during local development to verify the connector's functionality against predefined test cases. Requires `airbyte-ci` and potentially the development image built in the previous step.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pandadoc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pandadoc test\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte RECORD Message for User 'b' (JSON)\nDESCRIPTION: This JSON line represents a single record message from the 'users' stream in the Airbyte protocol. It contains data for key 'b' with a corresponding escaped JSON string '{\"b\": 3}' as the value, and includes an emission timestamp. This format is typical for data exchange in Airbyte connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/integration_tests/expected_records.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"users\", \"data\": {\"key\": \"b\", \"value\": \"{\\\"b\\\": 3}\"}, \"emitted_at\": 1640962800000}\n```\n\n----------------------------------------\n\nTITLE: Installing abctl via Bash Script - Shell\nDESCRIPTION: Downloads and installs the latest version of abctl using a cURL command piped to bash. Requires an active Internet connection and will install abctl in the user's PATH for immediate use. No parameters required; installation progress is output to the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/abctl-ec2.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -LsfS https://get.airbyte.com | bash -\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table in Markdown\nDESCRIPTION: Markdown table showing the supported features of the Zendesk Sell connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-sell.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n```\n\n----------------------------------------\n\nTITLE: Listing Doris Destination Setup Parameters in Markdown\nDESCRIPTION: Enumerates the required setup parameters for connecting to a Doris destination, including Host, HttpPort, QueryPort, Username, Password, and Database.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/doris.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- **Host**\n- **HttpPort**\n- **QueryPort**\n- **Username**\n- **Password**\n- **Database**\n```\n\n----------------------------------------\n\nTITLE: Running MySQL Connector Performance Tests (PR)\nDESCRIPTION: Command for triggering performance tests in pull requests with optional resource limits.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n/test-performance connector=connectors/source-mysql [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Cloud Sign-out Redirect URI Configuration\nDESCRIPTION: The sign-out redirect URI pattern required for Airbyte Cloud SSO integration with Okta. Contains a placeholder for company identifier.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/okta.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://cloud.airbyte.com/auth/realms/<your-company-identifier>/broker/default/endpoint/logout_response\n```\n\n----------------------------------------\n\nTITLE: Installing the Amazon SQS Connector with Poetry\nDESCRIPTION: Command to install the connector and its dependencies using Poetry package manager. The --with dev flag includes development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Younium connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-younium/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-younium:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-younium:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-younium:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-younium:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Specifying 'requests' Library Version in Python Dependencies\nDESCRIPTION: This line specifies that the Python project requires the 'requests' library, pinned to version 2.26.0. This is commonly found in a `requirements.txt` file and used by the `pip` package manager to install the exact version of the library needed for making HTTP requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/fixtures/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nrequests==2.26.0\n```\n\n----------------------------------------\n\nTITLE: Patch Creation Error\nDESCRIPTION: Error message that can occur when running abctl with low-resource-mode after a standard installation, showing conflicts in the patch list.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nunable to install airbyte chart: unable to install helm: failed to create patch:\nThe order in patch list: [map[name:JOB_MAIN_CONTAINER_CPU_REQUEST value:0]\nmap[name:JOB_MAIN_CONTAINER_CPU_REQUEST valueFrom:map[configMapKeyRef:\nmap[key:JOB_MAIN_CONTAINER_CPU_REQUEST name:airbyte-abctl-airbyte-env]]]\n map[name:JOB_MAIN_CONTAINER_CPU_LIMIT value:0] map[name:JOB_MAIN_CONTAINER_CPU_LIMIT\n valueFrom:map[configMapKeyRef:map[key:JOB_MAIN_CONTAINER_CPU_LIMIT\n name:airbyte-abctl-airbyte-env]]] map[name:JOB_MAIN_CONTAINER_MEMORY_REQUEST\n value:0] map[name:JOB_MAIN_CONTAINER_MEMORY_REQUEST\n```\n\n----------------------------------------\n\nTITLE: Setting up GSM Credentials\nDESCRIPTION: Commands to set up Google Secrets Manager credentials and install the credentials tool for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GCP_GSM_CREDENTIALS=`cat <path-to-gsm-service-account-key-file>`\n\npipx install airbyte-ci/connectors/ci_credentials/ --force --editable\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Processing Conflict Stream with Scalar Values in JSON\nDESCRIPTION: These JSON records show how scalar values are handled in conflict streams. Each record contains an 'id' and a 'conflict_stream_scalar' field with a simple numeric value.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_scalar\",\"data\":{\"id\":1,\"conflict_stream_scalar\": 2},\"emitted_at\":1623861660}}\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"conflict_stream_scalar\",\"data\":{\"id\":2,\"conflict_stream_scalar\": 2},\"emitted_at\":1623861660}}\n```\n\n----------------------------------------\n\nTITLE: Importing AvailabilityStrategy (Python)\nDESCRIPTION: Imports the `AvailabilityStrategy` class from the Airbyte CDK. This class is part of the legacy mechanism for checking stream availability, which is being disabled in favor of custom error handling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# import this library\nfrom airbyte_cdk.sources.streams.availability_strategy import AvailabilityStrategy\n```\n\n----------------------------------------\n\nTITLE: Running dbt debug command\nDESCRIPTION: Command to verify that dbt setup is configured correctly by running the debug command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt debug\n```\n\n----------------------------------------\n\nTITLE: Setting OpenSSL Build Flags for macOS (Bash)\nDESCRIPTION: Exports environment variables `LDFLAGS` and `CPPFLAGS` to specify the library and include paths for OpenSSL, respectively. This is often required on macOS with Homebrew-installed OpenSSL to allow C extensions (installed via pip during the build) to compile correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport LDFLAGS=\"-L/usr/local/opt/openssl/lib\"\nexport CPPFLAGS=\"-I/usr/local/opt/openssl/include\"\n```\n\n----------------------------------------\n\nTITLE: Building Deputy Airbyte Connector with Airbyte CI - Bash\nDESCRIPTION: Builds a development Docker image for the 'source-deputy' connector using the Airbyte CI command-line tool. Requires that 'airbyte-ci' is installed and available on the system. The produced image, 'source-deputy:dev', can be used locally for further testing or development. Key parameters: '--name=source-deputy' specifies the connector, 'build' triggers the image build process. Input: the command itself; Output: Docker image available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-deputy/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-deputy build\n```\n\n----------------------------------------\n\nTITLE: Note about schema changes detection in Airbyte\nDESCRIPTION: A note indicating that any detected schema changes will be listed for user review during the refresh source schema process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-ads-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nAny detected schema changes will be listed for your review.\n```\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appsflyer build\n```\n\n----------------------------------------\n\nTITLE: Defining Thinkific Courses Streams in Markdown\nDESCRIPTION: This snippet outlines the available data streams for the Thinkific Courses connector, including their primary keys, pagination methods, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific-courses.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| Courses |  | DefaultPaginator | ✅ |  ❌  |\n| Courses Chapters |  | DefaultPaginator | ✅ |  ❌  |\n| Contents |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Younium Supported Features Table\nDESCRIPTION: Markdown table showing the supported sync modes and features of the Younium connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/younium.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental - Append Sync     | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\n```\n\n----------------------------------------\n\nTITLE: Build Customization Python Module\nDESCRIPTION: Example module for customizing the connector build process with pre and post installation hooks using Dagger\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-milvus/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Decoding Base64 Encoded Kubernetes Secret Value (Shell)\nDESCRIPTION: Shows how to use the `base64 -d` command in Shell to decode a base64 encoded string. This is typically used after retrieving secrets from Kubernetes, such as the Airbyte password or client secret.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/integrations/authentication.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\necho 'cmhvQkhCODlMRmh1REdXMWt3REpHZTJMaUd3N3c2MjU=' | base64 -d\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Whisky Hunter Source Connector Commands\nDESCRIPTION: These commands demonstrate how to run standard source connector operations using the Whisky Hunter docker container. They include running spec, check, discover, and read operations with appropriate configurations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-whisky-hunter/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-whisky-hunter:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-whisky-hunter:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-whisky-hunter:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-whisky-hunter:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Snowflake Integration Environment\nDESCRIPTION: SQL commands to remove all integration environment components including database, user, role, and warehouse. Useful for resetting the environment or cleaning up resources.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/setup/snowflake.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nDROP DATABASE IF EXISTS INTEGRATION_TEST_NORMALIZATION;\nDROP USER IF EXISTS INTEGRATION_TEST_USER_NORMALIZATION;\nDROP ROLE IF EXISTS INTEGRATION_TESTER_NORMALIZATION;\nDROP WAREHOUSE IF EXISTS INTEGRATION_TEST_WAREHOUSE_NORMALIZATION;\n```\n\n----------------------------------------\n\nTITLE: Building the Twelve Data Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Twelve Data source connector using airbyte-ci. The resulting image is tagged as 'source-twelve-data:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twelve-data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twelve-data build\n```\n\n----------------------------------------\n\nTITLE: Building Convex Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: Builds the Docker image for the `source-convex` connector using the `airbyte-ci` tool. The resulting image will be tagged as `airbyte/source-convex:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-convex build\n```\n\n----------------------------------------\n\nTITLE: Using abs Filter in Jinja2\nDESCRIPTION: Demonstrates the `abs` filter in Jinja2, which returns the absolute value of a number. The example applies it to the number -3 within a template expression, resulting in 3.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_15\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ -3|abs }}\n```\n\n----------------------------------------\n\nTITLE: Building the Metabase Source Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the `source-metabase` connector. It requires `airbyte-ci` to be installed first. The resulting image will be tagged as `airbyte/source-metabase:dev` locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-metabase/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-metabase build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Apify dataset connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apify-dataset/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apify-dataset test\n```\n\n----------------------------------------\n\nTITLE: Example JSON Object with Nested Arrays - JSON\nDESCRIPTION: A JSON response containing an array of record objects under the 'data' key and additional metadata. Intended as input for field-based record extraction using the field_path [\"data\"] in Airbyte's selector configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [{\"id\": 0}, {\"id\": 1}],\n  \"metadata\": {\"api-version\": \"1.0.0\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Testing the source-clarif-ai Connector (Bash)\nDESCRIPTION: This Bash command uses the `airbyte-ci` tool to execute the acceptance tests defined for the `source-clarif-ai` connector. This is typically run after building the connector locally to ensure its functionality meets the required standards. Requires `airbyte-ci` and a built connector image (`source-clarif-ai:dev` or similar).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clarif-ai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clarif-ai test\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest\nDESCRIPTION: Command to execute the test suite using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Running TiDB Source Connector Unit Tests with Gradle\nDESCRIPTION: Command to run unit tests for the TiDB source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tidb/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-tidb:unitTest\n```\n\n----------------------------------------\n\nTITLE: Testing Beamer Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Beamer source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-beamer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-beamer test\n```\n\n----------------------------------------\n\nTITLE: Building the Source Piwik Connector Locally using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image tagged as `source-piwik:dev`. This image can then be used for local testing and development of the connector. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-piwik/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-piwik build\n```\n\n----------------------------------------\n\nTITLE: Publishing Java CDK via Slash Commands\nDESCRIPTION: GitHub slash commands for publishing the Java CDK with different options for dry-run and force publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n/publish-java-cdk                # Run with the defaults (dry-run=false, force=false)\n/publish-java-cdk dry-run=true   # Run in dry-run mode (no-op)\n/publish-java-cdk force=true     # Force-publish if needing to replace an already published version\n```\n\n----------------------------------------\n\nTITLE: Removing airbyte-ci from the system\nDESCRIPTION: Command to completely remove the airbyte-ci command from the system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nmake tools.airbyte-ci.clean\n```\n\n----------------------------------------\n\nTITLE: Basic YAML Value Reference\nDESCRIPTION: Demonstrates basic value referencing using #/ syntax to reference previously defined values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/references.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkey: 1234\nreference: \"#/key\"\n```\n\n----------------------------------------\n\nTITLE: Building Buzzsprout Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Buzzsprout source connector using airbyte-ci tool. Creates a dev image tagged as source-buzzsprout:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-buzzsprout/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-buzzsprout build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector's spec, check, and write operations locally using Python\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pgvector/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run python main.py spec\npoetry run python main.py check --config secrets/config.json\ncat examples/messages.jsonl | poetry run python main.py write --config secrets/config.json --catalog examples/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Yellowbrick Connector\nDESCRIPTION: Command to run acceptance and custom integration tests for the Yellowbrick connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-yellowbrick/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-yellowbrick:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: JSON Record with All Null Values\nDESCRIPTION: Third record showing all fields set to null values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_messages_in.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"problematic_types\", \"emitted_at\": 1602637589300, \"data\": { \"schemaless_object\": null, \"schematized_object\": null, \"combined_type\": null, \"union_type\": null, \"schemaless_array\": null, \"mixed_array_integer_and_schemaless_object\": null, \"array_of_union_integer_and_schemaless_array\": null, \"union_of_objects_with_properties_identical\": null, \"union_of_objects_with_properties_overlapping\": null, \"union_of_objects_with_properties_nonoverlapping\": null, \"empty_object\": null, \"object_with_null_properties\": null, \"combined_with_null\": null, \"union_with_null\": null, \"combined_nulls\": null, \"compact_union\": null } } }\n```\n\n----------------------------------------\n\nTITLE: Parsing Deduplicated CDC Excluded JSON Record for Deletion\nDESCRIPTION: This JSON object represents a deduplicated CDC excluded record for deletion. It demonstrates the ability to delete a record inserted in a previous sync, including CDC-specific timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_schema_change.txt#2025-04-23_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"dedup_cdc_excluded\",\"data\":{\"id\":8,\"name\":\"ford\",\"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\",\"_ab_cdc_updated_at\":1625000000000,\"_ab_cdc_lsn\":29020252,\"_ab_cdc_deleted_at\":1625000000000},\"emitted_at\":1625000000000}}\n```\n\n----------------------------------------\n\nTITLE: Building Castor EDC Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the Castor EDC connector named 'source-castor-edc:dev' for local testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-castor-edc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-castor-edc build\n```\n\n----------------------------------------\n\nTITLE: Linking to Airbyte Contributing Documentation in Markdown\nDESCRIPTION: This snippet provides a Markdown link to the official Airbyte contributing guidelines hosted on their documentation website. It directs potential contributors to the comprehensive guide for participating in the project.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Contributing\n\nView on [docs.airbyte.io](https://docs.airbyte.io/contributing-to-airbyte)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Amplitude source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amplitude/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-amplitude build\n```\n\n----------------------------------------\n\nTITLE: Displaying Doris Destination Features Support in Markdown Table\nDESCRIPTION: A markdown table showing the supported features of the Doris destination connector, including Full Refresh Sync, Incremental - Append Sync, and WAL/Logical replication support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/doris.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                                | Supported?(Yes/No) | Notes                    |\n| :------------------------------------- | :----------------- | :----------------------- |\n| Full Refresh Sync                      | Yes                |                          |\n| Incremental - Append Sync              | Yes                |                          |\n| Incremental - Append + Deduped         | No                 | it will soon be realized |\n| For databases, WAL/Logical replication | Yes                |                          |\n```\n\n----------------------------------------\n\nTITLE: Displaying Note on Data Reset in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display a note about the potential for data reset depending on the destination type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stripe-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nDepending on destination type you may not be prompted to reset your data.\n```\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Public-Apis Connector\nDESCRIPTION: Command to run pytest unit tests for the connector from the connector directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Configuration Example - Repository Format\nDESCRIPTION: Examples of how to specify GitHub repositories in the connector configuration, including single repository, multiple repositories, and organization-wide settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/unit_tests/data/docs/correct.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nairbytehq/airbyte           # single repository\nairbytehq/airbyte airbytehq/another-repo  # multiple repositories\nairbytehq/*                    # all repositories in organization\n```\n\n----------------------------------------\n\nTITLE: Using escape Filter in Jinja2\nDESCRIPTION: Demonstrates the `escape` filter in Jinja2, which escapes HTML characters (like '<', '>', '&') in a string to prevent them from being interpreted as HTML markup. The example escapes '<div>'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_21\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '<div>'|escape }}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Emailoctopus Source Connector\nDESCRIPTION: Command to run the full test suite for the Emailoctopus source connector using airbyte-ci. This validates that the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-emailoctopus/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-emailoctopus test\n```\n\n----------------------------------------\n\nTITLE: Configuring Acceptance Tests in YAML for Airbyte Connectors\nDESCRIPTION: Enables acceptance tests for medium to high use connectors by adding a configuration option in the metadata.yaml file. This ensures the connector is working as expected for frequently used integrations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nconnectorTestSuitesOptions:\n  suite: acceptanceTests\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte Connector Emitted Records (JSON Lines)\nDESCRIPTION: Shows the format of records emitted by the Airbyte Firebase Realtime Database connector for the 'dinosaurs' stream, corresponding to the fetched data. Each line is a JSON object representing an Airbyte record, containing the stream name, the original JSON key, the stringified JSON value, and an emission timestamp. This format is typical for Airbyte source connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/bootstrap.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"dinosaurs\", \"data\": {\"key\": \"lambeosaurus\", \"value\": \"{\\\"height\\\": 2.1,\\\"length\\\": 12.5,\\\"weight\\\": 5000}\"}, \"emitted_at\": 1640962800000}\n{\"stream\": \"dinosaurs\", \"data\": {\"key\": \"stegosaurus\", \"value\": \"{\\\"height\\\": 4,\\\"length\\\": 9,\\\"weight\\\": 2500}\"}, \"emitted_at\": 1640962800000}\n```\n\n----------------------------------------\n\nTITLE: Testing Box Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Box source connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-box test\n```\n\n----------------------------------------\n\nTITLE: Copying DuckDB File from Docker Container in Bash\nDESCRIPTION: Command to copy a DuckDB database file from the Airbyte server container to the host machine's current working directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/duckdb.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path} .\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Jina AI Reader Connector\nDESCRIPTION: Command to build a Docker image for the Jina AI Reader connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jina-ai-reader build\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Source PostHog CLI Commands Locally - Bash\nDESCRIPTION: Executes various Airbyte Source PostHog connector commands ('spec', 'check', 'discover', 'read') locally using Poetry. The '--config' flag specifies the required config file, and '--catalog' (for 'read') provides a catalog file. These commands allow users to interactively test the connector's specification, check configuration, discover available data, and read data locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-posthog/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-posthog spec\npoetry run source-posthog check --config secrets/config.json\npoetry run source-posthog discover --config secrets/config.json\npoetry run source-posthog read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building SimFin Connector in Airbyte\nDESCRIPTION: This command builds a development image (source-simfin:dev) using airbyte-ci for local testing of the SimFin connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simfin/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simfin build\n```\n\n----------------------------------------\n\nTITLE: Parsing Renamed Deduplicated CDC Excluded JSON Records\nDESCRIPTION: These JSON objects represent renamed and deduplicated CDC (Change Data Capture) excluded records. They include fields for ID, name, and CDC-specific timestamps.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_schema_change.txt#2025-04-23_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"renamed_dedup_cdc_excluded\",\"data\":{\"id\":8,\"name\":\"vw\",\"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\",\"_ab_cdc_updated_at\":1623949314663,\"_ab_cdc_lsn\":26985264,\"_ab_cdc_deleted_at\":null},\"emitted_at\":1623960160}}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bing-ads build\n```\n\n----------------------------------------\n\nTITLE: Running Metabase Source Connector CI Tests using airbyte-ci (Bash)\nDESCRIPTION: This command executes the full Airbyte connector test suite for the `source-metabase` connector locally. It uses the `airbyte-ci` tool, which must be installed beforehand. This is used to verify connector functionality and adherence to standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-metabase/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-metabase test\n```\n\n----------------------------------------\n\nTITLE: Running the Copper Connector CI Test Suite using airbyte-ci\nDESCRIPTION: Executes the full CI test suite for the source-copper connector locally using the airbyte-ci tool. This helps ensure the connector functions correctly and meets quality standards. Requires airbyte-ci to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-copper/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-copper test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-timeplus test\n```\n\n----------------------------------------\n\nTITLE: Parsing Exchange Rate JSON Records\nDESCRIPTION: These JSON objects represent exchange rate data records with various fields including currency, date, and special characters. The records are part of the 'exchange_rate' stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_schema_change.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602661281900, \"data\": { \"id\": 3.14, \"currency\": \"EUR\", \"new_column\": 2.1, \"date\": \"2020-11-01\", \"timestamp_col\": \"2020-11-01T00:00:00Z\", \"NZD\": 2.43, \"HKD@spéçiäl & characters\": 2.12, \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\", \"USD\": 7}}}\n```\n\n----------------------------------------\n\nTITLE: Testing Stream with Numeric Prefix in JSON\nDESCRIPTION: This snippet demonstrates records for a stream with a numeric prefix ('1_prefix_startwith_number'). It includes multiple records with varying timestamps and data to test handling of streams with non-standard names.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"1_prefix_startwith_number\",\n    \"emitted_at\": 1602637589000,\n    \"data\": {\n      \"id\": 1,\n      \"date\": \"2020-08-29\",\n      \"text\": \"hi 1\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for abctl Kind Cluster (Shell)\nDESCRIPTION: Exports the Kubernetes configuration from the `kind` cluster named `airbyte-abctl`, allowing `kubectl` commands to target this specific cluster. Requires `kind` CLI to be installed and the `airbyte-abctl` cluster to be running.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkind export kubeconfig -n airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Stripe Connector\nDESCRIPTION: This command runs the full test suite for the Stripe connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-stripe test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-airtable/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table\nDESCRIPTION: Defines the mapping between Appstore API data types and Airbyte data types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appstore.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type         | Airbyte Type | Notes |\n| :----------------------- | :----------- | :---- |\n| `string`                 | `string`     |       |\n| `int`, `float`, `number` | `number`     |       |\n| `date`                   | `date`       |       |\n| `datetime`               | `datetime`   |       |\n| `array`                  | `array`      |       |\n| `object`                 | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Listing Connectors and Saving Output to File\nDESCRIPTION: This command lists all connectors and writes the output to a JSON file named 'connectors.json'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors list --output=connectors.json\n```\n\n----------------------------------------\n\nTITLE: Running Mailgun Connector Test Suite\nDESCRIPTION: Command to run the full test suite for the Mailgun connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailgun/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailgun test\n```\n\n----------------------------------------\n\nTITLE: Running dbt debug with custom profiles directory\nDESCRIPTION: Command to verify dbt setup with custom profiles and project directories pointing to the current working directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt debug --profiles-dir=$(pwd) --project-dir=$(pwd)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-databend build\n```\n\n----------------------------------------\n\nTITLE: Building the Employment Hero Connector for Local Development\nDESCRIPTION: This command builds a development image of the Employment Hero connector that can be used for local testing. The resulting image will be tagged as 'source-employment-hero:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-employment-hero/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-employment-hero build\n```\n\n----------------------------------------\n\nTITLE: Querying Captain Data API v3 Endpoint\nDESCRIPTION: Example GET request to the base URL for the Captain Data API version 3. This endpoint is the foundation for API interactions used by the Airbyte connector to fetch data. An HTTP client is needed to execute this request.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/captain-data.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.captaindata.co/v3/\n```\n\n----------------------------------------\n\nTITLE: Streaming Sheet6-2000-rows Data in JSON Format\nDESCRIPTION: This code snippet represents a single JSON record from the Sheet6-2000-rows stream. It includes the stream name, data with ID and Name fields, and an emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"491\",\"Name\":\"mEgXbuQta\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Using wordwrap Filter in Jinja2\nDESCRIPTION: Demonstrates the `wordwrap` filter in Jinja2, which wraps words in a string to a specified line length, inserting newline characters. The example wraps 'hello world' at a width of 5.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_55\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello world'|wordwrap(5) }}\n```\n\n----------------------------------------\n\nTITLE: Defining MQTT Output Schema in Markdown\nDESCRIPTION: Describes the structure of the output data written to MQTT topics, including the four fields present in each record's payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mqtt.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `_airbyte_ab_id`: an uuid assigned by Airbyte to each event that is processed.\n- `_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n- `_airbyte_data`: a json blob representing with the event data.\n- `_airbyte_stream`: the name of each record's stream.\n```\n\n----------------------------------------\n\nTITLE: Using time Filter in Jinja2\nDESCRIPTION: Demonstrates the `time` filter in Jinja2 (requires Babel library), which formats a time object according to a specific format string (using strftime codes). The example assumes a variable `time` holds a time object and formats it as 'HH:MM:SS'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_59\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ time|time('%H:%M:%S') }}\n```\n\n----------------------------------------\n\nTITLE: Building When I Work Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the When I Work connector using airbyte-ci tool. Creates a dev image tagged as source-when-i-work:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-when-i-work/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-when-i-work build\n```\n\n----------------------------------------\n\nTITLE: Building the Source Fillout Connector Locally (Bash)\nDESCRIPTION: Builds a local development Docker image named `source-fillout:dev` using the `airbyte-ci` tool. This command compiles the connector code and packages it into an image that can be used for local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fillout/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fillout build\n```\n\n----------------------------------------\n\nTITLE: Using first Filter in Jinja2\nDESCRIPTION: Demonstrates the `first` filter in Jinja2, which returns the first item from a sequence (like a list). The example retrieves the first item from `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_23\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|first }}\n```\n\n----------------------------------------\n\nTITLE: Running the Secoda Source Connector Test Suite\nDESCRIPTION: Command to run the full test suite for the Secoda source connector using airbyte-ci, which verifies the connector's functionality and compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-secoda/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-secoda test\n```\n\n----------------------------------------\n\nTITLE: Creating Prefect Project for Airbyte\nDESCRIPTION: Command to create a new Prefect project named 'airbyte'\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-prefect-task.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nprefect create project \"airbyte\"\n```\n\n----------------------------------------\n\nTITLE: Importing urlparse for URL Manipulation in Python\nDESCRIPTION: This Python code snippet imports the `urlparse` function from the `urllib.parse` library. This function is necessary for parsing the next page URL provided by the SurveyMonkey API to extract query parameters for subsequent paginated requests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# add next library to import section\nfrom urllib.parse import urlparse\n```\n\n----------------------------------------\n\nTITLE: Testing the Rollbar Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-rollbar connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rollbar/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rollbar test\n```\n\n----------------------------------------\n\nTITLE: Running Python Tests\nDESCRIPTION: Command to execute the Python test suite using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: MixMax Available Streams Schema\nDESCRIPTION: Detailed table showing available data streams with their primary keys, pagination type, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mixmax.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| appointmentlinks | userId | DefaultPaginator | ✅ |  ❌  |\n| codesnippets | _id | DefaultPaginator | ✅ |  ✅  |\n| insightsreports | _id | DefaultPaginator | ✅ |  ✅  |\n| integrations_commands | _id | DefaultPaginator | ✅ |  ❌  |\n| integrations_enhancements | _id | DefaultPaginator | ✅ |  ❌  |\n| integrations_linkresolvers | _id | DefaultPaginator | ✅ |  ✅  |\n| integrations_sidebars | _id | DefaultPaginator | ✅ |  ❌  |\n| livefeed | uid | DefaultPaginator | ✅ |  ❌  |\n| meetingtypes | _id | DefaultPaginator | ✅ |  ✅  |\n| messages | _id | DefaultPaginator | ✅ |  ✅  |\n| rules | _id | DefaultPaginator | ✅ |  ✅  |\n| rules_actions | _id | DefaultPaginator | ✅ |  ✅  |\n| sequences | _id | DefaultPaginator | ✅ |  ✅  |\n| sequences_recipients | _id | DefaultPaginator | ✅ |  ✅  |\n| sequencefolders | _id | DefaultPaginator | ✅ |  ✅  |\n| snippettags | _id | DefaultPaginator | ✅ |  ✅  |\n| snippettags_snippets | _id | DefaultPaginator | ✅ |  ✅  |\n| userpreferences_me | _id | DefaultPaginator | ✅ |  ❌  |\n| users_me | _id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Testing Illumina Basespace Source Connector in Bash\nDESCRIPTION: This command uses airbyte-ci to run acceptance tests for the Illumina Basespace source connector. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-illumina-basespace/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-illumina-basespace test\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table\nDESCRIPTION: Mapping between Zendesk Talk data types and their corresponding Airbyte types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-talk.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type |\n|:-----------------|:-------------|\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Datetime without Timezone Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for datetime without timezone data tests in Airbyte's JSON record format. Includes examples with different datetime values without timezone information, covering current, historical, and far-future datetimes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_2\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"2022-11-22T01:23:45\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_2\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"1504-02-29T01:23:45\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_2\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"9999-12-21T01:23:45\" }}}\n```\n\n----------------------------------------\n\nTITLE: Integer Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for integer data tests in Airbyte's JSON record format. Includes examples with positive integers, zero, and negative integers with timestamps and stream identifiers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : 42 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : 0 }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"integer_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : -12345 }}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Faker Source with User Count for Data Generation in JSON\nDESCRIPTION: Configuration file for the faker source connector that specifies the number of users to generate (over 2 billion for 1TB of data) and the random seed value.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-faker/csv_export/README.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"count\": 2039840637,\n  \"seed\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linkedin-ads build\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Vectara Connector\nDESCRIPTION: Example Dockerfile to create a custom build of the Vectara connector based on the official image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-vectara:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Surveycto Connector\nDESCRIPTION: Command to build a Docker image for the Surveycto connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-surveycto build\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Table defining the required configuration parameters for ZapSign API integration, including api_token, start_date, and signer_ids.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zapsign.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_token` | `string` | API Token. Your static API token for authentication. You can find it in your ZapSign account under the 'Settings' or 'API' section. For more details, refer to the [Getting Started](https://docs.zapsign.com.br/english/getting-started#how-do-i-get-my-api-token) guide. |  |\n| `start_date` | `string` | Start date.  |  |\n| `signer_ids` | `array` | Signer IDs. The signer ids for signer stream |  |\n```\n\n----------------------------------------\n\nTITLE: Installing Connectors Insights with Poetry\nDESCRIPTION: Command to install the Connectors Insights project and its dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_insights/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: JSON Stream Records from Sheet6-2000-rows\nDESCRIPTION: Sequence of JSON records containing ID and Name field pairs from Sheet6, along with stream name and emission timestamp. Each record represents one row from a 2000-row spreadsheet.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"803\",\"Name\":\"rKKMNpyiG\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"804\",\"Name\":\"LcbtgoooG\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Installing CI Credentials Tool Locally\nDESCRIPTION: Command to install the ci_credentials tool in editable mode using pipx.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npipx install --editable --force --python=python3.11 airbyte-ci/connectors/ci_credentials/\n```\n\n----------------------------------------\n\nTITLE: Running Qualaroo Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Qualaroo source connector using airbyte-ci tool. This validates that your connector implementation meets all requirements.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qualaroo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-qualaroo test\n```\n\n----------------------------------------\n\nTITLE: Listing Affected Streams for LinkedIn Ads v5.0.0 Primary Key Change (Text)\nDESCRIPTION: Lists the specific streams (`ad_campaign_analytics`, `Custom Ad Analytics Reports`) affected by the primary key changes introduced in version 5.0.0 of the LinkedIn Ads connector. Users need to refresh these streams as part of the migration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n- `ad_campaign_analytics`\n- `Custom Ad Analytics Reports`\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record with NaN Value\nDESCRIPTION: JSON record format for Airbyte showing a data record containing a NaN (Not a Number) value. Includes the record type, stream name, emission timestamp, and data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/nan_type_test_message.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"nan_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"NaN\" }}}\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Unit and Integration Tests - Bash\nDESCRIPTION: This Bash snippet runs pytest on the 'tests' directory using the Poetry-managed environment. Requires test dependencies to be installed via Poetry. Outputs test results and status for any included unit or integration tests written for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Building Teamtailor Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Teamtailor source connector for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teamtailor/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-teamtailor build\n```\n\n----------------------------------------\n\nTITLE: Adding dependencies to Shopify connector\nDESCRIPTION: Command to add new dependencies to the Shopify connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Pardot Connector Docker Image (Bash)\nDESCRIPTION: Uses the 'airbyte-ci' tool to build the Docker image for the Pardot source connector. This command requires 'airbyte-ci' to be installed and should be run from a context where 'airbyte-ci' can locate the connector definition. The resulting image will be tagged as 'airbyte/source-pardot:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pardot/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pardot build\n```\n\n----------------------------------------\n\nTITLE: Running Surveycto Connector as Docker Container\nDESCRIPTION: Series of commands to run various connector operations using the Docker container, including spec, check, discover, and read functions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-surveycto:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-surveycto:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-surveycto:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-surveycto:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Mailjet Mail connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-mail/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailjet-mail test\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record Structure in JSON\nDESCRIPTION: Shows the format of an Airbyte record object containing a stream name, data payload with ID and updated_at timestamp, and record metadata including emitted_at timestamp. This structure is used for data synchronization in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_sparse_nested_streams/data_input/messages3.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"sparse_nested_stream\", \"data\": {\"id\": 3, \"updated_at\": 102}, \"emitted_at\": 1672569200}}\n```\n\n----------------------------------------\n\nTITLE: Testing the Datascope Connector with Airbyte CI - Bash\nDESCRIPTION: This shell snippet runs the complete Airbyte CI test suite for the Datascope source connector, evaluating changes or local connector implementations. The airbyte-ci tool must be installed and the --name parameter set appropriately for the connector. Upon execution, the test results will be displayed in the CLI, providing feedback for connector modifications. No other input parameters are necessary beyond those shown.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-datascope/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-datascope test\n```\n\n----------------------------------------\n\nTITLE: JSON Object with Various Schema Types and Nested Structures\nDESCRIPTION: A comprehensive JSON object containing multiple data types including schemaless objects, schematized objects, union types, arrays with mixed data types, and objects with different property patterns. This appears to be test data for schema validation or transformation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_coerced_schemaless_messages_out.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"schemaless_object\":\"{\\\"uuid\\\":\\\"38F52396-736D-4B23-B5B4-F504D8894B97\\\",\\\"probability\\\":1.5}\",\"schematized_object\":{\"id\":1,\"name\":\"Joe\"},\"combined_type\":\"string1\",\"union_type\":10,\"schemaless_array\":\"[10,\\\"foo\\\",null,{\\\"bar\\\":\\\"qua\\\"}]\",\"mixed_array_integer_and_schemaless_object\":[15,null,\"{\\\"hello\\\":\\\"world\\\"}\"],\"array_of_union_integer_and_schemaless_array\":[25,null,\"[\\\"goodbye\\\",\\\"cruel world\\\"]\"],\"union_of_objects_with_properties_identical\":{\"id\":10,\"name\":\"Joe\"},\"union_of_objects_with_properties_overlapping\":{\"id\":20,\"name\":\"Jane\",\"flagged\":true},\"union_of_objects_with_properties_nonoverlapping\":{\"id\":30,\"name\":\"Phil\",\"flagged\":false,\"description\":\"Very Phil\"}, \"union_of_objects_with_properties_contradicting\": { \"id\": 1, \"name\": \"Jenny\" }, \"empty_object\": \"{}\", \"object_with_null_properties\": \"{}\", \"combined_with_null\": \"foobar\", \"union_with_null\": \"barfoo\", \"combined_nulls\": null, \"compact_union\": { \"id\": 10, \"name\": \"Tyler\" } }\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies to SQLite Connector\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sqlite/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Parsing Sheet6-2000-rows Stream Data in JSON\nDESCRIPTION: This snippet shows the structure of a single JSON object from the Sheet6-2000-rows stream. Each object contains a stream name, data with ID and Name fields, and an emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_15\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1732\",\"Name\":\"kZIZIPEtk\"},\"emitted_at\":1673989570000}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yandex-metrica test\n```\n\n----------------------------------------\n\nTITLE: Testing Buildkite Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Buildkite source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-buildkite/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-buildkite test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Apple Search Ads connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apple-search-ads/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apple-search-ads test\n```\n\n----------------------------------------\n\nTITLE: Configuring Survicate Source Connector in Markdown\nDESCRIPTION: This snippet outlines the configuration parameters for the Survicate source connector, including the API key and start date inputs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/survicate.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Using trim Filter in Jinja2\nDESCRIPTION: Demonstrates the `trim` filter in Jinja2, which removes leading and trailing whitespace from a string. The example trims the whitespace from '  hello  '.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_49\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ '  hello  '|trim }}\n```\n\n----------------------------------------\n\nTITLE: Running the Check Operation via CLI (Bash)\nDESCRIPTION: This command uses `poetry` to execute the `check` operation of the `source-survey-monkey-demo` connector, providing the configuration file path via the `--config` argument. This is used to test the connection verification logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/4-check-and-error-handling.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo check --config secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Exchange Rate Records JSON\nDESCRIPTION: Sample exchange rate records demonstrating data duplication and timestamp variations. Includes special characters in field names and various currency conversions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_incremental.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637990800, \"data\": { \"id\": 2, \"currency\": \"EUR\", \"date\": \"\", \"timestamp_col\": \"\", \"NZD\": 2.43, \"HKD@spéçiäl & characters\": 5.4, \"HKD_special___characters\": \"column name collision?\", \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\"}}}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Recreation Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Recreation source connector using airbyte-ci, resulting in an image tagged as airbyte/source-recreation:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recreation/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recreation build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector's Docker container for various operations including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-braintree:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-braintree:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-braintree:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-braintree:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-instagram build\n```\n\n----------------------------------------\n\nTITLE: Testing SAP Fieldglass Connector with CI Suite\nDESCRIPTION: Command to run Airbyte's full test suite locally for the SAP Fieldglass connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sap-fieldglass/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sap-fieldglass test\n```\n\n----------------------------------------\n\nTITLE: Removing Git Hooks\nDESCRIPTION: Command to uninstall git hooks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nmake tools.git-hooks.clean\n```\n\n----------------------------------------\n\nTITLE: Testing the OpinionStage Airbyte Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet runs acceptance tests for the OpinionStage Airbyte connector using the airbyte-ci command-line interface. It ensures that the connector meets functionality and integration requirements. The command requires airbyte-ci to be installed and should be executed in the connector's root directory. Successful completion validates the connector setup and readiness for deployment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-opinion-stage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-opinion-stage test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Redis Destination Connector\nDESCRIPTION: Gradle command to run unit tests for the Redis destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redis/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-redis:unitTest\n```\n\n----------------------------------------\n\nTITLE: JSON Record with Complex Data Types - First Example\nDESCRIPTION: First record showing various data type combinations including schemaless objects, arrays, union types and nested structures.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_messages_in.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"problematic_types\", \"emitted_at\": 1602637589100, \"data\": { \"schemaless_object\": { \"uuid\": \"38F52396-736D-4B23-B5B4-F504D8894B97\", \"probability\": 1.5 }, \"schematized_object\": { \"id\": 1, \"name\": \"Joe\" }, \"combined_type\": \"string1\", \"union_type\": 10, \"schemaless_array\": [ 10, \"foo\", null, { \"bar\": \"qua\" } ], \"mixed_array_integer_and_schemaless_object\": [ 15, null, { \"hello\": \"world\" } ], \"array_of_union_integer_and_schemaless_array\": [ 25, null, [\"goodbye\", \"cruel world\"] ], \"union_of_objects_with_properties_identical\": { \"id\": 10, \"name\": \"Joe\" }, \"union_of_objects_with_properties_overlapping\": { \"id\": 20, \"name\": \"Jane\", \"flagged\": true }, \"union_of_objects_with_properties_contradicting\": { \"id\": 1, \"name\": \"Jenny\" }, \"union_of_objects_with_properties_nonoverlapping\": { \"id\": 30, \"name\": \"Phil\", \"flagged\": false, \"description\":\"Very Phil\" }, \"empty_object\": {},\"object_with_null_properties\": {}, \"combined_with_null\": \"foobar\", \"union_with_null\": \"barfoo\", \"combined_nulls\": null, \"compact_union\": { \"id\": 10, \"name\": \"Tyler\" } } } }\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Surveycto Connector\nDESCRIPTION: Command to execute the full CI test suite for the Surveycto connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveycto/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-surveycto test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Younium source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-younium/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-younium build\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Airbyte Syncs in Kestra\nDESCRIPTION: Example Kestra flow configuration that demonstrates parallel execution of multiple Airbyte connection syncs. The flow includes task defaults for connection settings and a scheduled trigger running every minute.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-kestra-plugin.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nid: airbyte_syncs\nnamespace: company.team\ndescription: Gotta catch 'em all!\n\ntasks:\n  - id: data_ingestion\n    type: io.kestra.core.tasks.flows.Parallel\n    tasks:\n      - id: charizard\n        type: io.kestra.plugin.airbyte.connections.Sync\n        connectionId: 9bb96539-73e7-4b9a-9937-6ce861b49cb9\n      - id: pikachu\n        type: io.kestra.plugin.airbyte.connections.Sync\n        connectionId: 39c38950-b0b9-4fce-a303-06ced3dbfa75\n      - id: psyduck\n        type: io.kestra.plugin.airbyte.connections.Sync\n        connectionId: 4de8ab1e-50ef-4df0-aa01-7f21491081f1\n\ntaskDefaults:\n  - type: io.kestra.plugin.airbyte.connections.Sync\n    values:\n      url: http://host.docker.internal:8000/\n      username: \"{{ secret('AIRBYTE_USERNAME') }}\"\n      password: \"{{ secret('AIRBYTE_PASSWORD') }}\"\n\ntriggers:\n  - id: every_minute\n    type: io.kestra.core.models.triggers.types.Schedule\n    cron: \"*/1 * * * *\"\n```\n\n----------------------------------------\n\nTITLE: Dynamically Modifying Static Airbyte Stream Schema in Python\nDESCRIPTION: This Python method demonstrates how to override the default `Stream.get_json_schema` behavior within a stream class. It first retrieves the base schema (typically loaded from a corresponding `.json` file in the `schemas/` directory) using `super().get_json_schema()`, then modifies this schema dictionary by adding a new property ('dynamically_determined_property'), and finally returns the updated schema dictionary. This allows combining static schema definitions with dynamic adjustments based on runtime logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/source_commcare/schemas/TODO.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for Python Destination Connector\nDESCRIPTION: These commands create a virtual environment, activate it, and install the required dependencies for the Kvdb destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Typeform Source Connector\nDESCRIPTION: Execute the full test suite for the Typeform source connector using airbyte-ci. This command runs all tests associated with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-typeform/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-typeform test\n```\n\n----------------------------------------\n\nTITLE: Building Coda Source Connector Docker Image using airbyte-ci - Bash\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the Coda manifest-only connector using Airbyte's CLI tool, airbyte-ci. It assumes airbyte-ci is installed and available on the PATH. The command builds an image tagged as airbyte/source-coda:dev, which can be used for further development, testing, or deployment. This step is required before running the connector as a container or executing integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coda/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coda build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Aircall Connector\nDESCRIPTION: Command to execute the full test suite for the Aircall connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aircall/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aircall test\n```\n\n----------------------------------------\n\nTITLE: Rate Limit Error Response\nDESCRIPTION: Example error message returned when hitting LinkedIn API rate limits after multiple retry attempts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"Caught retriable error '<some_error> or null' after <some_number> tries. Waiting <some_number> seconds then retrying...\"\n```\n\n----------------------------------------\n\nTITLE: Float Stream Records in Airbyte JSON Format\nDESCRIPTION: Examples of JSON records for a float data stream named 'float_test' with different values including positive decimal, zero, and negative decimal numbers. Each record includes a timestamp in the emitted_at field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"56.78\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"0\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"float_test\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"-12345.678\" }}}\n```\n\n----------------------------------------\n\nTITLE: Testing the Fastly Source Connector in Bash\nDESCRIPTION: Command to run the acceptance tests for the source-fastly connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fastly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fastly test\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Parquet Source Connector in Airbyte\nDESCRIPTION: YAML configuration for setting up an AWS S3 source connector for Parquet files in Airbyte. This example includes dataset name, path pattern, authentication with AWS, and Parquet-specific configuration options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/config/vocabularies/Airbyte/reject.txt#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Accept the defaults where they make sense for your data\ndataset: s3_data\npath_pattern: \"*.parquet\"\ndata_interval: P1D\n\n# S3 bucket\nbucket: airbyte-bucket\naws_access_key_id: \"{{ secrets.AWS_ACCESS_KEY_ID }}\"\naws_secret_access_key: \"{{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n\n# Parquet configuration\nformat: parquet\n\n# Schema inference\nschemas:\n  - name: users \n    format: parquet\n    globs: [\"**\"]\n    filetype: parquet\n    legacy_mode: False\n    validation_policy: Emit Record\n    parsing_mode: default\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record With Added Partition Value\nDESCRIPTION: The same order note record after applying a transformation that adds the order_id field using the stream_partition.order value. This makes it possible to associate notes with their respective orders.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"id\": 999, \"author\": \"Jon Doe\", \"note\": \"Great product!\", \"order_id\": 123 }\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Gradle (Shell)\nDESCRIPTION: Executes the JUnit unit tests for the e2e test source connector using the Gradle wrapper script. Note that the path 'sources-e2e-test' might refer to a shared test module rather than the specific 'source-e2e-test-cloud' module. This command should be run from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:sources-e2e-test:unitTest\n```\n\n----------------------------------------\n\nTITLE: Example Output of Pod Status Listing (Shell)\nDESCRIPTION: Provides a sample output of the `kubectl get pods -n airbyte-abctl` command, showing various Airbyte components (bootloader, server, webapp, worker, etc.) and their running status within the Kubernetes cluster.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nNAME                                                     READY   STATUS      RESTARTS        AGE\nairbyte-abctl-airbyte-bootloader                         0/1     Completed   0               4h20m\nairbyte-abctl-connector-builder-server-55bc78bd-6sxdp    1/1     Running     0               4h20m\nairbyte-abctl-cron-b48bccb78-jnz7b                       1/1     Running     0               4h20m\nairbyte-abctl-pod-sweeper-pod-sweeper-599fd8f56d-kj5t9   1/1     Running     0               4h20m\nairbyte-abctl-server-74465db7fd-gk25q                    1/1     Running     0               4h20m\nairbyte-abctl-temporal-bbb84b56c-jh8x7                   1/1     Running     0               4h33m\nairbyte-abctl-webapp-745c949464-brpjf                    1/1     Running     0               4h20m\nairbyte-abctl-worker-79c895c7dc-ssqvc                    1/1     Running     0               4h20m\nairbyte-db-0                                             1/1     Running     0               4h34m\nairbyte-minio-0                                          1/1     Running     0               4h34m\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Python Connector Base Image\nDESCRIPTION: Generated Dockerfile showing the configuration of airbyte/python-connector-base. It starts with Python 3.11 on slim-bookworm, adds an airbyte user, sets up directories, configures pip/poetry, and installs system dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/base_images/README.md#2025-04-23_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM docker.io/python:3.11.11-slim-bookworm@sha256:6ed5bff4d7d377e2a27d9285553b8c21cfccc4f00881de1b24c9bc8d90016e82\nRUN ln -snf /usr/share/zoneinfo/Etc/UTC /etc/localtime\nRUN adduser --uid 1000 --system --group --no-create-home airbyte\nRUN mkdir --mode 755 /custom_cache\nRUN mkdir --mode 755 /airbyte\nRUN chown airbyte:airbyte /airbyte\nENV PIP_CACHE_DIR=/custom_cache/pip\nRUN pip install --upgrade pip==24.0 setuptools==70.0.0\nENV POETRY_VIRTUALENVS_CREATE=false\nENV POETRY_VIRTUALENVS_IN_PROJECT=false\nENV POETRY_NO_INTERACTION=1\nRUN pip install poetry==1.8.4\nRUN sh -c apt-get update && apt-get upgrade -y && apt-get dist-upgrade -y && apt-get clean\nRUN sh -c apt-get install -y socat=1.7.4.4-2\nRUN sh -c apt-get update && apt-get install -y tesseract-ocr=5.3.0-2 poppler-utils=22.12.0-2+b1\nRUN mkdir -p 755 /usr/share/nltk_data\n```\n\n----------------------------------------\n\nTITLE: Building the Pypi Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Pypi source connector docker image using airbyte-ci. This creates an image with the tag 'airbyte/source-pypi:dev' on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pypi/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pypi build\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Avni Source Connector with Docker - Shell\nDESCRIPTION: Executes the acceptance and integration tests for the Avni source connector using a shell script. Assumes the presence of an 'acceptance-test-docker.sh' script configured for the test environment. Facilitates automated testing of the connector in a Dockerized context, ensuring compatibility and correctness.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./acceptance-test-docker.sh\n```\n\n----------------------------------------\n\nTITLE: Personal Access Token Input\nDESCRIPTION: Format for entering multiple GitHub personal access tokens to load balance API quota consumption.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/correct.md#2025-04-23_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntoken1,token2,token3  # multiple tokens separated by commas\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Handling Objects with Defined Schemas in Airbyte (New Behavior)\nDESCRIPTION: Illustrates the new behavior for objects with defined schemas in Airbyte. Undocumented fields present in the input data (like 'name') are silently dropped during processing, and only fields defined in the schema ('id') are retained in the output. The output schema remains the same as the input schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/s3-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n// Input Schema\n{ \"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\" } } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Input Data\n{ \"id\": 1, \"name\": \"Alice\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Output Schema\n{ \"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\" } } }\n```\n\nLANGUAGE: json\nCODE:\n```\n// Output Data\n{ \"id\": 1 }\n```\n\n----------------------------------------\n\nTITLE: Testing the Sharetribe Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Sharetribe source connector. This validates that the connector functions correctly according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sharetribe/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sharetribe test\n```\n\n----------------------------------------\n\nTITLE: Building the Avni Source Connector Docker Image - Shell\nDESCRIPTION: Builds a Docker image for the Avni source connector, tagging it as 'airbyte/source-avni:dev'. Assumes Docker is installed and the command is run in the connector's directory containing the Dockerfile. Produces a local Docker image suitable for running the connector or distributing for integration testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker build . -t airbyte/source-avni:dev\n```\n\n----------------------------------------\n\nTITLE: Installing Jira Connector Dependencies with Poetry\nDESCRIPTION: Command to install the Jira connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running the Full Connector Test Suite with airbyte-ci (bash)\nDESCRIPTION: This command invokes airbyte-ci to execute the complete connector test suite for source-plausible locally. Prerequisites: airbyte-ci must be installed and configured. Input: none; Output: logs/results from the test suite verifying connector integration and behavior. Limitations: Requires a functioning development environment and any required test dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plausible/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-plausible test\n```\n\n----------------------------------------\n\nTITLE: JSON Object with Alternative Values for Schema Testing\nDESCRIPTION: A second JSON object with different values for the same fields as the first object. This includes nested objects as string values, null values for previously populated fields, and string values for fields that previously contained numbers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_coerced_schemaless_messages_out.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"schemaless_object\":\"{\\\"address\\\":{\\\"street\\\":\\\"113 Hickey Rd\\\",\\\"zip\\\":\\\"37932\\\"},\\\"flags\\\":[true,false,false]}\",\"schematized_object\":{\"id\":2,\"name\":\"Jane\"},\"combined_type\":20,\"union_type\":\"string2\",\"schemaless_array\":\"[]\",\"mixed_array_integer_and_schemaless_object\":[],\"array_of_union_integer_and_schemaless_array\":[],\"union_of_objects_with_properties_identical\":{\"id\":null,\"name\":null},\"union_of_objects_with_properties_overlapping\":{\"id\":null,\"name\":null,\"flagged\":null},\"union_of_objects_with_properties_nonoverlapping\":{\"id\":null,\"name\":null,\"flagged\":null,\"description\":null}, \"union_of_objects_with_properties_contradicting\": { \"id\": \"seal-one-hippity\", \"name\": \"James\" }, \"empty_object\": \"{\\\"extra\\\":\\\"stuff\\\"}\", \"object_with_null_properties\": \"{\\\"more\\\":{\\\"extra\\\":\\\"stuff\\\"}}\", \"combined_with_null\": \"foobar2\", \"union_with_null\": \"barfoo2\", \"combined_nulls\": null, \"compact_union\": 4444 }\n```\n\n----------------------------------------\n\nTITLE: Building the Docker Image\nDESCRIPTION: This command builds a Docker image for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rki-covid build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the N8n source connector docker image using airbyte-ci tool. The resulting image will be tagged as airbyte/source-n8n:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-n8n/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-n8n build\n```\n\n----------------------------------------\n\nTITLE: Building S3 Connector Docker Image\nDESCRIPTION: Command to build a Docker image for the S3 connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-s3 build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Activecampaign connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-activecampaign/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-activecampaign test\n```\n\n----------------------------------------\n\nTITLE: JSON Stream State Message\nDESCRIPTION: Global state message containing shared state information with a start date parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_array_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Using length Filter in Jinja2\nDESCRIPTION: Demonstrates the `length` filter (or `count` alias) in Jinja2, which returns the number of items in a sequence or mapping. The example gets the length of the list `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_32\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|length }}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Statuspage Source Connector\nDESCRIPTION: Command to build the Docker image for the Statuspage source connector using airbyte-ci. This creates an image tagged as airbyte/source-statuspage:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-statuspage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-statuspage build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-webflow/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-webflow:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-webflow:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-webflow:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-webflow:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for SingleStore Connector\nDESCRIPTION: Gradle command to run unit tests for the SingleStore connector from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-singlestore/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-singlestore:unitTest\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message Example in JSON\nDESCRIPTION: An example of an Airbyte RECORD message containing a data record from the \"object_test_1\" stream. The record includes various data types such as strings, dates, timestamps, numbers, integers, booleans, and binary data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_object_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"object_test_1\", \"emitted_at\": 1602637589100, \"data\": {\"property_string\": \"foo bar\", \"property_date\": \"2021-01-23\", \"property_timestamp_with_timezone\": \"2022-11-22T01:23:45+00:00\", \"property_timestamp_without_timezone\": \"2022-11-22T01:23:45\", \"property_number\": \"56.78\", \"property_integer\": \"42\", \"property_boolean\": true, \"property_binary_data\" :  \"dGVzdA==\" }}}\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite with Airbyte CI\nDESCRIPTION: Command to run the complete test suite for the Fauna connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fauna test\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite Locally with airbyte-ci (Python Bash Command)\nDESCRIPTION: Executes the full connector test suite locally using Airbyte's CI pipeline tool 'airbyte-ci'. This can be used to validate integration and acceptance tests before merging changes. Assumes 'airbyte-ci' is installed and the repository is properly configured. Outputs detailed CI test reports to the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-directory test\n```\n\n----------------------------------------\n\nTITLE: Installing MarkDownLint CLI on Mac\nDESCRIPTION: Command to install markdownlint-cli2, a recommended command-line tool for linting Markdown files, using Homebrew on Mac.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbrew install markdownlint-cli2\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for JobNimbus Connector\nDESCRIPTION: This command executes the acceptance tests for the JobNimbus connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jobnimbus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jobnimbus test\n```\n\n----------------------------------------\n\nTITLE: Building Track PMS Connector with airbyte-ci\nDESCRIPTION: This command builds a dev image (source-track-pms:dev) that can be used for local testing of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-track-pms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-track-pms build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Trustpilot Source Connector\nDESCRIPTION: Command to run the full test suite for the Trustpilot source connector using airbyte-ci. This is used to verify changes and ensure compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-trustpilot/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-trustpilot test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Docker image for the Appfollow source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appfollow/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appfollow build\n```\n\n----------------------------------------\n\nTITLE: Airbyte STATE Message\nDESCRIPTION: Global state message indicating shared state configuration with a start date parameter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_object_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": {\"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters Table in Markdown\nDESCRIPTION: Defines the required configuration parameters for the NoCRM connector, including API key and subdomain settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nocrm.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. API key to use. Generate it from the admin section of your noCRM.io account. |  |\n| `subdomain` | `string` | Subdomain. The subdomain specific to your noCRM.io account, e.g., &#39;yourcompany&#39; in &#39;yourcompany.nocrm.io&#39;. |  |\n```\n\n----------------------------------------\n\nTITLE: Importing React Components in Airbyte Documentation\nDESCRIPTION: Imports the Tabs and TabItem components from Docusaurus theme to create tabbed interfaces in the documentation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/readme.md#2025-04-23_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Greenhouse Connector\nDESCRIPTION: This command runs the unit tests for the Greenhouse connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-greenhouse/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Freshservice Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Freshservice source connector using airbyte-ci. This creates an image with the tag 'airbyte/source-freshservice:dev' on your host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshservice/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshservice build\n```\n\n----------------------------------------\n\nTITLE: Local Connector Operations\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover and read functions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-mixpanel spec\npoetry run source-mixpanel check --config secrets/config.json\npoetry run source-mixpanel discover --config secrets/config.json\npoetry run source-mixpanel read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Installing Test Dependencies for Kvdb Connector\nDESCRIPTION: This command installs the test dependencies for the Kvdb connector into the virtual environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install .[tests]\n```\n\n----------------------------------------\n\nTITLE: Building Kafka Source Connector via Gradle\nDESCRIPTION: This command builds the Kafka source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kafka/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-kafka:build\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Check Command via Docker (Bash)\nDESCRIPTION: Executes the `check` command within a Docker container, validating the connection configuration provided in `/secrets/config.json`. Requires Docker, the connector image (`airbyte/source-google-pagespeed-insights:dev`), and a valid `config.json` file mounted from the local `secrets` directory. Outputs success or failure status.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-pagespeed-insights/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-google-pagespeed-insights:dev check --config /secrets/config.json\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Firebase-Realtime-Database Connector\nDESCRIPTION: Command to add new dependencies to the connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Local CI Tests for the Paystack Connector using airbyte-ci (Bash)\nDESCRIPTION: This command executes the full continuous integration test suite for the Paystack source connector locally using the `airbyte-ci` tool. This is essential for verifying connector functionality and ensuring changes pass all checks before contributing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paystack/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paystack test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-cumulio build\n```\n\n----------------------------------------\n\nTITLE: Conflict Stream Array Record Structure\nDESCRIPTION: Demonstrates the structure of records in the 'conflict_stream_array' stream. It contains a nested object and array where the object key and stream name are identical.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"RECORD\",\n  \"record\":{\n    \"stream\":\"conflict_stream_array\",\n    \"data\":{\n      \"id\":1,\n      \"conflict_stream_array\": {\n        \"conflict_stream_array\": [{\"id\": 1}, {\"id\": 2}, {\"id\": 3}]\n      }\n    },\n    \"emitted_at\":1623861660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Analytics Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Zoho Analytics metadata connector using airbyte-ci. Creates a dev image tagged as 'source-zoho-analytics-metadata-api:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-analytics-metadata-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-analytics-metadata-api build\n```\n\n----------------------------------------\n\nTITLE: Configuring Guru Source Connector in Markdown\nDESCRIPTION: Markdown table showing the configuration parameters for the Guru source connector, including input types, descriptions, and default values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/guru.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `team_id` | `string` | team_id. Team ID received through response of /teams streams, make sure about access to the team |  |\n| `search_cards_query` | `string` | search_cards_query. Query for searching cards |  |\n```\n\n----------------------------------------\n\nTITLE: Testing Mode Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Mode source connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mode/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mode test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Pagerduty Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet executes the full CI test suite for the Pagerduty connector using airbyte-ci. The --name parameter specifies which connector to test. Prior installation of airbyte-ci is required, and test-related dependencies should be configured as per Airbyte's contributing guidelines. Running this command validates connector integrity and is necessary before contributing new changes or publishing. Output appears as test results in the terminal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pagerduty/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pagerduty test\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte RECORD Message for User 'a' (JSON)\nDESCRIPTION: This JSON line represents a single record message from the 'users' stream in the Airbyte protocol. It contains data for key 'a' with a corresponding escaped JSON string '{\"a\": 1}' as the value, and includes an emission timestamp. This format is typical for data exchange in Airbyte connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/integration_tests/expected_records.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"users\", \"data\": {\"key\": \"a\", \"value\": \"{\\\"a\\\": 1}\"}, \"emitted_at\": 1640962800000}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instagram/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-instagram test\n```\n\n----------------------------------------\n\nTITLE: Building Lob Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the source-lob connector using airbyte-ci tooling. Creates a dev image tagged as source-lob:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lob/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lob build\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands for running the connector within a Docker container for various operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-box-data-extract:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-box-data-extract:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-box-data-extract:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-box-data-extract:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Documenting Thinkific Courses Connector Changelog in Markdown\nDESCRIPTION: This snippet presents the version history of the Thinkific Courses connector, including release dates, pull request links, and descriptions of changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific-courses.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.6 | 2025-04-12 | [57990](https://github.com/airbytehq/airbyte/pull/57990) | Update dependencies |\n| 0.0.5 | 2025-04-05 | [57419](https://github.com/airbytehq/airbyte/pull/57419) | Update dependencies |\n| 0.0.4 | 2025-03-29 | [56325](https://github.com/airbytehq/airbyte/pull/56325) | Update dependencies |\n| 0.0.3 | 2025-03-08 | [55630](https://github.com/airbytehq/airbyte/pull/55630) | Update dependencies |\n| 0.0.2 | 2025-03-01 | [55089](https://github.com/airbytehq/airbyte/pull/55089) | Update dependencies |\n| 0.0.1 | 2025-02-20 | | Initial release by [@gueroverde](https://github.com/gueroverde) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Displaying Rockset Connector Changelog in Markdown\nDESCRIPTION: A markdown table showing the version history and changes made to the Rockset connector, including version numbers, dates, pull request links, and subjects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/rockset.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                |\n| :------ | :--------- | :------------------------------------------------------- | :----------------------------------------------------- |\n| 0.1.4   | 2022-06-17 | [15395](https://github.com/airbytehq/airbyte/pull/15395) | Updated Destination Rockset to handle per-stream state |\n| 0.1.3   | 2022-06-17 | [13864](https://github.com/airbytehq/airbyte/pull/13864) | Updated stacktrace format for any trace message errors |\n| 0.1.2   | 2022-05-17 | [12820](https://github.com/airbytehq/airbyte/pull/12820) | Improved 'check' operation performance                 |\n| 0.1.1   | 2022-02-14 | [10256](https://github.com/airbytehq/airbyte/pull/10256) | Add `-XX:+ExitOnOutOfMemoryError` JVM option           |\n| 0.1.0   | 2021-11-15 | [8006](https://github.com/airbytehq/airbyte/pull/8006)   | Initial release                                        |\n```\n\n----------------------------------------\n\nTITLE: Installing Live Tests Dependencies with Poetry\nDESCRIPTION: Command to install the dependencies for the connector live testing project using Poetry from the airbyte-ci/connectors/live-tests directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/live-tests/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# From airbyte-ci/connectors/live-tests\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Poetry and Pytest - Bash\nDESCRIPTION: This bash command executes all Python unit tests located in the unit_tests directory using pytest via Poetry. It ensures tests are run within the managed Python environment, resolving dependencies as defined in pyproject.toml. No arguments are required beyond specifying the test folder. Passing these tests is necessary for validating changes prior to publication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci - Bash\nDESCRIPTION: This bash snippet demonstrates how to use the airbyte-ci command-line tool to build a Docker image for the source-close-com manifest-only connector. Prerequisites include having airbyte-ci installed and configured. The command takes the connector name as an argument and builds the image locally, producing an image tagged as airbyte/source-close-com:dev. No input files are required to run this command, and output is the Docker image placed in the local Docker host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-close-com/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-close-com build\n```\n\n----------------------------------------\n\nTITLE: Migrating Low-Code Connectors to Manifest-Only Format\nDESCRIPTION: Example of how to migrate all low-code connectors to the manifest-only format using the Airbyte CI connectors command with language filter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --language=low-code migrate-to-manifest-only\n```\n\n----------------------------------------\n\nTITLE: Building the Pretix Connector for Local Development\nDESCRIPTION: Command to build a development image of the Pretix connector using airbyte-ci. This creates a source-pretix:dev image that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pretix/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pretix build\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenShift Namespace for Airbyte Deployments (YAML)\nDESCRIPTION: This YAML snippet provides required Kubernetes namespace annotations to allow Airbyte components to run as UID 1000 and GID 1000 when deploying to OpenShift. These annotations must be added to your namespace to ensure proper permissions and rootless operation for Airbyte pods. There are no external dependencies except a Kubernetes cluster supporting OpenShift, and you must have cluster-admin privileges to apply these settings. Expected input is the YAML dictionary of annotations; output is compliant namespace configuration. Limitations include the necessity of correct RBAC and that these values may require adjustment in specific custom environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.3.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nopenshift.io/sa.scc.supplemental-groups: 1000/1\\nopenshift.io/sa.scc.uid-range: 1000/1\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailjet-sms/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailjet-sms test\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Node Groups for Airbyte\nDESCRIPTION: Extended YAML configuration that separates Airbyte components across different node groups. This separates static workloads from job-related workloads to improve stability and resource management.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/scaling-airbyte.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nairbyte-bootloader:\n  nodeSelector:\n    type: static\n\nserver:\n  nodeSelector:\n    type: static\n\nkeycloak:\n  nodeSelector:\n    type: static\n\nkeycloak-setup:\n  nodeSelector:\n    type: static\n\ntemporal:\n  nodeSelector:\n    type: static\n\nwebapp:\n  nodeSelector:\n    type: static\n\nworker:\n  nodeSelector:\n    type: jobs\n\nworkload-launcher:\n  nodeSelector:\n    type: static\n  ## Pods spun up by the workload launcher will run in the 'jobs' node group.\n  extraEnvs:\n    - name: JOB_KUBE_NODE_SELECTORS\n      value: type=jobs\n    - name: SPEC_JOB_KUBE_NODE_SELECTORS\n      value: type=jobs\n    - name: CHECK_JOB_KUBE_NODE_SELECTORS\n      value: type=jobs\n    - name: DISCOVER_JOB_KUBE_NODE_SELECTORS\n      value: type=jobs\n\norchestrator:\n  nodeSelector:\n    type: jobs\n  \nworkload-api-server:\n  nodeSelector:\n    type: jobs\n```\n\n----------------------------------------\n\nTITLE: Defining Data Records in JSON Lines for Airbyte Stream 'Sheet6-2000-rows'\nDESCRIPTION: This snippet shows the standard JSON Lines format used for records emitted from the 'Sheet6-2000-rows' Airbyte stream. Each line is a JSON object containing three keys: 'stream' (identifying the data source stream), 'data' (an object holding the actual record fields, like 'ID' and 'Name'), and 'emitted_at' (a Unix timestamp in milliseconds indicating when the record was processed). This format is common for streaming data pipelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1423\",\"Name\":\"OiHVAkKiH\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1424\",\"Name\":\"nRtYcQiLD\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1425\",\"Name\":\"grgYXECRs\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1426\",\"Name\":\"nVQeESoAw\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1427\",\"Name\":\"fiSztxVLo\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1428\",\"Name\":\"mGsOhMWyH\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1429\",\"Name\":\"NkeBZlQbc\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1430\",\"Name\":\"vAmFhPfyO\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1431\",\"Name\":\"kelhsDiat\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1432\",\"Name\":\"NfrmbEVoP\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1433\",\"Name\":\"lPGhMGqnj\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1434\",\"Name\":\"LmJkdYGbU\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1435\",\"Name\":\"cZZdRhleV\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1436\",\"Name\":\"rXIPnTvTz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1437\",\"Name\":\"eQPOTBgUE\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1438\",\"Name\":\"YFcvpbszb\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1439\",\"Name\":\"IKMZnAZHt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1440\",\"Name\":\"QciNlZWyV\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1441\",\"Name\":\"JqPlkebNc\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1442\",\"Name\":\"qDiVAomHo\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1443\",\"Name\":\"WNGrPGPkK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1444\",\"Name\":\"DCArfmUNl\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1445\",\"Name\":\"ttMbInVMx\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1446\",\"Name\":\"gZjZQWeMq\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1447\",\"Name\":\"etrWzbjCL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1448\",\"Name\":\"OgyElGZwL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1449\",\"Name\":\"grDFRsAoK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1450\",\"Name\":\"lUzLhURbT\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1451\",\"Name\":\"ptxWrqNBz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1452\",\"Name\":\"JhiVuyZiT\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1453\",\"Name\":\"gVqUFppKR\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1454\",\"Name\":\"qFrYxEBOF\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1455\",\"Name\":\"ZmtNzGGWl\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1456\",\"Name\":\"pPEUYBjFF\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1457\",\"Name\":\"vAkzBgMoj\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1458\",\"Name\":\"BCwvuZocc\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1459\",\"Name\":\"XqgtAuPzy\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1460\",\"Name\":\"BraGEfvpA\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1461\",\"Name\":\"TZJyINaqv\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1462\",\"Name\":\"xeGYgcCeX\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1463\",\"Name\":\"rbnoxVYeu\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1464\",\"Name\":\"cglWGYhCY\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1465\",\"Name\":\"RXuVoPusw\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1466\",\"Name\":\"QlkrlSGZq\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1467\",\"Name\":\"nOhJOzKUb\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1468\",\"Name\":\"jegzLKtmt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1469\",\"Name\":\"VVEqtobEg\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1470\",\"Name\":\"CBMTqGIEv\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1471\",\"Name\":\"PbnjJgUdo\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1472\",\"Name\":\"lhQUrnSBD\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1473\",\"Name\":\"EvOAwTjMb\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1474\",\"Name\":\"bWUzLxSnK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1475\",\"Name\":\"QtRynEduW\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1476\",\"Name\":\"tjZwZcuBK\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1477\",\"Name\":\"gChmJzgUR\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1478\",\"Name\":\"dZuhVKCdn\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1479\",\"Name\":\"ZIbXOPkRt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1480\",\"Name\":\"xLtAovVMS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1481\",\"Name\":\"RYZIcTeMI\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1482\",\"Name\":\"ZdIlGZbLU\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1483\",\"Name\":\"NoQIuwuBF\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1484\",\"Name\":\"GUzaXoFDX\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1485\",\"Name\":\"WkBLpjtnm\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1486\",\"Name\":\"GOfyeTzuF\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1487\",\"Name\":\"yKfOzJfSJ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1488\",\"Name\":\"nffcvzUUk\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1489\",\"Name\":\"MaoAMQJQO\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1490\",\"Name\":\"lpXyhOJwc\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1491\",\"Name\":\"dSneeJcLb\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1492\",\"Name\":\"QqorwNgUg\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1493\",\"Name\":\"KCQiqXpZq\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1494\",\"Name\":\"AmMkxJFVz\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1495\",\"Name\":\"LcCYXSNBT\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1496\",\"Name\":\"OLyhouwoB\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1497\",\"Name\":\"sfToRQtAA\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1498\",\"Name\":\"BSHQIXxZQ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1499\",\"Name\":\"trPCeyUWs\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1500\",\"Name\":\"WhemAUDkN\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1501\",\"Name\":\"uyQrauQKm\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1502\",\"Name\":\"pkWccmKQc\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1503\",\"Name\":\"AhhadNuGO\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1504\",\"Name\":\"cVdWKIiLS\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1505\",\"Name\":\"ZGbDkEpwJ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1506\",\"Name\":\"lAEatsInM\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1507\",\"Name\":\"nZLlSpcQt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1508\",\"Name\":\"HCECtypOn\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1509\",\"Name\":\"KmRDtAGJC\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1510\",\"Name\":\"uoIbARUqt\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1511\",\"Name\":\"ezgJnHQJi\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1512\",\"Name\":\"HbrYktpIq\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1513\",\"Name\":\"GpbvJheif\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1514\",\"Name\":\"HlanhGFTL\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1515\",\"Name\":\"ffMeVviuQ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1516\",\"Name\":\"oXOHfjBiv\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1517\",\"Name\":\"lUimsJUcO\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1518\",\"Name\":\"zGnJvwmuZ\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1519\",\"Name\":\"SqifSFueB\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1520\",\"Name\":\"mxKFyTUWe\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1521\",\"Name\":\"GFbKGZNlf\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1522\",\"Name\":\"YYzfTGcWr\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1523\",\"Name\":\"AJhHRrkPk\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1524\",\"Name\":\"xykNSJQOX\"},\"emitted_at\":1673989570000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1525\",\"Name\":\"qzKoQhBum\"},\"emitted_at\":1673989570000}\n```\n\n----------------------------------------\n\nTITLE: Displaying Invoiced Connector Changelog in Markdown\nDESCRIPTION: Markdown table showing the version history of the Invoiced connector, including version numbers, dates, pull request links, and update descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiced.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.22 | 2025-04-19 | [58195](https://github.com/airbytehq/airbyte/pull/58195) | Update dependencies |\n| 0.0.21 | 2025-04-12 | [57746](https://github.com/airbytehq/airbyte/pull/57746) | Update dependencies |\n| 0.0.20 | 2025-04-05 | [57045](https://github.com/airbytehq/airbyte/pull/57045) | Update dependencies |\n| 0.0.19 | 2025-03-29 | [56643](https://github.com/airbytehq/airbyte/pull/56643) | Update dependencies |\n| 0.0.18 | 2025-03-22 | [56021](https://github.com/airbytehq/airbyte/pull/56021) | Update dependencies |\n| 0.0.17 | 2025-03-08 | [55485](https://github.com/airbytehq/airbyte/pull/55485) | Update dependencies |\n| 0.0.16 | 2025-03-01 | [54818](https://github.com/airbytehq/airbyte/pull/54818) | Update dependencies |\n| 0.0.15 | 2025-02-22 | [54295](https://github.com/airbytehq/airbyte/pull/54295) | Update dependencies |\n| 0.0.14 | 2025-02-15 | [53824](https://github.com/airbytehq/airbyte/pull/53824) | Update dependencies |\n| 0.0.13 | 2025-02-08 | [53249](https://github.com/airbytehq/airbyte/pull/53249) | Update dependencies |\n| 0.0.12 | 2025-02-01 | [52755](https://github.com/airbytehq/airbyte/pull/52755) | Update dependencies |\n| 0.0.11 | 2025-01-25 | [52237](https://github.com/airbytehq/airbyte/pull/52237) | Update dependencies |\n| 0.0.10 | 2025-01-18 | [51828](https://github.com/airbytehq/airbyte/pull/51828) | Update dependencies |\n| 0.0.9 | 2025-01-11 | [51141](https://github.com/airbytehq/airbyte/pull/51141) | Update dependencies |\n| 0.0.8 | 2024-12-28 | [50600](https://github.com/airbytehq/airbyte/pull/50600) | Update dependencies |\n| 0.0.7 | 2024-12-21 | [50103](https://github.com/airbytehq/airbyte/pull/50103) | Update dependencies |\n| 0.0.6 | 2024-12-14 | [49650](https://github.com/airbytehq/airbyte/pull/49650) | Update dependencies |\n| 0.0.5 | 2024-12-12 | [49266](https://github.com/airbytehq/airbyte/pull/49266) | Update dependencies |\n| 0.0.4 | 2024-12-11 | [48987](https://github.com/airbytehq/airbyte/pull/48987) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.3 | 2024-10-29 | [47734](https://github.com/airbytehq/airbyte/pull/47734) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47534](https://github.com/airbytehq/airbyte/pull/47534) | Update dependencies |\n| 0.0.1 | 2024-10-21 | | Initial release by [@parthiv11](https://github.com/parthiv11) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Concrete Example of S3 Staging File Path with Component Annotations (Text)\nDESCRIPTION: Provides an annotated example of a concrete S3 path used to reference a temporary Iceberg table holding staged data for Airbyte sync operations. Comments visually clarify each component ('bucket name', 'bucket path', 'schema', temporary table name), helping users trace where their data and tables are located. Inputs are to be replaced with live values per deployment. Output is a visual, multi-line S3 URI with annotation arrows for interactive documentation. No dependencies, purely illustrative.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/starburst-galaxy.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ns3://galaxy_bucket/data_output_path/test_schema/_airbyte_tmp_qey_user\n     ↑              ↑                     ↑                ↑\n     |              |                     |                temporary Iceberg table holding data\n     |              |                     source namespace or provided schema name\n     |              |\n     |              bucket path\n     bucket name\n```\n\n----------------------------------------\n\nTITLE: Testing the Hugging Face Datasets Source Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the source-hugging-face-datasets connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hugging-face-datasets/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hugging-face-datasets test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Mailersend connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailersend/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailersend test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Intercom Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Intercom source connector locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-intercom/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-intercom test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Xero connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xero/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xero test\n```\n\n----------------------------------------\n\nTITLE: Running S3 Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the S3 connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-s3 test\n```\n\n----------------------------------------\n\nTITLE: Building Iceberg V2 Connector Docker Image\nDESCRIPTION: Gradle command to build the Docker image for the Iceberg V2 destination connector. The image will be tagged with the 'dev' label.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-iceberg-v2:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Local Connection Objects\nDESCRIPTION: Command to run connector tests with local connection objects (config.json, catalog.json, state.json) specifying connector image, paths to configuration files, target version, and PR URL.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/live-tests/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest src/live_tests \\\n --connector-image=airbyte/source-faker \\\n --config-path=<path-to-config-path> \\\n --catalog-path=<path-to-catalog-path> \\\n --target-version=dev \\\n --pr-url=<PR-URL> # The URL of the PR you are testing\n```\n\n----------------------------------------\n\nTITLE: YAML $ref Keyword Usage\nDESCRIPTION: Demonstrates using the $ref keyword to reference and extend an existing object with additional properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/references.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs:\n  $ref: \"#/key_value_pairs\"\n  k3: v3\n```\n\n----------------------------------------\n\nTITLE: Parsing Twitter API Response with Cursor Pagination\nDESCRIPTION: Example of a Twitter API response that uses cursor pagination with a next_token field. The API returns a set of tweets along with metadata containing the token for the next page of results.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"created_at\": \"2020-12-11T20:44:52.000Z\",\n      \"id\": \"1337498609819021312\",\n      \"text\": \"Thanks to everyone who tuned in today...\"\n    },\n    {\n      \"created_at\": \"2020-05-06T17:24:31.000Z\",\n      \"id\": \"1258085245091368960\",\n      \"text\": \"It's now easier to understand Tweet impact...\"\n    },\n    ...\n\n  ],\n  \"meta\": {\n    ...\n    \"result_count\": 100,\n    \"next_token\": \"7140w\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building HoorayHR Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the HoorayHR source connector using airbyte-ci. The resulting image is tagged as 'source-hoorayhr:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hoorayhr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hoorayhr build\n```\n\n----------------------------------------\n\nTITLE: Example Nested JSON for Inner Field Selection - JSON\nDESCRIPTION: A JSON object with a nested 'data' key containing an array of record objects under 'records'. Used to demonstrate how to extract inner records with a multi-segment field_path in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": {\n    \"records\": [\n      {\n        \"id\": 1\n      },\n      {\n        \"id\": 2\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Pinecone Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Pinecone connector Docker image using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-pinecone build\n```\n\n----------------------------------------\n\nTITLE: Building Gridly Connector Docker Image\nDESCRIPTION: Command to build the Docker image for the Gridly connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gridly build\n```\n\n----------------------------------------\n\nTITLE: Building the Docker image with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet builds the Airbyte manifest-only source-pokeapi connector Docker image using the airbyte-ci tool. It requires airbyte-ci CLI to be installed and the current directory context set appropriately. The script executes the build subcommand for connectors, targeting the source-pokeapi connector, resulting in a local Docker image tagged as airbyte/source-pokeapi:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pokeapi/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pokeapi build\n```\n\n----------------------------------------\n\nTITLE: Building the Perigon Connector Image using Airbyte-CI (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image named `source-perigon:dev` for the Perigon source connector. This image can then be used for local testing and development. Requires `airbyte-ci` to be installed and configured, and the connector source code must be present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-perigon/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-perigon build\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte-CI - Bash\nDESCRIPTION: This snippet demonstrates how to build the 'source-clockodo' Airbyte connector using the airbyte-ci tool by running a command in the terminal. It creates a development Docker image ('source-clockodo:dev') so that users can test the connector locally. 'airbyte-ci' must be installed prior to running this command, and the expected output is a local image ready for development and testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clockodo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-clockodo build\n```\n\n----------------------------------------\n\nTITLE: Building Web Scrapper Connector using airbyte-ci\nDESCRIPTION: Command to build a development image of the Web Scrapper connector using airbyte-ci tool. Creates a dev image tagged as source-web-scrapper:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-web-scrapper/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-web-scrapper build\n```\n\n----------------------------------------\n\nTITLE: Executing Partitioned GET Requests for Survey Responses - curl - Bash\nDESCRIPTION: Demonstrates how to use curl to send multiple partitioned GET requests to the SurveySparrow API. Each request includes a different survey_id query parameter corresponding to a static partition. These examples assume the user or config provides specific survey IDs and that the API requires authentication via headers or other means.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://api.surveysparrow.com/v3/responses?survey_id=123\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://api.surveysparrow.com/v3/responses?survey_id=456\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://api.surveysparrow.com/v3/responses?survey_id=789\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Allowed Hosts in YAML\nDESCRIPTION: YAML configuration for specifying dynamic allowed hosts using connector configuration values.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  allowedHosts:\n    hosts:\n      - \"${subdomain}.vendor.com\"\n      - \"${networking_options.tunnel_host}\"\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output after Nested AddFields - JSON\nDESCRIPTION: Resulting JSON record after adding a nested field (\"data.field1\") using the AddFields transformation. Shows both original and newly inserted fields. Used for validation and illustration in Airbyte docs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 0,\n  \"data\":\n  {\n    \"field0\": \"some_data\",\n    \"field1\": \"static_value\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating abctl to Latest Version\nDESCRIPTION: Downloads and executes the official Airbyte installation script using `curl`. This command ensures the `abctl` command-line tool is updated to the latest version before proceeding with the migration, guaranteeing compatibility and access to necessary features like migration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/migrating-from-docker-compose.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsfS https://get.airbyte.com | bash -\n```\n\n----------------------------------------\n\nTITLE: Example Airbyte RECORD Message for User 'c' (JSON)\nDESCRIPTION: This JSON line represents a single record message from the 'users' stream in the Airbyte protocol. It contains data for key 'c' with a corresponding escaped JSON string '{\"c\": 4}' as the value, and includes an emission timestamp. This format is typical for data exchange in Airbyte connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/integration_tests/expected_records.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\": \"users\", \"data\": {\"key\": \"c\", \"value\": \"{\\\"c\\\": 4}\"}, \"emitted_at\": 1640962800000}\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with Airbyte CI - Bash\nDESCRIPTION: This Bash command initiates the full suite of automated tests for the Okta source connector using Airbyte's CI tool. It requires 'airbyte-ci' to be installed. Running this command validates connector functionality and ensures compatibility with Airbyte's integration standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-okta/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-okta test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braze/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-braze test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Katana Source Connector\nDESCRIPTION: This command runs the acceptance tests for the Katana source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-katana/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-katana test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Survey Sparrow Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Survey Sparrow source connector locally. It helps ensure that any changes made to the connector pass all tests before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-survey-sparrow/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-survey-sparrow test\n```\n\n----------------------------------------\n\nTITLE: Testing Veeqo Source Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Veeqo source connector using airbyte-ci tooling. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-veeqo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-veeqo test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector in a Docker container for testing specification, configuration, and write operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-astra:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-astra:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-astra:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Snowflake Cortex Connector Docker Commands\nDESCRIPTION: Examples of running connector commands using the Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake-cortex/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-snowflake-cortex:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-snowflake-cortex:dev check --config /secrets/config.json\n# messages.jsonl is a file containing line-separated JSON representing AirbyteMessages\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-snowflake-cortex:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Tiktok-Marketing Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run various operations of the Tiktok-Marketing connector locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-tiktok-marketing spec\npoetry run source-tiktok-marketing check --config secrets/config.json\npoetry run source-tiktok-marketing discover --config secrets/config.json\npoetry run source-tiktok-marketing read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: YAML Object Reference\nDESCRIPTION: Shows how to reference complete objects using the #/ syntax, copying all key-value pairs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/references.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs: \"#/key_value_pairs\"\n```\n\n----------------------------------------\n\nTITLE: Making a Request with Bearer Token Authentication using cURL\nDESCRIPTION: This cURL command illustrates making a GET request to an API (using Sendgrid API as an example) that requires Bearer Token authentication. The 'Authorization' header is set to 'Bearer' followed by the actual token value. Replace `<bearer token>` with the valid token and `<stream path>` with the specific API endpoint path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X GET \\\n  -H \"Authorization: Bearer <bearer token>\" \\\n  https://api.sendgrid.com/<stream path>\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Pinterest Connector with Airbyte-CI (Bash)\nDESCRIPTION: This one-line bash command uses airbyte-ci to execute the entire test suite for the source-pinterest connector locally, automating both unit and integration/acceptance tests. It presumes airbyte-ci is installed and correctly configured and that all test dependencies and test data are available. Output includes comprehensive CI pass/fail results; issues in setup or environment may interrupt the process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pinterest test\n```\n\n----------------------------------------\n\nTITLE: Defining Partition Router Schema in YAML\nDESCRIPTION: Specifies the YAML schema for the top-level `partition_router` configuration in an Airbyte connector manifest. It allows defining either a single router (Custom, List, or Substream) or an array of routers, enabling Cartesian product partitioning when multiple are used. The `default` value is an empty array.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npartition_router:\n  default: []\n  anyOf:\n    - \"$ref\": \"#/definitions/CustomPartitionRouter\"\n    - \"$ref\": \"#/definitions/ListPartitionRouter\"\n    - \"$ref\": \"#/definitions/SubstreamPartitionRouter\"\n    - type: array\n      items:\n        anyOf:\n          - \"$ref\": \"#/definitions/CustomPartitionRouter\"\n          - \"$ref\": \"#/definitions/ListPartitionRouter\"\n          - \"$ref\": \"#/definitions/SubstreamPartitionRouter\"\n```\n\n----------------------------------------\n\nTITLE: Building Partnerize Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet shows how to build the development image of the 'source-partnerize' connector using the airbyte-ci CLI tool. It requires airbyte-ci to be installed in your environment and will create a Docker image (source-partnerize:dev) that can later be used for local testing or further development. The main parameter is the connector name specified with --name; the expected output is a built image in your local Docker environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-partnerize/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-partnerize build\n```\n\n----------------------------------------\n\nTITLE: Defining RequestPath Injection Schema - YAML\nDESCRIPTION: Defines the schema for the RequestPath object, indicating how to specify components that inject values into the API endpoint's HTTP path. Requires schema-conforming configuration and is used by Airbyte components needing dynamic URL path modification. Inputs are object fields specifying the injection type; outputs are used to correctly construct endpoint URLs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nRequestPath:\n  description: A component that specifies where in the request path a component's value should be inserted into.\n  type: object\n  required:\n    - type\n  properties:\n    type:\n      type: string\n      enum: [RequestPath]\n```\n\n----------------------------------------\n\nTITLE: Initiating Airbyte Migration with abctl\nDESCRIPTION: Runs the `abctl` command to install Airbyte locally while simultaneously migrating data from a previous Docker Compose installation. The `--migrate` flag instructs `abctl` to locate existing Docker volumes associated with the old Airbyte instance and transfer configurations, job history, and other relevant data to the new K3s-based deployment managed by `abctl`. This command should be run after stopping the Docker Compose instance and updating `abctl`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/migrating-from-docker-compose.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nabctl local install --migrate\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the 'airbyte-ci' tool to build the Docker image for the Gocardless source connector. It requires 'airbyte-ci' to be installed. The resulting image will be tagged as 'airbyte/source-gocardless:dev' on the local Docker host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gocardless build\n```\n\n----------------------------------------\n\nTITLE: Implementing AzureBlobStorageWriter in Java\nDESCRIPTION: Create a new class that implements AzureBlobStorageWriter, possibly extending BaseAzureBlobStorageWriter.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-azure-blob-storage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class NewFormatAzureBlobStorageWriter extends BaseAzureBlobStorageWriter {\n    // Implement methods for new format\n}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte CI Test Suite for the Connector - Bash\nDESCRIPTION: Runs the full test suite for the connector locally using the 'airbyte-ci' tool. This command ensures that all code changes and the connector implementation pass automated tests before publication or deployment. Requires 'airbyte-ci' to be installed and configured as per Airbyte documentation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-posthog/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-posthog test\n```\n\n----------------------------------------\n\nTITLE: Building Veeqo Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Veeqo source connector using airbyte-ci tooling. Creates a dev image tagged as 'source-veeqo:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-veeqo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-veeqo build\n```\n\n----------------------------------------\n\nTITLE: Building Solarwinds Service Desk Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Solarwinds Service Desk connector using airbyte-ci. The resulting image can be used for local testing of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-solarwinds-service-desk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-solarwinds-service-desk build\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the `source-open-exchange-rates` connector. It requires `airbyte-ci` to be installed. The resulting image will be available locally tagged as `airbyte/source-open-exchange-rates:dev`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-open-exchange-rates/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-open-exchange-rates build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Mailerlite connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailerlite/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailerlite test\n```\n\n----------------------------------------\n\nTITLE: Generating Sparse Nested Stream Record for Empty Stream Testing in JSON\nDESCRIPTION: This JSON object represents a record for a sparse nested stream used to test behavior when no new data is received after the initial sync. The structure is identical to the previous example, but it's associated with a different stream name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_sparse_nested_streams/data_input/messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"sparse_nested_stream_empty\", \"data\": {\"id\": 1, \"updated_at\": 100, \"obj_nest1\": {\"obj_nest2\": {\"foo\": \"bar\"}}, \"arr_nest1\": [{\"arr_nest2\": [{\"foo\": \"bar1\"}, {\"foo\": \"bar2\"}]}, {\"arr_nest2\": [{\"foo\": \"baz1\"}, {\"foo\": \"baz2\"}]}]}, \"emitted_at\": 1672567200}}\n```\n\n----------------------------------------\n\nTITLE: Running the Glassfrog Connector CI Test Suite Locally\nDESCRIPTION: This command executes the full Airbyte connector test suite for the `source-glassfrog` connector locally using the `airbyte-ci` tool. It requires `airbyte-ci` to be installed and runs various tests, including integration and acceptance tests, to ensure the connector functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-glassfrog/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-glassfrog test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twilio build\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Vitally connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vitally/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-vitally:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-vitally:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-vitally:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-vitally:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Navigating to Airbyte workspace normalize folder\nDESCRIPTION: Command to change directory to an Airbyte-generated normalize workspace folder where dbt files are located.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/dbt-project-template/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd /tmp/dev_root/workspace/1/0/normalize\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Humanitix Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Humanitix source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-humanitix/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-humanitix test\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for MDX Documentation in JavaScript\nDESCRIPTION: This snippet imports standard Docusaurus/Theme UI components (`Tabs`, `TabItem`) and custom React components (`SnowflakeMigrationGenerator`, `BigQueryMigrationGenerator`, `RedshiftMigrationGenerator`, `PostgresMigrationGenerator`) from a local JavaScript file (`./destinations_v2.js`). These components are likely used within the MDX document to render UI elements like tabs and potentially generate or display database-specific migration instructions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/upgrading_to_destinations_v2.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport {SnowflakeMigrationGenerator, BigQueryMigrationGenerator, RedshiftMigrationGenerator, PostgresMigrationGenerator} from './destinations_v2.js'\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secrets with kubectl CLI for Airbyte Configuration (Shell)\nDESCRIPTION: This `kubectl` command creates a generic Kubernetes Secret named `airbyte-config-secrets` within the `airbyte` namespace. It uses the `--from-literal` flag to directly provide sensitive key-value pairs (e.g., `key-1='value-1'`) without needing a separate YAML file. This method is suitable for creating secrets directly from the command line for Airbyte configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/creating-secrets.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl create secret generic airbyte-config-secrets \\\n  --from-literal=key-1='value-1' \\\n  --from-literal=key2='value-2' \\\n  --namespace airbyte\n```\n\n----------------------------------------\n\nTITLE: Disabling Automatic Schema Refresh in Airbyte (YAML, Airbyte)\nDESCRIPTION: This snippet demonstrates how to disable automatic schema refreshes by setting `DISCOVER_REFRESH_WINDOW_MINUTES` to 0 in `values.yaml`. When set to 0, schema changes are not detected automatically and must be refreshed manually via the Airbyte UI or API. This configuration is suited for users who want full manual control over when schema discovery is run.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/release_notes/v-1.4.md#2025-04-23_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nworker:\n  env_vars:\n    DISCOVER_REFRESH_WINDOW_MINUTES: 0 # Airbyte does not automatically detect schema changes.\n```\n\n----------------------------------------\n\nTITLE: Syncing Vale Styles\nDESCRIPTION: Command to synchronize Vale and get the latest style packages after installation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nvale sync\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Connector Images into Kind Cluster\nDESCRIPTION: Command to load a custom connector Docker image into the Kind cluster for use with Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkind load docker-image <image-name>:<image-tag> -n airbyte-abctl\n```\n\n----------------------------------------\n\nTITLE: Installing Smartsheets Connector Dependencies with Poetry\nDESCRIPTION: Command to install the Smartsheets connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartsheets/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Defining Airbyte Private Key Environment Variable\nDESCRIPTION: Environment variable name used to store the private key for Airbyte authentication and encryption.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/unit_tests/private_key_path.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nAIRBYTE_PRIVATE_KEY\n```\n\n----------------------------------------\n\nTITLE: Some Stream That Was Empty Record Structure\nDESCRIPTION: Illustrates the structure of records in the 'some_stream_that_was_empty' stream. It contains simple key-value pairs for id and date fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"RECORD\",\n  \"record\":{\n    \"stream\":\"some_stream_that_was_empty\",\n    \"data\":{\n      \"id\":1,\n      \"date\": \"2020-11-05\"\n    },\n    \"emitted_at\":1623871660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building the Factorial API Source Connector with airbyte-ci\nDESCRIPTION: Command to build the source-factorial connector locally, creating a dev image (source-factorial:dev) that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-factorial/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-factorial build\n```\n\n----------------------------------------\n\nTITLE: Running CI test suite for Sentry connector\nDESCRIPTION: Command to run the full test suite for the Sentry connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sentry/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sentry test\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment and install dependencies for local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npip install '.[tests]'\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image Using airbyte-ci (Python Bash Command)\nDESCRIPTION: Builds a Docker image for the connector utilizing Airbyte's CI tooling, tagging it as 'airbyte/source-google-directory:dev' on the local host. The 'airbyte-ci' tool must be installed, and the command should be run from the connector's directory. No further parameters are needed and upon completion, the Docker image can be used for local testing or as a deployment artifact.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-directory build\n```\n\n----------------------------------------\n\nTITLE: Building and Running Custom Connector Docker Image - Bash\nDESCRIPTION: These bash commands build a Docker image for the connector with a custom tag and run its 'spec' command to validate the build. The first command builds the image using the Dockerfile in the context directory. The second invokes the connector within a new container instance. Requires Docker to be installed and available, and assumes a valid Dockerfile and config are present.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-google-analytics-v4-service-account-only:dev .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run airbyte/source-google-analytics-v4-service-account-only:dev spec\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: Builds the Docker image for the `source-commcare` connector using the `airbyte-ci` tool. Requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-commcare:dev` on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-commcare/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-commcare build\n```\n\n----------------------------------------\n\nTITLE: Testing JustSift Source Connector for Airbyte\nDESCRIPTION: This command runs the acceptance tests for the JustSift source connector using airbyte-ci. It verifies the functionality and compatibility of the connector with Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-just-sift/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-just-sift test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Retently Source Connector\nDESCRIPTION: Command to run the full test suite for the Retently source connector using the airbyte-ci tool. This executes all tests defined for the connector to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-retently/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-retently test\n```\n\n----------------------------------------\n\nTITLE: Airbyte STATE Message with Global State\nDESCRIPTION: A JSON example of an Airbyte STATE message containing global state information. The state includes a shared start_date parameter which is commonly used to track incremental sync progress.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": {\"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Testing the Ding Connect Connector with airbyte-ci - Bash\nDESCRIPTION: Executes the acceptance tests for the Ding Connect Airbyte connector using the airbyte-ci command line tool. Assumes the airbyte-ci CLI is installed and configured properly. This command runs predefined test suites to validate the connector's functionality, ensuring compatibility and reliability before deployment; test logs and results are typically output to the console or associated log files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ding-connect/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ding-connect test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite with airbyte-ci - Bash\nDESCRIPTION: This snippet runs the full test suite for the connector via the airbyte-ci command-line tool. The airbyte-ci utility must be installed and configured prior to running the test command. This command ensures all integration and acceptance tests are performed before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-azure-blob-storage test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for SFTP-Bulk Connector\nDESCRIPTION: Command to build a Docker image for the connector using Airbyte's CI tools. Produces an image tagged as airbyte/source-sftp-bulk:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sftp-bulk build\n```\n\n----------------------------------------\n\nTITLE: Testing the Concord Source Connector using Bash\nDESCRIPTION: This command executes the acceptance tests for the `source-concord` connector using the `airbyte-ci` tool. It verifies the connector's functionality against a predefined set of test cases. Running this command requires `airbyte-ci` to be installed and typically depends on a previously built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-concord/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-concord test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Set of commands to run various connector operations like spec, check, discover, and read using Docker container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-northpass-lms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-northpass-lms:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-northpass-lms:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-northpass-lms:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-northpass-lms:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Google Forms Connector Dev Image using airbyte-ci (Bash)\nDESCRIPTION: Executes the `airbyte-ci` command to build a local development Docker image tagged as `source-google-forms:dev`. This command is part of the local development setup and requires `airbyte-ci` to be installed and configured. It targets the connector named `source-google-forms`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-forms/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-forms build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Organization Resource Roles Permission Matrix\nDESCRIPTION: Markdown table showing the permissions available to different organization-level roles (Member, Reader, Runner, Editor, Admin). Permissions include reading organization details, creating workspaces, and updating organization settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/rbac.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Permissions             | Member     |  Reader    | Runner |  Editor | Admin |\n| :---------------------- | :--------: | :--------: | :--------: | :--------: |:--------: |\n| **Read Organization**<br /><ul><li> Read individual organizations</li></ul> | X | X | X | X | X |\n| **Create Workspace**<br /><ul><li>Create new workspace within a specified organization</li><li>Delete a workspace</li></ul> | | | | X | X |\n| **Update Organization**<br /><ul><li>Modify organization settings, including billing, PbA, SSO</li><li>Modify user roles within the organization</li></ul> | | |  |  | X |\n```\n\n----------------------------------------\n\nTITLE: Making a Request with Basic HTTP Authentication using cURL\nDESCRIPTION: This cURL command demonstrates how to make a GET request to an API endpoint (specifically the Greenhouse API example) using Basic HTTP authentication. The 'Authorization' header is set to 'Basic' followed by the base64 encoded string of 'username:password' (dXNlcjpwYXNzd2Q= represents user:passwd). Replace `<stream path>` with the actual API endpoint path.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/authentication.md#2025-04-23_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X GET \\\n  -H \"Authorization: Basic dXNlcjpwYXNzd2Q=\" \\\n  https://harvest.greenhouse.io/v1/<stream path>\n```\n\n----------------------------------------\n\nTITLE: Defining Standard Main Section Content for Connector Documentation in Markdown\nDESCRIPTION: This snippet defines the standard Markdown content template required for the main section (under the primary connector name header) in Airbyte source connector documentation. It includes a `HideInUI` block and a standard introductory sentence linking to the connector's documentation page, using placeholders like `CONNECTOR_NAME_FROM_METADATA` and `{docs_link}`. This check ensures the introductory part of the documentation follows the standard format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n\n<HideInUI>\n\nThis page contains the setup guide and reference information for the [CONNECTOR_NAME_FROM_METADATA]({docs_link}) source connector.\n\n</HideInUI>\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies with Poetry (Bash)\nDESCRIPTION: This snippet installs all project dependencies, including development packages, using Poetry in the designated connector directory. Dependencies must be defined in pyproject.toml; Poetry (v1.7 or above) is required. The output is a fully provisioned Python development environment isolating project requirements. Limitations include platform compatibility of Poetry and pre-existing Python installation (>=3.9).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pinterest/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Enhanced Input Parameter Definition in spec.json (JSON)\nDESCRIPTION: Recommended `spec.json` definition for a 'user_name' input parameter using JSON Schema. This example includes 'title' and 'description' properties, which provide essential context to the user directly within the Airbyte UI, significantly improving usability and clarity as per the development guidelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/ux-handbook.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_name\": {\n      \"type\": \"string\",\n      \"description\": \"The username you use to login to the database\",\n      \"title\": \"Username\"\n    }\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Data Structure\nDESCRIPTION: A simple CSV data structure with two identical rows, each containing three values separated by commas.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/integration_tests/sample_files/pattern_match_test/not_this_folder/file_to_skip.txt#2025-04-23_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\na,b,c\na,b,c\n```\n\n----------------------------------------\n\nTITLE: Executing Unit Tests using Poetry and Pytest\nDESCRIPTION: This bash command executes the unit test suite for the project using `pytest`, managed via the `poetry` dependency manager. It is used to verify the correctness of the implemented code, including the pagination logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Bigin Connector Using airbyte-ci\nDESCRIPTION: Command to build a development image of the Zoho Bigin connector using airbyte-ci tool. Creates a dev image tagged as source-zoho-bigin:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-bigin/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-bigin build\n```\n\n----------------------------------------\n\nTITLE: Listing Modified Connectors\nDESCRIPTION: This command lists connectors that have been changed on the current branch using the modified flag.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --modified list\n```\n\n----------------------------------------\n\nTITLE: OAuth Response Example: Single Refresh Token - JSON\nDESCRIPTION: This JSON shows an OAuth server response where refresh_token is the only key, as required by the corresponding YAML spec. Airbyte extracts and stores this value using the provided configuration mapping. No additional tokens or data are expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_67\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"refresh_token\\\": \\\"YOUR_ACCESS_TOKEN_123\\\"\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Testing the source-getgist connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-getgist connector. Validates the connector functionality through Airbyte's testing framework.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-getgist/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-getgist test\n```\n\n----------------------------------------\n\nTITLE: Reloading Yellowbrick Configuration (SQL)\nDESCRIPTION: This SQL command reloads the Yellowbrick server configuration. It should be executed after enabling JSON support using either `ALTER SYSTEM` or `ALTER USER` to apply the changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/yellowbrick.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_reload_conf();\n```\n\n----------------------------------------\n\nTITLE: Building the Avni Source Connector Docker Image with Gradle - Shell\nDESCRIPTION: This shell command uses Gradle to build the Docker image for the Avni source connector. It relies on the Gradle setup and the project’s build configuration to bundle and tag the image according to the Dockerfile's LABEL properties. Useful during CI processes or for standardized Docker builds.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-avni/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-avni:airbyteDocker\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte Manifest-Only Connector Image with airbyte-ci (Bash)\nDESCRIPTION: Builds the Docker image for the 'source-polygon-stock-api' manifest-only Airbyte connector using the airbyte-ci tool. Requires installation of airbyte-ci and access to the connector source. The command tags the built image as 'airbyte/source-polygon-stock-api:dev' and stores it locally on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-polygon-stock-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-polygon-stock-api build\n```\n\n----------------------------------------\n\nTITLE: Building Primetric Source Connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Docker image for the Primetric source connector using airbyte-ci. This will create an image tagged as 'airbyte/source-primetric:dev' on your local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-primetric/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-primetric build\n```\n\n----------------------------------------\n\nTITLE: Building Connector Docker Image with airbyte-ci (Bash)\nDESCRIPTION: This Bash command uses the airbyte-ci tool to build the Docker image for the 'source-paypal-transaction' connector locally. Ensure airbyte-ci is installed per Airbyte documentation. The command targets manifest-only connectors, placing the resulting image as 'airbyte/source-paypal-transaction:dev' on the local host. Requires prior access to the Airbyte codebase and appropriate permissions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paypal-transaction/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paypal-transaction build\n```\n\n----------------------------------------\n\nTITLE: Building AgileCRM Connector Using airbyte-ci\nDESCRIPTION: Command to build the AgileCRM connector locally, creating a development image tagged as source-agilecrm:dev\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-agilecrm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-agilecrm build\n```\n\n----------------------------------------\n\nTITLE: Building Chameleon Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Chameleon source connector locally, creating a dev image named source-chameleon:dev for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-chameleon/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-chameleon build\n```\n\n----------------------------------------\n\nTITLE: Building Testrail Source Connector with airbyte-ci\nDESCRIPTION: This command builds a dev image (source-testrail:dev) for local testing of the connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-testrail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-testrail build\n```\n\n----------------------------------------\n\nTITLE: Building the SavvyCal Connector in Bash\nDESCRIPTION: Command for building the SavvyCal connector locally using airbyte-ci. This creates a dev image (source-savvycal:dev) that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-savvycal/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-savvycal build\n```\n\n----------------------------------------\n\nTITLE: Building Senseforce connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the Senseforce source connector using the airbyte-ci tool. This creates a Docker image with the tag airbyte/source-senseforce:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-senseforce/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-senseforce build\n```\n\n----------------------------------------\n\nTITLE: Reading SurveyMonkey Source with Poetry and CLI\nDESCRIPTION: Command line example showing execution of the survey-monkey source connector using Poetry. Invokes Python entry point to read using a JSON config and catalog argument. Inputs: valid config.json and catalog.json files. Outputs: streamed survey and survey response data printed to stdout or processed by Airbyte platform.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/7-reading-from-a-subresource.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Pages Stream Schema in JSON\nDESCRIPTION: JSON schema example for the pages stream in Google Analytics, including metrics related to individual page performance and user behavior.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_hostname\":\"mydemo.com\",\"ga_pagePath\":\"/home5\",\"ga_pageviews\":63,\"ga_uniquePageviews\":9,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_entrances\":9,\"ga_entranceRate\":14.285714285714285,\"ga_bounceRate\":0.0,\"ga_exits\":9,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring shared_preload_libraries with Multiple Extensions including pg_cron on Google Cloud SQL\nDESCRIPTION: Example configuration value for the `shared_preload_libraries` flag in Google Cloud SQL PostgreSQL settings when other extensions are already listed. It demonstrates adding `pg_cron` alongside `pg_stat_statements`, separated by a comma. This ensures all specified extensions are preloaded at startup. A database restart is required.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nshared_preload_libraries = 'pg_cron,pg_stat_statements'\n```\n\n----------------------------------------\n\nTITLE: Structuring Output Data for Kinesis Stream in JSON\nDESCRIPTION: This snippet illustrates the JSON structure of the data sent to Kinesis streams. It includes fields for a unique ID, timestamp, and the actual data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/kinesis.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_airbyte_ab_id\": \"UUID\",\n  \"_airbyte_emitted_at\": \"timestamp\",\n  \"_airbyte_data\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Airbyte Paddle Connector - Bash\nDESCRIPTION: This bash command initiates the acceptance tests for the source-paddle connector using the airbyte-ci tool. It relies on the same environment and prerequisites as the build command, specifically requiring airbyte-ci and the correct connector name argument. The command runs the full test suite, verifying that the connector implementation meets expected standards. Output includes the results of the acceptance tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-paddle/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-paddle test\n```\n\n----------------------------------------\n\nTITLE: Building Custom Clickhouse SSL Docker Image - Bash\nDESCRIPTION: Builds a Docker image named 'your_user/clickhouse-with-ssl:dev' using the Dockerfile 'Clickhouse.Dockerfile' located in the 'tools/integration-tests-ssl' directory. This command is required for setting up Clickhouse integration tests with SSL, and must be run from the correct directory. Input: No arguments required; Output: a Docker image with SSL-enabled Clickhouse ready for test use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clickhouse-strict-encrypt/ReadMe.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t your_user/clickhouse-with-ssl:dev -f Clickhouse.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: A markdown table showing the version history and changes made to the Google Workspace Admin Reports connector, including version numbers, dates, pull request links, and subjects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-workspace-admin-reports.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                   |\n| :------ | :--------- | :------------------------------------------------------- | :---------------------------------------- |\n| 0.1.8   | 2022-02-24 | [10244](https://github.com/airbytehq/airbyte/pull/10244) | Add Meet Stream                           |\n| 0.1.7   | 2021-12-06 | [8524](https://github.com/airbytehq/airbyte/pull/8524)   | Update connector fields title/description |\n| 0.1.6   | 2021-11-02 | [7623](https://github.com/airbytehq/airbyte/pull/7623)   | Migrate to the CDK                        |\n| 0.1.5   | 2021-10-07 | [6878](https://github.com/airbytehq/airbyte/pull/6878)   | Improve testing & output schemas          |\n```\n\n----------------------------------------\n\nTITLE: Building Imagga Source Connector for Airbyte using airbyte-ci\nDESCRIPTION: This command builds a development image of the Imagga source connector for local testing. It uses airbyte-ci to create a dev image named 'source-imagga:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-imagga/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-imagga build\n```\n\n----------------------------------------\n\nTITLE: Testing Nebius AI Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Nebius AI source connector using airbyte-ci tool. Validates the connector's functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nebius-ai/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nebius-ai test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Serpstat Connector\nDESCRIPTION: Command to run the full test suite for the source-serpstat connector using the airbyte-ci tool. This validates that any changes made to the connector are functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-serpstat/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-serpstat test\n```\n\n----------------------------------------\n\nTITLE: Exchange Rate Records with Special Characters\nDESCRIPTION: JSON records containing exchange rate data with various currencies, timestamps, and special characters in field names. Records include regular exchange rates with different time zones and formats.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rate\", \"emitted_at\": 1602637589000, \"data\": { \"id\": 1, \"currency\": \"USD\", \"date\": \"2020-08-29\", \"timestamp_col\": \"2020-08-29T00:00:00.000000-0000\", \"NZD\": 1.14, \"HKD@spéçiäl & characters\": 2.13, \"HKD_special___characters\": \"column name collision?\", \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\" }}}\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Connector Test Suite using airbyte-ci (bash)\nDESCRIPTION: This example runs the full test suite for the Airbyte source-pivotal-tracker connector using the airbyte-ci CLI tool. The test suite validates the connector's integrity and compatibility before publishing. The requirement is that 'airbyte-ci' is installed and configured for the local Airbyte repository. This is a standard validation step expected before bumping versions or submitting changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pivotal-tracker/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pivotal-tracker test\n\n```\n\n----------------------------------------\n\nTITLE: Sample MailGun API Request\nDESCRIPTION: Example GET request to the MailGun API endpoint for retrieving domains information using API v3.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mailgun.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.mailgun.net/v3/domains\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Pabbly Connector using Bash\nDESCRIPTION: This command executes the acceptance test suite for the 'source-pabbly-subscriptions-billing' connector using the 'airbyte-ci' tool. It validates the connector's implementation against Airbyte's specifications. This command typically runs against the development image built in the previous step and requires 'airbyte-ci'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pabbly-subscriptions-billing/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pabbly-subscriptions-billing test\n```\n\n----------------------------------------\n\nTITLE: Building Blogger Source Connector with Airbyte-CI\nDESCRIPTION: Command to build a development image of the Blogger source connector using airbyte-ci tool. Creates a dev image tagged as 'source-blogger:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-blogger/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-blogger build\n```\n\n----------------------------------------\n\nTITLE: Handling Untyped JSON Arrays in Avro Conversion\nDESCRIPTION: Example showing how JSON arrays with no item type specification are handled in Avro conversion by serializing the entire array as a string to maintain compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"identifier\": {\n      \"type\": \"array\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"identifier\": [\"151\", 152, true, { \"id\": 153 }, null]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"identifier\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"default\": null\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"identifier\": \"[151, 152, true, {\\\"id\\\": 153}, null]\"\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Files by Name Pattern using Regular Expression\nDESCRIPTION: Presents a regular expression pattern (`log-([0-9]{4})([0-9]{2})([0-9]{2})`) designed to match filenames following the format `log-YYYYMMDD`. This example illustrates how to use regular expressions within the SFTP Bulk connector configuration to select specific files for syncing based on their naming convention (e.g., `log-20230713`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sftp-bulk.md#2025-04-23_snippet_3\n\nLANGUAGE: regex\nCODE:\n```\nlog-([0-9]{4})([0-9]{2})([0-9]{2})\n```\n\n----------------------------------------\n\nTITLE: Example: WaitUntilTimeFromHeader Backoff with Regex and Minimum Wait in YAML\nDESCRIPTION: Provides a requester configuration example for backoff that waits until a time parsed (using regex) from a specified header, with a minimum enforced wait. This allows for sophisticated rate limiting and server-directed retry scheduling in Airbyte YAML pipelines.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md#2025-04-23_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitUntilTimeFromHeader\"\n          header: \"wait_until\"\n          regex: \"[-+]?\\d+\"\n          min_wait: 5\n```\n\n----------------------------------------\n\nTITLE: Nested Stream with Complex Columns Record Structure\nDESCRIPTION: Demonstrates the structure of records in the 'nested_stream_with_complex_columns_resulting_into_long_names' stream. It includes nested arrays, objects, and fields with special characters in their names.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"RECORD\",\n  \"record\": {\n    \"stream\": \"nested_stream_with_complex_columns_resulting_into_long_names\",\n    \"emitted_at\": 1602638599000,\n    \"data\": {\n      \"id\": 4.2,\n      \"date\": \"2020-08-29T00:00:00Z\",\n      \"partition\": {\n        \"double_array_data\": [[ { \"id\": \"EUR\" } ]],\n        \"DATA\": [ {\"currency\": \"EUR\" } ],\n        \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Export Parameters with Airbyte Source Config - JSON\nDESCRIPTION: This snippet provides a sample Airbyte source configuration file (config.json) in JSON format, specifying a large data generation parameter 'count' and seed value. The 'count' key determines how many fake user records to generate (over 2 billion in this example), and 'seed' enables reproducible random data. This file is required as an argument to the Airbyte source connector during export and should be customized for the intended dataset size.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/integration_tests/csv_export/README.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"count\\\": 2039840637,\\n  \\\"seed\\\": 0\\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Tiktok-Marketing Connector\nDESCRIPTION: This command builds a Docker image for the Tiktok-Marketing connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tiktok-marketing/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tiktok-marketing build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-microsoft-onedrive test\n```\n\n----------------------------------------\n\nTITLE: Running Stripe Connector Commands Locally\nDESCRIPTION: These commands demonstrate how to run various Stripe connector operations locally using Poetry, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-stripe spec\npoetry run source-stripe check --config secrets/config.json\npoetry run source-stripe discover --config secrets/config.json\npoetry run source-stripe read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example Input Record Structure for SQS Message\nDESCRIPTION: Shows the structure of an input AirbyteMessageRecord with nested data that will be processed into an SQS message.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\":\n    {\n        \"parent_key\": {\n            \"nested_key\": \"nested_value\"\n        },\n        \"top_key\": \"top_value\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Pipedrive Connector Read Command via Docker (Bash)\nDESCRIPTION: Executes the `read` command within a temporary Docker container using the `airbyte/source-pipedrive:dev` image. It mounts the local `secrets` directory (for `config.json`) and the `integration_tests` directory (for `configured_catalog.json`) to read data from the Pipedrive source according to the specified configuration and catalog.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pipedrive:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: DateTime Field Migration Examples\nDESCRIPTION: Detailed examples of date-time field format changes across different streams, showing the conversion from various formats to RFC3339 standard.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bing-ads-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Affected streams                                                                                                    | Field_name      | Old type                  | New type (`RFC3339`)            |\n| ------------------------------------------------------------------------------------------------------------------- | --------------- | ------------------------- | ------------------------------- |\n| `AppInstallAds`, `AppInstallAdLabels`, `Labels`, `Campaign Labels`, `Keyword Labels`, `Ad Group Labels`, `Keywords` | `Modified Time` | `04/27/2023 18:00:14.970` | `2023-04-27T16:00:14.970+00:00` |\n| `Budget Summary Report`                                                                                             | `Date`          | `6/10/2021`               | `2021-06-10`                    |\n| `* Report Hourly`                                                                                                   | `TimePeriod`    | `2023-11-04\\|11`          | `2023-11-04T11:00:00+00:00`     |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Smartengage Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Smartengage source connector. The resulting image will be tagged as 'airbyte/source-smartengage:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartengage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartengage build\n```\n\n----------------------------------------\n\nTITLE: Emitting Airbyte Record Message in JSON\nDESCRIPTION: Example of an `AirbyteRecordMessage` containing data for the \"users\" stream. The `data` field adheres to the JSON schema defined in the corresponding `AirbyteStream`, showcasing string, integer, and an array of timestamp strings conforming to the `timestamp_with_timezone` type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stream\": \"users\",\n  \"data\": {\n    \"username\": \"someone42\",\n    \"age\": 84,\n    \"appointments\": [\"2021-11-22T01:23:45+00:00\", \"2022-01-22T14:00:00+00:00\"]\n  },\n  \"emitted_at\": 1623861660\n}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-asana test\n```\n\n----------------------------------------\n\nTITLE: Building Marketstack Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Marketstack connector using airbyte-ci tool. Creates a dev image tagged as source-marketstack:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketstack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-marketstack build\n```\n\n----------------------------------------\n\nTITLE: Airbyte RECORD Message with UTC Timezone\nDESCRIPTION: Third Airbyte RECORD message example with UTC timezone (Z) specification in the datetime and time fields. Demonstrates how Zulu/UTC time is represented in the Airbyte protocol.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/every_time_type_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"every_time_type_test_1\", \"emitted_at\": 1721428635000, \"data\": { \"date\": \"2024-07-27\", \"datetime_unspecified\": \"2024-07-27T20:00:00Z\", \"datetime_with_timezone\": \"2024-07-27T20:00:00Z\", \"datetime_without_timezone\": \"2024-07-27T20:00:00\", \"time_unspecified\": \"20:00:00Z\", \"time_with_timezone\": \"20:00:00Z\", \"time_without_timezone\": \"20:00:00\" }}}\n```\n\n----------------------------------------\n\nTITLE: Upgrading abctl via Homebrew for Mac\nDESCRIPTION: Command to keep abctl up to date using Homebrew package manager on Mac.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew upgrade abctl\n```\n\n----------------------------------------\n\nTITLE: Testing the Openweather Connector with airbyte-ci - Bash\nDESCRIPTION: This snippet instructs how to run the Airbyte connector test suite for the Openweather source using airbyte-ci. The command should be executed in a development environment where airbyte-ci is installed and ensures that all connector specifications and integration requirements are validated before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openweather/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-openweather test\n\n```\n\n----------------------------------------\n\nTITLE: Testing the Oveit Connector using airbyte-ci\nDESCRIPTION: This command uses the `airbyte-ci` tool to execute the acceptance tests defined for the `source-oveit` connector. This ensures the connector functions correctly according to Airbyte's standards. Requires `airbyte-ci` to be installed and typically run after building the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oveit/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oveit test\n```\n\n----------------------------------------\n\nTITLE: Building the Recruitee Connector with airbyte-ci in Bash\nDESCRIPTION: Command to build the docker image for the Recruitee source connector using airbyte-ci. This creates a local image tagged as airbyte/source-recruitee:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recruitee/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recruitee build\n```\n\n----------------------------------------\n\nTITLE: Converting Untyped JSON Objects to Avro\nDESCRIPTION: Example showing how an untyped JSON object (without properties specification) is converted to Avro by serializing the entire object as a string.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"username\": \"343-guilty-spark\",\n  \"password\": 1439,\n  \"active\": true\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"string\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"{\\\"username\\\":\\\"343-guilty-spark\\\",\\\"password\\\":1439,\\\"active\\\":true}\"\n```\n\n----------------------------------------\n\nTITLE: Running the Gong Source Connector Test Suite with airbyte-ci (Bash)\nDESCRIPTION: This snippet illustrates running the full Airbyte CI test suite for the Gong source connector using the airbyte-ci CLI. The --name parameter sets the target connector for testing, ensuring that all necessary tests relevant to source-gong are executed locally. Requires airbyte-ci to be installed and appropriately configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gong/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gong test\n```\n\n----------------------------------------\n\nTITLE: Combinatorial Partitioned Requests - Google Pagespeed API Example - curl - Bash\nDESCRIPTION: Illustrates generating all combinations of partitioned GET requests for the Google Pagespeed API using multiple parameter lists (URLs and strategies). This configuration results in one API call for each combination of 'url' and 'strategy' parameter values, shown as curl commands.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/partitioning.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=example.com&strategy=desktop\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=example.com&strategy=mobile\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=example.org&strategy=desktop\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=example.org&strategy=mobile\n```\n\n----------------------------------------\n\nTITLE: Example Firebase Realtime Database JSON Structure\nDESCRIPTION: Illustrates the typical JSON tree structure used by Firebase Realtime Database. This example shows a root object containing 'my-data', which in turn contains 'dinosaurs' with specific dinosaur details (height, length, weight).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebase-realtime-database/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"my-data\": {\n    \"dinosaurs\": {\n      \"lambeosaurus\": {\n        \"height\": 2.1,\n        \"length\": 12.5,\n        \"weight\": 5000\n      },\n      \"stegosaurus\": {\n        \"height\": 4,\n        \"length\": 9,\n        \"weight\": 2500\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci (Bash)\nDESCRIPTION: Describes how to build a Docker image for the Google Analytics V4 connector using Airbyte's CI tool. The command builds the image locally and assigns the tag 'airbyte/source-google-analytics-v4:dev' for use in subsequent local Docker runs. The airbyte-ci tool must be installed and properly configured according to Airbyte's CI setup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-v4 build\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies using Shell\nDESCRIPTION: These shell commands first activate the previously created '.venv' virtual environment and then install the required Python packages listed in 'requirements.txt' using pip. Note that dependencies should be primarily managed in 'setup.py', while 'requirements.txt' is used for editable installs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Connector as a Docker Container - Bash\nDESCRIPTION: Runs connector CLI commands (spec, check, discover, read) inside a Docker container built from airbyte/source-file:dev. Uses Docker bind mounts to provide necessary configuration and secrets directories and outputs command results. The --rm flag ensures the container is removed after execution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-file:dev spec\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-file:dev check --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-file:dev discover --config /secrets/config.json\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-file:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests in Airbyte CI - Bash\nDESCRIPTION: This snippet shows how to run the Airbyte continuous integration test suite for the DynamoDB connector. It requires the 'airbyte-ci' CLI to be installed and will execute all relevant tests to validate changes within the connector before publishing a new version. The connector name must match the referenced source.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-dynamodb test\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: Version history table showing release dates and changes made to the connector, starting from initial release 0.0.1 to current version 3.0.0.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/track-pms.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date       | Subject        |\n|------------------|------------|----------------|\n| 3.0.0 | 2025-02-26 | Drop redundant streams & omit unneeded sensitive fields from accounting_* streams |\n| 2.0.0 | 2025-02-13 | Rename and alphabetize folio_id stream |\n| 1.0.0 | 2025-01-16 | Fix housekeeping_work_orders incremental field; add reservations endpoint |\n| 0.1.0 | 2025-01-16 | Move kebab case streams to snake case; alphabetize streams |\n| 0.0.1 | 2024-10-18 | Initial release by [@blakeflei](https://github.com/blakeflei) via Connector Builder|\n```\n\n----------------------------------------\n\nTITLE: Configuring Stripe Source with Strongly Typed Syntax in Terraform\nDESCRIPTION: Creates a Stripe source using strongly typed configuration in Terraform. This approach uses a specialized resource type specific to Stripe and structured parameter definitions with native HCL syntax instead of JSON encoding.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/terraform-documentation.md#2025-04-23_snippet_5\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"airbyte_source_stripe\" \"my_source_stripe\" {\n    configuration = {\n        source_type = \"stripe\"\n        account_id = \"YOUR_STRIPE_ACCOUNT_ID\"\n        client_secret = \"YOUR_STRIPE_CLIENT_SECRET\"\n        start_date = \"2023-07-01T00:00:00Z\"\n        lookback_window_days = 0\n        slice_range = 365\n    }\n    name = \"Stripe\"\n    workspace_id = var.workspace_id\n}\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Gradle (Shell)\nDESCRIPTION: Builds the source-e2e-test-cloud connector Java code using the Gradle wrapper script. This command should be executed from the root of the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-e2e-test-cloud/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-e2e-test-cloud:build\n```\n\n----------------------------------------\n\nTITLE: Deduplication Exchange Rate Records JSON\nDESCRIPTION: Exchange rate records with deduplication logic, containing similar data structure but in a dedicated deduplication stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_incremental.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"dedup_exchange_rate\", \"emitted_at\": 1602637990800, \"data\": { \"id\": 2, \"currency\": \"EUR\", \"date\": \"\", \"timestamp_col\": \"\", \"NZD\": 2.43, \"HKD@spéçiäl & characters\": 5.4, \"HKD_special___characters\": \"column name collision?\", \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\"}}}\n```\n\n----------------------------------------\n\nTITLE: Running SFTP Connector Unit Tests via Gradle in Java\nDESCRIPTION: Command to run unit tests for the SFTP source connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-sftp:unitTest\n```\n\n----------------------------------------\n\nTITLE: Running the Persistiq Connector Test Suite using airbyte-ci\nDESCRIPTION: This command employs the `airbyte-ci` tool to run the complete Continuous Integration (CI) test suite locally for the `source-persistiq` connector. This is crucial for verifying connector functionality and adherence to Airbyte standards before contributing changes or publishing a new version. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-persistiq/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-persistiq test\n```\n\n----------------------------------------\n\nTITLE: Installing Current Directory Package Editable (pip)\nDESCRIPTION: This shell command uses `pip`, the Python package installer, with the `-e` flag to install the package located in the current directory (`.`) in 'editable' mode. This is commonly used during development, as changes made to the source code are immediately effective without needing to reinstall the package.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4-service-account-only/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n-e .\n```\n\n----------------------------------------\n\nTITLE: JSON Object with All Null Values for Schema Testing\nDESCRIPTION: A JSON object with the same structure as the previous ones but with all values set to null. This appears to be a test case for handling null values across all fields in the schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_coerced_schemaless_messages_out.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"schemaless_object\": null, \"schematized_object\": null, \"combined_type\": null, \"union_type\": null, \"schemaless_array\": null, \"mixed_array_integer_and_schemaless_object\": null, \"array_of_union_integer_and_schemaless_array\": null, \"union_of_objects_with_properties_identical\": null, \"union_of_objects_with_properties_overlapping\": null, \"union_of_objects_with_properties_nonoverlapping\": null, \"union_of_objects_with_properties_contradicting\":null, \"empty_object\": null, \"object_with_null_properties\": null, \"combined_with_null\": null, \"union_with_null\": null, \"combined_nulls\": null, \"compact_union\": null }\n```\n\n----------------------------------------\n\nTITLE: Modifying Static Schema in Python Stream Class\nDESCRIPTION: Example showing how to override the get_json_schema method to dynamically modify a static schema loaded from a JSON file. The method first loads the base schema using the parent class implementation, then adds a new dynamic property before returning the modified schema.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/full-refresh-stream.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema\n```\n\n----------------------------------------\n\nTITLE: Example Output for Local File Location Script\nDESCRIPTION: Provides sample output generated by the script designed to locate local file destination outputs. It shows the file structure and example file (`exchange_rate_raw.csv`) as seen from both the container's perspective (`/local/...`) and the host's perspective (`/tmp/airbyte_local/...`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nIn the container:\n/local\n/local/data\n/local/data/exchange_rate_raw.csv\n\nOn the host:\n/tmp/airbyte_local\n/tmp/airbyte_local/data\n/tmp/airbyte_local/data/exchange_rate_raw.csv\n```\n\n----------------------------------------\n\nTITLE: Running Ip2whois Source Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the Ip2whois connector as a docker container. These commands include spec, check, discover, and read operations, demonstrating how to use the connector with various configurations and inputs.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ip2whois/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-ip2whois:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ip2whois:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-ip2whois:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-ip2whois:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Brevo Source Connector with Airbyte-CI\nDESCRIPTION: Command to build a development image of the Brevo source connector using airbyte-ci tool. Creates a dev image tagged as 'source-brevo:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-brevo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-brevo build\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte CI with Make\nDESCRIPTION: Installation of Airbyte CI development tools using make command from the repository root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n # From airbyte repo root:\n make tools.airbyte-ci-dev.install\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-databend test\n```\n\n----------------------------------------\n\nTITLE: Running the Test Suite for the Source Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash snippet runs the full test suite for the source-pokeapi connector using the airbyte-ci testing utility. It assumes airbyte-ci is installed and available on the PATH. The command runs integration and unit tests defined for the connector within the Airbyte framework and outputs the test results in the console.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pokeapi/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pokeapi test\n```\n\n----------------------------------------\n\nTITLE: Error Message from Failed Amazon Seller Partner API Report Retrieval\nDESCRIPTION: Sample error message shown when a report retrieval fails due to rate limiting or other API issues. It includes details about the report name, time period, report ID, and the specific error that occurred.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-seller-partner.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nFailed to retrieve the report 'YOUR_REPORT_NAME' for period 2024-01-01T12:01:15Z-2024-01-15T12:01:14Z. \nThis will be read during the next sync. Report ID: YOUR_REPORT_ID. Error: Failed to retrieve the report result document.\n```\n\n----------------------------------------\n\nTITLE: Values YAML File Reading Error\nDESCRIPTION: Error message when abctl is unable to read values from a YAML file, typically because the file path is incorrect.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nunable to merge values with values file './values.yaml': unable to read values\nfrom yaml file './values.yaml': failed to read file ./values.yaml: open ./values.yaml:\n no such file or directory\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for SFTP-JSON Connector\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-sftp-json test\n```\n\n----------------------------------------\n\nTITLE: Building the RentCast Connector in Bash\nDESCRIPTION: Command to build the RentCast source connector locally using airbyte-ci. This creates a development image named 'source-rentcast:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rentcast/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rentcast build\n```\n\n----------------------------------------\n\nTITLE: Airbyte Record Message Format in JSON\nDESCRIPTION: This snippet demonstrates the structure of an Airbyte record message. It includes the message type, stream name, emission timestamp, and a data payload with various fields such as id, str, num, list, dict, and double_list.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/integration_tests/test_data/messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_1\", \"emitted_at\": 1602637589000, \"data\": { \"id\" : \"7\", \"str\": \"test7\", \"num\": 7, \"list\": [\"test7\"], \"dict\": {\"key\": \"value7\"}, \"double_list\": [[\"test_double_list7\"]], \"other\": \"other\" }}}\n```\n\n----------------------------------------\n\nTITLE: Building SAP Fieldglass Connector with airbyte-ci\nDESCRIPTION: Command to build the docker image for the SAP Fieldglass source connector using airbyte-ci tool. This will create an image with the tag 'airbyte/source-sap-fieldglass:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sap-fieldglass/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sap-fieldglass build\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Streams for SpotlerCRM in Markdown\nDESCRIPTION: Lists the supported data streams for the SpotlerCRM connector, including their primary keys, pagination method, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/spotlercrm.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| accounts | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| opportunities | id | DefaultPaginator | ✅ |  ❌  |\n| documents |  | DefaultPaginator | ✅ |  ❌  |\n| campaigns | id | DefaultPaginator | ✅ |  ❌  |\n|  cases | id | DefaultPaginator | ✅ |  ❌  |\n| activities | id | DefaultPaginator | ✅ |  ❌  |\n| opportunity_histories | id | DefaultPaginator | ✅ |  ❌  |\n| opportunity_lines | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency with Poetry (Bash)\nDESCRIPTION: Adds a new Python package dependency to the project using the Poetry package manager. Replace `<package-name>` with the actual name of the package to add. This command updates the `pyproject.toml` and `poetry.lock` files to include the new dependency. Requires Poetry to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Integration and Acceptance Tests via Gradle - Bash\nDESCRIPTION: This Gradle command runs integration and acceptance tests (including custom tests) for the DynamoDB source connector. To function, it must be run from the project root with Gradle installed. Test files are expected to be in 'src/test-integration' and related directories.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-dynamodb:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Executing Airbyte Source Read with Forced Pagination\nDESCRIPTION: This bash command executes the connector's `read` operation again. Because the `_PAGE_SIZE` constant was previously set to 1, this execution specifically tests the pagination logic by forcing the connector to fetch records one by one across multiple pages.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-survey-monkey-demo read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Visualizing Azure Test Flow with Mermaid Diagram\nDESCRIPTION: A flowchart diagram showing the test suite execution flow, from initialization through file generation, container setup, and parametrized tests for different file formats (CSV/JSONL/Parquet/Avro). The diagram illustrates the complete test lifecycle including setup, execution, and cleanup phases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/integration_tests/README.md#2025-04-23_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ntitle: Test Suite for Azure\n---\nflowchart TD\n    A[Test Setup Initialization ] -->\n    B[Generate Random CSV Files using Source-Faker]-->\n    C[Docker Container Setup for Azurite] -->\n    D[Parametrized Tests]\n    subgraph Parametrized Tests: CSV/JSONL/Parquet/Avro\n    D[Convert & upload file]-->\n    E[Run Tests]-->\n    F[Assert # of Records]-->\n    G[Remove uploaded files]\n    end\n```\n\n----------------------------------------\n\nTITLE: Defining Website Overview Stream Schema in JSON\nDESCRIPTION: JSON schema example for the website_overview stream in Google Analytics, including various metrics like users, sessions, and pageviews.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Visualizing OAuth Flow in Airbyte with Mermaid Diagram\nDESCRIPTION: A flowchart diagram showing the OAuth authorization process between users, Airbyte, and third-party services. It illustrates the sequence of authorization, code exchange, token acquisition, and data retrieval.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/oauth.md#2025-04-23_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD;\n    A[You] -->| Authorize Airbyte| B[Third-party authorization server]\n    B -->| Issues authorization code| C[Airbyte]\n    C -->| Exchanges authorization code for access token| B\n    C -->| Sends API Request with access token| D[Third-party resource server]\n    D -->| Returns protected data| C\n```\n\n----------------------------------------\n\nTITLE: Nutshell CRM Available Data Streams\nDESCRIPTION: Table listing all available data streams from Nutshell CRM with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nutshell.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| accounts | id | DefaultPaginator | ✅ |  ❌  |\n| accounts_list_items | id | No pagination | ✅ |  ❌  |\n| account_types | id | No pagination | ✅ |  ❌  |\n| industries | id | No pagination | ✅ |  ❌  |\n| activities | id | DefaultPaginator | ✅ |  ❌  |\n| activity_types | id | No pagination | ✅ |  ❌  |\n| audiences | id | No pagination | ✅ |  ❌  |\n| competitors | id | No pagination | ✅ |  ❌  |\n| competitor_maps | id | No pagination | ✅ |  ❌  |\n| leads_custom_fields | id | No pagination | ✅ |  ❌  |\n| leads_list_items | id | No pagination | ✅ |  ❌  |\n| leads | id | DefaultPaginator | ✅ |  ❌  |\n| leads_report | id | No pagination | ✅ |  ❌  |\n| contacts_custom_fields | id | No pagination | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| contacts_list_items | id | No pagination | ✅ |  ❌  |\n| events | id | DefaultPaginator | ✅ |  ❌  |\n| filters | id | No pagination | ✅ |  ❌  |\n| notes | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | No pagination | ✅ |  ❌  |\n| lead_products | id | No pagination | ✅ |  ❌  |\n| sources | id | No pagination | ✅ |  ❌  |\n| stages | id | No pagination | ✅ |  ❌  |\n| pipelines | id | No pagination | ✅ |  ❌  |\n| tags | id | No pagination | ✅ |  ❌  |\n| users | id | No pagination | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running tests for connector_ops package\nDESCRIPTION: This snippet demonstrates how to run tests for the connector_ops package using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connector_ops/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies using Poetry (Bash)\nDESCRIPTION: This command uses Poetry to install the necessary project dependencies, including development dependencies specified in the `pyproject.toml` file. It should be run from the connector's directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-search-console/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Command to install the connector dependencies including development packages using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-onedrive/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Data with Escaped Quotes in Google Drive Connector\nDESCRIPTION: This CSV snippet demonstrates how to use escape characters to include double quotes within a field value. The backslash is used to escape the double quote in the product description.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-drive.md#2025-04-23_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Executing MySQL Benchmark Script\nDESCRIPTION: Command to execute the MySQL benchmark creation script using the MySQL command line client.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/README.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmysql -h hostname -u user database < src/test-performance/sql/create_mysql_benchmarks.sql\n```\n\n----------------------------------------\n\nTITLE: Defining Four Weekly Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema example for the four_weekly_active_users stream in Google Analytics, showing the count of 28-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_28dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Unnest Alias Record Structure\nDESCRIPTION: Shows the structure of records in the 'unnest_alias' stream. It includes nested arrays and objects, with fields containing special characters in their names.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages_incremental.txt#2025-04-23_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"RECORD\",\n  \"record\":{\n    \"stream\":\"unnest_alias\",\n    \"data\":{\n      \"id\":1,\n      \"children\": [\n        {\n          \"ab_id\": 1,\n          \"owner\": {\n            \"owner_id\": 1,\n            \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]\n          }\n        },\n        {\n          \"ab_id\": 2,\n          \"owner\": {\n            \"owner_id\": 2,\n            \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]\n          }\n        }\n      ]\n    },\n    \"emitted_at\":1623861660\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Position-based CDC Records JSON\nDESCRIPTION: CDC records with additional position tracking using log positions. Demonstrates update and deletion patterns with position-based CDC implementation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_incremental.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"pos_dedup_cdcx\",\"data\":{\"id\":1,\"name\":\"mazda\",\"_ab_cdc_updated_at\":1623849130530,\"_ab_cdc_lsn\":26971624,\"_ab_cdc_log_pos\": 33274,\"_ab_cdc_deleted_at\":null},\"emitted_at\":1623859926}}\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for Python Duckdb Connector\nDESCRIPTION: Commands to set up a virtual environment, activate it, and install dependencies for the Duckdb destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-duckdb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running LaunchDarkly Source Connector Commands\nDESCRIPTION: Commands to run various operations of the LaunchDarkly source connector as a docker container, including spec, check, discover, and read operations. These commands use the locally built docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-launchdarkly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-launchdarkly:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-launchdarkly:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-launchdarkly:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-launchdarkly:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Daily Active Users Stream Schema in JSON\nDESCRIPTION: JSON schema for the daily_active_users stream in the Google Analytics connector. It includes the count of 1-day active users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_1dayUsers\":1,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Running the Connector in a Docker Container - Bash\nDESCRIPTION: This snippet shows how to run the connector's commands (spec, check, discover, read) inside a Docker container. Environment volumes are mounted for secrets and integration test files as required. The image must have been built and tagged as airbyte/source-azure-blob-storage:dev beforehand. Paths like /secrets/config.json and /integration_tests/configured_catalog.json are required inside the container.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-azure-blob-storage:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-azure-blob-storage:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-azure-blob-storage:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-azure-blob-storage:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Enabling User-Specific JSON Support in Yellowbrick (SQL)\nDESCRIPTION: This SQL command enables full JSON support (`enable_full_json`) specifically for the indicated user (`<user>`). Replace `<user>` with the actual username. This change requires a configuration reload using `SELECT pg_reload_conf();` afterwards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/yellowbrick.md#2025-04-23_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER \"<user>\" SET enable_full_json TO TRUE;\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_definition_version` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_definition_version` table, representing a specific version of an actor definition. It contains the version ID (primary key), the associated actor definition ID (foreign key), timestamps, documentation URL, Docker repository and image tag, and the connector specification (spec) as JSON. A unique constraint ensures that an actor definition cannot have duplicate Docker image tags.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name         | Datatype     | Description                                  |\n| ------------------- | ------------ | -------------------------------------------- |\n| id                 | UUID         | Primary key. Unique identifier for the version. |\n| actor_definition_id | UUID         | Foreign key referencing `actor_definition(id)`. |\n| created_at         | TIMESTAMP    | Timestamp when the record was created.       |\n| updated_at         | TIMESTAMP    | Timestamp when the record was last modified. |\n| documentation_url  | VARCHAR(256) | Documentation URL for this version.         |\n| docker_repository | VARCHAR(256) | Docker repository name.                      |\n| docker_image_tag  | VARCHAR(256) | Docker image tag for this version.           |\n| spec              | JSONB        | Specification JSON blob.                     |\n\n#### Indexes and Constraints\n\n- Primary Key: (`id`)\n- Foreign Key: `actor_definition_id` references `actor_definition(id)`\n- Unique Constraint: `actor_definition_id, docker_image_tag`\n```\n\n----------------------------------------\n\nTITLE: Generating Nullable Avro Schema for Date Logical Type\nDESCRIPTION: Illustrates the final Avro schema generated for a JSON field originally defined with `type: \"string\"` and `format: \"date\"`. It defines a union type allowing either `null` or an Avro `int` annotated with the `date` logical type.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"int\",\n      \"logicalType\": \"date\"\n    }\n  ]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Array Example\nDESCRIPTION: Example showing how PostgreSQL array type is represented as a JSON-formatted string array.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres.md#2025-04-23_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n[\"10001\",\"10002\",\"10003\",\"10004\"]\n```\n\n----------------------------------------\n\nTITLE: Re-executing Unit Tests after Pagination Implementation\nDESCRIPTION: This bash command runs the unit tests again using `poetry run pytest`. This step is intended to confirm that the changes made to implement pagination (`request_params` and `next_page_token` methods) pass the previously defined tests, including `test_read_multiple_pages`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Converting Pocket Request Token to Access Token with cURL Shell Command\nDESCRIPTION: Shell command to convert the previously obtained request token into a permanent access token using cURL. This final step in the OAuth flow requires both your consumer key and the request token from the previous step.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/pocket.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl --insecure -X POST -H 'Content-Type: application/json' -H 'X-Accept: application/json' \\\n    https://getpocket.com/v3/oauth/authorize  -d '{\"consumer_key\":\"REPLACE-ME\",\"code\":\"REQUEST-TOKEN\"}'\n```\n\n----------------------------------------\n\nTITLE: Example Transformed Record Data (Array to String) in JSON\nDESCRIPTION: Example illustrating how record data might be transformed by a destination when handling unsupported types like arrays. The original array of timestamps is JSON-serialized into a single string value, corresponding to the transformed schema where 'appointments' is a string.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/supported-data-types.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"appointments\": \"[\\\"2021-11-22T01:23:45+00:00\\\", \\\"2022-01-22T14:00:00+00:00\\\"]\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building the YouSign Source Connector Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-yousign` Airbyte connector. The resulting image will be tagged as `source-yousign:dev` and can be used for local testing and development. Requires `airbyte-ci` to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yousign/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yousign build\n```\n\n----------------------------------------\n\nTITLE: TRACE Message with Stream Status in Airbyte\nDESCRIPTION: A TRACE message that provides status information about a data stream. This example shows a STREAM_STATUS trace indicating that the 'array_test_1' stream has completed processing, along with the timestamp when the status was emitted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"array_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the Freshcaller connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshcaller/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Custom Build Process Implementation\nDESCRIPTION: Example Python module for customizing the connector build process with pre and post install hooks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from dagger import Container\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-monday/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-monday build\n```\n\n----------------------------------------\n\nTITLE: Configuring Debezium Heartbeat Action Query for PostgreSQL in Airbyte\nDESCRIPTION: Example SQL INSERT statement to be used in the Airbyte PostgreSQL source connector's 'Debezium heartbeat query' configuration field. This query is periodically executed by Airbyte to insert a record into the `airbyte_heartbeat` table, generating WAL events to advance the LSN for CDC.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO airbyte_heartbeat (text) VALUES ('heartbeat')\n```\n\n----------------------------------------\n\nTITLE: Using datetimeformat Filter in Jinja2\nDESCRIPTION: Demonstrates the `datetimeformat` filter in Jinja2 (requires Babel library), which formats a datetime object according to a specific format string (using strftime codes). The example assumes a variable `datetime` holds a datetime object and formats it as 'YYYY-MM-DD HH:MM:SS'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_58\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ datetime|datetimeformat('%Y-%m-%d %H:%M:%S') }}\n```\n\n----------------------------------------\n\nTITLE: Command not found error example\nDESCRIPTION: Example of the error shown when the airbyte-ci command is not found in the system PATH.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n$ airbyte-ci\nzsh: command not found: airbyte-ci\n```\n\n----------------------------------------\n\nTITLE: Testing Marketstack Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Marketstack connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-marketstack/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-marketstack test\n```\n\n----------------------------------------\n\nTITLE: Building the EZOfficeInventory Connector Using airbyte-ci\nDESCRIPTION: Command to build a development image of the EZOfficeInventory connector. This creates a Docker image named 'source-ezofficeinventory:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ezofficeinventory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ezofficeinventory build\n```\n\n----------------------------------------\n\nTITLE: Running Source-Pendo Connector Test Suite with airbyte-ci in Bash\nDESCRIPTION: This command runs the full CI test suite for the source-pendo connector using airbyte-ci. It requires airbyte-ci to be set up and available, and runs all defined tests for the indicated connector, facilitating local validation before submitting changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pendo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pendo test\n```\n\n----------------------------------------\n\nTITLE: Testing Cin7 Source Connector (Bash)\nDESCRIPTION: This command runs the standard Airbyte acceptance tests for the `source-cin7` connector using the `airbyte-ci` tool. It requires `airbyte-ci` to be installed and typically depends on a development image (e.g., `source-cin7:dev`) having been built previously using the build command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cin7/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cin7 test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for the Perigon Connector using Airbyte-CI (Bash)\nDESCRIPTION: This command executes the acceptance tests defined for the `source-perigon` connector using the `airbyte-ci` tool. This step verifies the connector's functionality against predefined test cases. Requires `airbyte-ci`, the connector source code, and potentially a built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-perigon/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-perigon test\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker Container\nDESCRIPTION: Commands to run the connector's various functions within a Docker container, including specification, checking, discovery, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-ads/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-linkedin-ads:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linkedin-ads:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-linkedin-ads:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-linkedin-ads:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuration Table in Markdown\nDESCRIPTION: Table showing configuration parameters for the NewsData.io connector including API key, date ranges, and filtering options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/newsdata-io.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start Date.  |  |\n| `end_date` | `string` | End Date.  |  |\n| `categories` | `string` | Categories to filter news.  |  |\n| `countries` | `string` | Countries to filter news.  |  |\n| `languages` | `string` | Language to filter news.  |  |\n| `domains` | `string` | Specific domains to filter news  |  |\n```\n\n----------------------------------------\n\nTITLE: JSON Stream Record with Array Data Types\nDESCRIPTION: Record message containing arrays of different numeric types including strings representing numbers, floats and integers. Includes metadata like stream name and timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/number_data_type_array_test_messages.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"array_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"array_number\" : [\"-12345.678\", \"100000000000000000.1234\"],\"array_float\" : [\"-12345.678\", \"0\", \"1000000000000000000000000000000000000000000000000000.1234\"], \"array_integer\" : [\"42\", \"0\", \"12345\"]}}}\n```\n\n----------------------------------------\n\nTITLE: Running Jina AI Reader Connector as Docker Container\nDESCRIPTION: Commands to run various connector operations using the Docker container, including spec, check, discover, and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-jina-ai-reader:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-jina-ai-reader:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-jina-ai-reader:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-jina-ai-reader:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Instatus Source Connector Commands\nDESCRIPTION: Standard commands for running the Instatus source connector as a Docker container. These commands allow for specifying connector configuration, checking connection, discovering schemas, and reading data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instatus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-instatus:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-instatus:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-instatus:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-instatus:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Defining Devices Stream Schema in JSON\nDESCRIPTION: JSON schema example for the devices stream in Google Analytics, including device-specific metrics and user engagement data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_deviceCategory\":\"desktop\",\"ga_operatingSystem\":\"Macintosh\",\"ga_browser\":\"Chrome\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Example OAuth Token Response with Nested Access Token Field - JSON\nDESCRIPTION: This JSON snippet demonstrates the response structure for an OAuth token endpoint where the access token is returned within a nested object under the data property. Used to guide manifest modifications for extracting nested values. No dependencies required.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": {\n    \"super_duper_access_token\": \"YOUR_ACCESS_TOKEN_123\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector Docker Commands\nDESCRIPTION: Commands to run the connector's Docker container for various operations including spec, check, discover and read.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-crm/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zoho-crm:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zoho-crm:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zoho-crm:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zoho-crm:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Airbyte DynamoDB Connector via Gradle - Bash\nDESCRIPTION: This snippet provides the Gradle command to build the DynamoDB source connector from the Airbyte repository root. It requires Gradle to be installed and configured on the user's environment. The command will compile and package the connector for local development and further use.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-dynamodb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-dynamodb:build\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Kyve Connector with Poetry\nDESCRIPTION: Command to add a new dependency to the Kyve connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-youtube-analytics/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-youtube-analytics build\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Invoice Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Zoho Invoice source connector using airbyte-ci. Creates a dev image tagged as 'source-zoho-invoice:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-invoice/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-invoice build\n```\n\n----------------------------------------\n\nTITLE: Running Todoist Source Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the Todoist connector in a Docker container. These commands demonstrate how to run spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-todoist/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-todoist:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-todoist:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-todoist:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-todoist:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Applying abctl Configuration with values.yaml in Bash\nDESCRIPTION: This Bash command demonstrates how to apply the configuration specified in a `values.yaml` file during an Airbyte deployment using `abctl`. The `abctl local install` command is used with the `--values` flag pointing to the configuration file, which typically includes settings like enabling unsafe code for custom components.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/custom-components.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n```bash\nabctl local install --values values.yaml\n```\n```\n\n----------------------------------------\n\nTITLE: Testing Solarwinds Service Desk Connector with airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the Solarwinds Service Desk connector using airbyte-ci. It helps ensure the connector is functioning correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-solarwinds-service-desk/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-solarwinds-service-desk test\n```\n\n----------------------------------------\n\nTITLE: Testing the Revolut Merchant Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the source-revolut-merchant connector using airbyte-ci. This validates that the connector functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-revolut-merchant/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-revolut-merchant test\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Twilio Taskrouter connector in a Docker container. Includes commands for spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio-taskrouter/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-twilio-taskrouter:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twilio-taskrouter:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-twilio-taskrouter:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-twilio-taskrouter:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the Onepagecrm Connector using Bash\nDESCRIPTION: This command executes the acceptance tests defined for the `source-onepagecrm` connector using the `airbyte-ci` tool. It verifies the connector's functionality against a predefined test suite. Requires `airbyte-ci` to be installed and likely depends on a successful build.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onepagecrm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onepagecrm test\n```\n\n----------------------------------------\n\nTITLE: Rendering Markdown Changelog Table\nDESCRIPTION: A markdown table documenting version history, dates, pull request references and descriptions of changes for the Weaviate destination connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/weaviate.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                               | Subject                                                                                                                                      |\n|:--------| :--------- | :--------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------- |\n| 0.2.58 | 2025-03-29 | [56089](https://github.com/airbytehq/airbyte/pull/56089) | Update dependencies |\n| 0.2.57 | 2025-03-08 | [55424](https://github.com/airbytehq/airbyte/pull/55424) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Surveymonkey Connector in Docker\nDESCRIPTION: Commands to run various connector operations using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-surveymonkey:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-surveymonkey:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-surveymonkey:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-surveymonkey:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Verifying Scheduled pg_cron Jobs in PostgreSQL\nDESCRIPTION: SQL query to list all scheduled jobs managed by the `pg_cron` extension. This is used to verify that the 'periodic_logger' job has been successfully scheduled and to inspect its details, such as the schedule expression and the command.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/postgres-troubleshooting.md#2025-04-23_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM cron.job;\n```\n\n----------------------------------------\n\nTITLE: Running Amplitude Connector Docker Commands\nDESCRIPTION: Standard commands for running the Amplitude source connector container with various operations including spec, check, discover, and read functions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amplitude/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-amplitude:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amplitude:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amplitude:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-amplitude:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Explaining Consequence of Saving Connection\nDESCRIPTION: This note clarifies that saving the connection after making changes (like refreshing schema and resetting streams) will trigger a data reset in the destination and start a completely new data synchronization process.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instagram-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nnote\n This will reset the data in your destination and initiate a fresh sync.\n \n```\n\n----------------------------------------\n\nTITLE: Implementing State Management in Python Stream Class\nDESCRIPTION: Example implementation of state getter and setter methods in a Python stream class to manage cursor state for incremental syncs. The state property helps track the current sync position using a cursor field.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/incremental-stream.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef state(self) -> Mapping[str, Any]:\n   return {self.cursor_field: str(self._cursor_value)}\n\n@state.setter\ndef state(self, value: Mapping[str, Any]):\n   self._cursor_value = value[self.cursor_field]\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Captain Data source connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-captain-data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-captain-data:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-captain-data:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-captain-data:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-captain-data:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Elasticsearch Connector Docker Commands\nDESCRIPTION: Commands to run various connector operations using the Docker image, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticsearch/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-elasticsearch:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-elasticsearch:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-elasticsearch:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-elasticsearch:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Stream Status Trace Message in Airbyte JSON Format\nDESCRIPTION: Example of an Airbyte TRACE message containing stream status information. This message indicates that the stream 'object_array_test_1' has completed synchronization with a COMPLETE status and includes a timestamp of when the trace was emitted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_object_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"object_array_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Installing airbyte-ci for Development\nDESCRIPTION: Instructions for installing airbyte-ci for development purposes. It requires Python > 3.10 and either Poetry or pipx. The installation results in an editable install, allowing for immediate code changes without reinstallation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### System requirements\n\n- `Python` > 3.10\n- [`Poetry`](https://python-poetry.org/) or [`pipx`](https://github.com/pypa/pipx)\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Table for Supported Features in Talkdesk Explore Connector\nDESCRIPTION: This markdown table shows the supported features of the Talkdesk Explore connector, including Full Refresh Sync, Incremental - Append Sync, Incremental - Dedupe Sync, and SSL connection.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/talkdesk-explore.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | Yes        |\n| Incremental - Dedupe Sync | No         |\n| SSL connection            | Yes        |\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Stream Caching in Python\nDESCRIPTION: Example showing how to implement caching for parent-child HTTP streams using the Airbyte CDK. The parent Employees stream enables caching while the EmployeeDetails stream inherits from HttpSubStream to access cached parent data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/cdk-python/http-streams.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Employees(HttpStream):\n    ...\n\n    @property\n    def use_cache(self) -> bool:\n        return True\n\nclass EmployeeDetails(HttpSubStream):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Incremental Sync Query using Range and Index in FQL\nDESCRIPTION: This FQL query retrieves documents that have been modified since the last sync. It uses a Range operation on an index that includes timestamps to efficiently fetch only updated documents.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/bootstrap.md#2025-04-23_snippet_1\n\nLANGUAGE: FQL\nCODE:\n```\nPaginate(Range(Match(Index(\"index-name\")), <last-sync-ts>, []))\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI - Bash\nDESCRIPTION: Builds a Docker image for the Azure-Table source connector using Airbyte-CI tooling, tagging the output as 'airbyte/source-azure-table:dev'. Requires 'airbyte-ci' to be installed, proper working directory, and Docker available on the host system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-table/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-azure-table build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Vantage source connector using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-vantage/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-vantage build\n```\n\n----------------------------------------\n\nTITLE: Building Tremendous Source Connector with Airbyte CI\nDESCRIPTION: This command builds a development image of the Tremendous source connector using airbyte-ci. The resulting image (source-tremendous:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tremendous/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tremendous build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Smartengage Source Connector\nDESCRIPTION: This command uses airbyte-ci to run the full test suite for the Smartengage source connector locally. It helps ensure that any changes made to the connector pass all tests before submission.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartengage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartengage test\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Pocket Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet builds the manifest-only Pocket source connector Docker image using the airbyte-ci CLI tool. It must be run in an environment where airbyte-ci is installed and accessible. The primary input is the connector's name; the output is a Docker image tagged as \"airbyte/source-pocket:dev\" and available locally. No additional parameters are required unless customizing the build. This command is intended for local development and testing before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pocket/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pocket build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Tplcentral Connector\nDESCRIPTION: Command to run the full test suite for the Tplcentral connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tplcentral/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tplcentral test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Klarna Source Connector\nDESCRIPTION: Command to run the full test suite for the Klarna source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klarna/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klarna test\n```\n\n----------------------------------------\n\nTITLE: Running Weaviate Connector Docker Commands\nDESCRIPTION: These commands demonstrate how to run the Weaviate connector Docker image for various operations like spec, check, and write.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-weaviate/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-weaviate:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-weaviate:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-weaviate:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running RSS Source Connector as Docker Container\nDESCRIPTION: Commands for running the RSS source connector operations in a Docker container, including mounting volumes for configuration and catalog files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-rss:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rss:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-rss:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-rss:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-amazon-ads:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-ads:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-amazon-ads:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-amazon-ads:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Requesting Access Token Endpoint for Airbyte Cloud (YAML)\nDESCRIPTION: Specifies the HTTP POST endpoint URL used to request an access token for Airbyte Cloud. This endpoint requires a JSON body containing the `client_id` and `client_secret` obtained from the Airbyte UI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/configuring-api-access.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nPOST https://api.airbyte.com/api/v1/applications/token\n```\n\n----------------------------------------\n\nTITLE: Creating Mermaid Diagrams in Markdown\nDESCRIPTION: Demonstrates how to create entity-relationship diagrams using Mermaid in Markdown. Mermaid diagrams are preferred for their maintainability, accessibility and ease of translation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n```mermaid\n---\ntitle: Order example\n---\nerDiagram\n   CUSTOMER ||--o{ ORDER : places\n   ORDER ||--|{ LINE-ITEM : contains\n   CUSTOMER }|..|{ DELIVERY-ADDRESS : uses\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies via Poetry - Bash\nDESCRIPTION: Adds a new Python package to the project's managed dependencies using Poetry. The <package-name> argument specifies the required package. Automatically updates pyproject.toml and poetry.lock files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: STATE Message for Global State Management in Airbyte\nDESCRIPTION: A STATE message example that maintains global synchronization state information. It contains a start_date parameter used to track where a subsequent sync should begin from.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_array_test_messages.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"STATE\", \"state\": { \"type\": \"GLOBAL\", \"global\": {\"shared_state\": {\"start_date\": \"2022-02-14\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Running Zoom Connector Docker Commands\nDESCRIPTION: Standard source connector commands for running the Zoom connector in Docker, including spec, check, discover, and read operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoom/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-zoom:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zoom:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-zoom:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-zoom:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Tuple Badge in Markdown\nDESCRIPTION: This code snippet shows how to display a custom badge for Tuple, a collaboration tool used by Airbyte. The badge includes a link to Tuple's website and uses Markdown image syntax with HTML attributes for styling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/THANK-YOU.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[![Tuple](https://img.shields.io/badge/Tuple%20❤️%20OSS-5A67D8?style=for-the-badge&logo=tuple)](https://tuple.app/github-badge)\n```\n\n----------------------------------------\n\nTITLE: Querying Document Deletion Events in FQL\nDESCRIPTION: This FQL query paginates through all events in the documents of a collection. It's used to track document deletions for incremental syncs, allowing the connector to report when documents have been deleted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/bootstrap.md#2025-04-23_snippet_2\n\nLANGUAGE: FQL\nCODE:\n```\nPaginate(Events(Documents(Collection(\"collection-name\"))))\n```\n\n----------------------------------------\n\nTITLE: Exchanging Public Token for Access Token in Plaid Sandbox\nDESCRIPTION: This curl command exchanges a previously generated public token for an access token in the Plaid sandbox environment. The access token is required for configuring the Plaid source connector in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/plaid.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://sandbox.plaid.com/item/public_token/exchange' \\\n  --header 'Content-Type: application/json;charset=UTF-16' \\\n  --data-raw '{\n      \"client_id\": \"<your-client-id>\",\n      \"secret\": \"<your-sandbox-api-key>\",\n      \"public_token\": \"<public-token-returned-by-previous-request>\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: OneSignal Data Type Mapping Example\nDESCRIPTION: Mapping of OneSignal data types to corresponding Airbyte types for data synchronization\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/onesignal.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Custom Build Process Configuration\nDESCRIPTION: Example Python module for customizing the connector build process with pre and post install hooks.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-astra/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment for Firebolt Source Connector\nDESCRIPTION: Creates a Python virtual environment in the .venv directory for local development of the Firebolt source connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Defining Standard 'For Airbyte Cloud' Setup Instructions in Markdown\nDESCRIPTION: This Markdown snippet provides the standard template for the 'For Airbyte Cloud:' section within Airbyte connector documentation. It outlines the numbered steps users should follow to configure the connector in the Airbyte Cloud UI, including logging in, selecting the source type, and naming the connector, using `CONNECTOR_NAME_FROM_METADATA` as a placeholder. This check ensures consistency in setup instructions across connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\n1. [Log into your Airbyte Cloud](https://cloud.airbyte.com/workspaces) account.\n2. Click Sources and then click + New source.\n3. On the Set up the source page, select CONNECTOR_NAME_FROM_METADATA from the Source type dropdown.\n4. Enter a name for the CONNECTOR_NAME_FROM_METADATA connector.\n\n\n```\n\n----------------------------------------\n\nTITLE: Using map Filter in Jinja2\nDESCRIPTION: Demonstrates the `map` filter in Jinja2, which applies another filter or extracts an attribute from each item in a sequence. The example extracts the 'name' attribute from a list of dictionaries.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_35\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [{'name': 'a'}, {'name': 'b'}]|map(attribute='name') }}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry (Python Bash Commands)\nDESCRIPTION: Installs all required dependencies for the Google-Directory source connector using Poetry, including development dependencies. This sets up the Python environment needed for local development. Poetry version ~1.7 is required, and Python ~3.9 is a prerequisite. The command should be run from the connector's root directory; no input parameters are needed, and it outputs installed packages into a Poetry-managed virtual environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-directory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input for RemoveFields Transformation - JSON\nDESCRIPTION: A sample JSON record with multiple nested fields, which serves as input for RemoveFields transformation demonstrations. Contains both removable and retained data keys. Utilized to show practical effects of field removal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"path\":\n  {\n    \"to\":\n    {\n      \"field1\": \"data_to_remove\",\n      \"field2\": \"data_to_keep\"\n    }\n  },\n  \"path2\": \"data_to_remove\",\n  \"path3\": \"data_to_keep\"\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Streams Table\nDESCRIPTION: Table defining available data streams with their properties including primary keys, pagination details, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/assemblyai.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| transcripts | id | DefaultPaginator | ✅ |  ✅  |\n| transcript_sentences | uuid | DefaultPaginator | ✅ |  ❌  |\n| paragraphs | uuid | DefaultPaginator | ✅ |  ❌  |\n| transcript_subtitle | uuid | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Referencing JsonToAvroSchemaConverter for Schema Conversion in Java\nDESCRIPTION: This snippet provides a link to the JsonToAvroSchemaConverter class, which is responsible for converting JSON schemas to Avro schemas in the Airbyte project. The converter is implemented in Java and is part of the base-java-s3 module.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nJsonToAvroSchemaConverter\n```\n\n----------------------------------------\n\nTITLE: Building Aviationstack Connector Dev Image using Bash\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image tagged as `source-aviationstack:dev`. This step is part of the local development setup and requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aviationstack/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aviationstack build\n```\n\n----------------------------------------\n\nTITLE: Using truncate Filter in Jinja2\nDESCRIPTION: Demonstrates the `truncate` filter in Jinja2, which shortens a string to a specified length, appending an indicator (default '...') if truncated. The example truncates 'hello world' to 5 characters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_50\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello world'|truncate(5) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Read Tests with Empty Streams in YAML\nDESCRIPTION: YAML configuration for basic read tests with empty streams and bypass reasons in high strictness level.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nconnector_image: airbyte/source-recharge:dev\ntest_strictness_level: high\nacceptance_tests:\n  basic_read:\n    tests:\n      - config_path: secrets/config.json\n        empty_streams:\n          - name: collections\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n          - name: discounts\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n        timeout_seconds: 1200\n```\n\n----------------------------------------\n\nTITLE: Displaying Note on Refresh Feature in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display a note about the alternative process for destinations that don't support the Refresh feature.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/stripe-migrations.md#2025-04-23_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n:::note\nIf you are using a destination that does not support the `Refresh` feature, you will need to [Clear](/operator-guides/clear) your stream. This will remove the data from the destination for just that stream. You will then need to sync the connection again in order to sync all data again for that stream.\n:::\n```\n\n----------------------------------------\n\nTITLE: Building the Finnworlds Source Connector Image (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image for the `source-finnworlds` connector. The resulting image will be tagged as `source-finnworlds:dev`. This requires `airbyte-ci` to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-finnworlds/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-finnworlds build\n```\n\n----------------------------------------\n\nTITLE: Building Opsgenie Source Connector Docker Image with airbyte-ci - Bash\nDESCRIPTION: This snippet demonstrates building the manifest-only Opsgenie source connector Docker image using the 'airbyte-ci' tool. It requires having 'airbyte-ci' installed and available in the environment. The command tags the resulting image as 'airbyte/source-opsgenie:dev' on the host system. No input parameters are required beyond the connector name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-opsgenie/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-opsgenie build\n```\n\n----------------------------------------\n\nTITLE: Building the Concord Source Connector Development Image using Bash\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-concord` connector. The resulting image will be tagged as `source-concord:dev` and can be used for testing purposes. This command requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-concord/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-concord build\n```\n\n----------------------------------------\n\nTITLE: Running Pipedrive Connector Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to run the full test suite for the Pipedrive source connector locally. It ensures the connector passes all defined tests before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pipedrive/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pipedrive test\n```\n\n----------------------------------------\n\nTITLE: Enabling CDC on Table Level\nDESCRIPTION: SQL commands to enable Change Data Capture for specific tables, including configuration for role permissions and filegroup storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mssql.md#2025-04-23_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nUSE {database name}\nGO\n\nEXEC sys.sp_cdc_enable_table\n@source_schema = N'{schema name}',\n@source_name   = N'{table name}',\n@role_name     = N'{role name}',\n@filegroup_name = N'{filegroup name}',\n@supports_net_changes = 0\nGO\n```\n\n----------------------------------------\n\nTITLE: Building Basecamp Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Basecamp source connector using airbyte-ci. Creates a dev image tagged as source-basecamp:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-basecamp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-basecamp build\n```\n\n----------------------------------------\n\nTITLE: Running Tests for RSS Source Connector\nDESCRIPTION: Command to run the test suite for the RSS source connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Building the Shippo Source Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Shippo source connector using airbyte-ci. This creates a dev image tagged as 'source-shippo:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shippo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shippo build\n```\n\n----------------------------------------\n\nTITLE: Using list Filter in Jinja2\nDESCRIPTION: Demonstrates the `list` filter in Jinja2, which converts an iterable (like a string) into a list of its items. The example converts the string 'abc' into `['a', 'b', 'c']`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_33\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'abc'|list }}\n```\n\n----------------------------------------\n\nTITLE: Workspace Update Response in JSON\nDESCRIPTION: The successful response from the workspace update API call, showing the updated workspace with its new name and data residency settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/multi-region.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"workspaceId\": \"uuid-string\",\n  \"name\": \"updated-workspace-name\",\n  \"dataResidency\": \"region-identifier\",\n  \"notifications\": {\n    \"failure\": {},\n    \"success\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Tests with Future State in YAML\nDESCRIPTION: YAML configuration for incremental tests with future state and missing streams in high strictness level.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ntest_strictness_level: high\nconnector_image: airbyte/source-my-connector:dev\nacceptance_tests:\n  incremental:\n    tests:\n      - config_path: secrets/config.json\n        configured_catalog_path: integration_tests/configured_catalog.json\n        future_state:\n          future_state_path: integration_tests/abnormal_state.json\n          missing_streams:\n            - name: my_missing_stream\n              bypass_reason: \"Please fill a good reason\"\n```\n\n----------------------------------------\n\nTITLE: Configuration Table in Markdown\nDESCRIPTION: Table showing configuration parameters required for the Zoho Campaign connector including client ID, client secret, refresh token and domain.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zoho-campaign.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id_2` | `string` | Client ID.  |  |\n| `client_secret_2` | `string` | Client secret.  |  |\n| `client_refresh_token` | `string` | Refresh token.  |  |\n| `domain` | `string` | Domain.  |  |\n```\n\n----------------------------------------\n\nTITLE: Displaying DynamoDB Destination Features Table in Markdown\nDESCRIPTION: A markdown table showing the supported features of the DynamoDB destination connector, including sync modes and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/dynamodb.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Support | Notes                                                                                   |\n| :----------------------------- | :-----: | :-------------------------------------------------------------------------------------- |\n| Full Refresh Sync              |   ✅    | Warning: this mode deletes all previously synced data in the configured DynamoDB table. |\n| Incremental - Append Sync      |   ✅    |                                                                                         |\n| Incremental - Append + Deduped |   ❌    |                                                                                         |\n| Namespaces                     |   ✅    | Namespace will be used as part of the table name.                                       |\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Fauna Database from Example Script\nDESCRIPTION: Command to initialize the Fauna database using the setup script. This evaluates the FQL commands in the setup_database.fql file to create the necessary database structure for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/examples/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfauna eval \"$(cat examples/setup_database.fql)\" --domain localhost --port 8443 --scheme http --secret secret\n```\n\n----------------------------------------\n\nTITLE: Supported Sync Modes Table\nDESCRIPTION: Table showing the supported synchronization modes for the Zenloop connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zenloop.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) |\n| :---------------- | :------------------- |\n| Full Refresh Sync | Yes                  |\n| Incremental Sync  | Yes                  |\n| Namespaces        | No                   |\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters for Google Blogger API\nDESCRIPTION: Table showing required configuration parameters for authenticating with the Google Blogger API including client ID, client secret, and refresh token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/blogger.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id` | `string` | Client ID.  |  |\n| `client_secret` | `string` | Client secret.  |  |\n| `client_refresh_token` | `string` | Refresh token.  |  |\n```\n\n----------------------------------------\n\nTITLE: Version Changelog Table in Markdown\nDESCRIPTION: Markdown table displaying version history, dates, pull request references and change descriptions for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-sharepoint-enterprise.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                           | Subject                                                                   |\n|:--------|:-----------|:-------------------------------------------------------|:--------------------------------------------------------------------------|\n| 0.1.0 | 2025-04-10 | [134](https://github.com/airbytehq/airbyte-enterprise/pull/134) | New source |\n```\n\n----------------------------------------\n\nTITLE: Asana Data Type Mapping Table in Markdown\nDESCRIPTION: Markdown table showing the mapping between Asana data types and their corresponding Airbyte types for data transformation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/asana.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type         | Airbyte Type |\n| :----------------------- | :----------- |\n| `string`                 | `string`     |\n| `int`, `float`, `number` | `number`     |\n| `date`                   | `date`       |\n| `datetime`               | `datetime`   |\n| `array`                  | `array`      |\n| `object`                 | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Setting Up Error-Case User in Snowflake\nDESCRIPTION: SQL script for creating a schema and granting privileges to test error cases in the Snowflake integration environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-snowflake/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ndrop schema if exists INTEGRATION_TEST_DESTINATION.TEXT_SCHEMA;\ncreate schema INTEGRATION_TEST_DESTINATION.TEXT_SCHEMA;\ngrant ownership on schema INTEGRATION_TEST_DESTINATION.TEXT_SCHEMA to role INTEGRATION_TESTER_DESTINATION revoke current grants;\ngrant all privileges on schema INTEGRATION_TEST_DESTINATION.TEXT_SCHEMA to role NO_ACTIVE_WAREHOUSE_ROLE;\n```\n\n----------------------------------------\n\nTITLE: Listing MQTT Destination Configuration Parameters in Markdown\nDESCRIPTION: Enumerates the configurable parameters for setting up the MQTT destination in Airbyte, including connection details and message properties.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mqtt.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- **MQTT broker host**\n- **MQTT broker port**\n- **Use TLS**\n- **Username**\n- **Password**\n- **Topic pattern**\n- **Test topic**\n- **Client ID**\n- **Sync publisher**\n- **Connect timeout**\n- **Automatic reconnect**\n- **Clean session**\n- **Message retained**\n- **Message QoS**\n```\n\n----------------------------------------\n\nTITLE: Building the Persistiq Connector Docker Image using airbyte-ci\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image specifically for the `source-persistiq` connector. It requires `airbyte-ci` to be installed beforehand. Upon successful execution, a Docker image named `airbyte/source-persistiq` with the tag `dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-persistiq/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-persistiq build\n```\n\n----------------------------------------\n\nTITLE: Building AssemblyAI Connector Using airbyte-ci\nDESCRIPTION: Command to build a development image of the AssemblyAI connector using airbyte-ci. Creates a dev image tagged as 'source-assemblyai:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-assemblyai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-assemblyai build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Hibob Source Connector\nDESCRIPTION: Command to run the full test suite for the Hibob source connector using airbyte-ci. This executes all tests associated with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hibob/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hibob test\n```\n\n----------------------------------------\n\nTITLE: Testing YNAB Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the YNAB source connector using airbyte-ci. Executes the standard test suite to verify connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-you-need-a-budget-ynab/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-you-need-a-budget-ynab test\n```\n\n----------------------------------------\n\nTITLE: Building the SignNow Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the SignNow connector using airbyte-ci. This creates a dev image named 'source-signnow:dev' that can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-signnow/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-signnow build\n```\n\n----------------------------------------\n\nTITLE: Running Custom Integration Tests using Pytest\nDESCRIPTION: This command executes custom integration tests located in the 'integration_tests' directory using the pytest framework. It should be run from the connector's root directory.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython -m pytest integration_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the WorkRamp connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workramp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workramp test\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkPost Connector in Markdown\nDESCRIPTION: Defines the configuration parameters for the SparkPost connector, including API key, start date, and API endpoint prefix.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sparkpost.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `start_date` | `string` | Start Date.  |  |\n| `api_prefix` | `string` | API Endpoint Prefix (`api` or `api.eu`)  | api |\n```\n\n----------------------------------------\n\nTITLE: Copying Specific Connector Version to Development Bucket\nDESCRIPTION: Command to copy a specific connector version from production to a development bucket for testing purposes. Requires gsutil to be installed and authenticated.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTARGET_BUCKET=<YOUR-DEV_BUCKET> CONNECTOR=\"airbyte/source-stripe\" VERSION=\"3.17.0-dev.ea013c8741\" poetry run poe copy-connector-from-prod\n```\n\n----------------------------------------\n\nTITLE: Running Amazon SQS Connector Commands\nDESCRIPTION: Basic commands for running the connector locally to perform specification, configuration checking, discovery, and data reading operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-sqs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-amazon-sqs spec\npoetry run source-amazon-sqs check --config secrets/config.json\npoetry run source-amazon-sqs discover --config secrets/config.json\npoetry run source-amazon-sqs read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Using center Filter in Jinja2\nDESCRIPTION: Demonstrates the `center` filter in Jinja2, which centers a string within a field of a given width by adding padding. The example centers 'hello' within a width of 9.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_18\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello'|center(9) }}\n```\n\n----------------------------------------\n\nTITLE: Building Jamf Pro Source Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Jamf Pro source connector using airbyte-ci. The resulting image is tagged as 'source-jamf-pro:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jamf-pro/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jamf-pro build\n```\n\n----------------------------------------\n\nTITLE: Younium Changelog Table\nDESCRIPTION: Detailed version history table showing updates and changes to the Younium connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/younium.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                    |\n| :------ | :--------- | :------------------------------------------------------- | :--------------------------------------------------------- |\n| 0.4.14 | 2025-04-05 | [56833](https://github.com/airbytehq/airbyte/pull/56833) | Update dependencies |\n| 0.4.13 | 2025-03-22 | [56327](https://github.com/airbytehq/airbyte/pull/56327) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Checking Network Policy for Specific Snowflake User\nDESCRIPTION: SQL command to check if a network policy is set for a specific user in Snowflake. This can help identify any user-specific network restrictions that might affect connectivity.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/snowflake.md#2025-04-23_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW PARAMETERS LIKE 'network_policy' IN USER <username>;\n```\n\n----------------------------------------\n\nTITLE: Connection Timeline Markdown Table - Data Metrics\nDESCRIPTION: A markdown table showing the different data metrics displayed in the Connection Timeline for sync and refresh events.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/cloud/managing-airbyte-cloud/review-connection-timeline.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Data                             | Description                                                    |\n| -------------------------------- | -------------------------------------------------------------- |\n| `x` GB (also measured in KB, MB) | Amount of data moved during the sync.                          |\n| `x` extracted records            | Number of records read from the source during the sync.        |\n| `x` loaded records               | Number of records the destination confirmed it received.        |\n| `xh xm xs`                       | Total time (hours, minutes, seconds) for the sync to complete. |\n```\n\n----------------------------------------\n\nTITLE: Running Gridly Connector Commands Locally\nDESCRIPTION: Series of commands to run various Gridly connector operations locally, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-gridly spec\npoetry run source-gridly check --config secrets/config.json\npoetry run source-gridly discover --config secrets/config.json\npoetry run source-gridly read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Self-hosted Sign-out Redirect URI Configuration\nDESCRIPTION: The sign-out redirect URI pattern required for self-hosted Airbyte SSO integration with Okta. Contains placeholders for domain and app integration name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso-providers/okta.md#2025-04-23_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n<your-airbyte-domain>/auth/realms/airbyte/broker/<app-integration-name>/endpoint/logout_response\n```\n\n----------------------------------------\n\nTITLE: Checking JSON Support Status in Yellowbrick (SQL)\nDESCRIPTION: This SQL command checks if the `enable_full_json` setting is currently enabled in the Yellowbrick database system. This is a prerequisite step before configuring the Airbyte Yellowbrick destination.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/yellowbrick.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW show enable_full_json;\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Output Schema in JSON\nDESCRIPTION: Describes the JSON structure of records written to Kafka topics, including Airbyte-specific fields and the main data payload.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/kafka.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_airbyte_ab_id\": \"uuid\",\n  \"_airbyte_emitted_at\": \"timestamp\",\n  \"_airbyte_data\": \"json_blob\",\n  \"_airbyte_stream\": \"stream_name\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands for running the connector operations within a Docker container\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-adjust/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-adjust:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-adjust:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-adjust:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-adjust:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Source-Nylas Connector Image with airbyte-ci - Bash\nDESCRIPTION: This snippet demonstrates how to build a development Docker image (source-nylas:dev) for the Nylas connector using the airbyte-ci CLI tool. The command uses the connectors subcommand and specifies the connector name for targeted building. Requires the airbyte-ci tool to be installed and configured, and should be run in a shell with access to the Airbyte repo and Docker.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nylas/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nylas build\n```\n\n----------------------------------------\n\nTITLE: Building Qonto Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Qonto source connector Docker image using airbyte-ci tool. This builds an image tagged as airbyte/source-qonto:dev locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-qonto/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-qonto build\n```\n\n----------------------------------------\n\nTITLE: Building the Docker Image for Recurly Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the source-recurly connector using the airbyte-ci tool. This will create a Docker image tagged as airbyte/source-recurly:dev on the local machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recurly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-recurly build\n```\n\n----------------------------------------\n\nTITLE: Building Box Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Box source connector using airbyte-ci. Creates a dev image tagged as source-box:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-box build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Square Source Connector\nDESCRIPTION: Command to build the docker image for the Square source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-square/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-square build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Visma Economic Source Connector\nDESCRIPTION: Command to build the docker image for the Visma Economic source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-visma-economic:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-visma-economic/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-visma-economic build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for RSS Source Connector\nDESCRIPTION: Command to build a Docker image for the RSS source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rss build\n```\n\n----------------------------------------\n\nTITLE: Building the Zendesk Talk Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build the Docker image for the `source-zendesk-talk` connector. It requires `airbyte-ci` to be installed. Upon successful execution, a Docker image tagged `airbyte/source-zendesk-talk:dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-talk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-talk build\n```\n\n----------------------------------------\n\nTITLE: Building Smartwaiver Connector for Airbyte\nDESCRIPTION: This command builds a development image of the Smartwaiver connector using airbyte-ci. The resulting image (source-smartwaiver:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-smartwaiver/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-smartwaiver build\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile Example\nDESCRIPTION: Example Dockerfile for building a custom version of the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/source-netsuite:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Displaying Rate Limit Retry Message in LinkedIn Pages Source\nDESCRIPTION: Shows an example log message indicating a retryable error (like HTTP 429 Rate Limit Exceeded) was encountered by the Airbyte LinkedIn Pages source. The connector will wait for a specified duration before attempting the request again.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-pages.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"Caught retryable error '<some_error> or null' after <some_number> tries. Waiting <some_number> seconds then retrying...\"\n```\n\n----------------------------------------\n\nTITLE: Reading JSON Stream Data from Airbyte\nDESCRIPTION: These JSON objects represent data streamed through Airbyte from a source named 'Sheet6-2000-rows'. Each record contains an ID field, a randomly generated Name field of 9 characters, and an emitted_at timestamp representing when the record was processed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"387\",\"Name\":\"zUKzFZaGB\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Stream Test SCD Drop Records in JSON\nDESCRIPTION: JSON records containing test data with different data types including dates, timestamps, datetime strings, and numeric values. Each record has a unique ID and emitted_at timestamp, demonstrating various datetime formats and number representations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_reset_scd_overwrite/data_input/test_scd_reset_messages_incremental.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637589000, \"data\": { \"id\": 1, \"date\": \"2022-08-29\", \"timestamp_col\": \"2020-08-29T00:00:00.000000-0000\", \"datetime_to_string\":\"2022-10-01T01:04:04-04:00\", \"string_to_dt\":\"2022-11-01T02:03:04-07:00\", \"number_to_int\": 1, \"int_to_number\": 10}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637689100, \"data\": { \"id\": 2, \"date\": \"2022-08-30\", \"timestamp_col\": \"2020-08-30T00:00:00.000-00\", \"datetime_to_string\":\"2022-10-02T01:04:04-04:00\", \"string_to_dt\":\"2022-11-02T03:04:05-07:00\", \"number_to_int\": 10, \"int_to_number\": 11}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637789200, \"data\": { \"id\": 3, \"date\": \"2022-08-31\", \"timestamp_col\": \"2020-08-31T00:00:00+00\", \"datetime_to_string\":\"2022-10-03T01:04:04-04:00\", \"string_to_dt\":\"2022-11-03T03:04:06-07:00\", \"number_to_int\": 11, \"int_to_number\": 12}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637889300, \"data\": { \"id\": 4, \"date\": \"2022-09-01\", \"timestamp_col\": \"2020-08-31T00:00:00+0000\", \"datetime_to_string\":\"2022-10-04T01:04:04-04:00\", \"string_to_dt\":\"2022-11-04T03:04:07-07:00\", \"number_to_int\": 111, \"int_to_number\": 133}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637989400, \"data\": { \"id\": 5, \"date\": \"2022-09-02\", \"timestamp_col\": \"2020-09-01T00:00:00Z\", \"datetime_to_string\":\"2022-10-05T01:04:04-04:00\", \"string_to_dt\":\"2022-11-05T03:04:08-12:00\", \"number_to_int\": 1010, \"int_to_number\": 1300}}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stream_test_scd_drop\", \"emitted_at\": 1602637989400, \"data\": { \"id\": 6, \"date\": \"2022-09-03\", \"timestamp_col\": \"2020-09-01T00:00:00Z\", \"datetime_to_string\":\"this is a string, not a datetime value\", \"string_to_dt\":\"2022-11-05T03:04:08-12:00\", \"number_to_int\": 1010, \"int_to_number\": 1300.25}}}\n```\n\n----------------------------------------\n\nTITLE: Defining Invoiceninja Data Streams in Markdown\nDESCRIPTION: Markdown table listing available data streams for the Invoiceninja connector. It includes stream names, primary keys, pagination details, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/invoiceninja.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| clients | id | DefaultPaginator | ✅ |  ❌  |\n| products | id | DefaultPaginator | ✅ |  ❌  |\n| invoices | id | DefaultPaginator | ✅ |  ❌  |\n| recurring invoices | id | DefaultPaginator | ✅ |  ❌  |\n| payments | id | DefaultPaginator | ✅ |  ❌  |\n| quotes | id | DefaultPaginator | ✅ |  ❌  |\n| credits | id | DefaultPaginator | ✅ |  ❌  |\n| projects | id | DefaultPaginator | ✅ |  ❌  |\n| tasks | id | DefaultPaginator | ✅ |  ❌  |\n| vendors | id | DefaultPaginator | ✅ |  ❌  |\n| purchase_orders | id | DefaultPaginator | ✅ |  ❌  |\n| expenses | id | DefaultPaginator | ✅ |  ❌  |\n| recurring expenses | id | DefaultPaginator | ✅ |  ❌  |\n| bank transactions | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-rabbitmq/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-rabbitmq:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-rabbitmq:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-rabbitmq:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-rabbitmq:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Reply.io Connector Commands in Docker\nDESCRIPTION: Commands to run the Reply.io source connector in Docker for different operations including spec, check, discover, and read. These commands use volume mounts to access local configuration and test files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-reply-io/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-reply-io:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-reply-io:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-reply-io:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-reply-io:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Langchain Connector Docker Image\nDESCRIPTION: Commands to run various operations of the Langchain connector using the Docker image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-langchain/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-langchain:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-langchain:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-langchain:dev write --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Hellobaton Source Connector Docker Commands\nDESCRIPTION: Set of Docker commands to run various operations for the Hellobaton source connector, including spec, check, discover, and read. These commands use the built Docker image and mount necessary volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hellobaton/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-hellobaton:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hellobaton:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-hellobaton:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-hellobaton:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Testing the SafetyCulture Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the SafetyCulture connector using airbyte-ci. This validates that the connector meets the required specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-safetyculture/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-safetyculture test\n```\n\n----------------------------------------\n\nTITLE: Testing Codefresh Connector using Airbyte CI (Bash)\nDESCRIPTION: This command runs the acceptance tests for the `source-codefresh` connector using the `airbyte-ci` tool. It requires `airbyte-ci` and expects the necessary testing setup (like configuration files or a built connector image) to be available. The command initiates the test suite defined for the `source-codefresh` connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-codefresh/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-codefresh test\n```\n\n----------------------------------------\n\nTITLE: Running Hellobaton Source Connector CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Hellobaton source connector using airbyte-ci. This ensures all changes pass the required tests before contribution.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hellobaton/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-hellobaton test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite locally using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-babelforce/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-babelforce test\n```\n\n----------------------------------------\n\nTITLE: Testing the YouSign Source Connector using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to execute the acceptance tests for the `source-yousign` Airbyte connector. This helps ensure the connector functions correctly according to Airbyte's standards. Requires `airbyte-ci` to be installed and may need specific test configurations or a built connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yousign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yousign test\n```\n\n----------------------------------------\n\nTITLE: Testing Drip Source Connector with Airbyte CI - Bash\nDESCRIPTION: This command invokes the airbyte-ci CLI tool to run the acceptance test suite for the source-drip connector. The airbyte-ci utility must be properly installed and configured before use. It requires the --name argument to target the correct connector, and executes a set of standardized tests to verify connector functionality. The output includes test results in the console, and the command is meant for local validation during connector development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-drip/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-drip test\n```\n\n----------------------------------------\n\nTITLE: Running Tests for the EZOfficeInventory Connector\nDESCRIPTION: Command to execute acceptance tests for the EZOfficeInventory connector. This validates that the connector works correctly according to Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ezofficeinventory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ezofficeinventory test\n```\n\n----------------------------------------\n\nTITLE: JSON Record with Complex Data Types - Second Example\nDESCRIPTION: Second record showing different variations of the same fields, including empty arrays and objects.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_messages_in.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"problematic_types\", \"emitted_at\": 1602637589200, \"data\": { \"schemaless_object\": { \"address\": { \"street\": \"113 Hickey Rd\", \"zip\": \"37932\" }, \"flags\": [ true, false, false ] }, \"schematized_object\": { \"id\": 2, \"name\": \"Jane\" }, \"combined_type\": 20, \"union_type\": \"string2\", \"schemaless_array\": [], \"mixed_array_integer_and_schemaless_object\": [ ], \"array_of_union_integer_and_schemaless_array\": [ ], \"union_of_objects_with_properties_identical\": { }, \"union_of_objects_with_properties_overlapping\": {}, \"union_of_objects_with_properties_nonoverlapping\": {}, \"union_of_objects_with_properties_contradicting\": { \"id\": \"seal-one-hippity\", \"name\": \"James\" }, \"empty_object\": {\"extra\": \"stuff\"}, \"object_with_null_properties\": { \"more\": { \"extra\": \"stuff\" } }, \"combined_with_null\": \"foobar2\", \"union_with_null\": \"barfoo2\", \"combined_nulls\": null, \"compact_union\": 4444 } } }\n```\n\n----------------------------------------\n\nTITLE: Installing Kyriba Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyriba/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Public-Apis Connector\nDESCRIPTION: Command to add a new dependency to the connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-public-apis/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Managing Dependencies with Poetry (Bash)\nDESCRIPTION: Demonstrates how to add a new dependency to the project's environment via Poetry. When executed, the specified <package-name> will be added to both pyproject.toml and poetry.lock, ensuring proper tracking and reproducibility of dependencies. Requires active directory to be the root of the connector project and Poetry to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Hubspot Connector\nDESCRIPTION: Command to add a new dependency to the Hubspot connector using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Connector in Docker\nDESCRIPTION: Commands to run the connector's various functions within a Docker container\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appsflyer/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-appsflyer:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-appsflyer:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-appsflyer:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-appsflyer:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the Appfollow connector in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appfollow/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-appfollow:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-appfollow:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-appfollow:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-appfollow:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard source connector commands for running the connector in a docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-merge/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-merge:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-merge:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-merge:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-merge:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Iterable Connector Commands in Docker\nDESCRIPTION: Demonstrates how to run various connector commands using the Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-iterable/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-iterable:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-iterable:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-iterable:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-iterable:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Gridly Connector in Docker\nDESCRIPTION: Series of commands to run various Gridly connector operations in a Docker container, including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-gridly:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gridly:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-gridly:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-gridly:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Fauna Database with Docker\nDESCRIPTION: Command to start a local Fauna database container for development and testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --name faunadb -p 8443:8443 fauna/faunadb\n```\n\n----------------------------------------\n\nTITLE: Installing Glassflow Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-glassflow/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to SFTP-Bulk Connector\nDESCRIPTION: Command to add new dependencies to the connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Deletion Event JSON Record Structure\nDESCRIPTION: JSON record structure for deletion events, containing stream identifier, emission timestamp, and data payload with reference ID, timestamp, and deletion information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/integration_tests/expected_deletions_records.txt#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"stream\": \"deletions-data\", \"emitted_at\": \"1\", \"data\": { \"ref\": \"338836293305763911\", \"ts\": 1659398359360000, \"deleted_at\": \"2022-08-01T23:59:19.360000\" } }\n{ \"stream\": \"deletions-data\", \"emitted_at\": \"2\", \"data\": { \"ref\": \"338836293305761863\", \"ts\": 1659398366330000, \"deleted_at\": \"2022-08-01T23:59:26.330000\" } }\n{ \"stream\": \"deletions-data\", \"emitted_at\": \"3\", \"data\": { \"ref\": \"338836293305765959\", \"ts\": 1659398371330000, \"deleted_at\": \"2022-08-01T23:59:31.330000\" } }\n{ \"stream\": \"deletions-data\", \"emitted_at\": \"5\", \"data\": { \"ref\": \"338836293305762887\", \"ts\": 1659398320430000, \"data\": { \"a\": 6, \"nested\": { \"value\": 20 } }, \"ttl\": null } }\n{ \"stream\": \"deletions-data\", \"emitted_at\": \"7\", \"data\": { \"ref\": \"338836293305764935\", \"ts\": 1659398320430000, \"data\": { \"a\": 8, \"nested\": { \"value\": 30 } }, \"ttl\": null } }\n```\n\n----------------------------------------\n\nTITLE: Handling Additional Properties in JSON to Avro Conversion\nDESCRIPTION: Example demonstrating how additional properties in JSON objects that aren't specified in the schema are dropped during conversion to Avro, as Avro requires strict schema adherence.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/json-avro-conversion.md#2025-04-23_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": [\"null\", \"string\"]\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"username\": \"admin\",\n  \"active\": true,\n  \"age\": 21,\n  \"auth\": {\n    \"auth_type\": \"ssl\",\n    \"api_key\": \"abcdefg/012345\",\n    \"admin\": false,\n    \"id\": 1000\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"username\": \"admin\"\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Network Policy for Snowflake Account\nDESCRIPTION: SQL command to check if a network policy is set on the Snowflake account. This is useful for troubleshooting connection issues, especially when using Airbyte Cloud.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/snowflake.md#2025-04-23_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW PARAMETERS LIKE 'network_policy' IN ACCOUNT;\n```\n\n----------------------------------------\n\nTITLE: Granting Database Permissions in Databend SQL\nDESCRIPTION: SQL command to grant CREATE permissions on all tables to a dedicated Airbyte user. This enables the user to create the necessary resources in Databend for data syncing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/databend.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGRANT CREATE ON * TO airbyte_user;\n```\n\n----------------------------------------\n\nTITLE: Running Railz connector CI test suite using airbyte-ci\nDESCRIPTION: This command runs the full test suite for the Railz connector using the airbyte-ci tool, which helps validate connector functionality before submitting code changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-railz/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-railz test\n```\n\n----------------------------------------\n\nTITLE: Running Rocket Chat Connector Test Suite\nDESCRIPTION: Command to run the complete test suite for the Rocket Chat source connector using airbyte-ci. This validates that the connector functions correctly before submitting changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rocket-chat/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rocket-chat test\n```\n\n----------------------------------------\n\nTITLE: Formatting JSON Object in Airbyte Data Stream\nDESCRIPTION: Example of a JSON object structure from an Airbyte data source, showing nested fields for user information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/gcs.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building K6 Cloud Source Connector Docker Image\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the K6 Cloud source connector. The resulting image will be tagged as 'airbyte/source-k6-cloud:dev' on the host machine.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-k6-cloud/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-k6-cloud build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, discovery, and data reading operations\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-yandex-metrica spec\npoetry run source-yandex-metrica check --config secrets/config.json\npoetry run source-yandex-metrica discover --config secrets/config.json\npoetry run source-yandex-metrica read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Iceberg V2 Connector with airbyte-ci CLI\nDESCRIPTION: Command to build the Iceberg V2 destination connector using the Airbyte CI CLI tool. This should be executed from the root of the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-s3-data-lake/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=destination-iceberg-v2 build\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile for Pinecone Connector\nDESCRIPTION: Example Dockerfile to create a custom build of the Pinecone connector based on the latest version of the connector image.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-pinecone:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n\n# The entrypoint and default env vars are already set in the base image\n# ENV AIRBYTE_ENTRYPOINT \"python /airbyte/integration_code/main.py\"\n# ENTRYPOINT [\"python\", \"/airbyte/integration_code/main.py\"]\n```\n\n----------------------------------------\n\nTITLE: OneSignal Changelog Format\nDESCRIPTION: Version history and changelog entries for the OneSignal connector showing updates and changes over time\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/onesignal.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                      |\n| :------ | :--------- | :------------------------------------------------------- | :------------------------------------------- |\n| 1.2.19 | 2025-04-19 | [58518](https://github.com/airbytehq/airbyte/pull/58518) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Pexels API Source Connector Docker Commands (Bash)\nDESCRIPTION: This set of commands demonstrates how to run the Airbyte source-pexels-api connector in a Docker container to view the spec, check configuration, discover available streams, and read from a catalog. They require the airbyte/source-pexels-api:dev image, a secrets directory mounted with config.json conforming to the manifest spec, and optionally an integration_tests directory. Parameters specify the config and catalog paths to use for each operation. Outputs vary by command and include connector specs, configuration validation, schema discovery, or data read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pexels-api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pexels-api:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pexels-api:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pexels-api:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pexels-api:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building the Statsig Source Connector using airbyte-ci\nDESCRIPTION: This command builds a development image of the Statsig source connector. The resulting image is tagged as 'source-statsig:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-statsig/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-statsig build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Waiteraid Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the Waiteraid source connector. The resulting image will be tagged as 'airbyte/source-waiteraid:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-waiteraid/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-waiteraid build\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running various connector operations locally including spec, check, discover, and read operations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bing-ads/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-bing-ads spec\npoetry run source-bing-ads check --config secrets/config.json\npoetry run source-bing-ads discover --config secrets/config.json\npoetry run source-bing-ads read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Kyve Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Kyve connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-kyve/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-kyve build\n```\n\n----------------------------------------\n\nTITLE: Changelog Table\nDESCRIPTION: Version history and changes made to the connector over time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appstore.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                           | Subject                                           |\n| :------ | :--------- | :----------------------------------------------------- | :------------------------------------------------ |\n| 0.2.6   | 2021-12-23 | [8434](https://github.com/airbytehq/airbyte/pull/8434) | Update fields in source-connectors specifications |\n| 0.2.5   | 2021-12-09 | [7757](https://github.com/airbytehq/airbyte/pull/7757) | Migrate to the CDK                                |\n| 0.2.4   | 2021-07-06 | [4539](https://github.com/airbytehq/airbyte/pull/4539) | Add `AIRBYTE_ENTRYPOINT` for Kubernetes support   |\n```\n\n----------------------------------------\n\nTITLE: Testing Source-100ms Connector with Airbyte-CI\nDESCRIPTION: Command to run acceptance tests for the source-100ms connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-100ms/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-100ms test\n```\n\n----------------------------------------\n\nTITLE: Example Directory Structure in Text Format\nDESCRIPTION: Demonstrates a sample folder hierarchy to illustrate path pattern matching scenarios\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-sharepoint-enterprise.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMyFolder\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Testing SpotlerCRM Source Connector in Bash\nDESCRIPTION: This command runs the acceptance tests for the SpotlerCRM source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-spotlercrm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-spotlercrm test\n```\n\n----------------------------------------\n\nTITLE: Building Nexiopay Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the Nexiopay source connector using airbyte-ci. Creates a dev image tagged as source-nexiopay:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nexiopay/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nexiopay build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Keka Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Keka source connector using airbyte-ci. It verifies the connector's functionality and compatibility with Airbyte's standards.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-keka/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-keka test\n```\n\n----------------------------------------\n\nTITLE: Testing Tinyemail Connector with Airbyte CI (Bash)\nDESCRIPTION: This command runs the acceptance tests for the Tinyemail connector using airbyte-ci. It verifies the functionality of the connector against predefined test cases.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tinyemail/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tinyemail test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Tavus Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Tavus source connector using airbyte-ci. It helps ensure that the connector meets the required standards and functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tavus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tavus test\n```\n\n----------------------------------------\n\nTITLE: Using upper Filter in Jinja2\nDESCRIPTION: Demonstrates the `upper` filter in Jinja2, which converts a string to all uppercase characters. The example converts 'hello' to 'HELLO'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_52\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ 'hello'|upper }}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Statuspage Source Connector\nDESCRIPTION: Command to run the full test suite for the Statuspage source connector using airbyte-ci. This is used to validate changes before publishing a new version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-statuspage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-statuspage test\n```\n\n----------------------------------------\n\nTITLE: Config Migration Script\nDESCRIPTION: Python command to migrate legacy acceptance-test-config.yml files to the latest configuration format.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource ./.venv/bin/activate\npython connector_acceptance_test/tools/strictness_level_migration/config_migration.py ../../connectors/source-to-migrate/acceptance-test-config.yml\n```\n\n----------------------------------------\n\nTITLE: Adding dependencies to Recharge connector\nDESCRIPTION: Command to add a new dependency to the connector using Poetry package manager, which will update pyproject.toml and poetry.lock files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Systeme Source Connector in Airbyte\nDESCRIPTION: This command builds a development image of the Systeme source connector using airbyte-ci. The resulting image (source-systeme:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-systeme/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-systeme build\n```\n\n----------------------------------------\n\nTITLE: Markdown Feature Support Table\nDESCRIPTION: Table showing supported features of the Omnisend connector, indicating full refresh sync support but no incremental sync capability.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/omnisend.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) |\n|:------------------|:---------------------|\n| Full Refresh Sync | Yes                  |\n| Incremental Sync  | No                   |\n```\n\n----------------------------------------\n\nTITLE: Testing the Scryfall connector with airbyte-ci in Bash\nDESCRIPTION: Command to run acceptance tests for the Scryfall connector using airbyte-ci. This validates that the connector functions correctly according to specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scryfall/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-scryfall test\n```\n\n----------------------------------------\n\nTITLE: Building Miro Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Miro source connector using airbyte-ci. Creates a dev image tagged as 'source-miro:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-miro/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-miro build\n```\n\n----------------------------------------\n\nTITLE: Installing Python Packages from Current Directory using pip\nDESCRIPTION: This command instructs pip to install Python packages from the current directory. It's commonly used in requirements files or setup scripts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-e .\n```\n\n----------------------------------------\n\nTITLE: Displaying Redpanda Record Structure in Markdown\nDESCRIPTION: Describes the structure of each record written to a Redpanda topic, including Airbyte-specific fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redpanda.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- `_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n- `_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n- `_airbyte_data`: a json blob representing with the event data.\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Instatus Source Connector\nDESCRIPTION: Command to run the full test suite for the Instatus source connector using airbyte-ci. This is used to verify changes and ensure compatibility.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-instatus/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-instatus test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Locally\nDESCRIPTION: Commands to run the connector locally for specification, configuration checking, discovery, and data reading.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Starburst Galaxy Connector with Gradle\nDESCRIPTION: Executes acceptance and custom integration tests for the Starburst Galaxy connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-starburst-galaxy/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-starburst-galaxy:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for SingleStore Connector via Gradle\nDESCRIPTION: Command to run unit tests for the SingleStore destination connector using Gradle. Requires setting the SINGLESTORE_LICENSE environment variable.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-singlestore/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-singlestore:check\n```\n\n----------------------------------------\n\nTITLE: Testing Appfigures Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Appfigures source connector using airbyte-ci tool. Validates connector functionality and integration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appfigures/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appfigures test\n```\n\n----------------------------------------\n\nTITLE: Setting replication identity for PostgreSQL tables in CDC\nDESCRIPTION: SQL command to set the replication identity for a table, which defines how rows are identified during replication.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/postgres/cloud-sql-postgres.md#2025-04-23_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE tbl1 REPLICA IDENTITY DEFAULT;\n```\n\n----------------------------------------\n\nTITLE: Running Connector Commands Locally\nDESCRIPTION: Commands for running the connector locally to test specification, configuration check, and data writing functionality\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run destination-timeplus spec\npoetry run destination-timeplus check --config secrets/config.json\ncat integration_tests/messages.jsonl | poetry run destination-timeplus write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Airbyte TRACE Message\nDESCRIPTION: Stream status trace message indicating completion of a specific stream, including stream descriptor and timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_array_object_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"object_array_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for LaunchDarkly Source Connector\nDESCRIPTION: Command to run the full test suite for the LaunchDarkly source connector using airbyte-ci. This is used to validate changes before publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-launchdarkly/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-launchdarkly test\n```\n\n----------------------------------------\n\nTITLE: Building the Encharge Source Connector in Airbyte\nDESCRIPTION: Command to build the Encharge source connector locally using airbyte-ci, which creates a dev image (source-encharge:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-encharge/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-encharge build\n```\n\n----------------------------------------\n\nTITLE: Building the Source Scaffold Java JDBC connector with Gradle\nDESCRIPTION: Command to build the connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scaffold-java-jdbc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-scaffold-java-jdbc:build\n```\n\n----------------------------------------\n\nTITLE: Building Bunny Inc Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Bunny Inc source connector using airbyte-ci. Creates a dev image tagged as source-bunny-inc:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bunny-inc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bunny-inc build\n```\n\n----------------------------------------\n\nTITLE: Parsing Sheet6-2000-rows Stream Data in JSON\nDESCRIPTION: This snippet shows the structure of a single JSON object representing a row in the 'Sheet6-2000-rows' stream. It includes the stream name, data with ID and Name fields, and an emitted_at timestamp.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_17\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"stream\": \"Sheet6-2000-rows\",\n  \"data\": {\n    \"ID\": \"1938\",\n    \"Name\": \"SwJVlfcyh\"\n  },\n  \"emitted_at\": 1673989570000\n}\n```\n\n----------------------------------------\n\nTITLE: Testing the FreshBooks Connector with airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the FreshBooks connector using airbyte-ci. This validates that the connector is working as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-freshbooks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-freshbooks test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Langchain Connector\nDESCRIPTION: Commands to build the Docker image for the Langchain connector using airbyte-ci or docker build.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-langchain/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-langchain build\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-langchain:dev .\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: Table showing the mapping between BigCommerce integration types and Airbyte data types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bigcommerce.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: Building Firebolt Connector Docker Image Manually\nDESCRIPTION: Alternative command to build the Docker image for the Firebolt source connector using the docker build command directly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/source-firebolt:dev .\n```\n\n----------------------------------------\n\nTITLE: Installing airbyte-ci in development mode\nDESCRIPTION: Example of an error when trying to install airbyte-ci in development mode with Python 3.10 not found.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\n$ pipx install --editable --force --python=python3.10 airbyte-ci/connectors/pipelines/\nError: Python 3.10 not found on your system.\n```\n\n----------------------------------------\n\nTITLE: Running Vectara Connector Commands Locally\nDESCRIPTION: Commands to run the connector's spec, check, and write operations locally using Python.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py write --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Connector Unit Tests\nDESCRIPTION: Command to run only the unit tests for the Qdrant connector using pytest. This is useful for quick checks during development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest -s unit_tests\n```\n\n----------------------------------------\n\nTITLE: Building the Rollbar Source Connector with airbyte-ci\nDESCRIPTION: Command to build the source-rollbar connector locally, creating a dev image that can be used for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rollbar/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rollbar build\n```\n\n----------------------------------------\n\nTITLE: Building BigMailer Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the BigMailer source connector using airbyte-ci tool. Creates a dev image tagged as source-bigmailer:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bigmailer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bigmailer build\n```\n\n----------------------------------------\n\nTITLE: Parameter Overwriting in YAML Components\nDESCRIPTION: Demonstrates how child components can override parameter values inherited from parent components for specialization purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/parameters.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    $parameters:\n      MyKey: YourValue\n    k2: v2\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Todoist Source Connector\nDESCRIPTION: Command to build the docker image for the Todoist source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-todoist:dev' on the host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-todoist/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-todoist build\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to activate the virtual environment and install required dependencies from requirements.txt and setup.py.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\npip install '.[tests]'\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Connector Integration Tests\nDESCRIPTION: Command to run the integration tests for the Qdrant connector using pytest. This requires a valid configuration file in secrets/config.json.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest -s integration_tests\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for SingleStore Connector\nDESCRIPTION: Gradle command to run acceptance and custom integration tests for the SingleStore connector from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-singlestore/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-singlestore:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Setting MySQL Server Timeouts\nDESCRIPTION: SQL commands to extend MySQL server timeout values to prevent EventDataDeserializationException errors during CDC initial snapshot.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/mysql/mysql-troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nset global slave_net_timeout = 120;\nset global thread_pool_idle_timeout = 120;\n```\n\n----------------------------------------\n\nTITLE: Running Connector Operations Locally\nDESCRIPTION: Commands to run the connector's main operations with a local configuration file.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Testrail Source Connector in Markdown\nDESCRIPTION: Defines the configuration parameters for the Testrail source connector, including username, password, start date, and domain name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/testrail.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `username` | `string` | Username.  |  |\n| `password` | `string` | Password.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `domain_name` | `string` | The unique domain name for accessing testrail.  |  |\n```\n\n----------------------------------------\n\nTITLE: Building Airbyte Manifest-Only Connector with airbyte-ci (Bash)\nDESCRIPTION: Builds the development Docker image for the source-pennylane manifest-only connector using the airbyte-ci CLI tool. Requires airbyte-ci to be installed and configured. The --name key specifies the connector, and 'build' is the operation, producing a local dev image (source-pennylane:dev). Input: None; Output: Built Docker image tagged as source-pennylane:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pennylane/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-pennylane build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci - Bash\nDESCRIPTION: This snippet demonstrates how to build the Docker image for the Openweather manifest-only connector using the airbyte-ci CLI tool. It expects airbyte-ci to be installed and invoked from within the project directory. The built image will be tagged as airbyte/source-openweather:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-openweather/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-openweather build\n\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Kubeadm Init Failure\nDESCRIPTION: Error message when the kubeadm initialization fails during Kind cluster creation, showing the command that failed with exit status 1.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n unable to create kind cluster: failed to init node with kubeadm:\n command \"docker exec --privileged airbyte-abctl-control-plane kubeadm init\n  --skip-phases=preflight --config=/kind/kubeadm.conf --skip-token-print --v=6\"\n  failed with error: exit status 1\n```\n\n----------------------------------------\n\nTITLE: Writing Credentials for Specific Connector\nDESCRIPTION: Command to download GSM secrets for a specific connector to local storage.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=dev ci_credentials source-bing-ads write-to-storage\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies\nDESCRIPTION: Activates the Python virtual environment and installs the required dependencies from requirements.txt for the Firebolt source connector development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-firebolt/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing abctl using Homebrew\nDESCRIPTION: Command to install abctl, a tool for managing Airbyte installations, using Homebrew package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/enterprise-setup/implementation-guide.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbrew tap airbytehq/tap\nbrew install abctl\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Teradata Source Connector in Java\nDESCRIPTION: This Gradle command runs unit tests for the Teradata source connector from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teradata/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-teradata:unitTest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Yotpo connector docker image using airbyte-ci tool. Creates an image tagged as airbyte/source-yotpo:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yotpo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yotpo build\n```\n\n----------------------------------------\n\nTITLE: Rendering Documentation Card List in MDX\nDESCRIPTION: MDX code block for rendering a list of documentation cards using the DocCardList component.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/access-management/sso.md#2025-04-23_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Aha source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-aha/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-aha build\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Books Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Zoho Books connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-books/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-books test\n```\n\n----------------------------------------\n\nTITLE: Starting Airbyte Webapp\nDESCRIPTION: Commands to install dependencies and start the Airbyte React webapp using pnpm.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Running integration tests for Source Scaffold Java JDBC connector\nDESCRIPTION: Gradle command to run acceptance and custom integration tests for the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-scaffold-java-jdbc/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-scaffold-java-jdbc:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Apify dataset connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-apify-dataset/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-apify-dataset build\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Locally - Bash\nDESCRIPTION: This snippet demonstrates how to invoke the connector's spec, check, discover, and read commands using Poetry. Required files like secrets/config.json and sample_files/configured_catalog.json must be present. These commands help test and utilize the connector locally before deploying or containerizing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-azure-blob-storage/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run source-azure-blob-storage spec\npoetry run source-azure-blob-storage check --config secrets/config.json\npoetry run source-azure-blob-storage discover --config secrets/config.json\npoetry run source-azure-blob-storage read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running DynamoDB Unit Tests via Gradle\nDESCRIPTION: Command to execute unit tests for the DynamoDB connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dynamodb/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dynamodb:unitTest\n```\n\n----------------------------------------\n\nTITLE: Installing Convex Connector Dependencies with Poetry (Bash)\nDESCRIPTION: Installs the required Python packages for the Convex source connector, including development dependencies, using Poetry. Run this command from the connector's root directory after installing Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-yandex-metrica build\n```\n\n----------------------------------------\n\nTITLE: Running QA Checks on Specific Connectors\nDESCRIPTION: Command to run QA checks on specified connectors (source-faker and source-google-sheets) from the Airbyte repo root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa run --name=source-faker --name=source-google-sheets\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Teradata Connector with Gradle\nDESCRIPTION: Executes unit tests for the Teradata destination connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-teradata/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-teradata:unitTest\n```\n\n----------------------------------------\n\nTITLE: Installing SFTP-JSON Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-sftp-json/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing the SFTP-Bulk Connector with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sftp-bulk/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Airbyte with Custom Values\nDESCRIPTION: Command to install Airbyte locally with custom configuration values on port 8001.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nabctl local install --port 8001 --values ./values.yaml\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Jina AI Reader Connector\nDESCRIPTION: Command to run the full CI test suite for the Jina AI Reader connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jina-ai-reader test\n```\n\n----------------------------------------\n\nTITLE: Testing WordPress Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the WordPress source connector using airbyte-ci to verify functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-wordpress/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-wordpress test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the Twilio Taskrouter source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio-taskrouter/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twilio-taskrouter build\n```\n\n----------------------------------------\n\nTITLE: Download Kestra Docker Compose File\nDESCRIPTION: Command to download the Kestra Docker Compose configuration file needed for setup.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/using-kestra-plugin.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -o docker-compose.yml https://raw.githubusercontent.com/kestra-io/kestra/develop/docker-compose.yml\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Data Structure\nDESCRIPTION: A simple CSV file with two identical rows, each containing three comma-separated values represented as 'a,b,c'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-s3/integration_tests/sample_files/file_to_skip.txt#2025-04-23_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\na,b,c\na,b,c\n```\n\n----------------------------------------\n\nTITLE: SQS Message Body with Specified Key\nDESCRIPTION: Shows the message body output when message_body_key is set to 'parent_key'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/bootstrap.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"nested_key\": \"nested_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Dev Null Connector\nDESCRIPTION: Gradle command to execute integration and acceptance tests for the Dev Null connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dev-null/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dev-null:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Testing the Easypromos Source Connector with airbyte-ci\nDESCRIPTION: Command to run the acceptance tests for the Easypromos source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-easypromos/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-easypromos test\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency with Poetry\nDESCRIPTION: This command demonstrates how to add a new dependency to the connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Setting SurveyMonkey API Base URL in Python\nDESCRIPTION: This snippet sets the base URL for the SurveyMonkey API.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nurl_base = \"https://api.surveymonkey.com\"\n```\n\n----------------------------------------\n\nTITLE: Building Yellowbrick Connector with Gradle\nDESCRIPTION: Command to build the Yellowbrick destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-yellowbrick/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-yellowbrick:build\n```\n\n----------------------------------------\n\nTITLE: Building Elasticsearch Connector via Gradle\nDESCRIPTION: Command to build the Elasticsearch source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-elasticsearch/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-elasticsearch:build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Stripe Connector\nDESCRIPTION: This command runs the unit tests for the Stripe connector using pytest within the Poetry environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Installing the Connector with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Building Elasticsearch Connector with Gradle\nDESCRIPTION: Command to build the Elasticsearch destination connector using Gradle from the Airbyte repository root\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-elasticsearch/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-elasticsearch:build\n```\n\n----------------------------------------\n\nTITLE: Markdown Feature Support Table\nDESCRIPTION: Table showing supported sync features for My Hours integration\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/my-hours.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Installing the Shopify connector with Poetry\nDESCRIPTION: Command to install the Shopify connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for RSS Source Connector\nDESCRIPTION: Command to run the full CI test suite for the RSS source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rss/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rss test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-facebook-marketing/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-facebook-marketing test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the Alpha Vantage connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-alpha-vantage/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-alpha-vantage test\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Genesys Connector with Poetry\nDESCRIPTION: Command to add a new dependency to the Genesys connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-genesys/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Building Flexport Source Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Flexport source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flexport/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flexport build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Stripe Connector\nDESCRIPTION: This command builds a Docker image for the Stripe connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-stripe build\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to execute the full test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-us-census/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-us-census test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the My Hours source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-my-hours/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-my-hours build\n```\n\n----------------------------------------\n\nTITLE: Testing Cal.com Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Cal.com connector using Airbyte CI tooling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cal-com/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cal-com test\n```\n\n----------------------------------------\n\nTITLE: Feature Support Table (Markdown)\nDESCRIPTION: Markdown table showing supported features of the Newsdata API integration\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/newsdata.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? | Notes |\n| :---------------- | ---------- | :---- |\n| Full Refresh Sync | Yes        |       |\n| Incremental Sync  | No         |       |\n```\n\n----------------------------------------\n\nTITLE: Building MSSQL Connector with Gradle\nDESCRIPTION: Command to build the MSSQL destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-mssql-v2:build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Breezometer source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-breezometer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-breezometer build\n```\n\n----------------------------------------\n\nTITLE: Documentation of Firebolt Source Connector\nDESCRIPTION: Technical documentation detailing the Firebolt data warehouse connector implementation, including its architecture, data writing strategies, and core dependencies like firebolt-sdk and pyarrow. The connector supports both SQL-based and S3-based data writing approaches.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firebolt/bootstrap.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Firebolt Source\n\n## Overview\n\nFirebolt is a cloud data warehouse purpose-built to provide sub-second analytics performance on massive, terabyte-scale data sets.\n\nFirebolt has two main concepts: Databases, which denote the storage of data and Engines, which describe the compute layer on top of a Database.\n\nFirebolt has three types of tables: External, Fact and Dimension. External tables, which represent a raw file structure in storage. Dimension tables, which are optimised for fetching and store data on each node in an Engine. Fact tables are similar to Dimension, but they shard the data across the nodes. The usual workload is to write source data into a set of files on S3, wrap them with an External table and write this data to a fetch-optimised Fact or Dimension table.\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Running DynamoDB Integration Tests via Gradle\nDESCRIPTION: Command to execute acceptance and custom integration tests for the DynamoDB connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dynamodb/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dynamodb:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry (Bash)\nDESCRIPTION: Installs the Python dependencies defined in the project's `pyproject.toml` file using the Poetry package manager. This command should be run from the connector's root directory. Requires Poetry (~=1.7) to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Changelog Version History in Markdown\nDESCRIPTION: Details the version history and changes made to the Zendesk Chat connector over time\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-chat.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                                                                                                                |\n|:--------|:-----------|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.2.4 | 2025-04-19 | [58028](https://github.com/airbytehq/airbyte/pull/58028) | Update dependencies |\n| 1.2.3 | 2025-04-05 | [57404](https://github.com/airbytehq/airbyte/pull/57404) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Testing AgileCRM Connector Using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the AgileCRM connector using the airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-agilecrm/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-agilecrm test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Workable source connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-workable/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-workable build\n```\n\n----------------------------------------\n\nTITLE: Running unit tests for Shopify connector\nDESCRIPTION: Command to execute unit tests for the Shopify connector using pytest via Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mailchimp/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-mailchimp test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-twilio/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-twilio test\n```\n\n----------------------------------------\n\nTITLE: Testing the Sendowl Source Connector\nDESCRIPTION: Command to run acceptance tests for the Sendowl source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendowl/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendowl test\n```\n\n----------------------------------------\n\nTITLE: Displaying Shopify Rate Limit Error Message in Markdown\nDESCRIPTION: Shows an example error message that may be encountered when hitting Shopify's rate limits during data synchronization. This message indicates a retryable error and the sync operation will continue after a backoff period.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/shopify.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```text\n\"Caught retryable error '<some_error> or null' after <some_number> tries.\nWaiting <some_number> seconds then retrying...\"\n```\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest through Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-amazon-ads/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Forcefully reinstalling airbyte-ci\nDESCRIPTION: Command to forcefully reinstall the pipelines package when troubleshooting issues.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\n$ pipx reinstall pipelines --force\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Square Source Connector\nDESCRIPTION: Command to run the full test suite for the Square source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-square/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-square test\n```\n\n----------------------------------------\n\nTITLE: Building Firestore Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-firestore build\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Surveymonkey Connector\nDESCRIPTION: Command to run unit tests for the Surveymonkey connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Example for Vault Configuration\nDESCRIPTION: Example showing key-value format for vault configuration path prefix\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-airbyte.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nkv/airbyte/ or secret/airbyte/\n```\n\n----------------------------------------\n\nTITLE: Testing the SimpleCast Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the SimpleCast connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-simplecast/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-simplecast test\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-databend/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: AWS Secret Manager Tags Format\nDESCRIPTION: Format for specifying AWS Secret Manager tags using key-value pairs\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-airbyte.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nkey1=value1,key2=value2\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-timeplus/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Running Firestore Connector in Docker\nDESCRIPTION: Commands for running the connector operations in a Docker container with volume mounts for secrets and tests\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-firestore/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/destination-firestore:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-firestore:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-firestore:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/destination-firestore:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Zendesk Sell connector docker image using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-sell/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zendesk-sell build\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Guru Source Connector\nDESCRIPTION: This command executes the acceptance tests for the Guru source connector using airbyte-ci. It helps ensure the connector meets the required standards and functions correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-guru/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-guru test\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests Locally with Poetry (Bash)\nDESCRIPTION: Executes the unit tests located in the `unit_tests` directory using `pytest` via `poetry run`. Requires development dependencies, including `pytest`, to be installed via `poetry install`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-declarative-manifest/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linnworks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Listing Affected Analytics Streams for LinkedIn Ads v1.0.0 Primary Key Change (Text)\nDESCRIPTION: Enumerates the analytics streams impacted by primary key modifications in version 1.0.0 of the LinkedIn Ads connector. Users upgrading to this version needed to address these changes by clearing and refreshing data for these specific streams.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n- \"ad_campaign_analytics\"\n- \"ad_creative_analytics\"\n- \"ad_impression_device_analytics\"\n- \"ad_member_company_size_analytics\"\n- \"ad_member_country_analytics\"\n- \"ad_member_job_function_analytics\"\n- \"ad_member_job_title_analytics\"\n- \"ad_member_industry_analytics\"\n- \"ad_member_seniority_analytics\"\n- \"ad_member_region_analytics\"\n- \"ad_member_company_analytics\"\n```\n\n----------------------------------------\n\nTITLE: Database URL Configuration Format\nDESCRIPTION: JDBC URL format for configuring database connections without credentials\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/configuring-airbyte.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\njdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_DB}\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile Example\nDESCRIPTION: Example Dockerfile for custom builds of the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM airbyte/destination-typesense:latest\n\nCOPY . ./airbyte/integration_code\nRUN pip install ./airbyte/integration_code\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-cumulio/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Sheet6-2000-rows JSON Stream Records\nDESCRIPTION: JSON stream data containing records with ID and Name fields from a spreadsheet source. Each record has a stream identifier, data payload with ID and Name fields, and an emitted_at timestamp representing when the record was processed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"1114\",\"Name\":\"UToweBKOk\"},\"emitted_at\":1673989569000}\n```\n\n----------------------------------------\n\nTITLE: Building and Tagging ClickHouse with SSL Docker Image - Bash\nDESCRIPTION: This bash command builds a Docker image for ClickHouse with SSL support, tagging it with a custom user and dev label. It should be run from the 'tools\\integration-tests-ssl' directory where the 'Clickhouse.Dockerfile' is present. The resulting image is required to execute integration tests that rely on secure SSL connections within the Airbyte project; no additional dependencies are required beyond Docker being installed and available in the execution environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-clickhouse/ReadMe.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t your_user/clickhouse-with-ssl:dev -f Clickhouse.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Retrieving Connector Sandbox Secrets\nDESCRIPTION: Command to write connector secrets to the appropriate storage location for testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/connector-acceptance-test/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=dev ci_credentials connectors/source-faker write-to-storage\n```\n\n----------------------------------------\n\nTITLE: Selected Records Output from Field Extraction - JSON\nDESCRIPTION: An array of JSON records illustrating the result after extracting elements from a nested array (e.g., from 'data'). Used to show the output after applying a field-based extractor in Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 0\n  },\n  {\n    \"id\": 1\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image\nDESCRIPTION: Command to build the connector's Docker image using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zenloop build\n```\n\n----------------------------------------\n\nTITLE: Building Oncehub Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet utilizes the airbyte-ci command-line tool to build the source-oncehub connector. It generates a development Docker image (source-oncehub:dev) locally for testing and further modification. Requires airbyte-ci to be installed and run in an environment with proper permissions; key parameter is --name, specifying the connector to build. The output is a Docker image that facilitates local connector testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oncehub/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-oncehub build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the Bigcommerce source connector docker image using airbyte-ci tool\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bigcommerce/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bigcommerce build\n```\n\n----------------------------------------\n\nTITLE: Building Eventbrite Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image (source-eventbrite:dev) for local testing of the Eventbrite connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-eventbrite/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-eventbrite build\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image using airbyte-ci (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build the Docker image for the `source-customer-io` connector. It requires `airbyte-ci` to be installed. Upon successful execution, a Docker image tagged `airbyte/source-customer-io:dev` will be available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-customer-io/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-customer-io build\n```\n\n----------------------------------------\n\nTITLE: Building NinjaOne RMM Connector with airbyte-ci\nDESCRIPTION: Command to build a development image of the NinjaOne RMM connector using airbyte-ci. Creates a dev image tagged as 'source-ninjaone-rmm:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-ninjaone-rmm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-ninjaone-rmm build\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte CI\nDESCRIPTION: Command to build the connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-netsuite/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name source-netsuite build\n```\n\n----------------------------------------\n\nTITLE: Building Cloudbeds Source Connector Dev Image (Bash)\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image for the `source-cloudbeds` connector. The resulting image will be tagged as `source-cloudbeds:dev`, allowing for local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cloudbeds/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cloudbeds build\n```\n\n----------------------------------------\n\nTITLE: Building Teamwork Connector Image for Airbyte\nDESCRIPTION: This command builds a development image for the Teamwork connector using airbyte-ci. The resulting image is tagged as 'source-teamwork:dev' and can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teamwork/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-teamwork build\n```\n\n----------------------------------------\n\nTITLE: Building the CIMIS Connector Development Image using Bash\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a local development Docker image tagged `source-cimis:dev` for the CIMIS connector. This image is essential for local testing and development workflows. It requires `airbyte-ci` to be installed and properly configured in the development environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cimis/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-cimis build\n```\n\n----------------------------------------\n\nTITLE: Building the Repairshopr Connector with airbyte-ci\nDESCRIPTION: This command builds a development image (source-repairshopr:dev) for local testing of the connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-repairshopr/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-repairshopr build\n```\n\n----------------------------------------\n\nTITLE: Building the Fleetio Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the Docker image for the Fleetio source connector using airbyte-ci tool. The build process creates an image tagged as airbyte/source-fleetio:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fleetio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-fleetio build\n```\n\n----------------------------------------\n\nTITLE: Displaying Rockset Connector Features Table in Markdown\nDESCRIPTION: A markdown table showing the supported features of the Rockset connector, including full refresh sync, incremental append sync, and namespace support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/rockset.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                        | Support |\n| :----------------------------- | :-----: |\n| Full Refresh Sync              |   ✅    |\n| Incremental - Append Sync      |   ✅    |\n| Incremental - Append + Deduped |   ❌    |\n| Namespaces                     |   ❌    |\n```\n\n----------------------------------------\n\nTITLE: Testing the Giphy Source Connector in Bash\nDESCRIPTION: Command to run acceptance tests for the Giphy source connector using airbyte-ci. This validates that the connector works as expected.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-giphy/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-giphy test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the WooCommerce connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-woocommerce/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-woocommerce test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Google Sheets Connector\nDESCRIPTION: Command to run the full CI test suite for the Google Sheets connector using Airbyte's CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-google-sheets/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-google-sheets test\n```\n\n----------------------------------------\n\nTITLE: Running Full CI Test Suite with airbyte-ci (Bash)\nDESCRIPTION: Reviews the command to run the complete CI test suite for the connector locally using airbyte-ci. This encompasses all automated connector tests as defined in Airbyte's pipelines, helping maintain code quality before changes are pushed or released. Assumes airbyte-ci is installed and properly set up.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-v4/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-v4 test\n```\n\n----------------------------------------\n\nTITLE: Testing Bluetally Source Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the Bluetally source connector using Airbyte CI tooling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bluetally/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bluetally test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for TMDB Source Connector\nDESCRIPTION: Demonstrates how to run the full test suite for the TMDB source connector using airbyte-ci. This command executes all tests associated with the connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-tmdb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-tmdb test\n```\n\n----------------------------------------\n\nTITLE: Running Test Suite for Langchain Connector\nDESCRIPTION: Command to run the full test suite for the Langchain connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-langchain/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-langchain test\n```\n\n----------------------------------------\n\nTITLE: Running the Connector Test Suite with airbyte-ci - bash\nDESCRIPTION: This snippet shows how to execute all automated tests for the Outbrain Amplify connector locally using the airbyte-ci tool. It requires proper environment setup and dependency installation. All connector unit and integration tests are invoked, validating changes prior to publishing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-outbrain-amplify/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-outbrain-amplify test\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: JSON Data Structure Example in Firebase Realtime Database\nDESCRIPTION: Example JSON structure showing how data is stored in Firebase Realtime Database. This demonstrates a nested object with user data containing address and age fields.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/firebase-realtime-database.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"liam\": { \"address\": \"somewhere\", \"age\": 24 },\n  \"olivia\": { \"address\": \"somewhere\", \"age\": 30 }\n}\n```\n\n----------------------------------------\n\nTITLE: Running DB2 Connector Integration Tests with Gradle (Shell)\nDESCRIPTION: This shell command utilizes the Gradle wrapper (`gradlew`) to execute the integration tests specifically designed for the Airbyte IBM DB2 source connector. It assumes Gradle is installed and the command is run from the root of the Airbyte project directory, targeting the `integrationTest` task within the `:airbyte-integrations:connectors:db2` module.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-db2/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:db2:integrationTest\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Starburst Galaxy Connector with Gradle\nDESCRIPTION: Executes unit tests for the Starburst Galaxy connector using Gradle from the Airbyte project root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-starburst-galaxy/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-starburst-galaxy:unitTest\n```\n\n----------------------------------------\n\nTITLE: Running Standard Airbyte Source Connector Commands with Docker (Bash)\nDESCRIPTION: This snippet includes several Bash commands to run Airbyte source connector operations (spec, check, discover, read) as Docker containers. These commands assume the source-pokeapi image is built locally as airbyte/source-pokeapi:dev, and that configuration and secret files exist at the specified paths. Volumes are mounted as needed for credentials and test catalogs, and each command executes a standard connector operation. Outputs correspond to Airbyte's CLI expectations.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-pokeapi/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-pokeapi:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pokeapi:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-pokeapi:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-pokeapi:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Running Auth0 Connector Test Suite\nDESCRIPTION: Command to execute the full test suite for the Auth0 connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-auth0/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-auth0 test\n```\n\n----------------------------------------\n\nTITLE: Building DynamoDB Connector via Gradle\nDESCRIPTION: Command to build the DynamoDB destination connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-dynamodb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-dynamodb:build\n```\n\n----------------------------------------\n\nTITLE: Installing Stripe Connector Dependencies with Poetry\nDESCRIPTION: This command installs the necessary dependencies for the Stripe connector using Poetry, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Checking Documentation Links with Link Checker\nDESCRIPTION: Command to check for broken links in the documentation. This runs the link checker specifically against the documentation files.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/site/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./tools/site/link_checker.sh check_docs\n```\n\n----------------------------------------\n\nTITLE: Creating Integration Test Directory and File using Bash\nDESCRIPTION: Creates the necessary directory structure (`unit_tests/integration`) and an empty Python file (`test_surveys.py`) for the integration tests. It also opens the current directory in a code editor (assuming `code` is VS Code). This setup prepares the environment for writing integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/2-reading-a-page.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir unit_tests/integration\ntouch unit_tests/integration/test_surveys.py\ncode .\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies with Poetry\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mixpanel/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry in Connector Metadata Service\nDESCRIPTION: Command to install the necessary dependencies for the Connector Metadata Service library using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/lib/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Checking Code Format for Airbyte CI Pipelines\nDESCRIPTION: Command to check the code formatting of the Airbyte CI pipelines module using Ruff.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npoetry run ruff check pipelines\n```\n\n----------------------------------------\n\nTITLE: Building Appcues Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Appcues source connector using airbyte-ci tool. Creates a dev image tagged as source-appcues:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-appcues/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-appcues build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Google Webfonts Source Connector\nDESCRIPTION: Command to build the docker image for the Google Webfonts source connector using airbyte-ci. This creates an image tagged as 'airbyte/source-google-webfonts:dev' on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-webfonts/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-webfonts build\n```\n\n----------------------------------------\n\nTITLE: Displaying MQTT Connector Changelog in Markdown Table\nDESCRIPTION: A markdown table showing the version history and changes made to the MQTT destination connector over time.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/mqtt.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                         |\n| :------ | :--------- | :------------------------------------------------------- | :---------------------------------------------- |\n| 0.1.3   | 2022-09-02 | [16263](https://github.com/airbytehq/airbyte/pull/16263) | Marked password field in spec as airbyte_secret |\n| 0.1.2   | 2022-07-12 | [14648](https://github.com/airbytehq/airbyte/pull/14648) | Include lifecycle management                    |\n| 0.1.1   | 2022-05-24 | [13099](https://github.com/airbytehq/airbyte/pull/13099) | Fixed build's tests                             |\n```\n\n----------------------------------------\n\nTITLE: Customizing Vectara Connector Build Process\nDESCRIPTION: Example Python module for customizing the build process of the Vectara connector, allowing for environment variable settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-vectara/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    # Feel free to check the dagger documentation for more information on the Container object and its methods.\n    # https://dagger-io.readthedocs.io/en/sdk-python-v0.6.4/\n    from dagger import Container\n\n\nasync def pre_connector_install(base_image_container: Container) -> Container:\n    return await base_image_container.with_env_variable(\"MY_PRE_BUILD_ENV_VAR\", \"my_pre_build_env_var_value\")\n\nasync def post_connector_install(connector_container: Container) -> Container:\n    return await connector_container.with_env_variable(\"MY_POST_BUILD_ENV_VAR\", \"my_post_build_env_var_value\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Daemons for Connector Orchestrator\nDESCRIPTION: Command to start the Dagster daemons for the Connector Orchestrator, which allows access to the Dagster UI.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run dagster dev\n```\n\n----------------------------------------\n\nTITLE: Installing Qdrant Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry. This is a prerequisite for local development.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-qdrant/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Adding Project Dependencies with Poetry (Bash)\nDESCRIPTION: Adds a new Python package dependency to the project using the `poetry add` command. This command updates the `pyproject.toml` and `poetry.lock` files. Replace `<package-name>` with the actual name of the package to add.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-convex/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Using reverse Filter in Jinja2\nDESCRIPTION: Demonstrates the `reverse` filter in Jinja2, which reverses the order of items in a sequence or the characters in a string. The example reverses the list `[1, 2, 3]`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/advanced-topics/oauth.md#2025-04-23_snippet_40\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ [1, 2, 3]|reverse }}\n```\n\n----------------------------------------\n\nTITLE: Conditional Data Reset Prompt Notification\nDESCRIPTION: This note informs the user that the prompt to reset data when saving connection changes in Airbyte (after ensuring 'Reset affected streams' is checked) might not appear, as this behavior depends on the specific destination connector being used.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instagram-migrations.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnote\n     Depending on destination type, you may not be prompted to reset your data.\n     \n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Surveymonkey Connector\nDESCRIPTION: Command to build a Docker image for the Surveymonkey connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-surveymonkey/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-surveymonkey build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-box-data-extract/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: REST Config Creation Error\nDESCRIPTION: Error message when abctl is unable to create a REST configuration due to permission issues that prevent file and folder creation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nunable to initialize local command: error communicating with kubernetes:\ncould not create rest config: stat /root/.airbyte/abctl/abctl.kubeconfig:\nno such file or directory\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-cart/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Comparing Old and New Primary Keys for LinkedIn Ads v5.0.0 Account Users Stream (Markdown)\nDESCRIPTION: Shows the primary key change for the `account_users` stream in the LinkedIn Ads connector v5.0.0 update. The new primary key includes `user` in addition to `account`, necessitating a data refresh.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linkedin-ads-migrations.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Old PK      | New PK            | \n|:------------|:------------------|\n| `[account]` | `[account, user]` | \n```\n\n----------------------------------------\n\nTITLE: Building SingleStore Connector Docker Image via Gradle\nDESCRIPTION: Command to build the Docker image for the SingleStore destination connector using Gradle.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-singlestore/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:destination-singlestore:buildConnectorImage\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Commands\nDESCRIPTION: Standard commands for running the Yotpo source connector container, including spec, check, discover, and read operations with mounted volumes for secrets and integration tests.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yotpo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm airbyte/source-yotpo:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yotpo:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-yotpo:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-yotpo:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n----------------------------------------\n\nTITLE: Example CSV Data with Escape Character\nDESCRIPTION: This text snippet demonstrates a CSV row where an escape character (backslash `\\`) is used before a double quote within a quoted field. This prevents the double quote (`\"` for inches) from being misinterpreted as the closing quote for the 'Description' field, ensuring the value `Navy Blue, Bootcut, 34\"` is parsed correctly. This illustrates the usage of the 'Escape Character' setting in the CSV parser configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/microsoft-sharepoint.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nProduct,Description,Price\nJeans,\"Navy Blue, Bootcut, 34\\\"\",49.99\n```\n\n----------------------------------------\n\nTITLE: Parsing Deduplicated Exchange Rate JSON Records\nDESCRIPTION: These JSON objects represent deduplicated exchange rate data records. They are similar to the exchange rate records but belong to the 'dedup_exchange_rate' stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_simple_streams/data_input/messages_schema_change.txt#2025-04-23_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"dedup_exchange_rate\", \"emitted_at\": 1602661281900, \"data\": { \"id\": 3.14, \"currency\": \"EUR\", \"new_column\": 2.1, \"date\": \"2020-11-01\", \"timestamp_col\": \"2020-11-01T00:00:00Z\", \"NZD\": 2.43, \"HKD@spéçiäl & characters\": 2.12, \"column`_'with\\\"_quotes\":\"ma\\\"z`d'a\", \"USD\": 7}}}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Airbyte CI\nDESCRIPTION: Command to execute unit tests for the MSSQL connector using the Airbyte CI CLI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mssql/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nairbyte-ci connectors --name=destination-mssql-v2 test\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new package dependencies using Poetry\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-yandex-metrica/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding a New Dependency with Poetry\nDESCRIPTION: Command to add a new package dependency to the Slack connector using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-slack/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: Detailed version history table containing release versions, dates, pull request references, and change descriptions\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/apify-dataset.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                                 | Subject                                                                         |\n| :------ | :--------- | :----------------------------------------------------------- | :------------------------------------------------------------------------------ |\n| 2.2.17 | 2025-04-12 | [57599](https://github.com/airbytehq/airbyte/pull/57599) | Update dependencies |\n[...additional rows omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Changelog Table\nDESCRIPTION: A markdown table showing the version history and changes made to the YouTube Analytics connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/youtube-analytics.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                        |\n| :------ | :--------- | :------------------------------------------------------- | :--------------------------------------------- |\n| 0.1.7 | 2025-02-26 | [54696](https://github.com/airbytehq/airbyte/pull/54696) | Update requests-mock dependency versionb |\n| 0.1.6 | 2024-06-17 | [39529](https://github.com/airbytehq/airbyte/pull/39529) | Pin CDK version to 0.38.0 |\n| 0.1.5 | 2024-05-21 | [38546](https://github.com/airbytehq/airbyte/pull/38546) | [autopull] base image + poetry + up_to_date |\n| 0.1.4 | 2023-05-22 | [26420](https://github.com/airbytehq/airbyte/pull/26420) | Migrate to advancedAuth |\n| 0.1.3 | 2022-09-30 | [17454](https://github.com/airbytehq/airbyte/pull/17454) | Added custom backoff logic |\n| 0.1.2 | 2022-09-29 | [17399](https://github.com/airbytehq/airbyte/pull/17399) | Fixed `403` error while `check connection` |\n| 0.1.1 | 2022-08-18 | [15744](https://github.com/airbytehq/airbyte/pull/15744) | Fix `channel_basic_a2` schema fields data type |\n| 0.1.0 | 2021-11-01 | [7407](https://github.com/airbytehq/airbyte/pull/7407) | Initial Release |\n```\n\n----------------------------------------\n\nTITLE: Building Connector with Airbyte-CI\nDESCRIPTION: Command to build the connector using Airbyte's CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-motherduck/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name destination-motherduck build\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies\nDESCRIPTION: Command to add new dependencies to the project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-braintree/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Displaying DynamoDB Destination Changelog in Markdown\nDESCRIPTION: A markdown table showing the version history and changes for the DynamoDB destination connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/dynamodb.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                               | Subject                                                     |\n| :------ | :--------- | :--------------------------------------------------------- | :---------------------------------------------------------- |\n| 0.1.8   | 2024-01-03 | [#33924](https://github.com/airbytehq/airbyte/pull/33924)  | Add new ap-southeast-3 AWS region                           |\n| 0.1.7   | 2022-11-03 | [\\#18672](https://github.com/airbytehq/airbyte/pull/18672) | Added strict-encrypt cloud runner                           |\n| 0.1.6   | 2022-11-01 | [\\#18672](https://github.com/airbytehq/airbyte/pull/18672) | Enforce to use ssl connection                               |\n| 0.1.5   | 2022-08-05 | [\\#15350](https://github.com/airbytehq/airbyte/pull/15350) | Added per-stream handling                                   |\n| 0.1.4   | 2022-06-16 | [\\#13852](https://github.com/airbytehq/airbyte/pull/13852) | Updated stacktrace format for any trace message errors      |\n| 0.1.3   | 2022-05-17 | [12820](https://github.com/airbytehq/airbyte/pull/12820)   | Improved 'check' operation performance                      |\n| 0.1.2   | 2022-02-14 | [10256](https://github.com/airbytehq/airbyte/pull/10256)   | Add `-XX:+ExitOnOutOfMemoryError` JVM option                |\n| 0.1.1   | 2022-12-05 | [\\#9314](https://github.com/airbytehq/airbyte/pull/9314)   | Rename dynamo_db_table_name to dynamo_db_table_name_prefix. |\n| 0.1.0   | 2021-08-20 | [\\#5561](https://github.com/airbytehq/airbyte/pull/5561)   | Initial release.                                            |\n```\n\n----------------------------------------\n\nTITLE: Installing connector_ops package using Poetry\nDESCRIPTION: This snippet shows how to install the connector_ops package using Poetry. It assumes you are in the Airbyte repository root and uses Python 3.11 or higher.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connector_ops/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-ci/connectors/connector_ops\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Running Auto-Merge Package in CI Environment\nDESCRIPTION: Commands for running the auto-merge package in a CI environment. It sets the necessary environment variables, installs the package, and executes it.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/auto_merge/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GITHUB_TOKEN=<your_github_token>\nexport AUTO_MERGE_PRODUCTION=true\npoetry install\npoetry run auto-merge\n```\n\n----------------------------------------\n\nTITLE: Resetting Python Environment\nDESCRIPTION: Commands to reset a Python virtual environment and reinstall connector dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ndeactivate\nrm -rf .venv\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Asana Supported Features Table in Markdown\nDESCRIPTION: Markdown table defining the supported sync modes and features of the Asana connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/asana.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| Namespaces        | No         |\n```\n\n----------------------------------------\n\nTITLE: Installing Git Hooks\nDESCRIPTION: Command to install pre-commit git hooks for code formatting.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nmake tools.git-hooks.install\n```\n\n----------------------------------------\n\nTITLE: Sample Feature Branch Creation - Git CLI - Bash\nDESCRIPTION: This example demonstrates the creation of a specific feature branch called 'jdoe/source-stock-api-stream-fix'. It should be run from within the local Airbyte repo, and creates a new branch for tracking related documentation or code changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b jdoe/source-stock-api-stream-fix\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte-CI\nDESCRIPTION: Command to build the docker image for the LinkedIn Pages source connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-linkedin-pages/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-linkedin-pages build\n```\n\n----------------------------------------\n\nTITLE: Building the Reply.io Connector Docker Image with airbyte-ci\nDESCRIPTION: Command to build the docker image for the Reply.io source connector using the airbyte-ci tool. This creates an image tagged as airbyte/source-reply-io:dev on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-reply-io/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-reply-io build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the connector's Docker image using the Airbyte CI tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-notion/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-notion build\n```\n\n----------------------------------------\n\nTITLE: Building the Delighted Source Connector Docker Image using Bash\nDESCRIPTION: This Bash command utilizes the `airbyte-ci` tool to build the Docker image for the `source-delighted` connector. It requires `airbyte-ci` to be installed. The resulting image will be tagged as `airbyte/source-delighted:dev` on the local host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-delighted/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-delighted build\n```\n\n----------------------------------------\n\nTITLE: Building the Lightspeed Retail Connector with airbyte-ci (Bash)\nDESCRIPTION: This snippet demonstrates how to build the development Docker image for the Lightspeed Retail connector using the airbyte-ci command-line tool. The command requires airbyte-ci to be installed and references the source-lightspeed-retail connector by name. It produces a local dev image named source-lightspeed-retail:dev, which is used for further testing and iteration. The input is the connector name as an argument, and the command outputs a built dev image; no further parameters are needed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lightspeed-retail/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lightspeed-retail build\n```\n\n----------------------------------------\n\nTITLE: Building the Sendowl Source Connector\nDESCRIPTION: Command to build the Sendowl source connector using airbyte-ci, which creates a dev image (source-sendowl:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-sendowl/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-sendowl build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Harvest Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the docker image for the Harvest source connector. The resulting image will be tagged as 'airbyte/source-harvest:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-harvest/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-harvest build\n```\n\n----------------------------------------\n\nTITLE: Describing the `actor_definition_breaking_change` Table Schema in Database Schema\nDESCRIPTION: Defines the schema for the `actor_definition_breaking_change` table, which tracks breaking changes for actor definitions. It includes the associated actor definition ID, the version introducing the change, documentation URL, upgrade deadline, description, timestamps, impact scope (JSON), and required deadline action. The primary key is composite (`actor_definition_id`, `version`), and a foreign key links to `actor_definition`.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/understanding-airbyte/database-data-catalog.md#2025-04-23_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n| Column Name                     | Datatype     | Description                                                 |\n| -------------------------------- | ------------ | ----------------------------------------------------------- |\n| actor_definition_id              | UUID         | Foreign key referencing `actor_definition(id)`.              |\n| version                          | VARCHAR(256) | Version of the breaking change.                             |\n| migration_documentation_url      | VARCHAR(256) | URL linking to migration documentation.                     |\n| upgrade_deadline                 | DATE         | Deadline for upgrading to the new version.                   |\n| message                          | TEXT         | Description of the breaking change.                         |\n| created_at                       | TIMESTAMP    | Timestamp when the record was created.                      |\n| updated_at                       | TIMESTAMP    | Timestamp when the record was last modified.                |\n| scoped_impact                    | JSONB        | JSON object describing the impact scope.                    |\n| deadline_action                   | VARCHAR(256) | Action required before the deadline.                        |\n\n#### Indexes and Constraints\n\n- Primary Key: (`actor_definition_id`, `version`)\n- Foreign Key: `actor_definition_id` references `actor_definition(id)`\n```\n\n----------------------------------------\n\nTITLE: Building Teradata Source Connector via Gradle in Java\nDESCRIPTION: This command builds the Teradata source connector using Gradle from the Airbyte repository root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-teradata/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-teradata:build\n```\n\n----------------------------------------\n\nTITLE: Listing Connectors by Language\nDESCRIPTION: This command filters and lists connectors written in a specific programming language, in this case, Python.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --language=python list\n```\n\n----------------------------------------\n\nTITLE: Conditional Rendering Example of Docs for Cloud/Oss/UI using Airbyte Markdown Extensions - Markdown\nDESCRIPTION: Demonstrates a practical usage of combined UI/environment custom markdown tags, including `<HideInUI>`, and environment selectors, for context-aware documentation presentation. The snippet shows how to separate content for open source and Cloud builds while also hiding some content from in-app UI. Required: Airbyte markdown renderer; Inputs: Content for each environment; Outputs: Proper scoping of documentation sections by reader's environment. Limitation: Only effective in Airbyte's documentation system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/writing-connector-docs.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n# My connector\\n\\nThis content is rendered everywhere.\\n\\n<!-- env:oss -->\\n\\n<HideInUI>\\n\\n## For open source: \\n\\n</HideInUI>\\n\\nOnly self-managed builds of the Airbyte UI will render this content.\\n\\n<!-- /env:oss -->\\n\\n<!-- env:cloud -->\\n<HideInUI>\\n\\n## For Airbyte Cloud:\\n\\n</HideInUI>\\n\\nOnly Cloud builds of the Airbyte UI will render this content.\\n\\n<!-- /env:oss -->\n```\n\n----------------------------------------\n\nTITLE: Installing Pinecone Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-pinecone/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Python Connector Dependencies\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-chroma/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry - Bash\nDESCRIPTION: Installs the connector's Python dependencies using Poetry in the current directory. This requires that Python (^3.9) and Poetry (^1.7) are already installed. The '--with dev' flag ensures development dependencies are included. Ensures all dependencies needed for local development and testing are available.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-posthog/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Table for Talkdesk Explore Connector Changelog\nDESCRIPTION: This markdown table shows the version history of the Talkdesk Explore connector, including version numbers, dates, pull requests, and subjects of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/talkdesk-explore.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request | Subject                      |\n| ------- | ---------- | ------------ | ---------------------------- |\n| 0.1.0   | 2022-02-07 |              | New Source: Talkdesk Explore |\n| :---    | :---       | :---         | :---                         |\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to activate the virtual environment and install required dependencies from requirements.txt\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-typesense/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Devices Stream Schema in JSON\nDESCRIPTION: JSON schema for the devices stream in the Google Analytics connector. It includes metrics related to device categories, operating systems, and browsers.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"ga_date\":\"2021-02-11\",\"ga_deviceCategory\":\"desktop\",\"ga_operatingSystem\":\"Macintosh\",\"ga_browser\":\"Chrome\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Jira Connector\nDESCRIPTION: Command to add a new dependency to the Jira connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: Lists required Python packages: mysql-connector-python for MySQL database connectivity and pytz for timezone handling.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-mysql/integration_tests/seed/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmysql-connector-python\npytz\n```\n\n----------------------------------------\n\nTITLE: Running Specific QA Checks on All Connectors\nDESCRIPTION: Command to run specific QA checks on all connectors in the specified directory\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconnectors-qa run --connector-directory=airbyte-integrations/connectors --check=CheckConnectorIconIsAvailable --check=CheckConnectorUsesPythonBaseImage\n```\n\n----------------------------------------\n\nTITLE: Example Dataset for Offset Increment Pagination (JSON)\nDESCRIPTION: Illustrates a sample JSON dataset containing product information used to demonstrate the Offset Increment pagination method.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/connector-builder-ui/pagination.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"id\": 1, \"name\": \"Product A\"},\n  {\"id\": 2, \"name\": \"Product B\"},\n  {\"id\": 3, \"name\": \"Product C\"},\n  {\"id\": 4, \"name\": \"Product D\"},\n  {\"id\": 5, \"name\": \"Product E\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Adding a Dependency with Poetry - Bash\nDESCRIPTION: This command adds a new Python package dependency to the project using Poetry. The <package-name> placeholder should be replaced with the desired dependency. Upon execution, Poetry updates the pyproject.toml and poetry.lock files to reflect the change. Committing these files ensures consistent environments across development and CI environments.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-ads/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Defining Survicate Data Streams in Markdown\nDESCRIPTION: This snippet details the available data streams from the Survicate API, including their primary keys, pagination methods, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/survicate.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| surveys | id | DefaultPaginator | ✅ |  ✅  |\n| surveys_questions | id | DefaultPaginator | ✅ |  ❌  |\n| surveys_responses | uuid | DefaultPaginator | ✅ |  ✅  |\n| respondents_attributes |  | DefaultPaginator | ✅ |  ❌  |\n| respondents_responses |  | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Writing Credentials for All Connectors\nDESCRIPTION: Command to download GSM secrets for all available connectors.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=dev ci_credentials all write-to-storage\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table\nDESCRIPTION: Mapping between Zenloop integration types and corresponding Airbyte data types.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zenloop.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `integer`        | `integer`    |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |\n```\n\n----------------------------------------\n\nTITLE: Running Specific Tests for Airbyte CI\nDESCRIPTION: Command to run a specific subset of tests for Airbyte CI using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest pipelines/models/steps.py\n```\n\n----------------------------------------\n\nTITLE: Documenting Testrail Connector Changelog in Markdown\nDESCRIPTION: Provides a detailed changelog of the Testrail connector, including version numbers, dates, pull request links, and descriptions of changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/testrail.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n| ------------------ | ------------ | -- | ---------------- |\n| 0.0.20 | 2025-04-19 | [58413](https://github.com/airbytehq/airbyte/pull/58413) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57974](https://github.com/airbytehq/airbyte/pull/57974) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57456](https://github.com/airbytehq/airbyte/pull/57456) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56824](https://github.com/airbytehq/airbyte/pull/56824) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56283](https://github.com/airbytehq/airbyte/pull/56283) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55573](https://github.com/airbytehq/airbyte/pull/55573) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [55132](https://github.com/airbytehq/airbyte/pull/55132) | Update dependencies |\n| 0.0.13 | 2025-02-22 | [54527](https://github.com/airbytehq/airbyte/pull/54527) | Update dependencies |\n| 0.0.12 | 2025-02-15 | [54075](https://github.com/airbytehq/airbyte/pull/54075) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53563](https://github.com/airbytehq/airbyte/pull/53563) | Update dependencies |\n| 0.0.10 | 2025-02-01 | [53041](https://github.com/airbytehq/airbyte/pull/53041) | Update dependencies |\n| 0.0.9 | 2025-01-25 | [52447](https://github.com/airbytehq/airbyte/pull/52447) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [51963](https://github.com/airbytehq/airbyte/pull/51963) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51454](https://github.com/airbytehq/airbyte/pull/51454) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50825](https://github.com/airbytehq/airbyte/pull/50825) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50352](https://github.com/airbytehq/airbyte/pull/50352) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49400](https://github.com/airbytehq/airbyte/pull/49400) | Update dependencies |\n| 0.0.3 | 2024-11-04 | [47773](https://github.com/airbytehq/airbyte/pull/47773) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47630](https://github.com/airbytehq/airbyte/pull/47630) | Update dependencies |\n| 0.0.1 | 2024-09-29 | [46250](https://github.com/airbytehq/airbyte/pull/46250) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry in Bash\nDESCRIPTION: This Bash code snippet installs all required dependencies for the Airbyte Gitlab source connector using Poetry, including development dependencies. Poetry (~=1.7) must be installed in advance. The primary parameter is 'install', with the '--with dev' flag ensuring development dependencies are also included. No direct input or output is produced aside from package installation; successful execution readies the local environment for development. There are no special limitations, but the current working directory should be the connector's root.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitlab/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies with Poetry\nDESCRIPTION: Command to add new package dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-couchbase/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependency to Amazon SQS Connector\nDESCRIPTION: Command to add a new dependency to the Amazon SQS connector project using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-amazon-sqs/README.md#2025-04-23_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\npoetry add <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing the Recharge connector with Poetry\nDESCRIPTION: Command to install the connector and its dependencies using Poetry for local development, including development dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-asana/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: Installs the connector and development dependencies using Poetry package manager\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zenloop/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Deepset Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-deepset/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Installing Jina AI Reader Connector Dependencies with Poetry\nDESCRIPTION: Command to install the connector and its development dependencies using Poetry package manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jina-ai-reader/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev\n```\n\n----------------------------------------\n\nTITLE: Displaying Help Information\nDESCRIPTION: Command to show the help documentation for the ci_credentials tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=dev ci_credentials --help\n```\n\n----------------------------------------\n\nTITLE: Configuring Huntr Connector API Key in Markdown\nDESCRIPTION: Markdown table describing the configuration parameter for the Huntr connector. It specifies the API key as a required string input.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/huntr.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n```\n\n----------------------------------------\n\nTITLE: Documenting Connector Configuration in Markdown\nDESCRIPTION: This snippet displays a Markdown table that documents the configuration parameters required to set up the Linear Airbyte connector, including the API key. The table specifies the input name, data type, description, and default value. Users must provide the API Key, which is mandatory for successful authentication. There are no default values provided, and this configuration is necessary to establish a connection between Airbyte and Linear.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/linear.md#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\\n|-------|------|-------------|---------------|\\n| `api_key` | `string` | API Key |  |\n```\n\n----------------------------------------\n\nTITLE: Configuration Table in Markdown\nDESCRIPTION: Markdown table showing configuration parameters for the Appfigures connector, including API key, search store, start date, and group by options.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appfigures.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `search_store` | `string` | Search Store. The store which needs to be searched in streams | apple |\n| `start_date` | `string` | Start date.  |  |\n| `group_by` | `string` | Group by. Category term for grouping the search results | product |\n```\n\n----------------------------------------\n\nTITLE: Datetime with Timezone Test Records in Airbyte JSON Format\nDESCRIPTION: Sample records for datetime with timezone data tests in Airbyte's JSON record format. Includes examples with different datetime values including current, historical, and far-future datetimes with timezone offsets.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/data_type_basic_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_1\", \"emitted_at\": 1602637589100, \"data\": { \"data\" : \"2022-11-22T01:23:45+05:00\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_1\", \"emitted_at\": 1602637589200, \"data\": { \"data\" : \"1504-02-29T01:23:45+05:00\" }}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"datetime_test_1\", \"emitted_at\": 1602637589300, \"data\": { \"data\" : \"9999-12-21T01:23:45+05:00\" }}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Height Source Connector in Markdown\nDESCRIPTION: This snippet shows the configuration options for the Height source connector, including API key, start date, and search query parameters.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/height.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API secret which is copied from the settings page of height.app  |  |\n| `start_date` | `string` | Start date for incremental sync supported streams |  |\n| `search_query` | `string` | search_query. Search query to be used with search stream | task |\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with airbyte-ci - bash\nDESCRIPTION: Builds the Docker image for the Github source connector via the 'airbyte-ci' tool. The '--name=source-github' flag specifies the connector, while 'build' triggers the build process. 'airbyte-ci' must be installed and configured as per project documentation. The resulting Docker image is tagged as 'airbyte/source-github:dev' and available locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-github build\n```\n\n----------------------------------------\n\nTITLE: Creating Basic GA4 Custom Report\nDESCRIPTION: Example of configuring a single custom report with dimensions and metrics for page views and users analysis.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-analytics-v4-service-account-only.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"page_views_and_users\",\n    \"dimensions\": [\n      \"ga:date\",\n      \"ga:pagePath\",\n      \"ga:sessionDefaultChannelGrouping\"\n    ],\n    \"metrics\": [\"ga:screenPageViews\", \"ga:totalUsers\"]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Email for Authentication\nDESCRIPTION: Command to set a custom email address for authenticating with Airbyte when the email is not automatically populated.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nabctl local credentials --email <USER@COMPANY.EXAMPLE>\n```\n\n----------------------------------------\n\nTITLE: Running Oracle Source Connector Integration Tests with Gradle\nDESCRIPTION: This shell command employs the Gradle wrapper (`./gradlew`) from the Airbyte repository root to execute the `integrationTest` task for the `source-oracle` module. This runs the JUnit-based integration tests designed to validate the connector's interaction with an Oracle database. Requires appropriate test environment configuration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-oracle/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-oracle:integrationTest\n```\n\n----------------------------------------\n\nTITLE: JSON Record with Different Values\nDESCRIPTION: A second JSON record with different values but following the same structure as the first record, demonstrating how the same schema can handle varying data.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_disjoint_union_messages_out.txt#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"schemaless_object\":\"{\\\"address\\\":{\\\"street\\\":\\\"113 Hickey Rd\\\",\\\"zip\\\":\\\"37932\\\"},\\\"flags\\\":[true,false,false]}\",\"schematized_object\":{\"id\":2,\"name\":\"Jane\"},\"combined_type\":{\"type\":\"integer\",\"string\":null,\"integer\":20},\"union_type\":{\"type\":\"string\",\"string\":\"string2\",\"integer\":null},\"schemaless_array\":\"[]\",\"mixed_array_integer_and_schemaless_object\":[],\"array_of_union_integer_and_schemaless_array\":[],\"union_of_objects_with_properties_identical\":{\"id\":null,\"name\":null},\"union_of_objects_with_properties_overlapping\":{\"id\":null,\"name\":null,\"flagged\":null},\"union_of_objects_with_properties_nonoverlapping\":{\"id\":null,\"name\":null,\"flagged\":null,\"description\":null},\"union_of_objects_with_properties_contradicting\":{\"id\":{\"type\":\"string\",\"integer\":null,\"string\":\"seal-one-hippity\"},\"name\":\"James\"},\"empty_object\":\"{\\\"extra\\\":\\\"stuff\\\"}\",\"object_with_null_properties\":\"{\\\"more\\\":{\\\"extra\\\":\\\"stuff\\\"}}\",\"combined_with_null\":\"foobar2\",\"union_with_null\":\"barfoo2\",\"combined_nulls\":null,\"compact_union\":{\"type\":\"integer\",\"integer\":4444,\"object\":null}}\n```\n\n----------------------------------------\n\nTITLE: Data Type Mapping Table in Markdown\nDESCRIPTION: A markdown table showing the mapping between Integration types and Airbyte types for the Google Workspace Admin Reports connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-workspace-admin-reports.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n```\n\n----------------------------------------\n\nTITLE: AgileCRM Configuration Table\nDESCRIPTION: Configuration parameters required for setting up the AgileCRM connector, including email, domain, and API key specifications.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/agilecrm.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `email` | `string` | Email Address. Your Agile CRM account email address. This is used as the username for authentication. |  |\n| `domain` | `string` | Domain. The specific subdomain for your Agile CRM account |  |\n| `api_key` | `string` | API Key. API key to use. Find it at Admin Settings -> API & Analytics -> API Key in your Agile CRM account. |  |\n```\n\n----------------------------------------\n\nTITLE: Using demo.sh Script for Airbyte Demo Instance Management\nDESCRIPTION: Script that provides utilities for managing Airbyte's demo instance, including SSH connection and creating a local tunnel for accessing the configurable version of Airbyte.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/internal/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./tools/internal/demo.sh ssh # connects you to the airbyte instance\n./tools/internal/demo.sh tunnel # creates a local tunnel so you can access the configurable version of airbyte\n```\n\n----------------------------------------\n\nTITLE: Displaying Changelog for Toggl API Connector in Markdown\nDESCRIPTION: This snippet presents a detailed changelog in markdown format, showing version history, dates, pull request links, and subjects for updates to the Toggl API connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/toggl.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                              | Subject                                 |\n|:--------|:-----------| :-------------------------------------------------------- | :-------------------------------------- |\n| 0.2.20 | 2025-04-19 | [58422](https://github.com/airbytehq/airbyte/pull/58422) | Update dependencies |\n| 0.2.19 | 2025-04-12 | [57941](https://github.com/airbytehq/airbyte/pull/57941) | Update dependencies |\n| 0.2.18 | 2025-04-05 | [57410](https://github.com/airbytehq/airbyte/pull/57410) | Update dependencies |\n| 0.2.17 | 2025-03-29 | [56895](https://github.com/airbytehq/airbyte/pull/56895) | Update dependencies |\n| 0.2.16 | 2025-03-22 | [56256](https://github.com/airbytehq/airbyte/pull/56256) | Update dependencies |\n| 0.2.15 | 2025-03-08 | [55629](https://github.com/airbytehq/airbyte/pull/55629) | Update dependencies |\n| 0.2.14 | 2025-03-01 | [55088](https://github.com/airbytehq/airbyte/pull/55088) | Update dependencies |\n| 0.2.13 | 2025-02-22 | [54510](https://github.com/airbytehq/airbyte/pull/54510) | Update dependencies |\n| 0.2.12 | 2025-02-15 | [54059](https://github.com/airbytehq/airbyte/pull/54059) | Update dependencies |\n| 0.2.11 | 2025-02-08 | [53557](https://github.com/airbytehq/airbyte/pull/53557) | Update dependencies |\n| 0.2.10 | 2025-02-01 | [53090](https://github.com/airbytehq/airbyte/pull/53090) | Update dependencies |\n| 0.2.9 | 2025-01-25 | [52429](https://github.com/airbytehq/airbyte/pull/52429) | Update dependencies |\n| 0.2.8 | 2025-01-18 | [52026](https://github.com/airbytehq/airbyte/pull/52026) | Update dependencies |\n| 0.2.7 | 2025-01-11 | [51383](https://github.com/airbytehq/airbyte/pull/51383) | Update dependencies |\n| 0.2.6 | 2024-12-28 | [50775](https://github.com/airbytehq/airbyte/pull/50775) | Update dependencies |\n| 0.2.5 | 2024-12-21 | [50305](https://github.com/airbytehq/airbyte/pull/50305) | Update dependencies |\n| 0.2.4 | 2024-12-14 | [49738](https://github.com/airbytehq/airbyte/pull/49738) | Update dependencies |\n| 0.2.3 | 2024-12-12 | [49434](https://github.com/airbytehq/airbyte/pull/49434) | Update dependencies |\n| 0.2.2 | 2024-10-29 | [47883](https://github.com/airbytehq/airbyte/pull/47883) | Update dependencies |\n| 0.2.1 | 2024-08-16 | [44196](https://github.com/airbytehq/airbyte/pull/44196) | Bump source-declarative-manifest version |\n| 0.2.0 | 2024-08-14 | [44056](https://github.com/airbytehq/airbyte/pull/44056) | Refactor connector to manifest-only format |\n| 0.1.14 | 2024-08-12 | [43860](https://github.com/airbytehq/airbyte/pull/43860) | Update dependencies |\n| 0.1.13 | 2024-08-10 | [43485](https://github.com/airbytehq/airbyte/pull/43485) | Update dependencies |\n| 0.1.12 | 2024-08-03 | [43064](https://github.com/airbytehq/airbyte/pull/43064) | Update dependencies |\n| 0.1.11 | 2024-07-27 | [42755](https://github.com/airbytehq/airbyte/pull/42755) | Update dependencies |\n| 0.1.10 | 2024-07-20 | [42244](https://github.com/airbytehq/airbyte/pull/42244) | Update dependencies |\n| 0.1.9 | 2024-07-13 | [41736](https://github.com/airbytehq/airbyte/pull/41736) | Update dependencies |\n| 0.1.8 | 2024-07-10 | [41510](https://github.com/airbytehq/airbyte/pull/41510) | Update dependencies |\n| 0.1.7 | 2024-07-09 | [41227](https://github.com/airbytehq/airbyte/pull/41227) | Update dependencies |\n| 0.1.6 | 2024-07-06 | [40968](https://github.com/airbytehq/airbyte/pull/40968) | Update dependencies |\n| 0.1.5   | 2024-06-28 | [#38740](https://github.com/airbytehq/airbyte/pull/38740) | Make connector compatible with Builder  |\n| 0.1.4   | 2024-06-25 | [40493](https://github.com/airbytehq/airbyte/pull/40493) | Update dependencies |\n| 0.1.3   | 2024-06-22 | [40096](https://github.com/airbytehq/airbyte/pull/40096) | Update dependencies |\n| 0.1.2   | 2024-06-04 | [38985](https://github.com/airbytehq/airbyte/pull/38985) | [autopull] Upgrade base image to v1.2.1 |\n| 0.1.1   | 2024-05-20 | [38376](https://github.com/airbytehq/airbyte/pull/38376) | [autopull] base image + poetry + up_to_date |\n| 0.1.0   | 2022-10-28 | [#18507](https://github.com/airbytehq/airbyte/pull/18507) | 🎉 New Source: Toggl API [low-code CDK] |\n```\n\n----------------------------------------\n\nTITLE: Installing CI Credentials from GitHub\nDESCRIPTION: Command to install ci_credentials directly from the GitHub repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/ci_credentials/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npipx install git+https://github.com/airbytehq/airbyte.git#subdirectory=airbyte-ci/connectors/ci_credentials\n```\n\n----------------------------------------\n\nTITLE: Notifying User of Schema Change Review\nDESCRIPTION: This note clarifies that after refreshing the source schema within the Airbyte connection settings (Replication tab), any detected changes will be presented to the user for review before proceeding.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/instagram-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnote\n     Any detected schema changes will be listed for your review.\n     \n```\n\n----------------------------------------\n\nTITLE: Building Source-Referralhero Connector with Airbyte-CI\nDESCRIPTION: Command to build the Referral Hero source connector using airbyte-ci, which creates a development image (source-referralhero:dev) for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-referralhero/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-referralhero build\n```\n\n----------------------------------------\n\nTITLE: Building Zoho Books Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Zoho Books connector using airbyte-ci. Creates a dev image tagged as source-zoho-books:dev for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-books/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-books build\n```\n\n----------------------------------------\n\nTITLE: Building Xsolla Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Xsolla connector using airbyte-ci tool. Creates a dev image tagged as source-xsolla:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-xsolla/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-xsolla build\n```\n\n----------------------------------------\n\nTITLE: Building the Retail Express by Maropost Connector using airbyte-ci\nDESCRIPTION: This command builds a development image for the Retail Express by Maropost connector that can be used for local testing. It creates a Docker image tagged as 'source-retailexpress-by-maropost:dev'.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-retailexpress-by-maropost/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-retailexpress-by-maropost build\n```\n\n----------------------------------------\n\nTITLE: Building the GitBook Connector Development Image using airbyte-ci (Bash)\nDESCRIPTION: This command utilizes the `airbyte-ci` tool to build a local development Docker image tagged `source-gitbook:dev`. This image is necessary for testing the connector locally before deployment or further integration. Requires `airbyte-ci` to be installed and configured.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gitbook/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gitbook build\n```\n\n----------------------------------------\n\nTITLE: Documentation Header for Oracle Strict Encrypt Testing\nDESCRIPTION: Markdown header and description explaining the requirements for testing Oracle destination with SSL encryption. Specifies that the connector is a variant that only allows SSL connections.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-oracle-strict-encrypt/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Oracle Strict Encrypt Test Configuration\n\nIn order to test the Oracle destination, you need to have the up and running Oracle database that has SSL enabled.\n\nThis connector inherits the Oracle destination, but support SSL connections only.\n```\n\n----------------------------------------\n\nTITLE: Version Changelog Table in Markdown\nDESCRIPTION: Detailed changelog table showing version history, dates, pull request references, and changes made to the AppsFlyer connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appsflyer.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                           | Subject                                     |\n| :------ | :--------- | :----------------------------------------------------- | :------------------------------------------ |\n| 0.2.36 | 2025-04-19 | [58244](https://github.com/airbytehq/airbyte/pull/58244) | Update dependencies |\n| 0.2.35 | 2025-04-12 | [57618](https://github.com/airbytehq/airbyte/pull/57618) | Update dependencies |\n[...additional entries truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Zuora Data Type Mapping Table\nDESCRIPTION: Markdown table showing the mapping between Zuora data types and their corresponding Airbyte types\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zuora.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Integration Type           | Airbyte Type | Notes                                       |\n| :------------------------- | :----------- | :------------------------------------------ |\n| `decimal(22,9)`            | `number`     | float number                                |\n| `decimal`                  | `number`     | float number                                |\n| `float`                    | `number`     | float number                                |\n| `double`                   | `number`     | float number                                |\n| `integer`                  | `number`     |                                             |\n| `int`                      | `number`     |                                             |\n| `bigint`                   | `number`     |                                             |\n| `smallint`                 | `number`     |                                             |\n| `timestamp`                | `number`     | number representation of the unix timestamp |\n| `date`                     | `string`     |                                             |\n| `datetime`                 | `string`     |                                             |\n| `timestamp with time zone` | `string`     |                                             |\n| `picklist`                 | `string`     |                                             |\n| `text`                     | `string`     |                                             |\n| `varchar`                  | `string`     |                                             |\n| `zoql`                     | `object`     |                                             |\n| `binary`                   | `object`     |                                             |\n| `json`                     | `object`     |                                             |\n| `xml`                      | `object`     |                                             |\n| `blob`                     | `object`     |                                             |\n| `list`                     | `array`      |                                             |\n| `array`                    | `array`      |                                             |\n| `boolean`                  | `boolean`    |                                             |\n| `bool`                     | `boolean`    |                                             |\n```\n\n----------------------------------------\n\nTITLE: Importing Manifest YAML Definitions and Schema - React (JavaScript)\nDESCRIPTION: This snippet imports a React component for displaying YAML manifest definitions and a YAML schema file as a JavaScript object. The dependencies are '@site/src/components/ManifestYamlDefinitions' for the React component and '@site/src/data/declarative_component_schema.yaml' for the schema. These imports set up the data and presentation resources needed for the subsequent documentation logic.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/config-based/understanding-the-yaml-file/reference.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport ManifestYamlDefinitions from '@site/src/components/ManifestYamlDefinitions';\nimport schema from \"@site/src/data/declarative_component_schema.yaml\";\n```\n\n----------------------------------------\n\nTITLE: BigMailer Streams Table\nDESCRIPTION: Available data streams in the BigMailer connector with their properties including primary keys, pagination details, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/bigmailer.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| brands | id | DefaultPaginator | ✅ |  ❌  |\n| contacts | id | DefaultPaginator | ✅ |  ❌  |\n| lists | id | DefaultPaginator | ✅ |  ❌  |\n| fields | id | DefaultPaginator | ✅ |  ❌  |\n| message-types | id | DefaultPaginator | ✅ |  ❌  |\n| segments | id | DefaultPaginator | ✅ |  ❌  |\n| bulk_campaigns | id | DefaultPaginator | ✅ |  ❌  |\n| transactional_campaigns | id | DefaultPaginator | ✅ |  ❌  |\n| suppression_lists |  | DefaultPaginator | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Installing Node Dependencies\nDESCRIPTION: Command to install the required Node.js version for the Airbyte webapp using nvm.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-webapp\nnvm install\n```\n\n----------------------------------------\n\nTITLE: Redshift Connector Overview\nDESCRIPTION: Markdown documentation outlining the Redshift connector's capabilities, including direct insert and S3 staging modes, SSH tunneling support, and testing procedures. Includes information about secret management in Google Cloud Secrets Manager.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-redshift/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Destination Redshift\n\nThis is the repository for the Redshift Destination Connector.\n\nThis connector can run in one of two mode:\n\n- Direct Insert: using SQL to directly place data into tables.\n- S3 Staging: Data files are uploaded to the customer's S3 and a load is done into the database from these files directly. This is a directly\n  supported feature of Redshift. Consult Redshift documentation for more information and permissions.\n\nThis connector has a capability to query the database via an SSH Tunnel (bastion host). This can be useful for environments where Redshift has not\nbeen exposed to the internet.\n\n## Testing\n\nUnit tests are run as usual.\n\nIntegration/Acceptance tests are run via the command line with secrets managed out of Google Cloud Secrets Manager.\nConsult the integration test area for Redshift.\n\n## Actual secrets\n\nThe actual secrets for integration tests can be found in Google Cloud Secrets Manager. Search on redshift for the labels:\n\n- SECRET_DESTINATION-REDSHIFT**CREDS - used for Standard tests. (**config.json__)\n- SECRET_DESTINATION-REDSHIFT_STAGING**CREDS - used for S3 Staging tests. (**config_staging.json__)\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Jira Connector\nDESCRIPTION: Command to run the full CI test suite for the Jira connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-jira/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-jira test\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for LaunchDarkly Source Connector\nDESCRIPTION: Command to build the docker image for the LaunchDarkly source connector using airbyte-ci. This creates an image tagged as airbyte/source-launchdarkly:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-launchdarkly/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-launchdarkly build\n```\n\n----------------------------------------\n\nTITLE: Zonka Feedback Configuration Parameters\nDESCRIPTION: Configuration table showing required input parameters for connecting to Zonka Feedback API, including data center ID and authentication token.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zonka-feedback.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `dc_id` | `string` | Data Center ID. The identifier for the data center, such as 'us1' or 'e' for EU. |  |\n| `auth_token` | `string` | Auth Token. Auth token to use. Generate it by navigating to Company Settings > Developers > API in your Zonka Feedback account. |  |\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite for Airbyte Plaid Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash command launches the full CI test suite for the Airbyte Plaid source connector using the airbyte-ci tool. It requires airbyte-ci to be installed, with the working directory set to the connector's context. The command expects that necessary credentials and configurations are already set up and validates connector behavior prior to publishing or releasing changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-plaid/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-plaid test\n```\n\n----------------------------------------\n\nTITLE: Running Connectors Insights CLI\nDESCRIPTION: Command to run the Connectors Insights CLI to generate artifacts. This example shows how to specify output directory, GCS URI, connector directory, concurrency level, and whether to rewrite existing artifacts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_insights/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# From airbyte root directory\nconnectors-insights generate --output-directory <path-to-local-output-dir> --gcs-uri=gs://<bucket>/<key-prefix> --connector-directory airbyte-integrations/connectors/ --concurrency 2 --rewrite\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment for Airbyte Connector\nDESCRIPTION: Commands to create and activate a Python virtual environment for local connector development. This is required before installing dependencies.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-python-http-tutorial/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Changelog Table in Markdown\nDESCRIPTION: Version history table showing updates and changes to the Northpass LMS connector, including version numbers, dates, pull request references, and change descriptions.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/northpass-lms.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version  | Date       | Pull Request                                             | Subject                                                                                                                              |\n|:---------|:-----------|:---------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|\n| 0.2.21 | 2025-04-12 | [57915](https://github.com/airbytehq/airbyte/pull/57915) | Update dependencies |\n| 0.2.20 | 2025-04-05 | [57305](https://github.com/airbytehq/airbyte/pull/57305) | Update dependencies |\n| 0.2.19 | 2025-03-29 | [56727](https://github.com/airbytehq/airbyte/pull/56727) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Defining Imagga API Streams in Markdown\nDESCRIPTION: Markdown table listing the available streams for the Imagga API connector. It includes stream names, primary keys, pagination methods, and sync support information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/imagga.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| categorizers | id | DefaultPaginator | ❌ |  ❌  |\n| croppings | uuid | DefaultPaginator | ❌ |  ❌  |\n| colors | uuid | DefaultPaginator | ❌ |  ❌  |\n| faces_detections | uuid | DefaultPaginator | ❌ |  ❌  |\n| text | uuid | DefaultPaginator | ❌ |  ❌  |\n| usage | uuid | DefaultPaginator | ❌ |  ❌  |\n| barcodes | uuid | DefaultPaginator | ❌ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Workday Feature Support Table\nDESCRIPTION: Markdown table showing supported features of the Workday connector including Full Refresh Sync, Incremental Sync, Delete handling, SSL connection, and Namespaces support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/enterprise-connectors/source-workday.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         | \n| SSL connection                | Yes        |\n| Namespaces                    | No         |\n```\n\n----------------------------------------\n\nTITLE: Defining Jamf Pro Data Streams in Markdown\nDESCRIPTION: Specifies the available data streams from Jamf Pro, including their properties such as primary key, pagination, and sync support.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/jamf-pro.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| computers | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Deploying Connector Orchestrator to Dagster Cloud Manually\nDESCRIPTION: Commands to install the dagster-cloud CLI and deploy the orchestrator manually to Dagster Cloud.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/metadata_service/orchestrator/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-cloud\ndagster-cloud config\n\ncd orchestrator\nDAGSTER_CLOUD_API_TOKEN=<YOU-DAGSTER-CLOUD-TOKEN> airbyte-ci metadata deploy orchestrator\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests via Docker and Pytest\nDESCRIPTION: This sequence of shell commands first builds the connector's Docker image (disabling cache) and then runs the integration tests, including the Airbyte connector acceptance tests, using pytest with the specific acceptance test plugin enabled ('connector_acceptance_test.plugin').\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-couchbase/README.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker build . --no-cache -t airbyte/source-couchbase:dev \\\n&& python -m pytest -p connector_acceptance_test.plugin\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block - Schema Refresh\nDESCRIPTION: Markdown code block indicating schema change detection during refresh process\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads-migrations.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nAny detected schema changes will be listed for your review.\n```\n```\n\n----------------------------------------\n\nTITLE: Building Svix Source Connector for Local Development\nDESCRIPTION: This command uses airbyte-ci to build a development image of the Svix source connector. The resulting image (source-svix:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-svix/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-svix build\n```\n\n----------------------------------------\n\nTITLE: Building the Flowlu Connector with airbyte-ci\nDESCRIPTION: Command to build the Flowlu source connector locally, creating a dev image that can be used for testing. This requires airbyte-ci to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-flowlu/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-flowlu build\n```\n\n----------------------------------------\n\nTITLE: Building Watchmode Source Connector with airbyte-ci\nDESCRIPTION: This command builds a development image of the Watchmode source connector using airbyte-ci. The resulting image (source-watchmode:dev) can be used for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-watchmode/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-watchmode build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Airbyte CI\nDESCRIPTION: Command to build the docker image for the source-babelforce connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-babelforce/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-babelforce build\n```\n\n----------------------------------------\n\nTITLE: Building Canny Source Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the Canny source connector using airbyte-ci tool. Creates a dev image tagged as source-canny:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-canny/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-canny build\n```\n\n----------------------------------------\n\nTITLE: Building Manifest-Only Connector with airbyte-ci (Bash)\nDESCRIPTION: This bash code builds the manifest-only Drift source connector Docker image using airbyte-ci. It requires airbyte-ci to be installed beforehand, which can be obtained from the Airbyte GitHub repository. The command tags the built image as airbyte/source-drift:dev on the local Docker host, allowing it to be run for testing and development purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-drift/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-drift build\n\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for SpaceX API Source Connector\nDESCRIPTION: This command uses airbyte-ci to build the Docker image for the SpaceX API source connector. The resulting image will be tagged as airbyte/source-spacex-api:dev.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-spacex-api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-spacex-api build\n```\n\n----------------------------------------\n\nTITLE: Nylas Configuration Table\nDESCRIPTION: Configuration parameters required for setting up the Nylas integration, including API credentials and date range settings.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/nylas.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `api_server` | `string` | API Server.  |  |\n| `start_date` | `string` | Start date.  |  |\n| `end_date` | `string` | End date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Checking if airbyte-ci is installed\nDESCRIPTION: Command to verify if the airbyte-ci command is properly installed on the system.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nmake tools.airbyte-ci.check\n```\n\n----------------------------------------\n\nTITLE: Emitted Data Records for 'Sheet6-2000-rows' Stream in JSON\nDESCRIPTION: Displays multiple newline-delimited JSON objects representing records emitted by an Airbyte stream named 'Sheet6-2000-rows'. Each object includes the stream name, the actual data payload (with 'ID' and 'Name' fields), and an 'emitted_at' timestamp indicating when the record was processed. This format is standard for Airbyte message protocols.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"283\",\"Name\":\"BxhOlZjsc\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"284\",\"Name\":\"CMwZNQokF\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"285\",\"Name\":\"VzhxSKSgc\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"286\",\"Name\":\"HDEDArrcV\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"287\",\"Name\":\"oLKyKtZxD\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"288\",\"Name\":\"lquLSGnjh\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"289\",\"Name\":\"CycNjOUYF\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"290\",\"Name\":\"AzuVbTuyd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"291\",\"Name\":\"cUNvTQyPJ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"292\",\"Name\":\"rUGFDFGSe\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"293\",\"Name\":\"NhrjSBjDx\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"294\",\"Name\":\"xYatsZOiy\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"295\",\"Name\":\"LtnWeamBh\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"296\",\"Name\":\"DRFxVOUEM\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"297\",\"Name\":\"WlDPjNmSV\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"298\",\"Name\":\"UQPVwpmAP\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"299\",\"Name\":\"pZfHsWhcp\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"300\",\"Name\":\"lfypxATlf\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"301\",\"Name\":\"yzQLniaXM\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"302\",\"Name\":\"yzkYrMZfE\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"303\",\"Name\":\"jdQQfPocO\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"304\",\"Name\":\"YPyrDClLz\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"305\",\"Name\":\"zhxGHFTve\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"306\",\"Name\":\"ychyaNMtp\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"307\",\"Name\":\"XhpsfGgpF\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"308\",\"Name\":\"lolbURYcW\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"309\",\"Name\":\"IfbRGrrQZ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"310\",\"Name\":\"mehfuZacw\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"311\",\"Name\":\"BlqFvJQHW\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"312\",\"Name\":\"ZYTrVZYuf\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"313\",\"Name\":\"cjRazTbFZ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"314\",\"Name\":\"GaVgqAqfJ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"315\",\"Name\":\"CyEmWFnww\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"316\",\"Name\":\"mkxaZJeZc\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"317\",\"Name\":\"PjhmnGPtm\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"318\",\"Name\":\"kWwiKohgf\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"319\",\"Name\":\"NNbxUfISc\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"320\",\"Name\":\"nKGABysXI\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"321\",\"Name\":\"dPjFlZZph\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"322\",\"Name\":\"pRAFKBoJd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"323\",\"Name\":\"cYitflbQC\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"324\",\"Name\":\"vUTRaXOoM\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"325\",\"Name\":\"OqRoYPDhl\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"326\",\"Name\":\"VpqzAaLtD\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"327\",\"Name\":\"PnZepjSUd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"328\",\"Name\":\"DUrGcxNVo\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"329\",\"Name\":\"NBQtCdTSj\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"330\",\"Name\":\"cWCjPNzpQ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"331\",\"Name\":\"lHIXIadUs\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"332\",\"Name\":\"oqpCeRKVx\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"333\",\"Name\":\"ctBSuXWyn\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"334\",\"Name\":\"tYpcsHixA\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"335\",\"Name\":\"VcBhHdqzQ\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"336\",\"Name\":\"fAwCtjECq\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"337\",\"Name\":\"vfFMyzUwU\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"338\",\"Name\":\"UTrmoEfAG\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"339\",\"Name\":\"NIuNNEGgl\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"340\",\"Name\":\"QfVEDUddB\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"341\",\"Name\":\"WrGjEptQd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"342\",\"Name\":\"WOsMVQcrE\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"343\",\"Name\":\"myPkSbUAt\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"344\",\"Name\":\"mLlCSDRvd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"345\",\"Name\":\"tLkkxQECm\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"346\",\"Name\":\"LKKVdStiP\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"347\",\"Name\":\"jUlspaMCf\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"348\",\"Name\":\"hteUZjGYG\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"349\",\"Name\":\"JKqhqBveO\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"350\",\"Name\":\"rjEiNZfvv\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"351\",\"Name\":\"bQBHfyAoh\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"352\",\"Name\":\"oOmYiqxOL\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"353\",\"Name\":\"ubFqSuHyK\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"354\",\"Name\":\"gLRZWccdO\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"355\",\"Name\":\"BFhkfPOBB\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"356\",\"Name\":\"dZnIsdmSC\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"357\",\"Name\":\"eBUyEYidA\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"358\",\"Name\":\"QawwEZFue\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"359\",\"Name\":\"SNhQeRZtE\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"360\",\"Name\":\"cXXeXTwIR\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"361\",\"Name\":\"VqeYGWhHj\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"362\",\"Name\":\"PRNmOTFQT\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"363\",\"Name\":\"ouksKBUoo\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"364\",\"Name\":\"AVHyzoidp\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"365\",\"Name\":\"mtBKNuLDw\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"366\",\"Name\":\"XgfwBAFIT\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"367\",\"Name\":\"nZObsdLqA\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"368\",\"Name\":\"ueNTtibEM\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"369\",\"Name\":\"SOdbXjaBt\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"370\",\"Name\":\"SKmlaGriU\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"371\",\"Name\":\"CwWvXhgHm\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"372\",\"Name\":\"hAfPCxrMa\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"373\",\"Name\":\"NvFKEuExd\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"374\",\"Name\":\"WUpPLDrDu\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"375\",\"Name\":\"nxzniiRrI\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"376\",\"Name\":\"mxFhwxWEs\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"377\",\"Name\":\"uEVWhhjhx\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"378\",\"Name\":\"LRBoIyotF\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"379\",\"Name\":\"eRfyYlCXN\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"380\",\"Name\":\"oWsGQSpyy\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"381\",\"Name\":\"gDFEggyEB\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"382\",\"Name\":\"iAlcMymyk\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"383\",\"Name\":\"HfUoRYPql\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"384\",\"Name\":\"UDEzylwrU\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"385\",\"Name\":\"aZyaROHnX\"},\"emitted_at\":1673989568000}\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"386\",\"Name\":\"PtClOxnYE\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Building the Financial Modelling Connector for Local Development\nDESCRIPTION: Command to build a development image of the Financial Modelling connector using airbyte-ci. This creates a 'source-financial-modelling:dev' image for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-financial-modelling/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-financial-modelling build\n```\n\n----------------------------------------\n\nTITLE: Building the Shutterstock Source Connector\nDESCRIPTION: Command to build the Shutterstock source connector locally using airbyte-ci. This creates a dev image named 'source-shutterstock:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shutterstock/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-shutterstock build\n```\n\n----------------------------------------\n\nTITLE: Building the Onepagecrm Connector using Bash\nDESCRIPTION: This command uses the `airbyte-ci` tool to build a development Docker image for the `source-onepagecrm` connector. The resulting image will be tagged as `source-onepagecrm:dev` and can be used for local testing and development. Requires `airbyte-ci` to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-onepagecrm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-onepagecrm build\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Klarna Source Connector\nDESCRIPTION: Command to build the Docker image for the Klarna source connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-klarna/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-klarna build\n```\n\n----------------------------------------\n\nTITLE: Building the Copper Source Connector Docker Image using airbyte-ci\nDESCRIPTION: Builds the Docker image for the source-copper connector locally using the airbyte-ci tool. This command packages the connector, making it available on the host machine with the tag 'airbyte/source-copper:dev'. Requires airbyte-ci to be installed.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-copper/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-copper build\n```\n\n----------------------------------------\n\nTITLE: Building the Connector Docker Image using Airbyte-CI - Bash\nDESCRIPTION: Builds a Docker image for the source-google-drive connector via the airbyte-ci CLI tool. Requires airbyte-ci to be installed as described in the Airbyte CI pipelines documentation. The resulting image will be tagged as 'airbyte/source-google-drive:dev' and made available on the local Docker host.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-drive/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-drive build\n```\n\n----------------------------------------\n\nTITLE: Building the Easypromos Source Connector with airbyte-ci\nDESCRIPTION: Command to build the Easypromos source connector locally, creating a dev image (source-easypromos:dev) for testing purposes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-easypromos/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-easypromos build\n```\n\n----------------------------------------\n\nTITLE: Building NoCRM Connector with Airbyte CI\nDESCRIPTION: Command to build a development image of the NoCRM connector using airbyte-ci. Creates a dev image tagged as 'source-nocrm:dev' for local testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-nocrm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-nocrm build\n```\n\n----------------------------------------\n\nTITLE: Building Kvdb Connector Docker Image\nDESCRIPTION: These commands show how to build the Docker image for the Kvdb connector using either airbyte-ci or docker build.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-kvdb build\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t airbyte/destination-kvdb:dev .\n```\n\n----------------------------------------\n\nTITLE: Displaying Configuration Table in Markdown\nDESCRIPTION: This snippet shows a markdown table that lists the configuration inputs required for the Help Scout connector, including client_id, client_secret, and start_date.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/help-scout.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `client_id` | `string` | Application ID.  |  |\n| `client_secret` | `string` | Application Secret.  |  |\n| `start_date` | `string` | Start date.  |  |\n```\n\n----------------------------------------\n\nTITLE: Airbyte TRACE Message Example in JSON\nDESCRIPTION: An example of an Airbyte TRACE message containing stream status information. This message indicates that the stream \"object_test_1\" has completed processing, with a timestamp for when this status was emitted.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v1/data_type_object_test_messages.txt#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": {\"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"object_test_1\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1602637589101}}\n```\n\n----------------------------------------\n\nTITLE: Testing Lob Source Connector with airbyte-ci\nDESCRIPTION: Command to execute acceptance tests for the source-lob connector using airbyte-ci tooling. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-lob/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-lob test\n```\n\n----------------------------------------\n\nTITLE: Running Gridly Connector Unit Tests\nDESCRIPTION: Command to run unit tests for the Gridly connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gridly/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: This command executes the connector's unit tests using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with PyTest\nDESCRIPTION: Command to execute unit tests for the connector using PyTest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-xata/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for source-easypost Connector - Bash\nDESCRIPTION: This code demonstrates how to execute acceptance tests for the source-easypost connector using the airbyte-ci tool from the CLI. It requires prior installation and configuration of airbyte-ci, and assumes the connector build step has been completed. The command will run the predefined set of tests to verify connector correctness and stability locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-easypost/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-easypost test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Convex Connector\nDESCRIPTION: Command to run the full CI test suite for the Convex connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-convex/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-convex test\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Postmarkapp Connector\nDESCRIPTION: Command to run the full test suite for the source-postmarkapp connector using airbyte-ci tool. This validates that all connector functionality is working correctly.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postmarkapp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-postmarkapp test\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Kvdb Connector\nDESCRIPTION: This command runs the full test suite for the Kvdb connector using airbyte-ci.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-kvdb/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-kvdb test\n```\n\n----------------------------------------\n\nTITLE: Testing Zoho Campaign Connector using airbyte-ci\nDESCRIPTION: Command to run acceptance tests for the Zoho Campaign connector using airbyte-ci tool. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zoho-campaign/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-zoho-campaign test\n```\n\n----------------------------------------\n\nTITLE: Running Connector Tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the connector using the airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-aws-datalake/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=destination-aws-datalake test\n```\n\n----------------------------------------\n\nTITLE: Reverting Page Size Constant in Python\nDESCRIPTION: This Python code snippet reverts the `_PAGE_SIZE` constant back to its original value of 1000. This step restores the normal operational behavior of the connector after testing pagination with a smaller page size.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/connector-development/tutorials/custom-python-connector/3-reading-multiple-pages.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    _PAGE_SIZE: int = 1000\n```\n\n----------------------------------------\n\nTITLE: Checking Processor Architecture on Linux\nDESCRIPTION: Command to verify processor architecture on Linux to determine which version of abctl to download. Returns x86_64 for AMD or aarch64 for ARM.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/oss-quickstart.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuname -m\n```\n\n----------------------------------------\n\nTITLE: Testing UserVoice Connector with Airbyte CI\nDESCRIPTION: Command to run acceptance tests for the UserVoice connector using airbyte-ci tool. Executes the test suite to validate connector functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-uservoice/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-uservoice test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: Command to run the full test suite for the Bigcommerce connector using airbyte-ci\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bigcommerce/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bigcommerce test\n```\n\n----------------------------------------\n\nTITLE: Running the CI Test Suite using airbyte-ci (Bash)\nDESCRIPTION: This command uses the 'airbyte-ci' tool to run the full test suite locally for the Gocardless source connector. It requires 'airbyte-ci' to be installed and assumes the necessary configurations (like secrets) are set up as expected by the test environment.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gocardless/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-gocardless test\n```\n\n----------------------------------------\n\nTITLE: Running the Connector CI Test Suite with airbyte-ci (Bash)\nDESCRIPTION: This command runs the full CI test suite for the Orb manifest-only source connector using airbyte-ci. It ensures that any changes to the connector are tested before publishing. Prerequisites include an installed airbyte-ci with appropriate configurations, and the command will execute all registered tests for the connector identified by --name.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-orb/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-orb test\n```\n\n----------------------------------------\n\nTITLE: Running the Test Suite for the Connector with airbyte-ci (Bash)\nDESCRIPTION: This Bash command launches the full test suite for the Coinmarketcap connector locally over the airbyte-ci testing pipeline. It assumes that airbyte-ci is installed and configured. The --name parameter declares which connector's tests to run. Inputs rely on the current project configuration and test definitions, with output being the pass/fail results of the test execution. No external input parameters or output files are necessary unless the test definitions or environment require them.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-coinmarketcap/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-coinmarketcap test\n```\n\n----------------------------------------\n\nTITLE: Configuring Imagga API Input Parameters in Markdown\nDESCRIPTION: Markdown table defining the configuration parameters for the Imagga API connector. It includes API key, API secret, and image URL for detection endpoints.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/imagga.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key. Your Imagga API key, available in your Imagga dashboard. Could be found at `https://imagga.com/profile/dashboard` |  |\n| `api_secret` | `string` | API Secret. Your Imagga API secret, available in your Imagga dashboard. Could be found at `https://imagga.com/profile/dashboard` |  |\n| `img_for_detection` | `string` | Image URL for detection endpoints. An image for detection endpoints | https://imagga.com/static/images/categorization/child-476506_640.jpg |\n```\n\n----------------------------------------\n\nTITLE: Configuring Algolia Authentication and Parameters\nDESCRIPTION: Configuration table showing the required input parameters for setting up the Algolia connector, including API key, application ID, search query parameters, start date, and object ID.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/algolia.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Input | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `api_key` | `string` | API Key.  |  |\n| `application_id` | `string` | Application ID. The application ID for your application found in settings |  |\n| `search_query` | `string` | Indexes Search query. Search query to be used with indexes_query stream with format defined in `https://www.algolia.com/doc/rest-api/search/#tag/Search/operation/searchSingleIndex` | hitsPerPage=2&amp;getRankingInfo=1 |\n| `start_date` | `string` | Start date.  |  |\n| `object_id` | `string` | Object ID. Object ID within index for search queries | ecommerce-sample-data-9999996 |\n```\n\n----------------------------------------\n\nTITLE: Displaying Rockset Connector Configuration Parameters in Markdown\nDESCRIPTION: A markdown table outlining the configuration parameters for the Rockset connector, including api_key, api_server, and workspace.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/rockset.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Parameter  |  Type  | Notes                                                            |\n| :--------- | :----: | :--------------------------------------------------------------- |\n| api_key    | string | rockset api key                                                  |\n| api_server | string | api URL to rockset, specifying http protocol                     |\n| workspace  | string | workspace under which rockset collections will be added/modified |\n```\n\n----------------------------------------\n\nTITLE: Example Output Verifying Local Connector Image (Shell)\nDESCRIPTION: Displays sample output from the `docker images ls | grep ...` command, confirming that a connector image (e.g., `airbyte/destination-s3`) with the `dev` tag exists locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/developing-locally.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nairbyte/destination-s3 | dev | 70516a5908ce | 2 minutes ago | 968MB\n```\n\n----------------------------------------\n\nTITLE: Running unit tests for Recharge connector\nDESCRIPTION: Command to execute unit tests for the Recharge connector using pytest.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-recharge/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute unit tests for the connector using pytest\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-microsoft-dataverse/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest unit_tests\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Table\nDESCRIPTION: A table documenting version history, release dates, pull request references, and change descriptions for the Marketo CDK Connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/marketo.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version  | Date       | Pull Request                                             | Subject                                                                                          |\n|:---------|:-----------|:---------------------------------------------------------|:-------------------------------------------------------------------------------------------------|\n| 1.4.22 | 2025-04-12 | [57702](https://github.com/airbytehq/airbyte/pull/57702) | Update dependencies |\n| 1.4.21 | 2025-04-05 | [57071](https://github.com/airbytehq/airbyte/pull/57071) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Formatting GitHub Changelog in Markdown\nDESCRIPTION: A markdown table displaying the version history of the GitHub connector with dates, pull request references, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/connectors_qa/tests/unit_tests/test_checks/data/docs/correct.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                                                                                      | Subject                                                                                                                                                             |\n|:--------|:-----------|:------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.5.5   | 2023-12-26 | [33783](https://github.com/airbytehq/airbyte/pull/33783)                                                          | Fix retry for 504 error in GraphQL based streams                                                                                                                    |\n```\n\n----------------------------------------\n\nTITLE: Presenting Changelog for GlassFlow Destination in Markdown\nDESCRIPTION: This changelog table shows the version history of the GlassFlow destination, including dates, pull request links, and descriptions of changes made in each version.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/glassflow.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date              | Pull Request                                              | Subject                                         |\n|:--------|:------------------| :-------------------------------------------------------- | :---------------------------------------------- |\n| 0.1.4 | 2025-04-19 | [58260](https://github.com/airbytehq/airbyte/pull/58260) | Update dependencies |\n| 0.1.3 | 2025-04-12 | [57645](https://github.com/airbytehq/airbyte/pull/57645) | Update dependencies |\n| 0.1.2 | 2025-04-05 | [57127](https://github.com/airbytehq/airbyte/pull/57127) | Update dependencies |\n| 0.1.1 | 2025-03-29 | [56578](https://github.com/airbytehq/airbyte/pull/56578) | Update dependencies |\n| 0.1.0   | September 01, 2024 | [\\#7560](https://github.com/airbytehq/airbyte/pull/7560)  | Initial release                                 |\n```\n\n----------------------------------------\n\nTITLE: Processing Unnest Alias Stream in JSON\nDESCRIPTION: These JSON records represent an unnest alias stream with nested structures. Each record contains an 'id' and a 'children' array with nested 'owner' objects that include special characters in column names.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/bases/base-normalization/integration_tests/resources/test_nested_streams/data_input/messages.txt#2025-04-23_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"unnest_alias\",\"data\":{\"id\":1, \"children\": [{\"ab_id\": 1, \"owner\": {\"owner_id\": 1, \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]}},{\"ab_id\": 2, \"owner\": {\"owner_id\": 2, \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]}}]},\"emitted_at\":1623861660}}\n{\"type\":\"RECORD\",\"record\":{\"stream\":\"unnest_alias\",\"data\":{\"id\":2, \"children\": [{\"ab_id\": 3, \"owner\": {\"owner_id\": 3, \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]}},{\"ab_id\": 4, \"owner\": {\"owner_id\": 4, \"column`_'with\\\"_quotes\": [ {\"currency\": \"EUR\" } ]}}]},\"emitted_at\":1623861660}}\n```\n\n----------------------------------------\n\nTITLE: Running Acceptance Tests for Source Piwik Connector using airbyte-ci (Bash)\nDESCRIPTION: Executes the acceptance test suite for the `source-piwik` connector using the `airbyte-ci` tool. This command validates the connector's functionality against predefined test cases. Requires `airbyte-ci` to be installed and the connector to be built locally.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-piwik/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-piwik test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite\nDESCRIPTION: This command runs the full test suite for the connector using airbyte-ci tool.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-rki-covid/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-rki-covid test\n```\n\n----------------------------------------\n\nTITLE: Testing the Repairshopr Connector with airbyte-ci\nDESCRIPTION: This command runs the acceptance tests for the Repairshopr connector using the airbyte-ci tool to verify its functionality.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-repairshopr/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-repairshopr test\n```\n\n----------------------------------------\n\nTITLE: Running CI Test Suite for Opsgenie Connector with airbyte-ci - Bash\nDESCRIPTION: This Bash snippet shows how to execute the complete end-to-end test suite for the Opsgenie source connector using 'airbyte-ci'. It ensures that all connector functionality passes required tests before merging changes or publishing new versions. The command needs 'airbyte-ci' to be installed and run in an environment where the connector's dependencies are met.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-opsgenie/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-opsgenie test\n```\n\n----------------------------------------\n\nTITLE: Testing Bitly Source Connector\nDESCRIPTION: Command to run acceptance tests for the Bitly source connector using airbyte-ci tool. Validates connector functionality through automated testing.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-bitly/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-bitly test\n```\n\n----------------------------------------\n\nTITLE: Running Getlago Connector Tests with airbyte-ci\nDESCRIPTION: Command to run the full test suite for the Getlago source connector using the airbyte-ci tool. This validates connector functionality before submitting changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-getlago/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-getlago test\n```\n\n----------------------------------------\n\nTITLE: Running Connector CI Test Suite with Airbyte CI - Bash\nDESCRIPTION: This bash command uses airbyte-ci to execute the complete test suite for the source-google-analytics-data-api connector. It is meant for validating the connector against Airbyte's CI and pre-deployment pipelines. The tool must be installed, and the '--name' flag selects the specific connector. Proper environment configuration and test dependencies are required prior to invocation.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-analytics-data-api/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nairbyte-ci connectors --name=source-google-analytics-data-api test\n```\n\n----------------------------------------\n\nTITLE: Documenting Imagga API Connector Changelog in Markdown\nDESCRIPTION: Markdown table showing the version history of the Imagga API connector. It includes version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/imagga.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.3 | 2025-04-19 | [58222](https://github.com/airbytehq/airbyte/pull/58222) | Update dependencies |\n| 0.0.2 | 2025-04-12 | [57722](https://github.com/airbytehq/airbyte/pull/57722) | Update dependencies |\n| 0.0.1 | 2025-04-05 | | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Running Postgres Source Connector Performance Test via Gradle (Shell)\nDESCRIPTION: Runs the performance test suite for the Postgres source connector using Gradle. Optional command-line arguments allow limiting the CPU and memory resources for the test runner: '--cpulimit=cpulimit/<limit>' restricts CPU cores (minimum 2), and '--memorylimit=memorylimit/<limit>' restricts RAM (minimum 6MB, must specify units like MB or GB). If not supplied, tests use full system resources as specified in ResourceRequirements.java. Requires Gradle, an appropriate shell environment, and permission to invoke scripts.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-postgres/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew :airbyte-integrations:connectors:source-postgres:performanceTest [--cpulimit=cpulimit/<limit>] [--memorylimit=memorylimit/<limit>]\n```\n\n----------------------------------------\n\nTITLE: Specifying Example Bucket Directory Structure in Text Format\nDESCRIPTION: This snippet visually represents a sample bucket directory structure using plain text indentation and arrows. It helps users understand how files and directories are organized, providing context for constructing matching glob path patterns and for understanding sample data flows through the Airbyte connector. This textual listing does not require any external dependencies and is used purely for documentation and illustration.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/azure-blob-storage.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmyBucket\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv\n```\n\n----------------------------------------\n\nTITLE: Creating MongoDB Credentials File for Testing\nDESCRIPTION: Instructions for creating a credentials file for MongoDB testing. The file should be named 'credentials.json' and placed in the 'secrets' directory. It should contain the MongoDB connection details including host, port, database name, username, and password.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/destination-mongodb/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nAdd host, port, database name, username and password to `secrets/credentials.json` file\n```\n\n----------------------------------------\n\nTITLE: Setup Steps for Airbyte Open Source - Markdown List\nDESCRIPTION: Numbered list of configuration steps for setting up Amazon Seller Partner source in Airbyte Open Source\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-seller-partner.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n1. Navigate to the Airbyte Open Source dashboard.\n2. On the Set up the source page, select Amazon Seller Partner from the Source type dropdown.\n3. Enter a name for the Amazon Seller Partner connector.\n4. Using developer application from Step 1, [generate](https://developer-docs.amazon.com/sp-api/docs/self-authorization) refresh token.\n5. For Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. This field is optional - if not provided, the date 2 years ago from today will be used.\n6. For End Date, enter the date in YYYY-MM-DD format. Any data after this date will not be replicated. This field is optional - if not provided, today's date will be used.\n7. **Financial Events Step Size**: Select the time window size for fetching financial events data.\n8. You can specify report options for each stream using **Report Options** section.\n9. For `Wait between requests to avoid fatal statuses in reports`, enable if you want to use wating time between requests to avoid fatal statuses in report based streams.\n10. Click `Set up source`.\n```\n\n----------------------------------------\n\nTITLE: Running BLC (Broken Link Checker) with Options\nDESCRIPTION: Command to run the Broken Link Checker tool with additional options. The --help flag displays available options and usage information.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/tools/site/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./tools/site/link_checker.sh run --help\n```\n\n----------------------------------------\n\nTITLE: Ingress IP Address Error\nDESCRIPTION: Error message when trying to create an ingress with an IP address instead of a DNS name, which is not supported in older versions of abctl.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nunable to create ingress: Ingress.networking.k8s.io \"ingress-abctl\" is invalid:\nspec.rules[2].host: Invalid value: \"0.0.0.0\": must be a DNS name, not an IP address\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Connection Refused Error\nDESCRIPTION: Error message when abctl cannot communicate with the Kubernetes API server due to a connection refusal.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/troubleshoot-deploy.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nunable to initialize local command: error communicating with kubernetes:\n unable to fetch kubernetes server version: Get \"https://127.0.0.1:50124/version\":\n  dial tcp 127.0.0.1:[PORT]: connect: connection refused\n```\n\n----------------------------------------\n\nTITLE: Example Output of Listing Job Directory Files\nDESCRIPTION: Provides sample output for the `ls /data/9/2/` command executed inside the container. It shows typical files generated during an Airbyte sync attempt, such as configuration files (`catalog.json`, `tap_config.json`, `target_config.json`) and the main log file (`logs.log`).\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/operator-guides/browsing-output-logs.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ncatalog.json\ntap_config.json\nlogs.log\ntarget_config.json\n```\n\n----------------------------------------\n\nTITLE: Managing Changelog Table - Markdown\nDESCRIPTION: This snippet defines a markdown table for maintaining a versioned changelog of a software connector. It captures version numbers, release dates, PR references, and concise descriptions of changes. Suitable for inclusion in project documentation or repositories with no external dependencies and accepts plain text as input and output.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/shopify.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| 0.1.21  | 2021-10-14 | [7382](https://github.com/airbytehq/airbyte/pull/7382)   | Fixed `InventoryLevels` primary key                                                                                                                                                                                                                                                                                                                                                       |\n| 0.1.20  | 2021-10-14 | [7063](https://github.com/airbytehq/airbyte/pull/7063)   | Added `Location` and `InventoryLevels` as streams                                                                                                                                                                                                                                                                                                                                         |\n| 0.1.19  | 2021-10-11 | [6951](https://github.com/airbytehq/airbyte/pull/6951)   | Added support of `OAuth 2.0` authorisation option                                                                                                                                                                                                                                                                                                                                         |\n| 0.1.18  | 2021-09-21 | [6056](https://github.com/airbytehq/airbyte/pull/6056)   | Added `pre_tax_price` to the `orders/line_items` schema                                                                                                                                                                                                                                                                                                                                   |\n| 0.1.17  | 2021-09-17 | [5244](https://github.com/airbytehq/airbyte/pull/5244)   | Created data type enforcer for converting prices into numbers                                                                                                                                                                                                                                                                                                                             |\n| 0.1.16  | 2021-09-09 | [5965](https://github.com/airbytehq/airbyte/pull/5945)   | Fixed the connector's performance for `Incremental refresh`                                                                                                                                                                                                                                                                                                                               |\n| 0.1.15  | 2021-09-02 | [5853](https://github.com/airbytehq/airbyte/pull/5853)   | Fixed `amount` type in `order_refund` schema                                                                                                                                                                                                                                                                                                                                              |\n| 0.1.14  | 2021-09-02 | [5801](https://github.com/airbytehq/airbyte/pull/5801)   | Fixed `line_items/discount allocations` & `duties` parts of `orders` schema                                                                                                                                                                                                                                                                                                               |\n| 0.1.13  | 2021-08-17 | [5470](https://github.com/airbytehq/airbyte/pull/5470)   | Fixed rate limits throttling                                                                                                                                                                                                                                                                                                                                                              |\n| 0.1.12  | 2021-08-09 | [5276](https://github.com/airbytehq/airbyte/pull/5276)   | Added status property to product schema                                                                                                                                                                                                                                                                                                                                                   |\n| 0.1.11  | 2021-07-23 | [4943](https://github.com/airbytehq/airbyte/pull/4943)   | Fixed products schema up to API 2021-07                                                                                                                                                                                                                                                                                                                                                   |\n| 0.1.10  | 2021-07-19 | [4830](https://github.com/airbytehq/airbyte/pull/4830)   | Fixed for streams json schemas, upgrade to API version 2021-07                                                                                                                                                                                                                                                                                                                            |\n| 0.1.9   | 2021-07-04 | [4472](https://github.com/airbytehq/airbyte/pull/4472)   | Incremental sync is now using updated_at instead of since_id by default                                                                                                                                                                                                                                                                                                                   |\n| 0.1.8   | 2021-06-29 | [4121](https://github.com/airbytehq/airbyte/pull/4121)   | Added draft orders stream                                                                                                                                                                                                                                                                                                                                                                 |\n| 0.1.7   | 2021-06-26 | [4290](https://github.com/airbytehq/airbyte/pull/4290)   | Fixed the bug when limiting output records to 1 caused infinity loop                                                                                                                                                                                                                                                                                                                      |\n| 0.1.6   | 2021-06-24 | [4009](https://github.com/airbytehq/airbyte/pull/4009)   | Added pages, price rules and discount codes streams                                                                                                                                                                                                                                                                                                                                       |\n| 0.1.5   | 2021-06-10 | [3973](https://github.com/airbytehq/airbyte/pull/3973)   | Added `AIRBYTE_ENTRYPOINT` for Kubernetes support                                                                                                                                                                                                                                                                                                                                         |\n| 0.1.4   | 2021-06-09 | [3926](https://github.com/airbytehq/airbyte/pull/3926)   | New attributes to Orders schema                                                                                                                                                                                                                                                                                                                                                           |\n| 0.1.3   | 2021-06-08 | [3787](https://github.com/airbytehq/airbyte/pull/3787)   | Added Native Shopify Source Connector                                                                                                                                                                                                                                                                                                                                                     |\n```\n\n----------------------------------------\n\nTITLE: Documenting Statsig Connector Changelog in Markdown\nDESCRIPTION: Provides a detailed changelog of version updates for the Statsig connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/statsig.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n|---------|------|--------------|---------|\n| 0.0.20 | 2025-04-19 | [58381](https://github.com/airbytehq/airbyte/pull/58381) | Update dependencies |\n| 0.0.19 | 2025-04-12 | [57947](https://github.com/airbytehq/airbyte/pull/57947) | Update dependencies |\n| 0.0.18 | 2025-04-05 | [57408](https://github.com/airbytehq/airbyte/pull/57408) | Update dependencies |\n| 0.0.17 | 2025-03-29 | [56878](https://github.com/airbytehq/airbyte/pull/56878) | Update dependencies |\n| 0.0.16 | 2025-03-22 | [56319](https://github.com/airbytehq/airbyte/pull/56319) | Update dependencies |\n| 0.0.15 | 2025-03-08 | [55596](https://github.com/airbytehq/airbyte/pull/55596) | Update dependencies |\n| 0.0.14 | 2025-03-01 | [55104](https://github.com/airbytehq/airbyte/pull/55104) | Update dependencies |\n| 0.0.13 | 2025-02-22 | [54492](https://github.com/airbytehq/airbyte/pull/54492) | Update dependencies |\n| 0.0.12 | 2025-02-15 | [54101](https://github.com/airbytehq/airbyte/pull/54101) | Update dependencies |\n| 0.0.11 | 2025-02-08 | [53586](https://github.com/airbytehq/airbyte/pull/53586) | Update dependencies |\n| 0.0.10 | 2025-02-01 | [53076](https://github.com/airbytehq/airbyte/pull/53076) | Update dependencies |\n| 0.0.9 | 2025-01-25 | [52458](https://github.com/airbytehq/airbyte/pull/52458) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [51986](https://github.com/airbytehq/airbyte/pull/51986) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51440](https://github.com/airbytehq/airbyte/pull/51440) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50755](https://github.com/airbytehq/airbyte/pull/50755) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50310](https://github.com/airbytehq/airbyte/pull/50310) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49776](https://github.com/airbytehq/airbyte/pull/49776) | Update dependencies |\n| 0.0.3 | 2024-12-12 | [49419](https://github.com/airbytehq/airbyte/pull/49419) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47473](https://github.com/airbytehq/airbyte/pull/47473) | Update dependencies |\n| 0.0.1 | 2024-09-27 | | Initial release by [@topefolorunso](https://github.com/topefolorunso) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Formatting Markdown Table for BigQuery Connector Changelog\nDESCRIPTION: This code snippet represents a markdown table containing the version history and changelog for the BigQuery destination connector. Each row includes the version number, release date, pull request link, and a brief description of the changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/bigquery.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| 1.3.3   | 2023-04-27 | [\\#25346](https://github.com/airbytehq/airbyte/pull/25346) | Internal code cleanup                                                                                                                                                            |\n| 1.3.1   | 2023-04-20 | [\\#25097](https://github.com/airbytehq/airbyte/pull/25097) | Internal scaffolding change for future development                                                                                                                               |\n| 1.3.0   | 2023-04-19 | [\\#25287](https://github.com/airbytehq/airbyte/pull/25287) | Add parameter to configure the number of file buffers when GCS is used as the loading method                                                                                     |\n| 1.2.20  | 2023-04-12 | [\\#25122](https://github.com/airbytehq/airbyte/pull/25122) | Add additional data centers                                                                                                                                                      |\n| 1.2.19  | 2023-03-29 | [\\#24671](https://github.com/airbytehq/airbyte/pull/24671) | Fail faster in certain error cases                                                                                                                                               |\n| 1.2.18  | 2023-03-23 | [\\#24447](https://github.com/airbytehq/airbyte/pull/24447) | Set the Service Account Key JSON field to always_show: true so that it isn't collapsed into an optional fields section                                                           |\n| 1.2.17  | 2023-03-17 | [\\#23788](https://github.com/airbytehq/airbyte/pull/23788) | S3-Parquet: added handler to process null values in arrays                                                                                                                       |\n| 1.2.16  | 2023-03-10 | [\\#23931](https://github.com/airbytehq/airbyte/pull/23931) | Added support for periodic buffer flush                                                                                                                                          |\n| 1.2.15  | 2023-03-10 | [\\#23466](https://github.com/airbytehq/airbyte/pull/23466) | Changed S3 Avro type from Int to Long                                                                                                                                            |\n| 1.2.14  | 2023-02-08 | [\\#22497](https://github.com/airbytehq/airbyte/pull/22497) | Fixed table already exists error                                                                                                                                                 |\n| 1.2.13  | 2023-01-26 | [\\#20631](https://github.com/airbytehq/airbyte/pull/20631) | Added support for destination checkpointing with staging                                                                                                                         |\n| 1.2.12  | 2023-01-18 | [\\#21087](https://github.com/airbytehq/airbyte/pull/21087) | Wrap Authentication Errors as Config Exceptions                                                                                                                                  |\n| 1.2.11  | 2023-01-18 | [\\#21144](https://github.com/airbytehq/airbyte/pull/21144) | Added explicit error message if sync fails due to a config issue                                                                                                                 |\n| 1.2.9   | 2022-12-14 | [\\#20501](https://github.com/airbytehq/airbyte/pull/20501) | Report GCS staging failures that occur during connection check                                                                                                                   |\n| 1.2.8   | 2022-11-22 | [\\#19489](https://github.com/airbytehq/airbyte/pull/19489) | Added non-billable projects handle to check connection stage                                                                                                                     |\n| 1.2.7   | 2022-11-11 | [\\#19358](https://github.com/airbytehq/airbyte/pull/19358) | Fixed check method to capture mismatch dataset location                                                                                                                          |\n| 1.2.6   | 2022-11-10 | [\\#18554](https://github.com/airbytehq/airbyte/pull/18554) | Improve check connection method to handle more errors                                                                                                                            |\n| 1.2.5   | 2022-10-19 | [\\#18162](https://github.com/airbytehq/airbyte/pull/18162) | Improve error logs                                                                                                                                                               |\n| 1.2.4   | 2022-09-26 | [\\#16890](https://github.com/airbytehq/airbyte/pull/16890) | Add user-agent header                                                                                                                                                            |\n| 1.2.3   | 2022-09-22 | [\\#17054](https://github.com/airbytehq/airbyte/pull/17054) | Respect stream namespace                                                                                                                                                         |\n| 1.2.1   | 2022-09-14 | [\\#15668](https://github.com/airbytehq/airbyte/pull/15668) | (bugged, do not use) Wrap logs in AirbyteLogMessage                                                                                                                              |\n| 1.2.0   | 2022-09-09 | [\\#14023](https://github.com/airbytehq/airbyte/pull/14023) | (bugged, do not use) Cover arrays only if they are nested                                                                                                                        |\n| 1.1.16  | 2022-09-01 | [\\#16243](https://github.com/airbytehq/airbyte/pull/16243) | Fix Json to Avro conversion when there is field name clash from combined restrictions (`anyOf`, `oneOf`, `allOf` fields)                                                         |\n| 1.1.15  | 2022-08-22 | [\\#15787](https://github.com/airbytehq/airbyte/pull/15787) | Throw exception if job failed                                                                                                                                                    |\n| 1.1.14  | 2022-08-03 | [\\#14784](https://github.com/airbytehq/airbyte/pull/14784) | Enabling Application Default Credentials                                                                                                                                         |\n| 1.1.13  | 2022-08-02 | [\\#14801](https://github.com/airbytehq/airbyte/pull/14801) | Fix multiple log bindings                                                                                                                                                        |\n| 1.1.12  | 2022-08-02 | [\\#15180](https://github.com/airbytehq/airbyte/pull/15180) | Fix standard loading mode                                                                                                                                                        |\n| 1.1.11  | 2022-06-24 | [\\#14114](https://github.com/airbytehq/airbyte/pull/14114) | Remove \"additionalProperties\": false from specs for connectors with staging                                                                                                      |\n| 1.1.10  | 2022-06-16 | [\\#13852](https://github.com/airbytehq/airbyte/pull/13852) | Updated stacktrace format for any trace message errors                                                                                                                           |\n| 1.1.9   | 2022-06-17 | [\\#13753](https://github.com/airbytehq/airbyte/pull/13753) | Deprecate and remove PART_SIZE_MB fields from connectors based on StreamTransferManager                                                                                          |\n| 1.1.8   | 2022-06-07 | [\\#13579](https://github.com/airbytehq/airbyte/pull/13579) | Always check GCS bucket for GCS loading method to catch invalid HMAC keys.                                                                                                       |\n| 1.1.7   | 2022-06-07 | [\\#13424](https://github.com/airbytehq/airbyte/pull/13424) | Reordered fields for specification.                                                                                                                                              |\n| 1.1.6   | 2022-05-15 | [\\#12768](https://github.com/airbytehq/airbyte/pull/12768) | Clarify that the service account key json field is required on cloud.                                                                                                            |\n| 1.1.5   | 2022-05-12 | [\\#12805](https://github.com/airbytehq/airbyte/pull/12805) | Updated to latest base-java to emit AirbyteTraceMessage on error.                                                                                                                |\n| 1.1.4   | 2022-05-04 | [\\#12578](https://github.com/airbytehq/airbyte/pull/12578) | In JSON to Avro conversion, log JSON field values that do not follow Avro schema for debugging.                                                                                  |\n| 1.1.3   | 2022-05-02 | [\\#12528](https://github.com/airbytehq/airbyte/pull/12528) | Update Dataset location field description                                                                                                                                        |\n| 1.1.2   | 2022-04-29 | [\\#12477](https://github.com/airbytehq/airbyte/pull/12477) | Dataset location is a required field                                                                                                                                             |\n```\n\n----------------------------------------\n\nTITLE: Defining Standard 'For Airbyte Open Source' Setup Instructions in Markdown\nDESCRIPTION: This snippet presents the standard Markdown template for the 'For Airbyte Open Source:' section in Airbyte connector documentation. It provides the initial step ('Navigate to the Airbyte Open Source dashboard.') for users setting up the connector within the Airbyte Open Source environment. This ensures a consistent starting point for OSS users.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n\n1. Navigate to the Airbyte Open Source dashboard.\n\n\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Data for Sheet6-2000-rows\nDESCRIPTION: Each JSON object represents a row of data from Sheet6-2000-rows. The objects contain an ID (numeric), Name (alphanumeric string), and emitted_at timestamp (Unix timestamp in milliseconds). This format is commonly used for data ingestion in ETL processes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-google-sheets/integration_tests/expected_records.txt#2025-04-23_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stream\":\"Sheet6-2000-rows\",\"data\":{\"ID\":\"595\",\"Name\":\"PmKzzHAbW\"},\"emitted_at\":1673989568000}\n```\n\n----------------------------------------\n\nTITLE: Checking Python 3.10 installation location\nDESCRIPTION: Command to check if Python 3.10 is installed and accessible in the system PATH.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\nwhich python3.10\n```\n\n----------------------------------------\n\nTITLE: Version History Table in Markdown\nDESCRIPTION: A markdown table showing version history with dates, pull request references, and descriptions of changes made to the Airbyte codebase.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/destinations/redshift.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| 0.3.34  | 2022-05-16 | [\\#12869](https://github.com/airbytehq/airbyte/pull/12869) | Fixed NPE in S3 staging check                                                                                                                                                                                    |\n| 0.3.33  | 2022-05-04 | [\\#12601](https://github.com/airbytehq/airbyte/pull/12601) | Apply buffering strategy for S3 staging                                                                                                                                                                          |\n| 0.3.32  | 2022-04-20 | [\\#12085](https://github.com/airbytehq/airbyte/pull/12085) | Fixed bug with switching between INSERT and COPY config                                                                                                                                                          |\n| 0.3.31  | 2022-04-19 | [\\#12064](https://github.com/airbytehq/airbyte/pull/12064) | Added option to support SUPER datatype in _airbyte_raw*** table                                                                                                                                              |\n| 0.3.29  | 2022-04-05 | [\\#11729](https://github.com/airbytehq/airbyte/pull/11729) | Fixed bug with dashes in schema name                                                                                                                                                                             |\n| 0.3.28  | 2022-03-18 | [\\#11254](https://github.com/airbytehq/airbyte/pull/11254) | Fixed missing records during S3 staging                                                                                                                                                                          |\n| 0.3.27  | 2022-02-25 | [\\#10421](https://github.com/airbytehq/airbyte/pull/10421) | Refactor JDBC parameters handling                                                                                                                                                                                |\n| 0.3.25  | 2022-02-14 | [\\#9920](https://github.com/airbytehq/airbyte/pull/9920)   | Updated the size of staging files for S3 staging. Also, added closure of S3 writers to staging files when data has been written to an staging file.                                                              |\n| 0.3.24  | 2022-02-14 | [\\#10256](https://github.com/airbytehq/airbyte/pull/10256) | Add `-XX:+ExitOnOutOfMemoryError` JVM option                                                                                                                                                                     |\n| 0.3.23  | 2021-12-16 | [\\#8855](https://github.com/airbytehq/airbyte/pull/8855)   | Add `purgeStagingData` option to enable/disable deleting the staging data                                                                                                                                        |\n| 0.3.22  | 2021-12-15 | [\\#8607](https://github.com/airbytehq/airbyte/pull/8607)   | Accept a path for the staging data                                                                                                                                                                               |\n| 0.3.21  | 2021-12-10 | [\\#8562](https://github.com/airbytehq/airbyte/pull/8562)   | Moving classes around for better dependency management                                                                                                                                                           |\n| 0.3.20  | 2021-11-08 | [\\#7719](https://github.com/airbytehq/airbyte/pull/7719)   | Improve handling of wide rows by buffering records based on their byte size rather than their count                                                                                                              |\n| 0.3.19  | 2021-10-21 | [\\#7234](https://github.com/airbytehq/airbyte/pull/7234)   | Allow SSL traffic only                                                                                                                                                                                           |\n| 0.3.17  | 2021-10-12 | [\\#6965](https://github.com/airbytehq/airbyte/pull/6965)   | Added SSL Support                                                                                                                                                                                                |\n| 0.3.16  | 2021-10-11 | [\\#6949](https://github.com/airbytehq/airbyte/pull/6949)   | Each stream was split into files of 10,000 records each for copying using S3 or GCS                                                                                                                              |\n| 0.3.14  | 2021-10-08 | [\\#5924](https://github.com/airbytehq/airbyte/pull/5924)   | Fixed AWS S3 Staging COPY is writing records from different table in the same raw table                                                                                                                          |\n| 0.3.13  | 2021-09-02 | [\\#5745](https://github.com/airbytehq/airbyte/pull/5745)   | Disable STATUPDATE flag when using S3 staging to speed up performance                                                                                                                                            |\n| 0.3.12  | 2021-07-21 | [\\#3555](https://github.com/airbytehq/airbyte/pull/3555)   | Enable partial checkpointing for halfway syncs                                                                                                                                                                   |\n| 0.3.11  | 2021-07-20 | [\\#4874](https://github.com/airbytehq/airbyte/pull/4874)   | allow `additionalProperties` in connector spec                                                                                                                                                                   |\n```\n\n----------------------------------------\n\nTITLE: Changelog Version Table\nDESCRIPTION: Table showing version history, dates, pull request references and change descriptions for the connector\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/zendesk-talk.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date       | Pull Request                                             | Subject                                                                     |\n|:--------|:-----------|:---------------------------------------------------------|:----------------------------------------------------------------------------|\n| 1.2.4 | 2025-04-19 | [58541](https://github.com/airbytehq/airbyte/pull/58541) | Update dependencies |\n| 1.2.3 | 2025-04-13 | [58053](https://github.com/airbytehq/airbyte/pull/58053) | Update dependencies |\n```\n\n----------------------------------------\n\nTITLE: Documenting SparkPost Connector Changelog in Markdown\nDESCRIPTION: Provides a detailed changelog of version updates for the SparkPost connector, including version numbers, dates, pull request links, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sparkpost.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.21 | 2025-04-19 | [58432](https://github.com/airbytehq/airbyte/pull/58432) | Update dependencies |\n| 0.0.20 | 2025-04-12 | [57982](https://github.com/airbytehq/airbyte/pull/57982) | Update dependencies |\n| 0.0.19 | 2025-04-05 | [57486](https://github.com/airbytehq/airbyte/pull/57486) | Update dependencies |\n| 0.0.18 | 2025-03-29 | [56848](https://github.com/airbytehq/airbyte/pull/56848) | Update dependencies |\n| 0.0.17 | 2025-03-22 | [56281](https://github.com/airbytehq/airbyte/pull/56281) | Update dependencies |\n| 0.0.16 | 2025-03-08 | [55575](https://github.com/airbytehq/airbyte/pull/55575) | Update dependencies |\n| 0.0.15 | 2025-03-01 | [55085](https://github.com/airbytehq/airbyte/pull/55085) | Update dependencies |\n| 0.0.14 | 2025-02-22 | [54471](https://github.com/airbytehq/airbyte/pull/54471) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [53553](https://github.com/airbytehq/airbyte/pull/53553) | Update dependencies |\n| 0.0.12 | 2025-02-01 | [53054](https://github.com/airbytehq/airbyte/pull/53054) | Update dependencies |\n| 0.0.11 | 2025-01-25 | [52452](https://github.com/airbytehq/airbyte/pull/52452) | Update dependencies |\n| 0.0.10 | 2025-01-18 | [52022](https://github.com/airbytehq/airbyte/pull/52022) | Update dependencies |\n| 0.0.9 | 2025-01-11 | [51435](https://github.com/airbytehq/airbyte/pull/51435) | Update dependencies |\n| 0.0.8 | 2024-12-28 | [50792](https://github.com/airbytehq/airbyte/pull/50792) | Update dependencies |\n| 0.0.7 | 2024-12-21 | [50312](https://github.com/airbytehq/airbyte/pull/50312) | Update dependencies |\n| 0.0.6 | 2024-12-14 | [49741](https://github.com/airbytehq/airbyte/pull/49741) | Update dependencies |\n| 0.0.5 | 2024-12-12 | [49398](https://github.com/airbytehq/airbyte/pull/49398) | Update dependencies |\n| 0.0.4 | 2024-11-04 | [48315](https://github.com/airbytehq/airbyte/pull/48315) | Update dependencies |\n| 0.0.3 | 2024-10-29 | [47815](https://github.com/airbytehq/airbyte/pull/47815) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47612](https://github.com/airbytehq/airbyte/pull/47612) | Update dependencies |\n| 0.0.1 | 2024-10-22 | | Initial release by [@bishalbera](https://github.com/bishalbera) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: JSON Trace Message\nDESCRIPTION: Stream status trace message indicating completion of the problematic_types stream.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/java/airbyte-cdk/db-destinations/src/testFixtures/resources/v0/problematic_types_messages_in.txt#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"TRACE\", \"trace\": { \"type\": \"STREAM_STATUS\", \"stream_status\": {\"stream_descriptor\": {\"name\": \"problematic_types\"}, \"status\": \"COMPLETE\"}, \"emitted_at\": 1721428636000}}\n```\n\n----------------------------------------\n\nTITLE: Supported Features Table in Markdown\nDESCRIPTION: A markdown table listing the features supported by the Google Workspace Admin Reports connector, including Full Refresh Sync, Incremental Sync, SSL connection, and Namespaces.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/google-workspace-admin-reports.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| SSL connection    | Yes                  |       |\n| Namespaces        | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Defining Required Documentation Header Structure in Markdown\nDESCRIPTION: This Markdown snippet outlines the mandatory structure, naming, and order of headers for Airbyte connector documentation, based on a standard template. It ensures consistency across connector docs by defining sections like Prerequisites, Setup guide, Supported sync modes, and Changelog. Placeholders like `CONNECTOR_NAME_FROM_METADATA` indicate where connector-specific names should be inserted. Some headers are optional but must maintain the correct order if included.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n\n\n  # CONNECTOR_NAME_FROM_METADATA\n\n  ## Prerequisites\n\n  ## Setup guide\n\n  ## Set up CONNECTOR_NAME_FROM_METADATA\n\n  ### For Airbyte Cloud:\n\n  ### For Airbyte Open Source:\n\n  ### CONNECTOR_SPECIFIC_FEATURES\n\n  ## Set up the CONNECTOR_NAME_FROM_METADATA connector in Airbyte\n\n  ### For Airbyte Cloud:\n\n  ### For Airbyte Open Source:\n\n  ## CONNECTOR_SPECIFIC_FEATURES\n\n  ## Supported sync modes\n\n  ## Supported Streams\n\n  ## CONNECTOR_SPECIFIC_FEATURES\n\n  ### Performance considerations\n\n  ## Data type map\n\n  ## Limitations & Troubleshooting\n\n  ### CONNECTOR_SPECIFIC_FEATURES\n\n  ### Tutorials\n\n  ## Changelog\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Defining Standard 'Tutorials' Section Content in Markdown\nDESCRIPTION: This snippet defines the standard Markdown template for the 'Tutorials' section within Airbyte connector documentation. It provides a standard introductory sentence encouraging users to check out tutorials related to the specific `CONNECTOR_NAME_FROM_METADATA` (placeholder for the connector name) after setup. This ensures consistency in how users are directed to further learning resources.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/resources/qa-checks.md#2025-04-23_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n\nNow that you have set up the CONNECTOR_NAME_FROM_METADATA source connector, check out the following CONNECTOR_NAME_FROM_METADATA tutorials:\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Main documentation page markdown structure including sidebar configuration, grid layouts, and custom components for navigation cards and video embedding.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/readme.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ndisplayed_sidebar: docs\n---\n\n# Welcome to Airbyte Docs\n\n<Grid columns=\"1\">\n\n<CardWithIcon title=\"What is Airbyte?\" description=\"Airbyte is an open-source data movement infrastructure for building extract and load (EL) data pipelines. It is designed for versatility, scalability, and ease-of-use.\" icon=\"enterprise\"/>\n\n</Grid>\n```\n\n----------------------------------------\n\nTITLE: Rendering Upcoming Academy Courses with React Grid Component\nDESCRIPTION: This snippet displays a responsive grid showing upcoming courses (API and Python) that are not yet available. It uses the same Grid and CardWithIcon components but with a 'Coming soon!' message and a lock icon.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/using-airbyte/getting-started/academy.md#2025-04-23_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<Grid columns=\"2\">\n    <CardWithIcon title=\"API\" description=\"Coming soon!\" icon=\"fa-lock\" />\n    <CardWithIcon title=\"Python\" description=\"Coming soon!\" icon=\"fa-lock\" />\n</Grid>\n```\n\n----------------------------------------\n\nTITLE: Documenting Thinkific Connector Changelog in Markdown\nDESCRIPTION: This snippet presents the version history of the Thinkific connector, including update dates, pull request numbers, and descriptions of changes.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/thinkific.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.18 | 2025-04-12 | [57967](https://github.com/airbytehq/airbyte/pull/57967) | Update dependencies |\n| 0.0.17 | 2025-03-22 | [56271](https://github.com/airbytehq/airbyte/pull/56271) | Update dependencies |\n| 0.0.16 | 2025-03-08 | [55579](https://github.com/airbytehq/airbyte/pull/55579) | Update dependencies |\n| 0.0.15 | 2025-03-01 | [55087](https://github.com/airbytehq/airbyte/pull/55087) | Update dependencies |\n| 0.0.14 | 2025-02-22 | [54536](https://github.com/airbytehq/airbyte/pull/54536) | Update dependencies |\n| 0.0.13 | 2025-02-15 | [54048](https://github.com/airbytehq/airbyte/pull/54048) | Update dependencies |\n| 0.0.12 | 2025-02-08 | [53580](https://github.com/airbytehq/airbyte/pull/53580) | Update dependencies |\n| 0.0.11 | 2025-02-01 | [53038](https://github.com/airbytehq/airbyte/pull/53038) | Update dependencies |\n| 0.0.10 | 2025-01-25 | [52416](https://github.com/airbytehq/airbyte/pull/52416) | Update dependencies |\n| 0.0.9 | 2025-01-18 | [51962](https://github.com/airbytehq/airbyte/pull/51962) | Update dependencies |\n| 0.0.8 | 2025-01-11 | [51452](https://github.com/airbytehq/airbyte/pull/51452) | Update dependencies |\n| 0.0.7 | 2024-12-28 | [50799](https://github.com/airbytehq/airbyte/pull/50799) | Update dependencies |\n| 0.0.6 | 2024-12-21 | [50338](https://github.com/airbytehq/airbyte/pull/50338) | Update dependencies |\n| 0.0.5 | 2024-12-14 | [49798](https://github.com/airbytehq/airbyte/pull/49798) | Update dependencies |\n| 0.0.4 | 2024-12-12 | [49377](https://github.com/airbytehq/airbyte/pull/49377) | Update dependencies |\n| 0.0.3 | 2024-11-04 | [48142](https://github.com/airbytehq/airbyte/pull/48142) | Update dependencies |\n| 0.0.2 | 2024-10-29 | [47525](https://github.com/airbytehq/airbyte/pull/47525) | Update dependencies |\n| 0.0.1 | 2024-10-07 | | Initial release by [@parthiv11](https://github.com/parthiv11) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: MIT License Text for Airbyte Project\nDESCRIPTION: The full text of the MIT License that grants permission to use, modify, and distribute the software under certain conditions. This license applies to code originally from the write-good project by Brian Ford from 2014.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/vale-styles/write-good/README.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nThe MIT License (MIT)\n\nCopyright (c) 2014 Brian Ford\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Adding Product Badges in Markdown\nDESCRIPTION: Demonstrates how to specify which Airbyte products a documentation page applies to using the 'products' metadata tag. This example shows how to highlight only the Self-Managed Community badge.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n---\nproducts: oss-community\n---\n\n# This topic is only for Self-Managed Community\n\nSome text.\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Connector Directory\nDESCRIPTION: Command to change directory to the Fauna source connector location within the Airbyte repository.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-fauna/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd airbyte-integrations/connectors/source-fauna\n```\n\n----------------------------------------\n\nTITLE: Gorgias Source Connector Changelog\nDESCRIPTION: This table provides a detailed changelog of the Gorgias source connector, including version numbers, dates, pull request links, and descriptions of changes made in each update.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/gorgias.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Date | Pull Request | Subject |\n| ------------------ | ------------ | --- | ---------------- |\n| 0.1.9 | 2025-04-19 | [58193](https://github.com/airbytehq/airbyte/pull/58193) | Update dependencies |\n| 0.1.8 | 2025-04-12 | [57708](https://github.com/airbytehq/airbyte/pull/57708) | Update dependencies |\n| 0.1.7 | 2025-04-05 | [57041](https://github.com/airbytehq/airbyte/pull/57041) | Update dependencies |\n| 0.1.6 | 2025-03-29 | [56719](https://github.com/airbytehq/airbyte/pull/56719) | Update dependencies |\n| 0.1.5 | 2025-03-22 | [56041](https://github.com/airbytehq/airbyte/pull/56041) | Update dependencies |\n| 0.1.4 | 2025-03-08 | [55491](https://github.com/airbytehq/airbyte/pull/55491) | Update dependencies |\n| 0.1.3 | 2025-03-01 | [54794](https://github.com/airbytehq/airbyte/pull/54794) | Update dependencies |\n| 0.1.2 | 2025-02-22 | [54335](https://github.com/airbytehq/airbyte/pull/54335) | Update dependencies |\n| 0.1.1 | 2025-02-15 | [50638](https://github.com/airbytehq/airbyte/pull/50638) | Update dependencies |\n| 0.1.0 | 2025-01-30 | [52637](https://github.com/airbytehq/airbyte/pull/52637) | Add retries for rate limited streams |\n| 0.0.8 | 2024-12-23 | [49935](https://github.com/airbytehq/airbyte/pull/49935) | Add additional cursor datetime format |\n| 0.0.7 | 2024-12-21 | [50123](https://github.com/airbytehq/airbyte/pull/50123) | Update dependencies |\n| 0.0.6 | 2024-12-14 | [49219](https://github.com/airbytehq/airbyte/pull/49219) | Update dependencies |\n| 0.0.5 | 2024-12-11 | [48973](https://github.com/airbytehq/airbyte/pull/48973) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.4 | 2024-11-06 | [48378](https://github.com/airbytehq/airbyte/pull/48378) | Fix incremental sync format, Auto update schema with additional fields |\n| 0.0.3 | 2024-10-29 | [47923](https://github.com/airbytehq/airbyte/pull/47923) | Update dependencies |\n| 0.0.2 | 2024-10-28 | [47459](https://github.com/airbytehq/airbyte/pull/47459) | Update dependencies |\n| 0.0.1 | 2024-09-29 | [46221](https://github.com/airbytehq/airbyte/pull/46221) | Initial release by [@btkcodedev](https://github.com/btkcodedev) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Taboola API Connector Changelog in Markdown\nDESCRIPTION: This snippet presents the version history and changes made to the Taboola API connector over time, including update details and pull request references.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/taboola.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Version          | Date              | Pull Request | Subject        |\n|------------------|-------------------|--------------|----------------|\n| 0.0.17 | 2025-04-19 | [58430](https://github.com/airbytehq/airbyte/pull/58430) | Update dependencies |\n| 0.0.16 | 2025-04-12 | [57963](https://github.com/airbytehq/airbyte/pull/57963) | Update dependencies |\n| 0.0.15 | 2025-04-05 | [57466](https://github.com/airbytehq/airbyte/pull/57466) | Update dependencies |\n| 0.0.14 | 2025-03-29 | [56829](https://github.com/airbytehq/airbyte/pull/56829) | Update dependencies |\n| 0.0.13 | 2025-03-22 | [56261](https://github.com/airbytehq/airbyte/pull/56261) | Update dependencies |\n| 0.0.12 | 2025-03-08 | [55604](https://github.com/airbytehq/airbyte/pull/55604) | Update dependencies |\n| 0.0.11 | 2025-03-01 | [55149](https://github.com/airbytehq/airbyte/pull/55149) | Update dependencies |\n| 0.0.10 | 2025-02-22 | [54463](https://github.com/airbytehq/airbyte/pull/54463) | Update dependencies |\n| 0.0.9 | 2025-02-15 | [52407](https://github.com/airbytehq/airbyte/pull/52407) | Update dependencies |\n| 0.0.8 | 2025-01-18 | [52000](https://github.com/airbytehq/airbyte/pull/52000) | Update dependencies |\n| 0.0.7 | 2025-01-11 | [51453](https://github.com/airbytehq/airbyte/pull/51453) | Update dependencies |\n| 0.0.6 | 2024-12-28 | [50826](https://github.com/airbytehq/airbyte/pull/50826) | Update dependencies |\n| 0.0.5 | 2024-12-21 | [50356](https://github.com/airbytehq/airbyte/pull/50356) | Update dependencies |\n| 0.0.4 | 2024-12-14 | [49754](https://github.com/airbytehq/airbyte/pull/49754) | Update dependencies |\n| 0.0.3 | 2024-12-12 | [49409](https://github.com/airbytehq/airbyte/pull/49409) | Update dependencies |\n| 0.0.2 | 2024-12-11 | [49114](https://github.com/airbytehq/airbyte/pull/49114) | Starting with this version, the Docker image is now rootless. Please note that this and future versions will not be compatible with Airbyte versions earlier than 0.64 |\n| 0.0.1 | 2024-10-28 | | Initial release by [@aazam-gh](https://github.com/aazam-gh) via Connector Builder |\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Product Badges in Markdown\nDESCRIPTION: Shows how to highlight multiple product badges (Cloud Teams and Self-Managed Enterprise) while graying out others using the 'products' metadata tag.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/contributing-to-airbyte/writing-docs.md#2025-04-23_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n---\nproducts: cloud-teams, oss-enterprise\n---\n\n# This topic is only for Cloud Teams and Self-Managed Enterprise\n\nSome text.\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Airbyte CI\nDESCRIPTION: Command to run the test suite for Airbyte CI from the pipelines directory using Poetry.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block - Sync Initiation\nDESCRIPTION: Markdown code block explaining the data reset and sync initiation process\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/amazon-ads-migrations.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```note\nThis will reset the data in your destination and initiate a fresh sync.\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Apify Dataset Stream Properties\nDESCRIPTION: Markdown table showing supported sync features for the Apify dataset connector.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/apify-dataset.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n```\n\n----------------------------------------\n\nTITLE: Aviationstack Streams Table in Markdown\nDESCRIPTION: Markdown table listing all available data streams with their properties including primary keys, pagination type, and sync support capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/aviationstack.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| airports | id | DefaultPaginator | ✅ |  ❌  |\n| airlines | id | DefaultPaginator | ✅ |  ❌  |\n| airplanes | id | DefaultPaginator | ✅ |  ✅  |\n| aircraft_types | id | DefaultPaginator | ✅ |  ❌  |\n| cities | id | DefaultPaginator | ✅ |  ❌  |\n| countries | id | DefaultPaginator | ✅ |  ❌  |\n| taxes | id | DefaultPaginator | ✅ |  ❌  |\n```\n\n----------------------------------------\n\nTITLE: Displaying Supported Features Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table listing the supported features of the Sonar Cloud API connector, including Full Refresh Sync and Incremental Sync.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/sonar-cloud.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature           | Supported?\\(Yes/No\\) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n```\n\n----------------------------------------\n\nTITLE: Streams Table in Markdown\nDESCRIPTION: Markdown table listing available data streams with their properties including primary keys, pagination support, and sync capabilities.\nSOURCE: https://github.com/airbytehq/airbyte/blob/master/docs/integrations/sources/appfigures.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Stream Name | Primary Key | Pagination | Supports Full Sync | Supports Incremental |\n|-------------|-------------|------------|---------------------|----------------------|\n| status | id | No pagination | ✅ |  ❌  |\n| reports_sales |  | No pagination | ✅ |  ❌  |\n| data_categories |  | No pagination | ✅ |  ❌  |\n| users | id | DefaultPaginator | ✅ |  ✅  |\n| usage |  | DefaultPaginator | ✅ |  ❌  |\n| products_mine_search |  | DefaultPaginator | ✅ |  ❌  |\n| reports_revenue |  | No pagination | ✅ |  ❌  |\n| reports_subscriptions |  | No pagination | ✅ |  ❌  |\n| reports_ads |  | No pagination | ✅ |  ❌  |\n| reports_adspend |  | No pagination | ✅ |  ❌  |\n| reports_ratings |  | No pagination | ✅ |  ❌  |\n```"
  }
]
[
  {
    "owner": "openinterpreter",
    "repo": "open-interpreter",
    "content": "TITLE: Configuring Open Interpreter for Local Use in Python\nDESCRIPTION: Python code to configure Open Interpreter for local use with custom settings for offline mode, model, API key, and base URL.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Implementing FastAPI Server for Open Interpreter\nDESCRIPTION: Example implementation of a FastAPI server that provides chat and history endpoints for Open Interpreter. The server enables HTTP REST control with streaming response support.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# server.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom interpreter import interpreter\n\napp = FastAPI()\n\n@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\n@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages\n```\n\n----------------------------------------\n\nTITLE: Processing User Input in Python Interpreter\nDESCRIPTION: This snippet shows the chat method of the Interpreter class, which processes user input, manages the conversation flow, and generates responses using the language model. It handles streaming responses and updates the conversation history.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/protocols/i-protocol.mdx#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef chat(self, message=None, stream=False, display=True):\n    if message is not None:\n        if isinstance(message, str):\n            message = {\"role\": \"user\", \"content\": message}\n        self.messages.append(message)\n\n    if not self.messages:\n        raise ValueError(\"No messages to generate a response for.\")\n\n    try:\n        if self.local:\n            for chunk in self._local_chat_stream():\n                if stream:\n                    yield chunk\n                elif display:\n                    sys.stdout.write(chunk)\n                    sys.stdout.flush()\n        else:\n            last_response = \"\"\n            function_call = None\n            for chunk in self._azure_chat_completion_stream():\n                delta = chunk.choices[0].delta\n\n                if delta.function_call:\n                    if function_call is None:\n                        function_call = delta.function_call\n                    else:\n                        if delta.function_call.arguments:\n                            function_call.arguments += delta.function_call.arguments\n\n                if delta.content:\n                    last_response += delta.content\n                    if stream:\n                        yield delta.content\n                    elif display:\n                        sys.stdout.write(delta.content)\n                        sys.stdout.flush()\n\n            if function_call:\n                self.messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"function_call\": {\n                            \"name\": function_call.name,\n                            \"arguments\": function_call.arguments,\n                        },\n                    }\n                )\n            else:\n                self.messages.append({\"role\": \"assistant\", \"content\": last_response})\n\n            if stream or display:\n                yield \"\\n\"\n\n            if function_call:\n                # Process function call...\n\n    except Exception as e:\n        self._handle_error(e)\n\n    # More code for handling the conversation...\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with Custom Local Endpoint in Python\nDESCRIPTION: This code snippet demonstrates how to configure Open Interpreter to run locally using a custom endpoint and model in a Python script. It sets the offline mode, specifies the model, and defines the API base URL.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/running-locally.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True\ninterpreter.llm.model = \"ollama/codestral\"\ninterpreter.llm.api_base = \"http://localhost:11434\"\n\ninterpreter.chat(\"how many files are on my desktop?\")\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Command to install Open Interpreter using Python's package manager pip\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/introduction.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Customizing System Message in Python\nDESCRIPTION: Demonstrates how to modify the system message to extend functionality and permissions\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Chats in Python\nDESCRIPTION: Demonstrates how to save chat messages and restore them later in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmessages = interpreter.chat(\"My name is Killian.\") # Save messages to 'messages'\ninterpreter.messages = [] # Reset interpreter (\"Killian\" will be forgotten)\n\ninterpreter.messages = messages # Resume chat from 'messages' (\"Killian\" will be remembered)\n```\n\n----------------------------------------\n\nTITLE: Managing Messages in Open Interpreter\nDESCRIPTION: Shows how to work with message history in Open Interpreter, including viewing conversation logs and restoring previous conversations.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"Hi! Can you print hello world?\")\n\nprint(interpreter.messages)\n\n# This would output:\n\n[\n   {\n      \"role\": \"user\",\n      \"message\": \"Hi! Can you print hello world?\"\n   },\n   {\n      \"role\": \"assistant\",\n      \"message\": \"Sure!\"\n   }\n   {\n      \"role\": \"assistant\",\n      \"language\": \"python\",\n      \"code\": \"print('Hello, World!')\",\n      \"output\": \"Hello, World!\"\n   }\n]\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.messages = messages # A list that resembles the one above\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with Default OpenAI Model\nDESCRIPTION: Basic examples showing how to run Open Interpreter with the default model (gpt-4-turbo) using terminal or Python. The default model is chosen for its code interpretation capabilities.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Response in Python with Open Interpreter\nDESCRIPTION: This code demonstrates how to use the streaming feature of Open Interpreter to get real-time responses from a chat interaction. It prints each chunk of the streamed response, showing the progression of code generation, execution, and message output.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/streaming-response.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor chunk in interpreter.chat(\"What's 34/24?\", stream=True, display=False):\n  print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Launching Open Interpreter\nDESCRIPTION: Command to start the Open Interpreter interactive interface in the terminal\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/introduction.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Using GPT-4o with Open Interpreter\nDESCRIPTION: Examples demonstrating how to use the GPT-4o model with Open Interpreter through terminal commands or Python. This shows compatibility with any model listed on OpenAI's models page.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model gpt-4o\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-4o\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Interpreter with Claude models via Terminal and Python\nDESCRIPTION: This code shows how to initialize Open Interpreter with Claude models using both terminal commands and Python code. It demonstrates the basic setup to start using the Claude instant model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/anthropic.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model claude-instant-1\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"claude-instant-1\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Changing Language Models\nDESCRIPTION: Shows how to switch between different language models in both terminal and Python environments\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with Custom Local Endpoint in Terminal\nDESCRIPTION: This command shows how to run Open Interpreter locally using a custom API endpoint and model via the command line. It sets the API base URL and specifies the model to use.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/running-locally.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base \"http://localhost:11434\" --model ollama/codestral\n```\n\n----------------------------------------\n\nTITLE: Initializing Interactive Chat in Terminal and Python\nDESCRIPTION: Shows how to start an interactive chat session either via terminal command or Python method call\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots in Python using Open Interpreter\nDESCRIPTION: This function takes a screenshot of the primary display using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.display.view()\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python code\nDESCRIPTION: Python code snippet showing how to import and use the interpreter module to execute a single command or start an interactive chat.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"AAPL और META के मानकीकृत स्टॉक मूल्यों का चित्रित करें\") # एकल कमांड को निष्पादित करता है\ninterpreter.chat() # एक इंटरैक्टिव चैट शुरू करता है\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenInterpreter in Python\nDESCRIPTION: Shows the new class instantiation format for using Open Interpreter in Python applications, where you import the OpenInterpreter class and create an instance.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# From the module `interpreter`, import the class `OpenInterpreter`\nfrom interpreter import OpenInterpreter\n\n# Create an instance of `OpenInterpreter` to use it\nagent = OpenInterpreter()\nagent.chat()\n```\n\n----------------------------------------\n\nTITLE: Running Python Code with Open Interpreter\nDESCRIPTION: Demonstrates how to execute Python code directly using Open Interpreter's computer module. The example shows a simple 'Hello World' print statement execution.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/usage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.computer.run(\"python\", \"print('Hello World!')\")\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Command to install Open Interpreter using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Making Multiple OpenInterpreter Instances Communicate in Python\nDESCRIPTION: This code demonstrates how to create a conversational loop between two OpenInterpreter instances, where messages are passed back and forth with roles swapped automatically. It includes a helper function to change message roles between 'user' and 'assistant'.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/multiple-instances.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef swap_roles(messages):\n    for message in messages:\n        if message['role'] == 'user':\n            message['role'] = 'assistant'\n        elif message['role'] == 'assistant':\n            message['role'] = 'user'\n    return messages\n\nagents = [agent_1, agent_2]\n\n# Kick off the conversation\nmessages = [{\"role\": \"user\", \"message\": \"Hello!\"}]\n\nwhile True:\n    for agent in agents:\n        messages = agent.chat(messages)\n        messages = swap_roles(messages)\n```\n\n----------------------------------------\n\nTITLE: Enabling OS Mode in Open Interpreter via Command Line\nDESCRIPTION: Command to launch Open Interpreter with OS Mode enabled. This experimental mode allows the interpreter to control the operating system through mouse and keyboard based on visual information from screen captures.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/os-mode.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --os\n```\n\n----------------------------------------\n\nTITLE: Saving and restoring chat sessions in Open Interpreter\nDESCRIPTION: Python code demonstrating how to save Open Interpreter messages to a variable and restore them later to continue a conversation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages = interpreter.chat(\"मेरा नाम किलियन है।\") # संदेशों को 'messages' में सहेजें\n\ninterpreter.messages = messages # 'messages' से चैट को फिर से शुरू करें (\"किलियन\" याद रखा जाएगा)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Tool to Interpreter\nDESCRIPTION: Executes the custom tool definition in the interpreter's computer environment, making it available for use.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/custom_tool.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.run(\"python\", custom_tool)\n```\n\n----------------------------------------\n\nTITLE: Using Programmatic Chat in Python\nDESCRIPTION: Send specific messages to Open Interpreter and maintain conversation context programmatically in Python. This allows for more precise control over the interaction.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"Add subtitles to all videos in /videos.\")\n\n# ... Displays output in your terminal, completes task ...\n\ninterpreter.chat(\"These look great but can you make the subtitles bigger?\")\n\n# ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with Ollama Models in Python\nDESCRIPTION: Python code to configure and use Open Interpreter with a local Ollama model. This imports the interpreter module, disables online features, sets the model to use, configures the API base URL, and starts the chat interface.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/ollama.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"ollama_chat/<model-name>\"\ninterpreter.llm.api_base = \"http://localhost:11434\"\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Programmatic use of Open Interpreter in Python\nDESCRIPTION: Python code demonstrating programmatic control of Open Interpreter, passing specific messages for execution and following up with additional instructions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"सभी वीडियो में उपशीर्षक जोड़ें /videos में।\")\n\n# ... आपके टर्मिनल में आउटपुट स्ट्रीम करता है, कार्य पूरा करता है ...\n\ninterpreter.chat(\"ये बड़े दिख रहे हैं लेकिन क्या आप उपशीर्षक को और बड़ा कर सकते हैं?\")\n\n# ...\n```\n\n----------------------------------------\n\nTITLE: Programmatic Chat Control in Python\nDESCRIPTION: Demonstrates how to programmatically send messages and chain conversations in Python\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"Add subtitles to all videos in /videos.\")\n\n# ... Displays output in your terminal, completes task ...\n\ninterpreter.chat(\"These look great but can you make the subtitles bigger?\")\n\n# ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Language Model Generator in Python\nDESCRIPTION: Demonstrates how to create an OpenAI-compatible completions function that echoes user messages. The function uses Python generators to stream responses and follows OpenAI's response structure with 'choices' and 'delta' properties.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/custom-models.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef custom_language_model(messages, model, stream, max_tokens):\n    \"\"\"\n    OpenAI-compatible completions function (this one just echoes what the user said back).\n    To make it OpenAI-compatible and parsable, `choices` has to be the root property.\n    The property `delta` is used to signify streaming.\n    \"\"\"\n    users_content = messages[-1].get(\"content\") # Get last message's content\n\n    for character in users_content:\n        yield {\"choices\": [{\"delta\": {\"content\": character}}]}\n\n# Tell Open Interpreter to power the language model with this function\n\ninterpreter.llm.completions = custom_language_model\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter in terminal\nDESCRIPTION: Command to launch the interactive Open Interpreter interface directly from the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Specifying a Different OpenAI Model\nDESCRIPTION: Examples showing how to specify a particular OpenAI model (gpt-3.5-turbo in this case) when using Open Interpreter, using either terminal commands or Python code.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"gpt-3.5-turbo\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Python\nDESCRIPTION: Python code to change the language model used by Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python\nDESCRIPTION: Python code to import and use Open Interpreter for chat functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"Plot AAPL and META's normalized stock prices\") # Executes a single command\ninterpreter.chat() # Starts an interactive chat\n```\n\n----------------------------------------\n\nTITLE: WebSocket Client with Acknowledgment Handling in Python\nDESCRIPTION: Implements a WebSocket client that handles the acknowledgment feature, checking for message IDs and sending acknowledgment messages back to the server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport websockets\n\nasync def handle_messages(websocket):\n    async for message in websocket:\n        data = json.loads(message)\n        \n        # Process the message as usual\n        print(f\"Received: {data}\")\n\n        # Check if the message has an ID that needs to be acknowledged\n        if \"id\" in data:\n            ack_message = {\n                \"ack\": data[\"id\"]\n            }\n            await websocket.send(json.dumps(ack_message))\n            print(f\"Sent acknowledgment for message {data['id']}\")\n\nasync def main():\n    uri = \"ws://localhost:8000\"\n    async with websockets.connect(uri) as websocket:\n        await handle_messages(websocket)\n\n# Run the async function\nimport asyncio\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Multiple OpenInterpreter Instances\nDESCRIPTION: Creates two separate instances of OpenInterpreter with different system messages. Each instance can maintain its own independent state and configuration.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/multiple-instances.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import OpenInterpreter\n\nagent_1 = OpenInterpreter()\nagent_1.system_message = \"This is a separate instance.\"\n\nagent_2 = OpenInterpreter()\nagent_2.system_message = \"This is yet another instance.\"\n```\n\n----------------------------------------\n\nTITLE: Testing Server with cURL Request\nDESCRIPTION: Example cURL command to test the chat endpoint by sending a POST request with a sample prompt.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local_server.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:5001/chat \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"prompt\": \"Hello, how are you?\"}'\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python\nDESCRIPTION: Python code snippet demonstrating how to import and use Open Interpreter for single commands or interactive chats.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_DE.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"Stellen Sie AAPL und METAs normalisierte Aktienpreise dar\") # Führt einen einzelnen Befehl aus\ninterpreter.chat() # Startet einen interaktiven Chat\n```\n\n----------------------------------------\n\nTITLE: Accessing Clipboard Contents in Python using Open Interpreter\nDESCRIPTION: This function retrieves and returns the contents of the clipboard using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.clipboard.view()\n```\n\n----------------------------------------\n\nTITLE: Changing the language model in Open Interpreter\nDESCRIPTION: Python code showing how to manually set a different language model (specifically gpt-3.5-turbo) for Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Open Interpreter with Google Vertex AI Models (Bash)\nDESCRIPTION: Command-line instructions for initializing Open Interpreter with Gemini Pro and Gemini Pro Vision models from Google Vertex AI. These commands set the model to be used by the interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/vertex-ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model gemini-pro\ninterpreter --model gemini-pro-vision\n```\n\n----------------------------------------\n\nTITLE: One-line Mac installer for Open Interpreter\nDESCRIPTION: Experimental one-line installer script for Mac that automatically sets up Python, creates an environment, and installs Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-mac-installer.sh | bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter Settings\nDESCRIPTION: Sets up the Open Interpreter configuration with local LLM settings, including API endpoints and context parameters.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local3.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport subprocess\nfrom interpreter import interpreter\n\ninterpreter.offline = True\n\ninterpreter.llm.model = \"openai/local\" # Tells OI to use an OpenAI-compatible server\ninterpreter.llm.api_key = \"dummy_key\"\ninterpreter.llm.api_base = \"http://localhost:8081/v1\"\ninterpreter.llm.context_window = 7000\ninterpreter.llm.max_tokens = 1000\ninterpreter.llm.supports_functions = False\n```\n\n----------------------------------------\n\nTITLE: Specifying Huggingface API Base URL (Python)\nDESCRIPTION: This Python code shows how to set the API base URL for Huggingface in Open Interpreter programmatically. It imports the interpreter module and sets the api_base attribute of the llm object.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/huggingface.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.api_base = \"https://my-endpoint.huggingface.cloud\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Setting Max Tokens in Open Interpreter\nDESCRIPTION: Sets the maximum number of tokens that the model can generate in a single response.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_tokens 100\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.max_tokens = 100\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  max_tokens: 100\n```\n\n----------------------------------------\n\nTITLE: Using the OpenAI-Compatible Endpoint in Python\nDESCRIPTION: Demonstrates how to use the server's OpenAI-compatible endpoint with the OpenAI Python library by setting the appropriate API base URL.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nopenai.api_base = \"http://localhost:8000/openai\"  # Replace with your server URL if different\nopenai.api_key = \"dummy\"  # The key is not used but required by the OpenAI library\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  # This model name is ignored, but required\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message['content'])\n```\n\n----------------------------------------\n\nTITLE: One-line Linux installer for Open Interpreter\nDESCRIPTION: Experimental one-line installer script for Linux that automatically sets up Python, creates an environment, and installs Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sL https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-linux-installer.sh | bash\n```\n\n----------------------------------------\n\nTITLE: Streaming chat responses in Python\nDESCRIPTION: Example of how to stream chat responses from Open Interpreter in Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmessage = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cohere Models in Open Interpreter via Python\nDESCRIPTION: Python code to configure Open Interpreter to use a Cohere model. This example imports the interpreter module, sets the model to 'command-nightly', and initiates a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cohere.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"command-nightly\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Creating a Dockerfile for Open Interpreter with Python 3.11\nDESCRIPTION: A Dockerfile configuration to create a container image for running Open Interpreter. It uses Python 3.11 as the base image, sets the OPENAI_API_KEY environment variable, and installs the open-interpreter package.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Start with Python 3.11\nFROM python:3.11\n\n# Replace <your_openai_api_key> with your own key\nENV OPENAI_API_KEY <your_openai_api_key>\n\n# Install Open Interpreter\nRUN pip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter in interactive chat mode using Python\nDESCRIPTION: Python code snippet showing how to start an interactive chat session using the interpreter module.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple OpenInterpreter Instances in Python\nDESCRIPTION: This code shows how to create multiple instances of the OpenInterpreter class, each with a unique system message, allowing for different configurations and behaviors of interpreter agents.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/multiple-instances.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import OpenInterpreter\n\nagent_1 = OpenInterpreter()\nagent_1.system_message = \"This is a separate instance.\"\n\nagent_2 = OpenInterpreter()\nagent_2.system_message = \"This is yet another instance.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Temperature in Open Interpreter\nDESCRIPTION: Sets the randomness level of the model's output. Values range from 0 (deterministic) to 1 (more random and creative).\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --temperature 0.7\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.temperature = 0.7\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  temperature: 0.7\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter in Python with Custom LLM Settings\nDESCRIPTION: This Python snippet demonstrates how to configure Open Interpreter with custom LLM settings, including offline mode, model selection, API key, and API base. It also sets max_tokens and context_window values.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/best-practices.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.llm.max_tokens = 1000\ninterpreter.llm.context_window = 3000\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Python for Open Interpreter\nDESCRIPTION: Code snippet demonstrating how to disable telemetry in Open Interpreter when using it as a Python module. This is done by setting the disable_telemetry attribute to True on the interpreter object.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/telemetry/telemetry.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\ninterpreter.disable_telemetry = True\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python\nDESCRIPTION: Python code showing how to import and use Open Interpreter for chat interactions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"Draw a normalized stock price chart of AAPL and META \") # Run a single command\ninterpreter.chat() # Start an interactive chat\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with Replicate Model in Python\nDESCRIPTION: This snippet shows how to set a Replicate model for Open Interpreter using Python. It imports the interpreter module, sets the model, and starts a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/replicate.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Setting Context Window Size in Open Interpreter\nDESCRIPTION: Manually sets the context window size in tokens. Using a smaller context window for local models will use less RAM, making it more suitable for most devices.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --context_window 16000\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.context_window = 16000\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  context_window: 16000\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with Baseten Model\nDESCRIPTION: Examples showing how to set up Open Interpreter with a Baseten model using both command line and Python interfaces. Demonstrates the basic model configuration syntax.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/baseten.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model baseten/<baseten-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"baseten/<baseten-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Profile YAML Configuration\nDESCRIPTION: Example YAML configuration for creating a custom profile in Open Interpreter. This shows how to set custom instructions, specify the LLM model, and adjust the temperature setting.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/profiles.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncustom_instructions: \"Always use python, and be as concise as possible\"\nllm.model: gpt-4\nllm.temperature: 0.5\n# Any other settings you'd like to add\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast Mode\nDESCRIPTION: Configuration for using gpt-3.5-turbo model with automatic code execution without confirmation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --fast\n```\n\nLANGUAGE: yaml\nCODE:\n```\nfast: true\n```\n\n----------------------------------------\n\nTITLE: Connecting Open Interpreter to LM Studio using Python\nDESCRIPTION: Python code to configure Open Interpreter to use a local LM Studio server. This sets up offline mode, specifies the OpenAI-compatible message format, provides a placeholder API key, and points to the local server endpoint.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/lm-studio.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Disabling Conversation History in Python\nDESCRIPTION: Python code that demonstrates how to disable conversation history for a specific conversation session. This is done by setting the conversation_history attribute to False before starting the chat.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/conversation-history.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.conversation_history = False\ninterpreter.chat() # Conversation history will not be saved\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with AWS Sagemaker Model\nDESCRIPTION: Basic commands to set up Open Interpreter with AWS Sagemaker models in terminal or Python. The Python approach requires boto3 installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/aws-sagemaker.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model sagemaker/<model-name>\n```\n\nLANGUAGE: python\nCODE:\n```\n# Sagemaker requires boto3 to be installed on your machine:\n!pip install boto3\n\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"sagemaker/<model-name>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: API Key Configuration\nDESCRIPTION: Sets the API key for authentication with the model service.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_key \"your_api_key_here\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\napi_key: your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Starting Local Server and Running Open Interpreter\nDESCRIPTION: Initializes the local server with the Llama model and starts the Open Interpreter chat interface. Includes async output handling and process management.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local3.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nimport threading\nimport os\n\ndef read_output(process):\n    while True:\n        output = process.stdout.readline()\n        if output == b'' and process.poll() is not None:\n            break\n        if output:\n            print(output.decode().strip())\n\n# Check if the file exists and has execute permissions\nfile_path = os.path.abspath('Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile')\n\n# Why are the arguments not being used??\ncommand = [file_path, \"--nobrowser\", \"-ngl\", \"9999\"]\nprint(command)\n\n# Setting up the Popen call with stderr redirected to stdout\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n\n# Thread to handle the output asynchronously\nthread = threading.Thread(target=read_output, args=(process,))\nthread.start()\n\n# Here you can do other tasks concurrently\n# For example:\ninterpreter.chat()\n\n# Wait for the thread to finish if the process completes\nthread.join()\n\n# Ensure the process has completed\nprocess.wait()\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter Locally with Custom API\nDESCRIPTION: Command to run Open Interpreter locally using a custom API base URL for inference servers like LM Studio.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Vision Support in Open Interpreter\nDESCRIPTION: Informs Open Interpreter that the language model supports vision capabilities. Defaults to False.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --llm_supports_vision\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.supports_vision = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  supports_vision: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Instructions for Package Scanning\nDESCRIPTION: Example YAML configuration that includes custom instructions for the model to scan packages with guarddog before installation, providing an additional layer of security.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/safety/safe-mode.mdx#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: gpt-4\nverbose: false\nsafe_mode: ask\nsystem_message: |\n  # normal system message here\n  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.\n```\n\n----------------------------------------\n\nTITLE: Specifying Huggingface API Base URL (Bash)\nDESCRIPTION: This bash command demonstrates how to set the API base URL for Huggingface in Open Interpreter using the command line interface. It uses the --api_base flag to specify the endpoint URL.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/huggingface.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base <https://my-endpoint.huggingface.cloud>\n```\n\n----------------------------------------\n\nTITLE: Setting DeepInfra Model in Open Interpreter\nDESCRIPTION: Examples showing how to specify a DeepInfra model using both command line and Python API approaches. The model parameter requires the deepinfra prefix followed by the specific model name.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/deepinfra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model deepinfra/<deepinfra-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"deepinfra/<deepinfra-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Configuring NLP Cloud Model via Terminal\nDESCRIPTION: Shows how to set the NLP Cloud dolphin model using the command-line interface.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/nlp-cloud.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model dolphin\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Model Parameters in Python\nDESCRIPTION: Shows how to set essential parameters for the custom language model including context window size, maximum tokens, and capability flags for vision and function support.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/custom-models.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.context_window = 2000 # In tokens\ninterpreter.llm.max_tokens = 1000 # In tokens\ninterpreter.llm.supports_vision = False # Does this completions endpoint accept images?\ninterpreter.llm.supports_functions = False # Does this completions endpoint accept/return function calls?\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter Locally with Llamafile\nDESCRIPTION: Command to run Open Interpreter locally using Llamafile without additional software installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with Jan.ai Model in Python\nDESCRIPTION: Python code to set up Open Interpreter with a Jan.ai local model. The code disables online features, specifies the model ID, sets the API base URL, and starts an interactive chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/janai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"<model_id>\"\ninterpreter.llm.api_base = \"http://localhost:1337/v1 \"\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python\nDESCRIPTION: Python code snippet demonstrating how to import and use Open Interpreter programmatically.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"Plot stock prices for AAPL and META\") # Execute a command\ninterpreter.chat() # Start interactive chat\n```\n\n----------------------------------------\n\nTITLE: Running an Ollama Model via Command Line\nDESCRIPTION: Command to download and run a specific language model using Ollama. This needs to be executed after installing the Ollama application.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/ollama.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run <model-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with PaLM API via Terminal\nDESCRIPTION: Sets up Open Interpreter to use the PaLM API model 'chat-bison' through the command line interface. Requires installing the google-generativeai package first.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/palm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model palm/chat-bison\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Model in Open Interpreter (Bash)\nDESCRIPTION: This code snippet demonstrates how to set an Azure model for Open Interpreter using the command-line interface. It requires specifying the Azure deployment ID in the model flag.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/azure.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model azure/<your_deployment_id>\n```\n\n----------------------------------------\n\nTITLE: Configuring Message Templates\nDESCRIPTION: Setup for customizing user message templates and code output templates for interpreter interaction.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.user_message_template = \"{content} Please send me some code that would be able to answer my question, in the form of ```python\\n... the code ...\\n``` or ```shell\\n... the code ...\\n```\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Python Language Class with E2B Integration\nDESCRIPTION: Defines a custom language class PythonE2B that implements the required Open Interpreter interface using E2B for remote code execution. Includes run, stop, and terminate methods with basic output handling.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/custom-languages.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport e2b\n\nclass PythonE2B:\n    \"\"\"\n    This class contains all requirements for being a custom language in Open Interpreter:\n\n    - name (an attribute)\n    - run (a method)\n    - stop (a method)\n    - terminate (a method)\n\n    You can use this class to run any language you know how to run, or edit any of the official languages (which also conform to this class).\n\n    Here, we'll use E2B to power the `run` method.\n    \"\"\"\n\n    # This is the name that will appear to the LLM.\n    name = \"python\"\n\n    # Optionally, you can append some information about this language to the system message:\n    system_message = \"# Follow this rule: Every Python code block MUST contain at least one print statement.\"\n\n    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,\n    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)\n\n    def run(self, code):\n        \"\"\"Generator that yields a dictionary in LMC Format.\"\"\"\n\n        # Run the code on E2B\n        stdout, stderr = e2b.run_code('Python3', code)\n\n        # Yield the output\n        yield {\n            \"type\": \"console\", \"format\": \"output\",\n            \"content\": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!\n        }\n\n    def stop(self):\n        \"\"\"Stops the code.\"\"\"\n        # Not needed here, because e2b.run_code isn't stateful.\n        pass\n\n    def terminate(self):\n        \"\"\"Terminates the entire process.\"\"\"\n        # Not needed here, because e2b.run_code isn't stateful.\n        pass\n\n# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)\ninterpreter.computer.terminate()\n\n# Give Open Interpreter its languages. This will only let it run PythonE2B:\ninterpreter.computer.languages = [PythonE2B]\n\n# Try it out!\ninterpreter.chat(\"What's 349808*38490739?\")\n```\n\n----------------------------------------\n\nTITLE: Resetting Chat History\nDESCRIPTION: Shows how to start a fresh chat session in both terminal and Python environments\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.messages = []\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Profile for Open Interpreter Configuration\nDESCRIPTION: This Python profile configures Open Interpreter with specific settings including enabling OS access, vision support, using the gpt-4o model, setting context window size, and enabling auto-run and loop mode.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/profiles.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.os = True\ninterpreter.llm.supports_vision = True\n\ninterpreter.llm.model = \"gpt-4o\"\n\ninterpreter.llm.supports_functions = True\ninterpreter.llm.context_window = 110000\ninterpreter.llm.max_tokens = 4096\ninterpreter.auto_run = True\ninterpreter.loop = True\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter for local use in Python\nDESCRIPTION: Python code to configure Open Interpreter for local use with custom settings.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells AI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires an api key\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI-compatible server\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with PaLM API in Python\nDESCRIPTION: Demonstrates how to configure Open Interpreter to use the PaLM API model 'chat-bison' in Python code. This involves importing the interpreter module and setting the model property.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/palm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"palm/chat-bison\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Creating a YAML Profile for Open Interpreter Configuration\nDESCRIPTION: This YAML profile example shows how to configure Open Interpreter using YAML format. It includes model settings, temperature, computer API integration options, custom instructions, and general configuration flags like auto_run and offline mode.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/profiles.mdx#2025-04-22_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nllm:\n  model: \"gpt-4-o\"\n  temperature: 0\n  # api_key: ...  # Your API key, if the API requires it\n  # api_base: ...  # The URL where an OpenAI-compatible server is running to handle LLM API requests\n\n# Computer Settings\ncomputer:\n  import_computer_api: True # Gives OI a helpful Computer API designed for code interpreting language models\n\n# Custom Instructions\ncustom_instructions: \"\"  # This will be appended to the system message\n\n# General Configuration\nauto_run: False  # If True, code will run without asking for confirmation\noffline: False  # If True, will disable some online features like checking for updates\n\nversion: 0.2.5 # Configuration file version (do not modify)\n```\n\n----------------------------------------\n\nTITLE: Opening Open Interpreter Profiles Configuration\nDESCRIPTION: This command opens the file containing default settings for Open Interpreter. Users can edit these settings to customize the behavior of the interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/settings.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profiles\n```\n\n----------------------------------------\n\nTITLE: Starting the Server from Command Line with Bash\nDESCRIPTION: Shows how to start the Open Interpreter server using a command-line interface with the appropriate flags.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --server\n```\n\n----------------------------------------\n\nTITLE: Enabling Vision Support in Open Interpreter\nDESCRIPTION: Command to start Open Interpreter with vision capabilities enabled. This experimental feature allows the interpreter to process images using the GPT-4 Vision model when image file paths are detected in the input.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/vision.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --vision\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with Google Vertex AI Models (Python)\nDESCRIPTION: Python code snippet demonstrating how to set up Open Interpreter to use Gemini Pro and Gemini Pro Vision models from Google Vertex AI. It shows how to import the interpreter, set the model, and start a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/vertex-ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"gemini-pro\"\ninterpreter.llm.model = \"gemini-pro-vision\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with Petals Model in Python\nDESCRIPTION: Python code to configure Open Interpreter to use a Petals model. The model attribute must be set with the 'petals/' prefix followed by the model path.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/petals.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"petals/petals-team/StableBeluga2\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Running FastAPI Server for Open Interpreter\nDESCRIPTION: Shell commands to install dependencies and run the FastAPI server for Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\npip install fastapi uvicorn\nuvicorn server:app --reload\n```\n\n----------------------------------------\n\nTITLE: Creating FastAPI Server for Open Interpreter\nDESCRIPTION: Python code for setting up a FastAPI server to control Open Interpreter via HTTP REST endpoints.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# server.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom interpreter import interpreter\n\napp = FastAPI()\n\n@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\n@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages\n```\n\n----------------------------------------\n\nTITLE: Configuring Together AI Model via Command Line\nDESCRIPTION: Shows how to set up Open Interpreter to use a Together AI model using the command line interface with the --model flag.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/togetherai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model together_ai/<together_ai-model>\n```\n\n----------------------------------------\n\nTITLE: Loading a Profile in Open Interpreter via Command Line\nDESCRIPTION: This command loads a specific profile in Open Interpreter. The profile name should be specified as a YAML file.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/profiles.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profile <profile_name>.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Open Interpreter\nDESCRIPTION: This code snippet shows the command-line options to disable telemetry or run Open Interpreter in local/offline mode, ensuring data privacy.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/troubleshooting/faq.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--local, --offline, or --disable_telemetry\n```\n\n----------------------------------------\n\nTITLE: Starting the Server from Python\nDESCRIPTION: Demonstrates how to initialize the AsyncInterpreter and start the server with a custom port from within Python code.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import AsyncInterpreter\n\nasync_interpreter = AsyncInterpreter()\nasync_interpreter.server.run(port=8000)  # Default port is 8000, but you can customize it\n```\n\n----------------------------------------\n\nTITLE: Saving and restoring chats in Python\nDESCRIPTION: Example of how to save and restore chat sessions in Open Interpreter using Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = interpreter.chat(\"My name is Steve.\") # Save messages to \"messages\"\ninterpreter.messages = [] # Reset the interpreter (\"Steve\" will be forgotten)\n\ninterpreter.messages = messages # Restore the chat from \"messages\" (\"Steve\" will be remembered)\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Context Window with Jan.ai in Terminal\nDESCRIPTION: Terminal command to launch Open Interpreter with a Jan.ai model using a custom context window size. This is useful for models that support longer contexts than the default 3000 tokens.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/janai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base http://localhost:1337/v1  --model <model_id> --context_window 5000\n```\n\n----------------------------------------\n\nTITLE: Customizing Open Interpreter System Message\nDESCRIPTION: Python code showing how to customize the system message for Open Interpreter to modify its behavior.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nExecute shell commands with the '-y' flag to avoid user confirmation.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating LMC Message Flow in Python\nDESCRIPTION: Example showing the interaction flow between user, assistant, and computer roles in the LMC architecture for a simple multiplication calculation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/protocols/lmc-messages.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# The user sends a message.\n{\"role\": \"user\", \"type\": \"message\", \"content\": \"What's 2380*3875?\"}\n\n# The assistant runs some code.\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": \"2380*3875\"}\n\n# The computer responds with the result of the code.\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"output\", \"content\": \"9222500\"}\n\n# The assistant sends a message.\n{\"role\": \"assistant\", \"type\": \"message\", \"content\": \"The result of multiplying 2380 by 3875 is 9222500.\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter via Terminal for Local Mode\nDESCRIPTION: This bash command launches Open Interpreter in local mode with specific max_tokens and context_window settings. It's recommended to use smaller values to reduce RAM usage and improve performance.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/best-practices.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --local --max_tokens 1000 --context_window 3000\n```\n\n----------------------------------------\n\nTITLE: Configuring OS Mode with Custom Browser Preferences\nDESCRIPTION: Configures Open Interpreter's OS mode with custom instructions for using Safari browser and Raycast launcher instead of Spotlight search.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/example-profiles.mdx#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nos: True\ncustom_instructions: \"Always use Safari as the browser, and use Raycast instead of spotlight search by pressing option + space.\"\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter Dependencies\nDESCRIPTION: Installs the required packages including open-interpreter and opencv-python using pip.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local3.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install open-interpreter --quiet\n!pip install opencv-python --quiet\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Instructions for Photo Organization\nDESCRIPTION: Implements custom instructions for the interpreter to process and organize photos. Details the workflow for analyzing images using computer.vision.query() and moving them to appropriate category directories based on content analysis.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/organize_photos.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Custom Instructions\ninterpreter.custom_instructions=f\"\"\"\n    Recap the plan before answering the user's query!\n    Your job is to organize photos. You love organizing photos.\n    You will be given a parent directory with sub-directories. \n    One sub-directory will be of unorganized photos.\n    The other sub-directories will be categories that you move the photos in to.\n    Remember the sub-directories's names because they will be the categories for organizing.\n    It is extremely important because these are the only options for where you move the photos.\n    Loop through every photo in the unorganized photos directory. \n    Skip over non-photo files by checking for common photo extensions (.jpg, .jpeg, .png, etc).\n    In this loop you will determine the description of each image one at a time. \n    Use `computer.vision.query()` to get a description of the image.\n    `computer.vision.query()` takes a `path=` argument to know which photo to describe. \n    Print out the description so you can get the full context.\n    Determine which sub-directory the photo should go in to.\n    Every photo needs to go into one of the sub-directories.\n    Make sure you actually move the photo. \n    Your task is done when every photo in the unorganized photos directory has been moved to another directory. \n    **Confirm that the unorganized photos directory has no more photos**.\n    \"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Chats in Python\nDESCRIPTION: Save the current conversation to a variable and restore it later, allowing you to maintain conversation context across different sessions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Save messages to 'messages'\nmessages = interpreter.chat(\"My name is Killian.\")\n\n# Reset interpreter (\"Killian\" will be forgotten)\ninterpreter.messages = []\n\n# Resume chat from 'messages' (\"Killian\" will be remembered)\ninterpreter.messages = messages\n```\n\n----------------------------------------\n\nTITLE: Setting Perplexity Model in Open Interpreter (Bash and Python)\nDESCRIPTION: These code snippets demonstrate how to set a Perplexity model in Open Interpreter using both the command line interface and Python API. The '<perplexity-model>' placeholder should be replaced with a specific model name.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/perplexity.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model perplexity/<perplexity-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"perplexity/<perplexity-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Customizing System Messages in Python\nDESCRIPTION: Python code demonstrating how to customize system messages for Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Setting API Key in Open Interpreter\nDESCRIPTION: Sets the API key for authentication when making API calls. Required for services like OpenAI.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_key \"your_api_key_here\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.api_key = \"your_api_key_here\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  api_key: your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Setting up Mixtral with LlamaFile in Open Interpreter\nDESCRIPTION: Commands to download the Mixtral model using LlamaFile, set executable permissions, start the server, and connect Open Interpreter to the local model. This allows running large language models locally without relying on external API services.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/llamafile.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Download Mixtral\n\nwget https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile\n\n# Make it an executable\n\nchmod +x mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile\n\n# Start the server\n\n./mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile\n\n# In a separate terminal window, run OI and point it at the llamafile server\n\ninterpreter --api_base https://localhost:8080/v1\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with local models\nDESCRIPTION: Shell command to run Open Interpreter with locally hosted language models instead of OpenAI's API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Changing Language Models via Command Line\nDESCRIPTION: Shell commands to change the language model used by Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Initializing Interpreter Class in Python\nDESCRIPTION: This snippet defines the Interpreter class with various initialization parameters and methods. It sets up the interpreter's configuration, including the language model, conversation history, and system message.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/protocols/i-protocol.mdx#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass Interpreter:\n    def __init__(\n        self,\n        messages=None,\n        local=False,\n        auto_run=False,\n        max_output=2000,\n        safe_mode=False,\n        force_task_completion=False,\n        verbose=False,\n        debug_mode=False,\n        model=DEFAULT_MODEL,\n        context=\"\",\n        system_message=DEFAULT_SYSTEM_MESSAGE,\n        custom_instructions=\"\",\n        conversation=None,\n        **kwargs,\n    ):\n        self.local = local\n        self.auto_run = auto_run\n        self.max_output = max_output\n        self.safe_mode = safe_mode\n        self.force_task_completion = force_task_completion\n        self.verbose = verbose\n        self.debug_mode = debug_mode\n        self.model = model\n        self.context = context\n        self.system_message = system_message\n        self.custom_instructions = custom_instructions\n        self.custom_instructions_content = \"\"\n        self.conversation = conversation or []\n        self.messages = messages or []\n        self.api_key = kwargs.get(\"api_key\", None)\n        self.base_url = kwargs.get(\"base_url\", None)\n        self.warn_beta_feature = kwargs.get(\"warn_beta_feature\", True)\n        self.offline = kwargs.get(\"offline\", False)\n        self.disable_procedures = kwargs.get(\"disable_procedures\", False)\n\n        # More initialization code...\n```\n\n----------------------------------------\n\nTITLE: Managing Chat History in Python\nDESCRIPTION: Python code showing how to save, reset, and restore chat conversations.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmessages = interpreter.chat(\"My name is Killian.\") # Save messages to 'messages'\ninterpreter.messages = [] # Reset the interpreter (\"Killian\" will be forgotten)\n\ninterpreter.messages = messages # Resume conversation from 'messages' (\"Killian\" will be remembered)\n```\n\n----------------------------------------\n\nTITLE: Configuring Anyscale Model in Open Interpreter\nDESCRIPTION: Basic setup examples for using Anyscale models with Open Interpreter through both command line and Python interface. Shows how to specify a custom model using the model flag.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/anyscale.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model anyscale/<model-name>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\n# Set the model to use from AWS Bedrock:\ninterpreter.llm.model = \"anyscale/<model-name>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Customizing System Message for Enhanced Package Safety in Open Interpreter\nDESCRIPTION: YAML configuration example showing how to modify the system_message to include instructions for scanning packages with guarddog before installation. This enhances safety when installing new packages.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: gpt-4\nverbose: false\nsafe_mode: ask\nsystem_message: |\n  # normal system message here\n  BEFORE INSTALLING ANY PACKAGES WITH pip OR npm YOU MUST SCAN THEM WITH `guarddog` FIRST. Run `guarddog pypi scan $package` for pip packages and `guarddog npm scan $package` for npm packages. `guarddog` only accepts one package name at a time.\n```\n\n----------------------------------------\n\nTITLE: Initializing Mistral Model in Open Interpreter\nDESCRIPTION: Shows how to set up Open Interpreter with a Mistral AI model using both command line and Python interfaces. Allows specifying any supported Mistral model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/mistral-api.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model mistral/<mistral-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"mistral/<mistral-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with local LM Studio server via terminal\nDESCRIPTION: Command to run Open Interpreter using LM Studio's API endpoint. This requires specifying the API base URL (default http://localhost:1234/v1) and a placeholder API key.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/lm-studio.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter locally with Llamafile\nDESCRIPTION: Command to run Open Interpreter locally using Llamafile.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Python\nDESCRIPTION: Set a different language model for Open Interpreter to use in Python by modifying the llm.model property.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Output from Open Interpreter in Python\nDESCRIPTION: Python code demonstrating how to stream output from Open Interpreter using the stream parameter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmessage = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Setting Huggingface Model in Open Interpreter (Python)\nDESCRIPTION: This Python code shows how to set the Huggingface model for Open Interpreter programmatically. It imports the interpreter module and sets the model attribute of the llm object.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/huggingface.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"huggingface/<huggingface-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Setting Specific Perplexity Models in Open Interpreter (Python)\nDESCRIPTION: This Python snippet demonstrates how to set specific Perplexity models using the Open Interpreter Python API. It includes examples for all supported Perplexity models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/perplexity.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"perplexity/pplx-7b-chat\"\ninterpreter.llm.model = \"perplexity/pplx-70b-chat\"\ninterpreter.llm.model = \"perplexity/pplx-7b-online\"\ninterpreter.llm.model = \"perplexity/pplx-70b-online\"\ninterpreter.llm.model = \"perplexity/codellama-34b-instruct\"\ninterpreter.llm.model = \"perplexity/llama-2-13b-chat\"\ninterpreter.llm.model = \"perplexity/llama-2-70b-chat\"\ninterpreter.llm.model = \"perplexity/mistral-7b-instruct\"\ninterpreter.llm.model = \"perplexity/openhermes-2-mistral-7b\"\ninterpreter.llm.model = \"perplexity/openhermes-2.5-mistral-7b\"\ninterpreter.llm.model = \"perplexity/pplx-7b-chat-alpha\"\ninterpreter.llm.model = \"perplexity/pplx-70b-chat-alpha\"\n```\n\n----------------------------------------\n\nTITLE: Customizing system message in Python\nDESCRIPTION: Example of how to customize the system message for Open Interpreter in Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Model Parameters in Open Interpreter\nDESCRIPTION: Shell command demonstrating how to set max_tokens and context_window parameters for local models in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\ninterpreter --local --max_tokens 1000 --context_window 3000\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter in Local Mode\nDESCRIPTION: Shell command to run Open Interpreter in local mode using Llamafile without third-party software.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Customizing system messages in Open Interpreter\nDESCRIPTION: Python code showing how to customize the system message to extend capabilities, modify permissions, or provide additional context to the language model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nयूज़र को पुष्टि करने की आवश्यकता न हो, -y के साथ शेल कमांड चलाएँ।\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Resetting Open Interpreter Chat History\nDESCRIPTION: Python code showing how to reset the conversation history in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ninterpreter.messages = []\n```\n\n----------------------------------------\n\nTITLE: Setting Huggingface Model in Open Interpreter (Bash)\nDESCRIPTION: This snippet demonstrates how to set the Huggingface model for Open Interpreter using the command line interface. It uses the --model flag to specify the Huggingface model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/huggingface.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model huggingface/<huggingface-model>\n```\n\n----------------------------------------\n\nTITLE: Supported Cohere Models Python Configuration\nDESCRIPTION: Python code examples showing how to set different Cohere models in Open Interpreter. These examples demonstrate configuring the interpreter to use various model options including command, command-light, command-medium, and specialized beta versions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cohere.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"command\"\ninterpreter.llm.model = \"command-light\"\ninterpreter.llm.model = \"command-medium\"\ninterpreter.llm.model = \"command-medium-beta\"\ninterpreter.llm.model = \"command-xlarge-beta\"\ninterpreter.llm.model = \"command-nightly\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Model in Open Interpreter (Python)\nDESCRIPTION: This Python code shows how to set an Azure model for Open Interpreter programmatically. It imports the interpreter module, sets the LLM model to the Azure deployment, and initiates a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/azure.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"azure/<your_deployment_id>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter Locally via Command Line\nDESCRIPTION: Shell command to run Open Interpreter using a local API server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring local model parameters in Open Interpreter\nDESCRIPTION: Shell command showing how to modify max_tokens and context_window parameters when using local language models with Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --max_tokens 2000 --context_window 16000\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Open Interpreter Chat\nDESCRIPTION: Python code demonstrating how to save and restore chat sessions in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmessages = interpreter.chat(\"My name is Tanaka.\") # Save messages to 'messages'\ninterpreter.messages = [] # Reset interpreter (\"Tanaka\" is forgotten)\n\ninterpreter.messages = messages # Resume chat from 'messages' (\"Tanaka\" is remembered)\n```\n\n----------------------------------------\n\nTITLE: Resetting conversation history in Open Interpreter\nDESCRIPTION: Python code showing how to reset the message history in Open Interpreter to start a fresh conversation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.messages = []\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter with Cloudflare Workers AI Models\nDESCRIPTION: Examples showing how to specify a Cloudflare Workers AI model in Open Interpreter using both terminal commands and Python code. This sets the model to be used for interpretation tasks.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cloudflare.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model cloudflare/<cloudflare-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"cloudflare/<cloudflare-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Selecting Different Claude Models in Open Interpreter\nDESCRIPTION: These examples demonstrate how to specify different Claude models when using Open Interpreter. Supported models include claude-instant-1, claude-instant-1.2, and claude-2.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/anthropic.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model claude-instant-1\ninterpreter --model claude-instant-1.2\ninterpreter --model claude-2\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"claude-instant-1\"\ninterpreter.llm.model = \"claude-instant-1.2\"\ninterpreter.llm.model = \"claude-2\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Inter-Agent Communication Loop\nDESCRIPTION: Sets up a communication system between two OpenInterpreter instances by swapping message roles and maintaining a conversation loop. Includes a utility function to swap user/assistant roles in messages.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/multiple-instances.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef swap_roles(messages):\n    for message in messages:\n        if message['role'] == 'user':\n            message['role'] = 'assistant'\n        elif message['role'] == 'assistant':\n            message['role'] = 'user'\n    return messages\n\nagents = [agent_1, agent_2]\n\n# Kick off the conversation\nmessages = [{\"role\": \"user\", \"type\": \"message\", \"content\": \"Hello!\"}]\n\nwhile True:\n    for agent in agents:\n        messages = agent.chat(messages)\n        messages = swap_roles(messages)\n```\n\n----------------------------------------\n\nTITLE: Setting Language Model in Open Interpreter\nDESCRIPTION: Specifies which language model to use with Open Interpreter. The example shows how to set it to \"gpt-3.5-turbo\".\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  model: gpt-3.5-turbo\n```\n\n----------------------------------------\n\nTITLE: Setting context window and max tokens for local models\nDESCRIPTION: Command to set the context window and maximum tokens for locally run models in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local --max_tokens 1000 --context_window 3000\n```\n\n----------------------------------------\n\nTITLE: Setting Auto Run Mode\nDESCRIPTION: Shows how to configure automatic code execution without user confirmation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.auto_run = True  # Don't require user confirmation\ninterpreter.auto_run = False  # Require user confirmation (default)\n```\n\n----------------------------------------\n\nTITLE: Changing language model in Python\nDESCRIPTION: Python code to change the language model used by Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Setting the AI21 Model in Open Interpreter\nDESCRIPTION: Examples showing how to configure Open Interpreter to use AI21's j2-light model, either via command line or Python API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/ai21.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model j2-light\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"j2-light\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Launching Chrome with Remote Debugging for Browser Automation\nDESCRIPTION: A Python script to launch Chrome browser with remote debugging enabled on port 9222. This is part of the working notes for implementing a browser automation feature in the Open Interpreter project.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/ROADMAP.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nimport sys\nimport os\n\ndef launch_chrome():\n    chrome_path = \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"  # Update this path for your system\n    url = \"http://localhost:9222/json/version\"\n    subprocess.Popen([chrome_path, '--remote-debugging-port=9222'], shell=True)\n    print(\"Chrome launched with remote debugging on port 9222.\")\n\nif __name__ == \"__main__\":\n    launch_chrome()\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Profile Configuration for Open Interpreter\nDESCRIPTION: YAML configuration example showing how to permanently disable telemetry in the Open Interpreter profile settings, which will persist across terminal sessions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/telemetry/telemetry.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndisable_telemetry: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Mode Execution\nDESCRIPTION: Setup for running the model locally using LiteLLM and OpenAI-compatible servers. Includes offline mode configuration and API endpoint settings.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --local\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to local models, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.chat()\n```\n\nLANGUAGE: yaml\nCODE:\n```\nlocal: true\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with Local Model\nDESCRIPTION: Shell command to run Open Interpreter using a local language model through an OpenAI-compatible server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\ninterpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"\n```\n\n----------------------------------------\n\nTITLE: Starting Open Interpreter with Jan.ai Model via Terminal\nDESCRIPTION: Command to launch Open Interpreter connected to a Jan.ai local model API server. Requires specifying the API base URL and model ID of the downloaded model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/janai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base http://localhost:1337/v1  --model <model_id>\n```\n\n----------------------------------------\n\nTITLE: Supported OpenRouter Models via Python\nDESCRIPTION: Python code examples showing how to set various supported OpenRouter models in Open Interpreter, including models from OpenAI, Anthropic, Google, and Meta.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openrouter.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"openrouter/openai/gpt-3.5-turbo\"\ninterpreter.llm.model = \"openrouter/openai/gpt-3.5-turbo-16k\"\ninterpreter.llm.model = \"openrouter/openai/gpt-4\"\ninterpreter.llm.model = \"openrouter/openai/gpt-4-32k\"\ninterpreter.llm.model = \"openrouter/anthropic/claude-2\"\ninterpreter.llm.model = \"openrouter/anthropic/claude-instant-v1\"\ninterpreter.llm.model = \"openrouter/google/palm-2-chat-bison\"\ninterpreter.llm.model = \"openrouter/google/palm-2-codechat-bison\"\ninterpreter.llm.model = \"openrouter/meta-llama/llama-2-13b-chat\"\ninterpreter.llm.model = \"openrouter/meta-llama/llama-2-70b-chat\"\n```\n\n----------------------------------------\n\nTITLE: Supported AI21 Models for Open Interpreter\nDESCRIPTION: Code examples demonstrating how to use different AI21 models (j2-light, j2-mid, j2-ultra) with Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/ai21.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model j2-light\ninterpreter --model j2-mid\ninterpreter --model j2-ultra\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"j2-light\"\ninterpreter.llm.model = \"j2-mid\"\ninterpreter.llm.model = \"j2-ultra\"\n```\n\n----------------------------------------\n\nTITLE: Customizing FastAPI App and Hosting with Uvicorn in Python\nDESCRIPTION: This code snippet demonstrates how to access the FastAPI app directly from AsyncInterpreter, add a custom route, and host the app using Uvicorn. It imports necessary modules, creates an AsyncInterpreter instance, adds a custom route, and runs the app with Uvicorn.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import AsyncInterpreter\nfrom fastapi import FastAPI\nimport uvicorn\n\nasync_interpreter = AsyncInterpreter()\napp = async_interpreter.server.app\n\n@app.get(\"/custom\")\nasync def custom_route():\n    return {\"message\": \"This is a custom route\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Model in Python\nDESCRIPTION: Demonstrates how to configure Open Interpreter to use a Together AI model using the Python API by setting the model property and starting a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/togetherai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"together_ai/<together_ai-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Settings in Terminal\nDESCRIPTION: Access and edit profile settings including default language model, system message, and budget constraints.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --profiles\n```\n\n----------------------------------------\n\nTITLE: Setting Custom API Endpoint via Command Line\nDESCRIPTION: Demonstrates how to set a custom OpenAI-compatible API endpoint using the command line interface\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/custom-endpoint.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base <custom_endpoint>\n```\n\n----------------------------------------\n\nTITLE: Using Specific AWS Sagemaker Models with Open Interpreter\nDESCRIPTION: Commands to use specific Meta Llama 2 models or custom Huggingface models from AWS Sagemaker with Open Interpreter. Available in various sizes (7B, 13B, 70B) with chat-tuned variants.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/aws-sagemaker.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b\ninterpreter --model sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f\ninterpreter --model sagemaker/<your-hugginface-deployment-name>\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b\"\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-7b-f\"\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b\"\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-13b-f\"\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b\"\ninterpreter.llm.model = \"sagemaker/jumpstart-dft-meta-textgeneration-llama-2-70b-b-f\"\ninterpreter.llm.model = \"sagemaker/<your-hugginface-deployment-name>\"\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenRouter with Open Interpreter via Python\nDESCRIPTION: Python code example showing how to configure Open Interpreter to use an OpenRouter model by setting the model property and starting a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openrouter.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"openrouter/openai/gpt-3.5-turbo\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: DeepInfra Supported Model Examples\nDESCRIPTION: Comprehensive examples of setting various supported DeepInfra models including Llama-2, CodeLlama, Mistral, and Airoboros models using both command line and Python interfaces.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/deepinfra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model deepinfra/meta-llama/Llama-2-70b-chat-hf\ninterpreter --model deepinfra/meta-llama/Llama-2-7b-chat-hf\ninterpreter --model deepinfra/meta-llama/Llama-2-13b-chat-hf\ninterpreter --model deepinfra/codellama/CodeLlama-34b-Instruct-hf\ninterpreter --model deepinfra/mistral/mistral-7b-instruct-v0.1\ninterpreter --model deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"deepinfra/meta-llama/Llama-2-70b-chat-hf\"\ninterpreter.llm.model = \"deepinfra/meta-llama/Llama-2-7b-chat-hf\"\ninterpreter.llm.model = \"deepinfra/meta-llama/Llama-2-13b-chat-hf\"\ninterpreter.llm.model = \"deepinfra/codellama/CodeLlama-34b-Instruct-hf\"\ninterpreter.llm.model = \"deepinfra/mistral-7b-instruct-v0.1\"\ninterpreter.llm.model = \"deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1\"\n```\n\n----------------------------------------\n\nTITLE: Setting Various Replicate Models in Bash\nDESCRIPTION: This snippet demonstrates how to use different Replicate models with Open Interpreter in the command line interface. It shows examples for Llama 2, Vicuna, and FLAN-T5 models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/replicate.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf\ninterpreter --model replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52\ninterpreter --model replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b\ninterpreter --model replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f\n```\n\n----------------------------------------\n\nTITLE: Changing Language Models in Python\nDESCRIPTION: Python code to change the language model used by Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Open Interpreter (Python)\nDESCRIPTION: Python code showing how to change the language model used by Open Interpreter programmatically.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Settings\nDESCRIPTION: Shows how to access and modify default settings using profiles\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --profiles\n```\n\n----------------------------------------\n\nTITLE: Vision Mode Configuration\nDESCRIPTION: Enables vision mode for multimodal models with GPT-4-turbo as default.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --vision\n```\n\nLANGUAGE: yaml\nCODE:\n```\nvision: true\n```\n\n----------------------------------------\n\nTITLE: Specifying Specific Cloudflare Workers AI Models\nDESCRIPTION: Examples of how to configure specific Cloudflare Workers AI models, including Llama-2, Mistral, and CodeLlama variants. Each model requires the proper namespace and model identifier.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cloudflare.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model cloudflare/@cf/meta/llama-2-7b-chat-fp16\ninterpreter --model cloudflare/@cf/meta/llama-2-7b-chat-int8\ninterpreter --model @cf/mistral/mistral-7b-instruct-v0.1\ninterpreter --model @hf/thebloke/codellama-7b-instruct-awq\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"cloudflare/@cf/meta/llama-2-7b-chat-fp16\"\ninterpreter.llm.model = \"cloudflare/@cf/meta/llama-2-7b-chat-int8\"\ninterpreter.llm.model = \"@cf/mistral/mistral-7b-instruct-v0.1\"\ninterpreter.llm.model = \"@hf/thebloke/codellama-7b-instruct-awq\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cohere Models in Open Interpreter via Terminal\nDESCRIPTION: Command to initialize Open Interpreter with a Cohere model from the terminal. This example uses the 'command-nightly' model which is one of Cohere's advanced language models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cohere.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Configuring NLP Cloud Model in Python\nDESCRIPTION: Demonstrates how to set up the NLP Cloud dolphin model using the Python API, including initializing the interpreter and starting a chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/nlp-cloud.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"dolphin\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Mounting Current Directory to Docker Container\nDESCRIPTION: A bash command example that mounts the current working directory to a '/files' directory in the Docker container. This provides Open Interpreter access to all files in the current directory.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -it -v $(pwd):/files --name interpreter-instance openinterpreter interpreter\n```\n\n----------------------------------------\n\nTITLE: Starting Interactive Chat in Python\nDESCRIPTION: Initialize an interactive chat session with Open Interpreter from a Python script.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Accessing the Profiles Directory in Open Interpreter\nDESCRIPTION: This command opens the directory where all profiles are stored in Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/profiles.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profile\n\n```\n\n----------------------------------------\n\nTITLE: OS Mode Configuration\nDESCRIPTION: Enables OS mode for multimodal models with GPT-4-turbo as default.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --os\n```\n\nLANGUAGE: yaml\nCODE:\n```\nos: true\n```\n\n----------------------------------------\n\nTITLE: Starting Open Interpreter with Ollama Model via Terminal\nDESCRIPTION: Command to launch Open Interpreter with a specific Ollama model from the terminal. The model must be previously downloaded through Ollama.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/ollama.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model ollama/<model-name>\n```\n\n----------------------------------------\n\nTITLE: Supported Cohere Models Terminal Commands\nDESCRIPTION: List of terminal commands showing all supported Cohere models that can be used with Open Interpreter. The commands demonstrate how to initialize different models including command, command-light, command-medium, and specialized beta versions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/cohere.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model command\ninterpreter --model command-light\ninterpreter --model command-medium\ninterpreter --model command-medium-beta\ninterpreter --model command-xlarge-beta\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Using Different Petals Models in Python\nDESCRIPTION: Examples showing how to configure different Petals models when using Open Interpreter in Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/petals.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"petals/petals-team/StableBeluga2\"\ninterpreter.llm.model = \"petals/huggyllama/llama-65b\"\n```\n\n----------------------------------------\n\nTITLE: Setting E2B API Key in Python\nDESCRIPTION: Configures the E2B API key as an environment variable for authentication.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/e2b.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"E2B_API_KEY\"] = \"<your_api_key_here>\"\n```\n\n----------------------------------------\n\nTITLE: Handling Streaming Chunks for Open Interpreter in JavaScript\nDESCRIPTION: This code snippet demonstrates how to handle streaming chunks from Open Interpreter in JavaScript. It includes functions for sending POST requests, processing response chunks, and managing message state. The code handles different chunk types, including message starts, content updates, and active lines for code blocks.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nlet messages = []; //variable to hold all messages\nlet currentMessageIndex = 0; //variable to keep track of the current message index\nlet isGenerating = false; //variable to stop the stream\n\n// Function to send a POST request to the OI\nasync function sendRequest() {\n  // Temporary message to hold the message that is being processed\n  try {\n    // Define parameters for the POST request, add at least the full messages array, but you may also consider adding any other OI parameters here, like auto_run, local, etc.\n    const params = {\n      messages,\n    };\n\n    //Define a controller to allow for aborting the request\n    const controller = new AbortController();\n    const { signal } = controller;\n\n    // Send the POST request to your Python server endpoint\n    const interpreterCall = await fetch(\"https://YOUR_ENDPOINT/\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify(params),\n      signal,\n    });\n\n    // Throw an error if the request was not successful\n    if (!interpreterCall.ok) {\n      console.error(\"Interpreter didn't respond with 200 OK\");\n      return;\n    }\n\n    // Initialize a reader for the response body\n    const reader = interpreterCall.body.getReader();\n\n    isGenerating = true;\n    while (true) {\n      const { value, done } = await reader.read();\n\n      // Break the loop if the stream is done\n      if (done) {\n        break;\n      }\n      // If isGenerating is set to false, cancel the reader and break the loop. This will halt the execution of the code run by OI as well\n      if (!isGenerating) {\n        await reader.cancel();\n        controller.abort();\n        break;\n      }\n      // Decode the stream and split it into lines\n      const text = new TextDecoder().decode(value);\n      const lines = text.split(\"\\n\");\n      lines.pop(); // Remove last empty line\n\n      // Process each line of the response\n      for (const line of lines) {\n        const chunk = JSON.parse(line);\n        await processChunk(chunk);\n      }\n    }\n    //Stream has completed here, so run any code that needs to be run after the stream has finished\n    if (isGenerating) isGenerating = false;\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n  }\n}\n\n//Function to process each chunk of the stream, and create messages\nfunction processChunk(chunk) {\n  if (chunk.start) {\n    const tempMessage = {};\n    //add the new message's data to the tempMessage\n    tempMessage.role = chunk.role;\n    tempMessage.type = chunk.type;\n    tempMessage.content = \"\";\n    if (chunk.format) tempMessage.format = chunk.format;\n    if (chunk.recipient) tempMessage.recipient = chunk.recipient;\n\n    //add the new message to the messages array, and set the currentMessageIndex to the index of the new message\n    messages.push(tempMessage);\n    currentMessageIndex = messages.length - 1;\n  }\n\n  //Handle active lines for code blocks\n  if (chunk.format === \"active_line\") {\n    messages[currentMessageIndex].activeLine = chunk.content;\n  } else if (chunk.end && chunk.type === \"console\") {\n    messages[currentMessageIndex].activeLine = null;\n  }\n\n  //Add the content of the chunk to current message, avoiding adding the content of the active line\n  if (chunk.content && chunk.format !== \"active_line\") {\n    messages[currentMessageIndex].content += chunk.content;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Language Model Settings\nDESCRIPTION: Shows how to configure various language model settings including model selection, temperature, and token limits.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.temperature = 0.7\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\\nRun all shell commands with -y.\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.context_window = 16000\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.max_tokens = 100\n```\n\n----------------------------------------\n\nTITLE: Model Selection Configuration\nDESCRIPTION: Specifies which language model to use for interpretation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: gpt-3.5-turbo\n```\n\n----------------------------------------\n\nTITLE: Enabling Function Support in Open Interpreter\nDESCRIPTION: Informs Open Interpreter that the language model supports function calling.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --llm_supports_functions\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.supports_functions = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  supports_functions: true\n```\n\n----------------------------------------\n\nTITLE: Setting Various Replicate Models in Python\nDESCRIPTION: This snippet shows how to use different Replicate models with Open Interpreter in Python. It provides examples for setting Llama 2, Vicuna, and FLAN-T5 models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/replicate.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf\"\ninterpreter.llm.model = \"replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52\"\ninterpreter.llm.model = \"replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b\"\ninterpreter.llm.model = \"replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f\"\n```\n\n----------------------------------------\n\nTITLE: Setting Custom API Endpoint in Python\nDESCRIPTION: Shows how to configure a custom OpenAI-compatible API endpoint using the Python interpreter library and initiate a chat session\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/custom-endpoint.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.api_base = \"<custom_endpoint>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM API Base\nDESCRIPTION: Examples showing how to set the API base URL for vLLM integration in both terminal and Python environments. Requires vLLM to be installed via pip.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/vllm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base <https://your-hosted-vllm-server>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.api_base = \"<https://your-hosted-vllm-server>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with Replicate Model in Bash\nDESCRIPTION: This snippet demonstrates how to set a Replicate model for Open Interpreter using the command line interface. It uses the --model flag with a Replicate model identifier.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/replicate.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto Run in Open Interpreter\nDESCRIPTION: Automatically executes code without requiring user confirmation for each operation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --auto_run\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.auto_run = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauto_run: true\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Budget in Open Interpreter CLI\nDESCRIPTION: This command starts Open Interpreter with a maximum budget of $0.01 for the session. This helps control costs when using paid language models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/budget-manager.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_budget 0.01\n```\n\n----------------------------------------\n\nTITLE: Managing Conversation History Settings\nDESCRIPTION: Demonstrates how to configure conversation history storage settings including enabling/disabling and setting file paths.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.conversation_history = True  # To store history\ninterpreter.conversation_history = False  # To not store history\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.conversation_filename = \"my_conversation.json\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\ninterpreter.conversation_history_path = os.path.join(\"my_folder\", \"conversations\")\n```\n\n----------------------------------------\n\nTITLE: Setting Specific Mistral Models\nDESCRIPTION: Examples of setting specific Mistral model variants (tiny, small, medium) in Open Interpreter using both command line and Python interfaces.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/mistral-api.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model mistral/mistral-tiny\ninterpreter --model mistral/mistral-small\ninterpreter --model mistral/mistral-medium\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"mistral/mistral-tiny\"\ninterpreter.llm.model = \"mistral/mistral-small\"\ninterpreter.llm.model = \"mistral/mistral-medium\"\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with Local Mode dependencies\nDESCRIPTION: Installs Open Interpreter with additional dependencies required for Local Mode functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter[local]\n```\n\n----------------------------------------\n\nTITLE: Using Specific Perplexity Models in Open Interpreter (Bash)\nDESCRIPTION: This bash snippet shows how to use specific Perplexity models with Open Interpreter via the command line. It includes examples for all supported Perplexity models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/perplexity.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model perplexity/pplx-7b-chat\ninterpreter --model perplexity/pplx-70b-chat\ninterpreter --model perplexity/pplx-7b-online\ninterpreter --model perplexity/pplx-70b-online\ninterpreter --model perplexity/codellama-34b-instruct\ninterpreter --model perplexity/llama-2-13b-chat\ninterpreter --model perplexity/llama-2-70b-chat\ninterpreter --model perplexity/mistral-7b-instruct\ninterpreter --model perplexity/openhermes-2-mistral-7b\ninterpreter --model perplexity/openhermes-2.5-mistral-7b\ninterpreter --model perplexity/pplx-7b-chat-alpha\ninterpreter --model perplexity/pplx-70b-chat-alpha\n```\n\n----------------------------------------\n\nTITLE: Context Window Configuration\nDESCRIPTION: Sets the context window size in tokens for the model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --context_window 16000\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncontext_window: 16000\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Mode in Open Interpreter\nDESCRIPTION: Activates verbose mode to display debug information at each step, helpful for diagnosing issues.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --verbose\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.verbose = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\nverbose: true\n```\n\n----------------------------------------\n\nTITLE: Setting Budget Limit in Open Interpreter\nDESCRIPTION: Sets the maximum budget limit for the session in USD to control API usage costs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_budget 0.01\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.max_budget = 0.01\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmax_budget: 0.01\n```\n\n----------------------------------------\n\nTITLE: Enabling Loop Mode in Open Interpreter\nDESCRIPTION: Runs Open Interpreter in a loop, requiring it to explicitly state when it has completed or failed a task.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --loop\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.loop = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\nloop: true\n```\n\n----------------------------------------\n\nTITLE: Supported Anyscale Model Examples\nDESCRIPTION: Examples of configuring specific supported Anyscale models including Llama 2 variants, Mistral, and CodeLlama. Shows the exact model identifiers needed for each supported model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/anyscale.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model anyscale/meta-llama/Llama-2-7b-chat-hf\ninterpreter --model anyscale/meta-llama/Llama-2-13b-chat-hf\ninterpreter --model anyscale/meta-llama/Llama-2-70b-chat-hf\ninterpreter --model anyscale/mistralai/Mistral-7B-Instruct-v0.1\ninterpreter --model anyscale/codellama/CodeLlama-34b-Instruct-hf\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"anyscale/meta-llama/Llama-2-7b-chat-hf\"\ninterpreter.llm.model = \"anyscale/meta-llama/Llama-2-13b-chat-hf\"\ninterpreter.llm.model = \"anyscale/meta-llama/Llama-2-70b-chat-hf\"\ninterpreter.llm.model = \"anyscale/mistralai/Mistral-7B-Instruct-v0.1\"\ninterpreter.llm.model = \"anyscale/codellama/CodeLlama-34b-Instruct-hf\"\n\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with OS Mode dependencies\nDESCRIPTION: Installs Open Interpreter with additional dependencies required for OS Mode functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter[os]\n```\n\n----------------------------------------\n\nTITLE: Using Different Petals Models in CLI\nDESCRIPTION: Examples showing how to specify different Petals models when using the Open Interpreter CLI.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/petals.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model petals/petals-team/StableBeluga2\ninterpreter --model petals/huggyllama/llama-65b\n```\n\n----------------------------------------\n\nTITLE: Computer Configuration\nDESCRIPTION: Settings for the virtual computer object including offline mode, verbose output, and image emission controls.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.offline = True\ninterpreter.computer.verbose = True\ninterpreter.computer.emit_images = True\ninterpreter.computer.import_computer_api = True\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncomputer.offline: True\ncomputer.verbose: True\ncomputer.emit_images: True\ncomputer.import_computer_api: True\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Budget\nDESCRIPTION: Shows how to set a maximum budget limit for the session in USD.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.max_budget = 0.01 # 1 cent\n```\n\n----------------------------------------\n\nTITLE: Setting API Version in Open Interpreter\nDESCRIPTION: Sets the API version to use with the selected model. This will override environment variables if set.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_version 2.0.2\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.api_version = '2.0.2'\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  api_version: 2.0.2\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Terminal\nDESCRIPTION: Specify different language models to use with Open Interpreter in the terminal, using LiteLLM for model connections.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Context Window with Jan.ai in Python\nDESCRIPTION: Python code snippet showing how to adjust the context window size for Open Interpreter when using Jan.ai. This allows accommodating models with larger context capabilities.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/janai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.context_window = 5000\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with Safe Mode dependencies\nDESCRIPTION: Installs Open Interpreter with additional dependencies required for Safe Mode functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter[safe]\n```\n\n----------------------------------------\n\nTITLE: Configuring Specific Baseten Models\nDESCRIPTION: Examples of setting up specific supported Baseten models including Falcon 7b, Wizard LM, and MPT 7b Base using their respective model IDs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/baseten.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model baseten/qvv0xeq\ninterpreter --model baseten/q841o8w\ninterpreter --model baseten/31dxrj3\n\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"baseten/qvv0xeq\"\ninterpreter.llm.model = \"baseten/q841o8w\"\ninterpreter.llm.model = \"baseten/31dxrj3\"\n\n```\n\n----------------------------------------\n\nTITLE: LMC Message Structure Template\nDESCRIPTION: Template showing the basic structure of an LMC message with all possible parameters including role, type, format, and content fields.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/protocols/lmc-messages.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role\": \"<role>\",       # Who is sending the message.\n  \"type\": \"<type>\",       # What kind of message is being sent.\n  \"format\": \"<format>\"    # Some types need to be further specified, so they optionally use this parameter.\n  \"content\": \"<content>\", # What the message says.\n}\n```\n\n----------------------------------------\n\nTITLE: Fast Mode Configuration\nDESCRIPTION: Sets the model to gpt-3.5-turbo for faster processing.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --fast\n```\n\nLANGUAGE: yaml\nCODE:\n```\nfast: true\n```\n\n----------------------------------------\n\nTITLE: API Version Configuration\nDESCRIPTION: Specifies the API version for model interactions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_version 2.0.2\n```\n\nLANGUAGE: yaml\nCODE:\n```\napi_version: 2.0.2\n```\n\n----------------------------------------\n\nTITLE: Deleting Calendar Events in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function deletes a specific calendar event based on the provided details using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.calendar.delete_event(event_title=\"Title\", start_date=datetime, calendar=\"Work\")\n```\n\n----------------------------------------\n\nTITLE: Using Custom Model with Open Interpreter in Python\nDESCRIPTION: Demonstrates how to use the configured custom language model with Open Interpreter's chat interface.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/custom-models.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"Hi!\") # Returns/displays \"Hi!\" character by character\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with Server dependencies\nDESCRIPTION: Installs Open Interpreter with additional dependencies required for server functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter[server]\n```\n\n----------------------------------------\n\nTITLE: Configuring Safe Mode in YAML Config\nDESCRIPTION: Example YAML configuration for Open Interpreter that enables Safe Mode with the 'ask' option, which will prompt for confirmation before scanning code.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/safety/safe-mode.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: gpt-4\ntemperature: 0\nverbose: false\nsafe_mode: ask\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Responses in Python\nDESCRIPTION: Python code demonstrating how to stream chat responses chunk by chunk.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmessage = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Setting vLLM Model\nDESCRIPTION: Examples demonstrating how to specify the vLLM model to use with Open Interpreter in both terminal and Python environments.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/vllm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model vllm/<perplexity-model>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"vllm/<perplexity-model>\"\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Enabling Vision Mode in Open Interpreter\nDESCRIPTION: Activates vision mode, adding special instructions to the prompt and switching to a vision-capable model (gpt-4o).\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --vision\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.model = \"gpt-4o\" # Any vision supporting model\ninterpreter.llm.supports_vision = True\ninterpreter.llm.supports_functions = True\n\ninterpreter.custom_instructions = \"\"\"The user will show you an image of the code you write. You can view images directly.\nFor HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemeal—write all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.\nIf the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.\nIf you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you.\"\"\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nloop: True\n\nllm:\n  model: \"gpt-4o\"\n  temperature: 0\n  supports_vision: True\n  supports_functions: True\n  context_window: 110000\n  max_tokens: 4096\n  custom_instructions: >\n    The user will show you an image of the code you write. You can view images directly.\n    For HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemeal—write all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.\n    If the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.\n    If you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you.\n```\n\n----------------------------------------\n\nTITLE: Enabling OS Mode in Open Interpreter\nDESCRIPTION: Activates OS mode for multimodal models. This feature provides extended system access and is not currently available in Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --os\n```\n\nLANGUAGE: yaml\nCODE:\n```\nos: true\n```\n\n----------------------------------------\n\nTITLE: Local Mode Configuration\nDESCRIPTION: Enables local model execution instead of cloud-based processing.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --local\n```\n\nLANGUAGE: yaml\nCODE:\n```\nlocal: true\n```\n\n----------------------------------------\n\nTITLE: One-line Windows installer for Open Interpreter\nDESCRIPTION: Experimental one-line installer script for Windows using PowerShell that automatically sets up Python with Conda, creates an environment, and installs Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_6\n\nLANGUAGE: powershell\nCODE:\n```\niex \"& {$(irm https://raw.githubusercontent.com/openinterpreter/open-interpreter/main/installers/oi-windows-installer-conda.ps1)}\"\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with local Llamafile\nDESCRIPTION: Command to run Open Interpreter locally using Llamafile without any third-party software installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/local-models/lm-studio.mdx#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Message Structure Example in Open Interpreter 0.2.0\nDESCRIPTION: Illustrates the new flat message structure where each message contains only one type of data and has specific role, type, content, and format properties.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[\n  {\"role\": \"user\", \"type\": \"message\", \"content\": \"Please create a plot from this data and display it as an image and then as HTML.\"}, # implied format: text (only one format for type message)\n  {\"role\": \"user\", \"type\": \"image\", \"format\": \"path\", \"content\": \"path/to/image.png\"}\n  {\"role\": \"user\", \"type\": \"file\", \"content\": \"/path/to/file.pdf\"} # implied format: path (only one format for type file)\n  {\"role\": \"assistant\", \"type\": \"message\", \"content\": \"Processing your request to generate a plot.\"} # implied format: text\n  {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": \"plot = create_plot_from_data('data')\\ndisplay_as_image(plot)\\ndisplay_as_html(plot)\"}\n  {\"role\": \"computer\", \"type\": \"image\", \"format\": \"base64.png\", \"content\": \"base64\"}\n  {\"role\": \"computer\", \"type\": \"code\", \"format\": \"html\", \"content\": \"<html>Plot in HTML format</html>\"}\n  {\"role\": \"computer\", \"type\": \"console\", \"format\": \"output\", \"content\": \"{HTML errors}\"}\n  {\"role\": \"assistant\", \"type\": \"message\", \"content\": \"Plot generated successfully.\"} # implied format: text\n]\n```\n\n----------------------------------------\n\nTITLE: Changing language model in terminal\nDESCRIPTION: Commands to change the language model used by Open Interpreter in the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Output Tokens\nDESCRIPTION: Shows how to set the maximum token limit for output responses.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.max_output = 2000\n```\n\n----------------------------------------\n\nTITLE: Using fast mode in Open Interpreter via shell\nDESCRIPTION: Shell command to launch Open Interpreter with the faster gpt-3.5-turbo model instead of the default model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --fast\n```\n\n----------------------------------------\n\nTITLE: Temperature Setting Configuration\nDESCRIPTION: Controls the randomness level of model outputs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --temperature 0.7\n```\n\nLANGUAGE: yaml\nCODE:\n```\ntemperature: 0.7\n```\n\n----------------------------------------\n\nTITLE: Configuring Pre-execution Environment in Open Interpreter\nDESCRIPTION: Shows how to set up the environment before AI interaction by importing modules and setting API keys. The example demonstrates configuring Replicate API access and setting custom instructions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/usage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.run(\"python\", \"import replicate\\nreplicate.api_key='...'\")\n\ninterpreter.custom_instructions = \"Replicate has already been imported.\"\n\ninterpreter.chat(\"Please generate an image on replicate...\") # Interpreter will be logged into Replicate\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter with Petals Model using CLI\nDESCRIPTION: Command to launch Open Interpreter with a Petals model using the CLI. The model flag must begin with 'petals/' followed by the model path.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/petals.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model petals/petals-team/StableBeluga2\n```\n\n----------------------------------------\n\nTITLE: Using the Pre-instantiated Interpreter Object\nDESCRIPTION: Demonstrates how to use the pre-instantiated 'interpreter' object provided by the module for convenience.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n # From the module `interpreter`, import the included instance of `OpenInterpreter`\nfrom interpreter import interpreter\n\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter LLM Settings\nDESCRIPTION: Initializes Open Interpreter with Ollama/llama3.1 model settings, including token limits, context window, and execution parameters.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/talk_to_your_database.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# LLM settings\ninterpreter.llm.model = \"ollama/llama3.1\"\ninterpreter.llm.supports_functions = False\ninterpreter.llm.execution_instructions = False\ninterpreter.llm.max_tokens = 1000\ninterpreter.llm.context_window = 7000\ninterpreter.llm.load() \n\n# Computer settings\ninterpreter.computer.import_computer_api = False\n\n# Misc settings\ninterpreter.auto_run = False\ninterpreter.offline = True\n```\n\n----------------------------------------\n\nTITLE: Configuring Offline Mode\nDESCRIPTION: Demonstrates how to enable or disable offline features like open procedures.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.offline = True  # Check for updates, use procedures\ninterpreter.offline = False  # Don't check for updates, don't use procedures\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter Locally with Llamafile\nDESCRIPTION: Shell command to run Open Interpreter locally using Llamafile without additional software.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local\n```\n\n----------------------------------------\n\nTITLE: Max Tokens Configuration\nDESCRIPTION: Limits the maximum number of tokens in model responses.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_tokens 100\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmax_tokens: 100\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Terminal for Open Interpreter\nDESCRIPTION: Command-line example showing how to disable telemetry when running Open Interpreter from the terminal by using the --disable_telemetry flag.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/telemetry/telemetry.mdx#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --disable_telemetry\n```\n\n----------------------------------------\n\nTITLE: Configuring Safe Mode in Open Interpreter YAML Config\nDESCRIPTION: Example YAML configuration for Open Interpreter, demonstrating how to set the safe_mode option. This config also sets the model, temperature, and verbose options.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: gpt-4\ntemperature: 0\nverbose: false\nsafe_mode: ask\n```\n\n----------------------------------------\n\nTITLE: Scrolling Mouse in Python using Open Interpreter\nDESCRIPTION: This function scrolls the mouse a specified number of pixels using the Open Interpreter computer API. Negative values scroll down, positive values scroll up.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Scroll Down\ninterpreter.computer.mouse.scroll(-10)\n\n# Scroll Up\ninterpreter.computer.mouse.scroll(10)\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter with Custom Instructions Flag\nDESCRIPTION: A docker-compose command that runs Open Interpreter with custom instructions. This demonstrates how to pass command-line flags to the interpreter when running in Docker.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose run --rm oi interpreter --custom_instructions \"Be as concise as possible\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter Profile with Vision Model\nDESCRIPTION: Sets up an Open Interpreter profile with LLM and computer settings for photo organization. Configures the vision model (defaulting to GPT-4o), sets execution parameters, and initializes necessary components.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/organize_photos.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nThis is an Open Interpreter profile to organize a directory of photos. \n\"\"\"\n\nfrom interpreter import interpreter\n\n\n# LLM settings\ninterpreter.llm.model = \"gpt-4o\"\n#interpreter.llm.model = \"ollama/codestral\"\ninterpreter.llm.supports_vision = True\ninterpreter.llm.execution_instructions = False\ninterpreter.llm.max_tokens = 1000\ninterpreter.llm.context_window = 7000\ninterpreter.llm.load()  # Loads Ollama models\n\n# Computer settings\ninterpreter.computer.import_computer_api = True\n\n# Misc settings\ninterpreter.auto_run = False\n\n```\n\n----------------------------------------\n\nTITLE: Resetting chat history in Python\nDESCRIPTION: Code to reset the chat history in Open Interpreter when using Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.messages = []\n```\n\n----------------------------------------\n\nTITLE: Setting Context Window and Max Tokens for Local Mode\nDESCRIPTION: Command to run Open Interpreter locally with custom settings for max tokens and context window size.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local --max_tokens 1000 --context_window 3000\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Container for Open Interpreter\nDESCRIPTION: Shows the commands to build a Docker image for Open Interpreter and run it as a container, exposing the server on port 8000.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t open-interpreter .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8000:8000 open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Setting Custom API Base in Open Interpreter\nDESCRIPTION: Specifies a custom API base URL when using custom API services with Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base \"https://api.example.com\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.api_base = \"https://api.example.com\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  api_base: https://api.example.com\n```\n\n----------------------------------------\n\nTITLE: Setting Max Output in Open Interpreter\nDESCRIPTION: Sets the maximum number of characters for code outputs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_output 1000\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.max_output = 1000\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  max_output: 1000\n```\n\n----------------------------------------\n\nTITLE: Max Output Configuration\nDESCRIPTION: Sets the maximum character limit for code outputs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --max_output 1000\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmax_output: 1000\n```\n\n----------------------------------------\n\nTITLE: Enabling Acknowledgment Feature with Environment Variables in Bash\nDESCRIPTION: Shows how to enable the acknowledgment feature for reliable message delivery by setting the appropriate environment variable before starting the server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nexport INTERPRETER_REQUIRE_ACKNOWLEDGE=\"True\"\ninterpreter --server\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter and Setting Current Time in Python\nDESCRIPTION: This snippet sets up Open Interpreter with specific model and parameter configurations. It also adds the current date and time in UTC format.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/screenpipe.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Import necessary libraries\nfrom interpreter import interpreter\nfrom datetime import datetime, timezone\n\n# Configure Open Interpreter\ninterpreter.llm.model = \"groq/llama-3.1-70b-versatile\"\ninterpreter.computer.import_computer_api = False\ninterpreter.llm.supports_functions = False\ninterpreter.llm.supports_vision = False\ninterpreter.llm.context_window = 100000\ninterpreter.llm.max_tokens = 4096\n\n# Add the current date and time in UTC\ncurrent_datetime = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\nprint(f\"Current date and time: {current_datetime}\")\n```\n\n----------------------------------------\n\nTITLE: Managing Message History\nDESCRIPTION: Demonstrates how to work with conversation history and restore previous conversations.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat(\"Hi! Can you print hello world?\")\n\nprint(interpreter.messages)\n\n# This would output:\n\n# [\n#    {\n#       \"role\": \"user\",\n#       \"message\": \"Hi! Can you print hello world?\"\n#    },\n#    {\n#       \"role\": \"assistant\",\n#       \"message\": \"Sure!\"\n#    }\n#    {\n#       \"role\": \"assistant\",\n#       \"language\": \"python\",\n#       \"code\": \"print('Hello, World!')\",\n#       \"output\": \"Hello, World!\"\n#    }\n# ]\n\n#You can use this to restore `interpreter` to a previous conversation.\ninterpreter.messages = messages # A list that resembles the one above\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Budget Limit in Open Interpreter\nDESCRIPTION: This code demonstrates how to set a maximum budget limit for an interpreter session using the max_budget property. The value is specified in USD and will prevent the interpreter from exceeding this spending threshold.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/budget-manager.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.max_budget = 0.01 # 1 cent\n```\n\n----------------------------------------\n\nTITLE: Selecting a Profile in Open Interpreter\nDESCRIPTION: Selects a specific profile to use for configuration. If no profile is specified, the default profile will be used.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profile local.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenRouter with Open Interpreter via Command Line\nDESCRIPTION: Command line example showing how to specify an OpenRouter model for Open Interpreter by using the --model flag with the openrouter/ prefix.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openrouter.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model openrouter/openai/gpt-3.5-turbo\n```\n\n----------------------------------------\n\nTITLE: Setting Safe Mode in Open Interpreter\nDESCRIPTION: Configures experimental safety mechanisms like code scanning. Valid options are 'off', 'ask', and 'auto'.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --safe_mode ask\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.safe_mode = 'ask'\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsafe_mode: ask\n```\n\n----------------------------------------\n\nTITLE: Moving Mouse Cursor in Python using Open Interpreter\nDESCRIPTION: This function moves the mouse cursor to specified coordinates, text, or icons using the Open Interpreter computer API. It can use OCR to find text coordinates.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Click on coordinates\ninterpreter.computer.mouse.move(x=100, y=100)\n\n# Click on text on the screen\ninterpreter.computer.mouse.move(\"Onscreen Text\")\n\n# Click on a gear icon\ninterpreter.computer.mouse.move(icon=\"gear icon\")\n```\n\n----------------------------------------\n\nTITLE: Enabling and disabling verbose mode in chat\nDESCRIPTION: Commands to enable and disable verbose mode during an Open Interpreter chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n> %verbose true <- Enables verbose mode\n\n> %verbose false <- Disables verbose mode\n```\n\n----------------------------------------\n\nTITLE: JARVIS Voice Interface Implementation\nDESCRIPTION: Main implementation of the JARVIS voice interface using Gradio for the frontend.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    audio_input = gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n    btn = gr.Button(\"Submit\")\n    # ... (full implementation)\n```\n\n----------------------------------------\n\nTITLE: Setting E2B API Key for Python Cloud Execution\nDESCRIPTION: Configures the E2B API key as an environment variable for cloud-based code execution.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/custom-languages.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"E2B_API_KEY\"] = \"<your_api_key_here>\"\n```\n\n----------------------------------------\n\nTITLE: Handling Code Execution Confirmation in Python with Open Interpreter\nDESCRIPTION: This snippet shows how to implement a confirmation step before executing code in a streamed response. It checks for an 'executing' flag in the chunk and prompts the user for confirmation before proceeding with code execution.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/streaming-response.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This example asks the user before running code\n\nfor chunk in interpreter.chat(\"What's 34/24?\", stream=True):\n    if \"executing\" in chunk:\n        if input(\"Press ENTER to run this code.\") != \"\":\n            break\n```\n\n----------------------------------------\n\nTITLE: Configuring Verbose Mode\nDESCRIPTION: Demonstrates how to enable or disable verbose logging mode.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.verbose = True  # Turns on verbose mode\ninterpreter.verbose = False  # Turns off verbose mode\n```\n\n----------------------------------------\n\nTITLE: Defining Custom ScreenPipe Search Function in Python\nDESCRIPTION: This snippet defines a custom function for searching ScreenPipe data. It includes handling for query parameters, time range filtering, and duplicate removal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/screenpipe.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define the custom ScreenPipe search function\ncustom_tool = \"\"\"\nimport requests\nimport json\nfrom urllib.parse import quote\n\ndef search_screenpipe(query, limit=5, start_time=None, end_time=None):\n    base_url = f\"http://localhost:3030/search?q={quote(query)}&content_type=ocr&limit={limit}\"\n    \n    if start_time:\n        base_url += f\"&start_time={quote(start_time)}\"\n    if end_time:\n        base_url += f\"&end_time={quote(end_time)}\"\n    \n    response = requests.get(base_url)\n    if response.status_code == 200:\n        data = response.json()\n        # Remove duplicates based on text content\n        unique_results = []\n        seen_texts = set()\n        for item in data[\"data\"]:\n            text = item[\"content\"][\"text\"]\n            if text not in seen_texts:\n                unique_results.append(item)\n                seen_texts.add(text)\n        return unique_results\n    else:\n        return f\"Error: Unable to fetch data from ScreenPipe. Status code: {response.status_code}\"\n\"\"\"\n\n# Add the custom tool to the interpreter's environment\ninterpreter.computer.run(\"python\", custom_tool)\nprint(\"ScreenPipe search function defined and added to the interpreter's environment.\")\n```\n\n----------------------------------------\n\nTITLE: Customizing System Message in Python\nDESCRIPTION: Modify the system message to extend functionality, set permissions, or provide additional context to the interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Initializing Open Interpreter Demo\nDESCRIPTION: Demonstrates basic usage of Open Interpreter with streaming output.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor chunk in interpreter.chat(\"What's 34/24?\", stream=True, display=False):\n  print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Setting Custom SQL Instructions for Open Interpreter\nDESCRIPTION: Defines custom instructions for the interpreter to handle SQL queries, including database context, execution guidelines, and response formatting requirements.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/talk_to_your_database.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Custom Instructions\ninterpreter.custom_instructions = f\"\"\"\n    You are a SQL master and are the oracle of database knowledge. You are obsessed with SQL. You only want to discuss SQL. SQL is life.\n    Recap the plan before answering the user's query.\n    You will connect to a PostgreSQL database, with the connection string {connection_string}.\n    Remember to only query the {db_name} database.\n    Execute valid SQL commands to satisfy the user's query.\n    Write all code in a full Python script. When you have to re-write code, redo the entire script.\n    Execute the script to get the answer for the user's query.\n    **YOU CAN EXECUTE SQL COMMANDS IN A PYTHON SCRIPT.***\n    Get the schema of '{db_name}' before writing any other SQL commands. It is important to know the tables. This will let you know what commands are correct.\n    Only use real column names.\n    ***You ARE fully capable of executing SQL commands.***\n    Be VERY clear about the answer to the user's query. They don't understand technical jargon so make it very clear and direct.\n    You should respond in a very concise way.\n    You can do it, I believe in you.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Config File with Docker\nDESCRIPTION: A docker-compose command that specifies a custom configuration file for Open Interpreter. This allows using configuration options beyond the default settings when running in a container.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose run --rm oi interpreter --config_file config.yml\n```\n\n----------------------------------------\n\nTITLE: Setting Execution Instructions in Open Interpreter\nDESCRIPTION: Provides instructions to the language model about how to execute code when function calling is not supported.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.execution_instructions = \"To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language.\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.execution_instructions = \"To execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language.\"\n```\n\n----------------------------------------\n\nTITLE: Supported OpenRouter Models via Command Line\nDESCRIPTION: List of commands showing the various supported OpenRouter models that can be used with Open Interpreter, including models from OpenAI, Anthropic, Google, and Meta.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/openrouter.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --model openrouter/openai/gpt-3.5-turbo\ninterpreter --model openrouter/openai/gpt-3.5-turbo-16k\ninterpreter --model openrouter/openai/gpt-4\ninterpreter --model openrouter/openai/gpt-4-32k\ninterpreter --model openrouter/anthropic/claude-2\ninterpreter --model openrouter/anthropic/claude-instant-v1\ninterpreter --model openrouter/google/palm-2-chat-bison\ninterpreter --model openrouter/google/palm-2-codechat-bison\ninterpreter --model openrouter/meta-llama/llama-2-13b-chat\ninterpreter --model openrouter/meta-llama/llama-2-70b-chat\n```\n\n----------------------------------------\n\nTITLE: ElevenLabs Voice Synthesis Functions\nDESCRIPTION: Implementation of text-to-speech functionality using ElevenLabs API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef speak(text):\n  speaking = True\n  audio = generate(\n      text=text,\n      voice=\"Daniel\"\n  )\n  play(audio, notebook=True)\n\n  audio_length = get_audio_length(audio)\n  time.sleep(audio_length)\n```\n\n----------------------------------------\n\nTITLE: Python Type Definitions for Messages and Streaming Chunks\nDESCRIPTION: Defines Python type classes for Messages and StreamingChunks, specifying the possible values for role, type, format, recipient, and content properties.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Message:\n    role: Union[\"user\", \"assistant\", \"computer\"]\n    type: Union[\"message\", \"code\", \"image\", \"console\", \"file\", \"confirmation\"]\n    format: Union[\"output\", \"path\", \"base64.png\", \"base64.jpeg\", \"python\", \"javascript\", \"shell\", \"html\", \"active_line\", \"execution\"]\n    recipient: Union[\"user\", \"assistant\"]\n    content: Union[str, dict]  # dict should have 'code' and 'language' keys, this is only for confirmation messages\n\nclass StreamingChunk(Message):\n    start: bool\n    end: bool\n```\n\n----------------------------------------\n\nTITLE: Setting API Configuration\nDESCRIPTION: Demonstrates how to configure API settings including custom base URL and API key.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/arguments.mdx#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.api_base = \"https://api.example.com\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.api_key = \"your_api_key_here\"\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Instructions for Open Interpreter in Python\nDESCRIPTION: This snippet sets custom instructions for Open Interpreter, providing guidance on using ScreenPipe for context-aware interactions and information retrieval.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/screenpipe.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Set custom instructions for Open Interpreter\ninterpreter.custom_instructions = f\"\"\"\nCurrent date and time: {current_datetime}\n\nScreenPipe is a powerful tool that continuously captures and indexes the content displayed on your screen. It creates a searchable history of everything you've seen or interacted with on your computer. This includes text from websites, documents, applications, and even images (through OCR).\n\nYou have access to this wealth of information through the `search_screenpipe(query, limit=5, start_time=None, end_time=None)` function. This allows you to provide more contextual and personalized assistance based on the user's recent activities and viewed content.\n\nThe `search_screenpipe` function supports optional `start_time` and `end_time` parameters to narrow down the search to a specific time range. The time format should be ISO 8601, like this: \"2024-10-16T12:00:00Z\".\n\nHere's why querying ScreenPipe is valuable:\n1. Context Recall: Users often refer to things they've seen recently but may not remember the exact details. ScreenPipe can help recall this information.\n2. Information Verification: You can cross-reference user claims or questions with actual content they've viewed.\n3. Personalized Assistance: By knowing what the user has been working on or researching, you can provide more relevant advice and suggestions.\n4. Productivity Enhancement: You can help users quickly locate information they've seen before but can't remember where.\n\nUse the `search_screenpipe()` function when:\n- The user asks about something they've seen or read recently.\n- You need to verify or expand on information the user mentions.\n- You want to provide context-aware suggestions or assistance.\n- The user is trying to recall specific details from their recent computer usage.\n- The user wants to search within a specific time range.\n\nHere's how to use it effectively:\n1. When a user's query relates to recent activities or viewed content, identify key terms for the search.\n2. If the user specifies a time range, use the `start_time` and `end_time` parameters.\n3. Call the `search_screenpipe()` function with these parameters.\n4. Analyze the results to find relevant information.\n5. Summarize the findings for the user, mentioning the source (app name, window name) and when it was seen (timestamp).\n\nRemember to use this tool proactively when you think it might help answer the user's question, even if they don't explicitly mention ScreenPipe.\n\"\"\"\n\nprint(\"Custom instructions set for Open Interpreter.\")\n```\n\n----------------------------------------\n\nTITLE: Mounting Volumes to Docker Container for File Access\nDESCRIPTION: A bash command template for mounting a volume from the host machine to the Open Interpreter Docker container. This allows the interpreter to access and manipulate files from specific directories on the host.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -it -v /path/on/your/host:/path/in/the/container --name interpreter-instance openinterpreter interpreter\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Open Interpreter (CLI)\nDESCRIPTION: Shell commands demonstrating how to change the language model used by Open Interpreter from the command line.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Models via Command Line\nDESCRIPTION: Shell command to configure max tokens and context window for locally running models.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --local --max_tokens 1000 --context_window 3000\n```\n\n----------------------------------------\n\nTITLE: Initializing Interactive Chat in Open Interpreter (Python)\nDESCRIPTION: This code snippet demonstrates how to start an interactive chat session in Open Interpreter, where magic commands can be used for controlling the session behavior.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/magic-commands.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.chat()\n```\n\n----------------------------------------\n\nTITLE: Enabling Acknowledgment Feature in Python\nDESCRIPTION: Shows how to enable the acknowledgment feature for reliable message delivery by setting the environment variable in Python before starting the server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"INTERPRETER_REQUIRE_ACKNOWLEDGE\"] = \"True\"\n\nfrom interpreter import AsyncInterpreter\nasync_interpreter = AsyncInterpreter()\nasync_interpreter.server.run()\n```\n\n----------------------------------------\n\nTITLE: Accessing Saved Conversations in Terminal\nDESCRIPTION: List and restore previous conversations from the terminal using the --conversations flag.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --conversations\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting Up Llama3 Model\nDESCRIPTION: Downloads the Meta-Llama-3-8B-Instruct model file and makes it executable for local use.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local3.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Download the Meta-Llama-3-8B-Instruct.llamafile\n!curl -L -o Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile 'https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile?download=true'\n\n# Make the downloaded file executable\n!chmod +x Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile\n```\n\n----------------------------------------\n\nTITLE: Getting Screen Center Coordinates in Python using Open Interpreter\nDESCRIPTION: This function retrieves the x and y coordinates of the center of the screen using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx, y = interpreter.computer.display.center()\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter locally in terminal\nDESCRIPTION: Command to run Open Interpreter locally using a custom API base URL.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"\n```\n\n----------------------------------------\n\nTITLE: Opening Open Interpreter Profile Settings via CLI\nDESCRIPTION: Command to open and view the profile settings configuration file for Open Interpreter. This accesses default settings that apply across both Python and terminal interfaces.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/settings.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profiles\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter in Terminal\nDESCRIPTION: Command to start Open Interpreter in interactive mode from the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Execute Code Block Command in JSON\nDESCRIPTION: Shows the JSON structure for sending a 'go' command to execute a generated code block and allow the agent to proceed.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"role\": \"user\", \"type\": \"command\", \"content\": \"go\"}\n```\n\n----------------------------------------\n\nTITLE: Controlling Terminal Cursor in Unix-like Systems with ANSI Escape Codes\nDESCRIPTION: This snippet demonstrates how to use ANSI escape codes to control the cursor appearance in terminal environments on Unix-like systems. It shows examples for setting the cursor style, enabling/disabling blinking, and showing/hiding the cursor.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/computer/custom-languages.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\n# Set cursor style\nprint(\"\\033[0 q\")  # 0: blinking block\nprint(\"\\033[1 q\")  # 1: blinking block (default)\nprint(\"\\033[2 q\")  # 2: steady block\nprint(\"\\033[3 q\")  # 3: blinking underline\nprint(\"\\033[4 q\")  # 4: steady underline\nprint(\"\\033[5 q\")  # 5: blinking bar\nprint(\"\\033[6 q\")  # 6: steady bar\n\n# Enable/disable cursor blinking\nprint(\"\\033[?12h\")  # Enable cursor blinking\nprint(\"\\033[?12l\")  # Disable cursor blinking\n\n# Show/hide cursor\nprint(\"\\033[?25h\")  # Show cursor\nprint(\"\\033[?25l\")  # Hide cursor\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with Safety Toolkit Dependencies\nDESCRIPTION: Command to install Open Interpreter with the safety toolkit dependencies bundled together. This ensures all necessary components for safe mode are installed.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter[safe]\n```\n\n----------------------------------------\n\nTITLE: Performing Keyboard Hotkeys in Python using Open Interpreter\nDESCRIPTION: This function executes a keyboard hotkey combination on the computer using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.keyboard.hotkey(\" \", \"command\")\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Standard installation of Open Interpreter using Python's package installer (pip). Requires Python 3.10 or 3.11.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/getting-started/setup.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Opening Profiles Directory in Open Interpreter\nDESCRIPTION: Opens the profiles directory where yaml profile configuration files can be added.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --profiles\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter using pip\nDESCRIPTION: Command to install Open Interpreter using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Error Message Format in JSON\nDESCRIPTION: Demonstrates the format of error messages sent by the server when an issue occurs during processing.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"role\": \"server\", \"type\": \"error\", \"content\": \"Error traceback information\"}\n```\n\n----------------------------------------\n\nTITLE: Getting Selected Text in Python using Open Interpreter\nDESCRIPTION: This function retrieves the currently selected text on the screen using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.os.get_selected_text()\n```\n\n----------------------------------------\n\nTITLE: Building and Running Open Interpreter Docker Container\nDESCRIPTION: Bash commands to build a Docker image for Open Interpreter, run it as a detached interactive container, and attach to the container to start using the interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/docker.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t openinterpreter .\ndocker run -d -it --name interpreter-instance openinterpreter interpreter\ndocker attach interpreter-instance\n```\n\n----------------------------------------\n\nTITLE: Counting Unread Emails in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function retrieves the count of unread emails in the inbox using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.mail.unread_count()\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter in the terminal\nDESCRIPTION: Command to start Open Interpreter in interactive mode from the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_UK.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter and Tests with Poetry\nDESCRIPTION: Commands for running the Open Interpreter CLI and executing tests using Poetry.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run interpreter\npoetry run pytest -s -x\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip in shell\nDESCRIPTION: Command to install the Open Interpreter package using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_IN.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry for Open Interpreter\nDESCRIPTION: Commands for installing project dependencies using Poetry, including optional dependencies for different modes.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\npoetry install -E local\npoetry install -E os\npoetry install -E local -E os\n```\n\n----------------------------------------\n\nTITLE: Retrieving Contact Email Addresses in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function returns the email address of a specified contact name using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.contacts.get_phone_number(\"John Doe\")\n```\n\n----------------------------------------\n\nTITLE: Defining AWS Documentation Search Custom Tool\nDESCRIPTION: Creates a custom tool that searches AWS documentation using the Perplexity API. The tool requires a PPLX_API_KEY environment variable and returns search results in text format.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/custom_tool.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncustom_tool = \"\"\"\n\nimport os\nimport requests\n\ndef search_aws_docs(query):\n\n    url = \"https://api.perplexity.ai/chat/completions\"\n\n    payload = {\n        \"model\": \"llama-3.1-sonar-small-128k-online\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"Be precise and concise.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query\n            }\n        ],\n        \"temperature\": 0.2,\n        \"top_p\": 0.9,\n        \"return_citations\": True,\n        \"search_domain_filter\": [\"docs.aws.amazon.com\"],\n        \"return_images\": False,\n        \"return_related_questions\": False,\n        #\"search_recency_filter\": \"month\",\n        \"top_k\": 0,\n        \"stream\": False,\n        \"presence_penalty\": 0,\n        \"frequency_penalty\": 1\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {os.environ.get('PPLX_API_KEY')}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.request(\"POST\", url, json=payload, headers=headers)\n\n    print(response.text)\n\n    return response.text\n\n\"\n```\n\n----------------------------------------\n\nTITLE: Creating Calendar Events in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function creates a new calendar event with specified details using the Open Interpreter computer API. It uses the first calendar if none is specified and is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.calendar.create_event(title=\"Title\", start_date=datetime, end_date=datetime, location=\"Location\", notes=\"Notes\", calendar=\"Work\")\n```\n\n----------------------------------------\n\nTITLE: Launching Open Interpreter in Terminal\nDESCRIPTION: Command to start Open Interpreter in interactive mode from the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Running Open Interpreter Server in Bash\nDESCRIPTION: Command to start the Open Interpreter OpenAI-compatible server. Additional flags can be used to set the model, context window, or other settings.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/jan_computer_control.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --server\n```\n\n----------------------------------------\n\nTITLE: Launching Open Interpreter from the Command Line\nDESCRIPTION: Command to start Open Interpreter from the terminal after installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Instructions\nDESCRIPTION: Adding custom instructions to the system message for specifying system information and preferences.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --custom_instructions \"This is a custom instruction.\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.custom_instructions = \"This is a custom instruction.\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncustom_instructions: \"This is a custom instruction.\"\n```\n\n----------------------------------------\n\nTITLE: Sending Emails in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function sends an email with the given parameters using the default mail app through the Open Interpreter computer API. It supports attachments and is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.mail.send(\"john@email.com\", \"Subject\", \"Body\", [\"path/to/attachment.pdf\", \"path/to/attachment2.pdf\"])\n```\n\n----------------------------------------\n\nTITLE: Modifying Server Settings via HTTP API in Python\nDESCRIPTION: Shows how to change server settings using the HTTP API by sending a POST request with a payload conforming to the interpreter object's settings.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nsettings = {\n    \"llm\": {\"model\": \"gpt-4\"},\n    \"custom_instructions\": \"You only write Python code.\",\n    \"auto_run\": True,\n}\nresponse = requests.post(\"http://localhost:8000/settings\", json=settings)\nprint(response.status_code)\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Command to install Open Interpreter using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_JA.md#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: API Base Configuration\nDESCRIPTION: Specifies custom API base URL for model interactions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/terminal/arguments.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --api_base \"https://api.example.com\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\napi_base: https://api.example.com\n```\n\n----------------------------------------\n\nTITLE: Installing Developer Dependencies with Poetry\nDESCRIPTION: Command for adding development-specific dependencies to the Open Interpreter project using Poetry.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry add package-name --group dev\n```\n\n----------------------------------------\n\nTITLE: Installing Petals Prerequisites\nDESCRIPTION: Command to install the Petals library from GitHub, which is required to use Petals models with Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/petals.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/bigscience-workshop/petals\n```\n\n----------------------------------------\n\nTITLE: Unsafe Code Review Example in JSON\nDESCRIPTION: Shows the format of a review message when the executed code might be unsafe or have irreversible effects.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role\": \"assistant\",\n  \"type\": \"review\",\n  \"content\": \"This code performs file deletion operations which are irreversible. Please review carefully before proceeding.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Emails in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function fetches a specified number of emails from the inbox, with an option to filter for unread emails, using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.mail.get(number=10, unread=True)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Server Settings via HTTP API in Python\nDESCRIPTION: Shows how to get current server settings by sending a GET request to the settings endpoint with the specific property name.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.get(\"http://localhost:8000/settings/custom_instructions\")\nprint(response.json())\n# Output: {\"custom_instructions\": \"You only write Python code.\"}\n```\n\n----------------------------------------\n\nTITLE: Disabling Function Support in Open Interpreter\nDESCRIPTION: Informs Open Interpreter that the language model does not support function calling.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --no-llm_supports_functions\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.llm.supports_functions = False\n```\n\nLANGUAGE: yaml\nCODE:\n```\nllm:\n  supports_functions: false\n```\n\n----------------------------------------\n\nTITLE: Safe Code Review Example in JSON\nDESCRIPTION: Shows the format of a review message when the executed code is deemed completely safe.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role\": \"assistant\",\n  \"type\": \"review\",\n  \"content\": \"<SAFE>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Pre-commit Hooks\nDESCRIPTION: Commands for installing pre-commit hooks to automatically format code before committing changes.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry via Environment Variables for Open Interpreter\nDESCRIPTION: Environment variable example for disabling telemetry in Open Interpreter when running in Docker. This is set in a .env file placed in the same directory as the docker-compose.yml file.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/telemetry/telemetry.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nDISABLE_TELEMETRY=true\n```\n\n----------------------------------------\n\nTITLE: Chrome Accessibility Tree Navigation Implementation\nDESCRIPTION: Python script that connects to a Chrome instance via DevTools Protocol, navigates to a webpage, and retrieves its accessibility tree. Uses pychrome library to interface with Chrome and includes functions for browser control and accessibility data extraction.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/ROADMAP.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pychrome\nimport time\n\ndef get_accessibility_tree(tab):\n    # Enable the Accessibility domain\n    tab.call_method(\"Accessibility.enable\")\n\n    # Get the accessibility tree\n    tree = tab.call_method(\"Accessibility.getFullAXTree\")\n    return tree\n\ndef main():\n    # Create a browser instance\n    browser = pychrome.Browser(url=\"http://127.0.0.1:9222\")\n\n    # Create a new tab\n    tab = browser.new_tab()\n\n    # Start the tab\n    tab.start()\n\n    # Navigate to a URL\n    tab.set_url(\"https://www.example.com\")\n    time.sleep(3)  # Wait for page to load\n\n    # Retrieve the accessibility tree\n    accessibility_tree = get_accessibility_tree(tab)\n    print(accessibility_tree)\n\n    # Stop the tab (closes it)\n    tab.stop()\n\n    # Close the browser\n    browser.close()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Implementing Flask Server with Open Interpreter\nDESCRIPTION: Sets up a Flask server with Open Interpreter configuration and implements a chat endpoint that processes user prompts and streams responses. Includes both local and hosted model configuration options with customizable parameters.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local_server.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\napp = Flask(__name__)\n\n# Configure Open Interpreter\n\n## Local Model\n# interpreter.offline = True\n# interpreter.llm.model = \"ollama/llama3.1\"\n# interpreter.llm.api_base = \"http://localhost:11434\"\n# interpreter.llm.context_window = 4000\n# interpreter.llm.max_tokens = 3000\n# interpreter.auto_run = True\n# interpreter.verbose = True\n\n## Hosted Model\ninterpreter.llm.model = \"gpt-4o\"\ninterpreter.llm.context_window = 10000\ninterpreter.llm.max_tokens = 4096\ninterpreter.auto_run = True\n\n# Create an endpoint\n@app.route('/chat', methods=['POST'])\ndef chat():\n    # Expected payload: {\"prompt\": \"User's message or question\"}\n    data = request.json\n    prompt = data.get('prompt')\n    \n    if not prompt:\n        return jsonify({\"error\": \"No prompt provided\"}), 400\n\n    full_response = \"\"\n    try:\n        for chunk in interpreter.chat(prompt, stream=True, display=False):\n            if isinstance(chunk, dict):\n                if chunk.get(\"type\") == \"message\":\n                    full_response += chunk.get(\"content\", \"\")\n            elif isinstance(chunk, str):\n                # Attempt to parse the string as JSON\n                try:\n                    json_chunk = json.loads(chunk)\n                    full_response += json_chunk.get(\"response\", \"\")\n                except json.JSONDecodeError:\n                    # If it's not valid JSON, just add the string\n                    full_response += chunk\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n    return jsonify({\"response\": full_response.strip()})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5001)\n\nprint(\"Open Interpreter server is running on http://0.0.0.0:5001\")\n```\n\n----------------------------------------\n\nTITLE: Stop Execution Command in JSON\nDESCRIPTION: Demonstrates the JSON structure for sending a stop command to halt all execution and message processing on the server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"role\": \"user\", \"type\": \"command\", \"content\": \"stop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter with Safety Toolkit\nDESCRIPTION: Command to install Open Interpreter with the safety toolkit dependencies included in the installation bundle.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/safety/safe-mode.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter[safe]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Package with pip\nDESCRIPTION: Command to install the OpenAI Python package using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/gpt-4-setup.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai\n```\n\n----------------------------------------\n\nTITLE: Checking Open Interpreter Version\nDESCRIPTION: Displays the current installed version number of Open Interpreter.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --version\n```\n\n----------------------------------------\n\nTITLE: Whisper Speech Recognition Setup\nDESCRIPTION: Implementation of audio transcription using OpenAI's Whisper model.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef transcribe(audio):\n    audio = whisper.load_audio(audio)\n    audio = whisper.pad_or_trim(audio)\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n    _, probs = model.detect_language(mel)\n    options = whisper.DecodingOptions()\n    result = whisper.decode(model, mel, options)\n    return result.text\n```\n\n----------------------------------------\n\nTITLE: TypeScript Interface Definitions for Messages\nDESCRIPTION: Defines TypeScript interfaces for Message objects, specifying the possible string literal types for role, type, format, recipient, and content properties.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Message {\n  role: \"user\" | \"assistant\" | \"computer\";\n  type: \"message\" | \"code\" | \"image\" | \"console\" | \"file\", | \"confirmation\";\n  format: \"output\" | \"path\" | \"base64.png\" | \"base64.jpeg\" | \"python\" | \"javascript\" | \"shell\" | \"html\" | \"active_line\", | \"execution\";\n  recipient: \"user\" | \"assistant\";\n  content: string | { code: string; language: string };\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Command to install the Open Interpreter package using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_DE.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Completion Status Message in JSON\nDESCRIPTION: Shows the JSON structure the server uses to indicate when an interaction is completed.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"role\": \"server\", \"type\": \"status\", \"content\": \"complete\"}\n```\n\n----------------------------------------\n\nTITLE: Opening Local Models Directory in Open Interpreter\nDESCRIPTION: Opens the models directory where all downloaded Llamafiles are saved.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --local_models\n```\n\n----------------------------------------\n\nTITLE: Installing New Dependencies with Poetry\nDESCRIPTION: Command for adding new dependencies to the Open Interpreter project using Poetry.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry add package-name\n```\n\n----------------------------------------\n\nTITLE: Referencing Telemetry Documentation in Open Interpreter\nDESCRIPTION: This snippet links to the detailed documentation on telemetry usage in Open Interpreter. It provides more information on how telemetry data is collected and used.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/troubleshooting/faq.mdx#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[here](/telemetry/telemetry)\n```\n\n----------------------------------------\n\nTITLE: Simulating Keyboard Input in Python using Open Interpreter\nDESCRIPTION: This function writes text into the currently focused window using the Open Interpreter computer API.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.keyboard.write(\"hello\")\n```\n\n----------------------------------------\n\nTITLE: Installing pychrome Package\nDESCRIPTION: Command to install the pychrome Python package using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/ROADMAP.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pychrome\n```\n\n----------------------------------------\n\nTITLE: Complete WebSocket Interaction Example in Python\nDESCRIPTION: Demonstrates a full WebSocket interaction with the server, including sending multi-part messages and handling different types of responses.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport websockets\nimport json\nimport asyncio\n\nasync def websocket_interaction():\n    async with websockets.connect(\"ws://localhost:8000/\") as websocket:\n        # Send a multi-part user message\n        await websocket.send(json.dumps({\"role\": \"user\", \"start\": True}))\n        await websocket.send(json.dumps({\"role\": \"user\", \"type\": \"message\", \"content\": \"Analyze this image:\"}))\n        await websocket.send(json.dumps({\"role\": \"user\", \"type\": \"image\", \"format\": \"path\", \"content\": \"path/to/image.jpg\"}))\n        await websocket.send(json.dumps({\"role\": \"user\", \"end\": True}))\n\n        # Receive and process messages\n        while True:\n            message = await websocket.recv()\n            data = json.loads(message)\n            \n            if data.get(\"type\") == \"message\":\n                print(f\"Assistant: {data.get('content', '')}\")\n            elif data.get(\"type\") == \"review\":\n                print(f\"Code Review: {data.get('content')}\")\n            elif data.get(\"type\") == \"error\":\n                print(f\"Error: {data.get('content')}\")\n            elif data == {\"role\": \"assistant\", \"type\": \"status\", \"content\": \"complete\"}:\n                print(\"Interaction complete\")\n                break\n\nasyncio.run(websocket_interaction())\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Interpreter Instance Settings\nDESCRIPTION: Sets up basic configuration for an Open Interpreter instance including model selection, API imports, and context parameters.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/custom_tool.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Configure Open Interpreter\nfrom interpreter import interpreter\n\ninterpreter.llm.model = \"claude-3-5-sonnet-20240620\"\ninterpreter.computer.import_computer_api = True\ninterpreter.llm.supports_functions = True\ninterpreter.llm.supports_vision = True\ninterpreter.llm.context_window = 100000\ninterpreter.llm.max_tokens = 4096\n```\n\n----------------------------------------\n\nTITLE: Installing Semgrep Separately for Open Interpreter\nDESCRIPTION: Command to install the semgrep package separately in your virtual environment. Semgrep is used for code scanning in safe mode.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/SAFE_MODE.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install semgrep\n```\n\n----------------------------------------\n\nTITLE: Controlling Terminal Cursor in Windows with ANSI Escape Codes\nDESCRIPTION: This snippet shows how to control cursor appearance in Windows terminals using ANSI escape codes. It includes examples for showing/hiding the cursor, although Windows terminal support for cursor style modification is more limited compared to Unix systems.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/computer/custom-languages.mdx#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\n# Show/hide cursor in Windows terminal\nprint(\"\\033[?25h\")  # Show cursor\nprint(\"\\033[?25l\")  # Hide cursor\n```\n```\n\n----------------------------------------\n\nTITLE: Simulating Mouse Clicks in Python using Open Interpreter\nDESCRIPTION: This function performs mouse clicks at specified coordinates, on text, or on icons using the Open Interpreter computer API. It can use OCR to find text coordinates.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Click on coordinates\ninterpreter.computer.mouse.click(x=100, y=100)\n\n# Click on text on the screen\ninterpreter.computer.mouse.click(\"Onscreen Text\")\n\n# Click on a gear icon\ninterpreter.computer.mouse.click(icon=\"gear icon\")\n```\n\n----------------------------------------\n\nTITLE: Installing Open Interpreter via pip\nDESCRIPTION: Command to install Open Interpreter using pip package manager.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_VN.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install open-interpreter\n```\n\n----------------------------------------\n\nTITLE: Basic WebSocket Message Format in JSON\nDESCRIPTION: Shows the basic structure for sending messages to the Open Interpreter server using start and end flags in LMC message format.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"role\": \"user\", \"start\": true}\n{\"role\": \"user\", \"type\": \"message\", \"content\": \"Your message here\"}\n{\"role\": \"user\", \"end\": true}\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Open Interpreter Server\nDESCRIPTION: Imports required Python modules including Flask for the web server, and the interpreter package for AI functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/local_server.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import Flask, request, jsonify\nfrom interpreter import interpreter\nimport json\n```\n\n----------------------------------------\n\nTITLE: Installing Semgrep Separately\nDESCRIPTION: Command to install the semgrep vulnerability scanner separately in your virtual environment, which is a required dependency for the Safe Mode feature.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/safety/safe-mode.mdx#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install semgrep\n```\n\n----------------------------------------\n\nTITLE: Sending SMS in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function sends a text message using the default SMS app through the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.sms.send(\"2068675309\", \"Hello from Open Interpreter!\")\n```\n\n----------------------------------------\n\nTITLE: Code Review Message Format in JSON\nDESCRIPTION: Shows the format of the review message sent after code blocks are executed, indicating safety assessment and potential effects.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/server/usage.mdx#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role\": \"assistant\",\n  \"type\": \"review\",\n  \"content\": \"Review of the executed code, including safety assessment and potential irreversible actions.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Database Connection in Python\nDESCRIPTION: Sets up database connection parameters using environment variables with fallback defaults. Constructs a PostgreSQL connection string with optional password handling.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/talk_to_your_database.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\nimport os\n\n# Use environment variables for database connection or update defaults with your credentials\ndb_user = os.environ.get(\"DB_USER\", \"user\")\ndb_host = os.environ.get(\"DB_HOST\", \"localhost\")\ndb_port = os.environ.get(\"DB_PORT\", \"5432\")\ndb_name = os.environ.get(\"DB_NAME\", \"demo_database\")\ndb_password = os.environ.get(\"DB_PASSWORD\", \"\")\n\n# Construct connection string with optional password\nif db_password and db_password.strip():\n    connection_string = (\n        f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n    )\nelse:\n    connection_string = f\"postgresql://{db_user}@{db_host}:{db_port}/{db_name}\"\n```\n\n----------------------------------------\n\nTITLE: Starting a New Chat in Terminal\nDESCRIPTION: Launch a fresh chat session in the terminal, which doesn't retain memory of previous conversations.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Package Installation Commands\nDESCRIPTION: Installation commands for required packages including open-interpreter, whisper, gradio, and elevenlabs.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install open-interpreter\n!pip install git+https://github.com/openai/whisper.git -q\n!pip install gradio==3.50 -q\n!pip install elevenlabs -q\n```\n\n----------------------------------------\n\nTITLE: Displaying Help in Open Interpreter\nDESCRIPTION: Shows all available terminal arguments and their descriptions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --help\n```\n\n----------------------------------------\n\nTITLE: Starting Interactive Chat in Terminal\nDESCRIPTION: Launch an interactive chat session with Open Interpreter directly from your terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Installing Offline Documentation Dependencies\nDESCRIPTION: Command for installing Mintlify for accessing offline documentation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g mintlify@latest\n```\n\n----------------------------------------\n\nTITLE: Modifying System Message\nDESCRIPTION: Configuration for modifying the core system message, though custom instructions are recommended instead.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/settings/all-settings.mdx#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ninterpreter --system_message \"You are Open Interpreter...\"\n```\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message = \"You are Open Interpreter...\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsystem_message: \"You are Open Interpreter...\"\n```\n\n----------------------------------------\n\nTITLE: Resetting Chat History in Python\nDESCRIPTION: Clear the conversation history in Python to start a fresh chat session.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/examples.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.messages = []\n```\n\n----------------------------------------\n\nTITLE: TypeScript Interface Definition for Streaming Chunks\nDESCRIPTION: Defines a TypeScript interface for StreamingChunk that extends the Message interface to include start and end boolean properties for tracking chunks in a stream.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ninterface StreamingChunk extends Message {\n  start: boolean;\n  end: boolean;\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Structure Example in Open Interpreter 0.2.0\nDESCRIPTION: Shows the streaming data structure with start/end markers for chunks, and includes examples of message, code, confirmation, and console output chunks.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/NCU_MIGRATION_GUIDE.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{\"role\": \"assistant\", \"type\": \"message\", \"start\": True}\n{\"role\": \"assistant\", \"type\": \"message\", \"content\": \"Pro\"}\n{\"role\": \"assistant\", \"type\": \"message\", \"content\": \"cessing\"}\n{\"role\": \"assistant\", \"type\": \"message\", \"content\": \"your request\"}\n{\"role\": \"assistant\", \"type\": \"message\", \"content\": \"to generate a plot.\"}\n{\"role\": \"assistant\", \"type\": \"message\", \"end\": True}\n\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"start\": True}\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": \"plot = create_plot_from_data\"}\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": \"('data')\\ndisplay_as_image(plot)\"}\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": \"\\ndisplay_as_html(plot)\"}\n{\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"end\": True}\n\n# The computer will emit a confirmation chunk *before* running the code. You can break here to cancel the execution.\n\n{\"role\": \"computer\", \"type\": \"confirmation\", \"format\": \"execution\", \"content\": {\n    \"type\": \"code\",\n    \"format\": \"python\",\n    \"content\": \"plot = create_plot_from_data('data')\\ndisplay_as_image(plot)\\ndisplay_as_html(plot)\",\n}}\n\n{\"role\": \"computer\", \"type\": \"console\", \"start\": True}\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"output\", \"content\": \"a printed statement\"}\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"active_line\", \"content\": \"1\"}\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"active_line\", \"content\": \"2\"}\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"active_line\", \"content\": \"3\"}\n{\"role\": \"computer\", \"type\": \"console\", \"format\": \"output\", \"content\": \"another printed statement\"}\n{\"role\": \"computer\", \"type\": \"console\", \"end\": True}\n```\n\n----------------------------------------\n\nTITLE: Managing Chat Conversations\nDESCRIPTION: Demonstrates how to save and restore chat conversations in both terminal and Python interfaces\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/guides/basic-usage.mdx#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --conversations\n```\n\nLANGUAGE: python\nCODE:\n```\n# Save messages to 'messages'\nmessages = interpreter.chat(\"My name is Killian.\")\n\n# Reset interpreter (\"Killian\" will be forgotten)\ninterpreter.messages = []\n\n# Resume chat from 'messages' (\"Killian\" will be remembered)\ninterpreter.messages = messages\n```\n\n----------------------------------------\n\nTITLE: Setting Python Keyring Backend for Troubleshooting\nDESCRIPTION: Command to set the Python keyring backend to resolve potential issues with Poetry installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring\n```\n\n----------------------------------------\n\nTITLE: Implementing E2B Custom Language Class for Open Interpreter\nDESCRIPTION: Defines a custom PythonE2B class that implements the required interface for Open Interpreter integration, including run, stop, and terminate methods. Uses E2B for code execution in a sandboxed environment.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/integrations/e2b.mdx#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport e2b\n\nclass PythonE2B:\n    \"\"\"\n    This class contains all requirements for being a custom language in Open Interpreter:\n\n    - name (an attribute)\n    - run (a method)\n    - stop (a method)\n    - terminate (a method)\n\n    Here, we'll use E2B to power the `run` method.\n    \"\"\"\n\n    # This is the name that will appear to the LLM.\n    name = \"python\"\n\n    # Optionally, you can append some information about this language to the system message:\n    system_message = \"# Follow this rule: Every Python code block MUST contain at least one print statement.\"\n\n    # (E2B isn't a Jupyter Notebook, so we added ^ this so it would print things,\n    # instead of putting variables at the end of code blocks, which is a Jupyter thing.)\n\n    def run(self, code):\n        \"\"\"Generator that yields a dictionary in LMC Format.\"\"\"\n\n        # Run the code on E2B\n        stdout, stderr = e2b.run_code('Python3', code)\n\n        # Yield the output\n        yield {\n            \"type\": \"console\", \"format\": \"output\",\n            \"content\": stdout + stderr # We combined these arbitrarily. Yield anything you'd like!\n        }\n\n    def stop(self):\n        \"\"\"Stops the code.\"\"\"\n        # Not needed here, because e2b.run_code isn't stateful.\n        pass\n\n    def terminate(self):\n        \"\"\"Terminates the entire process.\"\"\"\n        # Not needed here, because e2b.run_code isn't stateful.\n        pass\n\n# (Tip: Do this before adding/removing languages, otherwise OI might retain the state of previous languages:)\ninterpreter.computer.terminate()\n\n# Give Open Interpreter its languages. This will only let it run PythonE2B:\ninterpreter.computer.languages = [PythonE2B]\n\n# Try it out!\ninterpreter.chat(\"What's 349808*38490739?\")\n```\n\n----------------------------------------\n\nTITLE: Linking to Open Interpreter's Telemetry Function\nDESCRIPTION: This snippet provides a link to the telemetry function in Open Interpreter's GitHub repository. The function is responsible for anonymously tracking usage data.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/troubleshooting/faq.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[function](https://github.com/OpenInterpreter/open-interpreter/blob/main/interpreter/core/core.py#L167)\n```\n\n----------------------------------------\n\nTITLE: Checking Conversation History Location Using Shell Command\nDESCRIPTION: A shell command that displays where conversation histories are saved in Open Interpreter. When executed, it opens a menu where users can navigate to and open the folder containing saved conversations.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/usage/python/conversation-history.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --conversations\n```\n\n----------------------------------------\n\nTITLE: Fetching Calendar Events in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function retrieves calendar events for a given date or date range from all calendars using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.calendar.get_events(start_date=datetime, end_date=datetime)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dangerous Command (Bash)\nDESCRIPTION: This snippet shows an example of a dangerous command that GPT-4 would refuse to execute due to its potential harmful effects on the system.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/safety/introduction.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /\n```\n\n----------------------------------------\n\nTITLE: Retrieving Contact Phone Numbers in Python using Open Interpreter (Mac only)\nDESCRIPTION: This function returns the phone number of a specified contact name using the Open Interpreter computer API. It is only available on Mac.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/code-execution/computer-api.mdx#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.computer.contacts.get_phone_number(\"John Doe\")\n```\n\n----------------------------------------\n\nTITLE: API Key Configuration\nDESCRIPTION: Setup for ElevenLabs and OpenAI API keys.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\neleven_labs_api_key = \"<your_api_key>\" # https://elevenlabs.io/speech-synthesis\nopenai_api_key = \"<your_api_key>\" # https://platform.openai.com/account/api-keys\n```\n\n----------------------------------------\n\nTITLE: JARVIS Text Interface Implementation\nDESCRIPTION: Text-only version of JARVIS interface using Gradio for chat interactions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/examples/JARVIS.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    # ... (full implementation)\n```\n\n----------------------------------------\n\nTITLE: Running Offline Documentation Server\nDESCRIPTION: Commands for navigating to the docs directory and starting the Mintlify documentation server.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Assuming you're at the project's root directory\ncd ./docs\n\n# Run the documentation server\nmintlify dev\n```\n\n----------------------------------------\n\nTITLE: Manual Code Formatting with Black and isort\nDESCRIPTION: Commands for manually formatting code using Black and isort if not using pre-commit hooks.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nblack .\nisort .\n```\n\n----------------------------------------\n\nTITLE: Using Open Interpreter in Python\nDESCRIPTION: Python code snippet demonstrating how to import and use Open Interpreter for chat functionality.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom interpreter import interpreter\n\ninterpreter.chat(\"Plot AAPL and META's normalized stock prices\") # Executes a single command\ninterpreter.chat() # Starts an interactive chat\n```\n\n----------------------------------------\n\nTITLE: Customizing System Message in Python\nDESCRIPTION: Shows how to modify the system message for Open Interpreter to extend functionality or modify permissions.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninterpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)\n```\n\n----------------------------------------\n\nTITLE: Installing FastAPI Server Dependencies\nDESCRIPTION: Commands for installing required dependencies and starting the FastAPI server with uvicorn.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npip install fastapi uvicorn\nuvicorn server:app --reload\n```\n\n----------------------------------------\n\nTITLE: Launching Open Interpreter in Terminal\nDESCRIPTION: Command to start the Open Interpreter interactive interface in the terminal after installation.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/README_DE.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in MacOS Environment\nDESCRIPTION: Command to add OpenAI API key as an environment variable in MacOS bash/zsh profile.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/gpt-4-setup.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI\\_API\\_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Windows Environment\nDESCRIPTION: Command to set OpenAI API key as an environment variable in Windows command prompt.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/docs/language-models/hosted-models/gpt-4-setup.mdx#2025-04-22_snippet_2\n\nLANGUAGE: batch\nCODE:\n```\nsetx OPENAI\\_API\\_KEY \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Changing Language Model in Terminal\nDESCRIPTION: Commands to change the language model used by Open Interpreter in the terminal.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ninterpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Responses in Python\nDESCRIPTION: Example of using Open Interpreter to stream chat responses chunk by chunk in Python.\nSOURCE: https://github.com/openinterpreter/open-interpreter/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmessage = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)\n```"
  }
]